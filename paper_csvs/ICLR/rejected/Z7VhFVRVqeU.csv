Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0036231884057971015,"Neural Processes learn to ﬁt a broad class of stochastic processes with neural net-
works. Modeling functional uncertainty is an important aspect of learning stochastic
processes. Recently, Bootstrapping Neural Processes (B(A)NP) propose a bootstrap
method to capture the functional uncertainty which can replace the latent variable in
(Attentive) Neural Processes ((A)NP), thus overcoming the limitations of Gaussian
assumption on the latent variable. However, B(A)NP conduct bootstrapping in a
non-parallelizable and memory-inefﬁcient way and fail to capture diverse patterns
in the stochastic processes. Furthermore, we found that ANP and BANP both tend
to overﬁt in some cases. To resolve these problems, we propose an efﬁcient and
easy-to-implement approach, Neural Bootstrapping Attentive Neural Processes
(NeuBANP). NeuBANP learns to generate the bootstrap distribution of random func-
tions by injecting multiple random weights into the encoder and the loss function.
We evaluate our models in benchmark experiments including Bayesian optimization
and contextual multi-armed bandit. NeuBANP achieves the best performance in the
sequential decision-making tasks among NP methods, and this empirically shows
that our method greatly improves the quality of functional uncertainty modeling."
INTRODUCTION,0.007246376811594203,"1
INTRODUCTION"
INTRODUCTION,0.010869565217391304,"Neural Processes (NP) (Garnelo et al., 2018b) deﬁne distributions over functions given a set of
observations, and are trained via a meta-learning framework so that it can adapt to new functions
rapidly. NP learns to model a wide range of stochastic processes and can estimate the uncertainty over
the predictions with less computational effort, compared to Gaussian Processes (GP) (Rasmussen,
2003). However, NP frequently suffers from a fundamental drawback of underﬁtting. As a remedy
of the underﬁtting issue, Attentive Neural Processes (ANP) (Kim et al., 2018) applies the attention
modules to the encoder network. Despite this modiﬁcation, a single Gaussian latent variable of (A)NP
has a limitation in inducing functional uncertainty (Louizos et al., 2019), a global uncertainty that
decides the distribution over the space of trajectories or functions."
INTRODUCTION,0.014492753623188406,"Appropriate modeling of functional uncertainty in stochastic processes improves the predictive
performance and diversity in function realizations (Le et al., 2018), thus provides a principled way
to guide agents to ﬁnd optimal candidates in sequential decision-making problems. In these tasks, a
model needs to approximate a function and estimate uncertainty correctly to optimize a black-box
function whose analytic information is not given. Although GP is widely used for these tasks, these
are the promising area for the application of NP because GP is computationally expensive, and it can
be hard to choose an appropriate prior. Recently, Bootstrapping Neural Processes (B(A)NP) (Lee
et al., 2020) modify (A)NP to induce more robust uncertainty estimation by employing the residual
bootstrapping. However, B(A)NP underperforms in capturing a functional uncertainty because the
residual bootstrapping works in a homoscedastic way, removing the connection between the feature
and the label in its bootstrapped samples. The bootstrap strategy used in B(A)NP demands a higher
computational burden compared to (A)NP since it requires multiple computations of the encoder
network and additional heuristics, including the adaptation layer and the lower bound on the variance1.
Furthermore, ANP and BANP tend to overﬁt for a simple regression task rather than learning the
underlying heteroscedasticity. We observed that this problem is a by-product of the attention modules"
INTRODUCTION,0.018115942028985508,"1We colored the revised or added sentences in blue only for the rebuttal. This color will not appear in the
ﬁnal version of the manuscript."
INTRODUCTION,0.021739130434782608,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.025362318840579712,"used in both models (see Figure 1 and 5). This ﬁnding suggests that effective regularization of
attention modules is required to prevent overﬁtting. Additionally, as explained in Section 4, ANP and
BANP often tend to estimate homogeneous uncertainty regardless of whether the observation is given."
INTRODUCTION,0.028985507246376812,"Fig. 1. Each plot shows predictions given by ANP, BANP, and the proposed method in a linear regression.
The ground-truth function is a simple linear function with heterogeneous variance: y = x + βϵ(x) where
ϵ(x) ∼N(0, σ2(x)) and σ(x) =
√"
INTRODUCTION,0.03260869565217391,x2 + 10−5. See Appendix B.1 for more details.
INTRODUCTION,0.036231884057971016,"To resolve the problems of previous NPs, we introduce a novel bootstrapping method for neural
processes, Neural Bootstrapping Attentive Neural Processes (NeuBANP). Motivated from the recent
work on efﬁcient bootstrapping of the neural network, Neural Bootstrapper (NeuBoots) (Shin et al.,
2021), we introduce bootstrapping of the attention in a computationally efﬁcient way by simple
modiﬁcation of the input of attention modules and the loss function, instead of memory-inefﬁcient
resampling and contrived heuristics employed in BANP. The simplicity and computational efﬁciency
of NeuBANP directly come from NeuBoots, but it does not guarantee the performance in modeling the
functional uncertainty since NeuBANP operates on the meta-learning framework. Thus, we modify the
method for training the model to learn the randomness present in the underlying function, allowing the
model to estimate random functions generated by any stochastic process. This modiﬁcation is simple
but gives a strong consistency between the bootstrapped samples and the representations from the
attention modules, unlike the residual bootstrapping used in BANP. Besides, our bootstrapping method,
which implements the concatenation and multiplication of random bootstrap weights, operates as
a regularizer on the attention networks, thus preventing overﬁtted predictions observed in previous
attention-based NP methods. NeuBANP is trained to generate a valid predictive distribution by utilizing
the uncertainty inherent in observations by bootstrapping instead of the uncertainty that depends
on the prior assumption on the latent variable as in (A)NP. This leads to the success in capturing
heteroscedasticity of the data and modeling functional uncertainty in stochastic processes. As a result,
NeuBANP achieves the best performance in sequential decision-making problems such as Bayesian
optimization and contextual multi-armed bandit. The experimental results demonstrate that our model
provides promising capabilities as an efﬁcient neural approximation of stochastic processes."
INTRODUCTION,0.03985507246376811,"Contributions
We propose NeuBANP, an easy-to-implement and computationally efﬁcient method
for bootstrapping ANP. The proposed method has a novelty in learning a generator for bootstrapping
stochastic processes under a meta-learning framework. NeuBANP resolves overﬁtting problem of
attention modules and shows robust performance on heteroscedastic models without heuristics like
the extra adaptation layer in BANP. NeuBANP estimates functional uncertainty better than BANP and
achieves the best performance in stochastic optimization problems, including multi-dimensional
Bayesian optimization and contextual multi-armed bandit, compared to previous NP methods."
PRELIMINARIES,0.043478260869565216,"2
PRELIMINARIES"
META-LEARNING FRAMEWORK OF NEURAL PROCESSES,0.04710144927536232,"2.1
META-LEARNING FRAMEWORK OF NEURAL PROCESSES"
META-LEARNING FRAMEWORK OF NEURAL PROCESSES,0.050724637681159424,"Consider data D = (X, Y ) = {(xi, yi)}n
i=1 ⊂X ×Y, the pairs of inputs xi ∈X and outputs yi ∈Y.
Let P be a probability distribution over functions f ∈F; yi = f(xi) + ϵi where ϵi∼N(0, σ2
i ),
hence P determines the distribution of D. For disjoint index sets C and T satisfying C ∪T = [n],
deﬁne context DC = (XC, YC) = {(xc, yc)}c∈C and target DT = (XT , YT ) = {(xt, yt)}t∈T , so
that D = DC ∪DT . The task is to learn the neural processes pθ that ﬁts the stochastic processes"
META-LEARNING FRAMEWORK OF NEURAL PROCESSES,0.05434782608695652,Under review as a conference paper at ICLR 2022
META-LEARNING FRAMEWORK OF NEURAL PROCESSES,0.057971014492753624,"f ∼P given DC when C ∼Pn is a randomly chosen subset of [n], as follows:"
META-LEARNING FRAMEWORK OF NEURAL PROCESSES,0.06159420289855073,"θ⋆= argmin
θ
Ef∼P
h
EC∼Pn
h
−log pθ(Y |X, DC)
ii
.
(1)"
META-LEARNING FRAMEWORK OF NEURAL PROCESSES,0.06521739130434782,"This meta-learning framework allows pθ to learn diverse patterns in F via a prior distribution P;
hence NP can predict target points conditioned on contexts adaptively in the inference phase."
META-LEARNING FRAMEWORK OF NEURAL PROCESSES,0.06884057971014493,"2.2
(BOOTSTRAPPING) ATTENTIVE NEURAL PROCESSES"
META-LEARNING FRAMEWORK OF NEURAL PROCESSES,0.07246376811594203,"ANP
ANP is a variant of NP equipped with attention modules in the encoder part. See Appendix
A for the detailed deﬁnition of attention operations. Let the context DC is given, and the model
aims to infer y for a given feature x ∈X. The encoder network of ANP maps (x, DC) into a pair of
representation vectors r = (z, h) as follows:"
META-LEARNING FRAMEWORK OF NEURAL PROCESSES,0.07608695652173914,"{sc}c∈C = SelfAttn(DC),
sC = mean({sc}c∈C),
z ∼N(z|µz(sC), σ2
z(sC))
(2)
{hc}c∈C = SelfAttn(DC),
h = CrossAttn(x, XC, {hc}c∈C)
(3)
Here, µz and σz are single linear layers that map sC to mean and standard deviation of the latent
variable z, respectively. In ANP, z and h refer to latent path and deterministic path, respectively. The
deterministic path models the overall skeleton of the encoder network, while the latent path models the
functional uncertainty using a stochastic global latent variable (Garnelo et al., 2018b; Kim et al., 2018).
Then the decoder network takes the representations r and target data x as inputs to predict µ(x, r)
and σ(x, r), the parameters of the conditional predictive distribution p(y|x, DC) = N(y|µ, σ2)."
META-LEARNING FRAMEWORK OF NEURAL PROCESSES,0.07971014492753623,"BANP
The global latent variable in ANP potentially limits the ﬂexibility in expressing functional
uncertainty. BANP proposes a method that utilizes paired bootstrapping and residual bootstrapping
to model stochasticity in a data-driven way. First, they resample pairs of (xc, yc) with replacement
to construct ˆD(b)
C
= ( ˆX(b)
C , ˆY (b)
C
). Here b = 1, . . . , B implies the number of bootstrap samples. The
resampled context is encoded into the representation vector ˆr(b) = (ˆz(b), ˆh(b)) by replacing DC
in (2) and (3) by ˆD(b)
C . Then BANP conducts the residual bootstrapping to construct bootstrapped
contexts again ˜D(b)
C
= ( ˜X(b)
C , ˜Y (b)
C
). By using DC and ˜D(b)
C
in (2) and (3) separately, BANP gets the
representations of contexts (r, ˜r(b)). Finally, BANP uses an adaptation layer to merge (r, ˜r(b)) and
obtain µ(b), σ(b) through the decoder network (see Figure 2)."
META-LEARNING FRAMEWORK OF NEURAL PROCESSES,0.08333333333333333,"p(y|x, r, ˜r(b)) = N(y|µ(b), (σ(b))2),
p(y|x, DC) ≈1 B B
X"
META-LEARNING FRAMEWORK OF NEURAL PROCESSES,0.08695652173913043,"b=1
p(y|x, r, ˜r(b)).
(4)"
META-LEARNING FRAMEWORK OF NEURAL PROCESSES,0.09057971014492754,"Due to the residual bootstrapping, BANP conducts the encoder computation three times and the
decoder computation twice for a single forward propagation. These additional calculations cause the
computational bottleneck in the training and inference (see Appendix B.6)."
NEURAL BOOTSTRAPPER,0.09420289855072464,"2.3
NEURAL BOOTSTRAPPER"
NEURAL BOOTSTRAPPER,0.09782608695652174,"Repetitions of training restrain the practical use of bootstrap procedures in deep neural networks due
to their high computational burden. To alleviate this, Shin et al. (2021) proposes Neural Bootstrapper
(NeuBoots). NeuBoots circumvents multiple training of networks by learning a bootstrap generator."
NEURAL BOOTSTRAPPER,0.10144927536231885,"Random Weight Bootstrapping
Let P"
NEURAL BOOTSTRAPPER,0.10507246376811594,"c∈C ℓ(f(xc), yc) be the loss function of interest for a
neural network f. In standard bootstrap procedures, a bootstrapped neural network can be obtained
by minimizing the loss function weighted by a random bootstrap weight wC := {wc : c ∈C}:"
NEURAL BOOTSTRAPPER,0.10869565217391304,"L(wC, f, DC) =
X"
NEURAL BOOTSTRAPPER,0.11231884057971014,"c∈C
wcℓ(f(xc), yc).
(5)"
NEURAL BOOTSTRAPPER,0.11594202898550725,"According to the choice of the distribution on wC, various bootstrap procedures can be represented
under the form of (5); e.g., the paired bootstrap by wC ∼Multinomial(n; 1/n, . . . , 1/n) and the
Random Weight Bootstrapping (RWB) (Præstgaard & Wellner, 1993; Newton & Raftery, 1994)
by wC ∼|C| × Dirichlet(1, . . . , 1). NeuBoots utilizes RWB to avoid the data discard problem
which can occur in the standard bootstrapping. We then compute bootstrapped neural networks
{ bf (b) : b = 1, . . . , B} via minimization of (5) for sampled w(1)
C , . . . , w(B)
C
."
NEURAL BOOTSTRAPPER,0.11956521739130435,Under review as a conference paper at ICLR 2022
NEURAL BOOTSTRAPPER,0.12318840579710146,"Learning To Generate Bootstrap Distribution
The main idea of NeuBoots is to construct a single
generative network that models the bootstrapped neural networks with varying bootstrap weights in
(5). This formulation modiﬁes the backbone network in a form of f(x, wC) that inputs both feature x
and bootstrap weight wC. Shin et al. (2021) show that the minimizer of the following loss generates
valid bootstrap evaluations that match the results of the standard bootstrap procedure:"
NEURAL BOOTSTRAPPER,0.12681159420289856,"L(f, DC) = EwC∼|C|×Dirichlet(1,...,1) [L(wC, f(·, wC), DC))] ,"
NEURAL BOOTSTRAPPER,0.13043478260869565,"We call this weighted bootstrapping loss. Once this generator is trained via a single optimization
procedure, we can efﬁciently generate bootstrapped predictions by plugging random bootstrap weights
in the trained generator; i.e., for a feature of interest x, the trained generator inputs {w(b)
C }B
b=1 and
produces bootstrapped predictions ˆy(b) = ˆf(x, w(b)
C ) for b = 1, . . . , B."
NEURAL BOOTSTRAPPING ATTENTIVE NEURAL PROCESSES,0.13405797101449277,"3
NEURAL BOOTSTRAPPING ATTENTIVE NEURAL PROCESSES"
NEURAL BOOTSTRAPPING ATTENTIVE NEURAL PROCESSES,0.13768115942028986,"We propose a novel class of NP, called Neural Bootstrapping Attentive Neural Processes (NeuBANP).
Aligned with the previous formulation of NP families, we can deﬁne our model pθ as:"
NEURAL BOOTSTRAPPING ATTENTIVE NEURAL PROCESSES,0.14130434782608695,"pθ(Y |X, DC) =
ˆ
pϕ(Y |X, h, z)q(z|DC)dz =
ˆ
n
Y"
NEURAL BOOTSTRAPPING ATTENTIVE NEURAL PROCESSES,0.14492753623188406,"i=1
pϕ(yi|xi, h, z)q(z|DC)dz
(6) ≈ n
Y"
NEURAL BOOTSTRAPPING ATTENTIVE NEURAL PROCESSES,0.14855072463768115,"i=1
pϕ(yi|xi, h, z) where z ∼q(z|DC).
(7)"
NEURAL BOOTSTRAPPING ATTENTIVE NEURAL PROCESSES,0.15217391304347827,"Here pϕ is the decoder and q denotes the posterior distribution. Since the above integral is intractable,
we approximate the predictive distribution by sampling z from the bootstrap distribution q(·|DC), in-
stead of Gaussian distribution as in (A)NP. Precisely, we train a generative encoder network gφ, which
outputs bootstrapped representation pairs (h, z) = {(h(b), z(b))}B
b=1 = gφ(X, DC, {w(b)
C }B
b=1).
Through the meta-learning framework (1), our model learns to generate bootstrapped predictions
for an arbitrarily given function f ∼P. Thus, our approach can be regarded as a learn-to-bootstrap
method for stochastic processes. Compared to the ﬁxed bootstrap method used in BANP, our learnable
bootstrap method can ﬁnd the best strategy to appropriately generate the random functions regarding
the given context and the general property of the underlying data generating process. This will lead
to the better modeling of functional uncertainty of the target stochastic processes. Also, note that
NeuBANP can obtain a number of bootstrapped predictions by simply plugging different bootstrap
weights into gφ, while BANP needs repetitive data resampling from scratch. Figure 2 shows the
difference between the forward computation paths of BANP and NeuBANP in detail."
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.15579710144927536,"3.1
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.15942028985507245,"To train a generative network, which outputs the bootstrapped representations, we modify the encoder
in ANP to take both (x, DC) and bootstrap weight wC as inputs. We introduce the posterior and
the prediction paths in the encoder, analogous to NP’s latent and deterministic paths. NeuBANP is
designed to leverage RWB, which is theoretically proven as a valid bootstrap method. In detail, the
posterior and prediction path in NeuBANP take the random bootstrap weight as an auxiliary input.
This input provides enough randomness into the network so that our model can successfully capture
the functional uncertainty."
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.16304347826086957,"Posterior path
We tag each context (xc, yc) ∈DC with a bootstrap weight w(b)
c
to construct
bootstrapped contexts D(b)
C
:= {(xc, yc, w(b)
c )}c∈C. Then the posterior path receives D(b)
C
as input
and outputs a latent variable z(b). In detail, we apply self-attention to D(b)
C
and multiply the resultant
representation z(b)
C
by the bootstrap weight w(b)
C
before mean aggregation. This path connects the con-
texts with weighted bootstrapping loss during the training and allows the bootstrap weights to model
bootstrapped posterior distribution q by controlling the magnitude of each context representation."
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.16666666666666666,"Prediction path
The prediction path outputs a target-speciﬁc representation h(b) that is relevant
for the prediction. We apply self-attention to contexts DC and multiply the bootstrap weight w(b)
C"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.17028985507246377,Under review as a conference paper at ICLR 2022
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.17391304347826086,"pairwise  
bootstrap"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.17753623188405798,(a) BANP
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.18115942028985507,Posterior path
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.18478260869565216,Prediction path
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.18840579710144928,"Self
Attention mean"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.19202898550724637,"Self
Attention ⊙ key query"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.1956521739130435,"Bootstrapping
copy MLP MLP value"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.19927536231884058,(b) NeuBANP ⊙
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.2028985507246377,"Cross 
Attention"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.20652173913043478,"MLP
|C| ⇥Dirichlet(1, . . ., 1) ⇠"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.21014492753623187,CWXicbVDLSsNAFJ3EV42PVrt0M1gEVyUpoi4LblxWsD6wpUymNzo4mYSZG20J/RK3+lHizhJs7CtFwYO59zHmROmUhj0/W/HXVvf2NyqbXs7u3v79cbB4Z1JMs2hzxOZ6IeQGZBCQR8FSnhINbA4lHAfvl4V+v0baCMSdYvTFIYxe1YiEpyhpUaN+uMoH8QMXziT+dVsNmq0/LZfFl0FQVapKre6MDpDMYJz2JQyCUz5inwUxzmTKPgEmbeIDOQMv7KnuHJQsViMO8dD6jJ5YZ0yjR9imkJft3ImexMdM4tJ2FSbOsFeS/WsFoExkr0lV1bIpzS94wuhzmQqUZguJza1EmKSa0CI6OhQaOcmoB41rY31H+wjTjaONd2K7gHScIk8UDc7o07NmUg+VMV8Fdpx2ctzs3Z61ut8q7Ro7IMTklAbkgXJNeqRPOMnIB/kX86P67g15u3uk410yQL5TZ/Adf8tuE=</latexit>YC XC XT
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.213768115942029,"⊙
Elementwise product XT"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.21739130434782608,"Self
Attention query MLP value"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.2210144927536232,"mean
residual 
bootstrap"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.2246376811594203,Adaptation Layer
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.22826086956521738,"Cross 
Attention
key MLP"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.2318840579710145,"Self
Attention
MLP XC YC ˆXC ˆYC ˆXC ˆYC"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.23550724637681159,"Self 
Attention
MLP"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.2391304347826087,"Cross 
Attention
MLP"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.2427536231884058,"Self 
Attention
MLP"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.2463768115942029,"Self 
Attention
MLP"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.25,"Cross 
Attention
MLP MLP ˜XC ˜YC XC YC XT ˜XC ˜YC XC YC"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.2536231884057971,9cbR8YNJMs2hzxOZ6EHIDEihoI8CJQxSDSwOJTyGrzeF/vgG2ohE9XCWwihmz0pEgjO01LhRH4zYczwhTOZ9+bzcaPlt/2y6DoIKtAiVd2Nj5zOcJLwLAaFXDJjngI/xVHONAouYe4NMwMp46/sGZ4sVCwGM8pL53N6ZpkJjRJtn0Jasn8nchYbM4tD21mYNKtaQf6rFYw2kbEiXVcnpji34g2j61EuVJohKL6wFmWSYkKL4OhEaOAoZxYwroX9HeUvTDONt6l7QrecYowXT6woEvDnk05WM10HTx02sFlu3N/0ep2q7xr5IScknMSkCvSJbfkjvQJxn5IJ/ky/lxHbfmeotW16lmSp3OYv9mC28Q=</latexit>XT
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.2572463768115942,"Self 
Attention
XT ˆYT query"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.2608695652173913,"value
key query"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.2644927536231884,"value
key"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.26811594202898553,CWnicbVDLSgMxFE3HZ1sf9bFzEyCqzJTRF0WRHCpYB/QlpLJ3GlDM5khuVNbhv6JW/0nwY8xU7uwrQcCh3PuTU6On0h0HW/Cs7W9s7u3n6xVD44PDqunJy2TJxqDk0ey1h3fGZACgVNFCihk2hgkS+h7Y8fcr89AW1ErF5xlkA/YkMlQsEZWmlQqfQpoiYPSoeB6Dng0rVrbkL0E3iLUmVLPE8OCnUe0HM0wgUcsmM6Xpugv2MaRcwrzUSw0kjI/ZELqWKhaB6WeL6HN6ZWAhrG2RyFdqH83MhYZM4t8OxkxHJl1Lxf/9XJFm9BYk26gcmfW8uG4X0/EypJERT/jRamkmJM8+ZoIDRwlDNLGNfC/o7yEdOMo+135XYFbzjNe51vyovAJduyt97pJmnVa95trf5yU20ln3vkwtySa6JR+5IgzyRZ9IknEzIO/kgn4Vvx3GKTvl31Cksd87ICpzHzVKt4s=</latexit>Encoder
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.2717391304347826,Decoder
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.2753623188405797,Dcqe2DP0Tt/pPgh9jpnZhWw8EDufcm5wcP5HCoOt+FZyt7Z3dvf1iqXxweHRcOTltmTjVHJo8lrHu+MyAFAqaKFBCJ9HAIl9C2x8/5H57AtqIWL3iLIF+xIZKhIztNKgUukhTBExewQeB6Dng0rVrbkL0E3iLUmVLPE8OCnUe0HM0wgUcsmM6Xpugv2MaRcwrzUSw0kjI/ZELqWKhaB6WeL6HN6ZWAhrG2RyFdqH83MhYZM4t8OxkxHJl1Lxf/9XJFm9BYk26gcmfW8uG4X0/EypJERT/jRamkmJM8+ZoIDRwlDNLGNfC/o7yEdOMo+135XYFbzjNe51vyovAJduyt97pJmnVa95trf5yU20ln3vkwtySa6JR+5IgzyRZ9IknEzIO/kgn4Vvx3GKTvl31Cksd87ICpzHyIbt4E=</latexit>Decoder
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.27898550724637683,Dcqe2DP0Tt/pPgh9jpnZhWw8EDufcm5wcP5HCoOt+FZyt7Z3dvf1iqXxweHRcOTltmTjVHJo8lrHu+MyAFAqaKFBCJ9HAIl9C2x8/5H57AtqIWL3iLIF+xIZKhIztNKgUukhTBExewQeB6Dng0rVrbkL0E3iLUmVLPE8OCnUe0HM0wgUcsmM6Xpugv2MaRcwrzUSw0kjI/ZELqWKhaB6WeL6HN6ZWAhrG2RyFdqH83MhYZM4t8OxkxHJl1Lxf/9XJFm9BYk26gcmfW8uG4X0/EypJERT/jRamkmJM8+ZoIDRwlDNLGNfC/o7yEdOMo+135XYFbzjNe51vyovAJduyt97pJmnVa95trf5yU20ln3vkwtySa6JR+5IgzyRZ9IknEzIO/kgn4Vvx3GKTvl31Cksd87ICpzHyIbt4E=</latexit>Decoder XC
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.2826086956521739,Encoder XC
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.286231884057971,JMzdqDfkXt/pH7vwUJ20XtvXCwOGc+zhzgkQKg5737bhr6xubW6Xt8s5uZW+/WjvomDjVHNo8lrF+DJgBKRS0UaCEx0QDiwIJ3eD5ptC7L6CNiNUDThLoR2ykRCg4Q0sNqkc9I0YRG2S9iOGYM5k95PmgWvca3rToKvDnoE7mdTeoOc3eMOZpBAq5ZMY8+V6C/YxpFxCXu6lBhLGn9kInixULALTz6b2c3pmSENY2fQjpl/05kLDJmEgW2szBplrWC/FcrG1CY0W6qg5NcW7JG4bX/UyoJEVQfGYtTCXFmBbp0aHQwFOLGBcC/s7ysdM424XtCl7xDeFt8cCMnhou25T95UxXQafZ8C8bzfuLeqs1z7tETsgpOSc+uSItckvuSJtw8k4+yCf5cn7cTbfi7s9aXWc+c0gWyj3+Bc8uTg=</latexit>σT µT ˆh
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.2898550724637681,"ˆz
ˆzC ˆhC wC
zC hC
wC wC r ˜r h z hC zC ˜hC"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.29347826086956524,Pdx89frL3dLz/7Mzb1gmcCausuyjBo5IGZyRJ4UXjEHSp8Ly8Pu7j51/ReWnNKS0bLDRcGlLARSpxZjnJFWFIdAV2UdvnfdYuUIUOE4euNJOk0H49sgW4MJW9vJYn90mldWtBoNCQXez7O0oSKAIykUdrt567EBcQ2XOI/QgEZfhGVjr+KTMVr6+IzxAf234oA2vulLmNmP6S/HevJ/8XmLdXviBN0xIasWpUt4qT5b0uvJIOBalBCcjLNycQUOBEX1NroY/EY3hDfUbdND+w26LHUkYlBYrcFUIReaqJtnRcj7T4x1Oiqd9+vWoKVahiEhjzeNAw0qTLhDtlt1bfB2ZtpdjhNvxMj6uL7LDXrCX7DXL2Ft2xD6zEzZjgv1gP9kv9jvZSw6S98mHVWoyWtc8ZxuWfPoLq2bNRg=</latexit>˜zC h z ˜h ˜z
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.2971014492753623,"Fig. 2. A single forward computation of (a) BANP and (b) NeuBANP. Note that the inference for each target point
requires B times of this forward computation."
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.3007246376811594,"element-wisely to compute another bootstrapped representation. Cross-attention module uses XC and
this bootstrapped representation as key-value pairs to which the target query x attends. Consequently,
reﬂecting the bootstrap weights shared with the posterior path, the prediction path models interactions
between the given context and the target input."
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.30434782608695654,We summarize the above posterior and prediction paths as follows:
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.3079710144927536,"z(b)
C
= {z(b)
c }c∈C = SelfAttn(D(b)
C ),
z(b) = mean(z(b)
C
⊙w(b)
C )
(8)"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.3115942028985507,"hC = {hc}c∈C = SelfAttn(DC),
h(b) = CrossAttn(x, XC, hC ⊙w(b)
C )
(9)"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.31521739130434784,"where ⊙denotes element-wise multiplication. We concatenate random bootstrap weights to context
data, unlike NeuBoots, which only utilizes random weight multiplication in the ﬁnal layer. Concatena-
tion of weights and the contexts in the posterior path yields two effects. First, concatenating random
information in given contexts provides sufﬁcient randomness to the model, allowing it to cover a
wide range of function samples in a space where a true function is likely to exist. Second, we made
this modiﬁcation to maximize the use of random weights and provide bootstrapping information
to the model without resampling the data, enabling better uncertainty estimation. Additionally, by
multiplying the random weights to the representations from both paths, as in (8) and (9), the repre-
sentations are consistent with the weights and maximize bootstrapping effect. We think that these
multiplications also provide the effect of regularization, which mitigates the overﬁtting tendency
which ANP and BANP show in a simple regression experiment (see Figure 1)."
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.3188405797101449,"Decoder and Bootsrapped Prediction
The decoder takes (z(b), h(b)) from the encoder and target
x ∈X as inputs to generate a prediction ˆy(b) = MLP(x, z(b), h(b)). We can generate bootstrap sam-
ples ˆy(1), ˆy(2), ..., ˆy(B) by plugging w(1)
C , w(2)
C , ..., w(B)
C
into the encoder gφ(x, DC, ·), respectively.
We estimate the predictive mean and standard deviation using bootstrap samples: µ = 1 B B
X"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.322463768115942,"b=1
ˆy(b),
σ ="
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.32608695652173914,"v
u
u
t
1
B −1 B
X"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.32971014492753625,"b=1
(ˆy(b) −µ)2
(10)"
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.3333333333333333,"In the previous NP methods, the decoder directly outputs the parameters of the predictive distribution,
but the decoder of NeuBANP outputs the stochastic predictions {ˆy(b)}B
b=1. This design of output is
a distinction of our model from other NP methods and allows the nonparametric estimation. Due
to this structure, existing NPs set the lower bound of standard deviation for robust performance."
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.33695652173913043,Under review as a conference paper at ICLR 2022
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES,0.34057971014492755,"We found that the performance was susceptible to the lower bound value. However, NeuBANP
naturally obtains parameters of predictive distribution using bootstrap predictions and shows better
performance without such heuristics. It also has the advantage of calculating higher-order statistics
without changing the structure."
TRAINING,0.3442028985507246,"3.2
TRAINING"
TRAINING,0.34782608695652173,"Weighted Bootstrapping Loss
We train NeuBANP with weighted loss similar to that of NeuBoots
as demonstrated in Section 2.3. Only context data has the corresponding bootstrap weight in our
setting, but the model still has to ﬁt target data. Thus we designed loss function as a sum of weighted
context loss Lcontext and non-weighted target loss Ltarget as follows:"
TRAINING,0.35144927536231885,"Ltotal = 1 B B
X b=1 1
|C| X c∈C"
TRAINING,0.35507246376811596,"
−w(b)
c
log N(yc|µc, σ2
c)
"
TRAINING,0.358695652173913,"|
{z
}
Lcontext + 1 |T | X t∈T"
TRAINING,0.36231884057971014,"
−log N(yt|µt, σ2
t )
"
TRAINING,0.36594202898550726,"|
{z
}
Ltarget (11)"
TRAINING,0.3695652173913043,"where µc, σc, µt, and σt are computed by (10) for context (xc, yc) ∈DC and target (xt, yt) ∈DT .
Averaging weighted context loss with multiple bootstrap weights improves the robustness of a
model by showing various bootstrap samples during training. We trained the model with negative
log-likelihood (NLL). One can replace the NLL with a different loss function, such as cross-entropy
according to the target tasks."
EXPERIMENTS,0.37318840579710144,"4
EXPERIMENTS"
EXPERIMENTS,0.37681159420289856,"We conducted experiments to compare NeuBANP with the previous NP methods for regression
and sequential decision-making problems. For regression tasks, we conducted one-dimensional
(1D) regression experiments on random functions generated from GP prior and image completion
tasks as two-dimensional (2D) regression (see Appendix B.5 for image completion). For sequential
decision-making problems, we evaluated each method in Bayesian optimization (BO) and Contextual
Multi-Armed Bandit (CMAB)."
NONPARAMETRIC REGRESSION AND UNCERTAINTY ESTIMATION,0.3804347826086957,"4.1
NONPARAMETRIC REGRESSION AND UNCERTAINTY ESTIMATION"
NONPARAMETRIC REGRESSION AND UNCERTAINTY ESTIMATION,0.38405797101449274,"Settings
We followed the settings in Lee et al. (2020). To obtain meta-training datasets, we sampled
batches of random functions from GP prior with RBF kernel, and context and target points were
chosen randomly from each function. In addition, kernel parameters of GP were randomly sampled so
that the models could learn about various functions. NeuBANP was trained with 10 bootstrap samples,
and we conﬁrmed that it is robust to the number of samples. Please refer to Appendix B.2 for details."
NONPARAMETRIC REGRESSION AND UNCERTAINTY ESTIMATION,0.38768115942028986,"Results
The numerical results are summarized in Table 2. ANP and BANP tend to estimate homo-
geneous uncertainties for all target points in a situation where context points are sufﬁciently given
(see Figure 3), because they place the heuristic lower bound on the variance. In the case of BANP,
homoscedasticity occurs even when the number of context is small. Another problem only occurs in
BANP, which is that it does not properly estimate functional uncertainty. Since the uncertainty about
the shape of the true function is high in the region where the context point is not given, models should
generate a wide range of function samples. However, we can see that BANP generates almost the same
functions, not like those of the other methods, so we argue that BANP is not an appropriate method
for modeling functional uncertainty. NeuBANP resolves these problems efﬁciently and achieves the
best performance except for target prediction in the Periodic kernel."
BAYESIAN OPTIMIZATION,0.391304347826087,"4.2
BAYESIAN OPTIMIZATION"
BAYESIAN OPTIMIZATION,0.39492753623188404,"Since NP can approximate a class of arbitrary functions, it can replace GP, the commonly used
surrogate model of BO. It is crucial to approximate the objective function from the given observations
using the surrogate model, but evaluating the acquisition function and determining the subsequent
samples are vital for efﬁcient exploration. We evaluated the proposed method on various black-box
functions, which may be unobserved in the meta-training step (see Algorithm 1)."
BAYESIAN OPTIMIZATION,0.39855072463768115,Under review as a conference paper at ICLR 2022
BAYESIAN OPTIMIZATION,0.40217391304347827,"Fig. 3. Comparison of ANP, BANP, and NeuBANP in 1D regression given 4 context points (top) and 20 context
points (bottom). Orange lines represent the ground-truth function. Blue lines are predictive mean given by each
model and shaded region denotes the standard deviation (amount of uncertainty). To visualize the quality of
functional uncertainty, we overlapped multiple shaded areas obtained with 30 sampled outputs for each input."
BAYESIAN OPTIMIZATION,0.4057971014492754,"Settings
For 1D, we followed the same setting in Lee et al. (2020). We set objective functions gen-
erated from GP with RBF, Matérn 5/2, and Periodic kernels and applied the models trained in Section
4.1. Furthermore, we demonstrated the BO performance of NeuBANP for multi-dimensional settings
(2D and 3D). We set objective functions as various benchmark functions used in the optimization
literature (Kim, 2020; Kim & Choi, 2017). See Appendix B.3 for details. A simple regret measured
the performance of each model, and the mean performance over 100 experiments is reported for
reliable evaluations."
BAYESIAN OPTIMIZATION,0.40942028985507245,"Fig. 4. Left: Multi-dimensional Bayesian optimization results on various benchmark functions with UCB as an
acquisition function. Bold lines represent the mean performance over 100 experiments. We indicate 20% of the
standard deviation. Right most: Time complexity of each model as the number of observations increases."
BAYESIAN OPTIMIZATION,0.41304347826086957,"Results
NeuBANP outperformed the other methods in BO experiments (see Figure 4 and 8). Table
3 and 4 shows numerical results. For multi-dimensional BO, NeuBANP achieved the best results for
every function except the Hartmann-3D when using Upper Conﬁdence Bound (UCB) as an acquisition
function. For the Hartmann-3D, the performance of NeuBANP is statistically comparable to the best
method (ANP) when considering the standard deviation. For Goldsteinprice and Rastrigin functions,
GP records poor performance due to its numerical errors during the optimization procedure (see
Appendix B.3). The rightmost plots of Figure 4 show the time complexity according to the number
of observation points. NeuBANP has the fastest decreasing rate of regrets in terms of iterations
in both cases. We also conducted BO experiments using Expected Improvement (EI) to test the
performance of NeuBANP to be independent of the selection of acquisition functions (Figure 9).
Figure 10 demonstrates the better exploration strategy of NeuBANP compared to BANP. The second"
BAYESIAN OPTIMIZATION,0.4166666666666667,Under review as a conference paper at ICLR 2022
BAYESIAN OPTIMIZATION,0.42028985507246375,"Regret
Method
δ = 0.5
δ = 0.7
δ = 0.9
δ = 0.95
δ = 0.99"
BAYESIAN OPTIMIZATION,0.42391304347826086,Cumulative
BAYESIAN OPTIMIZATION,0.427536231884058,"Uniform
100.00 ± 0.08
100.00 ± 0.09
100.00 ± 0.25
100.00 ± 0.37
100.00 ± 0.78
Neural Linear
0.95 ± 0.02
1.60 ± 0.03
4.65 ± 0.18
9.56 ± 0.36
49.63 ± 2.41
MAML
2.95 ± 0.12
3.11 ± 0.16
4.84 ± 0.22
7.01 ± 0.33
22.93 ± 1.57
NP
1.60 ± 0.06
1.75 ± 0.05
3.31 ± 0.10
5.71 ± 0.24
22.13 ± 1.23
ANP
2.17 ± 1.89
3.59 ± 8.03
5.63 ± 8.48
11.68 ± 8.97
24.75 ± 7.08
BANP
2.04 ± 1.52
2.34 ± 1.23
4.30 ± 0.77
6.76 ± 1.03
21.18 ± 1.69
NeuBANP
0.85 ± 0.22
1.02 ± 0.27
1.85 ± 0.56
3.04 ± 0.88
9.76 ± 1.93"
BAYESIAN OPTIMIZATION,0.4311594202898551,Simple
BAYESIAN OPTIMIZATION,0.43478260869565216,"Uniform
100.00 ± 0.45
100.00 ± 0.78
100.00 ± 1.18
100.00 ± 2.21
100.00 ± 4.21
Neural Linear
0.33 ± 0.04
0.79± 0.07
2.17 ± 0.14
4.08 ± 0.20
35.89 ± 2.98
MAML
2.49 ± 0.12
3.00 ± 0.35
4.75 ± 0.48
7.10 ± 0.77
22.89 ± 1.41
NP
1.04 ± 0.06
1.26 ± 0.21
2.90 ± 0.35
5.45 ± 0.47
21.45 ± 1.3
ANP
0.99 ± 1.68
1.50 ± 2.21
3.64 ± 4.71
6.32 ± 7.33
21.65 ± 1.72
BANP
1.22 ± 1.83
2.37 ± 3.04
3.27 ± 5.33
7.73 ± 12.16
20.63 ± 34.21
NeuBANP
0.86 ± 0.06
1.04 ± 0.08
1.88 ± 0.14
3.09 ± 0.23
9.96 ± 0.70"
BAYESIAN OPTIMIZATION,0.4384057971014493,"Table 1. Results of the wheel bandit problem according to the value of δ. Mean and standard deviation for
cumulative regret and simple regret over 50 runs are reported. Regrets are normalized to that of the uniform
policy."
BAYESIAN OPTIMIZATION,0.4420289855072464,"and fourth columns show the explored points by BANP and NeuBANP, respectively. In most cases,
NeuBANP converged to the optimum faster than BANP. The third and ﬁfth column shows the contour
plots of acquisition functions of each method. Note that NeuBANP accurately infers the potential
area of the optimum compared to BANP. As explained above, NeuBANP can estimate heterogeneous
uncertainty, and thus it is able to handle well for the exploration-exploitation trade-off, which is
essential in sequential decision-making problems."
CONTEXTUAL MULTI-ARMED BANDIT,0.44565217391304346,"4.3
CONTEXTUAL MULTI-ARMED BANDIT"
CONTEXTUAL MULTI-ARMED BANDIT,0.4492753623188406,"We conducted a CMAB experiment, the wheel bandit problem, as in Garnelo et al. (2018b) to show
that NeuBANP works efﬁciently as well as BO based on its performance of uncertainty estimation."
CONTEXTUAL MULTI-ARMED BANDIT,0.4528985507246377,"Settings
We followed the same environment in Garnelo et al. (2018b), but used UCB policy. For
more details, please refer to Appendix B.4. The parameter δ determines the environment of the
wheel bandit problem. As δ increases, high-reward observation becomes sparse, which makes the
problem more difﬁcult. We set the baselines of CMAB experiment to MAML (Finn et al., 2017), Neural
Linear (Riquelme et al., 2018) and NP. We measured cumulative regrets and simple regrets for 2,000
iterations to demonstrate the performance of each model."
CONTEXTUAL MULTI-ARMED BANDIT,0.45652173913043476,"Results
NeuBANP performed well for various δ values (see Table 1). Note that NeuBANP showed
better performance in challenging environments with sparse high rewards. NeuBANP showed the
ability to learn various reward distributions based on appropriate functional uncertainty modeling.
The result demonstrates that NeuBANP can utilize a small number of context information and make
accurate estimations. NeuBANP also has stable results as its small variance shows. On the other hand,
ANP has extremely high variance in the performance because it overﬁts to certain prediction and
failed to adapt to the various bandit environments."
RELATED WORK,0.4601449275362319,"5
RELATED WORK"
RELATED WORK,0.463768115942029,"Neural Processes
CNP (Garnelo et al., 2018a) uses a pair of encoder and decoder networks to
produce a predictive posterior distribution over functions. NP (Garnelo et al., 2018b) introduces a
global latent variable to embed functional uncertainty in the deterministic architecture of CNP and
predicts various outputs given the same context data. ANP (Kim et al., 2018) then enhances predictive
accuracy by replacing MLP modules in NP with the attention modules. BNP (Lee et al., 2020) proposes
the bootstrap method to model uncertainty in stochastic processes without the assumption of a single
latent variable on which previous methods rely. In addition to these works, there are many attempts
to use NPs in various tasks. Singh et al. (2019) tackles sequential stochastic processes, where the"
RELATED WORK,0.4673913043478261,Under review as a conference paper at ICLR 2022
RELATED WORK,0.47101449275362317,"dynamics of the given system changes as the time being. Leveraging time-variant context points,
Singh et al. (2019) models the underlying temporal 3D structures. Gordon et al. (2020) extends NP
families to contain translation equivariant functions, providing theoretical formulation to represent
translation-invariant functional representations. Louizos et al. (2019) do not assume explicit global
latent variables, instead supported by dependency graph among local latent variables, to encode
inductive bias for given data easier than NPs that use global latent variables."
RELATED WORK,0.4746376811594203,"Bootstrapping Neural Networks
Bootstrap method (Efron, 1987) is a reliable approach to esti-
mate predictive uncertainty (Lakshminarayanan et al., 2017; Osband et al., 2016). However, it is
computationally inefﬁcient to go through the feed-forward computation as much as the number of
bootstraps; hence, it discourages the practical application of bootstrap in neural networks. There have
been several works to circumvent this issue by approximating bootstrapped distribution. Amortized
bootstrap (Nalisnick & Smyth, 2017) approximates bootstrap distribution over model parameters
by using amortized inference and implicit models. Generative Bootstrap Sampler (Shin et al., 2020)
proposes a computational bootstrap procedure that constructs a generator function of bootstrap evalu-
ations for classical statistical models. Neural Bootstrapper (Shin et al., 2021) suggests a simple recipe
for generating bootstrapped predictive distributions of MLPs and convolutional neural networks."
RELATED WORK,0.4782608695652174,"Meta-Learning based Stochastic Optimization
NPs are trained with a meta-learning framework
to solve various tasks related to the data generation process through a single optimization. Santoro
et al. (2016); Chen et al. (2017) follow the same training procedure of NP. However, Santoro et al.
(2016) proposes an memory-augmented network for robust meta-learning, while Chen et al. (2017)
proposes the method to produce an algorithm for black-box optimization using recurrent networks.
Sharaf & Daumé III (2019) presents a meta-learning algorithm for learning a good exploration policy
in the contextual bandit. Ravi & Beatson (2019) also solves contextual bandits based on the Bayesian
framework by inferring a posterior on weights of neural networks. Galashov et al. (2019) introduces
a uniﬁed framework for applying NP to a wide range of sequential decision-making problems."
CONCLUSION,0.48188405797101447,"6
CONCLUSION"
CONCLUSION,0.4855072463768116,"We have proposed NeuBANP, a novel bootstrap method for a family of NP to model functional
uncertainty appropriately. Instead of the standard bootstrap, NeuBANP learns to construct a generator
function that produces valid bootstrapped distribution without resampling which can be considered as
a learn-to-bootstrap method. NeuBANP successfully worked in a meta-learning framework, providing
diverse trajectories of underlying data-generating processes, consistent to any given context. In
addition, NeuBANP estimates the local uncertainty accurately, resolving overﬁtted prediction and
variance overestimation problems observed in both ANP and BANP. We replace the additional layer
and repetitive computations in BANP with the simple attachment of bootstrap weights to the model,
which leads to lower computations and smaller memory. NeuBANP shows superior performance to
previous NP methods in regression and stochastic optimization tasks, including multi-dimensional
setting, which has been the desired application of NP. However, due to the numerical instability of GP,
there is a limit in sampling high-dimensional stochastic processes for the meta-learning framework.
We suggest that this is a primary task for scalable applications of NP in stochastic optimization, as a
challenging research direction."
REPRODUCIBILITY,0.4891304347826087,"7
REPRODUCIBILITY"
REPRODUCIBILITY,0.4927536231884058,"For reproducibility of experimental results, we provide a link to anonymous github that contains
our source code in Appendix B. The source code includes the implementation of our model, data
generation, and experiments. Additionally, the data generation steps are thoroughly explained in
Appendix B."
REFERENCES,0.4963768115942029,REFERENCES
REFERENCES,0.5,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016."
REFERENCES,0.5036231884057971,Under review as a conference paper at ICLR 2022
REFERENCES,0.5072463768115942,"Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, Andrew Gor-
don Wilson, and Eytan Bakshy. Botorch: A framework for efﬁcient Monte-Carlo bayesian opti-
mization. In Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.5108695652173914,"Yutian Chen, Matthew W. Hoffman, Sergio Gómez Colmenarejo, Misha Denil, Timothy P. Lillicrap,
Matt Botvinick, and Nando de Freitas. Learning to learn without gradient descent by gradient
descent. In International Conference on Machine Learning, pp. 748–756. PMLR, 2017."
REFERENCES,0.5144927536231884,"Gregory Cohen, Saeed Afshar, Jonathan Tapson, and André van Schaik. Emnist: Extending MNIST
to handwritten letters. In International Joint Conference on Neural Networks, pp. 2921–2926.
IEEE, 2017."
REFERENCES,0.5181159420289855,"Bradley Efron. Better bootstrap conﬁdence intervals. Journal of the American statistical Association,
82(397):171–185, 1987."
REFERENCES,0.5217391304347826,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In International Conference on Machine Learning, pp. 1126–1135. PMLR, 2017."
REFERENCES,0.5253623188405797,"Alexandre Galashov, Jonathan Schwarz, Hyunjik Kim, Marta Garnelo, David Saxton, Pushmeet
Kohli, S.M. Ali Eslami, and Yee Whye Teh. Meta-learning surrogate models for sequential decision
making. arXiv preprint arXiv:1903.11907, 2019."
REFERENCES,0.5289855072463768,"Jacob Gardner, Geoff Pleiss, Kilian Q. Weinberger, David Bindel, and Andrew Gordon Wilson.
Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. In Advances
in Neural Information Processing Systems, volume 31, pp. 7576–7586, 2018."
REFERENCES,0.532608695652174,"Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray
Shanahan, Yee Whye Teh, Danilo Rezende, and S.M. Ali Eslami. Conditional neural processes. In
International Conference on Machine Learning, pp. 1704–1713. PMLR, 2018a."
REFERENCES,0.5362318840579711,"Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami, and
Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018b."
REFERENCES,0.5398550724637681,"Jonathan Gordon, Wessel P. Bruinsma, Andrew Y. K. Foong, James Requeima, Yann Dubois, and
Richard E. Turner. Convolutional conditional neural processes, 2020."
REFERENCES,0.5434782608695652,"Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol
Vinyals, and Yee Whye Teh. Attentive neural processes. In International Conference on Learning
Representations, 2018."
REFERENCES,0.5471014492753623,"Jungtaek Kim.
Benchmark functions for bayesian optimization.
https://github.com/
jungtaekkim/bayeso-benchmarks, 2020."
REFERENCES,0.5507246376811594,"Jungtaek Kim and Seungjin Choi. BayesO: A Bayesian optimization framework in Python. https:
//bayeso.org, 2017."
REFERENCES,0.5543478260869565,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (Poster), 2015."
REFERENCES,0.5579710144927537,"Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Advances in Neural Information Processing
Systems, 2017."
REFERENCES,0.5615942028985508,"Tuan Anh Le, Hyunjik Kim, Marta Garnelo, Dan Rosenbaum, Jonathan Schwarz, and Yee Whye Teh.
Empirical evaluation of neural process objectives. In Advances in Neural Information Processing
Systems Workshop on Bayesian Deep Learning, 2018."
REFERENCES,0.5652173913043478,"Juho Lee, Yoonho Lee, Jungtaek Kim, Eunho Yang, Sung Ju Hwang, and Yee Whye Teh. Bootstrap-
ping neural processes. In Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.5688405797101449,"Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
International Conference on Computer Vision, pp. 3730–3738, 2015."
REFERENCES,0.572463768115942,"Christos Louizos, Xiahan Shi, Klamer Schutte, and Max Welling. The functional neural process.
arXiv preprint arXiv:1906.08324, 2019."
REFERENCES,0.5760869565217391,Under review as a conference paper at ICLR 2022
REFERENCES,0.5797101449275363,"Eric Nalisnick and Padhraic Smyth. The amortized bootstrap. In International Conference on
Machine Learning 2017 Workshop on Implicit Models, 2017."
REFERENCES,0.5833333333333334,"Michael A. Newton and Adrian E. Raftery. Approximate Bayesian inference with the weighted
likelihood bootstrap. Journal of the Royal Statistical Society: Series B (Methodological), 56(1):
3–26, 1994."
REFERENCES,0.5869565217391305,"Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped DQN. arXiv preprint arXiv:1602.04621, 2016."
REFERENCES,0.5905797101449275,"Jens Præstgaard and Jon A Wellner. Exchangeably weighted bootstraps of the general empirical
process. The Annals of Probability, pp. 2053–2086, 1993."
REFERENCES,0.5942028985507246,"Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer School on Machine
Learning, pp. 63–71. Springer, 2003."
REFERENCES,0.5978260869565217,"Sachin Ravi and Alex Beatson. Amortized bayesian meta-learning. In International Conference on
Learning Representations, 2019."
REFERENCES,0.6014492753623188,"Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An empirical
comparison of bayesian deep networks for thompson sampling. In International Conference on
Learning Representations, 2018."
REFERENCES,0.605072463768116,"Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-
learning with memory-augmented neural networks. In International Conference on Machine
Learning, pp. 1842–1850. PMLR, 2016."
REFERENCES,0.6086956521739131,"Amr Sharaf and Hal Daumé III. Meta-learning for contextual bandit exploration. arXiv preprint
arXiv:1901.08159, 2019."
REFERENCES,0.6123188405797102,"Minsuk Shin, Lu Wang, and Jun S Liu. Scalable uncertainty quantiﬁcation via generativebootstrap
sampler. arXiv preprint arXiv:2006.00767, 2020."
REFERENCES,0.6159420289855072,"Minsuk Shin, Hyungjoo Cho, Hyun-seok Min, and Sungbin Lim. Neural bootstrapper. In Advances
in Neural Information Processing Systems, 2021."
REFERENCES,0.6195652173913043,"Gautam Singh, Jaesik Yoon, Youngsung Son, and Sungjin Ahn. Sequential neural processes. In
Advances in Neural Information Processing Systems, volume 32, 2019."
REFERENCES,0.6231884057971014,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information
Processing Systems, 2017."
REFERENCES,0.6268115942028986,"A
MODEL"
REFERENCES,0.6304347826086957,"A.1
BASIC OPERATION"
REFERENCES,0.6340579710144928,"Muti-Layer Perceptron
MLP(di, dh, do, nl) denotes the multi-layer perceptron consisting of nl
linear transformations and ReLU activations between them. Let parameters di, dh, do the dimension
of input, hidden and output feature, repectively. We used same dh for every hidden layers. Lin(p, q)
denotes the linear transformation of input with feature dimension p into output with feature dimension
q."
REFERENCES,0.6376811594202898,"MLP(di, dh,do, nl)(X) = Lin(dh, do) ◦(ReLU ◦Lin(dh, dh))nl−2 ◦ReLU ◦Lin(di, dh)(X)
(12)"
REFERENCES,0.6413043478260869,"Dot Product Attention
DotProdAttn (Vaswani et al., 2017) denotes the attention operation with
attention score based on cosine similarity. Dot product of query (Q) and key (K) tensors calculates
the similarity of each vectors in query tensor relative to each vectors in key tensor. Attention score is
calculated through softmax operation of normalized similiarity. Let dk the feature dimension of key
and query tensors, and dv that of value tensors."
REFERENCES,0.644927536231884,"DotProdAttn(Q, K, V ) = softmax
 
QT K/
p"
REFERENCES,0.6485507246376812,"dk

V
(13)"
REFERENCES,0.6521739130434783,Under review as a conference paper at ICLR 2022
REFERENCES,0.6557971014492754,"Multi-Head Attention
MHA denotes multi-head attention with dot product attention. The input is
pre-processed with MLP and the output of attention is post-processed with layer normalization. For
simplicity, we omit parameters of each MLP layers. split(X, n) denotes splitting of tensor X with
respect to feature axis into a tuple of n tensors (X′
i)n
i=1 which have same dimension in feature axis.
[X1, X2, ..., Xn] denotes the concatenation of tensors X1, X2, ..., Xn with respect to feature axis.
LayerNorm denotes the layer normalization introduced in Ba et al. (2016). Let nhead the number of
heads in multi-head attention."
REFERENCES,0.6594202898550725,"(Q′
i)nhead
i=1
= split(MLPqk(Q), nhead)
(14)"
REFERENCES,0.6630434782608695,"(K′
i)nhead
i=1
= split(MLPqk(K), nhead)
(15)"
REFERENCES,0.6666666666666666,"(V ′
i )nhead
i=1
= split(MLPv(V ), nhead)
(16)"
REFERENCES,0.6702898550724637,"MHA(Q, K, V ) = LayerNorm([(DotProdAttn(Q′
i, K′
i, V ′
i ))nhead
i=1
])
(17)"
REFERENCES,0.6739130434782609,"Self-Attention
We used self-attention to calculate efﬁcient representations of context DC =
(XC, YC). We deﬁne self-attention based on multi-head attention as follows:"
REFERENCES,0.677536231884058,"D′
C = [XC, YC] = ([xc, yc])c∈C,
SelfAttn(DC) := MHA(D′
C, D′
C, D′
C)
(18)"
REFERENCES,0.6811594202898551,"Cross-Attention
We used cross-attention to calculate representation of context speciﬁc to target
feature x in interest, when original representations (hc)c∈C is given. We deﬁne cross-attention based
on multi-head attention as follows:"
REFERENCES,0.6847826086956522,"CrossAttn(x, XC, (hc)c∈C) := MHA(x, XC, [(hc)c∈C]).
(19)"
REFERENCES,0.6884057971014492,"A.2
ARCHITECTURE"
REFERENCES,0.6920289855072463,"For fair comparison, we used the same architecture of all models as in Lee et al. (2020). For NeuBANP,
we increased the input dimension of SelfAttn in the posterior path by one to take bootstrap weight as
additional input. Please refer to Lee et al. (2020) for detailed model architecture of ANP and BANP."
REFERENCES,0.6956521739130435,"A.3
NON-ATTENTIVE CASE"
REFERENCES,0.6992753623188406,"As an ablation study, we consider the non-attentive case of NeuBANP, called Neural Bootstrapping
Neural Processes (NeuBNP). Like the architecture of NeuBANP is based on ANP, the architecture of
this model is based on NP, the non-attentive counterpart of ANP. With D(b)
C
= {(xc, yc, w(b)
c }c∈C as
deﬁned in 3, we applied the similar strategy of using random weights in the encoder as follows, and
trained with the same loss function:"
REFERENCES,0.7028985507246377,"z(b)
C
= {z(b)
c }c∈C = MLP(D(b)
C ,
z(b) = mean(z(b)
C
⊙w(b)
C )
(20)"
REFERENCES,0.7065217391304348,"h(b)
C
= {h(b)
c }c∈C = MLP(D(b)
C ,
h(b) = mean(h(b)
C
⊙w(b)
C )
(21)
(22)"
REFERENCES,0.7101449275362319,"To make the non-attentive counterpart of NeuBANP, we use random weights in both paths, re-
sulting in two latent variables z(b), h(b). They induce randomness into the decoder output ˆy(b) =
MLP(x, z(b), h(b)), and the predictive distribution is construct by (10). The results in Table 2 and
Table 3 shows that this model performs worse than NeuBANP, but outperforms BNP and NP, showing
the quality of functional uncertainty the model learns with the bootstrap."
REFERENCES,0.7137681159420289,"B
EXPERIMENTS"
REFERENCES,0.717391304347826,"Implementation2 of NPs other than NeuBANP was borrowed from the source code of Lee et al. (2020)3.
Regression and Bayesian optimization experiment was done in single GeForce RTX 2080 Ti GPU
with the memory of 11, 019 MiB. Multi-dimensional regression including image completion was
done in Tesla V100 GPU with the memory of 32, 480 MiB."
REFERENCES,0.7210144927536232,"2https://anonymous.4open.science/r/neubanp_initial
3https://github.com/juho-lee/bnp, MIT License."
REFERENCES,0.7246376811594203,Under review as a conference paper at ICLR 2022
REFERENCES,0.7282608695652174,"Fig. 5. Each plot shows predictions given by NP, BNP in a linear regression. The ground-truth function is
a simple linear function with heterogeneous variance: y = x + βϵ(x) where ϵ(x) ∼N(0, σ2(x)) and
σ(x) =
√"
REFERENCES,0.7318840579710145,"x2 + 10−5. We used the ofﬁcial code provided by Lee et al. (2020). Unlike the case of ANP and
BANP, considering that overﬁtting does not occur in NP and BNP, we can notice that attention modules are the
leading cause of overﬁtting."
REFERENCES,0.7355072463768116,"B.1
SIMPLE LINEAR REGRESSION"
REFERENCES,0.7391304347826086,"We experimented using a linear function rather than a function sampled from the GP to analyze
how well the NP models work in a simple regression task. Additionally, we need to examine how
well the NP models predict uncertainty in the presence of heterogeneous noise in the data, and
we add ϵ(x) ∼N(0, σ2(x)), σ(x) =
√"
REFERENCES,0.7427536231884058,"x2 + 10−5 to the linear function. We multiplied the noise
by coefﬁcient β to follow the meta-learning framework, and the β value was uniformly randomly
sampled from [0.1, 1.0] during training. In this task, NP, BNP, and NeuBANP were able to estimate
the underlying true linear function and the uncertainty of heterogeneous noise. On the other hand, in
case of ANP and BANP, overﬁtting occurred and failed to predict the true function and its uncertainty
(see Figures 1 and 5). As explained in the main text, we can see that the overﬁtting phenomenon
occurs because of the attention mechanism that has appeared to solve the underﬁtting issue of the NP.
NeuBANP has the advantage of the attention mechanism and achieved better performance through
regularization using random weights. For (A)NP and B(A)NP, we used the ofﬁcial code provided by
Lee et al. (2020). The remaining training settings are the same as the 1D regression in Appendix B.2,
and we changed only the training iteration to 10, 000."
REFERENCES,0.7463768115942029,"B.2
1D REGRESSION"
REFERENCES,0.75,"Training
For all models, training dataset consists of randomly sampled context and target from
functions following GP with RBF kernel k(x, y) = s2 · exp(−||x −y||2/(2l2)). Parameters of
kernel are randomly sampled with s ∼Uniform(0.1, 1.0) and l ∼Uniform(0.1, 0.6). Feature values
(xi)i∈C∪T is chosen uniformly at random in [−2, 2]. The size of context and target are randomly
sampled with |C| ∼Uniform(3, 47) and |T | ∼Uniform(3, 50 −|C|). We trained all models for
100,000 iterations and used Adam optimizer (Kingma & Ba, 2015). For stable learning, we used the
cosine annealing scheduler with initial learning rate 5 × 10−4."
REFERENCES,0.7536231884057971,"Results
Table 2 shows log-likelihood of NPs for various evaluation dataset sampled from GP
with RBF, Matérn 5/2, Periodic kernel. As in generation of training dataset, parameters of Matérn
5/2 kernel k(x, y) = s2(1 +
√"
REFERENCES,0.7572463768115942,"5||x −y||2/(3l2)) exp(−
√"
REFERENCES,0.7608695652173914,"5||x −y||/l) and Periodic kernel
k(x, y) = s2 exp(−2 sin2(π||x −y||2/p)/l2) was randomly sampled with s ∼Uniform(0.1, 1.0),
l ∼Uniform(0.1, 0.6) and p ∼Uniform(0.1, 0.5). For RBF and Matérn 5/2 dataset, NeuBANP
showed state-of-the-art performance both in ﬁtting context and predicting target. We added ﬁgures
showing the predictions of ANP, BANP, and NeuBANP for the Matérn 5/2 and Periodic kernel (see
Figure 6 and 7). In the case of the Matérn kernel, the two problems described in the main text
occurred identically for ANP and BANP (see Section 4.1). However, in the case of Periodic, we can
see that all models failed to approximate the true function correctly. This result came out because we
experimented with testing the models’ generalization performance when trained with the RBF kernel.
And the quantitative results in Table 2 show that the prediction performance of the attention-based"
REFERENCES,0.7644927536231884,Under review as a conference paper at ICLR 2022
REFERENCES,0.7681159420289855,"Method
RBF
Matérn 5/2
Periodic
context
target
context
target
context
target"
REFERENCES,0.7717391304347826,"CNP
1.17 ± 0.08
0.87 ± 0.36
1.06 ± 0.11
0.65 ± 0.39
-0.31 ± 0.41
-2.05 ± 1.17
NP
1.11 ± 0.09
0.78 ± 1.47
0.99 ± 0.11
0.56 ± 0.50
-0.28 ± 0.37
-1.73 ± 1.09
ANP
1.38 ± 0.00
1.08 ± 0.41
1.38 ± 0.00
0.94 ± 0.47
0.21 ± 0.76
-6.82 ± 2.83
BNP
1.20 ± 0.07
0.92 ± 0.34
1.09 ± 0.09
0.72 ± 0.35
-0.18 ± 037
-1.16 ± 0.56
BANP
1.38 ± 0.00
1.12 ± 0.33
1.38 ± 0.00
0.99 ± 0.38
0.28 ± 0.69
-5.69 ± 2.37
NeuBNP
1.54 ± 0.20
1.01 ± 0.55
1.22 ± 0.22
0.53 ± 0.59
-0.34 ± 0.45
-2.34 ± 1.95
NeuBANP
3.17 ± 0.28
1.38 ± 0.60
3.09 ± 0.29
1.13 ± 0.64
1.56 ± 0.73
-11.49 ± 8.08"
REFERENCES,0.7753623188405797,"Table 2. Log-Likelihood of NPs for 48,000 different evaluations of context and target."
REFERENCES,0.7789855072463768,"model on the target data is poor. Among them, NeuBANP has the worst performance, and we think the
reason is the lower bound on the predicted variance set by ANP and BANP. Quantitative results show
similar predictions for all models, but ANP and BANP achieve numerically more robust performance
by setting a lower bound on the variance. We ﬁnd some cases with the jumps in the function values
which do not seem like a smooth function (See Figure 3, 6, and 7). This phenomena occurs in every
attentive models including ANP, BANP, and NeuBANP. It looks like a distorted prediction on particular
region, however, since our model predicts high variance in such region, this does not raise a problem
in predicting the global trend, as we can see in high average log likelihood in 1d regression."
REFERENCES,0.782608695652174,"Fig. 6. Comparison of ANP, BANP, and NeuBANP in 1D regression. Matérn 5/2 kernel case."
REFERENCES,0.7862318840579711,"Fig. 7. Comparison of ANP, BANP, and NeuBANP in 1D regression. Periodic kernel case."
REFERENCES,0.7898550724637681,"B.3
BAYESIAN OPTIMIZATION"
REFERENCES,0.7934782608695652,"One-dimensional case
Table 3 shows the performance of GP and various NPs for 1D Bayesian
optimization task. RBF, Matérn 5/2, and Periodic kernels are used to generate evaluation dataset."
REFERENCES,0.7971014492753623,Under review as a conference paper at ICLR 2022
REFERENCES,0.8007246376811594,"Algorithm 1: Neural Process based Bayesian Optimization.
Input
:Target function f ⋆; Acquisition function U; Observed data D0 = {(x0, f ⋆(x0))};
Maximum evaluation step T."
REFERENCES,0.8043478260869565,1 Meta-train a neural process pθ on f ∼P(F).
REFERENCES,0.8079710144927537,"2 for t = 1, . . . , T do"
REFERENCES,0.8115942028985508,"3
Find xt by optimizing acquisition function: xt = argmin
x∈X
U (pθ(y|x, Dt−1))"
REFERENCES,0.8152173913043478,"4
Evaluate f ⋆(xt) and update the observed data: Dt ←Dt−1 ∪{(xt, f ⋆(xt))}"
REFERENCES,0.8188405797101449,"Fig. 8. 1D Bayesian optimization results. Bold lines show the mean of simple regrets over 100 experiments. We
also report 10% of the standard deviation."
REFERENCES,0.822463768115942,"Fig. 9. Left: Multi-dimensional Bayesian optimization results on various benchmark functions with EI as an
acquisition function. Bold lines represent the mean performance over 100 experiments. We indicate 20% of the
standard deviation. Right most: Time complexity of each model as the number of observations increases."
REFERENCES,0.8260869565217391,"Multi-dimensional case
In multi-dimensional BO experiments, we used GPyTorch 4 (Gardner
et al., 2018) for scalable GP regression, and BoTorch 5 (Balandat et al., 2020) for overall BO process
(e.g., optimization of acquisition functions). GP was set to the default setting of BoTorch. In detail,
GP model was parameterized with Matérn 5/2 kernel with ARD and constant mean function, and
prior distribution for hyperparameters was set as Gamma(3, 6) for length scale l and Gamma(2, 0.15)
for output scale s. For three-dimensional BO experiment in Figure 4, the overall time complexity of
ANP and BANP is almost the same. This result is seemingly in contrast with the fact that ANP takes a
shorter time for prediction than BANP. However, since the BO algorithm contains the optimization of
the acquisition function, the qualities of acquisition functions obtained by the model predictions may
affect the overall time complexity. We conjecture that BANP gives an acquisition function easier to
optimize than that of ANP so that the overall time complexities of both models are similar. Additionally,
we did not report the results of GP for the two functions. Speciﬁcally, we omitted the Goldstein-Price"
REFERENCES,0.8297101449275363,"4https://github.com/cornellius-gp/gpytorch, MIT License.
5https://github.com/pytorch/botorch, MIT License."
REFERENCES,0.8333333333333334,Under review as a conference paper at ICLR 2022
REFERENCES,0.8369565217391305,"Fig. 10. First column: 2D objective functions Second & Fourth columns: Contour plots of functions and the
evaluated points during Bayesian optimization. Third & Fifth columns: UCB value at the last iteration."
REFERENCES,0.8405797101449275,"Method
RBF
Matérn 5/2
Periodic"
REFERENCES,0.8442028985507246,"GP (RBF)
0.016 ± 0.052
0.048 ± 0.206
0.104 ± 0.242
CNP
0.072 ± 0.188
0.081 ± 0.198
0.096 ± 0.166
NP
0.154 ± 0.273
0.187 ± 0.303
0.083 ± 0.121
ANP
0.209 ± 0.364
0.223 ± 0.328
0.107 ± 0.142
BNP
0.109 ± 0.214
0.105 ± 0.188
0.071 ± 0.091
BANP
0.114 ± 0.216
0.136 ± 0.256
0.077 ± 0.11
NeuBNP
0.069 ± 0.169
0.125 ± 0.238
0.058 ± 0.069
NeuBANP
0.006 ± 0.011
0.011 ± 0.055
0.028 ± 0.035"
REFERENCES,0.8478260869565217,"Table 3. 1D Bayesian optimization results. Mean and standard deviations of simple regrets over 100 runs are
reported."
REFERENCES,0.8514492753623188,"since the simple regret value of GP was too large (the performance was poor) compared to other
models and omitted the Rastrigin since the 46 errors occurred out of 100 experiments. An error may
occur when ill-conditioned data is given during the kernel training process of the GP, and the data
recommended by the UCB during the exploration (or exploitation) process seems to correspond to
this condition. If we report the performance ignoring the numerical error, the simple regret for the
Goldstein-Price was 52049.13, and the simple regret for the Rastrigin was 18.48. For the Rastrigin
function, GP with UCB is numerically unstable, but like EI, it achieved the best performance."
REFERENCES,0.855072463768116,"B.4
CONTEXTUAL MULTI-ARMED BANDIT"
REFERENCES,0.8586956521739131,"Setting
We followed wheel bandit setting introduced in Riquelme et al. (2018). At every step
t (< T), a two-dimensional point (xt, yt) inside the unit circle Runit = {(x, y) ∈R2 : x2 + y2 ≤1}
is given as a context ct. The algorithm chooses an action at ∈{1, 2, 3, 4, 5}. The stochastic reward
rt = r(at, ct) is sampled from the reward distribution. Let R1, R2, R3, R4, R5 ⊂Runit the disjoint"
REFERENCES,0.8623188405797102,Under review as a conference paper at ICLR 2022
REFERENCES,0.8659420289855072,"Dim
Target
GP
ANP
BNP
BANP
NeuBANP"
D,0.8695652173913043,2D
D,0.8731884057971014,"Ackley
2.84 ± 1.82
0.19 ± 0.53
1.82 ± 1.03
0.50 ± 1.03
0.08 ± 0.27
Dropwave
0.36 ± 0.17
0.22 ± 0.15
0.32 ± 0.18
0.28 ± 0.17
0.15 ± 0.10
Goldsteinprice
-
475.52 ± 469.15
2098.22 ± 1509.88
80.67 ± 65.85
30.33 ± 25.42
Michalewicz
0.67 ± 0.45
0.61 ± 0.40
1.00 ± 0.46
0.69 ± 0.38
0.45 ± 0.42"
D,0.8768115942028986,3D
D,0.8804347826086957,"Ackley
3.39 ± 1.25
5.36 ± 0.97
3.29 ± 1.11
4.30 ± 1.15
0.34 ± 0.26
Cosine
0.04 ± 0.24
0.10 ± 0.10
1.12 ± 0.57
0.25 ± 0.43
0.005 ± 0.003
Hartmann
0.42 ± 0.77
0.33 ± 0.50
1.94 ± 0.86
0.93 ± 0.98
0.39 ± 0.39
Rastrigin
-
54.94 ± 19.84
48.55 ± 14.34
38.36 ± 8.20
23.06 ± 19.77"
D,0.8840579710144928,"Table 4. Multi-dimensional Bayesian optimization results. Mean and standard deviations of simple regrets over
100 runs are reported."
D,0.8876811594202898,sets (regions) of unit circle as follows:
D,0.8913043478260869,"R1 = {(x, y) : x2 + y2 < δ}
(23)"
D,0.894927536231884,"R2 = {(x, y) : δ ≤x2 + y2 ≤1, x > 0, y > 0}
(24)"
D,0.8985507246376812,"R3 = {(x, y) : δ ≤x2 + y2 ≤1, x < 0, y > 0}
(25)"
D,0.9021739130434783,"R4 = {(x, y) : δ ≤x2 + y2 ≤1, x < 0, y < 0}
(26)"
D,0.9057971014492754,"R5 = {(x, y) : δ ≤x2 + y2 ≤1, x > 0, y < 0}
(27)"
D,0.9094202898550725,"where the constant δ determines the size of R1 relative to other regions. Each action results in rewards
following different distribution according to the region to which the given context belongs, where N
denotes the normal distribution."
D,0.9130434782608695,"r(1, c) ∼N(1.2, 0.012)
(28)"
D,0.9166666666666666,"r(a, c) ∼
N(50, 0.012),
if c ∈Ra
N(1, 0.012),
otherwise
∀a ∈{2, 3, 4, 5}
(29)"
D,0.9202898550724637,"Note that action {1} always produces a moderate reward, but the other actions {2, 3, 4, 5} sometimes
produce a very high reward when the context is sampled from the corresponding high-reward region.
Thus, learning different reward distributions for actions {2, 3, 4, 5} by apprehension of context
information is critical to bandit performance. As δ increases, the high-reward regions for each actions
become smaller. Then the model should learn from rare observation of such high reward, which
means the problem becomes more difﬁcult."
D,0.9239130434782609,"Training and Evaluation
When pre-training NeuBANP, as in Garnelo et al. (2018b), 8 training
batches of 512 contexts and 50 targets were generated from the environment with hyperparameter
randomly sampled; δ ∼Uniform(0, 1). We consider two-dimensional context point ci as feature
xi ∈R2 and ﬁve rewards for actions (r(1, ci), r(2, ci), r(3, ci), r(4, ci), r(5, ci)) as label yi ∈R5.
At evaluation, only rewards for chosen actions are observed by the model. Thus, we replace other
unobserved rewards with dummy values randomly sampled from N(0, 1), following the usual strategy."
D,0.927536231884058,"B.5
IMAGE COMPLETION"
D,0.9311594202898551,"Settings
We compared the baseline NPs and NeuBANP on image completion tasks. Following Lee
et al. (2020), we trained all models on EMNIST (Cohen et al., 2017) and CelebA (Liu et al., 2015)
which was resized to 32 × 32. For EMNIST, we used only 10 classes for training and reported
the evaluation results for both seen classes and unseen classes separately. NeuBANP was trained
with 10 samples, and the other baselines were trained with 4 samples. For evaluation, we used
50 samples for all methods. Similar to 1D regression experiment, we randomly select the pixels
of a given image as context/target, and the number of context/target were drawn randomly from
Uniform distribution. However, in this case, we increased the maximum number of given points;
i.e., |C| ∼Uniform(3, 197), |T | ∼Uniform(3, 200 −|C|). x values were rescaled to [−1, 1] and
the corresponding y values were rescaled to [−0.5, 0.5]. We trained all models for 200 epochs and
set a initial learning rate of 5 × 10−4 using the Adam optimizer (Kingma & Ba, 2015) with cosine
annealing scheduler for learning rate decay."
D,0.9347826086956522,Under review as a conference paper at ICLR 2022
D,0.9384057971014492,"Fig. 11. Qualitative result of EMNIST image comple-
tion."
D,0.9420289855072463,"Method
Seen classes (0-9)
Unseen classes (10-46)
context
target
context
target"
D,0.9456521739130435,"CNP
0.926 ± 0.007
0.751 ± 0.005
0.766 ± 0.009
0.498 ± 0.012
NP
0.948 ± 0.006
0.806 ± 0.005
0.808 ± 0.005
0.600 ± 0.009
ANP
1.383 ± 0.000
0.993 ± 0.005
1.383 ± 0.000
0.894 ± 0.004
BNP
1.004 ± 0.008
0.880 ± 0.005
0.883 ± 0.010
0.722 ± 0.006
BANP
1.383 ± 0.000
1.010 ± 0.006
1.382 ± 0.000
0.942 ± 0.005
NeuBANP
1.475 ± 0.345
1.337 ± 0.224
1.333 ± 0.516
1.119 ± 0.388"
D,0.9492753623188406,"Table 5. Quantitative result of EMNIST image comple-
tion. Mean and standard deviationd of likelihoood over
5 experiments."
D,0.9528985507246377,"Fig. 12. Qualitative result of CelebA image comple-
tion."
D,0.9565217391304348,"context
target"
D,0.9601449275362319,"CNP
2.975 ± 0.013
2.199 ± 0.003
NP
3.066 ± 0.011
2.492 ± 0.014
ANP
4.150 ± 0.000
2.731 ± 0.006
BNP
3.269 ± 0.008
2.788 ± 0.005
BANP
4.149 ± 0.000
3.129 ± 0.005
NeuBANP
13.946 ± 0.590
2.870 ± 0.021"
D,0.9637681159420289,"Table 6. Quantitative result of CelebA results. Mean and
standard deviationd of likelihoood over 5 experiments."
D,0.967391304347826,"Results
Figure 11 and 12 show the mean prediction and uncertainty estimation of ANP, BANP,
and NeuBANP for test images in unseen classes. For both datasets, though our model shows noisy
mean prediction due to the random weights, we can demonstrate the advantage of NeuBANP in
uncertainty estimation. NeuBANP estimated the uncertainty correctly in the area where the color of
the pixel changes and thus possesses signiﬁcant uncertainty. This leads to the overall improvement in
quantitative performance (see Table 5 and 6)."
D,0.9710144927536232,"B.6
TIME COMPLEXITY"
D,0.9746376811594203,"Settings
We measured the time complexity empirically according to the number of context points,
target points, and bootstrap samples. We ﬁxed the number of targets to 25 and adjusted the number
of contexts to 10, 20, 30, 40, and 50 to see how inference time varies with the number of contexts.
Conversely, to see the inference time according to the number of targets, we ﬁxed the number
of context points to 25. We ﬁxed the number of bootstrap samples to 50 as in the 1D regression
experiment. When conducting experiments with the number of bootstrap samples, the number of
context and target points was ﬁxed at 20 and 25. All experiments were conducted with a batch
containing 100 tasks."
D,0.9782608695652174,"Results
BANP places a remarkably high computational cost in that the approach of bootstrapping the
attention module is inefﬁcient, as demonstrated in Figure 13. The inference time becomes noticeably
longer as the number of context points increases. NeuBANP, on the other hand, learns to bootstrap
efﬁciently; therefore, its time complexity is comparable to that of BNP, which does not use the
attention module."
D,0.9818840579710145,"Method
Number of contexts
Number of targets
Number of bootstrap samples
10
20
30
40
50
10
20
30
40
50
10
50
100"
D,0.9855072463768116,"BNP
1.797
1.977
2.222
2.546
2.886
1.830
1.882
1.965
2.057
2.156
1.532
1.639
1.950
BANP
3.512
4.405
5.345
6.509
7.793
4.439
4.626
4.834
4.926
5.117
3.189
3.699
4.775
NeuBANP
1.632
1.813
2.217
2.757
3.369
1.768
1.941
2.063
2.212
2.357
1.606
1.705
2.149"
D,0.9891304347826086,Table 7. Inference time measurement. Mean of inference time over 5 runs are reported.
D,0.9927536231884058,Under review as a conference paper at ICLR 2022
D,0.9963768115942029,Fig. 13. Inference time measurement.
