Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00425531914893617,"To improve the efﬁciency of adversarial training, recent studies leverage Fast
Gradient Sign Method with Random Start (FGSM-RS) for adversarial training.
Unfortunately, such methods lead to relatively low robustness and catastrophic
overﬁtting, which means that the robustness against iterative attacks (e.g. Projected
Gradient Descent (PGD)) would suddenly drop to 0%. Different approaches
have been proposed to address this problem, however, recent studies show that
catastrophic overﬁtting still remains. In this paper, motivated by the fact that
expensive iterative adversarial training methods achieve high robustness without
catastrophic overﬁtting, we ask: Can we perform iterative adversarial training
in an efﬁcient way? To this end, we ﬁrst analyze the differences in perturbations
generated by FGSM-RS and PGD and ﬁnd that PGD tends to craft diverse discrete
values instead of ±1 in FGSM-RS. Based on this observation, we propose an
efﬁcient single-step adversarial training method I-PGD-AT by leveraging I-PGD
attack, in which I-PGD imitates PGD virtually. Unlike FGSM that crafts the
perturbations directly using the sign of gradient, I-PGD imitates the perturbation
of PGD based on the magnitude of gradient. Extensive empirical evaluations
on CIFAR-10 and Tiny ImageNet demonstrate that our I-PGD-AT can improve
the robustness compared with the baselines and signiﬁcantly delay catastrophic
overﬁtting. Moreover, we explore and discuss the factors that affect catastrophic
overﬁtting. Finally, to demonstrate the generality of I-PGD-AT, we integrate it into
PGD adversarial training and show that it can even further improve the robustness."
INTRODUCTION,0.00851063829787234,"1
INTRODUCTION"
INTRODUCTION,0.01276595744680851,"The breakthroughs in Deep Neural Networks (DNNs) have brought unprecedented boom to various
ﬁelds, such as Computer Vision (Krizhevsky et al., 2012; He et al., 2016) and Natural Language
Processing (Devlin et al., 2019). However, recent studies have revealed that DNNs are vulnerable to
adversarial examples, such that imperceptible perturbations with small magnitude added to the input
are sufﬁcient to mislead the target model (Szegedy et al., 2014; Goodfellow et al., 2015; Xiao et al.,
2018a;b; Bhattad et al., 2020). Such perturbations are often transferable across different models (Liu
et al., 2017; Dong et al., 2018; Wang & He, 2021) and can be generated without the knowledge of the
target model under black-box setting (Brendel et al., 2018; Li et al., 2020b; 2021)."
INTRODUCTION,0.01702127659574468,"Several works have been proposed to mitigate the aforementioned problem and improve the robustness
of DNNs, e.g. adversarial training (Goodfellow et al., 2015; Madry et al., 2018), input transforma-
tions (Xie et al., 2018; Naseer et al., 2020), certiﬁed defenses (Raghunathan et al., 2018; Gowal et al.,
2019) etc. Among these methods, adversarial training is one of the most effective empirical defense
methods (Athalye et al., 2018). Adversarial training can be formulated as a minimax optimization
problem (Madry et al., 2018), in which the inner maximization aims to search for an adversarial
example that maximizes the classiﬁcation loss, while the outer minimization aims to train a robust
classiﬁer against the worst-case adversarial perturbations. Projected Gradient Descent Adversarial
Training (PGD-AT) (Madry et al., 2018) approximately solves the inner maximization problem by
multiple steps of projected gradient descent and achieves empirical robustness (Carlini & Wagner,
2017; Athalye et al., 2018; Croce & Hein, 2020b). However, PGD-AT is much more expensive than
standard training, owing to the iterative optimization process of perturbation generation."
INTRODUCTION,0.02127659574468085,"To reduce the computational burden, Wong et al. (2020) propose Fast Adversarial Training (Fast-AT),
which adopts Fast Gradient Sign Method with Random Start (FGSM-RS) for training and leads to"
INTRODUCTION,0.02553191489361702,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.029787234042553193,"robust models. However, there are two limitations for Fast-AT: a) Fast-AT suffers from catastrophic
overﬁtting, i.e. the robustness against iterative attack (e.g. PGD) suddenly drops after a few training
epochs. b) There is still a gap between the robustness of Fast-AT and PGD-AT. Fast-AT directly adopts
early stopping to prevent catastrophic overﬁtting. Several attempts (Andriushchenko & Flammarion,
2020; S. & Babu, 2020; Sriramanan et al., 2020; Kim et al., 2021) are made to avoid catastrophic
overﬁtting, but these approaches either are computationally inefﬁcient or hurt the robustness."
INTRODUCTION,0.03404255319148936,"In this work, motivated by the beneﬁts of PGD-AT, we propose I-PGD Adversarial Training (I-PGD-
AT), to improve the robustness of Fast-AT with the same computational cost and delay catastrophic
overﬁtting. In particular, we ﬁrst analyze the differences in perturbations generated by FGSM-RS and
PGD and ﬁnd that the perturbation of PGD is more diverse in which the perturbations are different
discrete values instead of ±1 in FGSM-RS. Based on this observation, we imitate the perturbation
of PGD virtually via a single gradient calculation. Speciﬁcally, we ﬁrst calculate all the candidate
discrete values for PGD attack (e.g. 0 and ±2 for two-step PGD) and its corresponding probability.
Then we generate a new gradient vector by ﬁlling the cells with the candidate discrete value based on
the corresponding absolute gradient while keeping the original sign information. Unlike FGSM-RS
which directly crafts adversarial perturbation using the sign of gradient, we leverage this generated
vector with more diverse discrete values to generate adversarial perturbation for efﬁcient training."
INTRODUCTION,0.03829787234042553,"We conduct extensive empirical evaluations on CIFAR-10 and Tiny ImageNet datasets, which
demonstrate that I-PGD-AT can signiﬁcantly improve the robustness against various adversarial
attacks. We also show that I-PGD-AT can effectively stabilize the training and delay catastrophic
overﬁtting compared with Fast-AT. Furthermore, we empirically validate our hypothesis that under the
same ℓ∞-norm perturbation constraint, training the model using perturbation with a smaller mean
absolute value can delay catastrophic overﬁtting, which could provide new insights to address such
issues. Additionally, we extend our imitation strategy for multi-iteration, which achieves substantial
improvements on the robustness compared with PGD-AT with different iterations, and delays the
robust overﬁtting (Rice et al., 2020) in PGD-AT."
RELATED WORK,0.0425531914893617,"2
RELATED WORK"
RELATED WORK,0.04680851063829787,"To mitigate the threat of adversarial examples, a large variety of defense method has been proposed,
including adversarial training (Goodfellow et al., 2015; Madry et al., 2018), input transformations (Xie
et al., 2018; Guo et al., 2018; Liu et al., 2019), denoising (Liao et al., 2018; Mustafa et al., 2019;
Naseer et al., 2020), certiﬁed defense (Raghunathan et al., 2018; Cohen et al., 2019; Gowal et al.,
2019; Li et al., 2019). Among them, PGD-AT (Madry et al., 2018), as the most popular adversarial
training method, brings consistent improvements on robustness against various attacks and inspires
several adversarial training methods (Zhang et al., 2019b; Ding et al., 2020; Wang et al., 2020;
Song et al., 2020; Dong et al., 2020). Furthermore, Rice et al. (2020) identify robust overﬁtting in
PGD-AT and ﬁnd that early stopping or data augmentation (Tack et al., 2021) helps PGD-AT achieve
comparable robustness with advanced adversarial training methods (Zhang et al., 2019b)."
RELATED WORK,0.05106382978723404,"However, PGD-AT is computationally inefﬁcient due to the iterative optimization for adversarial
examples. To combat the computational overhead, recent works (Shafahi et al., 2019; Zhang et al.,
2019a) utilize a single backpropagation for both training and PGD adversary generation to accelerate
the training process. Wong et al. (2020) identify catastrophic overﬁtting in single-step adversarial
training and propose Fast Adversarial Training (Fast-AT) for efﬁcient training. To avoid catastrophic
overﬁtting, Andriushchenko & Flammarion (2020) propose GradAlign, which maximizes the gradient
alignment inside the perturbation set to strengthen the model’s local linearity. However, it leads
to 3× slowdown due to the double backpropagation. Kim et al. (2021) identify decision boundary
distortion after catastrophic overﬁtting, in which a smaller perturbation is sufﬁcient to fool the
model, while the model is robust against larger perturbation used in Fast-AT. They further propose
to search appropriate step size for each input sample to avoid catastrophic overﬁtting, but it also
hurts the robustness performance. Improved fast AT (FastAdv+) (Li et al., 2020a) incorporates
PGD-AT into Fast-AT when observing the catastrophic overﬁtting to stabilize the training. Chen et al.
(2020) propose backward smoothing which adopts a back-propogation for initialization to implement
single-step training with the objtective of TRADES (Zhang et al., 2019b)."
RELATED WORK,0.05531914893617021,"In this work, we identify the differences in perturbations generated by PGD and FGSM-RS and
propose a single-step adversarial training method I-PGD-AT to boost the robustness of models and
delay catastrophic overﬁtting effectively."
RELATED WORK,0.059574468085106386,Under review as a conference paper at ICLR 2022
METHODOLOGY,0.06382978723404255,"3
METHODOLOGY"
METHODOLOGY,0.06808510638297872,"In this section, we ﬁrst provide preliminaries and revisit FGSM-RS and PGD to identify the differences
in perturbations generated by them. Then we provide a detailed description of the proposed I-PGD
Adversarial Training (I-PGD-AT) and a discussion on catastrophic overﬁtting."
PRELIMINARIES,0.07234042553191489,"3.1
PRELIMINARIES"
PRELIMINARIES,0.07659574468085106,"Let J(x, y, θ) denote the loss function of deep neural net f with parameters θ on the input (x, y)
drawn from data distribution D. Adversarial training can be formulated as a minimax optimization
problem (Madry et al., 2018):"
PRELIMINARIES,0.08085106382978724,"min
θ
E(x,y)∼D[max
δ∈∆J(x + δ, y, θ)],
(1)"
PRELIMINARIES,0.0851063829787234,"where ∆is the perturbation set. In this paper, we focus on ℓ∞attack to align with previous work, i.e.
∆= {δ ∈Rd : ∥δ∥∞< ϵ}, where ϵ is the maximum magnitude of the perturbation. This minimax
problem is usually solved by ﬁrstly crafting adversarial examples to solve the inner maximization
problem and then optimizing the model parameters θ using the generated adversarial examples."
PRELIMINARIES,0.08936170212765958,"Fast Gradient Sign Method Adversarial Training (FGSM-AT) (Goodfellow et al., 2015) is the ﬁrst
adversarial training method that solves the inner maximization problem using one-step update:"
PRELIMINARIES,0.09361702127659574,"δ = ϵ · sign(∇xJ(x, y, θ)).
(2)"
PRELIMINARIES,0.09787234042553192,"However, FGSM-AT exhibits limited robustness against iterative attacks (e.g. iterative FGSM (Ku-
rakin et al., 2017)). Later, Madry et al. (2018) propose Project Gradient Descent Adversarial Training
(PGD-AT), which iteratively solves the inner maximization problem using PGD:"
PRELIMINARIES,0.10212765957446808,"δt+1 = Π∆(δt + α · sign(∇xJ(x + δt, y, θ))),
(3)"
PRELIMINARIES,0.10638297872340426,"where δt is the adversarial perturbation at the t-th iteration and δ0 ∼U(−ϵ, ϵ), Π∆(·) is the projection
function and α < ϵ is the step size."
PRELIMINARIES,0.11063829787234042,"Although PGD-AT is empirically robust, compared with FGSM-AT, the inner loop of PGD to craft
adversarial examples is very expensive. Further, Wong et al. (2020) identify that FGSM-AT suffers
from the catastrophic overﬁtting phenomenon, and propose Fast Adversarial Training (Fast-AT) using
FGSM adversarial example with random start (FGSM-RS), i.e., ρ ∼U(−ϵ, ϵ):"
PRELIMINARIES,0.1148936170212766,"δ = Π∆(ρ + α · sign(∇xJ(x + ρ, y, θ))).
(4)"
PRELIMINARIES,0.11914893617021277,Fast-AT adopts early stopping to avoid catastrophic overﬁtting.
REVISITING FGSM-RS AND PGD,0.12340425531914893,"3.2
REVISITING FGSM-RS AND PGD"
REVISITING FGSM-RS AND PGD,0.1276595744680851,"Let δ′ ∈Rd denote the perturbation obtained by gradient ascent and gi denote the gradient at i-th
iteration calculated by the corresponding adversarial attack. Suppose that the input x is normalized
into [0, 1], for existing gradient-based adversarial attacks, the generated adversarial example xadv
can be formulated as:
xadv = Π[0,1](x + Π∆(ρ + δ′)),
(5)
where ρ is the initialization of perturbation (e.g. ρ = 0 for FGSM and ρ ∼U(−ϵ, ϵ) for FGSM-RS
and PGD). We could reformulate the ﬁnal perturbation as δ = Π∆(ρ + δ′)."
REVISITING FGSM-RS AND PGD,0.13191489361702127,"For FGSM-RS, we could easily obtain δ′
F GSM−RS = α · sign(∇xJ(x + ρ, y; θ)) = α · sign(g0)
from Eq. 4. For PGDk where k is the number of iteration of PGD, we can approximately obtain that:"
REVISITING FGSM-RS AND PGD,0.13617021276595745,"δk = Π∆(ρ + δ′) = Π∆(δk−1 + α · sign(gk−1))
= Π∆(Π∆(δk−2 + α · sign(gk−2)) + α · sign(gk−1))
= Π∆(Π∆(· · · Π∆(ρ + α · sign(g0))) + α · sign(gk−1))"
REVISITING FGSM-RS AND PGD,0.14042553191489363,"≃Π∆(ρ + α · k−1
X"
REVISITING FGSM-RS AND PGD,0.14468085106382977,"i=0
sign(gi)), (6)"
REVISITING FGSM-RS AND PGD,0.14893617021276595,"where gk = ∇xJ(x + δk, y, θ). By ignoring the projection operator at each iteration as the step size
α is expected to be small w.r.t. the input, and we can see that δ′
P GDk ≃α · Pk−1
i=0 sign(gi) for PGDk."
REVISITING FGSM-RS AND PGD,0.15319148936170213,Under review as a conference paper at ICLR 2022 x1 x2
REVISITING FGSM-RS AND PGD,0.1574468085106383,FGSM-RS x1 x2 PGD2 x1 x2 PGD3 x1 x2 PGD4
REVISITING FGSM-RS AND PGD,0.16170212765957448,"Figure 1: The candidate perturbation δ′ (red dots) generated by gradient ascent in various attacks,
namely FGSM-RS and PGDk, where k denotes the number of iteration for PGD. For simplicity, we
normalize the perturbation into [−1, 1] and do not consider the hyper-parameter α."
REVISITING FGSM-RS AND PGD,0.16595744680851063,"Since FGSM-RS and PGD adopt the same random initialization, we only focus on the difference of
δ′. For simplicity, we treat the step size α as a scaled factor and do not consider it by normalizing the
accumulated sign δ′ into [−1, +1]."
REVISITING FGSM-RS AND PGD,0.1702127659574468,"Suppose the input sample x = (x1, x2) ∈[0, 1]2, the perturbation set for δ′ would be a square with
side length 2 centered at the origin. Note that the output of sign(·) is either 1 or −1. As shown
in Fig. 1, for FGSM-RS, δ′
F GSM−RS would be one of the four vertices of the square. In contrast,
taking PGD2 for example, there are four cases ⟨+1, +1⟩, ⟨+1, −1⟩, ⟨−1, +1⟩, ⟨−1, −1⟩of the sign
for each pixel xi ∈x, which lead to three different accumulated signs AS(2) = {+2, 0, −2}. The
general accumulated sign set for PGDk can be formulated as:
AS(k) = {±(2i + k%2) : 0 ≤i ≤⌊k/2⌋}.
(7)
We show the search space for PGD2, PGD3 and PGD4 in Fig. 1 and we can see that the larger k is, the
more diverse the search space for PGDk is. From the decision boundary distortion perspective (Kim
et al., 2021), it supports that PGDk can ﬁnd more diverse adversarial examples inside the perturbation
set so that it does not lead to catastrophic overﬁtting. Such difference of perturbation generated by
FGSM-RS and PGD motivates us to consider the following question:"
REVISITING FGSM-RS AND PGD,0.17446808510638298,"Could we imitate the diverse discrete values of the perturbation generated by PGDk with a single
gradient calculation to improve the robustness of single-step adversarial training and mitigate the
catastrophic overﬁtting?"
I-PGD ADVERSARIAL TRAINING,0.17872340425531916,"3.3
I-PGD ADVERSARIAL TRAINING"
I-PGD ADVERSARIAL TRAINING,0.1829787234042553,"From Sec. 3.2, we ﬁnd that FGSM-RS directly reaches the boundary of perturbation set, which is one
possible reason for catastrophic overﬁtting (Kim et al., 2021) in Fast-AT that performs adversarial
training with perturbation generated by FSGM-RS, while PGD can generate diverse perturbation
inside the boundary. To some extent, adopting random initialization which leads to more diverse
perturbation, can mitigate such phenomenon. Apart from catastrophic overﬁtting, the robustness is
also weaker for Fast-AT compared with PGD-AT. Considering the properties of perturbation generated
by PGD, we aim to explore how to improve the robustness of Fast-AT and mitigate catastrophic
overﬁtting by imitating the perturbation of PGD."
I-PGD ADVERSARIAL TRAINING,0.18723404255319148,"It is noted that both FGSM-RS and PGD adopt the sign of the gradient without considering the
relative magnitude of each element in the gradient vector g ∈Rd. Due to the multiple iterations, PGD
leads to diverse but still discrete candidate perturbation for δ′. In order to imitate the diverse values
of PGD by single gradient calculation, we need to generate a new imitative gradient vector ˜g ∈Rd
for crafting adversarial perturbation by additionally utilizing the magnitude of gradient. Generally
speaking, to achieve this goal, there are two issues to address."
I-PGD ADVERSARIAL TRAINING,0.19148936170212766,"What discrete values does ˜g contain? To imitate a speciﬁc PGDk attack, we can effectively calculate
the accumulated sign set from Eq. 7. For instance, if k = 2, we have three values {−2, 0, +2} which
can be assigned to each element. For any k, the candidate values are symmetric regarding the origin.
Thus, we only need to consider the absolute values for simplicity, i.e. we only have {0, 2} for PGD2,
and the assigned values keep the same sign with the corresponding gradient dimensions."
I-PGD ADVERSARIAL TRAINING,0.19574468085106383,"Which value do we assign to the element ˜g[i]? After determining what values does ˜g contain, we
can assign the value to each element to obtain ˜g by utilizing the relative magnitude of the gradient g."
I-PGD ADVERSARIAL TRAINING,0.2,Under review as a conference paper at ICLR 2022
I-PGD ADVERSARIAL TRAINING,0.20425531914893616,Algorithm 1 The general algorithm for I-PGD Adversarial Training
I-PGD ADVERSARIAL TRAINING,0.20851063829787234,"Input: Training data D, loss function J(x, y, θ), training epochs T, magnitude of perturbation ϵ, adversarial
step size α, learning rate η, imitative values and corresponding portions I = {vi : pi}
1: Initialize model parameters θ
2: for t = 1 →T do
3:
for each batch (x, y) ⊂D do
4:
δ = U(−ϵ, ϵ)
5:
g = ∇xJ(x + δ, y, θ),
˜g = g
6:
Calculate the quantile Q on |g| based on the portions pi ∈I
7:
for i-th qi ∈Q do
▷pi elements in |g| are smaller than qi
8:
˜g[qi−1 < |g| ≤qi] = vi
9:
end for
10:
˜g = ˜g · sign(g)
▷˜g share the same sign of g
11:
δ = Π∆(δ + α · ˜g)
▷δ = Π∆(δ + α · sign(g)) for FGSM-RS
12:
θ = θ −η · ∇θJ(x + δ, y, θ)
13:
end for
14: end for"
I-PGD ADVERSARIAL TRAINING,0.2127659574468085,"We only need to consider the absolute value. The smaller |g|[i] is, the smaller value is assigned to
|˜g|[i]. Speciﬁcally, we ﬁrst calculate the probability of each possible value for PGDk. Let p denote the
probability that a speciﬁc element obtains the same sign (i.e. ⟨· · · , +1, +1 · · · ⟩or ⟨· · · , −1, −1 · · · ⟩)
in two consecutive iterations, and we assume that p > 1/2 due to the neighborhood constraint by
adversarial attacks. We can calculate the probability for each value by summing the probability for
each possible sequence of sign. For a given gradient g, we can calculate the quantile on |g| according
to the accumulated probability P(|δ′|[i] ≤v) for each candidate value v and set all the elements in
this quantile range with value v to obtain ˜g for adversarial training. We denote adopting ˜g for attack
as I-PGD attack and such adversarial training method as I-PGD Adversarial Training (I-PGD-AT).
Taking PGD2 for example, we could get that the imitative value is 0 and 2 with the probability
P(|δ′|[i] = 0) = 1 −p, P(|δ′|[i] = 2) = p, i.e. I = {0 : 1 −p, 2 : 1}. We provide the detailed
calculation for I = {vi : pi} in Appendix A.1 and overall algorithm of I-PGD-AT in Algorithm 1."
DISCUSSION ON CATASTROPHIC OVERFITTING,0.2170212765957447,"3.4
DISCUSSION ON CATASTROPHIC OVERFITTING"
DISCUSSION ON CATASTROPHIC OVERFITTING,0.22127659574468084,"The major difference between FGSM-RS and I-PGD attack is that for each input sample, FGSM-RS
generates the adversarial example only using the sign of gradient, while I-PGD adopts the imitative
gradient vector ˜g of PGD to craft adversarial example. Since PGD-AT does not lead to catastrophic
overﬁtting and our I-PGD-AT utilizes I-PGD for adversarial training, we wonder if such imitation
could effectively avoid catastrophic overﬁtting."
DISCUSSION ON CATASTROPHIC OVERFITTING,0.225531914893617,"Unfortunately, as discussed in Sec. 4.5, we ﬁnd that catastrophic overﬁtting still occurs in I-PGD-AT.
However, compared with Fast-AT, I-PGD-AT can stabilize the training process and delay catastrophic
overﬁtting effectively. We further explore the relationship between the magnitude of perturbation and
catastrophic overﬁtting and validate a new hypothesis that under the same ℓ∞-norm constraint on the
perturbation, the perturbation with a smaller mean absolute value can effectively delay catastrophic
overﬁtting. This hypothesis sheds new light on how to avoid catastrophic overﬁtting in single-step
adversarial training effectively."
EXPERIMENTS,0.2297872340425532,"4
EXPERIMENTS"
EXPERIMENTAL SETTING,0.23404255319148937,"4.1
EXPERIMENTAL SETTING"
EXPERIMENTAL SETTING,0.23829787234042554,"Datasets and Models. We conduct empirical evaluations on CIFAR-10 (Krizhevsky et al., 2009) and
Tiny ImageNet1 using PreAct ResNet-18 (He et al., 2016). All the images are normalized into [0, 1]."
EXPERIMENTAL SETTING,0.2425531914893617,"Baselines. To verify the effectiveness of I-PGD-AT, we imitate PGD2 and PGD3 dubbed I-PGD2-AT
and I-PGD3-AT with single-step adversarial training, respectively. We compare them with Free-
AT (Shafahi et al., 2019) and four single-step adversarial training methods, i.e. Fast-AT (Wong
et al., 2020), GradAlign (Andriushchenko & Flammarion, 2020), Kim et al. (Kim et al., 2021) and"
EXPERIMENTAL SETTING,0.24680851063829787,1https://www.kaggle.com/c/tiny-imagenet
EXPERIMENTAL SETTING,0.251063829787234,Under review as a conference paper at ICLR 2022
EXPERIMENTAL SETTING,0.2553191489361702,"Table 1: Classiﬁcation accuracy (%) and training time (min) of various single-step adversarial training
methods and the proposed methods I-PGD2-AT and I-PGD3-AT on CIFAR-10 and Tiny ImageNet
against white-box attacks (ϵ = 8/255). We bold the highest classiﬁcation accuracy and lowest
training time which outperforms the runner-up by at least 0.2. The proposed methods are in gray ."
EXPERIMENTAL SETTING,0.25957446808510637,(a) Evaluations on CIFAR-10.
EXPERIMENTAL SETTING,0.26382978723404255,"Method
Clean
FGSM
PGD20
PGD50
MIM
CW20
CW50
AA
Time(min)"
EXPERIMENTAL SETTING,0.2680851063829787,"Free-AT
78.75
71.89
45.57
45.34
45.85
43.60
43.51
41.55
120.8
Fast-AT
83.27
55.31
46.85
46.28
47.82
46.56
46.31
43.28
26.0
GradAlign
81.24
54.55
47.48
47.18
48.00
46.91
46.69
44.09
81.4
Kim et al.
88.91
51.60
37.71
37.25
38.79
39.22
38.90
36.07
29.2
FastAdv+
83.17
76.68
47.10
46.69
47.48
46.40
46.22
43.50
230.1
I-PGD2-AT
81.41
55.31
48.26
47.96
49.32
47.19
46.95
44.47
26.2
I-PGD3-AT
81.42
55.39
48.82
48.34
49.82
47.61
47.25
44.70
26.2"
EXPERIMENTAL SETTING,0.2723404255319149,(b) Evaluations on Tiny ImageNet.
EXPERIMENTAL SETTING,0.2765957446808511,"Method
Clean
FGSM
PGD20
PGD50
MIM
CW20
CW50
AA
Time(min)"
EXPERIMENTAL SETTING,0.28085106382978725,"Fast-AT
46.80
21.18
17.82
17.56
17.88
16.16
16.10
14.11
114.9
Kim et al.
56.21
16.62
9.32
9.03
9.70
10.86
10.64
8.10
130.2
I-PGD2-AT
45.53
21.84
18.57
18.37
18.71
16.76
16.64
14.55
115.0
I-PGD3-AT
45.55
21.82
18.56
18.44
18.64
16.84
16.81
14.79
115.0"
EXPERIMENTAL SETTING,0.2851063829787234,"FastAdv+ (Li et al., 2020a). We also extend I-PGD into PGD2-AT and PGD7-AT to imitate more
powerful iterative adversarial training and compare them with the corresponding PGD-AT methods."
EXPERIMENTAL SETTING,0.28936170212765955,"Training Details. We use SGD optimizer with momentum of 0.9 and decay factor of 5 × 10−4, and
adopt the perturbation budget ϵ = 8/255 for adversarial training. For single-step adversarial training,
we train the model with 30 epochs on CIFAR-10 and 40 epochs on Tiny ImageNet, using cyclic
learning rate (Smith, 2017) with the maximum learning rate of 0.2 and increasing until the half of the
training epochs. For PGDk-AT, we train the model with 200 epochs using the maximum learning rate
of 0.2, which decays to 0.02 at 50-th epoch. The probability p is set to 2/3 and α = ϵ for I-PGD-AT."
EXPERIMENTAL SETTING,0.2936170212765957,"Evaluations. We evaluate the robustness using various attacks, i.e. FGSM (Goodfellow et al., 2015),
PGD (Madry et al., 2018), Momentum Iterative Method (MIM) (Dong et al., 2018), Carlini-Wagner
(CW) (Carlini & Wagner, 2017) and AutoAttack (AA) (Croce & Hein, 2020b). More details about
the experimental setting are summarized in Appendix A.3."
EVALUATION ON I-PGD-AT,0.2978723404255319,"4.2
EVALUATION ON I-PGD-AT"
EVALUATION ON I-PGD-AT,0.3021276595744681,"To validate the effectiveness of the proposed I-PGD-AT, we empirically evaluate the robustness of
I-PGD2-AT and I-PGD3-AT, which performs the single-step training to imitate PGD with different
steps for perturbation optimization on CIFAR-10 and Tiny ImageNet."
EVALUATION ON I-PGD-AT,0.30638297872340425,"Results on CIFAR-10. As Fast-AT may lead to catastrophic overﬁtting after 24-th epoch on CIFAR-
10 (Andriushchenko & Flammarion, 2020), we train Fast-AT multiple times and provide the best
results. As shown in Table 1a, Kim et al. achieves the lowest robustness against various attacks but
higher clean accuracy due to the smaller generated perturbation on average. Fast-AT exhibits better
robustness than Free-AT while GradAlign and FastAdv+ achieve higher robustness than Fast-AT
and do not lead to catastrophic overﬁtting. Our proposed I-PGD2-AT could stabilize the training
process and achieve the highest robustness compared with the three baselines, without catastrophic
overﬁtting during the training. It is noted that I-PGD3-AT, which only adopts different imitative
values and corresponding portions (i.e. {1 : 5/9, 3 : 4/9} for I-PGD3-AT and {0 : 1/3, 2 : 2/3}
for I-PGD2-AT), exhibits higher performance than I-PGD2-AT with the same computational cost.
It further shows the reasonability and effectiveness of the proposed imitation strategy. As for the
comparison of training time, although GradAlign improves the robustness, it leads to roughly 3×
slowdown compared with Fast-AT. Free-AT takes smaller computational cost for each epoch but
larger training epochs, leading to longer training time. FastAdv+ takes much longer times due to the
larger training epochs and using PGD10 to recover catastrophic overﬁtting. Our proposed methods
take about the same training time as Fast-AT since we only process the calculated gradient with
constant time complexity, and achieves the highest robustness."
EVALUATION ON I-PGD-AT,0.31063829787234043,Under review as a conference paper at ICLR 2022
EVALUATION ON I-PGD-AT,0.3148936170212766,"Table 2: Classiﬁcation accuracy (%) and training time (hour) of PGD-AT w/ and w/o the proposed
I-PGD on CIFAR-10 against white-box attacks (ϵ = 8/255) without early stopping. We bold the
highest classiﬁcation accuracy and lowest training time which outperforms the runner-up by at least
0.5 and mark the results of the proposed method in gray ."
EVALUATION ON I-PGD-AT,0.3191489361702128,"Method
Clean
FGSM
PGD20
PGD50
CW20
CW50
MIM
AA
Time(h)"
EVALUATION ON I-PGD-AT,0.32340425531914896,"PGD2-AT
84.78
50.28
39.31
38.64
40.78
39.58
39.22
36.52
4.3
PGD4-AT
84.83
50.32
39.91
39.42
41.29
42.20
39.90
37.12
7.0
I-PGD2-ATPGD2
82.84
52.13
44.06
43.59
45.09
43.14
42.97
40.55
4.4"
EVALUATION ON I-PGD-AT,0.3276595744680851,"PGD7-AT
82.27
53.17
45.89
45.41
46.88
44.74
44.55
42.23
11.1
PGD14-AT
82.02
53.78
47.07
46.76
48.01
45.47
45.29
43.01
20.5
I-PGD2-ATPGD7
82.32
53.17
46.67
46.28
47.38
45.26
44.98
42.91
11.3"
EVALUATION ON I-PGD-AT,0.33191489361702126,"Results on Tiny ImageNet. To further validate the effectiveness of I-PGD-AT, we perform similar
experiments on Tiny ImageNet. Due to the high computational cost of GradAlign, here we do not
take it as baseline. As shown in Table 1b, the results consistently demonstrate that our I-PGD-AT can
improve the robustness over Fast-AT without extra computational cost. We ﬁnd that the improvement
of I-PGD3-AT over I-PGD2-AT is not so obvious as on CIFAR-10, which might due to the fact that
the sign consistency on Tiny ImageNet is higher than that on CIFAR-10 as discussed in Sec. 4.4."
INTEGRATING I-PGD INTO PGD-AT,0.33617021276595743,"4.3
INTEGRATING I-PGD INTO PGD-AT"
INTEGRATING I-PGD INTO PGD-AT,0.3404255319148936,"Though the primary goal of I-PGD-AT is to boost Fast-AT, the imitation strategy is not limited to
single-step adversarial training and can be integrated with PGD-AT during each step. Speciﬁcally, at
each iteration, we incorporate I-PGD2 into PGD2-AT and PGD7-AT, denoted as I-PGD2-ATPGD2
and I-PGD2-ATPGD7, to imitate more powerful iterative adversarial training, namely PGD4-AT and
PGD14-AT. As shown in Table 2, I-PGD2-ATPGD2 and I-PGD2-ATPGD7 consistently achieve better
robustness than the corresponding baselines PGD2-AT and PGD7-AT, with almost the same training
time. Surprisingly, I-PGD2-ATPGD2 exhibits much better robustness than PGD4-AT. I-PGD2-ATPGD7
achieves slightly lower robustness than PGD14-AT, which takes double iterations for optimizing the
inner maximization problem of adversarial training and doubles the training time."
SIGN CONSISTENCY FOR PGD,0.3446808510638298,"4.4
SIGN CONSISTENCY FOR PGD"
SIGN CONSISTENCY FOR PGD,0.34893617021276596,"0
10
20
30
40
50
PGD Iterations 0.85 0.90 0.95 1.00"
SIGN CONSISTENCY FOR PGD,0.35319148936170214,Sign Consistency
SIGN CONSISTENCY FOR PGD,0.3574468085106383,"Fast-AT-10
Fast-AT-20
Fast-AT-30"
SIGN CONSISTENCY FOR PGD,0.3617021276595745,"0
10
20
30
40
50
PGD Iterations 0.900 0.925 0.950 0.975 1.000"
SIGN CONSISTENCY FOR PGD,0.3659574468085106,Sign Consistency
SIGN CONSISTENCY FOR PGD,0.3702127659574468,"Fast-AT-10
Fast-AT-20"
SIGN CONSISTENCY FOR PGD,0.37446808510638296,"Fast-AT-30
Fast-AT-40"
SIGN CONSISTENCY FOR PGD,0.37872340425531914,"Figure 2: The sign consistency for PGD50 on Fast-AT at
various epochs on CIFAR-10 (left) and Tiny ImageNet (right),
where Fast-AT-k denotes the model trained at k-th epoch. The
results are averaged over 3 random seeds used for training and
reported with the standard deviation. The high and stable sign
consistency at various epochs makes it possible for imitation."
SIGN CONSISTENCY FOR PGD,0.3829787234042553,"The proposed method I-PGD is based on
the hypothesis that due to the neighborhood
constraint by adversarial attacks, the proba-
bility p that a speciﬁc element in the input
obtains the same sign in two consecutive
iterations would be high and stable. To val-
idate this hypothesis, we deﬁne the sign
consistency (SC) for i-th iteration of PGD
as follows:"
SIGN CONSISTENCY FOR PGD,0.3872340425531915,"SC(i) = 1 N N
X"
SIGN CONSISTENCY FOR PGD,0.39148936170212767,"j=1
1(sign(gi[j]) = sign(gi+1[j])),"
SIGN CONSISTENCY FOR PGD,0.39574468085106385,"where gi denotes the gradient at i-th iteration and 1(·) is the indicator function. To some extent,
SC(i) also indicates the local linearity of the deep neural models. If SC(i) = 1, PGD attack would
degenerate to FGSM-RS attack because the direction for perturbation update in Eq. 3 is constant at
each iteration so that the iterations do not make any difference. In contrast, the perturbation of PGD
differs from that of FGSM-RS signiﬁcantly if SC(i) is small. Besides, the stability of SC(i) makes
it possible for I-PGD-AT to adopt a ﬁxed probability to imitate PGD-AT effectively."
SIGN CONSISTENCY FOR PGD,0.4,"We calculate SC(i) for PGD50 attack on Fast-AT at various training epochs over 3 random seeds for
training. As shown in Fig. 2, generally speaking, the sign consistency of PGD50 is high and stable
at various iterations but training longer degrades the sign consistency and increases the standard
deviation. Moreover, on Tiny ImageNet, the sign consistency decreases much slower with smaller
standard deviation than that on CIFAR-10, which might be the reason why I-PGD3-AT cannot
signiﬁcantly enhance I-PGD2-AT and Fast-AT is more stable on Tiny ImageNet."
SIGN CONSISTENCY FOR PGD,0.40425531914893614,Under review as a conference paper at ICLR 2022
SIGN CONSISTENCY FOR PGD,0.4085106382978723,"0.5
1.0
1.5
2.0
3 4 5 6 7 8 | |"
SIGN CONSISTENCY FOR PGD,0.4127659574468085,"Fast-AT-U
Fast-AT-S"
SIGN CONSISTENCY FOR PGD,0.41702127659574467,"(a) The mean absolute value of
perturbation ¯|δ|."
SIGN CONSISTENCY FOR PGD,0.42127659574468085,"0.5
1.0
1.5
2.0 0 10 20 30 40 50"
SIGN CONSISTENCY FOR PGD,0.425531914893617,Robust Accuracy (%)
SIGN CONSISTENCY FOR PGD,0.4297872340425532,"Fast-AT-U
Fast-AT-S"
SIGN CONSISTENCY FOR PGD,0.4340425531914894,"(b) Robust Accuracy against
PGD50 attack."
SIGN CONSISTENCY FOR PGD,0.43829787234042555,"0.5
1.0
1.5
2.0 0 10 20 30 40 50 60 Epoch"
SIGN CONSISTENCY FOR PGD,0.4425531914893617,"Fast-AT-U
Fast-AT-S"
SIGN CONSISTENCY FOR PGD,0.44680851063829785,"(c) The epoch when catas-
trophic overﬁtting occurs."
SIGN CONSISTENCY FOR PGD,0.451063829787234,"Figure 4: The (a) mean absolute value of perturbation ¯|δ| (normalized into [0, ϵ]), (b) robust accuracy, and (c)
the epoch when catastrophic overﬁtting occurs over various step size α for Fast-AT-U and Fast-AT-S. Initially,
catastrophic overﬁtting does not happen and increasing ¯|δ| leads to better robustness. However, if catastrophic
overﬁtting occurs, increasing ¯|δ| will boost catastrophic overﬁtting."
FACTORS RELATED TO CATASTROPHIC OVERFITTING,0.4553191489361702,"4.5
FACTORS RELATED TO CATASTROPHIC OVERFITTING"
FACTORS RELATED TO CATASTROPHIC OVERFITTING,0.4595744680851064,"0
10
20
30
40
50
60
Epochs 0 20 40 60 80 100"
FACTORS RELATED TO CATASTROPHIC OVERFITTING,0.46382978723404256,Robust Accuracy (\%)
FACTORS RELATED TO CATASTROPHIC OVERFITTING,0.46808510638297873,"FGSM Acc. on Fast-AT
FGSM Acc. on I-PGD2-ATFast AT
PGD10 Acc. on Fast-AT
PGD10 Acc. on I-PGD2-ATFast AT"
FACTORS RELATED TO CATASTROPHIC OVERFITTING,0.4723404255319149,"0
20
40
60
Epochs 0.00 0.05 0.10 0.15 0.20"
FACTORS RELATED TO CATASTROPHIC OVERFITTING,0.4765957446808511,Learning Rate
FACTORS RELATED TO CATASTROPHIC OVERFITTING,0.4808510638297872,"0
10
20
30
40
50
60
Epochs 0 20 40 60 80 100"
FACTORS RELATED TO CATASTROPHIC OVERFITTING,0.4851063829787234,Robust Accuracy (\%)
FACTORS RELATED TO CATASTROPHIC OVERFITTING,0.48936170212765956,"FGSM Acc. on Fast-AT
FGSM Acc. on I-PGD2-ATFast AT
PGD10 Acc. on Fast-AT
PGD10 Acc. on I-PGD2-ATFast AT"
FACTORS RELATED TO CATASTROPHIC OVERFITTING,0.49361702127659574,"0
20
40
60
Epochs 0.00 0.05 0.10 0.15 0.20"
FACTORS RELATED TO CATASTROPHIC OVERFITTING,0.4978723404255319,Learning Rate
FACTORS RELATED TO CATASTROPHIC OVERFITTING,0.502127659574468,"Figure 3: The robustness against FGSM (dashed line) and PGD10
(solid line) of Fast-AT and I-PGD2-AT over different training
epochs with two different learning rate schedulers evaluated on
randomly sampled 10% CIFAR-10 testset. The learning rate sched-
ulers, namely Cyclic Learning Rate (CLR, left) and Cyclic Learning
Rate followed by Piecewise Learning Rate (CPLR, right), are shown
in the left upper corner in the corresponding ﬁgures. The red line
and blue line denote the performance of Fast-AT and I-PGD2-AT
trained over 60 epochs."
FACTORS RELATED TO CATASTROPHIC OVERFITTING,0.5063829787234042,"In this section, we investigate what
factors would affect the catastrophic
overﬁtting in single-step adversarial
training. First, we explore if our I-
PGD-AT can avoid catastrophic over-
ﬁtting. We train the model with 60
epochs using two learning rate sched-
ulers, i.e. single cyclic learning rate
and the cyclic learning rate in the ﬁrst
30 epochs followed with piecewise
learning rate, in which the learning
rate is decayed by a factor of 0.1 per
10 epochs, dubbed CLR and CPLR re-
spectively. As shown in Fig. 3, with
CLR, catastrophic overﬁtting occurs
in both Fast-AT and I-PGD2-AT but I-PGD2-AT postpones it for 7 epochs compared with Fast-AT.
Besides, with CPLR, Fast-AT still leads to catastrophic overﬁtting while I-PGD2-AT stably main-
tains its high robustness after 30th epochs. This indicates that our I-PGD-AT can effectively delay
catastrophic overﬁtting and stablize the training process."
FACTORS RELATED TO CATASTROPHIC OVERFITTING,0.5106382978723404,"We further explore the factors that affect catastrophic overﬁtting on CIFAR-10. Previous works (An-
driushchenko & Flammarion, 2020; Kim et al., 2021) show that small ℓ∞-norm constraint ϵ for
training prevents catastrophic overﬁtting. We ﬁnd that even with the same ϵ, the mean absolute value
of the perturbation ¯|δ| generated by I-PGD2 is 6.5, which is a little smaller than the one of FGSM-RS.
Furthermore, we ﬁnd that adopting the sign of uniform perturbation for initialization also prevents
catastrophic overﬁtting, which leads to ¯|δ| = 5.0. Thus, we speculate that under the same ℓ∞-norm
constraint on the perturbation δ, smaller ¯|δ| can effectively delay catastrophic overﬁtting."
FACTORS RELATED TO CATASTROPHIC OVERFITTING,0.5148936170212766,"To validate this hypothesis, we ﬁx ϵ = 8/255, while vary the step size α in Eq. 4 from ϵ/2 to 2 · ϵ to
adjust ¯|δ| for Fast-AT with uniform perturbation as initialization, dubbed Fast-AT-U. To show the
generality of such hypothesis, we further adopt the perturbation sampled from {−1, +1} uniformly
as initialization, denoted as Fast-AT-S. As shown in Fig. 4a, when we increase the value of α, ¯|δ|
increases in Fast-AT-U while ¯|δ| ﬁrst decreases before α = 1, then increases linearly in Fast-AT-S.
This is because when α = 1, the random perturbation inconsistent with the sign of gradient will
cancel each other out, resulting in the minimum perturbation. As shown in Fig. 4b, before the
catastrophic overﬁtting occurring, the larger α which means larger ¯|δ| leads to better robustness for
both initialization methods. If we continue to increase the value of α, as depicted in Fig. 4c, the
catastrophic overﬁtting occurs and the larger α leads to the earlier epoch for catastrophic overﬁtting. It
supports the hypothesis and shows the trade-off between high robustness and catastrophic overﬁtting."
ABLATION STUDY,0.5191489361702127,"4.6
ABLATION STUDY"
ABLATION STUDY,0.5234042553191489,"To further gain insights on the performance improvement obtained by I-PGD-AT, we conduct ablation
study for the probability p for I-PGD2-AT on CIFAR-10. As shown in Sec. A.1, the higher probability
p indicates the higher portion that the value 2 would occupy in |˜g| at line 7-9 in Algorithm 1, leading"
ABLATION STUDY,0.5276595744680851,Under review as a conference paper at ICLR 2022
ABLATION STUDY,0.5319148936170213,"p
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00"
ABLATION STUDY,0.5361702127659574,"Clean Acc.
82.14
82.03
81.55
81.71
81.59
86.09
84.21
85.11
85.71
84.33
84.46
PGD50 Acc.
46.18
46.99
47.40
48.09
48.16
0.00
0.00
0.00
0.00
0.00
0.00
|¯δ|
5.90
6.12
6.31
6.50
6.70
6.90
7.10
7.29
7.48
7.68
7.88
Epoch
–
–
–
–
–
28
23
20
14
10
10"
ABLATION STUDY,0.5404255319148936,"Table 3: Ablation study for the probability p used in I-PGD2-AT on CIFAR-10. |¯δ∥denotes the mean absolute
value of perturbation and Epoch denotes the epoch when catastrophic overﬁtting occurs"
ABLATION STUDY,0.5446808510638298,"to larger |¯δ|. We vary p from 0.5 to 1 and summarize the results in Table 3. As we can see, when we
increase the value of p, the clean accuracy decreases a little but the robustness increases signiﬁcantly
when p ≤0.7. However, when we continue to increase p, |¯δ| increases and the catastrophic overﬁtting
happens. The larger p indicates the larger |¯δ| and results in the earlier epoch for catastrophic
overﬁtting. This is also consistent with our experiments and analysis in Sec. 4.5 and further support
our hypothesis that under the same ℓ-norm constraint on the perturbation δ, smaller |¯δ| can effectively
delay the catastrophic overﬁtting."
ROBUST OVERFITTING IN I-PGD-AT,0.548936170212766,"4.7
ROBUST OVERFITTING IN I-PGD-AT"
ROBUST OVERFITTING IN I-PGD-AT,0.5531914893617021,"0
25
50
75
100
125
150
175
200
Epochs 25 30 35 40 45 50 55"
ROBUST OVERFITTING IN I-PGD-AT,0.5574468085106383,Robust Accuracy (%)
ROBUST OVERFITTING IN I-PGD-AT,0.5617021276595745,"PGD10 Acc. on PGD2-AT
PGD10 Acc. on PGD4-AT
PGD10 Acc. on I-PGD2-ATPGD2"
ROBUST OVERFITTING IN I-PGD-AT,0.5659574468085107,"Figure 5: The robust accuracy (%) against
PGD10 attack over the training epochs for
PGD2-AT, PGD4-AT and I-PGD2-ATPGD2."
ROBUST OVERFITTING IN I-PGD-AT,0.5702127659574469,"5
10
15
20
25
30
35
40
45
50
PGD Iterations 48 50 52 54 56"
ROBUST OVERFITTING IN I-PGD-AT,0.574468085106383,Robust Accuracy
ROBUST OVERFITTING IN I-PGD-AT,0.5787234042553191,"PGDk
I-PGDk"
ROBUST OVERFITTING IN I-PGD-AT,0.5829787234042553,"Figure 6: The robust accuracy (%) of I-
PGD2-AT on CIFAR-10 against PGDk and
I-PGDk attacks where k denotes the num-
ber of iteration."
ROBUST OVERFITTING IN I-PGD-AT,0.5872340425531914,"Rice et al. (2020) identify robust overﬁtting in PGD-AT that
the robust accuracy begins to decrease as training progresses
after a few epochs. Here we brieﬂy explore whether I-PGD-
AT is helpful to delay or avoid the robust overﬁtting. As
shown in Fig. 5, I-PGD2-AT consistently exhibits better
robust accuracy than PGD2-AT and PGD4-AT throughout
the training process. As for the robust overﬁtting, I-PGD-AT
maintains the peak performance between 50-th and 75-th
epochs, while the robust accuracy of PGD2-AT and PGD4-
AT begins to decrease around 50-th epoch. Hence, it is clear
that I-PGD-AT can delay (not avoid) the robust overﬁtting in
PGD-AT with better ﬁnal robustness, and the sustainability
for peak robustness might be helpful to choose a better epoch
for early stopping."
COMPARING I-PGD WITH PGD ATTACKS,0.5914893617021276,"4.8
COMPARING I-PGD WITH PGD ATTACKS"
COMPARING I-PGD WITH PGD ATTACKS,0.5957446808510638,"Though we have observed that I-PGD-AT can signiﬁcantly
improve the robustness of single-step and multi-step adver-
sarial training, we wonder whether I-PGD attack indeed
imitates the attack effectiveness of PGD. To verify this, we
incorporate the imitation strategy for PGD2 into PGD attack
to evaluate the robustness of I-PGD2-AT on CIFAR-10. We
denote such attack as I-PGDk where k is the number of iteration. As illustrated in Fig. 6, we ﬁnd that
for various number of iteration k, I-PGDk consistently achieves similar attack performance as PGDk.
The imitation for PGD3 also exhibits the same trend and we do not show it in the ﬁgure for clarity.
The reasons should be two-fold: a) As shown in Fig. 6, increasing the number of iteration cannot
effectively improve the attack performance for PGDk when k ≥20. b) Our imitation only considers
the diverse value of perturbation without effectively predicting the sign of generated perturbation at
each iteration, which might limit the attack performance. We believe that it is possible to improve the
attack performance through imitating powerful attacks and leave this as our future work."
CONCLUSION,0.6,"5
CONCLUSION"
CONCLUSION,0.6042553191489362,"In this work, we identiﬁed the differences in perturbations crafted by FGSM-RS and PGD. Based
on this observation, we proposed I-PGD adversarial training (I-PGD-AT) by adopting I-PGD attack
that imitates PGD virtually through single gradient calculation. Experiments on CIFAR-10 and Tiny
ImageNet showed that I-PGD-AT could effectively improve the robustness and delay catastrophic
overﬁtting and robust overﬁtting. We believe that this work would inspire more precise imitation of
PGD, which can improve the attack effectiveness and further enhance the robustness of DNNs."
CONCLUSION,0.6085106382978723,Under review as a conference paper at ICLR 2022
REFERENCES,0.6127659574468085,REFERENCES
REFERENCES,0.6170212765957447,"Maksym Andriushchenko and Nicolas Flammarion. Understanding and improving fast adversarial
training. In Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.6212765957446809,"Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack:
a query-efﬁcient black-box adversarial attack via random search. In European Conference on
Computer Vision, 2020."
REFERENCES,0.625531914893617,"Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense
of security: Circumventing defenses to adversarial examples. In International Conference on
Machine Learning, 2018."
REFERENCES,0.6297872340425532,"Anand Bhattad, Min Jin Chong, Kaizhao Liang, Bo Li, and David A. Forsyth. Unrestricted adversarial
examples via semantic manipulation. In International Conference on Learning Representations,
2020."
REFERENCES,0.6340425531914894,"Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable
attacks against black-box machine learning models. In International Conference on Learning
Representations, 2018."
REFERENCES,0.6382978723404256,"Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Srivacy. IEEE, 2017."
REFERENCES,0.6425531914893617,"Jinghui Chen, Yu Cheng, Zhe Gan, Quanquan Gu, and Jingjing Liu. Efﬁcient robust training via
backward smoothing. In arXiv preprint arXiv:2010.01278, 2020."
REFERENCES,0.6468085106382979,"Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. Certiﬁed adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, 2019."
REFERENCES,0.6510638297872341,"Francesco Croce and Matthias Hein. Minimally distorted adversarial examples with a fast adaptive
boundary attack. In International Conference on Machine Learning, 2020a."
REFERENCES,0.6553191489361702,"Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In International Conference on Machine Learning, 2020b."
REFERENCES,0.6595744680851063,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
deep bidirectional transformers for language understanding. In North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, 2019."
REFERENCES,0.6638297872340425,"Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. MMA training: Direct
input space margin maximization through adversarial training. In International Conference on
Learning Representations, 2020."
REFERENCES,0.6680851063829787,"Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting
adversarial attacks with momentum. In Conference on Computer Vision and Pattern Recognition,
2018."
REFERENCES,0.6723404255319149,"Yinpeng Dong, Zhijie Deng, Tianyu Pang, Jun Zhu, and Hang Su. Adversarial distributional training
for robust deep learning. In Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.676595744680851,"Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015."
REFERENCES,0.6808510638297872,"Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Relja Arandjelovic, Timothy Arthur Mann, and Pushmeet Kohli. Scalable veriﬁed training
for provably robust image classiﬁcation. In International Conference on Computer Vision, 2019."
REFERENCES,0.6851063829787234,"Chuan Guo, Mayank Rana, Moustapha Ciss´e, and Laurens van der Maaten. Countering adversarial
images using input transformations. In International Conference on Learning Representations,
2018."
REFERENCES,0.6893617021276596,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Conference on Computer Vision and Pattern Recognition, 2016."
REFERENCES,0.6936170212765957,Under review as a conference paper at ICLR 2022
REFERENCES,0.6978723404255319,"Hoki Kim, Woojin Lee, and Jaewook Lee. Understanding catastrophic overﬁtting in single-step
adversarial training. In AAAI Conference on Artiﬁcial Intelligence, 2021."
REFERENCES,0.7021276595744681,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009."
REFERENCES,0.7063829787234043,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convolu-
tional neural networks. In Advances in Neural Information Processing Systems, 2012."
REFERENCES,0.7106382978723405,"Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In
International Conference on Learning Representations, Workshop Track Proceedings, 2017."
REFERENCES,0.7148936170212766,"Bai Li, Shiqi Wang, Suman Jana, and Lawrence Carin. Towards understanding fast adversarial
training. In arXiv preprint arXiv:2006.03089, 2020a."
REFERENCES,0.7191489361702128,"Huichen Li, Xiaojun Xu, Xiaolu Zhang, Shuang Yang, and Bo Li. QEBA: query-efﬁcient boundary-
based blackbox attack. In Conference on Computer Vision and Pattern Recognition, 2020b."
REFERENCES,0.723404255319149,"Huichen Li, Linyi Li, Xiaojun Xu, Xiaolu Zhang, Shuang Yang, and Bo Li. Nonlinear projection
based gradient estimation for query efﬁcient blackbox attacks. In International Conference on
Artiﬁcial Intelligence and Statistics, 2021."
REFERENCES,0.7276595744680852,"Linyi Li, Zexuan Zhong, Bo Li, and Tao Xie. Robustra: Training provable robust neural networks
over reference adversarial space. In International Joint Conference on Artiﬁcial Intelligence, 2019."
REFERENCES,0.7319148936170212,"Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu. Defense against
adversarial attacks using high-level representation guided denoiser. In Conference on Computer
Vision and Pattern Recognition, 2018."
REFERENCES,0.7361702127659574,"Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples
and black-box attacks. In International Conference on Learning Representations, 2017."
REFERENCES,0.7404255319148936,"Zihao Liu, Qi Liu, Tao Liu, Nuo Xu, Xue Lin, Yanzhi Wang, and Wujie Wen. Feature distillation:
Dnn-oriented JPEG compression against adversarial examples. In Conference on Computer Vision
and Pattern Recognition, 2019."
REFERENCES,0.7446808510638298,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018."
REFERENCES,0.7489361702127659,"Aamir Mustafa, Salman H Khan, Munawar Hayat, Jianbing Shen, and Ling Shao. Image super-
resolution as a defense against adversarial attacks. In IEEE Transactions on Image Processing,
2019."
REFERENCES,0.7531914893617021,"Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Fatih Porikli. A self-
supervised approach for adversarial robustness. In Conference on Computer Vision and Pattern
Recognition, 2020."
REFERENCES,0.7574468085106383,"Aditi Raghunathan, Jacob Steinhardt, and Percy Liang.
Certiﬁed defenses against adversarial
examples. In International Conference on Learning Representations, 2018."
REFERENCES,0.7617021276595745,"Leslie Rice, Eric Wong, and J. Zico Kolter. Overﬁtting in adversarially robust deep learning. In
International Conference on Machine Learning, 2020."
REFERENCES,0.7659574468085106,"Vivek B. S. and R. Venkatesh Babu. Single-step adversarial training with dropout scheduling. In
Conference on Computer Vision and Pattern Recognition, 2020."
REFERENCES,0.7702127659574468,"Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John P. Dickerson, Christoph Studer, Larry S.
Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In Advances in Neural
Information Processing Systems, 2019."
REFERENCES,0.774468085106383,"Leslie N Smith. Cyclical learning rates for training neural networks. In IEEE Winter Conference on
Applications of Computer Vision, 2017."
REFERENCES,0.7787234042553192,Under review as a conference paper at ICLR 2022
REFERENCES,0.7829787234042553,"Chuanbiao Song, Kun He, Jiadong Lin, Liwei Wang, and John E. Hopcroft. Robust local features
for improving the generalization of adversarial training. In International Conference on Learning
Representations, 2020."
REFERENCES,0.7872340425531915,"Gaurang Sriramanan, Sravanti Addepalli, Arya Baburaj, and R Venkatesh Babu. Guided adversarial
attack for evaluating and enhancing adversarial defenses. In Advances in Neural Information
Processing Systems, 2020."
REFERENCES,0.7914893617021277,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations, 2014."
REFERENCES,0.7957446808510639,"Jihoon Tack, Sihyun Yu, Jongheon Jeong, Minseon Kim, Sung Ju Hwang, and Jinwoo Shin. Consis-
tency regularization for adversarial robustness. In arXiv preprint arXiv:2103.04623, 2021."
REFERENCES,0.8,"Xiaosen Wang and Kun He. Enhancing the transferability of adversarial attacks through variance
tuning. In Conference on Computer Vision and Pattern Recognition, 2021."
REFERENCES,0.8042553191489362,"Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving
adversarial robustness requires revisiting misclassiﬁed examples. In International Conference on
Learning Representations, 2020."
REFERENCES,0.8085106382978723,"Eric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than free: Revisiting adversarial training. In
International Conference on Learning Representations, 2020."
REFERENCES,0.8127659574468085,"Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. Generating adversarial
examples with adversarial networks. In International Joint Conference on Artiﬁcial Intelligence,
2018a."
REFERENCES,0.8170212765957446,"Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially transformed
adversarial examples. In International Conference on Learning Representations, 2018b."
REFERENCES,0.8212765957446808,"Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan L. Yuille. Mitigating adversarial
effects through randomization. In International Conference on Learning Representations, 2018."
REFERENCES,0.825531914893617,"Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate
once: Accelerating adversarial training via maximal principle. In Advances in Neural Information
Processing Systems, 2019a."
REFERENCES,0.8297872340425532,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, 2019b."
REFERENCES,0.8340425531914893,Under review as a conference paper at ICLR 2022
REFERENCES,0.8382978723404255,"A
APPENDIX"
REFERENCES,0.8425531914893617,"A.1
CALCULATION FOR I = {vi : pi}"
REFERENCES,0.8468085106382979,"In this section, we ﬁrst provide the detailed calculation for I = {vi : pi} with PGD2 as an example
and then summarize a general algorithm for PGDk."
REFERENCES,0.851063829787234,"Taking PGD2 for example, there would be four cases of the sign for each pixel xi ∈x, namely
⟨+1, +1⟩, ⟨+1, −1⟩, ⟨−1 + 1⟩and ⟨−1, −1⟩. Suppose the sign of the ﬁrst iteration is either +1 or
−1 with equal probability, we could calculate the probability for each sign sequence as following:"
REFERENCES,0.8553191489361702,"P(⟨+1, +1⟩) = 1"
REFERENCES,0.8595744680851064,2 · p = p
REFERENCES,0.8638297872340426,"2,
P(⟨+1, −1⟩) = 1"
REFERENCES,0.8680851063829788,"2 · (1 −p) = 1 −p 2
,"
REFERENCES,0.8723404255319149,"P(⟨−1, −1⟩) = 1"
REFERENCES,0.8765957446808511,2 · p = p
REFERENCES,0.8808510638297873,"2,
P(⟨−1, +1⟩) = 1"
REFERENCES,0.8851063829787233,"2 · (1 −p) = 1 −p 2
,"
REFERENCES,0.8893617021276595,"where p is the probability that a speciﬁc element xi obtains the same sign (i.e. ⟨· · · + 1, +1 · · · ⟩or
⟨· · · −1, −1 · · · ⟩) in two consecutive iterations. The imitative value would be {0, 2} and we could
calculate the corresponding probability:"
REFERENCES,0.8936170212765957,"P(v = 0) = P(⟨+1, −1⟩) + P(⟨−1, +1⟩) = 1 −p
P(v = 2) = P(⟨+1, +1⟩) + P(⟨−1, −1⟩) = p"
REFERENCES,0.8978723404255319,"Thus, we could obtain that I[0] = P(v ≤0) = 1 −p and I[2] = P(v ≤2) = 1 −p + p = p, i.e.
I = {0 : 1 −p, 2 : 1}. For an arbitrary value of k, we ﬁrst traverse each possible sign sequence to
calculate the corresponding probability. Then we calculate the probability for each potential imitative
value by accumulating the probability of related sign sequences. To obtain the quantile for each
possible value, we further accumulate the probability based on the imitative value in ascending order.
The general calculation for any k is summarized in Algorithm 2."
REFERENCES,0.902127659574468,Algorithm 2 Calculation for imitative value and accumulated probability: I = {vi : pi}
REFERENCES,0.9063829787234042,"Input: The number of iteration k for imitative PGD, the probability p that a speciﬁc element obtains
the same sign in two consecutive iterations
1: Initialize I = {}
2: for i-th binary sign sequence si with value {+1, −1} and size of k do
3:
vi = the absolute value of the sum of si
4:
pvi = the probability for the sign sequence si
5:
if v ∈I then
6:
I[vi] = 0
▷Initialize the probability for value vi in I
7:
end if
8:
I[vi] = I[vi] + pvi
▷vi might be related to various sign sequences
9:
Sort I based on the key v in ascending order
10:
for each (vi, pi) ∈I do
▷Accumulate the probability as the quantile
11:
I[vi] = Pi
j=1 pj
12:
end for
13:
return I = {vi : pi}
14: end for"
REFERENCES,0.9106382978723404,"A.2
AN EXAMPLE OF CALCULATING THE QUANTILE"
REFERENCES,0.9148936170212766,"To better illustrate the algorithm, we also provide a simple example of imitating PGD2 with the
probability p = 2/3 as follows."
REFERENCES,0.9191489361702128,"First, we could obtain the probability P(|δ′|[i] = 0) = 1 −p = 1/3, P(|δ′|[i] = 2) = p = 2/3 and
calculate the imitative value and accumulated probability I = {0 : 1/3, 2 : 1}. Suppose we obtain"
REFERENCES,0.9234042553191489,the gradient matrix g =
REFERENCES,0.9276595744680851,""" 0.2
−0.3
−0.1
0.5
−0.6
0.3
0.7
0.1
0.9 #"
REFERENCES,0.9319148936170213,"at line 5 of Algorithm 1, we need to calculate the"
REFERENCES,0.9361702127659575,Under review as a conference paper at ICLR 2022
REFERENCES,0.9404255319148936,"start and end value for the range with the portion from 0 to 1/3 and from 1/3 to 1 on the sorted array
[0.1, 0.1, 0.2, 0.3, 0.3, 0.5, 0.6, 0.7, 0.9] (the absolute values of g), respectively. Thus, we could get
the quantile Q = {0.1, 0.2, 0.9} at Line 6 of Algorithm 1 (0 is the minimal value for the portion of 0,
0.2 is the smaller 3-quantile for the portion of 1/3 and 0.9 is the maximum value for the portion of"
REFERENCES,0.9446808510638298,"1.). After the for-loop at line 7-9 of Algorithm 1, we have ˜g ="
REFERENCES,0.948936170212766,""" 0
2
0
2
2
2
2
0
2 #"
REFERENCES,0.9531914893617022,by calculating each
REFERENCES,0.9574468085106383,"element ˜g[i] =

0
if 0.1 ≤|g|[i] ≤0.2
2
if 0.2 < |g|[i] ≤0.9 . Then we let ˜g share the same sign of g at line 10 and"
REFERENCES,0.9617021276595744,obtain ˜g =
REFERENCES,0.9659574468085106,""" 0
−2
0
2
−2
2
2
0
2 #"
REFERENCES,0.9702127659574468,", which could be used to generate the perturbation for adversarial training"
REFERENCES,0.9744680851063829,at line 11-12 of Algorithm 1.
REFERENCES,0.9787234042553191,"A.3
EXPERIMENTAL SETTING"
REFERENCES,0.9829787234042553,"In this section, we provide more details about the experimental setting, including datasets, hyper-
parameters for baselines and the attack settings."
REFERENCES,0.9872340425531915,"Datasets. We adopt two widely investigated benchmark datasets for the evaluation, namely CIFAR-
10 and Tiny ImageNet. CIFAR-10 contains 10 classes with 5, 000 training and 1, 000 testing images
of resolution 32 × 32 per class. Tiny ImageNet contains 120, 000 images of 200 classes downsized
to 64 × 64 colored images, in which each class has 500 training images, 50 validation images and 50
testing images."
REFERENCES,0.9914893617021276,"Hyper-parameters for baselines. For Free-AT, we set α = ϵ, m = 8 and train the model for 200
epochs. We adopt α = 1.25 · ϵ for Fast-AT (Wong et al., 2020), set the regularization parameter
for GradAlign to 0.2 (Andriushchenko & Flammarion, 2020) and utilize 3 checkpoints for Kim et
al. (Kim et al., 2021). We follow the setting of FastAdv+ (Li et al., 2020a) which reports the best
results among 200 epochs with piecewise learning rate and adopts PGD-10 to recover the catastrophic
overﬁtting. For PGDk-AT, the step size α is set to max(2/255, ϵ/k). For all the baselines, we
evaluate the effectiveness of the models at the last training epoch (Wong et al., 2020; Andriushchenko
& Flammarion, 2020; Kim et al., 2021)."
REFERENCES,0.9957446808510638,"Attack settings. CW is implemented by adopting the margin-based loss function (Carlini & Wagner,
2017) and using PGD for optimization. We use 20 and 50 steps for PGD and CW and 50 steps for
MIM with 10 random restarts. The perturbation budget ϵ is set to 8/255 and step size α is set to
2/255. AutoAttack adopts the default setting (Croce & Hein, 2020b) with four adversarial attacks,
namely untargeted APGD-CE, targeted APGD-DLR, targeted FAB (Croce & Hein, 2020a) and
Squared attack (Andriushchenko et al., 2020)."
