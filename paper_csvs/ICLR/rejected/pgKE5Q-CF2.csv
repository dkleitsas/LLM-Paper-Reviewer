Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0021645021645021645,"This paper presents a novel recommendation method called neuron-enhanced au-
toencoder based collaborative ﬁltering (NE-AECF). The method uses an addi-
tional neural network to enhance the reconstruction capability of autoencoder.
Different from the main neural network implemented in a layer-wise manner, the
additional neural network is implemented in an element-wise manner. They are
trained simultaneously to construct an enhanced autoencoder of which the acti-
vation function in the output layer is learned adaptively to approximate possibly
complicated response functions in real data. We provide theoretical analysis for
NE-AECF to investigate the generalization ability of autoencoder and deep learn-
ing in collaborative ﬁltering. We prove that the element-wise neural network is
able to reduce the upper bound of the prediction error for the unknown ratings,
the data sparsity is not problematic but useful, and the prediction performance is
closely related to the difference between the number of users and the number of
items. Numerical results show that our NE-AECF has promising performance on
a few benchmark datasets."
INTRODUCTION,0.004329004329004329,"1
INTRODUCTION"
INTRODUCTION,0.006493506493506494,"Recommendation system aims to provide personalized recommendation based on various informa-
tion such as user purchase records, social networks, user features, and item (or product) features.
With the fast growth of E-commence, social media, and content provider, recommendation systems
play more and more important roles in our daily life and have changed our life both explicitly and
implicitly. In general, recommendation systems can be organized into three categories (Adomavi-
cius & Tuzhilin, 2005; Zhang et al., 2019): content based method, collaborative ﬁltering, and hybrid
methods. The content based methods recommend similar items to a user or recommend one item
to similar users, where the similarity is usually obtained from side information such as genre, oc-
cupation, and age. Collaborative ﬁltering (CF) assumes that there exist potential correlations within
both users and items, which can be implicitly used to predict unknown ratings. Hybrid methods are
combinations of content based methods and CF methods (Adomavicius & Tuzhilin, 2005; Zhang
et al., 2019; Su & Khoshgoftaar, 2009). CF is at the cores of many recommendation systems."
INTRODUCTION,0.008658008658008658,"Early CF methods (Resnick et al., 1994) compute the similarity between users or items directly
from the ratings to make prediction. This kind of method is also called memory based CF, which
is easy to implement and has high interpretability. One limitation is that the similarity computed
from the ratings is not informative owing to the high sparsity of the rating. Another line of CF is
model based method (Ungar & Foster, 1998; Shani et al., 2002) that utilizes historical data to train
a machine learning model such as Bayesian network (Breese et al., 1998; Miyahara et al., 2000) for
recommendation. Model based methods are more effective than content based methods in learning
complex hidden preference and handling the sparsity problem. Note that both content based and
model based methods do not work when there are new items or users without ratings, which is
known as the cold start problem. A popular strategy for solving the problem is to incorporate side
information into CF methods (Adams et al., 2010; Welling et al., 2012; Zhang et al., 2017)."
INTRODUCTION,0.010822510822510822,"In the past decades, matrix factorization (Billsus & Pazzani, 1998; Mnih & Salakhutdinov, 2008; Ko-
ren et al., 2009) and matrix completion (Cand`es & Recht, 2009; Shamir & Shalev-Shwartz, 2014;
Sun & Luo, 2015; Chen et al., 2016; Fan et al., 2019) have been extensively studied and used in
CF. These methods usually exploit the potential low-rank structure of the incomplete rating matrix"
INTRODUCTION,0.012987012987012988,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.015151515151515152,"via embedding items and users into a latent space of reduced dimension, where the observed rat-
ings are approximated by the inner products of the user feature vectors and item feature vectors.
The low-rankness is usually obtained by low-rank factorization (Koren et al., 2009), nuclear norm
minimization (Cand`es & Recht, 2009), or Schatten-p quasi norm minimization (Fan et al., 2019).
Particularly, Lee et al. (2016) proposed a local low-rank matrix approximation (LLORMA) that
approximates the rating matrix as a weighted sum of a few low-rank matrices. LLORMA outper-
formed vanilla low-rank matrix completion methods in collaborative ﬁltering, which indicates that
the rating matrices in real applications may have more complicated structures rather than a single
low-rank structure."
INTRODUCTION,0.017316017316017316,"The success of neural networks and deep learning in computer vision and natural language process-
ing inspired researchers to design neural networks for CF (Salakhutdinov et al., 2007; Dziugaite &
Roy, 2015; Sedhain et al., 2015; Wu et al., 2016; Zheng et al., 2016; He et al., 2017; van den Berg
et al., 2017; Fan & Cheng, 2018; Yi et al., 2020). For instance, Salakhutdinov et al. (2007) proposed
a restricted Boltzmann machines (Hinton et al., 2006) based CF method called RBM-CF, which
showed high performance in the Netﬂix challenge (Bennett & Lanning, 2007). Sedhain et al. (2015)
proposed AutoRec, an autoencoder (Hinton & Salakhutdinov, 2006; Bengio et al., 2007) based CF
method, which predicts unknown ratings by an encoder-decoder model ˆx = W2σ(W1x), where
x denotes the incomplete ratings on one item or of one user and W1, W2 are weight matrices to
optimize. Unlike RBM-CF, which is probabilistic and generative, AutoRec provides a discrimina-
tive approach. AutoRec outperformed LLORMA slightly on several benchmark datasets (Sedhain
et al., 2015). In addition, adding depth is able to improve the performance of AutoRec (Sedhain
et al., 2015). Inspried by Neural Autoregressive Distribution Estimator (NADE) (Larochelle &
Murray, 2011) and RBM-CF (Salakhutdinov et al., 2007), Zheng et al. (2016) proposed a method
called CF-NADE, in which parameters are shared between different ratings and achieved promising
performance in several benchmarks. Muller et al. (2018) proposed a kernel based reparametrized
neural network, in which the weight between two units is set to be a weighted kernel-function of
the location vectors. The method works well in data visualization and recommendation systems.
Interestingly, Yi et al. (2020) found that the expected value of the output layer of a neural network
depends on the sparsity of the input data. They proposed a simple yet effective method called spar-
sity normalization to improve the performance of neural networks with sparse input data such as the
highly incomplete rating matrices in CF."
INTRODUCTION,0.01948051948051948,"It is worth mentioning that existing autoencoder based CF methods such as (Sedhain et al., 2015;
Wu et al., 2016; Muller et al., 2018; Yi et al., 2020) use linear activation function in the output of the
decoder, i.e., ˆx = WLhL−1, where WL denotes the weights of the output layer and hL−1 denotes
the features given by the last hidden layer. Thus, these methods are under the assumption that the
ratings are linear interactions between user features and item features, though the features can be
nonlinear. Such an assumption may not be true or not optimal in real problems, especially when
the data are bounded (e.g. images) or are collected by sensors (e.g. medical and chemical sensors)
with nonlinear response functions. We suspect that the rating values given by users on items are
from some nonlinear response functions because humans have complex emotion or decision curves
(LeDoux, 2000; Baker, 2001). A naive method to incorporate nonlinear interaction is using nonlin-
ear activation functions such as sigmoid function (with rescaling) in the output layer of the decoder,
which however has much lower performance than using a linear activation function. That’s why ex-
isting autoencoder based CF methods use only linear activation function. Note that a pre-speciﬁed
activation function for the output layer of the decoder may work on speciﬁc data but may be far
away from the possible optimal choice. On the other hand, the theoretical analysis for autoencoder
and deep learning based CF is very limited, while there have been many works on the theory of
low-rank matrix completion based CF (Srebro & Shraibman, 2005; Cand`es & Recht, 2009; Shamir
& Shalev-Shwartz, 2014; Fan et al., 2019)."
INTRODUCTION,0.021645021645021644,"Contribution. In this paper, we present a novel neural network CF method named NE-AECF, an en-
hanced autoencoder approach for recommendation system. NE-AECF is composed of two different
neural networks, one is an autoencoder to reconstruct the incomplete rating matrix, while the other is
an element-wise neural network to learn an activation function adaptively for the output layer of the
autoencoder. We provide theoretical analysis for NE-AECF, which explains the superiority of our
method. Speciﬁcally, we prove that the element-wise neural network can reduce the upper bound of
the prediction error for the unknown ratings. We also prove that the data sparsity is not problematic
but useful and the prediction performance is closely related to the difference between the number of"
INTRODUCTION,0.023809523809523808,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.025974025974025976,"users and the number of items. Further, we demonstrate empirically our NE-AECF on benchmarks:
MovieLen-100k and MovieLen-1M, achieving state-of-the-art results."
INTRODUCTION,0.02813852813852814,"Notation. We use x (or X), x, and X to denote scalar, vector, and matrix respectively. We use
∥x∥to denote the Euclidean norm of vector x, use ∥X∥F and ∥X∥2 to denote the Frobenius norm
and spectral norm of matrix X respectively. The ℓ21 norm of matrix is denoted by ∥X∥2,1 :=
P"
INTRODUCTION,0.030303030303030304,"i ∥xi∥, where xi denotes the i-th column of X. The ℓ∞norm of matrix is denoted by ∥X∥∞:=
maxij |Xij|. We use |S| to denote the cardinality of set S. The symbol ′⊙′ denotes the Hadamard
product between vectors or matrices. The symbol ′◦′ denotes function composition."
NEURON-ENHANCED AECF,0.032467532467532464,"2
NEURON-ENHANCED AECF"
NEURON-ENHANCED AECF,0.03463203463203463,"Suppose we have an incomplete rating matrix ˜
X = (˜x1, ˜x2, . . . , ˜xn) ∈Rm×n, where m is the
number of users and n is the number of items (without loss of generality).
˜Xij ≥0 denotes the
rating given by user i on item j and ˜Xij = 0 indicates an unobserved rating. S denotes the set of
observed ratings. We have ˜Xij = Xij for all (i, j) ∈S. Our goal is to predict the unobserved
ratings Xij, (i, j) ∈[m] × [n]\S, from ˜
X."
NEURON-ENHANCED AECF,0.0367965367965368,"We want to learn a nonlinear function f : Rm 7→Rm such that n
X i=1"
NEURON-ENHANCED AECF,0.03896103896103896,"si ⊙
 ˜xi −f(˜xi)
2
(1)"
NEURON-ENHANCED AECF,0.04112554112554113,"is as small as possible, where si is a binary vector denoting whether the the corresponding element
in xi is zero (unknown) or not. The motivation is predicting the missing entries of xi using its
observed entries, though the missing entries of xi are ﬁlled by zeros before performing f. More
formally, we consider the following problem"
NEURON-ENHANCED AECF,0.04329004329004329,"minimize
f∈F"
NEURON-ENHANCED AECF,0.045454545454545456,"S ⊙

˜
X −f( ˜
X)

2 F
(2)"
NEURON-ENHANCED AECF,0.047619047619047616,"where S = (s1, s2, . . . , sn), f is performed on each column of ˜
X separately, and F denotes a
hypothesis set of m to m functions. We have inﬁnite choices for F. For example, F can be a set
of functions in the form of neural network with some parameters W ∈W, where W denotes a set
of matrices under some constraints. In this case, problem (2) deﬁnes a denoising autoencoder or
stacked denoising autoencoders (Vincent et al., 2010), where the noises are introduced by ﬁlling the
missing ratings with zeros."
NEURON-ENHANCED AECF,0.049783549783549784,Let f be an autoencoder with linear activation function in the output layer. Then (2) becomes
NEURON-ENHANCED AECF,0.05194805194805195,"minimize
W1,W2"
NEURON-ENHANCED AECF,0.05411255411255411,"S ⊙

˜
X −W2σ(W1 ˜
X)

2"
NEURON-ENHANCED AECF,0.05627705627705628,"F + λ
 
∥W1∥2
F + ∥W2∥2
F

,
(3)"
NEURON-ENHANCED AECF,0.05844155844155844,"where W1 ∈Rd×m and W2 ∈Rm×d are weights matrices to learn and λ is a nonnegative constant
to control the strength of weight decay. We have omitted the bia terms for simplicity. σ denotes an
activation function such as"
NEURON-ENHANCED AECF,0.06060606060606061,"ReLU σ(x) = max(x, 0) and
Sigmoid σ(x) = 1/(1 + exp(−x))."
NEURON-ENHANCED AECF,0.06277056277056277,"Note that (3) is exactly the basic model considered by Sedhain et al. (2015),Wu et al. (2016),Muller
et al. (2018), and Yi et al. (2020). Once (3) is used, the following assumption is made implicitly."
NEURON-ENHANCED AECF,0.06493506493506493,"Assumption 1. There exist two matrices A ∈Rm×d and B ∈Rd×n such that ∥S⊙(X −AB) ∥F
is small enough."
NEURON-ENHANCED AECF,0.0670995670995671,"The assumption indicates that if d is much smaller than min(m, n), X can be well approximated by
a low-rank matrix, which however may not always hold in real applications. Consider the following
data generating model
X = h(A′B′),
(4)"
NEURON-ENHANCED AECF,0.06926406926406926,Under review as a conference paper at ICLR 2022
NEURON-ENHANCED AECF,0.07142857142857142,"where h : R1 7→R1 is an element-wise nonlinear function and A′ ∈Rm×d, B′ ∈Rd×n may
be generated by some nonlinear functions. If the nonlinearity of h is high, X cannot be well ap-
proximated by a rank-d matrix. These analysis indicates that if the element-wise nonlinearity in
generating X is strong, (3) should use a large d to ensure a small enough training error."
NEURON-ENHANCED AECF,0.0735930735930736,"The element-wise nonlinearity widely exists in real data. For example, in imaging science, the
intensity of pixels are nonlinear responses of photoelectric element to spectrum. In chemical en-
gineering, many sensors have nonlinear responses. In biomedical engineering, the dose-responses
are often nonlinear curves. Hence, in collaborative ﬁltering, the ratings may be nonlinear responses
to some latent values, according to the studies on response curve in neuroscience and psychology
(LeDoux, 2000; Baker, 2001)."
NEURON-ENHANCED AECF,0.07575757575757576,"Therefore, instead of (3), one may consider the following problem"
NEURON-ENHANCED AECF,0.07792207792207792,"minimize
W1,W2"
NEURON-ENHANCED AECF,0.08008658008658008,"S ⊙

˜
X −h
 
W2σ(W1 ˜
X)

2"
NEURON-ENHANCED AECF,0.08225108225108226,"F + λ
 
∥W1∥2
F + ∥W2∥2
F

,
(5)"
NEURON-ENHANCED AECF,0.08441558441558442,"where h should be determined beforehand. A naive approach to determining h is choosing a bounded
or partially bounded nonlinear function according to the range of the data. For example, if the data
are image pixels within [0, 1], one may use Sigmoid function. If the data are nonnegative, one may
use ReLU. However, such choices only considered the range of the data, which is just a small portion
of the nonlinearity. Within the range, the true response functions are not necessarily linear (ReLU)
or related to exponential (Sigmoid), and can be much more complicated."
NEURON-ENHANCED AECF,0.08658008658008658,"As it is difﬁcult to choose a suitable nonlinear function h in advance, we propose to learn h from the
data adaptively, i.e.,"
NEURON-ENHANCED AECF,0.08874458874458875,"minimize
W1,W2,h∈H"
NEURON-ENHANCED AECF,0.09090909090909091,"S ⊙

˜
X −h
 
W2σ(W1 ˜
X)

2"
NEURON-ENHANCED AECF,0.09307359307359307,"F + λ
 
∥W1∥2
F + ∥W2∥2
F

,
(6)"
NEURON-ENHANCED AECF,0.09523809523809523,"where H denotes a hypothesis set of nonlinear functions from R1 to R1. We have different ap-
proaches to learning h. The ﬁrst approach is combining various activation functions, i.e.,"
NEURON-ENHANCED AECF,0.09740259740259741,"hθ(z) = k
X"
NEURON-ENHANCED AECF,0.09956709956709957,"i
θiσi(z),
(7)"
NEURON-ENHANCED AECF,0.10173160173160173,"where σi(·) are different activation functions and θ = (θ1, . . . , θk)⊤are parameters to estimate.
However, it is not clear whether (7) is able to approximate a wide range of nonlinear functions. The
second approach is using polynomial functions, i.e.,"
NEURON-ENHANCED AECF,0.1038961038961039,"hθ(z) = k
X"
NEURON-ENHANCED AECF,0.10606060606060606,"i
θzk.
(8)"
NEURON-ENHANCED AECF,0.10822510822510822,"It is a k-order polynomial function and can well approximate any smooth functions provided that k
is sufﬁciently large. Another approach is using a neural network, i.e.,"
NEURON-ENHANCED AECF,0.11038961038961038,"hΘ(z) = ΘLΘ(σΘ(ΘLΘ−1σΘ(· · · σΘ(Θ1z) · · · ))),
(9)"
NEURON-ENHANCED AECF,0.11255411255411256,"where Θ1 and ΘLΘ are vectors, Θ2, . . . , ΘLΘ−1 are matrices, and σΘ is a ﬁxed activation function.
According to the universal approximation theorems (Pinkus, 1999; Sonoda & Murata, 2017; Lu
et al., 2017), (9) is able to approximate any continuous functions provided that the network is wide
enough or deep enough."
NEURON-ENHANCED AECF,0.11471861471861472,"Since (9) is more ﬂexible than (7) and (8) in function approximation, we propose to solve the fol-
lowing problem"
NEURON-ENHANCED AECF,0.11688311688311688,"minimize
W,Θ"
NEURON-ENHANCED AECF,0.11904761904761904,"S ⊙

˜
X −hΘ
 
gW ( ˜
X)

2"
NEURON-ENHANCED AECF,0.12121212121212122,"F + λW LW
X"
NEURON-ENHANCED AECF,0.12337662337662338,"l=1
∥Wl∥2
F + λΘ LΘ
X"
NEURON-ENHANCED AECF,0.12554112554112554,"l=1
∥Θl∥2
F ,
(10)"
NEURON-ENHANCED AECF,0.1277056277056277,"where W = {W1, . . . , WLW }, Θ = {Θ1, . . . , ΘLΘ}, and"
NEURON-ENHANCED AECF,0.12987012987012986,"gW ( ˜
X) = WLW

σW
 
WLW −1σW (· · · σW (W1 ˜
X) · · · )

.
(11)"
NEURON-ENHANCED AECF,0.13203463203463203,Under review as a conference paper at ICLR 2022
NEURON-ENHANCED AECF,0.1341991341991342,"In addition, we assume Wl ∈Rdl×dl−1, l ∈[LW ], and Θl ∈Rpl×pl−1, l ∈[LΘ]. Note that
d0 = dLW = m and p0 = pLΘ = 1. Comparing (10) with (2), we see that we have replaced f
by hW ◦gΘ with Frobenius-norm constrained weight matrices. Model (10) is exactly our neuron-
enhanced autoencoder based collaborative ﬁltering (NE-AECF) method. There are two different
neural networks. The ﬁrst one is an autoencoder deﬁned by hΘ◦gW , which is to learn an contraction
map from the incomplete rating matrix ˜
X to itself or its observed entries more precisely. The
second neural network is performed in an element-wise manner to learn an activation function h
adaptively for the output layer of the autoencoder or stancked autoencoders. Figure 1 shows an
example schematic of NE-AECF, where LW = LΘ = 2 and Z = gW ( ˜
X)."
NEURON-ENHANCED AECF,0.13636363636363635,"Figure 1: An example schematic of NE-AECF. The left part demonstrates a rating matrix where
users and items are represented by each row and column. Every square in the left part corresponds
to a rating. Observed ratings are colored and unobserved rating are left white. Target rating being
predicted is marked with question mark."
GENERALIZATION ERROR BOUND OF NE-AECF,0.13852813852813853,"3
GENERALIZATION ERROR BOUND OF NE-AECF"
GENERALIZATION ERROR BOUND OF NE-AECF,0.1406926406926407,"In this section, we analyze the capability of NE-AECF in predicting the unknown ratings of X. Note
that
1
|S|∥S ⊙(X −ˆ
X)∥2
F =
1
|S|∥S ⊙( ˜
X −ˆ
X)∥2
F := LS, where ˆ
X = hΘ(gW ( ˜
X)). We have"
GENERALIZATION ERROR BOUND OF NE-AECF,0.14285714285714285,LS = 1 |S| X
GENERALIZATION ERROR BOUND OF NE-AECF,0.14502164502164502,"(i,j)∈S
ℓ(Xij, ˆXij),"
GENERALIZATION ERROR BOUND OF NE-AECF,0.1471861471861472,"where ℓ(Xij, ˆXij) = (Xij −ˆXij)2. Note that instead of the square loss, we may use other functions
such as |Xij −ˆXij|. In the remainder of this paper, ℓ(Xij, ˆXij) denotes a general loss. Let Sc ≜
[m] × [n]\S, the generalization error of NE-AECF is quantiﬁed by"
GENERALIZATION ERROR BOUND OF NE-AECF,0.14935064935064934,"LSc =
1
|Sc| X"
GENERALIZATION ERROR BOUND OF NE-AECF,0.15151515151515152,"(i,j)∈Sc
ℓ(Xij, ˆXij),"
GENERALIZATION ERROR BOUND OF NE-AECF,0.15367965367965367,"which is a measurement of the prediction error of NE-AECF for the unknown ratings in X. We
have the following generalization bound.
Theorem 1. Suppose a set S of ratings of X ∈Rm×n are observed uniformaly and randomly,
which results in an incomplete matrix ˜
X with unknown ratings replaced by some values such as
zero. Suppose mn −|S| > |S| > 50. Let ˆ
X = hΘ(gW ( ˜
X)), where hΘ is deﬁned by (9) and gW is
deﬁned by (11). Suppose ∥Wl∥2 ≤al, ∥Wl∥2,1 ≤a′
l, l ∈[LW ], ¯d := max
 
d1, . . . , dLW −1

< m,
and ∥Θl∥2 ≤bl, ∥Θl∥F ≤b′
l, l ∈[LΘ]. Suppose the Lipschitz constants of σW and σΘ are ρ and
ϱ respectively. Suppose supi,j |ℓ(Yij, Xij) | ≤τℓ, ℓis ηℓ-Lipschitz, and ∥ˆ
X∥∞≤µ. Then with"
GENERALIZATION ERROR BOUND OF NE-AECF,0.15584415584415584,Under review as a conference paper at ICLR 2022
GENERALIZATION ERROR BOUND OF NE-AECF,0.15800865800865802,"probability at least 1 −δ over the random sampling S,"
GENERALIZATION ERROR BOUND OF NE-AECF,0.16017316017316016,"1
|Sc| X"
GENERALIZATION ERROR BOUND OF NE-AECF,0.16233766233766234,"(i,j)∈Sc
ℓ

Xij, ˆXij

−1 |S| X"
GENERALIZATION ERROR BOUND OF NE-AECF,0.1645021645021645,"(i,j)∈S
ℓ

Xij, ˆXij
"
GENERALIZATION ERROR BOUND OF NE-AECF,0.16666666666666666,≤C1ηℓv1 ln |S|
GENERALIZATION ERROR BOUND OF NE-AECF,0.16883116883116883,"|S|
+ C2ηℓµ s"
GENERALIZATION ERROR BOUND OF NE-AECF,0.170995670995671,v2 ln v3
GENERALIZATION ERROR BOUND OF NE-AECF,0.17316017316017315,"|S|
+ 11τℓmn
p"
GENERALIZATION ERROR BOUND OF NE-AECF,0.17532467532467533,"|Sc||S|
+ 3τℓ s"
GENERALIZATION ERROR BOUND OF NE-AECF,0.1774891774891775,"mn
|Sc||S| ln 1 δ , (12)"
GENERALIZATION ERROR BOUND OF NE-AECF,0.17965367965367965,"where v1
=
ρLW −1ϱLΘ−1∥˜
X∥F
√"
GENERALIZATION ERROR BOUND OF NE-AECF,0.18181818181818182,"ln m
QLW
l=1 al
  
PLW
l=1"
GENERALIZATION ERROR BOUND OF NE-AECF,0.18398268398268397,"a′
l
al"
GENERALIZATION ERROR BOUND OF NE-AECF,0.18614718614718614,"2/3!3/2 QLΘ
l=1 bl

, v2
="
GENERALIZATION ERROR BOUND OF NE-AECF,0.18831168831168832,"PLΘ
l=1 plpl−1, v3 = LΘρLW −1ϱLΘ−1µ−1∥˜
X∥F
QLW
l=1 al
 QLΘ
l=1 bl

maxl
b′
l
bl
, and C1, C2 are"
GENERALIZATION ERROR BOUND OF NE-AECF,0.19047619047619047,some absolute constants.
GENERALIZATION ERROR BOUND OF NE-AECF,0.19264069264069264,"First, let’s show that the bound is non-trivial. Since activation functions are often at most 1-Lipschitz,
we let ρ = ϱ = 1. Suppose a1 = · · · = aLW = 1 and b1 = · · · = bLΘ = 1. Since a′
l/al ≤dl−1, we"
GENERALIZATION ERROR BOUND OF NE-AECF,0.19480519480519481,"have
PLW
l=1
 
a′
la−1
l
2/33/2
≤L3/2
W maxl a′
la−1
l
≤L3/2
W ¯d. In addition, maxl b′
l/bl ≤maxl √pl.
Then the bound in Theorem 1 becomes"
GENERALIZATION ERROR BOUND OF NE-AECF,0.19696969696969696,LSc ≤LS + ˜O
GENERALIZATION ERROR BOUND OF NE-AECF,0.19913419913419914,"ηℓL3/2
W ¯d∥˜
X∥F
|S| ! + ˜O  ηℓµ"
GENERALIZATION ERROR BOUND OF NE-AECF,0.2012987012987013,"sPLΘ
l=1 plpl−1 |S|  "
GENERALIZATION ERROR BOUND OF NE-AECF,0.20346320346320346,"+ 11τℓmn
p"
GENERALIZATION ERROR BOUND OF NE-AECF,0.20562770562770563,"|Sc||S|
+ 3τℓ s"
GENERALIZATION ERROR BOUND OF NE-AECF,0.2077922077922078,"mn
|Sc||S| ln 1 δ . (13)"
GENERALIZATION ERROR BOUND OF NE-AECF,0.20995670995670995,"Note that ∥˜
X∥F ≤√mn maxij | ˆXij|. If |S| > C3 max

L3/2
W ¯d√mn maxij | ˆXij|, PLΘ
l=1 plpl−1
"
GENERALIZATION ERROR BOUND OF NE-AECF,0.21212121212121213,"holds for some constant C3, the bound is non-trivial. Obviously, the condition holds if n is much
larger than m and |S| is sufﬁciently large. A smaller ¯d leads to a tighter bound."
GENERALIZATION ERROR BOUND OF NE-AECF,0.21428571428571427,"More speciﬁcally, Theorem 1 provides the following results."
GENERALIZATION ERROR BOUND OF NE-AECF,0.21645021645021645,"A. The error bound of prediction for the unknown ratings can be reduced via including the
additional (element-wise) neural network."
GENERALIZATION ERROR BOUND OF NE-AECF,0.21861471861471862,"Given a ﬁxed autoencoder, denote by L0
S the training error without the element-wise neural network
of NE-AECF. As discussed in Section 2, the additional neural network of NE-AECF aims to learn
an activation function adaptively for the output layer of the decoder. Hence, the training error L0
S
can be reduced to LS. Denote L0
S −LS = ∆S. In (13), ϱLΘ−1, QLΘ
l=1 bl, and C2ηℓµ
q"
GENERALIZATION ERROR BOUND OF NE-AECF,0.22077922077922077,v2 ln v3
GENERALIZATION ERROR BOUND OF NE-AECF,0.22294372294372294,"|S|
are"
GENERALIZATION ERROR BOUND OF NE-AECF,0.22510822510822512,"introduced by the additional neural network. Denote v0
1 by the v1 without ϱLΘ−1 and QLΘ
l=1 bl. Then
we obtain L0
Sc ≤B0 while"
GENERALIZATION ERROR BOUND OF NE-AECF,0.22727272727272727,"LSc ≤B0 −∆S + (ϱLΘ−1
LΘ
Y"
GENERALIZATION ERROR BOUND OF NE-AECF,0.22943722943722944,"l=1
bl −1)C1ηℓv0
1 ln |S|
|S|
+ C2ηℓµ s"
GENERALIZATION ERROR BOUND OF NE-AECF,0.23160173160173161,v2 ln v3
GENERALIZATION ERROR BOUND OF NE-AECF,0.23376623376623376,"|S|
|
{z
}
∆ ."
GENERALIZATION ERROR BOUND OF NE-AECF,0.23593073593073594,"Note that ϱLΘ−1 QLΘ
l=1 bl can be very close to 1 and v2 is much smaller than |S| (provided that the
additional network is not too large). Therefore, ∆can be negative, which means the element-wise
neural network can reduce the upper bound of the prediction error. On the other hand, if we do not
use the additional neural network but still want reduce B to B0 −∆S, we have to increase the depth
or width of the main neural network, which will raise the value of v1 and hence increase the upper
bound. These results veriﬁed the superiority of our NE-AECF over classical autoencoder based CF
methods such as the AutoRec of (Sedhain et al., 2015)."
GENERALIZATION ERROR BOUND OF NE-AECF,0.23809523809523808,B. Filling the unknown ratings with zeros reduces the upper bound of prediction error.
GENERALIZATION ERROR BOUND OF NE-AECF,0.24025974025974026,Under review as a conference paper at ICLR 2022
GENERALIZATION ERROR BOUND OF NE-AECF,0.24242424242424243,"In Theorem 1, if the unknown ratings are replaced by zeros, ∥˜
X∥F will decrease. Hence the bound
become tighter. Speciﬁcally, (13) becomes"
GENERALIZATION ERROR BOUND OF NE-AECF,0.24458874458874458,LSc ≤LS + ˜O
GENERALIZATION ERROR BOUND OF NE-AECF,0.24675324675324675,"ηℓL3/2
W ¯d maxij | ˜Xij|
p |S| ! + ˜O  ηℓµ"
GENERALIZATION ERROR BOUND OF NE-AECF,0.24891774891774893,"sPLΘ
l=1 plpl−1 |S|  "
GENERALIZATION ERROR BOUND OF NE-AECF,0.2510822510822511,"+ 11τℓmn
p"
GENERALIZATION ERROR BOUND OF NE-AECF,0.2532467532467532,"|Sc||S|
+ 3τℓ s"
GENERALIZATION ERROR BOUND OF NE-AECF,0.2554112554112554,"mn
|Sc||S| ln 1 δ . (14)"
GENERALIZATION ERROR BOUND OF NE-AECF,0.25757575757575757,"The bound is tight if
p"
GENERALIZATION ERROR BOUND OF NE-AECF,0.2597402597402597,"|S| is much larger than ¯d, the maximum size of the hidden layers."
GENERALIZATION ERROR BOUND OF NE-AECF,0.2619047619047619,C. Increasing n reduces the upper bound of prediction error.
GENERALIZATION ERROR BOUND OF NE-AECF,0.26406926406926406,Let the sampling rate |S|
GENERALIZATION ERROR BOUND OF NE-AECF,0.2662337662337662,mn ≜ζ and network structures be ﬁxed. Then (14) becomes
GENERALIZATION ERROR BOUND OF NE-AECF,0.2683982683982684,LSc ≤LS + ˜O
GENERALIZATION ERROR BOUND OF NE-AECF,0.27056277056277056,"ηℓL3/2
W maxij | ˜Xij|
p"
GENERALIZATION ERROR BOUND OF NE-AECF,0.2727272727272727,ζmn/ ¯d2 ! + ˜O  ηℓµ
GENERALIZATION ERROR BOUND OF NE-AECF,0.2748917748917749,"sPLΘ
l=1 plpl−1 ζmn  "
GENERALIZATION ERROR BOUND OF NE-AECF,0.27705627705627706,"+
11τℓ
p"
GENERALIZATION ERROR BOUND OF NE-AECF,0.2792207792207792,"(1 −ζ)ζ2mn
+ 3τℓ s"
GENERALIZATION ERROR BOUND OF NE-AECF,0.2813852813852814,"1
(1 −ζ)ζmn ln 1 δ . (15)"
GENERALIZATION ERROR BOUND OF NE-AECF,0.28354978354978355,"Therefore, when n increases, the bound becomes tighter. In addition, LSc ≤LS when n →∞. In
real application, if there are more users than items, we need to construct a neural network such that
the input is a vector of each user’s rating, where items correspond to features and users correspond
samples. In other word, larger difference between the number of users and the number of items leads
to tighter upper bound of the prediction error, because we can construct the autoencoder along the
smaller size of the rating matrix."
CONNECTION WITH PREVIOUS WORK,0.2857142857142857,"4
CONNECTION WITH PREVIOUS WORK"
CONNECTION WITH PREVIOUS WORK,0.2878787878787879,"The element-wise neural network of NE-AECF can be regarded as an activation function adaptively
learned from the data. It is closely related to the previous work on adaptive activation functions
such (Lin et al., 2013; Agostinelli et al., 2014; Hou et al., 2017; Goyal et al., 2019). For instance,
Lin et al. (2013) proposed to use micro neural networks to improve the convolution operator in
convolutional neural networks. Hou et al. (2017) showed that applying adaptive activation functions
in the regression (second-to-last) layer of a neural network can signiﬁcantly decrease the bias of
the regression. Their adaptive activation function is in the form of piece-wise polynomials. We
found that, empirically, in NE-AECF, the improvement given by polynomials (8) and piece-wise
polynomials (Hou et al., 2017) are not signiﬁcant, which may be caused by the unboundedness of
polynomials."
CONNECTION WITH PREVIOUS WORK,0.29004329004329005,"As mentioned in Introduction, theoretical study for autoencoder and deep learning based collabo-
rative ﬁltering is very limited. In recent years, a few researchers have studied the generalization
ability or sample complexity of deep neural networks (Bartlett et al., 2017; Neyshabur et al., 2018;
Golowich et al., 2018) but their results do not apply to autoencoder based CF and our AE-NECF.
Our proof for Theorem 1 has taken advantage of the result of (Bartlett et al., 2017). There are two
major differences. First, our setting is collaborative ﬁltering, in which the training samples are ma-
trix elements. Hence we utilized the transductive Rademacher complexity proposed by (El-Yaniv &
Pechyony, 2009). Second, computing the complexity of the element-wise neural network required a
different method rather than that of (Bartlett et al., 2017)."
CONNECTION WITH PREVIOUS WORK,0.2922077922077922,"Shamir & Shalev-Shwartz (2014) provided the following generalization bound for nuclear norm
minimization based CF: LSc ≤LS + O

ηℓ∥ˆ
X∥∗(√m+√n)"
CONNECTION WITH PREVIOUS WORK,0.2943722943722944,"|S|

+ R, where ˆ
X denotes the recovered
matrix given by nuclear norm minimization and R stands for the remainder of their result (Theorem
6). Suppose the rank of ˆ
X is ¯d. Then the term related to the nuclear norm can be as large as"
CONNECTION WITH PREVIOUS WORK,0.29653679653679654,"O

ηℓ
√¯d∥ˆ
X∥F (√m+√n)"
CONNECTION WITH PREVIOUS WORK,0.2987012987012987,"ζmn

or O

ηℓµ
q"
CONNECTION WITH PREVIOUS WORK,0.3008658008658009,"¯d
ζ2m

, where we have assumed m < n. According to (15),"
CONNECTION WITH PREVIOUS WORK,0.30303030303030304,"the dominating term in our bound can be ˜O

ηℓµ
q"
CONNECTION WITH PREVIOUS WORK,0.3051948051948052,"¯d
ζ2m q"
CONNECTION WITH PREVIOUS WORK,0.30735930735930733,"L3
W ζ ¯d n"
CONNECTION WITH PREVIOUS WORK,0.30952380952380953,"
. Note that with the same ¯d, the"
CONNECTION WITH PREVIOUS WORK,0.3116883116883117,Under review as a conference paper at ICLR 2022
CONNECTION WITH PREVIOUS WORK,0.31385281385281383,"training error of our NE-AECF is less than the training error of nuclear norm minimization because
we are using neural networks. Now we conclude that when n is sufﬁciently large (compared to
L3
W ζ ¯d), our bound is tighter than that of (Shamir & Shalev-Shwartz, 2014)."
NUMERICAL RESULTS,0.31601731601731603,"5
NUMERICAL RESULTS"
EXPERIMENTS ON MOVIELENS DATASETS,0.3181818181818182,"5.1
EXPERIMENTS ON MOVIELENS DATASETS"
EXPERIMENTS ON MOVIELENS DATASETS,0.3203463203463203,"In this section, we evaluate the proposed method NE-AECF on two benchmark datasets of collab-
orative ﬁltering: Movielens 100K and Movielens 1M (Harper & Konstan, 2015). These datasets
contains 100 thousand (1 million) real world ratings for 1682 (3900) movies by 943 (6040) users, on
a 5-stars scale. We randomly sample 90% of the known ratings as training set, leaving the remaining
10% as the test set. Among the training set, 5% are held out for hyperparameter tuning. Following
Lee et al. (2016), test items or users without training observations are assigned with default rating
of 3. The model performance is evaluated by the root mean squared error deﬁned as"
EXPERIMENTS ON MOVIELENS DATASETS,0.3225108225108225,RMSE = sP
EXPERIMENTS ON MOVIELENS DATASETS,0.3246753246753247,"(i,j)∈Sc(Xij −ˆXij)2"
EXPERIMENTS ON MOVIELENS DATASETS,0.3268398268398268,"|Sc|
,
(16)"
EXPERIMENTS ON MOVIELENS DATASETS,0.329004329004329,"where Sc denotes the set of test ratings. For all the experiments, we use Adam (Kingma & Ba, 2015)
to minimize the objective function of NE-AECF."
EXPERIMENTS ON MOVIELENS DATASETS,0.33116883116883117,"We report the mean RMSE based on 50 random splits and compare our NE-AECF with the following
methods: BisaMF (Koren et al., 2009), NNMF (Dziugaite & Roy, 2015), AutoRec (Sedhain et al.,
2015), CF-NADE (Zheng et al., 2016), LLOMRA (Lee et al., 2016), GC-MC (van den Berg et al.,
2017), Sparse FC (Muller et al., 2018), DMF+ (Yi et al., 2019), and AutoRec W/SN (Yi et al., 2020).
In our NE-AECF, the main neural network has one hidden layer. The numbers of hidden units are
700 and 2000 for MovieLens-100k and MovieLens-1M respectively. For both datasets, the element-
wise neural network has one hidden layer, of which the size is 200. The regularization parameters
λW and λΘ were chosen from {0.01, 0.1, 1, 10, 50, 100, 200, 500}."
EXPERIMENTS ON MOVIELENS DATASETS,0.3333333333333333,"As shown1 in Table 1, on MovieLens-100k, LLORMA outperformed all low-rank factorization
methods, which indicates that a more sophisticated model (weighted sum of local low-rank mod-
els) ﬁts the data better than a single low-rank model. As expected, our NE-AECF outperformed all
baseline methods. Similarly, in Table 2, the RMSE of NE-AECF is less than those of other methods.
It is worth mentioning that we have tried to increase the depth of the main neural network and the
element-wise neural network, but the improvements in terms of RMSE are not signiﬁcant."
EXPERIMENTS ON MOVIELENS DATASETS,0.3354978354978355,Table 1: RMSE result of NE-AECF and compared methods on MovieLens-100k dataset.
EXPERIMENTS ON MOVIELENS DATASETS,0.33766233766233766,"Model
ML-100k
BiasMF (Koren et al., 2009)
0.911
GC-MC (van den Berg et al., 2017)
0.905
AutoSVD++ (Zhang et al., 2017)
0.904
AutoSVD (Zhang et al., 2017)
0.901
NNMF (Dziugaite & Roy, 2015)
0.903
DMF+ (Yi et al., 2019)
0.8889
LLORMA (Lee et al., 2016)
0.8881
AutoRec w/SN (Yi et al., 2020)
0.8816
NE-AECF (ours)
0.8791 ± 0.0063"
EXPERIMENTS ON DOUBAN AND FLIXSTER,0.3398268398268398,"5.2
EXPERIMENTS ON DOUBAN AND FLIXSTER"
EXPERIMENTS ON DOUBAN AND FLIXSTER,0.341991341991342,"In this section, we evaluate the proposed NE-AECF on two more datasets Douban and Flixter, which
are poplular benchmarks for recommendation systems. For these data, we use the preprocessed sub-
sets and splits provided by Monti et al. (2017). These datasets both contain 3000 users and 3000"
EXPERIMENTS ON DOUBAN AND FLIXSTER,0.34415584415584416,"1The RMSEs of the compared methods shown in all tables are from the original papers, in which the
experimental settings are the same or comparable."
EXPERIMENTS ON DOUBAN AND FLIXSTER,0.3463203463203463,Under review as a conference paper at ICLR 2022
EXPERIMENTS ON DOUBAN AND FLIXSTER,0.3484848484848485,Table 2: RMSE result of NE-AECF and compared methods on MovieLens-1M dataset
EXPERIMENTS ON DOUBAN AND FLIXSTER,0.35064935064935066,"Model
ML-1M
AutoSVD (Zhang et al., 2017)
0.864
AutoSVD++ (Zhang et al., 2017)
0.848
BiasMF (Koren et al., 2009)
0.845
NNMF (Dziugaite & Roy, 2015)
0.843
LLORMA (Lee et al., 2016)
0.833
DMF+ (Yi et al., 2019)
0.8321
GC-MC (van den Berg et al., 2017)
0.832
AutoRec (Sedhain et al., 2015)
0.831
CF-NADE (Zheng et al., 2016)
0.829
AutoRec w/SN (Yi et al., 2020)
0.8260
NE-AECF (ours)
0.8252 ± 0.0024"
EXPERIMENTS ON DOUBAN AND FLIXSTER,0.3528138528138528,"items. Douban contains 136,891 ratings with density 0.0152 on a rating scale {1, 2, 3, 4, 5}. Flixster
contains 26,173 ratings with density 0.0029 on a rating scale {0.5, 1, 1.5, ..., 5}, which is a bit dif-
ferent from the other three datasets. Among the training samples, 5% are used for hyperparameter
tuning."
EXPERIMENTS ON DOUBAN AND FLIXSTER,0.354978354978355,"We report the mean RMSE on 5 repeated experiments and compare our NE-AECF with the following
methods: PMF (Mnih & Salakhutdinov, 2008), GRALS (Rao et al., 2015), sRGCNN (Monti et al.,
2017), GC-MC (van den Berg et al., 2017), Factorized EAE (Hartford et al., 2018) and GRAEM
(Strahl et al., 2020). In our NE-AECF, the main neural network has one hidden layer, of which the
size is 500, for both datasets. The structure of the element-wise neural network is the same as that
used for the MovieLens datasets."
EXPERIMENTS ON DOUBAN AND FLIXSTER,0.35714285714285715,"As shown in Table 3, our proposed NE-AECF outperforms other baseline method on both Douban
and Flixster. Note that some of the compared methods include extra content like the side information
of users and items into the model, while NE-AECF does not require extra content."
EXPERIMENTS ON DOUBAN AND FLIXSTER,0.3593073593073593,Table 3: RMSE result of NE-AECF and compared methods on Douban and Flixster dataset
EXPERIMENTS ON DOUBAN AND FLIXSTER,0.36147186147186144,"Model
Douban
Flixster
GRALS (Rao et al., 2015)
0.8326
1.245
PMF (Mnih & Salakhutdinov, 2008)
0.7492
0.9809
sRGCNN (Monti et al., 2017)
0.801
0.926
Factorized EAE (Hartford et al., 2018)
0.738
0.908
GC-MC (van den Berg et al., 2017)
0.734
0.917
GRAEM (Strahl et al., 2020)
0.7497
0.8857
NE-AECF (ours)
0.7286
0.8816"
CONCLUSION,0.36363636363636365,"6
CONCLUSION"
CONCLUSION,0.3658008658008658,"This paper presented a novel collaborative ﬁltering method called NE-AECF. We analyzed the gen-
eralization ability for NE-AECF and showed that the element-wise neural network is useful in re-
ducing the upper bound of the prediction error in CF. The theoretical analysis indicates that ﬁlling
the unknown ratings with zeros can make the error bound tighter. It also provides a guideline to
make a choice between item-based autoencoder and user-based autoencoder. These theoretical ﬁnd-
ings were further validated by the numerical results, in which our NE-AECF has state-of-the-art
performance in terms of RMSE."
CONCLUSION,0.36796536796536794,"It is possible to incorporate side information or graph neural network into our NE-AECF, which can
be a future work."
CONCLUSION,0.37012987012987014,Under review as a conference paper at ICLR 2022
REFERENCES,0.3722943722943723,REFERENCES
REFERENCES,0.37445887445887444,"Ryan P. Adams, George E. Dahl, and Iain Murray. Incorporating side information into probabilistic
matrix factorization using Gaussian processes. In Proceedings of the 26th Conference on Uncer-
tainty in Artiﬁcial Intelligence, pp. 1–9, 2010."
REFERENCES,0.37662337662337664,"G. Adomavicius and A. Tuzhilin. Toward the next generation of recommender systems: a survey
of the state-of-the-art and possible extensions. IEEE Transactions on Knowledge and Data Engi-
neering, 17(6):734–749, 2005."
REFERENCES,0.3787878787878788,"Forest Agostinelli, Matthew Hoffman, Peter Sadowski, and Pierre Baldi. Learning activation func-
tions to improve deep neural networks. arXiv preprint arXiv:1412.6830, 2014."
REFERENCES,0.38095238095238093,"Frank B Baker. The basics of item response theory. ERIC, 2001."
REFERENCES,0.38311688311688313,"Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran
Associates, Inc., 2017."
REFERENCES,0.3852813852813853,"Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training
of deep networks. In Advances in neural information processing systems, pp. 153–160, 2007."
REFERENCES,0.3874458874458874,"J. Bennett and S. Lanning. The netﬂix prize. In Proceedings of the KDD Cup Workshop 2007, pp.
3–6, New York, August 2007. ACM."
REFERENCES,0.38961038961038963,"Daniel Billsus and Michael J. Pazzani. Learning collaborative information ﬁlters. pp. 46–54, 1998.
URL http://portal.acm.org/citation.cfm?id=645527.657311."
REFERENCES,0.3917748917748918,"John Breese, David Heckerman, and Carl Kadie. Empirical analysis of predictive algorithms for
collaborative ﬁltering. In Proceedings of the 14th Conference on Uncertainty in Artiﬁcial Intelli-
gence, pp. 43–52. Morgan Kaufmann, 1998."
REFERENCES,0.3939393939393939,"Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun-
dations of computational mathematics, 9(6):717–772, 2009. ISSN 1615-3375."
REFERENCES,0.3961038961038961,"Chao Chen, Dongsheng Li, Qin Lv, Junchi Yan, Stephen M. Chu, and Li Shang. Mpma: Mixture
probabilistic matrix approximation for collaborative ﬁltering. In Proceedings of the Twenty-Fifth
International Joint Conference on Artiﬁcial Intelligence, IJCAI’16, pp. 1382–1388. AAAI Press,
2016. ISBN 9781577357704."
REFERENCES,0.39826839826839827,"Gintare Karolina Dziugaite and Daniel M. Roy.
Neural network matrix factorization.
CoRR,
abs/1511.06443, 2015."
REFERENCES,0.4004329004329004,"Ran El-Yaniv and Dmitry Pechyony.
Transductive rademacher complexity and its applications.
Journal of Artiﬁcial Intelligence Research, 35:193–234, 2009."
REFERENCES,0.4025974025974026,"Jicong Fan and Jieyu Cheng. Matrix completion by deep matrix factorization. Neural Networks, 98:
34–41, 2018."
REFERENCES,0.40476190476190477,"Jicong Fan, Lijun Ding, Yudong Chen, and Madeleine Udell. Factor group-sparse regularization for
efﬁcient low-rank matrix recovery. In Advances in Neural Information Processing Systems, pp.
5104–5114, 2019."
REFERENCES,0.4069264069264069,"Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Conference On Learning Theory, pp. 297–299. PMLR, 2018."
REFERENCES,0.4090909090909091,"Mohit Goyal, Rajan Goyal, and Brejesh Lall. Learning activation functions: A new paradigm for
understanding neural networks. arXiv preprint arXiv:1906.09529, 2019."
REFERENCES,0.41125541125541126,"F Maxwell Harper and Joseph A Konstan. The MovieLens datasets: History and context. ACM
Transactions on Interactive Intelligent Systems, 5(4):1–19, December 2015."
REFERENCES,0.4134199134199134,Under review as a conference paper at ICLR 2022
REFERENCES,0.4155844155844156,"Jason S. Hartford, Devon R. Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep Models
of Interactions Across Sets. In Proceedings of the 35th International Conference on Machine
Learning, pp. 1914–1923. PMLR, 2018."
REFERENCES,0.41774891774891776,"Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural col-
laborative ﬁltering. In Proceedings of the 26th international conference on world wide web, pp.
173–182, 2017."
REFERENCES,0.4199134199134199,"Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504–507, 2006."
REFERENCES,0.42207792207792205,"Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief
nets. Neural computation, 18(7):1527–1554, 2006."
REFERENCES,0.42424242424242425,"Le Hou, Dimitris Samaras, Tahsin Kurc, Yi Gao, and Joel Saltz. Convnets with smooth adaptive
activation functions for regression. In Artiﬁcial Intelligence and Statistics, pp. 430–439. PMLR,
2017."
REFERENCES,0.4264069264069264,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Interna-
tional Conference on Learning Representations,, 2015."
REFERENCES,0.42857142857142855,"Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender
systems. Computer, 42(8):30–37, 2009. doi: 10.1109/MC.2009.263."
REFERENCES,0.43073593073593075,"Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Geoffrey
Gordon, David Dunson, and Miroslav Dud´ık (eds.), Proceedings of the Fourteenth International
Conference on Artiﬁcial Intelligence and Statistics, volume 15 of Proceedings of Machine Learn-
ing Research, pp. 29–37, Fort Lauderdale, FL, USA, 11–13 Apr 2011. PMLR."
REFERENCES,0.4329004329004329,"Joseph E. LeDoux. Emotion circuits in the brain. Annual Review of Neuroscience, 23(1):155–184,
2000."
REFERENCES,0.43506493506493504,"Joonseok Lee, Seungyeon Kim, Guy Lebanon, Yoram Singer, and Samy Bengio. Llorma: Local
low-rank matrix approximation. Journal of Machine Learning Research, 17(15):1–24, 2016."
REFERENCES,0.43722943722943725,"Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400,
2013."
REFERENCES,0.4393939393939394,"Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of
neural networks: A view from the width. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pp. 6232–6240, 2017."
REFERENCES,0.44155844155844154,"Koji Miyahara, Michael J. Pazzani, and John Slaney. Collaborative ﬁltering with the simple bayesian
classiﬁer. In PRICAI 2000 Topics in Artiﬁcial Intelligence, pp. 679–689, Berlin, Heidelberg, 2000.
Springer Berlin Heidelberg. ISBN 978-3-540-44533-3."
REFERENCES,0.44372294372294374,"Andriy Mnih and Russ R Salakhutdinov. Probabilistic matrix factorization. In J. Platt, D. Koller,
Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems, volume 20,
2008."
REFERENCES,0.4458874458874459,"Federico Monti, Michael Bronstein, and Xavier Bresson. Geometric matrix completion with re-
current multi-graph neural networks. In Advances in Neural Information Processing Systems,
volume 30. Curran Associates, Inc., 2017."
REFERENCES,0.44805194805194803,"Lorenz Muller, Julien Martel, and Giacomo Indiveri.
Kernelized synaptic weight matrices.
In
Proceedings of the 35th International Conference on Machine Learning, pp. 3654–3663, 2018."
REFERENCES,0.45021645021645024,"Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on Learn-
ing Representations, 2018."
REFERENCES,0.4523809523809524,"Allan Pinkus. Approximation theory of the mlp model in neural networks. Acta numerica, 8:143–
195, 1999."
REFERENCES,0.45454545454545453,Under review as a conference paper at ICLR 2022
REFERENCES,0.45670995670995673,"Nikhil Rao, Hsiang-Fu Yu, Pradeep K Ravikumar, and Inderjit S Dhillon. Collaborative ﬁltering
with graph information: Consistency and scalable methods. In C. Cortes, N. Lawrence, D. Lee,
M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems, vol-
ume 28. Curran Associates, Inc., 2015."
REFERENCES,0.4588744588744589,"Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, and John Riedl. Grouplens: an
open architecture for collaborative ﬁltering of netnews. In CSCW ’94: Proceedings of the 1994
ACM conference on Computer supported cooperative work, pp. 175–186, New York, NY, USA,
1994. ACM Press."
REFERENCES,0.461038961038961,"Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton.
Restricted boltzmann machines for
collaborative ﬁltering. In ICML ’07: Proceedings of the 24th international conference on Machine
learning, pp. 791–798, 2007."
REFERENCES,0.46320346320346323,"Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. Autorec: Autoencoders
meet collaborative ﬁltering. In Proceedings of the 24th International Conference on World Wide
Web Companion, pp. 111–112, 2015."
REFERENCES,0.4653679653679654,"Ohad Shamir and Shai Shalev-Shwartz. Matrix completion with the trace norm: Learning, bounding,
and transducing. Journal of Machine Learning Research, 15(98):3401–3423, 2014."
REFERENCES,0.4675324675324675,"Guy Shani, Ronen Brafman, and David Heckerman. An MDP-based Recommender System. In
Proceedings of the Eighteenth Conference Annual Conference on Uncertainty in Artiﬁcial Intelli-
gence, UAI ’02, pp. 453–460, San Francisco, CA, 2002. Morgan Kaufmann."
REFERENCES,0.4696969696969697,"Sho Sonoda and Noboru Murata. Neural network with unbounded activation functions is universal
approximator. Applied and Computational Harmonic Analysis, 43(2):233–268, 2017."
REFERENCES,0.47186147186147187,"Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In International Conference
on Computational Learning Theory, pp. 545–560. Springer, 2005."
REFERENCES,0.474025974025974,"Jonathan Strahl, Jaakko Peltonen, Hiroshi Mamitsuka, and Samuel Kaski. Scalable probabilistic
matrix factorization with graph-based priors. In The Thirty-Fourth AAAI Conference on Artiﬁcial
Intelligence, AAAI, pp. 5851–5858. AAAI Press, 2020."
REFERENCES,0.47619047619047616,"Xiaoyuan Su and Taghi M. Khoshgoftaar. A survey of collaborative ﬁltering techniques. Adv. in
Artif. Intell., 2009:4:2–4:2, January 2009."
REFERENCES,0.47835497835497837,"Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via nonconvex factorization. In 2015
IEEE 56th Annual Symposium on Foundations of Computer Science, pp. 270–289, 2015."
REFERENCES,0.4805194805194805,"Lyle H. Ungar and Dean P. Foster. Clustering methods for collaborative ﬁltering. In Workshop on
Recommender Systems at the 15th National Conference on Artiﬁcial Intelligence (AAAI’98), pp.
112–125, Madison, Wisconsin, USA, July 1998. AAAI Press."
REFERENCES,0.48268398268398266,"Rianne van den Berg, Thomas N. Kipf, and Max Welling. Graph convolutional matrix completion.
abs/1706.02263, 2017."
REFERENCES,0.48484848484848486,"Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and
L´eon Bottou. Stacked denoising autoencoders: Learning useful representations in a deep network
with a local denoising criterion. Journal of machine learning research, 11(12), 2010."
REFERENCES,0.487012987012987,"Max Welling, Ian Porteous, and Kenichi Kurihara. Exchangeable inconsistent priors for bayesian
posterior inference.
In 2012 Information Theory and Applications Workshop, ITA 2012, San
Diego, CA, USA, February 5-10, 2012, pp. 407–414. IEEE, 2012."
REFERENCES,0.48917748917748916,"Yao Wu, Christopher DuBois, Alice X. Zheng, and Martin Ester. Collaborative denoising auto-
encoders for top-n recommender systems. In Proceedings of the Ninth ACM International Con-
ference on Web Search and Data Mining, pp. 153–162, 2 2016."
REFERENCES,0.49134199134199136,"Baolin Yi, Xiaoxuan Shen, Hai Liu, Zhaoli Zhang, Wei Zhang, Sannyuya Liu, and Naixue Xiong.
Deep matrix factorization with implicit feedback embedding for recommendation system. IEEE
Trans. Ind. Informatics, 15(8):4591–4601, 2019."
REFERENCES,0.4935064935064935,Under review as a conference paper at ICLR 2022
REFERENCES,0.49567099567099565,"Joonyoung Yi, Juhyuk Lee, Kwang Joon Kim, Sung Ju Hwang, and Eunho Yang. Why not to
use zero imputation? correcting sparsity bias in training neural networks. In 8th International
Conference on Learning Representations,, 2020."
REFERENCES,0.49783549783549785,"Shuai Zhang, Lina Yao, and Xiwei Xu. Autosvd++: An efﬁcient hybrid collaborative ﬁltering model
via contractive auto-encoders. In SIGIR, pp. 957–960. ACM, 2017."
REFERENCES,0.5,"Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. Deep learning based recommender system: A survey
and new perspectives. ACM Comput. Surv., 52(1):1–38, 2019."
REFERENCES,0.5021645021645021,"Yin Zheng, Bangsheng Tang, Wenkui Ding, and Hanning Zhou. A neural autoregressive approach to
collaborative ﬁltering. In Proceedings of the 33nd International Conference on Machine Learn-
ing, volume 48, pp. 764–773, 2016."
REFERENCES,0.5043290043290043,"A
PROOF FOR THE MAIN THEOREM"
REFERENCES,0.5064935064935064,"First of all, we give the following lemmas."
REFERENCES,0.5086580086580087,"Lemma 1. Let H = {H ∈Rm×n : hij = ΘLΘσ (ΘLΘ−1(· · · σ(Θ1zij) · · · )) , ∀(i, j) ∈[m] ×
[n]; Θl ∈Rpl×pl−1, ∥Θl∥2 ≤bl, ∥Θl∥F ≤b′
l, ∀l ∈[LΘ]; Z ∈Z, ∥Z∥F ≤sz}, where the
Lipschitz constant of σ is ϱ. Suppose the covering number of Z with respect to ∥· ∥F is upper-
bounded by κε and ε = ϵ
 
2ϱLΘ−1 QLΘ
l=1 bl
−1. Then the cover covering number of H with respect
to ∥· ∥F is bounded as"
REFERENCES,0.5108225108225108,"N(H, ∥· ∥F , ϵ) ≤κε LΘ
Y l=1 3
√"
REFERENCES,0.512987012987013,"2ϱLΘ−1(LΘ + 1)sz
QLΘ
l=1 bl
ϵ"
REFERENCES,0.5151515151515151,"!plpl−1
."
REFERENCES,0.5173160173160173,Proof. See Section B.1.
REFERENCES,0.5194805194805194,"Lemma 1 provides an upper bound of the covering number of the element-wise neural network. The
following lemma shows an upper bound of the covering number of the main neural network."
REFERENCES,0.5216450216450217,"Lemma
2
(Theorem
3.3
of
Bartlett
et
al.
(2017),
reformulated).
Let
Z
=
WLW σ

WLW −1

· · · σ(W1 ˜
X) · · ·

,
where
Wl
∈
Rdl+1dl,
l
∈
[LW ],
and"
REFERENCES,0.5238095238095238,"max(m, d1, . . . , dLW ) ≤D. Denote the Lipschitz constant of σ by ρ. Suppose the reference
matrices (M1, . . . , MLW ) are given. Deﬁne"
REFERENCES,0.525974025974026,"C = {FW( ˜
X) : W = (W1, . . . , WLW ) , ∥Wl∥σ ≤al, ∥Wl −Ml∥σ ≤a′
l, ∀∈[LW ]}."
REFERENCES,0.5281385281385281,"The for any ϵ > 0,"
REFERENCES,0.5303030303030303,"ln N (C, ϵ, ∥· ∥F ) ≤∥˜
X∥2
F ln 2D2 ϵ2 "
REFERENCES,0.5324675324675324,"ρ2(LW −1)
LW
Y"
REFERENCES,0.5346320346320347,"l=1
a2
l"
REFERENCES,0.5367965367965368,"!  LW
X l=1"
REFERENCES,0.538961038961039,"a′
l
al"
REFERENCES,0.5411255411255411,2/3!3 .
REFERENCES,0.5432900432900433,Now we can get an upper bound for the covering number of the entire neural network in NE-AECF.
REFERENCES,0.5454545454545454,"Lemma 3. The covering number of HW,Θ = {HΘ
 
FW( ˜
X)

} with respect to ∥· ∥F satisﬁes"
REFERENCES,0.5476190476190477,"ln N(H, ∥· ∥F , ϵ) ≤4ϱ2(LΘ−1)ρ2(LW −1)∥˜
X∥2
F ln 2D2 ϵ2 LW
Y"
REFERENCES,0.5497835497835498,"l=1
a2
l"
REFERENCES,0.551948051948052,"!  LΘ
Y"
REFERENCES,0.5541125541125541,"l=1
b2
l"
REFERENCES,0.5562770562770563,"!  LW
X l=1"
REFERENCES,0.5584415584415584,"a′
l
al"
REFERENCES,0.5606060606060606,"2/3!3 + LΘ
X"
REFERENCES,0.5627705627705628,"l=1
plpl−1 ! ln  

"
REFERENCES,0.564935064935065,"6LΘϱLΘ−1ρLW −1∥˜
X∥F
QLW
l=1 al
 QLΘ
l=1 bl

maxl
b′
l
bl
ϵ "
REFERENCES,0.5670995670995671,"

."
REFERENCES,0.5692640692640693,Proof. See Section B.2.
REFERENCES,0.5714285714285714,Under review as a conference paper at ICLR 2022
REFERENCES,0.5735930735930735,"Now we can calculate the upper bound of the Rademacher complexity of HW,Θ via using Lemma 3
and Dudley entropy integral bound."
REFERENCES,0.5757575757575758,"Lemma 4. Let v1 = 4ρ2(LW −1)ϱ2(LΘ−1)∥˜
X∥2
F ln 2D2 QLW
l=1 a2
l
  
PLW
l=1"
REFERENCES,0.577922077922078,"a′
l
al"
REFERENCES,0.5800865800865801,"2/3!3 QLΘ
l=1 b2
l

,"
REFERENCES,0.5822510822510822,"v2 = PLΘ
l=1 plpl−1, and v3 = 6LΘρLW −1ϱLΘ−1∥˜
X∥F
QLW
l=1 al
 QLΘ
l=1 bl

maxl b′
lb−1
l
. Sup-"
REFERENCES,0.5844155844155844,"pose ∥hΘ
 
gW ( ˜
X)

∥∞≤µ. The Rademacher complexity of HW,Θ is bounded as"
REFERENCES,0.5865800865800865,"RS(HW,Θ) ≤4µ"
REFERENCES,0.5887445887445888,"S + 12
p"
REFERENCES,0.5909090909090909,v1 + µ2v2 ln S
REFERENCES,0.5930735930735931,"S
+ 12µ
p"
REFERENCES,0.5952380952380952,"v2 ln µ−1v3
√"
REFERENCES,0.5974025974025974,"S
.
(17)"
REFERENCES,0.5995670995670995,Proof. See Section B.3.
REFERENCES,0.6017316017316018,"The following lemma provides a sample complexity bound for transductive learning, which is con-
sistent with the objective function and evaluation metric (RMSE) widely used in collaborative ﬁlter-
ing.
Lemma 5 (Corollary 1 of (El-Yaniv & Pechyony, 2009), reformulated). Let H be a ﬁxed hypothesis
set and suppose supi,j|X∈H |ℓ(Yij, Xij) | ≤τℓ. Suppose a ﬁxed set S of distinct indices is uniformly
and randomly split to two subsets Strain and Stest, where2 |Stest| > |Strain| > 50. Then with probability
at least 1 −δ over the random split, we have
1
|Stest| X"
REFERENCES,0.6038961038961039,"(i,j)∈Stest
ℓ(Yij, Xij) ≤
1
|Strain| X"
REFERENCES,0.6060606060606061,"(i,j)∈Strain
ℓ(Yij, Xij) + 4RS(ℓ◦H)"
REFERENCES,0.6082251082251082,"+ 11τℓ(|Strain| + |Stest|)
p"
REFERENCES,0.6103896103896104,"|Strain||Stest|
+ 3τℓ s"
REFERENCES,0.6125541125541125,(|Strain| + |Stest|)
REFERENCES,0.6147186147186147,"|Strain||Stest|
ln 1 δ (18)"
REFERENCES,0.6168831168831169,Then Theorem 1 can be proved as follows.
REFERENCES,0.6190476190476191,"Proof. Accoding to the Rademacher contraction property, we have RS(ℓ◦H) ≤ηℓRS(H), where
ηℓdenotes the lipschitz constant of ℓ. Using Lemma 5 with a slightly different notation and Lemma
4 where µ2v2 ≪v1 provided that the element-wise neural network is small enough, we have
1
|Sc| X"
REFERENCES,0.6212121212121212,"(i,j)∈Sc
ℓ

Xij, ˆXij

≤1 |S| X"
REFERENCES,0.6233766233766234,"(i,j)∈S
ℓ

Xij, ˆXij
 + ηℓ 16µ"
REFERENCES,0.6255411255411255,"|S| + 48
p"
REFERENCES,0.6277056277056277,v1 + µ2v2 ln |S|
REFERENCES,0.6298701298701299,"|S|
+ 48µ
p"
REFERENCES,0.6320346320346321,"v2 ln µ−1v3
p |S| !"
REFERENCES,0.6341991341991342,"+ 11τℓ(|S| + |Sc|)
p"
REFERENCES,0.6363636363636364,"|S||Sc|
+ 3τℓ s"
REFERENCES,0.6385281385281385,(|S| + |Sc|)
REFERENCES,0.6406926406926406,"|S||Sc|
ln 1 δ ≤1 |S| X"
REFERENCES,0.6428571428571429,"(i,j)∈S
ℓ

Xij, ˆXij
"
REFERENCES,0.645021645021645,"+ C1ηℓv′
1 ln |S|
|S|
+ C2ηℓµ s"
REFERENCES,0.6471861471861472,"v2 ln v′
3
|S|"
REFERENCES,0.6493506493506493,"+ 11τℓmn
p"
REFERENCES,0.6515151515151515,"|S||Sc|
+ 3τℓ s"
REFERENCES,0.6536796536796536,"mn
|S||Sc| ln 1 δ , (19)"
REFERENCES,0.6558441558441559,"where C1 and C2 are some ﬁxed constants, and"
REFERENCES,0.658008658008658,"v′
1 = ρ(LW −1)ϱ(LΘ−1)∥X∥F
√ ln D LW
Y"
REFERENCES,0.6601731601731602,"l=1
al"
REFERENCES,0.6623376623376623,"!  LW
X l=1"
REFERENCES,0.6645021645021645,"a′
l
al"
REFERENCES,0.6666666666666666,"2/3!3/2  LΘ
Y"
REFERENCES,0.6688311688311688,"l=1
bl ! ,"
REFERENCES,0.670995670995671,2We use these assumptions to simplify the theorem.
REFERENCES,0.6731601731601732,Under review as a conference paper at ICLR 2022
REFERENCES,0.6753246753246753,"and v′
3 = LΘµ−1γρLW −1ϱLΘ−1∥X∥F
QLW
l=1 al
 QLΘ
l=1 bl

. Rename v′
1 and v′
3 as v1 and v3
respectively, we ﬁnish the proof."
REFERENCES,0.6774891774891775,"B
PROOF FOR LEMMAS"
REFERENCES,0.6796536796536796,"B.1
PROOF FOR LEMMA 1"
REFERENCES,0.6818181818181818,"Proof. Let SΘl := {Θl ∈Rpl+1×pl : ∥Θl∥2 ≤bl, ∥Θl∥F ≤b′
l}, ∀l ∈[LΘ]. It is known that there
exists an ϵl-net ¯SΘl obeying"
REFERENCES,0.683982683982684,"N(SΘl, ∥· ∥F , ϵl) ≤
3b′
l
ϵl"
REFERENCES,0.6861471861471862,plpl−1
REFERENCES,0.6883116883116883,such that ∥Θl −¯Θl∥F ≤ϵl. We have
REFERENCES,0.6904761904761905,"|hij −¯hij| = ∥hij −¯hij∥F
=
ΘLΘσ (ΘLΘ−1(· · · σ(Θ1zij) · · · )) −¯ΘLΘσ
  ¯ΘLΘ−1(· · · σ( ¯Θ1¯zij) · · · )

F
=
ΘLΘσ (ΘLΘ−1(· · · σ(Θ1zij) · · · )) −¯ΘLΘσ (ΘLΘ−1(· · · σ(Θ1zij) · · · ))"
REFERENCES,0.6926406926406926,"+ ¯ΘLΘσ (ΘLΘ−1(· · · σ(Θ1zij) · · · )) −¯ΘLΘσ
  ¯ΘLΘ−1(· · · σ(Θ1zij) · · · )

+ · · ·"
REFERENCES,0.6948051948051948,"+ ¯ΘLΘσ
  ¯ΘLΘ−1(· · · σ(Θ1zij) · · · )

−¯ΘLΘσ
  ¯ΘLΘ−1(· · · σ( ¯Θ1zij) · · · )
 
F
+ ¯ΘLΘσ
  ¯ΘLΘ−1(· · · σ( ¯Θ1zij) · · · )

−¯ΘLΘσ
  ¯ΘLΘ−1(· · · σ( ¯Θ1¯zij) · · · )
 
F
≤
ΘLΘσ (ΘLΘ−1(· · · σ(Θ1zij) · · · )) −¯ΘLΘσ (ΘLΘ−1(· · · σ(Θ1zij) · · · ))

F
+
 ¯ΘLΘσ (ΘLΘ−1(· · · σ(Θ1zij) · · · )) −¯ΘLΘσ
  ¯ΘLΘ−1(· · · σ(Θ1zij) · · · )
 
F + · · ·"
REFERENCES,0.696969696969697,"+
 ¯ΘLΘσ
  ¯ΘLΘ−1(· · · σ(Θ1zij) · · · )

−¯ΘLσ
  ¯ΘLΘ−1(· · · σ( ¯Θ1zij) · · · )
 
F
+
 ¯ΘLΘσ
  ¯ΘLΘ−1(· · · σ( ¯Θ1zij) · · · )

−¯ΘLΘσ
  ¯ΘLΘ−1(· · · σ( ¯Θ1¯zij) · · · )
 
F"
REFERENCES,0.6991341991341992,"(a)
≤ϱLΘ−1 |zij|
ΘLΘ −¯ΘLΘ

F"
REFERENCES,0.7012987012987013,"LΘ−1
Y l=1"
REFERENCES,0.7034632034632035,"Θl

2"
REFERENCES,0.7056277056277056,"+ ϱLΘ−1 |zij|
 ¯ΘLΘ

2
ΘLΘ−1 −¯ΘLΘ−1

F"
REFERENCES,0.7077922077922078,"LΘ−2
Y"
REFERENCES,0.70995670995671,"l=1
∥Θl∥2 + · · ·"
REFERENCES,0.7121212121212122,"+ ϱL−1 |zij| LΘ
Y l=2"
REFERENCES,0.7142857142857143,"¯Θl

2"
REFERENCES,0.7164502164502164,"!
Θ1 −¯Θ1

F"
REFERENCES,0.7186147186147186,"+ ϱLΘ−1 ∥zij −¯zij∥F LΘ
Y"
REFERENCES,0.7207792207792207,"l=1
∥¯Θl∥2"
REFERENCES,0.7229437229437229,≤ϱLΘ−1 
REFERENCES,0.7251082251082251,"szijϵLΘ
Y"
REFERENCES,0.7272727272727273,"l̸=LΘ
bl + szijϵLΘ−1
Y"
REFERENCES,0.7294372294372294,"l̸=LΘ−1
bl + szijϵ1 LΘ
Y"
REFERENCES,0.7316017316017316,"l̸=1
bl + · · · + ∥zij −¯zij∥F LΘ
Y"
REFERENCES,0.7337662337662337,"l=1
bl  . (20)"
REFERENCES,0.7359307359307359,Under review as a conference paper at ICLR 2022
REFERENCES,0.7380952380952381,"In (a), we used the facts ∥XY ∥F ≤∥X∥2∥Y ∥F and ∥σ(X)−σ(Y )∥F ≤ϱ∥X−Y ∥F recursively.
It follows that"
REFERENCES,0.7402597402597403,"∥H −¯
H∥F =
sX"
REFERENCES,0.7424242424242424,"ij
h2
ij"
REFERENCES,0.7445887445887446,"(a)
≤ϱLΘ−1"
REFERENCES,0.7467532467532467,"v
u
u
u
u
t X ij
2 "
REFERENCES,0.7489177489177489,"
s2zij "
REFERENCES,0.7510822510822511,"ϵLΘ
Y"
REFERENCES,0.7532467532467533,"l̸=LΘ
bl + ϵLΘ−1
Y"
REFERENCES,0.7554112554112554,"l̸=LΘ−1
bl + · · · + ϵ1 LΘ
Y"
REFERENCES,0.7575757575757576,"l̸=1
bl   2"
REFERENCES,0.7597402597402597,"+ ∥zij −ˆzij∥2
F LΘ
Y"
REFERENCES,0.7619047619047619,"l=1
sl !2 
 =
√"
REFERENCES,0.7640692640692641,2ϱLΘ−1
REFERENCES,0.7662337662337663,"v
u
u
u
t "
REFERENCES,0.7683982683982684,"ϵLΘ
Y"
REFERENCES,0.7705627705627706,"l̸=LΘ
bl + ϵLΘ−1
Y"
REFERENCES,0.7727272727272727,"l̸=LΘ−1
bl + · · · + ϵ1 LΘ
Y"
REFERENCES,0.7748917748917749,"l̸=1
bl   2"
REFERENCES,0.7770562770562771,"∥Z∥2
F + LΘ
Y"
REFERENCES,0.7792207792207793,"l=1
bl !2"
REFERENCES,0.7813852813852814,"∥Z −¯Z∥2
F ≤
√"
REFERENCES,0.7835497835497836,2ϱLΘ−1
REFERENCES,0.7857142857142857,"v
u
u
u
t "
REFERENCES,0.7878787878787878,"ϵLΘ
Y"
REFERENCES,0.79004329004329,"l̸=LΘ
bl + ϵLΘ−1
Y"
REFERENCES,0.7922077922077922,"l̸=LΘ−1
bl + · · · + ϵ1 LΘ
Y"
REFERENCES,0.7943722943722944,"l̸=1
bl   2 s2z + LΘ
Y"
REFERENCES,0.7965367965367965,"l=1
bl !2 ϵ2z. (21)"
REFERENCES,0.7987012987012987,"In (a), we used the fact (x + y)2 ≤2(x2 + y2). Let ϵl =
ϵ/(
√"
REFERENCES,0.8008658008658008,"2LΘ)
√"
REFERENCES,0.803030303030303,"2ϱLΘ−1sz
Q"
REFERENCES,0.8051948051948052,"k̸=l bk
, ∀l ∈[LΘ]. Let"
REFERENCES,0.8073593073593074,"ϵz =
ϵ/
√ 2
√"
REFERENCES,0.8095238095238095,"2ϱLΘ−1 QLΘ
l=1 bl
. We arrive at"
REFERENCES,0.8116883116883117,"∥H −¯
H∥F ≤ϵ.
(22)"
REFERENCES,0.8138528138528138,"It means that ¯
H is an ϵ-cover of H. Then the covering number of H is bounded as"
REFERENCES,0.816017316017316,"N(H, ∥· ∥F , ϵ)"
REFERENCES,0.8181818181818182,"≤N(Z, ∥· ∥F , ϵz) LΘ
Y"
REFERENCES,0.8203463203463204,"l=1
N(SΘl, ∥· ∥F , ϵl) ≤κε LΘ
Y l=1"
REFERENCES,0.8225108225108225,"6ϱLΘ−1LΘszb′
l
Q"
REFERENCES,0.8246753246753247,k̸=l bk) ϵ
REFERENCES,0.8268398268398268,"!plpl−1 =κε LΘ
Y l=1"
REFERENCES,0.829004329004329,"6ϱLΘ−1LΘszb′
lb−1
l
QLΘ
k=1 bk
ϵ"
REFERENCES,0.8311688311688312,!plpl−1 ≤κε CΘ ϵ
REFERENCES,0.8333333333333334,"PLΘ
l=1 plpl−1
, (23)"
REFERENCES,0.8354978354978355,"where CΘ = 6ϱLΘ−1LΘszγ QLΘ
l=1 bl and γ = maxl b′
lb−1
l
. This ﬁnished the proof."
REFERENCES,0.8376623376623377,"B.2
PROOF FOR LEMMA 3"
REFERENCES,0.8398268398268398,Proof. It is easy to show that
REFERENCES,0.841991341991342,"sz ≥∥Z∥F =
WLW

σW
 
WLW −1σW (· · · σW (W1 ˜
X) · · · )

F
(24)"
REFERENCES,0.8441558441558441,"≥ρLW −1∥X∥F LW
Y"
REFERENCES,0.8463203463203464,"l=1
al.
(25)"
REFERENCES,0.8484848484848485,Under review as a conference paper at ICLR 2022
REFERENCES,0.8506493506493507,"Combining Lemma 1 and Lemma 2, we have"
REFERENCES,0.8528138528138528,"ln N(H, ∥· ∥F , ϵ)"
REFERENCES,0.854978354978355,"≤ln κε + LΘ
X"
REFERENCES,0.8571428571428571,"l=1
plpl−1 ! ln(CΘ ϵ )"
REFERENCES,0.8593073593073594,"≤4ρ2(LW −1)ϱ2(LΘ−1)∥X∥2
F ln 2D2 ϵ2 LW
Y"
REFERENCES,0.8614718614718615,"l=1
a2
l"
REFERENCES,0.8636363636363636,"!  LW
X l=1"
REFERENCES,0.8658008658008658,"a′
l
al"
REFERENCES,0.8679653679653679,"2/3!3  LΘ
Y"
REFERENCES,0.8701298701298701,"l=1
b2
l ! + LΘ
X"
REFERENCES,0.8722943722943723,"l=1
plpl−1 ! ln  

"
REFERENCES,0.8744588744588745,"6LΘϱLΘ−1ρLW −1∥X∥F
QLW
l=1 al
 QLΘ
i=1 bl

maxl
b′
l
bl
ϵ "
REFERENCES,0.8766233766233766,"

. (26)"
REFERENCES,0.8787878787878788,"B.3
PROOF FOR LEMMA 4"
REFERENCES,0.8809523809523809,"Before proof, we give the following lemma, which is a variant of the Dudley entropy integral bound
on Rademacher complexity."
REFERENCES,0.8831168831168831,"Lemma 6 (Lemma A.5 of Bartlett et al. (2017)). Let F be a real-valued function class taking values
in [0, 1] and assume that 0 ∈F. Then"
REFERENCES,0.8852813852813853,"R(F|S) ≤inf
α>0 4α
√"
REFERENCES,0.8874458874458875,"S
+ 12 S Z √ S α q"
REFERENCES,0.8896103896103896,"ln S(F|S, ∥· ∥F , ϵ)dϵ !"
REFERENCES,0.8917748917748918,".
(27)"
REFERENCES,0.8939393939393939,"Proof. For convenience, let"
REFERENCES,0.8961038961038961,"v1 = 4ρ2(LW −1)ϱ2(LΘ−1)∥X∥2
F ln 2D2
 LW
Y"
REFERENCES,0.8982683982683982,"l=1
a2
l"
REFERENCES,0.9004329004329005,"!  LW
X l=1"
REFERENCES,0.9025974025974026,"a′
l
al"
REFERENCES,0.9047619047619048,"2/3!3  LΘ
Y"
REFERENCES,0.9069264069264069,"l=1
b2
l ! ,"
REFERENCES,0.9090909090909091,"v2 = PLΘ
l=1 plpl−1, and v3 = 6LΘρLW −1ϱLΘ−1∥X∥F
QLW
l=1 al
 QLΘ
l=1 bl

maxl b′
lb−1
l
. Then"
REFERENCES,0.9112554112554112,"ln N(H, ∥· ∥F , ϵ) ≤v1"
REFERENCES,0.9134199134199135,"ϵ2 + v2 ln
v3 ϵ"
REFERENCES,0.9155844155844156,"
from (26). Let µ = maxH∈H ∥H∥∞and ¯H = { ¯H : µ ¯H ∈"
REFERENCES,0.9177489177489178,"H}. It follows that ln N( ¯H, ∥· ∥F , ϵ) ≤µ−2v1"
REFERENCES,0.9199134199134199,"ϵ2
+ v2 ln
µ−1v3 ϵ 
."
REFERENCES,0.922077922077922,"According to Lemma 6, we have"
REFERENCES,0.9242424242424242,"RS( ¯H) ≤inf
α>0 4α
√"
REFERENCES,0.9264069264069265,"S
+ 12 S Z √ S α s µ−2v1"
REFERENCES,0.9285714285714286,"ϵ2
+ v2 ln
µ−1v3 ϵ 
dϵ !"
REFERENCES,0.9307359307359307,"(a)
≤inf
α>0 4α
√"
REFERENCES,0.9329004329004329,"S
+ 12 S Z √ S α r"
REFERENCES,0.935064935064935,µ−2v1 + v2
REFERENCES,0.9372294372294372,"ϵ2
+ v2 ln µ−1v3 dϵ !"
REFERENCES,0.9393939393939394,"(b)
≤inf
α>0 4α
√"
REFERENCES,0.9415584415584416,"S
+ 12 S Z √ S α p"
REFERENCES,0.9437229437229437,"µ−2v1 + v2 ϵ
+
p"
REFERENCES,0.9458874458874459,v2 ln µ−1v3 ! dϵ !
REFERENCES,0.948051948051948,"= inf
α>0 4α
√"
REFERENCES,0.9502164502164502,"S
+ 12 S p"
REFERENCES,0.9523809523809523,µ−2v1 + v2 ln √
REFERENCES,0.9545454545454546,"S
α +
p"
REFERENCES,0.9567099567099567,"v2 ln µ−1v3
√"
REFERENCES,0.9588744588744589,"S −α
!!"
REFERENCES,0.961038961038961,"(c)
≤4"
REFERENCES,0.9632034632034632,"S + 12
p"
REFERENCES,0.9653679653679653,µ−2v1 + v2
REFERENCES,0.9675324675324676,"S
ln S + 12(S −1)
p"
REFERENCES,0.9696969696969697,"v2 ln µ−1v3
S
√ S ≤4"
REFERENCES,0.9718614718614719,"S + 12
p"
REFERENCES,0.974025974025974,µ−2v1 + v2 ln S
REFERENCES,0.9761904761904762,"S
+ 12
p"
REFERENCES,0.9783549783549783,"v2 ln µ−1v3
√ S
. (28)"
REFERENCES,0.9805194805194806,Under review as a conference paper at ICLR 2022
REFERENCES,0.9826839826839827,"In (a), we used the fact ln 1"
REFERENCES,0.9848484848484849,"x ≤
1
x2 . The inequality (b) holds according to √x + y ≤√x + √y. In
(c), we have let α =
1
√"
REFERENCES,0.987012987012987,"S , which though may not be the best choice. We arrive at"
REFERENCES,0.9891774891774892,"RS(HW,Θ) ≤4µ"
REFERENCES,0.9913419913419913,"S + 12
p"
REFERENCES,0.9935064935064936,v1 + µ2v2 ln S
REFERENCES,0.9956709956709957,"S
+ 12µ
p"
REFERENCES,0.9978354978354979,"v2 ln µ−1v3
√ S
."
