Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0049504950495049506,"Conventional reinforcement learning (RL) needs an environment to collect fresh
data, which is impractical when an online interaction is costly. Ofﬂine RL pro-
vides an alternative solution by directly learning from the logged dataset. How-
ever, it usually yields unsatisfactory performance due to a pessimistic update
scheme or/and the low quality of logged datasets. Moreover, how to evaluate
the policy under the ofﬂine setting is also a challenging problem. In this paper,
we propose a uniﬁed framework called Adaptive Q-learning for effectively tak-
ing advantage of ofﬂine and online learning. Speciﬁcally, we explicitly consider
the difference between the online and ofﬂine data and apply an adaptive update
scheme accordingly, i.e., a pessimistic update strategy for the ofﬂine dataset and
a greedy or no pessimistic update scheme for the online dataset. When combin-
ing both, we can apply very limited online exploration steps to achieve expert
performance even when the ofﬂine dataset is poor, e.g., random dataset. Such a
framework provides a uniﬁed way to mix the ofﬂine and online RL and gain the
best of both worlds. To understand our framework better, we then provide an ini-
tialization following our framework. Extensive experiments are done to verify the
effectiveness of our proposed method."
INTRODUCTION,0.009900990099009901,"1
INTRODUCTION"
INTRODUCTION,0.01485148514851485,"Conventional online reinforcement learning (RL) methods (Haarnoja et al., 2018; Fujimoto et al.,
2018) usually learn from experiences generated by interactions with the online environment. They
are impractical in some real-world applications, e.g., dialog (Jaques et al., 2019) and educa-
tion (Mandel et al., 2014), where interactions are costly. Recently, ofﬂine RL (Levine et al., 2020)
has aroused much attention. It targets the above challenge by making the agent learn from an ofﬂine
dataset collected by other policies in a purely data-driven manner. The difference between online
RL and ofﬂine RL is shown in Figure 1."
INTRODUCTION,0.019801980198019802,"Existing ofﬂine RL studies try to target the distribution mismatch or out-of-distribution actions issue
by employing a pessimistic update scheme (Kumar et al., 2019; 2020) or in combination with imi-
tation learning (Fujimoto et al., 2019). However, when the dataset is ﬁxed, ofﬂine RL cannot learn
the optimal policy (Kidambi et al., 2020), and even worse, when the dataset’s quality is poor, ofﬂine
RL usually gains a relatively bad performance (Kumar et al., 2020; Fu et al., 2020; Levine et al.,
2020). On the other hand, it is challenging to evaluate the learned policy when learning totally from
the ofﬂine dataset. Even though some research topics, e.g., off-policy evaluation (Dann et al., 2014),
study how to evaluate the learned policy without the interaction with the environment, it is still not
ideal for the practical purpose."
INTRODUCTION,0.024752475247524754,"Some recent works try to address the above issues by employing an ofﬂine-online setting. Such
methods (Lee et al., 2021; Nair et al., 2020) focus on pre-training a policy using the ofﬂine dataset
and ﬁne-tuning the policy through further online interactions. Even though their methods allevi-
ate the above issues to some extent, their main bottleneck is that they do not consider the different
characteristics of ofﬂine and online data. For instance, pre-existing ofﬂine data can prevent agents
from converging prematurely due to the potential diverse ofﬂine dataset, while online data can im-
prove stability and accelerate convergence. Generally, these different data are mixed and used by a
pessimistic strategy to update the policy in their methods, which may be problematic since using a
pessimistic strategy for online data may harm the policy performance (Nair et al., 2020). Moreover,"
INTRODUCTION,0.0297029702970297,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.034653465346534656,"Figure 1: Online RL collect the data by interacting with the environment and they don’t utilize the
existing logged dataset while ofﬂine RL only exploit logged dataset without any future performance
improvement. By contrast, we focus on obtaining the best of both worlds."
INTRODUCTION,0.039603960396039604,"with sufﬁciently large and diverse ofﬂine data, a high-performing policy can be learned just using a
pure online RL algorithm (Agarwal et al., 2020). And the online near-on-policy data also play a key
role in improving the RL algorithm’s stability (Fujimoto et al., 2019). Hence, we should take full
advantage of both ofﬂine and online dataset."
INTRODUCTION,0.04455445544554455,"To tackle the above problems, in this paper, we emphasize that online and ofﬂine RL should be cou-
pled organically. First, a separate updating strategy should be employed for online and ofﬂine data,
respectively, considering their different characteristics. To do so, we present a framework called
adaptive Q-learning that integrates the advantage of ofﬂine learning and online learning effectively.
When learning from the ofﬂine dataset, we conduct a pessimistic update strategy. In contrast, we
use a greedy or non-pessimistic update strategy when learning from the online dataset. Second,
we design a novel replay buffer to distinguish the ofﬂine from online datasets in a simple way. By
utilizing such a novel framework and buffer design, the agent can achieve an expert policy using lim-
ited online interaction steps regardless of the quality of the ofﬂine dataset. In the experiments, our
proposed framework can achieve better performance by using only one ﬁfth number of interactions
compared with the previous method (Nair et al., 2020)."
INTRODUCTION,0.04950495049504951,Our contributions can be summarized as below:
INTRODUCTION,0.054455445544554455,"• We propose a uniﬁed framework called Adaptive Q-learning that can effectively beneﬁt
from both the ofﬂine dataset and limited number of online interaction data.
• Based on the general framework, we initialize a practical algorithm, called Greedy-
conservative Q-ensemble learning (GCQL) that builds on top of State-of-the-Art ofﬂine
RL and online RL method.
• We empirically verify the effectiveness of our method by comprehensive experiments on
the popular continuous control tasks MuJoCo (Todorov et al., 2012) with ofﬂine dataset
coming from D4RL (Fu et al., 2020)."
RELATED WORK,0.0594059405940594,"2
RELATED WORK"
RELATED WORK,0.06435643564356436,"Online RL In general, online RL algorithms can be divided into two categories, i.e., on-policy
and off-policy algorithms. On-policy methods (Schulman et al., 2015; 2017) update the policy
using data collected by its current behavior policy. As ignoring the logged data collected by its
history behaviour policies, they usually have a lower sample efﬁciency than the off-policy RL. On
the other hand, off-policy methods (Fujimoto et al., 2018; Chen et al., 2021) enable the policy to
learn from experience collected by history behavior polices, however, they cannot learn well from
history trajectories collected by other agents’ behavior policies (Fujimoto et al., 2019; Kumar et al.,
2020). Consequently, the need for huge online interaction makes online RL impractical for some
real-world applications,such as dialog agents (Jaques et al., 2019) or education system (Mandel
et al., 2014)."
RELATED WORK,0.06930693069306931,"Ofﬂine RL Ofﬂine RL algorithms assume the online environment is unavailable and learn policies
only from the pre-collected dataset. As the value estimation error cannot be corrected using online
interactions here, these methods tend to utilize a pessimistic updating strategy to relieve the distri-
bution mismatch problem (Fujimoto et al., 2019; Kumar et al., 2019). To implement such a strategy,"
RELATED WORK,0.07425742574257425,Under review as a conference paper at ICLR 2022
RELATED WORK,0.07920792079207921,"model-free ofﬂine RL methods generally employ value or policy penalties to constrain the updated
policy close to the data collecting policy (Wu et al., 2019; Kumar et al., 2020; Fujimoto et al., 2019;
He & Hou, 2020). And model-based methods use predictive models to estimate uncertainties of
states and then update the policy in a pessimistic way based on them (Kidambi et al., 2020; Yu et al.,
2020). Those ofﬂine RL methods cannot guarantee a good performance, especially when the data
quality is poor (Kumar et al., 2020). Besides, policy evaluation when the online environment is un-
available is also challenging. Even though off-policy evaluation (OPE) methods (Dann et al., 2014)
present alternative solutions, they are still far from perfect."
RELATED WORK,0.08415841584158416,Above issues of online and ofﬂine RL motivate us to investigate the ofﬂine-online setting.
RELATED WORK,0.0891089108910891,"Ofﬂine-online RL Lee et al. (2021) and Nair et al. (2020) focus on the mixed setting where the
agent is ﬁrst learned from the ofﬂine dataset, and then trained online. Nair et al. (2020) propose
an advantage-weighted actor-critic (AWAC) method that restricts the policy to select actions close
to those in the ofﬂine data by an implicit constraint. When online interactions are available, such
conservative constraint may have adverse effects on the performance. Lee et al. (2021) employ a
balanced replay scheme to address the distribution shift issue. It uses the ofﬂine data by only se-
lecting near-on-policy samples. Unlike these two works, our method utilizes all online and ofﬂine
data, and explicitly considers the difference between them by adaptively applying non-conservative
or conservative updating schemes, respectively. Matsushima et al. (2021) focuses on optimizing
deployment efﬁciency, i.e., the number of distinct data-collection policies used during learning, by
employing a behavior-regularized policy updating strategy. Although in terms of deployment efﬁ-
ciency, their work is between online and ofﬂine RL, it ignores existing ofﬂine dataset, and dose not
focusing on improving sample efﬁciency, while both are addressed in our paper. Some works (Zhu
et al., 2019; Vecerik et al., 2017; Rajeswaran et al., 2018; Kim et al., 2013) can also learn from on-
line interactions and ofﬂine data. However, they need expert demonstrations instead of any dataset,
and this limits their applicability."
PRELIMINARIES,0.09405940594059406,"3
PRELIMINARIES"
PRELIMINARIES,0.09900990099009901,"In RL, the interaction between the agent and environment is usually modelled using Markov de-
cision process (MDP) (S, A, pM, r, γ), with state space S (state s ∈S), action space A (action
a ∈A). At each discrete time step, the agent takes an action a based on the current state s, and
the state changes into s′ according to the transition dynamics pM (s′ | s, a), and the agent receives
a reward r (s, a, s′) ∈R. The agent’s objective is to maximize the return, which is deﬁned as
Rt = P∞
i=t+1 γir (si, ai, si+1), where t is the time step, and γ ∈[0, 1) is the discounted factor.
The mapping from s to a is denoted by the stochastic policy π : a ∼π(·|s). Policy can be stochastic
or deterministic, and we use the stochastic from in this paper for generality. Each policy π have a
corresponding action-value function Qπ(s, a) = Eπ [Rt | s, a], which is the expected return follow-
ing the policy after taking action a in state s. The policy π’s action-value function can be updated
by the Bellman operator T π:"
PRELIMINARIES,0.10396039603960396,"T πQ(s, a) = Es′ [r + γQ (s′, π (s′))]
(1)"
PRELIMINARIES,0.10891089108910891,"Q-learning (Sutton & Barto, 2011) directly learns the optimal action-value function Q∗(s, a) =
maxπ Qπ(s, a), and such Q-function can be modelled using neural networks (Mnih et al., 2015)."
PRELIMINARIES,0.11386138613861387,"In principle, off-policy methods, such as Q-learning, can utilize experiences collected by any poli-
cies, and thus they usually maintain a replay buffer B to store and repeatedly learn from experiences
collected by behavior policies (Agarwal et al., 2020). Such capability also enables off-policy meth-
ods to be used in the ofﬂine setting, by storing ofﬂine data into the buffer B, and not updating the
buffer during learning since no further interactions are available here (Levine et al., 2020). But this
simple adjusting cannot guarantee the agent to have a reasonable performance, especially when the
dataset is not diverse (Kumar et al., 2020; Agarwal et al., 2020; Fujimoto et al., 2019), and this is
also the problem tackled in most ofﬂine RL works."
PRELIMINARIES,0.1188118811881188,"In this paper, we focus on the ofﬂine-online setting, where the agent is ﬁrst learned from the ofﬂine
dataset, and then trained via online interactions. And without additional remarks, online RL meth-
ods refer to off-policy algorithms in the rest of this paper. We only use off-policy methods because
they can make more use of ofﬂine data than on-policy ones for gaining high sample efﬁciency, and
on-policy methods are not compatible with our proposed framework introduced next."
PRELIMINARIES,0.12376237623762376,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.12871287128712872,"Figure 2: Learning curves on the D4RL (Fu et al., 2020) task Hopper-medium-replay-v0. The re-
ported results are the averaged performance across ﬁve random seeds and the shaded areas represent
the standard deviation across different seeds. The normalized score of 100 is the average returns of
a domain-speciﬁc expert while normalized score of 0 corresponds to the average returns of an agent
taking actions uniformly at random across the action space."
METHODOLOGY,0.13366336633663367,"4
METHODOLOGY"
METHODOLOGY,0.13861386138613863,"In this section, we ﬁrst give an illustrative example to explain our motivation. After that, we in-
troduce the proposed general framework, namely Adaptive Q-learning, trying to couple online and
ofﬂine RL in an organic way. To implement the proposed framework, then we present the Online-
Ofﬂine Replay Buffer, targeting retaining and distinguishing online and ofﬂine data simultaneously.
And ﬁnally we incorporate State-of-the-Art (SotA) online and ofﬂine RL algorithms into the frame-
work, and introduce the proposed Greedy-Conservative Q-ensemble Learning algorithm in detail."
AN ILLUSTRATIVE EXAMPLE,0.14356435643564355,"4.1
AN ILLUSTRATIVE EXAMPLE"
AN ILLUSTRATIVE EXAMPLE,0.1485148514851485,"We test the SotA ofﬂine RL method conservative Q-learning (CQL) (Kumar et al., 2020) and on-
line RL method Randomized Ensembled Double Q-learning (REDQ) (Chen et al., 2021) under the
ofﬂine-online setting. Here, we choose a widely used locomotion task Hopper (Todorov et al., 2012),
and use the dataset hopper-medium-replay-v0 in the D4RL benchmark (Fu et al., 2020) as the ofﬂine
dataset, which contains diverse experiences from different policies. Speciﬁcally, we ﬁrst pre-train
the agent with the ofﬂine dataset for 100K steps. Then, the agent is ﬁne-tuned online by alternately
conducting the interaction and updating process, where the agent interacts with the environment
for 1K steps, and is updated for 10K steps. Such interleaving process ends until the total online
interaction steps reach 90K."
AN ILLUSTRATIVE EXAMPLE,0.15346534653465346,"The ofﬂine-online setting in this task should be a favourable one for policy learning, because both di-
verse ofﬂine data and online interactions are available here. Therefore, we expect following results:
the initial ofﬂine training using the dataset will provide a relatively good but not perfect starting
point for the agent, and then the agent can be further improved by the online alternating process
since online interaction data can be obtained, and ﬁnally we may acquire a well-performed policy,
even with normalized score close to or better than 100 (i.e., the performance of an expert)."
AN ILLUSTRATIVE EXAMPLE,0.15841584158415842,"Nonetheless, experiment results are shown in Figure 2, and they do not totally meet our expectation.
Speciﬁcally, starting points of two curves show that the ofﬂine algorithm CQL and online algorithm
REDQ can both obtain normalized scores greater than 0 (i.e., the performance of a random policy)
but not very high after the initial ofﬂine training process. This result is what we expected, because
CQL is designed for the ofﬂine setting, and online off-policy methods, such as REDQ, can also learn
from the diverse and large ofﬂine dataset even though it is ﬁxed (Agarwal et al., 2020). However,
although the starting scores (below 30) are far from the expert score and leave a large room for
further promoting, both algorithms have troubles in the following online process. REDQ suffers
from severe instability issue and its performance drops signiﬁcantly during online learning, and in
the end, the policy almost degenerates into a random one. One the other hand, even though the CQL
agent can keep stable during learning, its improvement is very limited."
AN ILLUSTRATIVE EXAMPLE,0.16336633663366337,Under review as a conference paper at ICLR 2022
AN ILLUSTRATIVE EXAMPLE,0.16831683168316833,"These results indicate that pure online RL algorithm may be problematic for effectively handling
ofﬂine data and online interaction data in a single training process, and pure ofﬂine RL algorithm
cannot make good use of valuable online interaction data due to its conservative updating strategy.
Such observation motivates us to couple them in an organic way."
ADAPTIVE Q-LEARNING FRAMEWORK,0.17326732673267325,"4.2
ADAPTIVE Q-LEARNING FRAMEWORK"
ADAPTIVE Q-LEARNING FRAMEWORK,0.1782178217821782,"In this subsection, we introduce our proposed framework. The underlying idea is simple and can
be described as follows. When the agent learns from online and near-on-policy interaction data, we
choose a more greedy or no-pessimistic updating strategy, since these data reﬂect the truth situation
of the current policy. By contrast, when data are sampled from the ofﬂine dataset, we tend to use
a more pessimistic updating strategy. Through such an adaptive way, we can make full use of
both online and ofﬂine data, and explicitly consider their differences by separately applying suitable
updating schemes."
ADAPTIVE Q-LEARNING FRAMEWORK,0.18316831683168316,"Then, we give a formalization for above intuition based on Q-learning, and call this framework
Adaptive Q-learning. The updating function of this framework can be deﬁned by the following
equation:
Qk+1 ←arg min
Qk

A(Qk) + W(s, a)B(Qk)

.
(2)"
ADAPTIVE Q-LEARNING FRAMEWORK,0.18811881188118812,"This function consists of two terms. The ﬁrst term A(Q) stands for the greedy updating strategy,
which is a regular updating function for Q-value in online RL algorithms, e.g., the bellman error.
The second term B(Q) stands for the pessimistic updating strategy, which is the value penalty, e.g.,
the Q-value regularizer (Wu et al., 2019). Besides, a weight function W(s, a) is applied to the
penalty term B(Q). This weight function is based on the sampled data type. Speciﬁcally, when we
use online and near-on-policy interaction data, W(s, a) will be a smaller value, and the updating
relies more on the objective of online RL, leading to a relatively greedy strategy. On the contrary,
when we use ofﬂine data, W(s, a) will be a bigger value, and the updating strategy is relatively
pessimistic."
ADAPTIVE Q-LEARNING FRAMEWORK,0.19306930693069307,"Remark: Our behind intuition can also be easily formalized via the policy learning objective. Such
variant of our framework can be denoted by πk+1 ←arg maxπk

A
 
πk
+ W(s, a)B
 
πk
, where
A(πk) is a objective for the policy in online RL and B(πk) is a policy penalty term. We also provide
a implementation for this variant based on TD3+BC algorithm (Fujimoto & Gu, 2021), and our
framework can largely boost its performance within limited environment steps in most tasks (see
Appendix D for details)."
ADAPTIVE Q-LEARNING FRAMEWORK,0.19801980198019803,"4.3
OORB: ONLINE-OFFLINE REPLAY BUFFER"
ADAPTIVE Q-LEARNING FRAMEWORK,0.20297029702970298,"Next, we introduce a simple but effective online-ofﬂine replay buffer (OORB) to distinguish between
near-on-policy online interaction data, and the ofﬂine data. OORB consists of two replay buffers.
One is the online buffer that collects the online interaction data. Besides, to ensure the data in the
online buffer is near-on-policy, we set it to be very small, and fresh online interaction data are stored
into it by following a ﬁrst-in-ﬁrst-out rule. The other is the ofﬂine buffer consisting of the newly
generated online interaction data and the ofﬂine dataset which may come from any policies."
ADAPTIVE Q-LEARNING FRAMEWORK,0.2079207920792079,"Data are sampled from OORB following a Bernoulli distribution, which means that with a proba-
bility p, they are sampled from the online buffer, and with probability 1 −p, they are sampled from
the ofﬂine buffer. To beneﬁt from both online and ofﬂine data in a balanced way, we empirically set
p to 0.5, and its effect on the ﬁnal performance is further tested via ablation studies in Section 5.4.
Results show that p = 0.5 works best overall, which conﬁrms our claim that ofﬂine data and online
interaction data are both crucial for policy learning."
ADAPTIVE Q-LEARNING FRAMEWORK,0.21287128712871287,"4.4
GCQL: GREEDY-CONSERVATIVE Q-ENSEMBLE LEARNING"
ADAPTIVE Q-LEARNING FRAMEWORK,0.21782178217821782,"We then present a detailed implementation of our proposed framework, by incorporating SotA of-
ﬂine RL algorithm CQL and online RL algorithm REDQ, and we call our implemented algorithm
Greedy Conservative Q-ensemble Learning (GCQL). Speciﬁcally, we use the updating function in
REDQ as the ﬁrst term A(Q) in Equation 2, and use the conservative regularizer in CQL as the"
ADAPTIVE Q-LEARNING FRAMEWORK,0.22277227722772278,Under review as a conference paper at ICLR 2022
ADAPTIVE Q-LEARNING FRAMEWORK,0.22772277227722773,Algorithm 1: Greedy-conservative Q-ensemble learning
ADAPTIVE Q-LEARNING FRAMEWORK,0.23267326732673269,1 Initialization:
ADAPTIVE Q-LEARNING FRAMEWORK,0.2376237623762376,"2 Initialize policy πφ, ensemble Q functions Qθi, i ∈N,ofﬂine training steps t"
ADAPTIVE Q-LEARNING FRAMEWORK,0.24257425742574257,"3 Online exploration steps Ton, ofﬂine update steps Toff"
ADAPTIVE Q-LEARNING FRAMEWORK,0.24752475247524752,"4 Initialize Tinitial, sample possibility p, start sampling steps:Ts"
ADAPTIVE Q-LEARNING FRAMEWORK,0.2524752475247525,"5 Initialize online buffer Bon to empty, ofﬂine buffer Boff ←ofﬂine dataset"
ADAPTIVE Q-LEARNING FRAMEWORK,0.25742574257425743,6 Initialize online buffer size Son ←0
ADAPTIVE Q-LEARNING FRAMEWORK,0.2623762376237624,7 Initial ofﬂine learning:
TRAIN THE AGENT FOR TINITIAL STEPS USING THE LOGGED DATASET,0.26732673267326734,8 Train the agent for Tinitial steps using the logged dataset
WHILE TRUE DO,0.2722772277227723,9 while True do
WHILE TRUE DO,0.27722772277227725,"10
t ←0"
EXPLORE TON STEPS ONLINE,0.28217821782178215,"11
Explore Ton steps online"
EXPLORE TON STEPS ONLINE,0.2871287128712871,"12
Store the Ton steps experiences to both online buffer Bon and ofﬂine buffer Boff"
EXPLORE TON STEPS ONLINE,0.29207920792079206,"13
Son ←Son + Ton"
EXPLORE TON STEPS ONLINE,0.297029702970297,"14
for t < Toff do"
EXPLORE TON STEPS ONLINE,0.30198019801980197,"15
Sample a random value ps ∼U(0, 1)"
EXPLORE TON STEPS ONLINE,0.3069306930693069,"16
if ps < p and Son > Ts then"
EXPLORE TON STEPS ONLINE,0.3118811881188119,"17
Sample a batch (s, a) from online buffer Bon"
EXPLORE TON STEPS ONLINE,0.31683168316831684,"18
Set the W(s, a) to 0"
END,0.3217821782178218,"19
end"
ELSE,0.32673267326732675,"20
else"
ELSE,0.3316831683168317,"21
Sample a batch (s, a) from ofﬂine buffer Boff"
ELSE,0.33663366336633666,"22
Set the W(s, a) to 1"
END,0.3415841584158416,"23
end"
END,0.3465346534653465,"24
Update the Q functions by Equation 3"
END,0.35148514851485146,"25
Update the policy by Equation 5"
END,0.3564356435643564,"26
t += 1"
END,0.3613861386138614,"27
end"
END,0.36633663366336633,28 end
END,0.3712871287128713,"second term B(Q), and the updating function can be presented by the following equation:"
END,0.37623762376237624,"Qk+1
i
←arg min
Qk
i 1"
END,0.3811881188118812,"2Es,a,s′∼DOORB,a′∼πφ(·|s′)"
END,0.38613861386138615,"
Qk
i (s, a) −ˆBπ ˆQk(s′, a′)
2"
END,0.3910891089108911,"+ W(s, a)αEs∼DOORB "" log
X"
END,0.39603960396039606,"˙a
exp(Q(s, ˙a)) −Ea∼DOORB[Q(s, a)]"
END,0.400990099009901,"#)
(3)"
END,0.40594059405940597,"where the action ˙a is sampled from current policy, i.e., ˙a ∼πφ(·|s), s ∼DOORB and DOORB is the
OORB replay buffer. ˆBπ ˆQk(s, a) is deﬁned by"
END,0.41089108910891087,"r + γ min
i∈M
ˆQk
i (s′, a′) ,
a′ ∼πφ (· | s′) .
(4)"
END,0.4158415841584158,"We randomly select two Q functions from the ensemble Q functions and the M represents the
selected Q functions’ index just following the REDQ’s setting. The ˆQ stands for a target Q function
for stabilizing the learning process (Mnih et al., 2015). The update function of policy is deﬁned by:"
END,0.4207920792079208,"πφk+1 ←arg max
πφk
E [Ei∈N [Qi (s, a)] −α log πφk (a | s)] ,
a ∼πφk (· | s)
(5)"
END,0.42574257425742573,"When sampled data is from the online replay buffer, we set W(s, a) to 0, otherwise 1, and this is
why we use the term ”greedy-conservative” to describe our algorithm. In another word, we greedy
exploit the near-on-policy online data by the regular online RL scheme without any conservative
regularizer. On the contrary, we conservatively exploit the ofﬂine data by employing the ofﬂine RL"
END,0.4306930693069307,Under review as a conference paper at ICLR 2022
END,0.43564356435643564,"Environment
GCQL (Ours)
CQL
REDQ
AWAC"
END,0.4405940594059406,"walker2d-random
53±27
16±9
19±3
12
hopper-random
84±40
11±1
12±17
63
halfcheetah-random
100±2
46±4
34±1
53"
END,0.44554455445544555,"walker2d-medium
94±6
83±1
5±3
80
hopper-medium
105±1
100±1
3±1
91
halfcheetah-medium
66±3
42±0
46±1
41"
END,0.4504950495049505,"walker2d-expert
117±2
113±1
6±1
103
hopper-expert
114±1
113±1
12±6
112
halfcheetah-expert
110±0
108±1
1±0
106"
END,0.45544554455445546,"walker2d-medium-expert
117±4
113±1
12±3
78
hopper-medium-expert
115±1
114±0
40±15
112
halfcheetah-medium-expert
107±1
95±2
9±3
41"
END,0.4603960396039604,"walker2d-medium-replay
114±6
64±5
53±11
-
hopper-medium-replay
96±9
39±4
38±9
-
halfcheetah-medium-replay
59±2
50±0
50±1
-"
END,0.46534653465346537,"Table 1: Performance of policies trained using 90K online interaction steps for GCQL, CQL and
REDQ, and 500K online interaction for AWAC whose results are taken from Nair et al. (2020).±
captures the standard deviation over seeds. The reported results are the average test performance
across ﬁve random seeds in our experiments. The learning curves are showed in Appendix A."
END,0.47029702970297027,"regularizer. Formally, this strategy can be explained by Deﬁnition 6:"
END,0.4752475247524752,"W(s, a) ←
0
if (s, a) is sampled from the online replay buffer
1
otherwise
(6)"
END,0.4801980198019802,"Algorithm 1 summarizes our proposed method. And we also explain the main steps in the Algorithm
as follows. Firstly, We ﬁrst learn from the existing ofﬂine data for Tinitial steps to leverage them. To
make good use of the ofﬂine data, we usually set a big value for Tinitial, e.g., 100K. Secondly, We
begin the following interleaving learning steps. We conduct the online exploration process for Ton
steps and store the new experiences to OORB. We set the online exploration steps Ton to a small
value, e.g., 1K. In contrast, the ofﬂine update step Toff is set to be larger than Ton, e.g., 10K.
We sample a batch from our OORB and update the policy and Q-functions. If the sampled batch
comes from the online buffer, then we set the weight value W(s, a) to 0, otherwise 1. The above
interleaving learning process is repeated till the end."
EXPERIMENTS,0.48514851485148514,"5
EXPERIMENTS"
EXPERIMENTS,0.4900990099009901,"In this section, we design experiments to verify the effectiveness of our method from three perspec-
tives: (1) the performance superiority compared with other baselines; (2) ablation studies to test the
effect of each component used in our method. (3) the inﬂuence of different hyper-parameters."
SETTINGS,0.49504950495049505,"5.1
SETTINGS"
SETTINGS,0.5,"All experiments were done on the continuous control benchmark MuJoCo (Todorov et al., 2012),
and the ofﬂine dataset comes from the popular ofﬂine RL benchmark D4RL (Fu et al., 2020). To
make interaction limited, we set the number of online interaction steps for each iteration to a small
value, i.e., 1K. To better exploit the ofﬂine dataset, we set the number of ofﬂine training steps to a
large value, i.e., 100K for Tintial, and 10K for Toff. The training process ends until the number
of all online interaction steps reach 90K, and the number of all ofﬂine updating steps reach 1M.
For OORB, we set p = 0.5 as described in Section 4.3. The size of the online buffer is set to 20K,
and the size of the ofﬂine buffer is set to 3M. For other hyper-parameters, we follow the default
setting in baselines, except for the number of the ensemble Q, which is 5 in our experiments. The
above conﬁgurations keep same across all tasks. As our main purpose is to present a new framework
instead of gaining SotA performance, we do not ﬁne-tune these hyper-parameters for each task."
SETTINGS,0.504950495049505,Under review as a conference paper at ICLR 2022
SETTINGS,0.5099009900990099,"(a) walker2d-expert
(b) walker2d-medium-expert
(c) walker2d-medium-replay"
SETTINGS,0.5148514851485149,"(d) walker2d-medium
(e) walker2d-random"
SETTINGS,0.5198019801980198,Figure 3: Ablation study on Walker2d task.
OVERALL PERFORMANCE,0.5247524752475248,"5.2
OVERALL PERFORMANCE"
OVERALL PERFORMANCE,0.5297029702970297,"4 methods, i.e., GCQL (Ours), CQL, REDQ and AWAC, are tested on 3 tasks, i.e., Walker, Hop-
per and HalfCheetah, and each task has 5 different kinds of ofﬂine dataset, which are random-v0,
medium-v0, expert-v0, medium-expert-v0, and medium-replay-v0. As online interactions are avail-
able, we take the maximization over testing scores during the whole training process, and report
the average of these max scores across ﬁve different seeds in Table 1. Please note that the results of
AWAC are directly taken from their paper (Nair et al., 2020). As shown in Table 1, our method gains
a better performance than the baselines in all tasks. Particularly, for the medium-replay and random
dataset, our GCQL outperforms the baselines by a large margin. By contrast, CQL also achieves
an expert performance on the high-quality dataset, e.g., expert and medium-expert tasks. For these
high-quality datasets, our method achieves a comparable or slightly better performance over CQL. In
terms of the online RL method REDQ, it fails on almost all tasks. Even though we employ an online
limited exploration process, conventional online RL cannot beneﬁt from such limited experiences.
On the contrary, such limited online experiences could bring catastrophic consequences, e.g., on the
halfcheetah-medium-replay task, REDQ suffers a signiﬁcant instability issue. On the other hand,
when dataset quality is high, e.g., expert or medium-expert dataset, our method can leverage the
beneﬁt of the ofﬂine RL that gains an expert performance with very limited interaction steps, such
as around 20K steps. By contrast, when dataset quality is poor, e.g., the random or medium-replay
dataset, only our method can learn effectively with less than 100K online steps, which demonstrate
that our method can take the advantage of the limited online experiences as much as possible. In
sum, our method can beneﬁt the most from the limited online experience while still maintain the
ofﬂine learning’s ability. Besides, We also include the corresponding learning curves in Appendix
A for a comprehensive understanding. These learning curves demonstrate that our method also
achieves a better ﬁnal average score at the last iteration in most cases."
ABLATION STUDIES,0.5346534653465347,"5.3
ABLATION STUDIES"
ABLATION STUDIES,0.5396039603960396,"To investigate each component’s effect in our method, we conduct the following ablation studies.
To do this, we design four variants of our method. GCQL WE: GCQL without the ensemble where
we only use two Q functions same as the setting of CQL. GCQL WO: GCQL without the online
replay buffer where we only sample from the ofﬂine buffer. GCQL WG: GCQL without the greedy"
ABLATION STUDIES,0.5445544554455446,Under review as a conference paper at ICLR 2022
ABLATION STUDIES,0.5495049504950495,"strategy where we ﬁx the weighed W to 1. GCQL WC: GCQL without the conservative term where
we ﬁx the W to 0."
ABLATION STUDIES,0.5544554455445545,"We test all 5 kinds of ofﬂine dataset for the task Walker2d, and results averaged over three differ-
ent random seeds are shown in Figure 3. First, from Figure 3, it is easy to deduce that when the
quality of data is high, i.e., including the expert dataset, almost all variants perform well except the
GCQL WC. The underlying reason is obvious as we ﬁrstly pre-train the policy by ofﬂine data. If
no such conservative restriction, then the pre-trained policy would suffer a serious distribution mis-
match issue (Kumar et al. (2020)) and gain a poor pre-trained policy at last. By contrast, with the
conservative scheme, a relatively good pre-trained policy can be obtained. That is why the starting
point of GCQL WC is much lower than other variants."
ABLATION STUDIES,0.5594059405940595,"Secondly, when the dataset is diverse, which means that the dataset is collected by different behavior
policies and includes data from different distributions, such as the medium-replay dataset, the greedy
scheme (including the ofﬂine-online replay buffer and the greedy update strategy) plays an important
role while the ensemble feature seems have limited inﬂuence. For instance, the performance declined
signiﬁcantly for GCQL WO and GCQL WG on walker2d-medium-replay."
ABLATION STUDIES,0.5643564356435643,"Thirdly, when the dataset is not diverse, such as the medium dataset which is collected by one
medium policy, almost all curves grow slowly except GCQL WC. This may be caused by the charac-
teristic of the dataset, because dataset diversity plays an important role for policy learning (Agarwal
et al., 2020). However, according to the learning curves, our method still has a clear performance
improvement at the latter learning stage. For the random dataset, all variants fail due to the poor
quality of the dataset. Instead, our method achieves a clear performance improvement under it,
which indicates that every component is important in this case."
ANALYSIS ON HYPER-PARAMETERS,0.5693069306930693,"5.4
ANALYSIS ON HYPER-PARAMETERS"
ANALYSIS ON HYPER-PARAMETERS,0.5742574257425742,"As we do not ﬁne-tune the hyper-parameters in our experiments, one may wonder how the hyper-
parameters affect the performance. To this end, we conduct experiments to investigate their inﬂu-
ence. The detailed learning curves are shown in Appendix B. As one main characteristic of our
method is limited interaction, we ﬁx the online exploration step as 1K for each iteration in the Mu-
JoCo benchmark. We then try settings with different initial update steps Tinitial, ofﬂine update steps
Toff, and sample possibility from online buffer p. Speciﬁcally, Tinitial is tested with 2e5 and 5e4;
Toff is tested with 2e4 and 5e3; p is tested with 0.3, 0.4, 0.5, 0.6 and 0.7."
ANALYSIS ON HYPER-PARAMETERS,0.5792079207920792,"According to the evaluation results, we may conclude that the performance of our method is insen-
sitive to the Tinitial and Toff, especially for datasets with not poor quality, e.g., datasets except
for the random one. By contrast, p has a bigger impact on the performance. Particularly, methods
with higher p performs better on the medium and medium-replay tasks, while the ones with lower p
perform better on the other tasks. That indicates that when dataset quality is very good or very bad,
methods with more ofﬂine updating perform better. On the other hand, only the variant of p = 0.5
can achieve a clear performance improvement in both the medium and random datasets. Overall, the
default setting p = 0.5 is the most suitable setting, which indicates that taking the online and ofﬂine
data equally important may be the best option in most cases."
CONCLUSIONS,0.5841584158415841,"6
CONCLUSIONS"
CONCLUSIONS,0.5891089108910891,"This paper ﬁrst discusses the disadvantages of online and ofﬂine RL, and the shortcomings of current
ofﬂine-online RL methods. Then, considering that ofﬂine and near-on-policy online datasets are both
crucial for policy learning, we propose a uniﬁed framework that can adaptively and effectively take
advantage of both ofﬂine and online data. Furthermore, a practical algorithm based on the framework
that greedily exploits the online experiences and conservatively exploits the ofﬂine experiences is
presented. We conduct comprehensive experiments to verify the effectiveness of our method. In
terms of the shortcomings, although our framework can take advantage of the ofﬂine and limited
online dataset, the quality of the ofﬂine dataset still has a big impact on the performance. For
instance, our method is relatively slower when learning from the random dataset. At last, we hope
this work could contribute to bridging the gap between the practice and DRL research."
CONCLUSIONS,0.594059405940594,Under review as a conference paper at ICLR 2022
REFERENCES,0.599009900990099,REFERENCES
REFERENCES,0.6039603960396039,"Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on ofﬂine
reinforcement learning. In ICML, 2020."
REFERENCES,0.6089108910891089,"Xinyue Chen, Che Wang, Zijian Zhou, and Keith W. Ross.
Randomized ensembled double q-
learning: Learning fast without a model. ICLR, abs/2101.05982, 2021."
REFERENCES,0.6138613861386139,"Christoph Dann, Gerhard Neumann, Jan Peters, et al. Policy evaluation with temporal differences:
A survey and comparison. Journal of Machine Learning Research, 2014."
REFERENCES,0.6188118811881188,"Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020."
REFERENCES,0.6237623762376238,"Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to ofﬂine reinforcement learning,
2021."
REFERENCES,0.6287128712871287,"Scott Fujimoto, Herke van Hoof, and Dave Meger. Addressing function approximation error in
actor-critic methods. In International Conference on Machine Learning, 2018."
REFERENCES,0.6336633663366337,"Scott Fujimoto, D. Meger, and Doina Precup. Off-policy deep reinforcement learning without ex-
ploration. In ICML, 2019."
REFERENCES,0.6386138613861386,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
Soft actor-critic:
Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018."
REFERENCES,0.6435643564356436,"Qiang He and X. Hou. Popo: Pessimistic ofﬂine policy optimization. ArXiv, abs/2012.13682, 2020."
REFERENCES,0.6485148514851485,"Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah
Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of
implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019."
REFERENCES,0.6534653465346535,"R. Kidambi, A. Rajeswaran, Praneeth Netrapalli, and T. Joachims. Morel : Model-based ofﬂine
reinforcement learning. NeurIPS, abs/2005.05951, 2020."
REFERENCES,0.6584158415841584,"Beomjoon Kim, Amir massoud Farahmand, Joelle Pineau, and Doina Precup. Learning from limited
demonstrations. In NeurIPS, 2013."
REFERENCES,0.6633663366336634,"Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. In Advances in Neural Information Processing Sys-
tems, pp. 11761–11771, 2019."
REFERENCES,0.6683168316831684,"Aviral Kumar, Aurick Zhou, G. Tucker, and Sergey Levine. Conservative q-learning for ofﬂine
reinforcement learning. NeurIPS, 2020."
REFERENCES,0.6732673267326733,"Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin.
Ofﬂine-to-
online reinforcement learning via balanced replay and pessimistic q-ensemble. arXiv preprint
arXiv:2107.00591, 2021."
REFERENCES,0.6782178217821783,"Sergey Levine, Aviral Kumar, G. Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tutorial,
review, and perspectives on open problems. ArXiv, abs/2005.01643, 2020."
REFERENCES,0.6831683168316832,"Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic. Ofﬂine policy
evaluation across representations with applications to educational games. In AAMAS, volume
1077, 2014."
REFERENCES,0.6881188118811881,"Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Oﬁr Nachum, and Shixiang Gu. Deployment-
efﬁcient reinforcement learning via model-based ofﬂine optimization. ICLR, 2021."
REFERENCES,0.693069306930693,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529–533, 2015."
REFERENCES,0.698019801980198,Under review as a conference paper at ICLR 2022
REFERENCES,0.7029702970297029,"Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with ofﬂine datasets. CoRR, abs/2006.09359, 2020."
REFERENCES,0.7079207920792079,"Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel
Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement
learning and demonstrations. Robotics: Science and Systems, 2018."
REFERENCES,0.7128712871287128,"John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889–1897. PMLR,
2015."
REFERENCES,0.7178217821782178,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.7227722772277227,Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 2011.
REFERENCES,0.7277227722772277,"Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033.
IEEE, 2012."
REFERENCES,0.7326732673267327,"Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nico-
las Heess, Thomas Roth¨orl, Thomas Lampe, and Martin Riedmiller.
Leveraging demonstra-
tions for deep reinforcement learning on robotics problems with sparse rewards. arXiv preprint
arXiv:1707.08817, 2017."
REFERENCES,0.7376237623762376,"Y. Wu, G. Tucker, and Oﬁr Nachum. Behavior regularized ofﬂine reinforcement learning. ArXiv,
abs/1911.11361, 2019."
REFERENCES,0.7425742574257426,"Tianhe Yu, G. Thomas, Lantao Yu, S. Ermon, J. Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma.
Mopo: Model-based ofﬂine policy optimization. NeurIPS, abs/2005.13239, 2020."
REFERENCES,0.7475247524752475,"Henry Zhu, Abhishek Gupta, Aravind Rajeswaran, Sergey Levine, and Vikash Kumar. Dexterous
manipulation with deep reinforcement learning: Efﬁcient, general, and low-cost. In 2019 Inter-
national Conference on Robotics and Automation (ICRA), pp. 3651–3657. IEEE, 2019."
REFERENCES,0.7524752475247525,"A
LEARNING CURVES FOR ALL TASKS"
REFERENCES,0.7574257425742574,"Figure 4 indicate the whole learning curves for GCQL, REDQ and CQL."
REFERENCES,0.7623762376237624,"B
EXTRA EXPERIMENTS ON HYPER-PARAMETERS"
REFERENCES,0.7673267326732673,Figure 5 and 6 shows the different hyper-parameters’ setting on the performance.
REFERENCES,0.7722772277227723,"C
EXTRA ABLATION STUDY ON HALFCHEETAH"
REFERENCES,0.7772277227722773,"Figure 7 present extra ablation study on halfcheetah. GCQL WG is the GCQL without the greedy
update scheme but still with the online-ofﬂine two-level buffer, while GCQL WGO is the GCQL
without the greedy scheme and online-ofﬂine replay buffer. From Figure 7 c,d,e, it is clear that the
greedy scheme plays an important role in boosting the performance when the dataset is not optimal
or near-optimal. On the other hand, from Figure 7 d, we can see the online-ofﬂine replay buffer is
crucial for stabilizing the learning process where the GCQL WGO and CQL suffer serious stability
issues which may be caused by extrapolation error (Fujimoto et al., 2019)."
REFERENCES,0.7821782178217822,Under review as a conference paper at ICLR 2022
REFERENCES,0.7871287128712872,"(a) walker2d-random
(b) hopper-random
(c) halfcheetah-random"
REFERENCES,0.7920792079207921,"(d) walker2d-medium
(e) hopper-medium
(f) halfcheetah-medium"
REFERENCES,0.7970297029702971,"(g) walker2d-expert
(h) hopper-expert
(i) halfcheetah-expert"
REFERENCES,0.801980198019802,"(j) walker2d-medium-expert
(k) hopper-medium-expert
(l) halfcheetah-medium-expert"
REFERENCES,0.806930693069307,"(m) walker2d-medium-replay
(n) hopper-medium-replay
(o) halfcheetah-medium-replay"
REFERENCES,0.8118811881188119,"Figure 4: Training curves on D4RL continuous control benchmark across ﬁve random seeds. The
shaded areas represent the standard deviation across different seeds."
REFERENCES,0.8168316831683168,Under review as a conference paper at ICLR 2022
REFERENCES,0.8217821782178217,"(a) walker2d-expert
(b) walker2d-medium-expert
(c) walker2d-medium-replay"
REFERENCES,0.8267326732673267,"(d) walker2d-medium
(e) walker2d-random"
REFERENCES,0.8316831683168316,"Figure 5: Different ofﬂine update steps on walker2d task across three random seeds.GCQL-
i2e5:initial steps is 2e5, GCQL-i5e4: initial stesp is 5e4, GCQL-o2e4: ofﬂine update steps is 2e4,
GCQL-o5e3: ofﬂine update steps is 5e3."
REFERENCES,0.8366336633663366,"(a) walker2d-expert
(b) walker2d-medium-expert
(c) walker2d-medium-replay"
REFERENCES,0.8415841584158416,"(d) walker2d-medium
(e) walker2d-random"
REFERENCES,0.8465346534653465,"Figure 6: Different possibility setting on walker2d task across three random seeds.GCQL-X means
the sample possibility from online buffer is X."
REFERENCES,0.8514851485148515,"D
EXTRA EXPERIMENTS ON TD3+BC"
REFERENCES,0.8564356435643564,"Extra experiments on TD3+BC (Fujimoto & Gu, 2021) (without states normalization) for the sub-
optimal dataset, i.e., random, medium, and medium-replay, as the ofﬂine RL method can per-"
REFERENCES,0.8613861386138614,Under review as a conference paper at ICLR 2022
REFERENCES,0.8663366336633663,"(a) halfcheetah-expert
(b) halfcheetah-medium-expert
(c) halfcheetah-medium-replay"
REFERENCES,0.8712871287128713,"(d) halfcheetah-medium
(e) halfcheetah-random"
REFERENCES,0.8762376237623762,Figure 7: Extra ablation study on Halfcheetah.
REFERENCES,0.8811881188118812,"form very well on expert or near-on-expert datasets. We simply apply our greedy-conservative
learning framework and online-ofﬂine buffer to TD3+B without any ﬁne-tuning and call it:
TD3BCGC. The learning curves indicate that our greedy-conservative framework can still im-
prove policy-penalty-based methods by a large margin in most tasks.
Moreover, it is very
simple to apply our framework to TD3+BC where less than 20 lines codes are needed.
We
modify the policy objective from π = argmaxπE(s,a)∼D

λQ(s, π(s)) −(π(s) −a)2
to π =
argmaxπE(s,a)∼D

λQ(s, π(s)) −W(s, a)(π(s) −a)2
, where W(s, a) follows our setting intro-
duced in section 4.4."
REFERENCES,0.8861386138613861,"E
EXTRA EXPERIMENTS ON OFF2ON"
REFERENCES,0.8910891089108911,"Here, we compared our method with OFF2ON (Lee et al., 2021) which also employs an online
update scheme to ﬁne-tune a pre-trained agent. As the author did not provide the pre-training code
for CQL, we use the pre-trained CQL agents provided by OFF2ON’s author for online ﬁne-tuning.
All the hyper-parameters follow the default setting except the online interaction steps. Figure 9
indicated the whole learning curves. From this ﬁgure, we can see GCQL has a comparable or better
performance than OFF2ON. It is worth noting that OFF2ON ﬁne-tuning their method, for instance,
the critic’s neural network architecture is different from the original CQL paper. In contrast, we did
not ﬁne-tune these parameters, and all settings are the same for all tasks."
REFERENCES,0.8960396039603961,"F
EXTRA EXPERIMENTS ON OFFICIAL ONLINE REDQ"
REFERENCES,0.900990099009901,"In this section, we conduct extra experiments on the online REDQ (Chen et al., 2021). To guarantee
the reproduction performance, we use the ofﬁcial code from the author, and all hyper-parameters
are following its default setting, e.g., the number of Q is 10, and the utd-ratio is 20. In contrast,
the number of Q in GCQL is 5, and utd-ratio is 10. REDQ-ONLINE updates its policy following
a conventional online RL setting, i.e., learning from scratch without the ofﬂine pre-training. From
Figure 10, it is clear that GCQL achieves a better or comparable performance than REDQ-ONLINE
except for the walker2d-random dataset. Speciﬁcally, when the dataset is good, e.g., including the
expert dataset, GCQL can outperform the REDQ-ONLINE by a large margin. On the other hand,"
REFERENCES,0.905940594059406,Under review as a conference paper at ICLR 2022
REFERENCES,0.9108910891089109,"(a) walker2d-random
(b) hopper-random
(c) halfcheetah-random"
REFERENCES,0.9158415841584159,"(d) walker2d-medium
(e) hopper-medium
(f) halfcheetah-medium"
REFERENCES,0.9207920792079208,"(g) walker2d-medium-replay
(h) hopper-medium-replay
(i) halfcheetah-medium-replay"
REFERENCES,0.9257425742574258,"Figure 8: Training curves for TD3+BC on D4RL continuous control benchmark across three ran-
dom seeds on tasks: random-v0, medium-v0 and medium-replay-v0. TD3BC means the algorithm
introduced by (Fujimoto & Gu, 2021) while TD3BCGC is the variant with our greedy-conservative
framework and online-buffer replay buffer."
REFERENCES,0.9306930693069307,"when the ofﬂine dataset is of poor quality, i.e., random-dataset, GCQL can still learn a comparable
or better policy than REDQ-ONLINE in two of three tasks, i.e., the hopper-random and halfcheetah-
random."
REFERENCES,0.9356435643564357,Under review as a conference paper at ICLR 2022
REFERENCES,0.9405940594059405,"(a) walker2d-random
(b) hopper-random
(c) halfcheetah-random"
REFERENCES,0.9455445544554455,"(d) walker2d-medium
(e) hopper-medium
(f) halfcheetah-medium"
REFERENCES,0.9504950495049505,"(g) walker2d-medium-replay
(h) hopper-medium-replay
(i) halfcheetah-medium-replay"
REFERENCES,0.9554455445544554,"(j) walker2d-medium-expert
(k) hopper-medium-expert
(l) halfcheetah-medium-expert"
REFERENCES,0.9603960396039604,"Figure 9: Training curves for OFF2ON (Lee et al., 2021) on D4RL continuous control benchmark
across four random seeds on tasks: random-v0, medium-v0,medium-expert-v0 and medium-replay-
v0."
REFERENCES,0.9653465346534653,Under review as a conference paper at ICLR 2022
REFERENCES,0.9702970297029703,"(a) walker2d-random
(b) hopper-random
(c) halfcheetah-random"
REFERENCES,0.9752475247524752,"(d) walker2d-medium
(e) hopper-medium
(f) halfcheetah-medium"
REFERENCES,0.9801980198019802,"(g) walker2d-expert
(h) hopper-expert
(i) halfcheetah-expert"
REFERENCES,0.9851485148514851,"(j) walker2d-medium-expert
(k) hopper-medium-expert
(l) halfcheetah-medium-expert"
REFERENCES,0.9900990099009901,"(m) walker2d-medium-replay
(n) hopper-medium-replay
(o) halfcheetah-medium-replay"
REFERENCES,0.995049504950495,"Figure 10: REDQ-ONLINE learn from scratch without the ofﬂine pre-training while GCQL and
CQL learn from both ofﬂine dataset and online interaction."
