Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0025380710659898475,"We propose a new perspective on designing powerful Graph Neural Networks
(GNNs). In a nutshell, this enables a general solution to inject structural properties
of graphs into a message-passing aggregation scheme of GNNs. As a theoretical
basis, we develop a new hierarchy of local isomorphism on neighborhood sub-
graphs. Then, we theoretically characterize how message-passing GNNs can be
designed to be more expressive than the Weisfeiler Lehman test. To elaborate
this characterization, we propose a novel neural model, called GraphSNN, and
prove that this model is strictly more expressive than the Weisfeiler Lehman test in
distinguishing graph structures. We empirically verify the strength of our model on
different graph learning tasks. It is shown that our model consistently improves the
state-of-the-art methods on the benchmark tasks without sacriﬁcing computational
simplicity and efﬁciency."
INTRODUCTION,0.005076142131979695,"1
INTRODUCTION"
INTRODUCTION,0.007614213197969543,"Many Graph Neural Networks (GNNs) employ a message-passing aggregation scheme to learn
low-dimensional vector space representations for nodes in a graph (Kipf & Welling, 2017; Veliˇckovi´c
et al., 2017; Hamilton et al., 2017; Gilmer et al., 2017; Sato, 2020; Loukas, 2020; de Haan et al.,
2020). Let G = (V, E) be a graph. For each node v ∈V , a message-passing aggregation scheme
recursively aggregates the feature vectors of nodes in the neighborhood of v and combines the
aggregated information with the feature vector of v itself to obtain a representation. Since there is
no natural ordering on nodes, such message-passing aggregation schemes are usually required to be
permutation-invariant (Maron et al., 2018; Keriven & Peyré, 2019; Garg et al., 2020)."
INTRODUCTION,0.01015228426395939,"Despite advances of GNNs in various graph learning tasks such as node classiﬁcation (Kipf &
Welling, 2017; Xu et al., 2018), graph classiﬁcation (Xu et al., 2019; Wu et al., 2019) and link
prediction (Zhang & Chen, 2017), there is still a lack of theoretical understanding of how to design
powerful and practically useful GNNs that can capture rich structural information of graphs. Recent
studies (Xu et al., 2019; Morris et al., 2019) have explored the connections between GNNs and the
Weisfeiler-Lehman (WL) test (Weisfeiler & Leman, 1968). By representing a neighborhood as a
multiset of feature vectors and treating the neighborhood aggregation as an aggregation function
over multisets, Xu et al. (2019) showed that message-passing GNNs are at most as powerful as the
WL test in distinguishing graph structures. However, many simple graph structures still cannot be
distinguished by the WL test, e.g., G1 and G2 shown in Figure 1. A question is: how to design
expressive yet simple GNNs that can go beyond the WL test with a theoretically provable guarantee?"
INTRODUCTION,0.012690355329949238,"Recently, there have been three main directions of extending GNNs beyond WL: (1) building GNNs
for higher-order WL (i.e. k-WL with k ≥3) or variants (Maron et al., 2019; Morris et al., 2020;
2019); (2) counting on pre-deﬁned substructures as additional features (Bouritsas et al., 2020); (3)
augmenting node identiﬁers or random features into GNNs (You et al., 2021; Vignac et al., 2020;
Sato et al., 2021). Unlike these works, we aim to introduce a general solution upon which GNNs can
be enhanced to capture structural properties of graphs. This solution enables GNNs to provably be
more expressive than the Weisfeiler-Lehman test, but still computationally efﬁcient. It overcomes
the following limitations of existing works. Compared with higher-order WL methods in (1) which
require high computational overhead and are impractical, our method goes beyond the WL test but
is still computationally efﬁcient. Compared with the methods on counting substructures in (2), our"
INTRODUCTION,0.015228426395939087,Published as a conference paper at ICLR 2022
INTRODUCTION,0.017766497461928935,"Graphs
Neighborhood
Overlap
Subgraphs
Subgraphs 1
1 1
1
u u v
v1 u1 u1 v2 u2 u2 v3 u3 u3 v4 u4 u4
G2 G1"
INTRODUCTION,0.02030456852791878,"1.5
v1 1.5 v4 1.67 v"
INTRODUCTION,0.02284263959390863,"v2
1.67 v3"
INTRODUCTION,0.025380710659898477,GMP Framework
INTRODUCTION,0.027918781725888325,AGGREGATEN(·)
INTRODUCTION,0.030456852791878174,AGGREGATEI(·)
INTRODUCTION,0.03299492385786802,COMBINE(·)
INTRODUCTION,0.03553299492385787,"Embedding
Space hu hv"
INTRODUCTION,0.03807106598984772,"Figure 1: An overview of our proposed framework for GNNs that can go beyond the WL test
in distinguishing non-isomorphic graphs G1 and G2. The overlap subgraphs of G1 and G2 are
structurally different, which are captured by structural coefﬁcients deﬁned in Eq. 4."
INTRODUCTION,0.04060913705583756,"method does not require to handcraft substructures. Compared with the methods of augmenting node
identiﬁers or random features in (3), our method can ﬂexibly quantify local structures (see examples
in Figure 3) and also capture different classes of local structures w.r.t. different graph learning tasks."
INTRODUCTION,0.04314720812182741,"Our work is grounded in three observations: (i) Treating a neighborhood as a multiset of feature
vectors ignores the rich structure information among vertices in the neighborhood, thereby limiting
the representational capacity of the model. Thus, we represent a neighborhood as a neighborhood
subgraph in which vertices are structurally related, and show that the WL test is only as powerful
as distinguishing neighborhood subgraphs in terms of their subtree structures in the neighborhood.
(ii) There exists a natural class of isomorphic graphs, which strictly lies in between neighborhood
subgraph isomorphism and neighborhood subtree isomorphism. We call it overlap (subgraph)
isomorphism. The notion of overlap subgraph enables us to characterize structural interactions of
vertices and inject them into a message-passing aggregation scheme for GNNs. (iii) By designing a
proper function for quantifying structural interactions of vertices and preserving the injectiveness of
a message-passing aggregation scheme, more expressive GNNs can be developed. We propose a new
GNN model that is strictly more expressive than the WL test to demonstrate an instance of this kind."
INTRODUCTION,0.04568527918781726,"Contributions. In summary, the main contributions of this work are as follows:"
INTRODUCTION,0.048223350253807105,"• We introduce a new hierarchy of local isomorphism to characterise different classes of local
structures in neighborhood subgraphs, and discuss its connections with the WL test and
GNNs (Section 2 and Theorems 1-2).
• We develop a simple yet powerful framework to inject structural properties into a message-
passing aggregation scheme, and theoretically characterize how GNNs can be designed to
be more expressive beyond the WL test (Section 3 and Theorem 3).
• We propose a novel neural model for graph learning, called GraphSNN, and prove that
GraphSNN is strictly more expressive than the the WL test in distinguishing graph structures
(Section 4 and Theorem 4).
• We show that, due to the way of injecting structural properties into a structured-message-
passing aggregation scheme, GraphSNN can overcome the oversmoothing issue (Chen et al.,
2020a; Zhao & Akoglu, 2019; Li et al., 2018) (Section 5.4)."
INTRODUCTION,0.050761421319796954,"We have conducted experiments on benchmark tasks (Hu et al., 2020). The experimental results show
that our model is highly efﬁcient and can signiﬁcantly improve the state-of-the-art methods without
sacriﬁcing computational simplicity."
INTRODUCTION,0.0532994923857868,"Related work.
Weisfeiler-Lehman (WL) hierarchy is a well-established framework for graph
isomorphism tests (Grohe, 2017). Introduced by Weisfeiler and Lehman (Weisfeiler & Leman, 1968),
the Weisfeiler-Lehman algorithm (also called 1-WL or color reﬁnement) is a computationally efﬁcient
heuristic for testing graph isomorphism (Babai & Kucera, 1979). It is known that k-WL is strictly
more powerful than (k-1)-WL when k≥3 (Cai et al., 1992; Grohe, 2017)."
INTRODUCTION,0.05583756345177665,Published as a conference paper at ICLR 2022
INTRODUCTION,0.0583756345177665,"Message-passing GNNs are typically considered as a differentiable neural generalization of the
Weisfeiler-Lehman algorithms on graphs. It has been reported (Xu et al., 2019) that some popular
GNNs such as GCN (Kipf & Welling, 2017) and GraphSAGE (Hamilton et al., 2017) are at most
powerful as 1-WL in distinguishing graph structures. Xu et al. (2019) has shown that Graph
Isomorphism Network (GIN) can be as powerful as 1-WL. At its core, GIN provides an injective
aggregation scheme that is deﬁned as a function over multisets of feature vectors, and thus GIN
has the representational power to map any two different multisets of feature vectors to different
representations in an embedding space."
INTRODUCTION,0.06091370558375635,"A considerable amount of efforts has been devoted to improve the expressive power of GNNs beyond
1-WL. Generally, there are three directions: (1) Several works proposed higher-order variants of
GNNs that are as powerful as k-WL with k ≥3 (Azizian & Lelarge, 2020). For example, Morris
et al. (2019) introduced k-order graph networks that are expressive as a set-based variant of k-WL,
Maron et al. (2019) proposed a reduced 2-order graph network that is as expressive as 3-WL, and
Morris et al. (2020) proposed a local version of k-WL which considers only a subset of vertices in a
neighborhood. However, these more expressive GNNs are impractical to use due to their inherent high
computational costs and sophisticated design. (2) Some works attempted to incorporate inductive
biases based on isomorphism counting on pre-deﬁned topological features such as triangles, cliques,
and rings (Bouritsas et al., 2020; Liu et al., 2020; Monti et al., 2018), similar to the traditional ideas of
graph kernels (Yanardag & Vishwanathan, 2015). However, pre-deﬁning topological features requires
domain-speciﬁc expertise, which is often not readily available. (3) Most recently, several works
explored the ideas of augmenting GNNs using node identiﬁers or random features. For example,
Vignac et al. (2020) proposed a method that maintains a “local context"" for each node based on
manipulating node identiﬁers in a permutation equivariant way. You et al. (2021) developed ID-GNNs
by taking into account the identity information of vertices. Chen et al. (2020b) and Murphy et al.
(2019) assigned one-hot IDs to vertices based on the ideas of relational pooling. Sato et al. (2021)
added a random feature to each node to improve the representational capability of GNNs."
INTRODUCTION,0.06345177664974619,"Our work is fundamentally different from existing models by injecting properties of structural
interactions among vertices based on a natural class of isomorphic graphs in the local neighborhood
(i.e., overlap subgraph isomorphism) into a message-passing aggregation scheme of GNNs."
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.06598984771573604,"2
A NEW HIERARCHY OF LOCAL ISOMORPHISM"
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.06852791878172589,"In this section, we characterize a hierarchy of graph isomorphism based on local neighborhood
subgraphs and explore its connections to 1-WL."
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.07106598984771574,"Let G = (V, E) be a simple, undirected graph with a set V of vertices and a set E of edges. The set
of neighbors of a vertex v is denoted by N(v) = {u ∈V |(v, u) ∈E}. The neighborhood subgraph
of a vertex v, denoted by Sv, is the subgraph induced in G by ˜
N(v) = N(v) ∪{v}, which contains
all edges in E that have both endpoints in ˜
N(v). For two adjacent vertices v and u, i.e., (v, u) ∈E,
the overlap subgraph Svu between v and u is deﬁned as Svu = Sv ∩Su."
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.07360406091370558,"Let Si and Sj be the neighborhood subgraphs of two vertices i and j that are not necessarily adjacent,
and hv be the feature vector of a vertex v ∈V . In the following, we deﬁne three notions of
isomorphism, which correspond to different classes of local structures in neighborhood subgraphs."
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.07614213197969544,"Deﬁnition 1. Si and Sj are subgraph-isomorphic, denoted as Si ≃subgraph Sj, if there exists a
bijective mapping g : ˜
N(i) →˜
N(j) such that g(i) = j and for any two vertices v1, v2 ∈˜
N(i), v1
and v2 are adjacent in Si iff g(v1) and g(v2) are adjacent in Sj, and hv1 = hg(v1) and hv2 = hg(v2)."
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.07868020304568528,"Deﬁnition 2. Si and Sj are overlap-isomorphic, denoted as Si ≃overlap Sj, if there exists a bijective
mapping g : ˜
N(i) →˜
N(j) such that g(i) = j and for any v′ ∈N(i) and g(v′) = u′, Siv′ and Sju′
are subgraph-isomorphic."
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.08121827411167512,"Deﬁnition 3. Si and Sj are subtree-isomorphic, denoted as Si ≃subtree Sj, if there exists a bijective
mapping g : ˜
N(i) →˜
N(j) such that g(i) = j and for any v′ ∈˜
N(i) and g(v′) = u′, hv′ = hu′."
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.08375634517766498,"Theorem 1 states that there is a hierarchy among these notions of local isomorphism on neighborhood
subgraphs, where subgraph-isomorphism is the strongest one, subtree-isomorphism is the weakest,"
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.08629441624365482,Published as a conference paper at ICLR 2022
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.08883248730964467,"(a)
(b) i"
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.09137055837563451,"j
Si
Sj
Sv1"
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.09390862944162437,"v1
v4
v3
v2"
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.09644670050761421,"Sv2
Sv3
Sv4"
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.09898477157360407,"Subtree
Overlap  
subgraph"
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.10152284263959391,"Figure 2: (a) Si and Sj are overlap-isomorphic (i.e., having the same overlap subgraph) but not
subgraph-isomorphic; (b) Four neighborhood subgraphs {Svi|i = 1, 2, 3, 4} are subtree-isomorphic
(i.e., having the same subtree) but not overlap-isomorphic."
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.10406091370558376,"and overlap-isomorphism lies in between. Figure 2 shows two groups of graphs: one is distinguishable
w.r.t. subgraph-isomorphism but not overlap-isomorphism, while the other is distinguishable by
overlap-isomorphism but not subtree-isomorphism."
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.1065989847715736,"Theorem 1. The following statements are true: (a) If Si ≃subgraph Sj, then Si ≃overlap Sj; but not
vice versa; (b) If Si ≃overlap Sj, then Si ≃subtree Sj; but not vice versa."
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.10913705583756345,"Let S = {Sv|v ∈V } and ζ : S →Rd mapping each neighborhood subgraph in S into a node
embedding in Rd. The following theorem states that GNNs that are as powerful as 1-WL can
distinguish two neighborhood subgraphs only w.r.t. subtree-isomorphism at each layer.
Theorem 2. Let M be a GNN. M is as powerful as 1-WL in distinguishing non-isomorphic graphs
if M has a sufﬁcient number of layers and each layer can map any Si and Sj in S into two different
embeddings (i.e., ζ(Si) ̸= ζ(Sj)) if and only if Si ̸≃subtree Sj."
A NEW HIERARCHY OF LOCAL ISOMORPHISM,0.1116751269035533,The complete proofs of these theorems are provided in Appendix C.
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.11421319796954314,"3
A GENERALISED MESSAGE-PASSING FRAMEWORK"
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.116751269035533,"In this section, we present a generalised message-passing framework (GMP) which enables to inject
local structure into an aggregation scheme, in light of overlap subgraphs. We theoretically characterize
how GNNs can be designed to be more expressive than 1-WL in this framework."
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.11928934010152284,"Let S∗= {Svu|(v, u) ∈E} be the set of overlap subgraphs in G. We deﬁne structural coefﬁcients for
each vertex v and its neighbors, i.e., ω : S×S∗→R such that Avu = ω(Sv, Svu). A question arising
is: what are the desirable properties of such a function ω? Ideally, it should quantify how a vertex v
structurally interacts with its neighbor u in the local neighborhood. Thus, given Svu = (Vvu, Evu)
and Svu′ = (Vvu′, Evu′), a carefully designed ω should exhibit the following properties:"
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.1218274111675127,"(1) Local closeness: ω(Sv, Svu) > ω(Sv, Svu′) if Svu and Svu′ are complete graphs with
Svu = Ki, Svu′ = Kj, and i > j, where Ki refers to a complete graph on i vertices.
(2) Local denseness: ω(Sv, Svu) > ω(Sv, Svu′) if Svu and Svu′ have the same number of
vertices but differ in the number of edges s.t. |Vvu| = |Vvu′| and |Evu| > |Evu′|.
(3) Isomorphic invariant: ω(Sv, Svu) = ω(Sv, Svu′) if Svu and Svu′ are isomorphic."
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.12436548223350254,"Figure 3 illustrates the ﬁrst two properties. Let {{·}} denote a multiset, ˜A = ( ˜Avu)v,u∈V where ˜Avu
is a normalised value of Avu, and X ∈R|V |×f be a matrix of input feature vectors where xv ∈Rf"
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.12690355329949238,"associates each v ∈V . We denote the feature vector of v at the t-th layer by h(t)
v
and h(0)
v
= xv.
Then, the (t+1)-th layer of an aggregation scheme can be deﬁned as:"
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.12944162436548223,"m(t)
a
= AGGREGATEN
( ˜Avu, h(t)
u )|u ∈N(v)
		
,
(1)"
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.1319796954314721,"m(t)
v
= AGGREGATEI ˜Avu|u ∈N(v)
	
}

h(t)
v ,
(2)"
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.13451776649746192,"h(t+1)
v
= COMBINE

m(t)
v , m(t)
a

.
(3)"
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.13705583756345177,"AGGREGATEN(·) and AGGREGATEI(·) are two possibly different parameterized functions. Here,
m(t)
a is a message aggregated from the neighbors of v and their structural coefﬁcients, and m(t)
v
is an"
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.13959390862944163,Published as a conference paper at ICLR 2022
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.14213197969543148,"(a)
(b)"
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.1446700507614213,"<
<
<
<
<
<"
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.14720812182741116,"v
v
v
v
v
v
v
v
u
u
u
u
u
u
u
u"
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.14974619289340102,"Figure 3: (a) Local closeness: for overlap subgraphs that are complete graphs, their structural
coefﬁcients increase with the number of vertices; (b) Local denseness: for overlap subgraphs that
have the same number of vertices, their structural coefﬁcients increase with the number of edges."
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.15228426395939088,"“adjusted” message from v after performing an element-wise multiplication between AGGREGATEI(·)
and h(t)
v
to account for structural effects from its neighbors. Then, m(t)
v
and m(t)
a are combined by
COMBINE(·) to obtain the feature vector h(t+1)
v
."
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.1548223350253807,"The following theorem states that a GNN can be more expressive than 1-WL if ω is powerful enough
to distinguish structure beyond neighborhood subtrees and the neighborhood aggregation function Φ
is injective under a sufﬁcient number of layers. The proof is provided in Appendix C.
Theorem 3. Let M be a GNN whose aggregation scheme Φ is deﬁned by Eq. 1-Eq. 3. M is strictly
more expressive than 1-WL in distinguishing non-isomorphic graphs if M has a sufﬁcient number of
layers and also satisﬁes the following conditions:"
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.15736040609137056,"(1) M can distinguish at least two neighborhood subgraphs Si and Sj with Si ≃subtree Sj,
Si ̸≃subgraph Sj and {{ ˜Aiv′|v′ ∈N(i)}} ̸= {{ ˜Aju′|u′ ∈N(j)}};"
A GENERALISED MESSAGE-PASSING FRAMEWORK,0.1598984771573604,"(2) Φ

h(t)
v , {{h(t)
u |u ∈N(v)}}, {{( ˜Avu, h(t)
u )|u ∈N(v)}}

is injective."
GRAPHSNN,0.16243654822335024,"4
GRAPHSNN"
GRAPHSNN,0.1649746192893401,"Generally, there are many different ways of designing ω and Φ functions, leading to GNNs with
different expressive powers. To elaborate this, we propose a novel GNN model, named GraphSNN,
whose aggregation scheme is an instantiation of our generalised message-passing framework. We
prove that the expressive power of GraphSNN goes beyond 1-WL."
GRAPHSNN,0.16751269035532995,"Model design. In the following, we provide a deﬁnition of ω that satisﬁes the properties of local
closeness, local denseness, and isomorphic invariant. One key idea behind this deﬁnition is to make it
capable of being generalized to support different graph learning tasks, controlled by λ > 0 (will be
further discussed in Section 5.3):"
GRAPHSNN,0.1700507614213198,"ω(Sv, Svu) =
|Evu|
|Vvu| · |Vvu −1||Vvu|λ.
(4)"
GRAPHSNN,0.17258883248730963,"This deﬁnition allows us to formulate a weighted adjacency matrix A = (Avu)v,u∈V for Graph-"
GRAPHSNN,0.1751269035532995,"SNN. To compare structural coefﬁcients across different nodes, we normalize A to ˜A by ˜Avu =
Avu
P"
GRAPHSNN,0.17766497461928935,"u∈N (v) Avu . Alternatively, A can be normalized using Softmax or other normalization techniques.
For each vertex v ∈V , the feature vector at the (t+1)-th layer is generated by"
GRAPHSNN,0.1802030456852792,"h(t+1)
v
= MLPθ

γ(t) X"
GRAPHSNN,0.18274111675126903,"u∈N(v)
˜Avu + 1

h(t)
v +
X"
GRAPHSNN,0.18527918781725888,u∈N(v)
GRAPHSNN,0.18781725888324874,"
˜Avu + 1

h(t)
u

,
(5)"
GRAPHSNN,0.19035532994923857,"where γ(t) is a learnable scalar parameter. Since N(v) refers to one-hop neighbors of v, one can
stack multiple layers to handle more than one-hop neighborhood. Note that, to ensure the injectivity
in the feature aggregation in the presence of structural coefﬁcients, we add 1 into the ﬁrst and second
terms in Eq. 5. This design is critical for guaranteeing the expressiveness of GraphSNN beyond
1-WL, as will be discussed in the proofs of the lemmas and Theorem 4 later."
GRAPHSNN,0.19289340101522842,"Expressiveness analysis. We ﬁrst generalise the result of universal functions over multisets (Xu et al.,
2019) to universal functions over pairs of multisets since Eq. 5 involves not only node features but"
GRAPHSNN,0.19543147208121828,Published as a conference paper at ICLR 2022
GRAPHSNN,0.19796954314720813,"Method
Cora
Citeseer
Pubmed
NELL
ogbn-arxiv
GCN
81.5 ± 0.4
70.3 ± 0.5
79.0 ± 0.5
66.0 ± 1.7
71.74 ± 0.29
GraphSNNGCN
83.1 ± 1.8
72.3 ± 1.5
79.8 ± 1.2
68.3 ± 1.6
72.20 ± 0.90
GAT
83.0 ± 0.6
72.6 ± 0.6
78.5 ± 0.3
-
-
GraphSNNGAT
83.8 ± 1.2
73.5 ± 1.6
79.6 ± 1.4
-
-
GIN
77.6 ± 1.1
66.1 ± 1.5
77.0 ± 1.2
61.5 ± 2.3
-
GraphSNNGIN
79.2 ± 1.7
68.3 ± 1.5
78.8 ± 1.3
63.8 ± 2.7
-
GraphSAGE
79.2 ± 3.7
71.6 ± 1.9
77.4 ± 2.2
63.7 ± 5.2
71.49 ± 0.27
GraphSNNGraphSAGE
80.5 ± 2.5
72.7 ± 3.2
79.0 ± 3.5
66.3 ± 5.6
71.80 ± 0.70"
GRAPHSNN,0.20050761421319796,Table 1: Classiﬁcation accuracy (%) averaged over 10 runs on node classiﬁcation.
GRAPHSNN,0.20304568527918782,"also structural coefﬁcients. Assume that H, A and W are countable sets where H is a node feature
space, A is a structural coefﬁcient space, and W = {Aijhi|Aij ∈A, hi ∈H}. Let H and W be two
multisets containing elements from H and W, respectively, and |H| = |W|. We can prove Lemma 1,
Lemma 2 and Theorem 4 below, where the proof details are provided in Appendix C."
GRAPHSNN,0.20558375634517767,"Lemma 1. There exists a function f s.t. π(H, W) = P"
GRAPHSNN,0.20812182741116753,"h∈H,w∈W f(h, w) is unique for any distinct
pair of multisets (H, W)."
GRAPHSNN,0.21065989847715735,"Then, the injectiveness of π(H, W) can be extended to π′(a, H, W) as in the lemma below."
GRAPHSNN,0.2131979695431472,"Lemma 2. There exists a function f s.t. π′(hv, H, W) = γf(hv, |H|hv) + P"
GRAPHSNN,0.21573604060913706,"h∈H,w∈W f(h, w) is
unique for any distinct (hv, H, W), where hv ∈H, |H|hv ∈W, and γ can be an irrational number."
GRAPHSNN,0.2182741116751269,"Since any function over (hv, H, W) can be decomposed as g(γf(hv, |H|hv)+P"
GRAPHSNN,0.22081218274111675,"h∈H,w∈W f(h, w)),
similar to Xu et al. (2019), we use a parameterized multi-layer perceptron (MLP) to learn f and g.
The following theorem characterises the expressive power of GraphSNN."
GRAPHSNN,0.2233502538071066,Theorem 4. GraphSNN is more expressive than 1-WL in testing non-isomorphic graphs.
GRAPHSNN,0.22588832487309646,"Since GIN is as powerful as 1-WL (Xu et al., 2019), this theorem implies that GraphSNN is more
expressive than GIN, i.e., GraphSNN can map at least two different neighborhood subgraphs that
correspond to the same multiset of feature vectors to different representations."
GRAPHSNN,0.22842639593908629,"Complexity analysis. Similar to GCN and GIN, GraphSNN is computationally efﬁcient. The time
complexity and memory complexity are linear w.r.t. the number of edges in a graph. Further, due
to the locality of GraphSNN, the computation of aggregating feature vectors from neighborhood
subgraphs at each layer can be parallelized across all vertices. Structural coefﬁcients can be pre-
computed with the time complexity O(ml), where m is the number of edges and l is the maximum
degree of vertices in a graph, and this computation can also be parallelized across all edges. Table
9 in Appendix A summarizes the time and space complexities of several popular message-passing
GNNs in comparison with GraphSNN."
NUMERICAL EXPERIMENTS,0.23096446700507614,"5
NUMERICAL EXPERIMENTS"
NUMERICAL EXPERIMENTS,0.233502538071066,"In this section, we evaluate our models on node classiﬁcation and graph classiﬁcation benchmark
tasks. All the results of our models are statistically signiﬁcant at 0.05 level of signiﬁcance."
NODE CLASSIFICATION,0.23604060913705585,"5.1
NODE CLASSIFICATION"
NODE CLASSIFICATION,0.23857868020304568,"Datasets. We use ﬁve datasets: three citation network datasets Cora, Citeseer, and Pubmed (Sen et al.,
2008) for semi-supervised document classiﬁcation, one knowledge graph dataset NELL (Carlson
et al., 2010) for semi-supervised entity classiﬁcation, and one OGB dataset ogbn-arxiv from (Hu
et al., 2020). Table 10 in Appendix B contains statistics for these datasets."
NODE CLASSIFICATION,0.24111675126903553,"Baseline methods. We consider the popular message-passing GNNs: GCN (Kipf & Welling, 2017),
GAT (Veliˇckovi´c et al., 2017), GIN (Xu et al., 2019), and GraphSAGE (Hamilton et al., 2017). For
each of these baselines, we construct a GraphSNNM model by replacing its aggregation scheme by
our aggregation scheme, which is detailed in Appendix A. The purpose of this setup is to evaluate"
NODE CLASSIFICATION,0.2436548223350254,Published as a conference paper at ICLR 2022
NODE CLASSIFICATION,0.24619289340101522,"Method
MUTAG
PTC-MR
PROTEINS
D&D
BZR
COX2
IMDB-B
RDT-M5K
WL
90.4 ± 5.7
59.9 ± 4.3
75.0 ± 3.1
79.4 ± 0.3
78.5 ± 0.6
81.7 ± 0.7
73.8 ± 3.9
52.5 ± 2.1
RetGK
90.3 ± 1.1
62.5 ± 1.6
75.8 ± 0.6
81.6 ± 0.3
-
-
71.9 ± 1.0
-
GNTK
90.0 ± 8.5
67.9 ± 6.9
75.6 ± 4.2
75.6 ± 3.9
83.6 ± 2.9
-
76.9 ± 3.6
-
P-WL
90.5 ± 1.3
64.0 ± 0.8
75.2 ± 0.3
78.6 ± 0.3
-
-
-
-
WL-PM
87.7 ± 0.8
61.4 ± 0.8
-
78.6 ± 0.2
-
-
-
-
WWL
87.2 ± 1.5
66.3 ± 1.2
74.2 ± 0.5
79.6 ± 0.5
84.4 ± 2.0
78.2 ± 0.4
74.3 ± 0.8
-
FGW
88.4 ± 5.6
65.3 ± 7.9
74.5 ± 2.7
-
85.1 ± 4.1
77.2 ± 4.8
63.8 ± 3.4
-
DGCNN
85.8 ± 1.7
58.6 ± 2.5
75.5 ± 0.9
79.3 ± 0.9
-
-
70.0 ± 0.9
48.7 ± 4.5
CapsGNN
86.6 ± 6.8
66.0 ± 1.8
76.2 ± 3.6
75.4 ± 4.1
-
-
73.1 ± 4.8
52.9 ± 1.5
†GraphSAGE
85.1 ± 7.6
63.9 ± 7.7
75.9 ± 3.2
72.9 ± 2.0
-
-
72.3 ± 5.3
50.0 ± 1.3
†GIN
89.4 ± 5.6
64.6 ± 7.0
75.9 ± 2.8
-
-
-
75.1 ± 5.1
57.5 ± 1.5"
NODE CLASSIFICATION,0.24873096446700507,"†GraphSNN (S)
91.57 ± 2.8
66.70 ± 3.7
76.83 ± 2.5
81.97 ± 2.6
88.69 ± 3.2
82.86 ± 3.1
77.86 ± 3.6
58.43 ± 2.3
†GraphSNN (R)
91.24 ± 2.5
66.96 ± 3.5
76.51 ± 2.5
82.46 ± 2.7
88.97 ± 2.9
83.13 ± 3.5
76.93 ± 3.3
58.51 ± 2.7
GraphSNN (S)
94.70 ± 1.9
70.58 ± 3.1
78.42 ± 2.7
83.92 ± 2.3
91.12 ± 3.0
86.28 ± 3.3
78.51 ± 2.8
59.86 ± 2.6
GraphSNN (R)
94.14 ± 1.2
71.01 ± 3.6
78.21 ± 2.9
84.61 ± 1.5
91.88 ± 3.2
86.72 ± 2.9
77.87 ± 3.1
60.23 ± 2.2"
NODE CLASSIFICATION,0.2512690355329949,"Table 2: Classiﬁcation accuracy (%) averaged over 10 runs on graph classiﬁcation. The results of
WL and RetGK are taken from (Du et al., 2019), GraphSAGE from (Xu et al., 2019), DGCNN from
(Maron et al., 2019) and others from their original papers. † indicates the reporting setting used in
GIN and further details on the experimental settings are discussed in Appendix B."
NODE CLASSIFICATION,0.25380710659898476,"how effectively our aggregation scheme with structural coefﬁcients can learn representations for
vertices, compared with the standard message-passing aggregation scheme."
NODE CLASSIFICATION,0.2563451776649746,"Experimental setup. We use the Adam optimizer (Kingma & Ba, 2015) and λ = 1. For ogbn-
arxiv, our models are trained for 500 epochs with the learning rate 0.01, dropout 0.5, hidden
units 256, and γ = 0.1. For the other datasets, we use 200 epochs with the learning rate 0.001,
and choose the best values for weight decay from {0.001, 0.002, ..., 0.009} and hidden units from
{64, 128, 256, 512}. For γ and dropout at each layer, the best value for each model in each dataset is
selected from {0.1, 0.2, ..., 0.6}. GraphSNNGAT uses the attention dropout 0.6 and 8 multi-attention
heads. GraphSNNGraphSAGE uses the neighborhood sample size 25 with the mean aggregation."
NODE CLASSIFICATION,0.25888324873096447,"We consider two settings of data splits for all datasets except for ogbn-arxiv: (1) the standard splits in
Kipf & Welling (2017), i.e., 20 nodes from each class for training, 500 nodes for validation and 1000
nodes for testing, for which the results are presented in Table 1; (2) the random splits in Pei et al.
(2020), i.e., randomly splitting nodes into 60%, 20% and 20% for training, validation and testing,
respectively, for which the results are presented in Table 13 in Appendix B. For ogbn-arxiv, we follow
Hu et al. (2020) to use a time-based data split based on publication dates."
GRAPH CLASSIFICATION,0.2614213197969543,"5.2
GRAPH CLASSIFICATION"
GRAPH CLASSIFICATION,0.2639593908629442,"We evaluate GraphSNN from three aspects: (1) small standard graph datasets, (2) large graph datasets
and (3) comparison with GNNs that are go beyond 1-WL."
GRAPH CLASSIFICATION,0.26649746192893403,"Experiments on small graphs. We use eight datasets from two categories: (1) bioinformatics
datasets: MUTAG, PTC-MR, COX2, BZR, PROTEINS, and D&D (Debnath et al., 1991; Kriege et al.,
2016; Wale et al., 2008; Shervashidze et al., 2011; Sutherland et al., 2003; Borgwardt & Kriegel,
2005); (2) social network datasets: IMDB-B and RDT-M5K (Yanardag & Vishwanathan, 2015).
Table 11 in Appendix B contains statistics for these small graph datasets."
GRAPH CLASSIFICATION,0.26903553299492383,"We compare against eleven baselines: (1) Graph kernel based methods: WL subtree kernel (Sher-
vashidze et al., 2011), RetGK (Zhang et al., 2018b), GNTK (Du et al., 2019), P-WL (Rieck et al.,
2019), WL-PM (Nikolentzos et al., 2017), WWL (Togninalli et al., 2019) and FGW (Titouan et al.,
2019); (2) GNN based methods: DGCNN (Zhang et al., 2018a), CapsGNN (Xinyi & Chen, 2018),
GIN (Xu et al., 2019), and GraphSAGE (Hamilton et al., 2017)."
GRAPH CLASSIFICATION,0.2715736040609137,"Both the standard stratiﬁed splits (Xu et al., 2019) and the random splits are considered. We use
10-fold cross validation with 90% training and 10 % testing, and report the best mean accuracy. For
both settings, we use the Adam optimizer (Kingma & Ba, 2015), batch size 64, hidden dimension 64,
weight decay of 0.009, a 2-layer MLP with batch normalization, 500 epochs and dropout of 0.6, and
γ = 0.1 over all datasets. The readout function as in (Xu et al., 2019) is used which concatenates
representations of all layers to obtain a ﬁnal graph representation. For the standard stratiﬁed splits,
we use the learning rate 0.009 over all datasets. For the random splits, we use the learning rate 0.008
for MUTAG and RDT-M5K, and 0.007 for the other datasets. Table 2 presents the results."
GRAPH CLASSIFICATION,0.27411167512690354,Published as a conference paper at ICLR 2022
GRAPH CLASSIFICATION,0.2766497461928934,"Method
ogbg-molhiv
ogbg-moltox21
ogbg-moltoxcast
ogbg-ppa
ogbg-molpcba
GIN
75.58±1.40
74.91±0.51
63.41±0.74
68.92±1.00
22.66±0.28
GIN+VN
75.20±1.30
76.21±0.82
66.18±0.68
70.37±1.07
27.03±0.23
GSN
77.99±1.00
-
-
-
-
PNA
79.05±1.30
-
-
-
28.38±0.35
ID-GNN
78.30±2.00
-
-
-
-
Deep LRP
77.19±1.40
-
-
-
-
GraphSNN
78.51±1.70
75.45±1.10
65.40±0.71
70.66±1.65
24.96±1.50
GraphSNN+VN
79.72±1.83
76.78±1.27
67.68±0.92
72.02±1.48
28.50±1.68"
GRAPH CLASSIFICATION,0.27918781725888325,"Table 3: Classiﬁcation accuracy (%) averaged over 10 runs on graph classiﬁcation, where λ = 2. The
results of the baselines are taken from (Hu et al., 2020) and the leaderboard of the OGB website."
GRAPH CLASSIFICATION,0.2817258883248731,"Method
MUTAG
PTC-MR
PROTEINS
BZR
IMDB-B"
GRAPH CLASSIFICATION,0.28426395939086296,"GSN
GSN-e
90.6 ± 7.5
68.2 ± 7.2
76.6 ± 5.0
-
77.8 ± 3.3
GSN-v
92.2 ± 7.5
67.4 ± 5.7
74.5 ± 5.0
-
76.8 ± 2.0"
GRAPH CLASSIFICATION,0.2868020304568528,"ID-GNNs
ID-GNN Fast
96.5 ± 3.2
61.9 ± 5.4
78.0 ± 3.5
86.4 ± 3.0
-
ID-GNN Full
93.0 ± 5.6
62.5 ± 5.3
77.9 ± 2.4
88.1 ± 4.0
-
Ours
GraphSNN
91.57 ± 2.8
66.70 ± 3.7
76.83 ± 2.5
88.69 ± 3.2
77.86 ± 3.6"
GRAPH CLASSIFICATION,0.2893401015228426,"k-WL
GNNs"
-GNNNT,0.2918781725888325,"1-GNNNT
82.7 ± 0.0
51.2 ± 0.0
-
-
69.4 ± 0.0
1-GNN
82.2 ± 0.0
59.0 ± 0.0
-
-
71.2 ± 0.0
1-2-3-GNNNT
84.4 ± 0.0
59.3 ± 0.0
-
-
70.3 ± 0.0
1-2-3-GNN
86.1 ± 0.0
60.9 ± 0.0
-
-
74.2 ± 0.0
Ours
GraphSNN
87.30 ± 3.1
61.63 ± 2.8
74.01 ± 3.2
82.72 ± 3.9
74.81 ± 3.5"
-GNNNT,0.29441624365482233,"Table 4: Classiﬁcation accuracy (%) averaged over 10 runs on graph classiﬁcation, where λ = 2.
The results of the baselines are taken from their original papers. GSN and ID-GNNs use the same
experimental setup as GIN, while k-WL GNNs uses the same experimental setup as CapsGNN. These
experimental setups are detailed in Appendix B."
-GNNNT,0.2969543147208122,"Experiments on large graphs. We use ﬁve large graph datasets from Open Graph Benchmark
(OGB) Hu et al. (2020), including four molecular graph datasets (ogbg-molhiv, ogbg-moltox21,
ogbg-moltoxcast and ogb-molpcba) and one protein-protein association network (ogbg-ppa). Table
12 in Appendix B contains statistics for these large graph datasets."
-GNNNT,0.29949238578680204,"We compare against the following methods that have reported the results on the above OGB datasets:
GIN and GIN+VN (Hu et al., 2020), GSN (Bouritsas et al., 2020), PNA (Corso et al., 2020), ID-GNNs
(You et al., 2021) and Deep LRP (Chen et al., 2020b). In addition to the original model of GraphSNN,
we also consider a variant, denoted as GraphSNN+VN, which performs the message passing over
augmented graphs with virtual nodes in GraphSNN (Hu et al., 2020; Ishiguro et al., 2019)."
-GNNNT,0.3020304568527919,"We follow the same experiment setup as in Hu et al. (2020). We use the Adam optimizer with learning
rate 0.001, batch size 32, dropout 0.5 and 100 epochs for all datasets. GraphSNN uses a 8-layer MLP
with embedding dimension 512 for ogbg-moltoxcast and ogbg-moltox21, while GraphSNN+VN has
the embedding dimensions 300 and 256, and 8-layer and 5-layer MLPs for ogbg-moltoxcast and
ogbg-moltox21, respectively. For ogbg-molhiv, ogbg-molpcba and ogbg-ppa, both GraphSNN and
GraphSNN+VN use a 5-layer MLP and embedding dimension 200. Table 3 shows the results for
the classiﬁcation accuracy. Table 15 in Appendix B shows the results for the running time of the
prepocessing step."
-GNNNT,0.30456852791878175,"Comparison with GNNs beyond 1-WL. We compare GraphSNN with the other GNNs that are
more expressive than 1-WL, including: GSN (Bouritsas et al., 2020), ID-GNNs (You et al., 2021)
and k-WL GNN (Morris et al., 2019). We use the same experimental setup as in (Xu et al., 2019;
Bouritsas et al., 2020; Maron et al., 2019). Table 4 shows the results."
ABLATION STUDY,0.30710659898477155,"5.3
ABLATION STUDY"
ABLATION STUDY,0.3096446700507614,"We perform an ablation study to analyze the effect of λ values on model performance. Tables 5 and 6
show that λ = 1 yields the highest performance for node classiﬁcation, while λ = 2 is the best for
graph classiﬁcation. This reﬂects a critical point - different classes of structure information are needed
by different graph learning tasks. λ = 1 captures local density, e.g., two overlap subgraphs may"
ABLATION STUDY,0.31218274111675126,Published as a conference paper at ICLR 2022
ABLATION STUDY,0.3147208121827411,"Dataset
Method
λ=1
λ=2
λ=3
λ=4
λ=5 Cora"
ABLATION STUDY,0.31725888324873097,"GraphSNNGCN
83.1±1.8
82.8±1.3
82.3±2.4
81.8±1.6
82.1±1.6
GraphSNNGIN
79.2±1.7
78.8±1.2
78.5±1.3
78.1±1.6
77.7±1.2
GraphSNNGraphSAGE
80.5±2.5
80.3±2.1
79.8±1.9
79.2±1.9
79.4±2.2
GraphSNNGAT
83.8±1.2
83.5±1.5
83.2±1.7
82.8±1.3
83.2±1.9"
ABLATION STUDY,0.3197969543147208,Citeseer
ABLATION STUDY,0.3223350253807107,"GraphSNNGCN
72.3±1.5
71.7±1.3
71.1±1.6
70.6±1.2
70.9±1.1
GraphSNNGIN
68.3±1.5
68.3±1.9
67.7±1.4
67.1±1.3
67.3±1.4
GraphSNNGraphSAGE
72.7±3.2
72.0±2.5
71.6±2.9
71.9±2.1
71.3±2.3
GraphSNNGAT
73.5±1.6
72.9±1.7
72.5±1.1
72.6±1.6
72.0±1.3"
ABLATION STUDY,0.3248730964467005,Table 5: Classiﬁcation accuracy (%) averaged over 10 runs on node classiﬁcation with standard splits.
ABLATION STUDY,0.32741116751269034,"Dataset
Method
λ=1
λ=2
λ=3
λ=4
λ=5
MUTAG"
ABLATION STUDY,0.3299492385786802,GraphSNN
ABLATION STUDY,0.33248730964467005,"92.66±2.4
94.14±1.2
93.38±1.5
92.25±2.1
92.79±2.0
PTC-MR
70.76±5.1
71.01±3.6
70.67±2.8
69.59±2.1
69.97±3.1
PROTEINS
77.90±4.9
78.21±2.9
78.15±2.1
77.20±3.1
76.93±3.2
D&D
82.70±4.6
84.61±1.5
84.34±1.2
82.60±2.6
82.30±2.3
BZR
87.61±4.9
91.88±3.2
91.45±2.6
91.38±2.1
90.90±3.1
COX2
86.20±3.3
86.72±2.9
83.81±3.1
83.13±2.6
83.94±3.2
IMDB-B
77.07±5.2
77.87±3.1
77.60±3.6
77.32±3.2
77.10±3.3
RDT-M5K
59.53±2.6
60.23±2.2
60.10±2.3
60.00±2.1
59.90±2.6"
ABLATION STUDY,0.3350253807106599,Table 6: Classiﬁcation accuracy (%) averaged over 10 runs on graph classiﬁcation with random splits.
ABLATION STUDY,0.33756345177664976,"considerably vary in the number of vertices but their local density can be very close. Our experiments
show that injecting such local density helps improve the performance of node classiﬁcation. λ = 2
captures local similarity, i.e., how similar two overlap subgraphs are. Two overlap subgraphs that
considerably differ in the number of vertices would have very different structural coefﬁcients. Since
graph classiﬁcation requires to compare the similarity of two graphs, λ = 2 is thus the best."
OVERSMOOTHING ANALYSIS,0.3401015228426396,"5.4
OVERSMOOTHING ANALYSIS"
OVERSMOOTHING ANALYSIS,0.3426395939086294,"We analyse the impact of model depth (number of layers) on node classiﬁcation performance. In
addition to GCN and GraphSNNGCN, we also compare these models with a residual connection
(i.e., GCN+residual and GraphSNNGCN+residual). We evaluate all the models on Cora dataset using
the standard splits and same hyperparameters as in Section 5.1. Table 7 shows the results. When
increasing the model depth, GraphSNNGCN performs consistently better than GCN at each layer.
This is because structural coefﬁcients capture structural connectivity between a target vertex and its
neighbors. Thus, a neighbor whose structural connectivity is weak would pass little messages to
the target vertex, whereas a neighbor whose structural connectivity is strong would pass a strong
message to the target vertex. GraphSNN helps alleviate the oversmoothing issue even in the presence
of residual connections. Further results of the oversmoothing analysis are provided in Appendix B."
OVERSMOOTHING ANALYSIS,0.34517766497461927,"#Layers
GCN
GCN+residual
GraphSNNGCN
GraphSNNGCN+residual
1
79.6±0.5
80.3±0.7
80.1±0.8
81.6±1.6
2
81.5±0.4
82.8±1.2
83.1±1.8
84.1±1.7
3
80.3±0.6
82.3±0.5
82.0±0.8
83.4±0.7
4
78.2±0.9
81.5±0.9
80.1±0.7
82.9±0.9
5
74.3±1.3
81.0±1.3
79.1±1.2
82.3±0.3
6
35.6±1.5
80.6±0.5
76.5±1.3
81.5±1.2
7
31.6±0.9
79.7±0.6
76.3±1.3
80.9±0.9
8
16.2±1.2
78.4±1.1
75.7±1.2
80.3±1.3"
OVERSMOOTHING ANALYSIS,0.3477157360406091,Table 7: Classiﬁcation accuracy (%) averaged over 10 runs on Cora dataset.
CONCLUSIONS,0.350253807106599,"6
CONCLUSIONS"
CONCLUSIONS,0.35279187817258884,"In this paper, we have introduced a GNN framework, which enables a general way of injecting
structural information into a message-passing aggregation scheme. We have also introduced a novel
GNN model, GraphSNN, for graph learning, and prove that GraphSNN is more expressive than
1-WL in distinguishing graph structures. It is shown that GraphSNN consistently outperforms all the
state-of-the-art approaches in both node classiﬁcation and graph classiﬁcation benchmark tasks."
CONCLUSIONS,0.3553299492385787,Published as a conference paper at ICLR 2022
REFERENCES,0.35786802030456855,REFERENCES
REFERENCES,0.3604060913705584,"Waïss Azizian and Marc Lelarge. Expressive power of invariant and equivariant graph neural networks.
In International Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.3629441624365482,"László Babai and Ludik Kucera. Canonical labelling of graphs in linear average time. In 20th Annual
Symposium on Foundations of Computer Science (SFCS), pp. 39–46, 1979."
REFERENCES,0.36548223350253806,"Karsten M Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs. In Fifth IEEE
international conference on data mining (ICDM’05), pp. 8–pp. IEEE, 2005."
REFERENCES,0.3680203045685279,"Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M Bronstein. Improving graph
neural network expressivity via subgraph isomorphism counting. arXiv preprint arXiv:2006.09252,
2020."
REFERENCES,0.37055837563451777,"Jin-Yi Cai, Martin Fürer, and Neil Immerman. An optimal lower bound on the number of variables
for graph identiﬁcation. Combinatorica, 12(4):389–410, 1992."
REFERENCES,0.3730964467005076,"Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka, and Tom M
Mitchell. Toward an architecture for never-ending language learning. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence (AAAI), 2010."
REFERENCES,0.3756345177664975,"Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-
smoothing problem for graph neural networks from the topological view. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence (AAAI), pp. 3438–3445, 2020a."
REFERENCES,0.37817258883248733,"Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count
substructures? Advances in neural information processing systems (NeurIPS), 2020b."
REFERENCES,0.38071065989847713,"Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Liò, and Petar Veliˇckovi´c. Principal
neighbourhood aggregation for graph nets. Advances in Neural Information Processing Systems
(NeurIPS), 2020."
REFERENCES,0.383248730964467,"Pim de Haan, Taco Cohen, and Max Welling.
Natural graph networks.
arXiv preprint
arXiv:2007.08349, 2020."
REFERENCES,0.38578680203045684,"Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin
Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds.
correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34
(2):786–797, 1991."
REFERENCES,0.3883248730964467,"Simon S Du, Kangcheng Hou, Barnabás Póczos, Ruslan Salakhutdinov, Ruosong Wang, and Keyulu
Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. arXiv preprint
arXiv:1905.13192, 2019."
REFERENCES,0.39086294416243655,"Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. A fair comparison of graph
neural networks for graph classiﬁcation. 2020."
REFERENCES,0.3934010152284264,"Vikas Garg, Stefanie Jegelka, and Tommi Jaakkola. Generalization and representational limits of
graph neural networks. In International Conference on Machine Learning (ICML), pp. 3419–3430,
2020."
REFERENCES,0.39593908629441626,"Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International Conference on Machine Learning (ICML),
pp. 1263–1272. PMLR, 2017."
REFERENCES,0.39847715736040606,"Martin Grohe. Descriptive complexity, canonisation, and deﬁnable graph structure theory, volume 47.
Cambridge University Press, 2017."
REFERENCES,0.4010152284263959,"Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
Advances in Neural Information Processing Systems (NeurIPS), pp. 1024–1034, 2017."
REFERENCES,0.4035532994923858,"Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in
Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.40609137055837563,Published as a conference paper at ICLR 2022
REFERENCES,0.4086294416243655,"Katsuhiko Ishiguro, Shin-ichi Maeda, and Masanori Koyama. Graph warp module: an auxiliary
module for boosting the power of graph neural networks in molecular graph analysis. arXiv
preprint arXiv:1902.01020, 2019."
REFERENCES,0.41116751269035534,"Nicolas Keriven and Gabriel Peyré. Universal invariant and equivariant graph neural networks. In
Advances in Neural Information Processing Systems (NeurIPS), 2019."
REFERENCES,0.4137055837563452,"Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015."
REFERENCES,0.41624365482233505,"Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.
In International Conference on Learning Representations (ICLR), 2017."
REFERENCES,0.41878172588832485,"Nils M Kriege, Pierre-Louis Giscard, and Richard Wilson. On valid optimal assignment kernels
and applications to graph classiﬁcation. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 1623–1631, 2016."
REFERENCES,0.4213197969543147,"Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI),
2018."
REFERENCES,0.42385786802030456,"Xin Liu, Haojie Pan, Mutian He, Yangqiu Song, Xin Jiang, and Lifeng Shang. Neural subgraph
isomorphism counting. In Proceedings of the 26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pp. 1959–1969, 2020."
REFERENCES,0.4263959390862944,"Andreas Loukas. What graph neural networks cannot learn: depth vs width. In International
Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.4289340101522843,"Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. In International Conference on Learning Representations (ICLR), 2018."
REFERENCES,0.43147208121827413,"Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph
networks. Advances in Neural Information Processing Systems (NeurIPS), 2019."
REFERENCES,0.434010152284264,"Federico Monti, Karl Otness, and Michael M Bronstein. Motifnet: a motif-based graph convolutional
network for directed graphs. In 2018 IEEE Data Science Workshop (DSW), pp. 225–228, 2018."
REFERENCES,0.4365482233502538,"Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), pp. 4602–4609, 2019."
REFERENCES,0.43908629441624364,"Christopher Morris, Gaurav Rattan, and Petra Mutzel. Weisfeiler and leman go sparse: Towards
scalable higher-order graph embeddings. Advances in Neural Information Processing Systems
(NeurIPS), 2020."
REFERENCES,0.4416243654822335,"Ryan Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Relational pooling for
graph representations. In International Conference on Machine Learning (ICML), pp. 4663–4673,
2019."
REFERENCES,0.44416243654822335,"Giannis Nikolentzos, Polykarpos Meladianos, and Michalis Vazirgiannis. Matching node embeddings
for graph similarity. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI),
2017."
REFERENCES,0.4467005076142132,"Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric
graph convolutional networks. In International Conference on Learning Representations (ICLR),
2020."
REFERENCES,0.44923857868020306,"Bastian Rieck, Christian Bock, and Karsten Borgwardt. A persistent weisfeiler-lehman procedure for
graph classiﬁcation. In International Conference on Machine Learning (ICML), pp. 5448–5458.
PMLR, 2019."
REFERENCES,0.4517766497461929,"Ryoma Sato.
A survey on the expressive power of graph neural networks.
arXiv preprint
arXiv:2003.04078, 2020."
REFERENCES,0.4543147208121827,Published as a conference paper at ICLR 2022
REFERENCES,0.45685279187817257,"Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features strengthen graph neural
networks. In Proceedings of the 2021 SIAM International Conference on Data Mining (SDM), pp.
333–341, 2021."
REFERENCES,0.4593908629441624,"Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classiﬁcation in network data. AI magazine, 29(3):93–93, 2008."
REFERENCES,0.4619289340101523,"Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M
Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9), 2011."
REFERENCES,0.46446700507614214,"Jeffrey J Sutherland, Lee A O’brien, and Donald F Weaver. Spline-ﬁtting with a genetic algorithm:
A method for developing classiﬁcation structure- activity relationships. Journal of chemical
information and computer sciences, 43(6):1906–1915, 2003."
REFERENCES,0.467005076142132,"Vayer Titouan, Nicolas Courty, Romain Tavenard, and Rémi Flamary. Optimal transport for structured
data with application on graphs. In International Conference on Machine Learning (ICML), pp.
6275–6284, 2019."
REFERENCES,0.46954314720812185,"Matteo Togninalli, Elisabetta Ghisu, Felipe Llinares-López, Bastian Rieck, and Karsten Borgwardt.
Wasserstein weisfeiler-lehman graph kernels. In Advances in Neural Information Processing
Systems (NeurIPS), pp. 6439–6449, 2019."
REFERENCES,0.4720812182741117,"Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. International Conference on Learning Representations (ICLR),
2017."
REFERENCES,0.4746192893401015,"Clément Vignac, Andreas Loukas, and Pascal Frossard. Building powerful and equivariant graph
neural networks with structural message-passing. In Advances in Neural Information Processing
Systems (NeurIPS), 2020."
REFERENCES,0.47715736040609136,"Nikil Wale, Ian A Watson, and George Karypis. Comparison of descriptor spaces for chemical
compound retrieval and classiﬁcation. Knowledge and Information Systems, 14(3):347–375, 2008."
REFERENCES,0.4796954314720812,"Boris Weisfeiler and Andrei Leman. The reduction of a graph to canonical form and the algebra
which appears therein. NTI, Series, 2(9):12–16, 1968."
REFERENCES,0.48223350253807107,"Asiri Wijesinghe and Qing Wang. Dfnets: Spectral cnns for graphs with feedback-looped ﬁlters.
Advances in neural information processing systems (NeurIPS), 2019."
REFERENCES,0.4847715736040609,"Jun Wu, Jingrui He, and Jiejun Xu. Demo-net: Degree-speciﬁc graph neural networks for node
and graph classiﬁcation. In Proceedings of the 25th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pp. 406–415, 2019."
REFERENCES,0.4873096446700508,"Zhang Xinyi and Lihui Chen. Capsule graph neural network. In International conference on learning
representations (ICLR), 2018."
REFERENCES,0.48984771573604063,"Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jumping knowledge networks. In International
Conference on Machine Learning (ICML), pp. 5453–5462. PMLR, 2018."
REFERENCES,0.49238578680203043,"Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations (ICLR), 2019."
REFERENCES,0.4949238578680203,"Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM
SIGKDD international conference on knowledge discovery and data mining, pp. 1365–1374, 2015."
REFERENCES,0.49746192893401014,"Jiaxuan You, Jonathan Gomes-Selman, Rex Ying, and Jure Leskovec. Identity-aware graph neural
networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), 2021."
REFERENCES,0.5,"Muhan Zhang and Yixin Chen. Weisfeiler-lehman neural machine for link prediction. In Proceedings
of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
pp. 575–583, 2017."
REFERENCES,0.5025380710659898,Published as a conference paper at ICLR 2022
REFERENCES,0.5050761421319797,"Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning archi-
tecture for graph classiﬁcation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence
(AAAI), 2018a."
REFERENCES,0.5076142131979695,"Zhen Zhang, Mianzhi Wang, Yijian Xiang, Yan Huang, and Arye Nehorai. Retgk: Graph kernels
based on return probabilities of random walks. In Advances in Neural Information Processing
Systems (NeurIPS), pp. 3964–3974, 2018b."
REFERENCES,0.5101522842639594,"Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International
Conference on Learning Representations (ICLR), 2019."
REFERENCES,0.5126903553299492,Published as a conference paper at ICLR 2022
REFERENCES,0.5152284263959391,APPENDIX
REFERENCES,0.5177664974619289,A. CONNECTIONS TO PREVIOUS WORK
REFERENCES,0.5203045685279187,"In the following, we discuss how our framework generalizes the existing message-passing GNNs
in the literature such as GCN (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017), GAT
(Veliˇckovi´c et al., 2017) and GIN (Xu et al., 2019) as special cases. Table 8 presents the local
aggregation schemes used by these existing GNN models. They differ from each other w.r.t. the way
of aggregating feature vectors in a neighborhood and how they are combined with the current vertex’s
feature itself, i.e., summation or concatenation. Here, αvu is an attention coefﬁcient capturing the
importance of a neighbor in GAT, ϵ is a learnable or ﬁxed scalar parameter used in GIN, W is a
learnable weight matrix and σ is a non-linear activation function, such as ReLU."
REFERENCES,0.5228426395939086,"Note that, as deﬁned in Equation 3, m(t)
a
and m(t)
v
refer to the messages aggregated by AGGRE-
GATEN(·) and AGGREGATEI(·), respectively."
REFERENCES,0.5253807106598984,"GNN Model
AGGREGATEN(·)
AGGREGATEI(·)
COMBINE(·) GCN
P"
REFERENCES,0.5279187817258884,u∈N(v)
REFERENCES,0.5304568527918782,"W (t)h(t)
u
√"
REFERENCES,0.5329949238578681,|N (u)||N (v)|
REFERENCES,0.5355329949238579,"W (t)h(t)
v
√"
REFERENCES,0.5380710659898477,"|N (v)||N (v)|
σ(SUM(m(t)
v , m(t)
a ))"
REFERENCES,0.5406091370558376,"GraphSAGE
P"
REFERENCES,0.5431472081218274,u∈N(v)
REFERENCES,0.5456852791878173,"h(t)
u
|N (v)|
h(t)
v
σ(W (t) · CONCAT(m(t)
v , m(t)
a )) GAT
P"
REFERENCES,0.5482233502538071,"u∈N(v)
αvuW (t)h(t)
u
αvvW (t)h(t)
v
σ(SUM(m(t)
v , m(t)
a )) GIN
P"
REFERENCES,0.550761421319797,"u∈N(v)
h(t)
u
(1 + ϵ)h(t)
v
MLPθ(SUM(m(t)
v , m(t)
a ))"
REFERENCES,0.5532994923857868,Table 8: Comparison of the aggregation schemes used in existing message-passing GNNs
REFERENCES,0.5558375634517766,COMPLEXITY ANALYSIS
REFERENCES,0.5583756345177665,"Table 9 summarizes the time and space complexities of several popular message-passing GNNs and
GraphSNN, where n and m are the numbers of vertices and edges in a graph, respectively, k refers to
the number of layers, f and d are the dimensions of input and output feature vectors, respectively, a
is the number of attention heads used in GAT, and s is the number of neighbors sampled for each
node at each layer in GraphSAGE."
REFERENCES,0.5609137055837563,"GNN Model
Time Complexity
Memory Complexity
GCN (Kipf & Welling, 2017)
O(kmfd)
O(m)
GIN (Xu et al., 2019)
O(kmfd)
O(m)
GAT (Veliˇckovi´c et al., 2017)
O(k(anfd + amd))
O(n2)
GraphSAGE (Hamilton et al., 2017)
O(snfd)
O(n)"
REFERENCES,0.5634517766497462,"GraphSNN (ours)
O(kmfd)
O(m)"
REFERENCES,0.565989847715736,Table 9: Time and space complexities of message-passing GNNs and GraphSNN.
REFERENCES,0.5685279187817259,FORMULATION OF GRAPHGNNM
REFERENCES,0.5710659898477157,"For each of these message-passing GNNs, denoted as M, we construct a variant GraphSNNM by
replacing its existing aggregation scheme by our aggregation scheme with structural coefﬁcients as
formulated in Eq. 5. These variants are used in our experiments for node classiﬁcation benchmark
tasks (see Section 5.1) in order to evaluate how our aggregation scheme with structural coefﬁcients
can improve performance, compared with their standard message-passing aggregation schemes.
Below are the details of these variants."
REFERENCES,0.5736040609137056,GCN and GraphSNNGCN
REFERENCES,0.5761421319796954,Published as a conference paper at ICLR 2022
REFERENCES,0.5786802030456852,"Graph Convolutional Network (GCN) (Kipf & Welling, 2017) applies a normalized mean aggregation
to combine the feature vector of a node v with the feature vectors in its neighborhood N(v):"
REFERENCES,0.5812182741116751,"h(t+1)
v
= σ

W (t)h(t)
v
p"
REFERENCES,0.583756345177665,"|N(v)||N(v)|
+
X"
REFERENCES,0.5862944162436549,u∈{N (v)}
REFERENCES,0.5888324873096447,"W (t)h(t)
u
p"
REFERENCES,0.5913705583756346,|N(v)||N(u)|
REFERENCES,0.5939086294416244,"
.
(6) p"
REFERENCES,0.5964467005076142,"|N(u)||N(v)| is a normalization constant for the edge (v, u), which originates from the normalized
adjacency matrix D−1/2AD−1/2. W (t) is a trainable weight matrix and σ is a non-linear activa-
tion function such as ReLU. We generalise GCN to a model under the GMP framework, namely
GraphSNNGCN, to improve the expressive power of GCN. We ﬁrst construct a normalized structural
coefﬁcient matrix ˜A. Formally, each neural layer of GraphSNNGCN may then be expressed as:"
REFERENCES,0.5989847715736041,"h(t+1)
v
= σ "
REFERENCES,0.6015228426395939,γ(t) X
REFERENCES,0.6040609137055838,"u∈N(v)
˜Avu + 1

W (t)h(t)
v
q"
REFERENCES,0.6065989847715736,"| ˜
N(v)|| ˜
N(v)|
+
X"
REFERENCES,0.6091370558375635,u∈N(v)
REFERENCES,0.6116751269035533,"
˜Avu + 1

W (t)h(t)
u
q"
REFERENCES,0.6142131979695431,"| ˜
N(u)|| ˜
N(v)| ! . (7)"
REFERENCES,0.616751269035533,GraphSAGE and GraphSNNGraphSAGE
REFERENCES,0.6192893401015228,"GraphSAGE (Hamilton et al., 2017) learns aggregation functions to induce new node feature vectors
by sampling and aggregating features from a node’s local neighborhood. GraphSAGE has considered
three different aggregation functions such as mean aggregator, LSTM aggregator and pooling aggre-
gator. In our work, we mainly focus on the mean aggregator that, for each vertex v, takes the mean of
the feature vectors of the nodes in its neighborhood and concatenates it with the feature vector of v as
shown below:
h(t+1)
v
= σ

W (t) · CONCAT

1
|N(v)| X"
REFERENCES,0.6218274111675127,"u∈N(v)
h(t)
u , h(t)
v

,
(8)"
REFERENCES,0.6243654822335025,"where W (t) is a learnable weight matrix, and σ represents a non-linear activation function. We
also generalise GraphSNN to a model under the GMP framework, namely GraphSNNGraphSAGE.
This model ﬁrst takes a mean aggregation of the feature vectors in the neighborhood N(v) and then
concatenates it with the feature vector of v itself in the following manner:"
REFERENCES,0.6269035532994924,"h(t+1)
v
= σ

W (t) · CONCAT

1
|N(v)| X"
REFERENCES,0.6294416243654822,u∈N(v)
REFERENCES,0.631979695431472,"
˜Avu + 1

h(t)
u , γ(t) X"
REFERENCES,0.6345177664974619,"u∈N(v)
˜Avu + 1

h(t)
v

.
(9)"
REFERENCES,0.6370558375634517,GAT and GraphSNNGAT
REFERENCES,0.6395939086294417,"Graph Attention Network (GAT) (Veliˇckovi´c et al., 2017) linearly transforms the input feature
vectors and performs a weighted sum of the feature vectors for vertices in a neighborhood after the
transformation. GAT computes attention weights α(t)
vu using an attention mechanism and aggregates
the feature vectors in a neighborhood as follows:"
REFERENCES,0.6421319796954315,"h(t+1)
v
= σ

X"
REFERENCES,0.6446700507614214,"(v,u)∈E
α(t)
vuW (t)h(t)
u

,
(10)"
REFERENCES,0.6472081218274112,"where W (t) is a trainable weight matrix and σ represents a non-linear activation function. We
generalise GAT to a model, called GraphSNNGAT , in the GMP framework. Firstly, we aggregate
the feature vectors based on structural coefﬁcients in our aggregation scheme, i.e., we compute"
REFERENCES,0.649746192893401,"˜h(t)
u = γ(t) X"
REFERENCES,0.6522842639593909,"z∈N(u)
˜Auz + 1

h(t)
u
q"
REFERENCES,0.6548223350253807,"| ˜
N(u)|| ˜
N(u)|
+
X"
REFERENCES,0.6573604060913706,z∈N(u)
REFERENCES,0.6598984771573604,"
˜Auz + 1

h(t)
z
q"
REFERENCES,0.6624365482233503,"| ˜
N(z)|| ˜
N(u)|
(11) and"
REFERENCES,0.6649746192893401,"˜h(t)
v
= γ(t) X"
REFERENCES,0.6675126903553299,"z′∈N(v)
˜Avz′ + 1

h(t)
v
q"
REFERENCES,0.6700507614213198,"| ˜
N(v)|| ˜
N(v)|
+
X"
REFERENCES,0.6725888324873096,z′∈N(v)
REFERENCES,0.6751269035532995,"
˜Avz′ + 1

h(t)
z′
q"
REFERENCES,0.6776649746192893,"| ˜
N(z′)|| ˜
N(v)|
.
(12)"
REFERENCES,0.6802030456852792,Published as a conference paper at ICLR 2022
REFERENCES,0.682741116751269,"We then construct attention coefﬁcients α(t)
vu on these aggregated feature vectors as follows:"
REFERENCES,0.6852791878172588,"α(t)
vu =
exp

LeakyReLU
 
aT [W (t)˜h(t)
v ||W (t)˜h(t)
u ]
 P"
REFERENCES,0.6878172588832487,"z∈N(v) exp

LeakyReLU
 
aT [W (t)˜h(t)
v ||W (t)˜h(t)
z ]
,
(13)"
REFERENCES,0.6903553299492385,"where || represents the concatenation, W (t) is a learnabe weight matrix and a is a learnable weight
vector. After that, we aggregate the neighborhood features as follows using attention coefﬁcients."
REFERENCES,0.6928934010152284,"h(t+1)
v
= σ

X"
REFERENCES,0.6954314720812182,"(v,u)∈E
α(t)
vuW (t)˜h(t)
u

,
(14)"
REFERENCES,0.6979695431472082,"where W (t) is a learnable weight matrix, and σ represents a non-linear activation function. We use
multi-head attention as stated in the original work Veliˇckovi´c et al. (2017)."
REFERENCES,0.700507614213198,GIN and GraphSNNGIN
REFERENCES,0.7030456852791879,"Graph Isomorphism Network (GIN) (Xu et al., 2019) takes the sum aggregation over a neighborhood,
followed by a 2-layer MLP. The ϵ(t+1) is a learnable parameter or ﬁxed scalar. Each neural layer is
expressed as:
h(t+1)
v
= MLP(t+1)
(1 + ϵ(t+1))h(t)
v +
X"
REFERENCES,0.7055837563451777,"u∈N(v)
h(t)
u

.
(15)"
REFERENCES,0.7081218274111675,"Here, we consider one of GIN variants employed in the original paper, where the learnable parameter
ϵ = 0, and generalise it to GraphSNNGIN as deﬁned bwlow:"
REFERENCES,0.7106598984771574,"h(t+1)
v
= MLP(t+1)
γ(t) X"
REFERENCES,0.7131979695431472,"u∈N(v)
˜Avu + 1

h(t)
v +
X"
REFERENCES,0.7157360406091371,u∈N(v)
REFERENCES,0.7182741116751269,"
˜Avu + 1

h(t)
u

.
(16)"
REFERENCES,0.7208121827411168,B. EXPERIMENTS
REFERENCES,0.7233502538071066,"DATASETS
Table 10 contains the statistics for the ﬁve datasets used in our experiments for node classiﬁcation in
Section 5.1."
REFERENCES,0.7258883248730964,"Dataset
Type
#Nodes
#Edges
#Classes
#Features
Cora
Citation network
2,708
5,429
7
1,433
Citeseer
Citation network
3,327
4,732
6
3,703
Pubmed
Citation network
19,717
44,338
3
500
NELL
Knowledge graph
65,755
266,144
210
5,414
ogbn-arxiv
Citation network
169,343
1,166,243
40
128"
REFERENCES,0.7284263959390863,Table 10: Statistics for node classiﬁcation datasets.
REFERENCES,0.7309644670050761,"Table 11 below contains the statistics for the datasets used in our experiments on small graph
classiﬁcation in Section 5.2, as well as the datasets used in an additional experiment for graph
classiﬁcation following the data splits and experimental setup in (Errica et al., 2020). The results of
this additional experiment are reported under Section “Graph Classiﬁcation using Setup (Errica et al.,
2020)” in Appendix B."
REFERENCES,0.733502538071066,"Table 12 contains the statistics for the ﬁve large graph datasets from from Open Graph Benchmark
(OGB) Hu et al. (2020), used in our experiments for large graph classiﬁcation in Section 5.2."
REFERENCES,0.7360406091370558,EXPERIMENTAL SETUP ON SMALL GRAPHS
REFERENCES,0.7385786802030457,"Previously, several experimental setups have been considered for evaluating graph classiﬁcation on
small graphs in TUD benchmark datasets (https://chrsmrrs.github.io/datasets/).
All the baseline methods in our paper use the 10-fold cross validation technique. However, they differ
in how they split training/validation/testing data and how they report the ﬁnal results in terms of
classiﬁcation accuracy. Below, we discuss the details of their experimental setups."
REFERENCES,0.7411167512690355,Published as a conference paper at ICLR 2022
REFERENCES,0.7436548223350253,"Dataset
#Graphs
Avg # Nodes
Avg # Edges
#Classes
MUTAG
188
17.93
19.79
2
PTC-MR
344
14.29
14.69
2
BZR
405
35.75
38.36
2
COX2
467
41.22
43.45
2
ENZYMES
600
32.63
64.14
6
IMDB-B
1000
19.77
96.53
2
PROTEINS
1113
39.06
72.82
2
D & D
1178
284.32
715.66
2
NCI1
4110
29.87
32.30
2
RDT-M5K
5000
508.52
594.87
5
COLLAB
5000
74.49
2457.78
3"
REFERENCES,0.7461928934010152,Table 11: Statistics for small graph classiﬁcation datasets.
REFERENCES,0.748730964467005,"Dataset
#Graphs
Avg # Nodes
Avg # Edges
#Tasks
Task Type
ogbg-molmolhiv
41,127
25.5
27.5
1
Binary classiﬁcation
ogbg-moltox21
7,831
18.6
19.3
12
Binary classiﬁcation
ogbg-moltoxcast
8,576
18.8
19.3
617
Binary classiﬁcation
ogbg-molpcba
437,929
26.0
28.1
128
Binary classiﬁcation
ogbg-ppa
158,100
243.4
2,266.1
1
Multi-class classiﬁcation"
REFERENCES,0.751269035532995,Table 12: Statistics for large graph classiﬁcation dataset (OGB graph datasets).
REFERENCES,0.7538071065989848,"• CapsGNN (Xinyi & Chen, 2018) splits the datasets into 80 % for training, 10 % for
validation, and 10 % for testing. The training is stopped when the performance on the
validation set goes to the highest. Then they obtain the test set accuracy that corresponds to
the epoch with the highest validation accuracy in each fold. The ﬁnal results are reported by
computing the mean accuracy and standard deviation over 10 folds."
REFERENCES,0.7563451776649747,"• DGCNN Zhang et al. (2018a) splits the datasets into 90 % for training and 10 % for testing.
They obtain the test accuracy of the last epoch in each fold. They report the ﬁnal results by
computing the mean accuracy and standard deviation on the test accuracy over 10 folds."
REFERENCES,0.7588832487309645,"• GIN and GraphSAGE (Xu et al., 2019) split the datasets into 90 % for training and 10 % for
testing. They average the test accuracy on 10 folds and select the epoch with the highest
averaged accuracy. Then they report the ﬁnal results by computing the mean accuracy and
standard deviation based on the selected epoch."
REFERENCES,0.7614213197969543,"• FGW (Titouan et al., 2019) splits the datasets into 90 % for training and 10 % for testing.
Then, they use the nested cross validation technique on the same folds, and repeat the
process 10 times. They report the ﬁnal results by computing the mean accuracy and standard
deviation."
REFERENCES,0.7639593908629442,"• The other baseline methods split the datasets into 90 % for training and 10 % for testing,
and repeat their experiment 10 times. Then they report the ﬁnal results by computing the
mean accuracy and standard deviation."
REFERENCES,0.766497461928934,"In our work, we split the datasets into 90 % for training and 10 % for testing. We obtain the best
validation accuracy on each fold. Then we report the ﬁnal results by computing the mean accuracy
and standard deviation over 10 folds1."
REFERENCES,0.7690355329949239,NODE CLASSIFICATION USING RANDOM SPLITS
REFERENCES,0.7715736040609137,"Following the work Pei et al. (2020), we randomly split graph nodes into 60%, 20% and 20% for
training, validation and testing, respectively. The other hyperparameter settings are the same as
in Section 5.1. Table 13 shows the results. We see that our models consistently outperform all of
the baseline methods on all benchmark datasets. Speciﬁcally, GraphSNGCN improves upon GCN
by a margin of 1.5%, 1.7%, 1.6% and 2.4% on Cora, Citeseer, Pubmed and NELL, respectively."
REFERENCES,0.7741116751269036,1The implementation can be found at: https://github.com/wokas36/GraphSNN
REFERENCES,0.7766497461928934,Published as a conference paper at ICLR 2022
REFERENCES,0.7791878172588832,"GraphSNGAT improves upon GAT by 1.3%, 1.6% and 2.0% on Cora, Citeseer and Pubmed, respec-
tively. GraphSNGIN improves upon GIN by 3.8%, 1.7%, 1.8% and 1.6% on Cora, Citeseer, Pubmed
and NELL, respectively. GraphSNGraphSAGE improves upon GraphSAGE by 1.3%, 1.7%, 1.1% and
2.3% on Cora, Citeseer, Pubmed and NELL, respectively."
REFERENCES,0.7817258883248731,"Method
Cora
Citeseer
Pubmed
NELL
GCN
85.7 ± 1.6
73.6 ± 1.0
88.1 ± 1.2
72.2 ± 5.6
GraphSNNGCN
87.2 ± 1.5
75.3 ± 1.3
89.7 ± 1.7
74.6 ± 6.3
GAT
86.3 ± 0.3
74.3 ± 0.3
87.6 ± 0.1
-
GraphSNNGAT
87.6 ± 0.9
75.9 ± 0.8
89.6 ± 0.6
-
GIN
82.5 ± 0.8
70.8 ± 1.9
85.0 ± 1.5
66.7 ± 3.3
GraphSNNGIN
86.3 ± 0.7
72.5 ± 1.5
86.8 ± 1.2
68.3 ± 3.7
GraphSAGE
86.8 ± 1.9
74.2 ± 1.8
88.3 ± 1.1
69.4 ± 4.3
GraphSNNGraphSAGE
88.1 ± 1.5
75.9 ± 1.3
89.4 ± 2.4
71.7 ± 4.5"
REFERENCES,0.7842639593908629,Table 13: Classiﬁcation accuracy (%) averaged over 10 random splits on node classiﬁcation.
REFERENCES,0.7868020304568528,"GRAPH CLASSIFICATION USING SETUP (ERRICA ET AL., 2020)"
REFERENCES,0.7893401015228426,"Following the data splits and experiment setup introduced in (Errica et al., 2020), we further evaluate
our method. The experimental setup in (Errica et al., 2020) provides a fair performance comparison
process on GNN methods. The evaluation process has two different phases: (1) model selection on
the validation set, (2) model assessment on the test set. More speciﬁcally, they ﬁrst split the datasets
into 90 % for training and 10 % for testing. Then the entire training set is further split into 90% of
training and 10% of validation. They apply the inner hold-out method to select the best model based
on validation accuracy. After selecting the best model, they train the model three times on the entire
training set with early stopping."
REFERENCES,0.7918781725888325,"We have conducted experiments on four bioinformatics datasets (NCI1, PROTEINS, ENZYMES and
D&D) and three social network datasets (COLLAB, IMDB-B and REDDIT-5k) with node features.
The results of the baseline, DGCNN and GIN are taken from the paper (Errica et al., 2020). Note that
the ﬁnal results of DGCNN and GIN from the paper (Errica et al., 2020) are reported by computing
the mean accuracy and standard deviation on the test set in these three runs, which are different from
the original papers of DGCNN and GIN. Table 14 shows the results."
REFERENCES,0.7944162436548223,"Method
NCI1
PROTEINS
ENZYMES
D&D
COLLAB
IMDB-B
REDDIT-5k
Baseline
69.8±2.2
75.8 ± 3.7
65.2±6.4
78.4 ± 4.5
70.2±1.5
70.8±5.0
52.2±1.5
DGCNN
76.4±1.7
72.9±3.5
38.9±5.7
76.6±4.3
71.2±1.9
69.2±3.0
49.2±1.2
GIN
80.0±1.4
73.3±4.0
59.6±4.5
75.3±2.9
75.6±2.3
71.2±3.9
56.1±1.7
GraphSNN
81.6 ± 2.8
74.5 ± 3.5
61.7 ± 3.4
77.1 ± 3.3
77.0 ± 3.1
72.3 ± 3.6
57.1 ± 3.1"
REFERENCES,0.7969543147208121,Table 14: Classiﬁcation accuracy (%) averaged over 10 runs on graph classiﬁcation.
REFERENCES,0.799492385786802,GRAPH CLASSIFICATION ON OGB GRAPH DATASETS
REFERENCES,0.8020304568527918,"Table 15 shows the results for the running time of the prepocessing step in our method GraphSNN
for large graph datasets (averaged over 5 runs). Note that the preprocessing step can be parallellized
efﬁciently at the node level. The CPU time shows the total preprocessing time of a dataset in which
each node is preprocessed sequentially, and the CPU time per node shows the average preprocessing
time per node."
REFERENCES,0.8045685279187818,OVERSMOOTHING ANALYSIS
REFERENCES,0.8071065989847716,"We have also conducted further experiments to analyze the effectiveness of our method in alleviating
the over-smoothing issue. We compare GIN (i.e., a spatial GNN), DFNets (Wijesinghe & Wang,
2019) (i.e., a spectral GNN), GraphSNNGIN and GraphSNNGCN. For a fair comparison, we remove
the dense-net architecture of DFNets and use the same hyperparameters as in the original paper. We"
REFERENCES,0.8096446700507615,Published as a conference paper at ICLR 2022
REFERENCES,0.8121827411167513,"Dataset
CPU time (seconds)
CPU time per node (milliseconds)
ogbg-molhiv
66.97
0.06383
ogbg-moltox21
79.37
0.54565
ogbg-moltoxcast
380.84
2.36417
ogbg-ppa
820.12
4.71235"
REFERENCES,0.8147208121827412,Table 15: Running time of the prepocessing step for large graph datasets averaged over 5 runs.
REFERENCES,0.817258883248731,"evaluate all models over the cora dataset using the standard splits. The classiﬁcation accuracy is
averaged over 10 runs on node-classiﬁcation."
REFERENCES,0.8197969543147208,"#Layers
GIN
GraphSNNGIN
DFNet
GraphSNNGCN
1
73.3±1.5
76.1±1.6
80.5±0.6
80.1±0.8
2
77.6±1.3
79.2±1.7
81.9±0.5
83.1±1.8
3
75.2±1.7
78.5±1.3
82.6±0.3
82.0±0.8
4
48.6±2.1
77.2±2.3
80.7±0.6
80.1±0.7
5
40.3±1.9
75.9±2.1
75.6±0.3
79.1±1.2
6
36.1±2.3
73.3±1.8
65.3±1.3
76.5±1.3
7
27.5±2.1
71.9±1.5
60.9±1.5
76.3±1.3
8
20.3±1.8
69.3±2.2
53.6±1.3
75.7±1.2"
REFERENCES,0.8223350253807107,Table 16: Oversmoothing analysis of GIN and spectral GNN (DFNet) on cora dataset.
REFERENCES,0.8248730964467005,"GraphSNN can alleviate over-smoothing is because structural coefﬁcients capture structural connec-
tivity between a target vertex and its neighbors. Thus, a neighbor whose structural connectivity is
weak would pass little message to the target vertex, whereas a neighbor whose structural connectivity
is strong would pass strong message to the target vertex."
REFERENCES,0.8274111675126904,"Figure 4 shows the results of GCN and GraphSNNGCN on the datasets Cora, Citeseer and Pubmed,
in terms of classiﬁcation accuracy averaged over 10 runs in the setting of standard splits."
REFERENCES,0.8299492385786802,"1
2
3
4
5
6
7
8
Number of Layers 20 30 40 50 60 70 80 90"
REFERENCES,0.8324873096446701,Accuracy
REFERENCES,0.8350253807106599,(a) Cora
REFERENCES,0.8375634517766497,"1
2
3
4
5
6
7
8
Number of Layers 20 30 40 50 60 70 80"
REFERENCES,0.8401015228426396,"90
(b) Citeseer"
REFERENCES,0.8426395939086294,"1
2
3
4
5
6
7
8
Number of Layers 20 30 40 50 60 70 80"
REFERENCES,0.8451776649746193,"90
(c) Pubmed"
REFERENCES,0.8477157360406091,"GraphSNGCN
GCN"
REFERENCES,0.850253807106599,Figure 4: Oversmoothing analysis w.r.t. the model depth for node classiﬁcation.
REFERENCES,0.8527918781725888,ABLATION STUDY WITH AUGMENTED NODE FEATURES
REFERENCES,0.8553299492385786,"We consider an experimental evaluation setup called BL, which serves as the baseline for all experi-
ments in this ablation study. In the setting of BL, the AGGREGATEI in GraphSNN is set to 1. Then,
different variants of BL consider different local substructure counts as additional node features. This
allows us to analyse what types of local substructures our proposed architecture can distinguish."
REFERENCES,0.8578680203045685,There are ﬁve variants of BL being considered in the ablation study:
REFERENCES,0.8604060913705583,"(1) BLSC: Setting AGGREGATIONI of GraphSNN to 1 and keeping structural coefﬁcients for
neighbors."
REFERENCES,0.8629441624365483,"(2) BLclique
NF
: Setting AGGREGATIONI of GraphSNN to 1, removing structural coefﬁcients
for neighbors, and adding additional node features (triangle and 4-clique counts) into the
original feature vectors."
REFERENCES,0.8654822335025381,Published as a conference paper at ICLR 2022
REFERENCES,0.868020304568528,"Method
GSN-v
BLclique
NF
BLSC
BLclique
SC+NF
GraphSNN
MUTAG
92.20±7.5
90.21±2.3
94.06±2.4
95.16±2.5
94.70±1.9
PTC-MR
67.40±5.7
67.13±2.9
70.18±3.1
71.04±3.1
70.58±3.1
PROTEINS
74.59±5.0
76.42±2.6
78.05±2.3
78.66±2.1
78.42±2.7
BZR
-
86.82±3.1
90.67±3.1
91.98±3.2
91.12±3.0
IMDB-B
76.80±2.0
77.00±3.1
77.23±2.8
78.53±2.9
78.01±2.8"
REFERENCES,0.8705583756345178,"Table 17: Analysis the effects of our structural coefﬁcients with substructure counts, i.e, triangle and
4-clique counts. Classiﬁcation accuracy (%) averaged over 10 runs on graph classiﬁcation."
REFERENCES,0.8730964467005076,"Method
ID-GNN
BLcycle
NF
BLSC
BLcycle
SC+NF
GraphSNN
MUTAG
96.50±3.2
91.36±2.1
94.06±2.4
96.61±2.3
94.70±1.9
PTC-MR
61.90±5.4
67.57±3.3
70.18±3.1
71.76±3.2
70.58±3.1
PROTEINS
78.00±3.5
77.26±2.5
78.05±2.3
78.95±2.5
78.42±2.7
BZR
86.40±3.0
86.83±3.3
90.67±3.1
91.75±3.4
91.12±3.0
IMDB-B
-
76.36±2.6
77.23±2.8
78.58±2.4
78.01±2.8"
REFERENCES,0.8756345177664975,"Table 18: Analysis the effects of our structural coefﬁcients with substructure counts, i.e, cycle counts.
Classiﬁcation accuracy (%) averaged over 10 runs on graph classiﬁcation."
REFERENCES,0.8781725888324873,"(3) BLclique
SC+NF : Setting AGGREGATIONI of GraphSNN to 1, keeping structural coefﬁcients
for neighbors, and adding additional node features (triangle and 4-clique counts) into the
original feature vectors."
REFERENCES,0.8807106598984772,"(4) BLcycle
NF : Setting AGGREGATIONI of GraphSNN to 1, removing structural coefﬁcients
for neighbors, and adding additional node features (cycle counts) into the original feature
vectors."
REFERENCES,0.883248730964467,"(5) BLcycle
SC+NF : Setting AGGREGATIONI of GraphSNN to 1, keeping structural coefﬁcients
for neighbors, and adding additional node features (cycle counts) into the original feature
vectors."
REFERENCES,0.8857868020304569,"We compare GraphSNN with GSN-v (Bouritsas et al., 2020), BLclique
NF
, BLSC, and BLclique
SC+NF to
analyze how our proposed architecture relates to the models with triangle and 4 clique counts as
additional node features. Similarly, we compare GraphSNN with ID-GNNs (You et al., 2021),
BLcycle
NF , BLSC, and BLcycle
SC+NF to analyze how our proposed architecture relates to the models
with cycle counts as additional node features. We concatenate the counts of cycles with length 1
to 4 starting and ending at the given source node with its original feature vector as in (You et al.,
2021). Table 17 and Table 18 show the experimental results. As AGGREGATEI is set to 1 in
the setting of BL, the performance gap between BLNF and BLSC+NF reﬂects the effectiveness of
structural coefﬁcients on enhancing relational inference between a target vertex and its neighbors. The
performance gap between BLSC and GraphSNN above shows the effectiveness of AGGREGATEI
in our proposed model GraphSNN. Furthermore, BLSC+NF consistently performs best since we
incorporate both extra node features and structural coefﬁcients into the feature aggregation. There is
a small performance gap between BLSC+NF and GraphSNN due to augmented node features that
can capture additional structural information that cannot be captured using structural coefﬁcients."
REFERENCES,0.8883248730964467,C. PROOFS FOR LEMMAS AND THEOREMS
REFERENCES,0.8908629441624365,Proof for Theorem 1
REFERENCES,0.8934010152284264,"Theorem 1. The following statements are true: (a) If Si ≃subgraph Sj, then Si ≃overlap Sj; but not
vice versa; (b) If Si ≃overlap Sj, then Si ≃subtree Sj; but not vice versa."
REFERENCES,0.8959390862944162,"Proof. In the following, we prove the statements in this theorem one by one."
REFERENCES,0.8984771573604061,"For Statement (a), by Si ≃subgraph Sj and Deﬁnition 1, we know that there exists a bijective mapping
g′ : ˜
N(i) →˜
N(j) such that for the vertex i and any vertex v′ ∈N(i), i and v′ are adjacent in Si
iff j = g(i) and u′ = g(v′) are adjacent in Sj, and hi = hj and hv′ = hu′, where g is a bijective
mapping between Si and Sj as deﬁned by Deﬁnition 1. Then for each pair of overlap subgraphs Siv′"
REFERENCES,0.9010152284263959,Published as a conference paper at ICLR 2022
REFERENCES,0.9035532994923858,"and Sju′, we can further extend g′ along g on Siv′ and Sju′. That is, g′(v) = u iff g(v) = u. If v in
Siv′, by the deﬁnition of overlap subgraph, v must either be i or a neighbor of i. Hence u = g′(v)
in this case must be either j or a neighbor of j. By the deﬁnition of g and the fact that g′(v) = u
iff g(v) = u, we know that for any two vertices v1 and v2 in Siv′, they are adjacent in Siv′ iff their
corresponding vertices g′(v1) and g′(v2) are adjacent in Sju′ and their corresponding feature vectors
are indistinguishable, i.e, Siv′ ≃subgraph Sju′ for any v′ ∈N(i) and g(v′) = u′. Conversely, if
Si ≃overlap Sj, then it is possible that Si ̸≃subgraph Sj as shown by the two graphs in Figure 2(a)."
REFERENCES,0.9060913705583756,"For Statement (b), if Si ≃overlap Sj, then to prove Si ≃subtree Sj we need to show that there exists
a bijective mapping g : ˜
N(i) →˜
N(j) such that g(i) = j and, for any v′ ∈˜
N(i) and g(v′) = u′, the
feature vectors of v′ and u′ are indistinguishable, i.e., hv′ = hu′. By Def. 2, we can ﬁnd a bijective
mapping g′ : ˜
N(i) →˜
N(j) such that g′(i) = j and, for any v′ ∈N(i) and g′(v′) = u′, Siv′ and
Sju′ are subgraph-isomorphic. This implies that g′ cannot distinguish the feature vectors of v′ and
u′ for any v′ ∈˜
N(i) and g(v′) = u′. Similarly, the converse does not necessarily hold and one
counterexample is the set of graphs as shown in Figure 2(b) which are subtree-isomorphic but not
overlap-isomorphic."
REFERENCES,0.9086294416243654,Proof for Theorem 2
REFERENCES,0.9111675126903553,"Theorem 2. Let M be a GNN. M is as powerful as 1-WL in distinguishing non-isomorphic graphs
if M has a sufﬁcient number of layers and each layer can map any Si and Sj in S into two different
embeddings (i.e., ζ(Si) ̸= ζ(Sj)) if and only if Si ̸≃subtree Sj."
REFERENCES,0.9137055837563451,"Proof. We ﬁrst show that, for any two graphs G1 and G2, if they can be distinguished by 1-WL,
then they must be distinguishable by such a GNN M as well. Suppose that 1-WL takes k iterations
to distinguish G1 and G2, i.e., 1-WL yields the same multiset of node labels on G1 and G2 in the
iterations from 0 to k-1, but two different multisets of node labels on G1 and G2 in the k-th iteration.
To derive a contradiction, we assume that a GNN M that satisﬁes the above two conditions cannot
distinguish G1 and G2 in the iterations from 0 to k. Since 1-WL can distinguish G1 and G2 in the
k-th iteration, it means that there must exist two neighborhood subgraphs, say Si and Sj, which
correspond to two different multisets of node labels on G1 and G2 at the k-th iteration. These two
different multisets of node labels correspond to two different multisets of feature vectors in their
neighborhoods, i.e., {{hv|v ∈N(i)}} ̸= {{hu|u ∈N(j)}}. By Def. 3, we know that Si ̸≃subtree Sj.
Then this means that ζ(Si) ̸= ζ(Sj), which contradicts the assumption that M cannot distinguish G1
and G2 in the iteration k."
REFERENCES,0.916243654822335,"Now, we show the other direction that, for any two graphs G1 = (V1, E1) and G2 = (V2, E2), if
they can be distinguished by such a GNN M, then they must be distinguishable by 1-WL. Similarly,
suppose that at the k-th iteration, M maps the neighborhood subgraphs of these two graphs into two
different multisets of node embeddings, i.e., {{ζ(Sv)|v ∈V1}} ̸= {{ζ(Su)|v ∈V2}}. This is means
that we can ﬁnd at least two different neighborhood subgraphs Si and Sj such that ζ(Si) ̸= ζ(Sj).
For such neighborhood subgraphs Si and Sj, we know that Si ̸≃subtree Sj. Then this means that
Si and Sj correspond to either hi ̸= hj or {{hv|v ∈N(i)}} ̸= {{hu|u ∈N(j)}}, which can be
relabeled by 1-WL into two different new labels. Thus, 1-WL can also distinguish such neighborhood
subgraphs, and accordingly distinguish G1 and G2."
REFERENCES,0.9187817258883249,The proof is completed.
REFERENCES,0.9213197969543148,Proof for Theorem 3
REFERENCES,0.9238578680203046,"Theorem 3. Let M be a GNN whose aggregation scheme Φ is deﬁned by Eq. 1-Eq. 3. M is strictly
more expressive than 1-WL in distinguishing non-isomorphic graphs if M has a sufﬁcient number of
layers and also satisﬁes the following conditions:"
REFERENCES,0.9263959390862944,"(1) M can distinguish at least two neighborhood subgraphs Si and Sj with Si ≃subtree Sj,
Si ̸≃subgraph Sj and {{ ˜Aiv′|v′ ∈N(i)}} ̸= {{ ˜Aju′|u′ ∈N(j)}};"
REFERENCES,0.9289340101522843,"(2) Φ

h(t)
v , {{h(t)
u |u ∈N(v)}}, {{( ˜Avu, h(t)
u )|u ∈N(v)}}

is injective."
REFERENCES,0.9314720812182741,Published as a conference paper at ICLR 2022
REFERENCES,0.934010152284264,"Proof. We prove this theorem in two steps. First, we prove that a GNN M satisfying the above
conditions can distinguish any two graphs that are distinguishable by 1-WL by contradiction. Assume
that there exist two graphs G1 and G2 which can be distinguished by 1-WL but cannot be distinguished
by M. Further, suppose that 1-WL cannot distinguish these two graphs in the iterations from 0 to k-1,
but can distinguish them in the k-th iteration. Then, there must exist two neighborhood subgraphs
Si and Sj whose neighboring nodes correspond to two different multisets of node labels at the k-th
iteration, i.e., {{h(k)
v |v ∈N(i)}} ̸= {{h(k)
u |u ∈N(j)}}. By the above condition (2), we know that Φ
is injective. Thus, for Si and Sj, Φ would yield two different feature vectors at the k-th iteration.
This means that M can also distinguish G1 and G2, which contradicts the assumption. Our proof in
the ﬁrst step is done. For the second step, we can prove that there exist at least two graphs that can be
distinguished by M but cannot be distinguished by 1-WL. Figure 1 presents two of such graphs."
REFERENCES,0.9365482233502538,Proof for Theorem 4
REFERENCES,0.9390862944162437,"We consider that, for each vertex in a graph, its node features are from a countable set; similarly, for
each pair of adjacent vertices in a graph, its structural coefﬁcient is also from a countable set. Assume
that H, A and W are countable sets where H is a node feature space, A is a structural coefﬁcient
space, and W = {Aijhi|Aij ∈A, hi ∈H}. Let H and W be two multisets containing elements
from H and W, respectively, and |H| = |W|."
REFERENCES,0.9416243654822335,"Lemma 1. There exists a function f s.t. π(H, W) = P"
REFERENCES,0.9441624365482234,"h∈H,w∈W f(h, w) is unique for any distinct
pair of multisets (H, W)."
REFERENCES,0.9467005076142132,"Proof. Since H and W are countable, there must exist two functions ψ1 : H →Nodd mapping h ∈H
to odd natural numbers and ψ2 : W →Neven mapping w ∈W to even natural numbers. Further,
for any pair of multisets (H, W), since the cardinality of H and W is bounded, there must exist a
number N ∈N such that |H| < N and |W| < N. Thus, we can ﬁnd a prime number P > 2N. Then
we have a mapping f as f(h, w) = P −ψ1(h) + P −ψ2(w) such that P"
REFERENCES,0.949238578680203,"h∈H,w∈W f(h, w) is unique for
each distinct pair of (H, W)."
REFERENCES,0.9517766497461929,"Lemma 2. There exists a function f s.t. π′(hv, H, W) = γf(hv, |H|hv) + P"
REFERENCES,0.9543147208121827,"h∈H,w∈W f(h, w) is
unique for any distinct (hv, H, W), where hv ∈H, |H|hv ∈W, and γ can be an irrational number."
REFERENCES,0.9568527918781726,"Proof. As hv ∈H and |H|hv ∈W, we may have f(hv, |H|hv) = P −ψ1(hv) + P −ψ1(|H|hv) where
ψ1 : H →Nodd and ψ2 : W →Neven as deﬁned in the proof for Lemma 1. Let (hv1, H1, W1) and
(hv2, H2, W2) be two different tuples. Then, there are two cases:"
REFERENCES,0.9593908629441624,"(1) When
hv1
=
hv2
but
(H1, W1)
̸=
(H2, W2),
by
Lemma
1,
we
know
that P"
REFERENCES,0.9619289340101523,"h∈H1,w∈W1 f(h, w)
̸=
P"
REFERENCES,0.9644670050761421,"h∈H2,w∈W2 f(h, w).
Thus, π′(hv1, H1, W1)
̸=
π′(hv2, H2, W2)."
REFERENCES,0.9670050761421319,"(2) When hv1 ̸= hv2, we prove π′(hv1, H1, W1) ̸= π′(hv2, H2, W2) by contradiction. Assume
that π′(hv1, H1, W1) = π′(hv2, H2, W2). Then, we have:"
REFERENCES,0.9695431472081218,"γf(hv1, |H1|hv1) +
X"
REFERENCES,0.9720812182741116,"h∈H1,w∈W1
f(h, w) = γf(hv2, |H2|hv2) +
X"
REFERENCES,0.9746192893401016,"h∈H2,w∈W2
f(h, w)."
REFERENCES,0.9771573604060914,This gives us the following equation:
REFERENCES,0.9796954314720813,"γ

f(hv1, |H1|hv1)−f(hv2, |H2|hv2)

=

X"
REFERENCES,0.9822335025380711,"h∈H2,w∈W2
f(h, w)

−

X"
REFERENCES,0.9847715736040609,"h∈H1,w∈W1
f(h, w)

."
REFERENCES,0.9873096446700508,"When γ is an irrational number, L.H.S. of the above equation is irrational but R.H.S. is
rational. There is a contradiction. Thus, π′(hv1, H1, W1) ̸= π′(hv2, H2, W2)."
REFERENCES,0.9898477157360406,"Based on Lemma 1 and Lemma 2, we can prove the following theorem."
REFERENCES,0.9923857868020305,Published as a conference paper at ICLR 2022
REFERENCES,0.9949238578680203,Theorem 4. GraphSNN is more expressive than 1-WL in testing non-isomorphic graphs.
REFERENCES,0.9974619289340102,"Proof. We prove this theorem by showing that GraphSNN is a GNN satisfying the conditions stated
in Theorem 3. For the ﬁrst condition, consider the two graphs shown in Figure 1. GraphSNN can
distinguish these two neighborhood subgraphs Si and Sj with {{ ˜Aiv′|v′ ∈N(i)}} ̸= {{ ˜Aju′|u′ ∈
N(j)}}. For the second condition, by Lemmas 1 and 2 as well as the fact that MLP as a universal
approximator (Xu et al., 2019) can be used to model and learn the functions f and g, we know that
GraphSNN also satisﬁes this condition."
