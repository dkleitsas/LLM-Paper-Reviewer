Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.004366812227074236,"The study of first-order optimization is sensitive to the assumptions made on the
objective functions. These assumptions induce complexity classes which play a
key role in worst-case analysis, including the fundamental concept of algorithm
optimality. Recent work argues that strong convexity and smoothnessâ€”popular
assumptions in literatureâ€”lead to a pathological definition of the condition number
[12]. Motivated by this result, we focus on the class of functions satisfying a
lower restricted secant inequality and an upper error bound. On top of being
robust to the aforementioned pathological behavior and including some non-convex
functions, this pair of conditions displays interesting geometrical properties. In
particular, the necessary and sufficient conditions to interpolate a set of points and
their gradients within the class can be separated into simple conditions on each
sampled gradient. This allows the performance estimation problem (PEP, [7, 33])
to be solved analytically, leading to a lower bound on the convergence rate that
proves gradient descent to be exactly optimal on this class of functions among all
first-order algorithms."
INTRODUCTION,0.008733624454148471,"1
Introduction"
INTRODUCTION,0.013100436681222707,"The typical framework to study convergence properties of first-order algorithms in the context of
machine learning is to first establish a class of objective functions to optimize through assumptions
usually bound to a constant, such as L-smoothness and Âµ-strong convexity. A tuning prescription of
an algorithm is then made based on the constants, (e.g. step size Î± =
2
Âµ+L in the case of the gradient
descent method on smooth and strongly convex functions), and finally a worst-case convergence rate
can be derived for this algorithm when using this tuning prescription. In some cases, a lower bound on
the achievable worst-case convergence rate can also be derived, leading to the theoretical optimality
of an algorithm on the considered class of function, for instance the Nesterov accelerated gradient
method [27] is known to be optimal up to a constant on strongly convex and smooth functions."
INTRODUCTION,0.017467248908296942,"However in [12], the authors establish that such framework and its derived results are very sensitive
to the choice of assumptions, and that strong convexity and smoothness can exhibit pathological
behaviors leading to conservative tuning and arbitrarily sub-optimal convergence rates, even when the
resulting algorithm achieves theoretical worst-case optimality on this class of functions. Furthermore,
they propose a set of more robust alternative conditions. In this work, we focus on a specific pair
of such alternative conditions : lower restricted secant inequality (RSIâˆ’) and upper error bounds
(EB+). Our main contribution is to show that the gradient descent (GD) method with a certain tuning"
INTRODUCTION,0.021834061135371178,"is exactly optimal on the classes of objective functions induced by these conditions, confirming that
optimality results are highly sensitive to the choice of conditions. Another consequence is that no
algorithm can accelerate on this class of functions, implying that additional assumptions are required
to explain the practical efficiency of accelerated methods."
INTRODUCTION,0.026200873362445413,"Notation
Let F the set of differentiable functions from Rd to R that admit a convex set of global
minima Xâˆ—
f. We focus on the problem of optimizing a function f âˆˆF, i.e. finding x âˆˆXâˆ—
f. For any
x âˆˆRd, we denote xâˆ—
f the orthogonal projection of x on Xâˆ—
f. By abuse of notation, when the context
is not ambiguous, we will simply denote xâˆ—
f as xâˆ—."
INTRODUCTION,0.03056768558951965,"We call gradient descent (GD) the standard optimization algorithm based on the following update,
where Î± is the step size :
xi+1 = xi âˆ’Î±âˆ‡f(xi)"
INTRODUCTION,0.034934497816593885,"We call first-order algorithm all A that consider past iterates, function values and gradients and output
a next iterate. Formally, A can be seen as a sequence of functions {An | n âˆˆN} such that for any
n âˆˆN, An is a function defined on

Rd Ã— R Ã— Rdn+1 with values in Rd. Under that formalism,
applying A to optimize an objective function f starting in x0 âˆˆRd generates a sequence of iterates
(xi)i such that âˆ€i, xi+1 = Ai

(xj, f(xj), âˆ‡f(xj))jâ‰¤i

. Note that we do not require the iterates of
A to lie within the span of observed gradients as it is often the case in the literature."
INTRODUCTION,0.039301310043668124,"Outline
Section 2 introduces RSIâˆ’and EB+ and provides some basic properties and motivation.
Section 3 discusses related works in the literature. Section 4 defines and establishes the necessary
and sufficient interpolation conditions for RSIâˆ’and EB+, which is a key element of our analysis.
Section 5 proves the lower bound on the convergence rate of first-order algorithms under RSIâˆ’and
EB+, and finally Section 6 concludes our work. Detailed proofs are provided in the appendix."
LIT REVIEW,0.043668122270742356,"2
Lower restricted secant inequality and upper error bounds"
LIT REVIEW,0.048034934497816595,We now define RSIâˆ’and EB+ and discuss some basic properties.
LIT REVIEW,0.05240174672489083,Definition 2.1 (Lower restricted secant inequality) Let f âˆˆF and Âµ > 0
LIT REVIEW,0.056768558951965066,"f âˆˆRSIâˆ’(Âµ) â‡”âˆ€x âˆˆRd, âŸ¨âˆ‡f(x) | x âˆ’xâˆ—âŸ©â‰¥Âµ âˆ¥x âˆ’xâˆ—âˆ¥2
2"
LIT REVIEW,0.0611353711790393,"Intuitively, RSIâˆ’(Âµ) enforces that the further x is from Xâˆ—
f, the stronger the gradient of f in x must
be in the opposite direction of Xâˆ—
f."
LIT REVIEW,0.06550218340611354,"Remark 2.2 RSIâˆ’(Âµ) includes non-convex functions. However, it prevents flat landscapes outside
of Xâˆ—
f, and requires f to increase at least quadratically with the distance to Xâˆ—
f, as established in
[12] :
f âˆˆRSIâˆ’(Âµ) â‡’âˆ€x âˆˆRd, f(x) âˆ’f âˆ—â‰¥Âµ"
LIT REVIEW,0.06986899563318777,"2 âˆ¥x âˆ’xâˆ—âˆ¥2
2"
LIT REVIEW,0.07423580786026202,Definition 2.3 (Upper Error Bounds) Let f âˆˆF and L > 0
LIT REVIEW,0.07860262008733625,"f âˆˆEB+(L) â‡”âˆ€x âˆˆRd, âˆ¥âˆ‡f(x)âˆ¥2 â‰¤L âˆ¥x âˆ’xâˆ—âˆ¥2"
LIT REVIEW,0.08296943231441048,"EB+(L) thus enforces that the gradient of f is controlled by the distance to Xâˆ—
f."
LIT REVIEW,0.08733624454148471,"Remark 2.4 L-smoothness implies EB+(L) and Âµ-strong convexity implies RSIâˆ’(Âµ). However, one
must be careful before claiming that RSIâˆ’and EB+ are respectively weaker than strong convexity
and smoothness : for Âµ0 > Âµ1, RSIâˆ’(Âµ0) is neither stronger or weaker than Âµ1-strong convexity,
and for L0 < L1, EB+(L0) is neither stronger or weaker than L1-smoothness."
LIT REVIEW,0.09170305676855896,"Therefore, even when the objective function is smooth and strongly convex, considering convergence
results under RSIâˆ’and EB+ is relevant, as we might obtain better constants Âµ and L. Many"
LIT REVIEW,0.09606986899563319,"convergence results depend of the condition number Îº =
L
Âµ. Better constants leads to a better
condition number, and thus a potentially better convergence rate (including when the dependence in
the condition number Îº is the same or worse). As a consequence, machine learning problems with
strongly convex and smooth objective functions are all potential applications of results under RSIâˆ’"
LIT REVIEW,0.10043668122270742,"and EB+, provided we can obtain better constants under these conditions."
LIT REVIEW,0.10480349344978165,"Prop 1 (convergence rate of GD under RSIâˆ’and EB+) Let f âˆˆRSIâˆ’(Âµ)âˆ©EB+(L). Then
gradient descent with learning rate Î± =
Âµ
L2 on f will guarantee the following convergence rate:"
LIT REVIEW,0.1091703056768559,"âˆ¥xi âˆ’xâˆ—
i âˆ¥2
2 â‰¤

1 âˆ’Âµ2 L2"
LIT REVIEW,0.11353711790393013,"i
âˆ¥x0 âˆ’xâˆ—
0âˆ¥2
2
(1)"
LIT REVIEW,0.11790393013100436,"Moreover, when the learning rate is set to Î±0 =
1
2Âµ on the first step, and Î± =
Âµ
L2 on every other
step, gradient descent guarantees the following convergence rate:"
LIT REVIEW,0.1222707423580786,"âˆ¥xi âˆ’xâˆ—
i âˆ¥2
2 â‰¤âˆ¥âˆ‡f(x0)âˆ¥2
2
4Âµ2"
LIT REVIEW,0.12663755458515283,"
1 âˆ’Âµ2 L2"
LIT REVIEW,0.13100436681222707,"iâˆ’1
(2)"
LIT REVIEW,0.13537117903930132,"Proof. When Î± =
Âµ
L2 , we have

xi+1 âˆ’xâˆ—
i+1

2
2 â‰¤âˆ¥xi+1 âˆ’xâˆ—
i âˆ¥2
2
= âˆ¥xi âˆ’Î±âˆ‡f(xi) âˆ’xâˆ—
i âˆ¥2
2
= âˆ¥xi âˆ’xâˆ—
i âˆ¥2
2 âˆ’2Î± âŸ¨âˆ‡f(xi) | xi âˆ’xâˆ—
i âŸ©+ Î±2 âˆ¥âˆ‡f(xi)âˆ¥2
2
â‰¤

1 âˆ’2Î±Âµ + L2Î±2
âˆ¥xi âˆ’xâˆ—
i âˆ¥2
2"
LIT REVIEW,0.13973799126637554,"=

1 âˆ’Âµ2 L2"
LIT REVIEW,0.14410480349344978,"
âˆ¥xi âˆ’xâˆ—
i âˆ¥2
2 , (3)"
LIT REVIEW,0.14847161572052403,"which proves (1). To prove (2), we simply note that when using Î± =
1
2Âµ, we have:"
LIT REVIEW,0.15283842794759825,"âˆ¥x1 âˆ’xâˆ—
1âˆ¥2
2 â‰¤âˆ¥x0 âˆ’xâˆ—
0âˆ¥2
2 âˆ’2Î± âŸ¨âˆ‡f(x0) | x0 âˆ’xâˆ—
0âŸ©+ Î±2 âˆ¥âˆ‡f(x0)âˆ¥2
2 â‰¤1"
LIT REVIEW,0.1572052401746725,"Âµ âŸ¨âˆ‡f(x0) | x0 âˆ’xâˆ—
0âŸ©âˆ’1"
LIT REVIEW,0.1615720524017467,"Âµ âŸ¨âˆ‡f(x0) | x0 âˆ’xâˆ—
0âŸ©+ âˆ¥âˆ‡f(x0)âˆ¥2
2
4Âµ2"
LIT REVIEW,0.16593886462882096,"= âˆ¥âˆ‡f(x0)âˆ¥2
2
4Âµ2
. (4) â– "
LIT REVIEW,0.1703056768558952,"Interestingly, RSIâˆ’and EB+ are direct bounds on the two additional terms obtained by developing
âˆ¥xi âˆ’Î±âˆ‡f(xi) âˆ’xâˆ—
i âˆ¥2
2, leading to an extremely simple proof. On the intuitive level, RSIâˆ’lower
bounds the gain from stepping in the direction of Xâˆ—
f, while EB+ upper bounds the error coming
from the component of the gradient orthogonal to that direction."
LIT REVIEW,0.17467248908296942,Remark 2.5 The literature gives a worst case convergence rate of gradient descent on Âµ-strongly
LIT REVIEW,0.17903930131004367,"convex and L-smooth functions of âˆ¥xi âˆ’xâˆ—
i âˆ¥2
2 â‰¤

1 âˆ’
2Âµ
Âµ+L
2i
âˆ¥x0 âˆ’xâˆ—
0âˆ¥2
2, using the step size"
LIT REVIEW,0.18340611353711792,"Î± =
2
Âµ+L [28, 30]. While this rate is better for a fixed Âµ and L, we again emphasize that the
constants Âµ and L may be very different depending on the chosen conditions, and thus these two rates
can not directly be compared."
LIT REVIEW,0.18777292576419213,"Motivation: we empirically verify in Appendix A that the optimization path of a ResNet18 [16]
trained for classification on CIFAR10 [20] verifies the interpolation conditions of RSIâˆ’âˆ©EB+,
which are introduced in Section 4. This result guarantees the existence of a function in RSIâˆ’âˆ©EB+
which exactly interpolates the observed gradients, and thus all convergence guarantees of RSIâˆ’âˆ©
EB+ naturally apply in this practical deep learning setting. RSIâˆ’âˆ©EB+ therefore provides linear
convergence guarantees while being empirically applicable to neural networks (although with respects
to a local minima), an impressive feat given the highly non-convex nature of neural networks loss
functions."
LIT REVIEW,0.19213973799126638,"3
Related Work"
LIT REVIEW,0.1965065502183406,"Throughout the literature, many choices of assumptions have been used to study first-order optimiza-
tion. Most assumptions fall into one of two categories : lower conditions and upper conditions that
respectively take the form of a lower and an upper bound on properties of the objective function. For
instance, strong convexity lower bounds the curvature of the objective function and is thus a lower
condition. Similarly, smoothness is an upper condition."
LIT REVIEW,0.20087336244541484,"Lower conditions have been the most extensively studied assumptions, such as Polyak-Åojasiewicz
[29], local-quasi-convexity [15], weak quasi-convexity [14], quadratic growth [2, 4, 18], Kurdyka-
Åojasiewicz [21, 3], optimal strong convexity [22, 25, 8], weak strong convexity [19, 26], error bounds
[24]. Some recent works have explored the relations between these lower conditions [19, 38]. In this
work we focus on the restricted secant inequality (RSIâˆ’) (which we denote as lower restricted secant
inequality to differentiate it from its upper bound equivalent) which was introduced in [39], and has
been used (along with its convex extension restricted strong convexity) in many recent theoretical
derivations of linear convergence rates [36, 31, 37]."
LIT REVIEW,0.2052401746724891,"On the contrary, because most machine learning objective functions are naturally smooth, fewer works
have explored alternatives to smoothness. However as discussed in Remark 2.4, it is still relevant
to study these alternatives on smooth objectives due to potentially better conditioning. The most
notable ones in the literature are local smoothness [15], restricted smoothness [1], relative smoothness
[23, 13, 40], weak-smoothness [14], expected smoothness [11]. In [12], the authors argue that lower
conditions can be naturally translated into equivalent upper conditions by changing the lower bound
into an upper bound, and vice-versa. Subsequently, they introduce a set of upper equivalent to existing
lower conditions, such as upper error bounds EB+, the natural upper equivalent to error bounds
from [24]. Throughout this work, we focus on EB+ as an upper condition."
LIT REVIEW,0.2096069868995633,"Finally, a key to our analysis are the necessary and sufficient interpolation conditions of RSIâˆ’(Âµ) âˆ©
EB+(L) (Section 4). The search of these conditions has been largely motivated by Performance
Estimation Problems (PEP), introduced in [33] and with many recent successful applications [17, 32, 6,
35, 34]. PEP is a framework for computer-assisted worst-case convergence analysis, that only requires
necessary and sufficient interpolation conditions for the considered class of objective functions. In the
case of RSIâˆ’âˆ©EB+ however, we found the interpolation conditions to be independent (see Section
4), which made the worst-case analysis directly solvable analytically."
IMPLEMENTATION/METHODS,0.21397379912663755,"4
Interpolation conditions"
IMPLEMENTATION/METHODS,0.2183406113537118,"In this section we provide and discuss the necessary and sufficient interpolation conditions for
RSIâˆ’âˆ©EB+. Their importance stems from the framework used in PEP and in this work to analyze
worst-case convergence. Given a first-order optimization algorithm A we want to find the slowest
convergence rate of A among all f within a class of objective functions C and starting point x0 âˆˆ
Rd \ Xâˆ—
f. That is equivalent to solving the following optimization problem at any step number n :"
IMPLEMENTATION/METHODS,0.22270742358078602,"min
fâˆˆC,(xi)iâ‰¤nâˆˆ(Rd)n+1
âˆ¥x0 âˆ’xâˆ—
0âˆ¥2
âˆ¥xn âˆ’xâˆ—nâˆ¥2
s.t.
âˆ€i â‰¤n âˆ’1, xi+1 = A ((x0, f(x0), âˆ‡f(x0)) , ..., (xi, f(xi), âˆ‡f(xi)))
(5)"
IMPLEMENTATION/METHODS,0.22707423580786026,"Directly searching for f in the functional space is generally intractable, however if we can explicitly
find the set G of all families (xi, fi, gi)i such that âˆƒf âˆˆC, âˆ€i, âˆ‡f(xi) = gi and f(xi) = fi (we say
that f interpolates (xi, fi, gi)i), then problem (5) can be reduced to:"
IMPLEMENTATION/METHODS,0.2314410480349345,"min
(xi,gi,fi)iâ‰¤nâˆˆ(RdÃ—RdÃ—R)n+1
âˆ¥x0 âˆ’xâˆ—
0âˆ¥2
âˆ¥xn âˆ’xâˆ—nâˆ¥2
s.t.
âˆ€i â‰¤n âˆ’1, xi+1 = A ((x0, f0, g0) , ..., (xi, fi, gi))
and
(xi, fi, gi)iâ‰¤n âˆˆG (6)"
IMPLEMENTATION/METHODS,0.23580786026200873,"In many cases (see Section 3), problem (6) is tractable and becomes a very powerful analysis tool
providing lower bounds, upper bounds, and optimal tuning for different types of algorithms and
assumptions used. A crucial and difficult component of this analysis is to formulate the interpolation"
IMPLEMENTATION/METHODS,0.24017467248908297,"conditions, that is the necessary and sufficient conditions for a family (xi, fi, gi)i to belong in G.
Driven by these motivations, we now establish the interpolation conditions for RSIâˆ’âˆ©EB+."
IMPLEMENTATION/METHODS,0.2445414847161572,"In Theorem 1, we introduce the necessary and sufficient conditions to interpolate a family (xi, gi),
without considering the function values (fi)i. This theorem could be used to find the worst case
convergence rate over all first-order algorithms that ignore function values. However, in Corollary 1,
we deduce from Theorem 1 sufficient (but not necessary) conditions to interpolate a family (xi, fi, gi).
These conditions allow us in Section 5 to find a lower bound on the worst-case convergence rate
for all first-order algorithms (including algorithms that have access to function values information),
which we know is tight thanks to Prop 1."
IMPLEMENTATION/METHODS,0.24890829694323144,"Theorem 1 (Interpolation conditions) Let (xi, gi)iâ‰¤n âˆˆ

Rd Ã— Rdn+1, such that the xi are
separate points."
IMPLEMENTATION/METHODS,0.25327510917030566,"Then, âˆ€Âµ, L > 0:
âˆƒf âˆˆRSIâˆ’(Âµ) âˆ©EB+(L), s.t. âˆ€i, âˆ‡f(xi) = gi
â‡•"
IMPLEMENTATION/METHODS,0.2576419213973799,"âˆƒXâˆ—âŠ†Rd convex, s.t. âˆ€i,"
IMPLEMENTATION/METHODS,0.26200873362445415,"âˆ¥giâˆ¥2 â‰¤L âˆ¥xi âˆ’xâˆ—
i âˆ¥2
and
âŸ¨gi | xi âˆ’xâˆ—
i âŸ©â‰¥Âµ âˆ¥xi âˆ’xâˆ—
i âˆ¥2
2 ,
(7)"
IMPLEMENTATION/METHODS,0.2663755458515284,"where xâˆ—
i is the orthogonal projection of xi onto Xâˆ—."
IMPLEMENTATION/METHODS,0.27074235807860264,"Proof. In order to preserve concision and clarity, we will only present the broad outline of the proof
here. For a complete technical proof, see Appendix B."
IMPLEMENTATION/METHODS,0.27510917030567683,"The direct implication is trivial as it is a direct application of RSIâˆ’(Âµ) and EB+(L) definitions to
f in (xi)i. For the reverse implication, we will construct a function fÏµ,Î² that interpolates (xi, gi)i.
The function fÏµ,Î² is a quadratic everywhere except in the spheres of radius Ïµ around each xi, Ïµ being
small enough for these spheres to never intersect. Inside the sphere of radius Ïµ around a given xi,
fÏµ,Î² will be perturbed by adding a term Î»(âˆ¥x âˆ’xiâˆ¥2)h(x) where h is affine in x, and Î» is a scaling
term so that Î»(Ïµ) = 0 at the border of the sphere, and Î»(0) = 1 in its center xi."
IMPLEMENTATION/METHODS,0.2794759825327511,The key is to find a function Î» that preserves the properties of RSIâˆ’(Âµ) and EB+(L). We use
IMPLEMENTATION/METHODS,0.2838427947598253,"Î»Ïµ,Î²(u) =
1 + cos

Ï€ uÎ² ÏµÎ²
 2
(8)"
IMPLEMENTATION/METHODS,0.28820960698689957,"And our construction fÏµ,Î² is given by:"
IMPLEMENTATION/METHODS,0.2925764192139738,"fÏµ,Î²(x) = ( Âµ+L"
IMPLEMENTATION/METHODS,0.29694323144104806,"4
âˆ¥x âˆ’xâˆ—âˆ¥2
2
if âˆ€i, âˆ¥x âˆ’xiâˆ¥2 â‰¥Ïµ Âµ+L"
IMPLEMENTATION/METHODS,0.30131004366812225,"4
âˆ¥x âˆ’xâˆ—âˆ¥2
2 + Î»Ïµ,Î² (âˆ¥x âˆ’xiâˆ¥2)
D
gi âˆ’Âµ+L"
IMPLEMENTATION/METHODS,0.3056768558951965,"2 (xi âˆ’xâˆ—
i ) | x âˆ’xi
E
if âˆƒi, âˆ¥x âˆ’xiâˆ¥2 < Ïµ (9)"
IMPLEMENTATION/METHODS,0.31004366812227074,"The rest of the proof is to use the Taylor expansions of fÏµ,Î² to show that for sufficiently small Ïµ and
Î², fÏµ,Î² will belong in RSIâˆ’(Âµ) and EB+(L) (see Appendix B). â– "
IMPLEMENTATION/METHODS,0.314410480349345,"Corollary 1 Let (xi, fi, gi)iâ‰¤n âˆˆ

Rd Ã— R Ã— Rdn+1, such that the xi are separate points."
IMPLEMENTATION/METHODS,0.31877729257641924,"Then, âˆ€Âµ, L > 0:
âˆƒXâˆ—âŠ†Rd convex, s.t. âˆ€i,"
IMPLEMENTATION/METHODS,0.3231441048034934,"âˆ¥giâˆ¥2 â‰¤L âˆ¥xi âˆ’xâˆ—
i âˆ¥2
âŸ¨gi | xi âˆ’xâˆ—
i âŸ©â‰¥Âµ âˆ¥xi âˆ’xâˆ—
i âˆ¥2
2"
IMPLEMENTATION/METHODS,0.32751091703056767,fi = Âµ + L
IMPLEMENTATION/METHODS,0.3318777292576419,"4
âˆ¥xi âˆ’xâˆ—
i âˆ¥2
2 (10)"
IMPLEMENTATION/METHODS,0.33624454148471616,"â‡“
âˆƒf âˆˆRSIâˆ’(Âµ) âˆ©EB+(L), s.t. âˆ€i, âˆ‡f(xi) = gi
and
f(xi) = fi,"
IMPLEMENTATION/METHODS,0.3406113537117904,"where xâˆ—
i is the orthogonal projection of xi onto Xâˆ—."
IMPLEMENTATION/METHODS,0.34497816593886466,"Proof. We simply use the function fÏµ,Î² from the proof of Theorem 1 and note that âˆ€i, fÏµ,Î²(xi) =
Âµ+L"
IMPLEMENTATION/METHODS,0.34934497816593885,"4
âˆ¥xi âˆ’xâˆ—
i âˆ¥2
2. â– "
IMPLEMENTATION/METHODS,0.3537117903930131,Theorem 1 and Corollary 1 are the key elements to prove the optimality of gradient descent on RSIâˆ’
IMPLEMENTATION/METHODS,0.35807860262008734,and EB+ among all first-order algorithms (see Section 5).
IMPLEMENTATION/METHODS,0.3624454148471616,"Remark 4.1 The interpolation conditions in Theorem 1 are independent, in the sense that a family
(xi, gi)i admits an interpolation in RSIâˆ’(Âµ) âˆ©EB+(L) if and only if each {(xi, gi)} admits an
interpolation in RSIâˆ’(Âµ) âˆ©EB+(L). Similarly, the sufficient interpolation conditions in Corollary 1
are also independent."
IMPLEMENTATION/METHODS,0.36681222707423583,"This property of independent interpolation conditions drastically simplifies convergence analysis and
is the main reason we are able to analytically derive a lower bound in Section 5. Indeed, given an
interpolable family (xi, fi, gi)iâ‰¤n for a given set Xâˆ—, it is sufficient to show that (xn+1, fn+1, gn+1)
is interpolable with Xâˆ—to prove that the entire family (xi, fi, gi)iâ‰¤n+1 can be interpolated. It
is thus simple to find the set of interpolable (fn+1, gn+1) given Xâˆ—, xn+1, and an interpolable
(xi, fi, gi)iâ‰¤n."
IMPLEMENTATION/METHODS,0.37117903930131,"5
Lower bound on the convergence rate"
IMPLEMENTATION/METHODS,0.37554585152838427,In this Section we derive a lower bound on the convergence rate of first-order algorithms on RSIâˆ’
IMPLEMENTATION/METHODS,0.3799126637554585,"and EB+. This lower bounds applies under the assumption that the number of steps taken is smaller
than the number of dimension d. This assumption is frequent in the literature (e.g. [5]) and not
constraining for high-dimensional optimization. The observed gradients of the worst-case functions
for optimal algorithms are typically orthogonal to one another (see [6]) which is not possible when
the number of steps becomes larger than the dimension d. We conjecture that when not bounding the
number of steps, it is possible to achieve an asymptotic rate in O

2âˆ’n"
IMPLEMENTATION/METHODS,0.38427947598253276,"d 
which would be better than
the usual rates obtained for very ill-conditioned functions, while having little to no practical uses due
to the bad convergence properties on a lower number of steps."
IMPLEMENTATION/METHODS,0.388646288209607,"We now introduce Lemma 1, which is the cornerstone of the proof of Theorem 2:"
IMPLEMENTATION/METHODS,0.3930131004366812,"Lemma 1 Let Âµ > 0 and L > Âµ. Let Î±0 âˆˆ
h
Âµ
L2 , max

Âµ
L2 , 1"
IMPLEMENTATION/METHODS,0.39737991266375544,"2Âµ
i
. For any first-order"
IMPLEMENTATION/METHODS,0.4017467248908297,"optimization algorithm A and starting point x0 âˆˆRd, there exists (gi)iâ‰¤dâˆ’2 âˆˆRd, (fi)iâ‰¤dâˆ’2 âˆˆ
R and Sdâˆ’2 âŠ†Sdâˆ’1 âŠ†Â· Â· Â· âŠ†S0 âŠ†Rd such that:"
IMPLEMENTATION/METHODS,0.40611353711790393,"1. âˆ€i â‰¤d âˆ’2, there exists a (d âˆ’i âˆ’1)-dimensional affine space Hi containing Si and"
IMPLEMENTATION/METHODS,0.4104803493449782,"in which Si is a (d âˆ’i âˆ’2)-sphere of radius ri =
q Î±0"
IMPLEMENTATION/METHODS,0.4148471615720524,"Âµ âˆ’Î±2
0 âˆ¥g0âˆ¥2

1 âˆ’Âµ2"
IMPLEMENTATION/METHODS,0.4192139737991266,"L2
 i"
IMPLEMENTATION/METHODS,0.42358078602620086,"2 and
center ci âˆˆHi."
IMPLEMENTATION/METHODS,0.4279475982532751,"2. Let (xi)i be the iterates generated by A starting from x0 and reading gradients (gi)i
and function values (fi)i, then for any i â‰¤dâˆ’2 and any x âˆˆSi, there exists a function
f in RSIâˆ’(Âµ) âˆ©EB+(L) minimized by {x} that interpolates (xj, fj, gj)jâ‰¤i."
IMPLEMENTATION/METHODS,0.43231441048034935,"Proof. In order to preserve concision and clarity, we will only present the broad outline of the proof
here. For a complete technical proof, see Appendix C."
IMPLEMENTATION/METHODS,0.4366812227074236,"We construct the sequence iteratively.
For initialisation, we take any non-zero g0, set f0 =
Âµ+L"
IMPLEMENTATION/METHODS,0.4410480349344978,"4Âµ Î±0 âˆ¥g0âˆ¥2
2, c0 = x0 âˆ’Î±0g0, and finally"
IMPLEMENTATION/METHODS,0.44541484716157204,"S0 =

x âˆˆRdâŸ¨x âˆ’c0 | g0âŸ©= 0
	
âˆ©

x âˆˆRd
âˆ¥x âˆ’c0âˆ¥2 =
rÎ±0"
IMPLEMENTATION/METHODS,0.4497816593886463,"Âµ âˆ’Î±2
0 âˆ¥g0âˆ¥2 "
IMPLEMENTATION/METHODS,0.45414847161572053,"Then assuming we have a sequence (fj, gj, Sj)jâ‰¤i<(dâˆ’2) respecting the conditions of the Lemma,
noting Hi the d âˆ’i âˆ’1 dimensional affine space in which Si is a (d âˆ’i âˆ’2) dimensional sphere,
and xi+1 the (i + 1)-th iterate returned by A. Let hi+1 the orthogonal projection of xi+1 into Hi."
IMPLEMENTATION/METHODS,0.4585152838427948,"If hi+1 Ì¸= ci, let v =
(hi+1âˆ’ci)
âˆ¥hi+1âˆ’ciâˆ¥2 . If hi+1 = ci, let s âˆˆSi and v =
(sâˆ’ci)
âˆ¥sâˆ’ciâˆ¥2 ."
IMPLEMENTATION/METHODS,0.462882096069869,We then construct:
IMPLEMENTATION/METHODS,0.4672489082969432,ci+1 = ci âˆ’Âµ Lriv
IMPLEMENTATION/METHODS,0.47161572052401746,fi+1 = Âµ + L
IMPLEMENTATION/METHODS,0.4759825327510917,"4
(âˆ¥xi+1 âˆ’ci+1âˆ¥2
2 + (1 âˆ’Âµ2"
IMPLEMENTATION/METHODS,0.48034934497816595,"L2 )r2
i )"
IMPLEMENTATION/METHODS,0.4847161572052402,gi+1 = L âˆ¥xi+1 âˆ’xâˆ—âˆ¥2
IMPLEMENTATION/METHODS,0.4890829694323144,"âˆ¥xi+1 âˆ’ci+1âˆ¥2
(xi+1 âˆ’ci+1)"
IMPLEMENTATION/METHODS,0.49344978165938863,"Hi+1 =
n
x âˆˆHi | âŸ¨x âˆ’ci | vâŸ©= âˆ’Âµ Lri
o"
IMPLEMENTATION/METHODS,0.4978165938864629,Si+1 = Si âˆ©Hi+1
IMPLEMENTATION/METHODS,0.5021834061135371,We verify in Appendix C that this construction respects the properties of Lemma 1. â– 
IMPLEMENTATION/METHODS,0.5065502183406113,"We can now introduce Theorem 2 which gives us a lower bound on the worst-case convergence rate
of any first-order algorithm on RSIâˆ’(Âµ) and EB+(L)."
IMPLEMENTATION/METHODS,0.5109170305676856,"Theorem 2 (Lower bound on RSIâˆ’âˆ©EB+) Let A be any first-order algorithm on Rd, Âµ > 0
and L â‰¥Âµ. For any x0 âˆˆRd, there exists xâˆ—âˆˆRd and a function f in RSIâˆ’(Âµ) âˆ©EB+(L)
minimized by Xâˆ—= {xâˆ—} such that"
IMPLEMENTATION/METHODS,0.5152838427947598,"âˆ€i â‰¤d âˆ’1, âˆ¥xi âˆ’xâˆ—
i âˆ¥2
2 â‰¥

1 âˆ’Âµ2 L2"
IMPLEMENTATION/METHODS,0.519650655021834,"i
âˆ¥x0 âˆ’xâˆ—
0âˆ¥2
2
(11)"
IMPLEMENTATION/METHODS,0.5240174672489083,"Furthermore, if L Âµ â‰¥
âˆš"
IMPLEMENTATION/METHODS,0.5283842794759825,"2, there exists h in RSIâˆ’(Âµ) âˆ©EB+(L) minimized by Xâˆ—= {xâˆ—} such
that"
IMPLEMENTATION/METHODS,0.5327510917030568,"âˆ€i â‰¤d âˆ’1, âˆ¥zi âˆ’zâˆ—
i âˆ¥2
2 â‰¥âˆ¥âˆ‡h(z0)âˆ¥2
2
4Âµ2"
IMPLEMENTATION/METHODS,0.537117903930131,"
1 âˆ’Âµ2 L2"
IMPLEMENTATION/METHODS,0.5414847161572053,"iâˆ’1
(12)"
IMPLEMENTATION/METHODS,0.5458515283842795,where (xi) (resp. (zi)) is the trajectory obtained by applying A to f (resp. h) starting in x0.
IMPLEMENTATION/METHODS,0.5502183406113537,"Note that since Xâˆ—is a singleton, âˆ€i, xâˆ—
i = zâˆ—
i = xâˆ—."
IMPLEMENTATION/METHODS,0.5545851528384279,"Proof. If L = Âµ, then the inequalities are trivial from the positivity of the norm. If L > Âµ, let
(gi, fi, Si)iâ‰¤(dâˆ’2) be the sequence introduced in Lemma 1 for Î±0 âˆˆ
h
Âµ
L2 , max

Âµ
L2 , 1"
IMPLEMENTATION/METHODS,0.5589519650655022,"2Âµ
i
. Let us"
IMPLEMENTATION/METHODS,0.5633187772925764,"note that for any x âˆˆS0, âˆ¥x0 âˆ’xâˆ¥2
2 = Î±0âˆ¥g0âˆ¥2
2
Âµ
(see initialisation in Appendix C)."
IMPLEMENTATION/METHODS,0.5676855895196506,"Let i âˆˆ{1, . . . , d âˆ’1}. Siâˆ’1 has radius riâˆ’1, thus there exists xâˆ—âˆˆSiâˆ’1 such that âˆ¥xi âˆ’xâˆ—âˆ¥2 â‰¥
riâˆ’1, and thus :"
IMPLEMENTATION/METHODS,0.5720524017467249,"âˆ¥xi âˆ’xâˆ—âˆ¥2
2 â‰¥r2
iâˆ’1 =
Î±0"
IMPLEMENTATION/METHODS,0.5764192139737991,"Âµ âˆ’Î±2
0"
IMPLEMENTATION/METHODS,0.5807860262008734,"
âˆ¥g0âˆ¥2
2"
IMPLEMENTATION/METHODS,0.5851528384279476,"
1 âˆ’Âµ2 L2"
IMPLEMENTATION/METHODS,0.5895196506550219,"iâˆ’1
(13)"
IMPLEMENTATION/METHODS,0.5938864628820961,"When setting Î±0 =
Âµ
L2 and observing that xâˆ—âˆˆSiâˆ’1 âŠ†S0 and thus âˆ¥x0 âˆ’xâˆ—âˆ¥2
2 = Î±0âˆ¥g0âˆ¥2
2
Âµ
in (13),
we obtain (11). If L Âµ â‰¥
âˆš"
IMPLEMENTATION/METHODS,0.5982532751091703,"2, we set Î±0 =
1
2Âµ and (13) immediately yields (12). â– "
IMPLEMENTATION/METHODS,0.6026200873362445,"Remark 5.1 Since the lower bounds established in Theorem 2 are exactly matched by the conver-
gence guarantees of gradient descent (see Prop 1), these bounds are tight and gradient descent is
exactly optimal on RSIâˆ’(Âµ) âˆ©EB+(L). This is a concrete example of the sensitivity of theoretical
optimality to the choice of complexity classes."
IMPLEMENTATION/METHODS,0.6069868995633187,"5.1
Discussion"
CONCLUSION/DISCUSSION ,0.611353711790393,The first bound presented in Theorem 2 gives the optimal solution when trying to solve
CONCLUSION/DISCUSSION ,0.6157205240174672,"min
A max
f,x0
âˆ¥xn âˆ’xâˆ—
nâˆ¥2
âˆ¥x0 âˆ’xâˆ—
0âˆ¥2
(14)"
CONCLUSION/DISCUSSION ,0.6200873362445415,While the second bound gives the optimal solution when trying to solve
CONCLUSION/DISCUSSION ,0.6244541484716157,"min
A max
f,x0
âˆ¥xn âˆ’xâˆ—
nâˆ¥2
âˆ¥âˆ‡f(x0)âˆ¥2
(15)"
CONCLUSION/DISCUSSION ,0.62882096069869,"For general smooth and convex functions, (15) will not have a solution (for any A, the quantity will
not have a worst case upper bound), which is why (14) has historically been the focus of optimization
literature. However in practice, when (15) admits a solution, as is the case for RSIâˆ’(Âµ) âˆ©EB+(L), it
fits practical motivations better than (14) : when starting from x0 and observing an initial gradient g0,
the solution to (15) is the one that will minimize âˆ¥xn âˆ’xâˆ—
nâˆ¥2 in the worst case. In comparison, for
a fixed âˆ¥âˆ‡f(x0)âˆ¥2, the solution to (14) will be faster when âˆ¥x0 âˆ’xâˆ—
0âˆ¥2 is small, and slower when
âˆ¥x0 âˆ’xâˆ—
0âˆ¥2 is large, leading to a slower worst-case convergence."
CONCLUSION/DISCUSSION ,0.6331877729257642,"When L Âµ >
âˆš"
CONCLUSION/DISCUSSION ,0.6375545851528385,"2, using a tuning of Î±0 =
1
2Âµ on the first step instead of
Âµ
L2 leads to a worst-case"
CONCLUSION/DISCUSSION ,0.6419213973799127,"convergence of âˆ¥xn âˆ’xâˆ—
nâˆ¥2 better by a constant c =
L2
2(L2âˆ’Âµ2). While such small constant factor
is often considered not impactful, the number of steps required to make up for this constant factor (a) L Âµ >
âˆš"
CONCLUSION/DISCUSSION ,0.6462882096069869,"2
(b) L Âµ =
âˆš"
CONCLUSION/DISCUSSION ,0.6506550218340611,"2
(c) L Âµ <
âˆš 2"
CONCLUSION/DISCUSSION ,0.6550218340611353,"Figure 1: 2D representations of the possible positions of xâˆ—given x0 and g0, for three possible values
of L"
CONCLUSION/DISCUSSION ,0.6593886462882096,"Âµ . xâˆ—must be in CRSI (red) but not in CEB (green). When L Âµ >
âˆš"
CONCLUSION/DISCUSSION ,0.6637554585152838,"2 (left), doing a larger step to
reach the center of CRSI will minimize worst-case sub-optimality."
CONCLUSION/DISCUSSION ,0.6681222707423581,"is n =
âˆ’log(2)"
CONCLUSION/DISCUSSION ,0.6724890829694323,"log

1âˆ’Âµ2"
CONCLUSION/DISCUSSION ,0.6768558951965066,"L2
 âˆ’1, which yields n â‰ˆ68 for L"
CONCLUSION/DISCUSSION ,0.6812227074235808,Âµ = 10 and n â‰ˆ6930 for L
CONCLUSION/DISCUSSION ,0.6855895196506551,"Âµ = 100, and can thus"
CONCLUSION/DISCUSSION ,0.6899563318777293,become substantial on ill-conditioned functions.
CONCLUSION/DISCUSSION ,0.6943231441048034,"Finally, we propose a geometric interpretation of the threshold L Âµ =
âˆš"
CONCLUSION/DISCUSSION ,0.6986899563318777,"2. Given x0 and g0, RSIâˆ’(Âµ)"
CONCLUSION/DISCUSSION ,0.7030567685589519,requires xâˆ—to be within the circle of center x0 âˆ’g0
CONCLUSION/DISCUSSION ,0.7074235807860262,2Âµ and radius âˆ¥g0âˆ¥2
CONCLUSION/DISCUSSION ,0.7117903930131004,"2Âµ , while EB+(L) requires xâˆ—to"
CONCLUSION/DISCUSSION ,0.7161572052401747,not be within the circle of center x0 and radius âˆ¥g0âˆ¥2
CONCLUSION/DISCUSSION ,0.7205240174672489,"L
. In Figure 1 we show these circles for different
values of L"
CONCLUSION/DISCUSSION ,0.7248908296943232,"Âµ. When L Âµ >
âˆš"
CONCLUSION/DISCUSSION ,0.7292576419213974,"2 (Figure 1(a)), x1 = x0 âˆ’
Âµ
L2 g0 minimizes âˆ¥x1âˆ’xâˆ—âˆ¥2"
CONCLUSION/DISCUSSION ,0.7336244541484717,âˆ¥x0âˆ’xâˆ—âˆ¥2 over all possible
CONCLUSION/DISCUSSION ,0.7379912663755459,"xâˆ—, while x1 = x0 âˆ’g0"
CONCLUSION/DISCUSSION ,0.74235807860262,2Âµ minimizes âˆ¥x1 âˆ’xâˆ—âˆ¥2. As L
CONCLUSION/DISCUSSION ,0.7467248908296943,"Âµ becomes smaller than
âˆš"
CONCLUSION/DISCUSSION ,0.7510917030567685,"2 (Figure 1(b) and
1(c)), the same point x1 = x0 âˆ’
Âµ
L2 g0 minimizes both quantities."
CONCLUSION/DISCUSSION ,0.7554585152838428,"5.2
PEP experiment"
CONCLUSION/DISCUSSION ,0.759825327510917,"Since we have found necessary and sufficient interpolation conditions in Theorem 1, we can use the
PEP framework on RSIâˆ’âˆ©EB+ to confirm our results and derive the worst-case convergence rate of
first-order algorithms. In Figure 2 we show the worst-case linear rate of convergence of Heavy Ball
(HB) [29] on RSIâˆ’(0.1) âˆ©EB+(1.0) for regularly sampled learning rate Î± and momentum Î² (bright
yellow means no linear convergence), generated with PEPit [9]. We remind the update rule of HB"
CONCLUSION/DISCUSSION ,0.7641921397379913,xn+1 = xn âˆ’Î±âˆ‡f(xn) + Î²(xn âˆ’xnâˆ’1)
CONCLUSION/DISCUSSION ,0.7685589519650655,"Since gradient descent is a special case of HB where Î² = 0, we observe as expected that the optimal
rate of convergence is achieved for Î² = 0 and Î± = 0.1 =
Âµ
L2 . Moreover, Figure 2 shows that
momentum does not do well on RSIâˆ’âˆ©EB+ but gradient decent benefits from a relative robustness
to the tuning of Î± : we get similar convergence rates for any Î± âˆˆ[0.05, 0.15]."
CONCLUSION/DISCUSSION ,0.7729257641921398,"6
Conclusion"
CONCLUSION/DISCUSSION ,0.777292576419214,"Our main result is to prove that for any Âµ > 0 and L â‰¥Âµ, gradient descent is exactly optimal
on the class of functions RSIâˆ’(Âµ) âˆ©EB+(L) (by exact optimality, we mean that the convergence
guarantees of GD match the lower bound of worst-case performances exactly, without a constant
factor of difference). This result confirms the observation in [12] that optimality is overly sensitive to
the choice of assumptions, and should thus be considered with a lot of caution."
CONCLUSION/DISCUSSION ,0.7816593886462883,"Interestingly, our analysis also identifies two similar notions of optimality, one of which suggests
using a larger step size on the first iteration when the function is not particularly well-conditioned to
improve worst-case convergence speed (see section 5.1)."
CONCLUSION/DISCUSSION ,0.7860262008733624,"We verified empirically that RSIâˆ’âˆ©EB+, with respect to the last iterate, are verified on the
optimization paths of simple deep neural networks (c.f. Appendix A). This suggests that unlike usual
alternatives which are known to not be verified on the highly non-convex loss landscapes of neural
networks, convergence guarantees on RSIâˆ’âˆ©EB+ realistically apply to deep learning. On the"
CONCLUSION/DISCUSSION ,0.7903930131004366,"Figure 2: Worst-case linear convergence rate of heavy ball on RSIâˆ’(0.1) âˆ©EB+(1) depending of its
hyperparameters Î± and Î², as calculated by PEP. The best rate is achieved for Î± = 0.1 and Î² = 0."
CONCLUSION/DISCUSSION ,0.7947598253275109,"other hand, the impossibility to accelerate the convergence rate on RSIâˆ’âˆ©EB+ implies that these
assumptions are insufficient to explain the empirical successes of accelerated methods."
CONCLUSION/DISCUSSION ,0.7991266375545851,"For the scope of this work we have focused on worst-case convergence analysis. While in practice
average-case convergence rates are more insightful than their worst-case counterparts, such results
are rare due to the necessity of defining a reasonable distribution on the considered class of functions,
which is generally unfeasible. While such distribution on RSIâˆ’âˆ©EB+ is equally difficult to define,
it should be feasible to define instead a reasonable distribution of the observed gradient for a given
sampling point x and nearest minima xâˆ—, e.g. an uniform distribution over the (simple) set of possible
gradients. Such approach is conceivable with RSIâˆ’âˆ©EB+ only because the interpolation conditions
are independent (see remark 4.1), and thus we can easily make sure that any set of gradients sampled
from this distribution can be interpolated within the class. While the distribution will necessarily
be arbitrary, we believe such analysis could yield very useful insights and RSIâˆ’âˆ©EB+ is a rare
opportunity to follow this approach."
CONCLUSION/DISCUSSION ,0.8034934497816594,"Finally, many alternative conditions have been introduced in the literature (see Section 3), for which
optimality results are still unknown. The PEP framework is a powerful tool to study conditions for
which we can determine sufficient and necessary interpolation conditions, which would improve our
understanding of first-order algorithm properties and tuning on a wide variety of objective function
classes."
CONCLUSION/DISCUSSION ,0.8078602620087336,Acknowledgments and Disclosure of Funding
CONCLUSION/DISCUSSION ,0.8122270742358079,"The authors would like to thank Leonard Boussioux, for useful discussions and feedback. Ioannis
Mitliagkas acknowledges support by an NSERC Discovery grant (RGPIN-2019-06512), a Samsung
grant and a Canada CIFAR AI chair."
REFERENCES,0.8165938864628821,References
REFERENCES,0.8209606986899564,"[1] A. Agarwal, S. N. Negahban, and M. J. Wainwright. Fast global convergence of gradient
methods for high-dimensional statistical recovery. Ann. Statist., 40(5):2452â€“2482, 2012."
REFERENCES,0.8253275109170306,"[2] M. Anitescu. Degenerate nonlinear programming with a quadratic growth condition. SIAM
Journal on Optimization, 10(4):1116â€“1135, 2000."
REFERENCES,0.8296943231441049,"[3] J. Bolte, A. Daniilidis, O. Ley, and L. Mazet. Characterizations of Å‚ojasiewicz inequalities and
applications. arXiv:0802.0826, 2008."
REFERENCES,0.834061135371179,"[4] J. F. Bonnans and A. D. Ioffe. Second-order sufficiency and quadratic growth for non isolated
minima. PhD thesis, INRIA, 1993."
REFERENCES,0.8384279475982532,"[5] S. Bubeck. Convex optimization: Algorithms and complexity, 2015."
REFERENCES,0.8427947598253275,"[6] Y. Drori and A. B. Taylor. Efficient first-order methods for convex minimization: a con-
structive approach.
Mathematical Programming, 184(1-2):183â€“220, Jun 2019.
ISSN
1436-4646. doi: 10.1007/s10107-019-01410-2. URL http://dx.doi.org/10.1007/
s10107-019-01410-2."
REFERENCES,0.8471615720524017,"[7] Y. Drori and M. Teboulle. Performance of first-order methods for smooth convex minimization:
a novel approach, 2012."
REFERENCES,0.851528384279476,"[8] P. Gong and J. Ye. Linear convergence of variance-reduced stochastic gradient without strong
convexity. arXiv:1406.1102, 2014."
REFERENCES,0.8558951965065502,"[9] B. Goujaud, C. Moucer, F. Glineur, J. Hendrickx, A. Taylor, and A. Dieuleveut. Pepit: computer-
assisted worst-case analyses of first-order optimization methods in python, 2022."
REFERENCES,0.8602620087336245,"[10] R. M. Gower.
Convergence theorems for gradient descent.
Lecture notes for Statistical
Optimization, 2018."
REFERENCES,0.8646288209606987,"[11] R. M. Gower, O. Sebbouh, and N. Loizou. Sgd for structured nonconvex functions: Learning
rates, minibatching and interpolation. arXiv:2006.1031, 2020."
REFERENCES,0.868995633187773,"[12] C. Guille-Escuret, M. Girotti, B. Goujaud, and I. Mitliagkas. A study of condition numbers for
first-order optimization. In International Conference on Artificial Intelligence and Statistics,
pages 1261â€“1269. PMLR, 2021."
REFERENCES,0.8733624454148472,"[13] F. Hanzely, P. Richtarik, and L. Xiao. Accelerated Bregman proximal gradient methods for
relatively smooth convex optimization. Technical Report MSR-TR-2018-22, Microsoft, 2018."
REFERENCES,0.8777292576419214,"[14] M. Hardt, T. Ma, and B. Recht. Gradient descent learns linear dynamical systems. Journal of
Machine Learning Research, 19, 2018."
REFERENCES,0.8820960698689956,"[15] E. Hazan, K. Levy, and S. Shalev-Shwartz.
Beyond convexity: Stochastic quasi-convex
optimization. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,
Advances in Neural Information Processing Systems 28, pages 1594â€“1602. Curran Associates,
Inc., 2015."
REFERENCES,0.8864628820960698,"[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition, 2015. URL
https://arxiv.org/abs/1512.03385."
REFERENCES,0.8908296943231441,"[17] B. Hu and L. Lessard. Dissipativity theory for nesterovâ€™s accelerated method, 2017."
REFERENCES,0.8951965065502183,"[18] A. Ioffe. On sensitivity analysis of nonlinear programs in banach spaces: the approach via
composite unconstrained optimization. SIAM Journal on Optimization, 4(1):1â€“43, 1994."
REFERENCES,0.8995633187772926,"[19] H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient
methods under the polyak-Å‚ojasiewicz condition. CoRR, abs/1608.04636, 2016. URL http:
//arxiv.org/abs/1608.04636."
REFERENCES,0.9039301310043668,"[20] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009."
REFERENCES,0.9082969432314411,"[21] K. Kurdyka. On gradients of functions definable in o-minimal structures. Annales de lâ€™institut
Fourier, 48:769â€“783, 1998."
REFERENCES,0.9126637554585153,"[22] J. Liu and S. J. Wright. Asynchronous stochastic coordinate descent: Parallelism and conver-
gence properties. SIAM J. Optim., 25(1):351â€”376, 2015."
REFERENCES,0.9170305676855895,"[23] H. Lu, R. M. Freund, and Y. Nesterov. Relatively-smooth convex optimization by first-order
methods, and applications. SIAM J. Optim., 28(1):333â€“354, 2018."
REFERENCES,0.9213973799126638,"[24] Z.-Q. Luo and P. Tseng. Error bounds and convergence analysis of feasible descent methods: a
general approach. Annals of Operations Research, 46(1):157â€“178, 1993."
REFERENCES,0.925764192139738,"[25] C. Ma, R. Tappenden, and M. TakÃ Ë‡c. Linear convergence of the randomized feasible descent
method under the weak strong convexity assumption. Journal of Machine Learning Research,
17(228):1â€“24, 2016."
REFERENCES,0.9301310043668122,"[26] I. Necoara, Y. Nesterov, and F. Glineur. Linear convergence of first order methods for non-
strongly convex optimization, 2016."
REFERENCES,0.9344978165938864,"[27] Y. Nesterov. A method of solving a convex programming problem with convergence rate
O
 1"
REFERENCES,0.9388646288209607,"k2

. Soviet Mathematics Doklady, 27:372â€“376, 1983."
REFERENCES,0.9432314410480349,"[28] Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003."
REFERENCES,0.9475982532751092,"[29] B. T. Polyak. Gradient methods for the minimisation of functionals. USSR Computational
Mathematics and Mathematical Physics, 3(4):864 â€“ 878, 1963."
REFERENCES,0.9519650655021834,"[30] B. T. Polyak. Introduction to optimization. optimization software. Inc., Publications Division,
New York, 1:32, 1987."
REFERENCES,0.9563318777292577,"[31] F. SchÃ¶pfer. Linear convergence of descent methods for the unconstrained minimization of
restricted strongly convex functions. SIAM J. Optim., 26:1883â€“1911, 2016."
REFERENCES,0.9606986899563319,"[32] A. B. Taylor. Convex interpolation and performance estimation of first-order methods for convex
optimization, 2017."
REFERENCES,0.9650655021834061,"[33] A. B. Taylor, J. M. Hendrickx, and F. Glineur. Smooth strongly convex interpolation and exact
worst-case performance of first-order methods, 2016."
REFERENCES,0.9694323144104804,"[34] A. B. Taylor, J. M. Hendrickx, and F. Glineur. Exact worst-case performance of first-order
methods for composite convex optimization. SIAM Journal on Optimization, 27(3):1283â€“1313,
Jan 2017. ISSN 1095-7189. doi: 10.1137/16m108104x. URL http://dx.doi.org/10.
1137/16M108104X."
REFERENCES,0.9737991266375546,"[35] A. B. Taylor, J. M. Hendrickx, and F. Glineur. Exact worst-case convergence rates of the
proximal gradient method for composite convex minimization, 2020."
REFERENCES,0.9781659388646288,"[36] X. Yi, S. Zhang, T. Yang, K. H. Johansson, and T. Chai. Exponential convergence for distributed
smooth optimization under the restricted secant inequality condition, 2019."
REFERENCES,0.982532751091703,"[37] K. Yuan, Q. Ling, and W. Yin. On the convergence of decentralized gradient descent. SIAM J.
Optim., 26:1835â€“1854, 2016."
REFERENCES,0.9868995633187773,"[38] H. Zhang. The restricted strong convexity revisited: analysis of equivalence to error bound and
quadratic growth. Optimization Letters, 11(4):817â€“833, 2017."
REFERENCES,0.9912663755458515,"[39] H. Zhang and W. Yin. Gradient methods for convex minimization: better rates under weaker
conditions. Cam report, UCLA, 2013."
REFERENCES,0.9956331877729258,"[40] Y. Zhou, Y. Liang, and L. Shen. A simple convergence analysis of bregman proximal gradient
algorithm. Computational Optimization and Applications, 73(3):903â€“912, 2019."
