Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017331022530329288,"Existing Graph Neural Networks (GNNs) compute the message exchange between
1"
ABSTRACT,0.0034662045060658577,"nodes by either aggregating uniformly (convolving) the features of all the neighbor-
2"
ABSTRACT,0.005199306759098787,"ing nodes, or by applying a non-uniform score (attending) to the features. Recent
3"
ABSTRACT,0.006932409012131715,"works have shown the strengths and weaknesses of the resulting GNN architectures,
4"
ABSTRACT,0.008665511265164644,"respectively, GCNs and GATs. In this work, we aim at exploiting the strengths
5"
ABSTRACT,0.010398613518197574,"of both approaches to their full extent. To that end, we first introduce a graph
6"
ABSTRACT,0.012131715771230503,"convolutional attention layer (CAT), which relies on convolutions to compute the
7"
ABSTRACT,0.01386481802426343,"attention scores. Unfortunately, as in the case of GCNs and GATs, we then show
8"
ABSTRACT,0.01559792027729636,"that there exists no clear winner between the three—neither theoretically nor in
9"
ABSTRACT,0.01733102253032929,"practice—since their performance directly depends on the nature of the data (i.e.,
10"
ABSTRACT,0.019064124783362217,"of the graph and features). This result brings us to the main contribution of this
11"
ABSTRACT,0.02079722703639515,"work, the learnable graph convolutional attention network (L-CAT): a GNN archi-
12"
ABSTRACT,0.022530329289428077,"tecture that allows us to automatically interpolate between GCN, GAT and CAT
13"
ABSTRACT,0.024263431542461005,"in each layer, by only introducing two additional (scalar) parameters. Our results
14"
ABSTRACT,0.025996533795493933,"demonstrate that L-CAT is able to efficiently combine different GNN layers across
15"
ABSTRACT,0.02772963604852686,"the network, outperforming competing methods in a wide range of datasets, and
16"
ABSTRACT,0.029462738301559793,"resulting in a more robust model that needs less cross-validation.
17"
INTRODUCTION,0.03119584055459272,"1
Introduction
18"
INTRODUCTION,0.03292894280762565,"In recent years, Graph Neural Networks (GNNs) [25] have become ubiquitous in machine learning,
19"
INTRODUCTION,0.03466204506065858,"emerging as the standard approach in many settings. For example, they have been successfully
20"
INTRODUCTION,0.036395147313691506,"applied for tasks such as topic prediction in citation networks [26]; molecule prediction [11]; and
21"
INTRODUCTION,0.038128249566724434,"link prediction in recommender systems [33]. These applications typically make use of message-
22"
INTRODUCTION,0.03986135181975736,"passing GNNs [11], whose idea is fairly simple: in each layer, nodes are updated by aggregating the
23"
INTRODUCTION,0.0415944540727903,"information (messages) coming from their neighboring nodes.
24"
INTRODUCTION,0.043327556325823226,"Depending on how this aggregation is implemented, we can define different types of GNN layers.
25"
INTRODUCTION,0.045060658578856154,"Two important and widely adopted layers are graph convolutional networks (GCNs) [18], which
26"
INTRODUCTION,0.04679376083188908,"uniformly average the neighboring information; and graph attention networks (GATs) [30], which
27"
INTRODUCTION,0.04852686308492201,"instead perform a weighted average, based on an attention score between receiver and sender nodes.
28"
INTRODUCTION,0.05025996533795494,"More recently, a number of works have shown the strengths and limitations of both approaches from
29"
INTRODUCTION,0.05199306759098787,"a theoretical [2, 3, 10], and empirical [19] point of view. These results show that their performance
30"
INTRODUCTION,0.053726169844020795,"depends on the nature of the data at hand (i.e., the graph and the features), thus the standard approach
31"
INTRODUCTION,0.05545927209705372,"is to select between GCNs and GATs via computationally demanding cross-validation.
32"
INTRODUCTION,0.05719237435008666,"In this work, we aim to exploit effectively and efficiently the benefits of both convolution and
33"
INTRODUCTION,0.058925476603119586,"attention operations in the design of GNN architectures. To that end, we first introduce a novel graph
34"
INTRODUCTION,0.060658578856152515,"convolutional attention layer (CAT), which extends existing attention layers by taking the convolved
35"
INTRODUCTION,0.06239168110918544,"features as inputs of the score function, thus taking advantage of both operations. Following [10],
36"
INTRODUCTION,0.06412478336221837,"we rely on a contextual stochastic block model to theoretically compare GCN, GAT, and CAT
37"
INTRODUCTION,0.0658578856152513,"architectures. Our analysis shows that, unfortunately, no free lunch exists among these three GNN
38"
INTRODUCTION,0.06759098786828423,"architectures since their performance, as expected, is fully data-dependent.
39"
INTRODUCTION,0.06932409012131716,"This result motivates the main contribution of the paper, the learnable graph convolutional attention
40"
INTRODUCTION,0.07105719237435008,"network (L-CAT): a novel GNN which, in each layer, is capable of automatically interpolating
41"
INTRODUCTION,0.07279029462738301,"between the three operations during training by introducing only two additional (scalar) parameters.
42"
INTRODUCTION,0.07452339688041594,"As a result, L-CAT is able to learn the proper operation to apply at each layer, thus combining different
43"
INTRODUCTION,0.07625649913344887,"layer types in the same GNN architecture while overcoming the need to cross-validate—a process
44"
INTRODUCTION,0.0779896013864818,"that was prohibitively expensive prior to this work. Our extensive empirical analysis demonstrates the
45"
INTRODUCTION,0.07972270363951472,"capabilities of L-CAT on a wide range of datasets, outperforming existing baseline GNNs in terms of
46"
INTRODUCTION,0.08145580589254767,"both performance, and robustness to input noise and network initialization.
47"
PRELIMINARIES,0.0831889081455806,"2
Preliminaries
48"
PRELIMINARIES,0.08492201039861352,"Assume we are given as an input an undirected graph G = (V, E), where V = [n] denotes the set
49"
PRELIMINARIES,0.08665511265164645,"of vertices of the graph, and E ⊆V × V the set of edges. Each node i ∈[n] is represented by a
50"
PRELIMINARIES,0.08838821490467938,"d-dimensional feature vector Xi ∈Rd, and the goal is to produce a set of predictions {ˆyi}n
i=1.
51"
PRELIMINARIES,0.09012131715771231,"To this end, a message-passing GNN layer yields, for each node i, a representation ˜hi ∈Rd′, by
52"
PRELIMINARIES,0.09185441941074524,"collecting the information from each of its neighbors; aggregating them into a single message; and
53"
PRELIMINARIES,0.09358752166377816,"using the aggregated message to update its representation from the previous layer, hi ∈Rd. For the
54"
PRELIMINARIES,0.09532062391681109,"purposes of this work, we can define this operation as the following:
55"
PRELIMINARIES,0.09705372616984402,"˜hi = f(h′
i)
where
h′
i
def
=
X"
PRELIMINARIES,0.09878682842287695,"j∈N ∗
i
γijWvhj ,
(1)"
PRELIMINARIES,0.10051993067590988,"where N ∗
i = Ni ∪{i}, and Ni denotes the set of neighbors of node i, Wv ∈Rd′×d a learnable
56"
PRELIMINARIES,0.1022530329289428,"weight matrix, f an elementwise function, and γij ∈[0, 1] are coefficients such that P"
PRELIMINARIES,0.10398613518197573,"j∈N ∗
i γij = 1
57"
PRELIMINARIES,0.10571923743500866,"for each node i.
58"
PRELIMINARIES,0.10745233968804159,"Let the input features be h0
i = Xi, and the predictions be hL
i = ˆyi, we can readily define a message-
59"
PRELIMINARIES,0.10918544194107452,"passing GNN [11] as a sequence of L layers as defined above. Depending on the way the coefficients
60"
PRELIMINARIES,0.11091854419410745,"γij are computed, we can identify different GNN flavors.
61"
PRELIMINARIES,0.11265164644714037,"Graph convolutional networks (GCNs) [18] are a simple (yet effective) type of layers. In short,
62"
PRELIMINARIES,0.11438474870017332,"GCNs simply compute the average of the messages, i.e., they assign the same coefficient γij = 1/|N ∗
i |
63"
PRELIMINARIES,0.11611785095320624,"to every neighbor:
64"
PRELIMINARIES,0.11785095320623917,"˜hi = f(h′
i)
where
h′
i
def
=
1
|N ∗
i | X"
PRELIMINARIES,0.1195840554592721,"j∈N ∗
i
Wvhj ,
(2)"
PRELIMINARIES,0.12131715771230503,"Graph attention networks take a different approach. Instead of assigning a fixed value to each
65"
PRELIMINARIES,0.12305025996533796,"coefficient γij, they dynamically compute it as a function of the sender and receiver nodes. A general
66"
PRELIMINARIES,0.12478336221837089,"formulation for these models can be written as follows:
67"
PRELIMINARIES,0.1265164644714038,"˜hi = f(h′
i)
where
h′
i
def
=
X"
PRELIMINARIES,0.12824956672443674,"j∈N∗
i
γijWvhj
and
γij
def
=
exp(Ψ(hi, hj))
P"
PRELIMINARIES,0.12998266897746968,"ℓ∈N∗
i exp(Ψ(hi, hℓ)) .
(3)"
PRELIMINARIES,0.1317157712305026,"Here, Ψ(hi, hj)
def
= α(Wqhi, Wkhj) is known as the score function (or attention architecture), and
68"
PRELIMINARIES,0.13344887348353554,"measures the similarity between the messages hi and hj (or more generally, between a learnable
69"
PRELIMINARIES,0.13518197573656845,"mapping of the messages). From these scores, the (attention) coefficients are obtained by normalizing
70"
PRELIMINARIES,0.1369150779896014,"them, such that P"
PRELIMINARIES,0.1386481802426343,"j γij = 1. We can find in the literature different attention layers. Throughout this
71"
PRELIMINARIES,0.14038128249566725,"work, we focus on two types, the original GAT [30], and its extension GATv2 [5]:
72"
PRELIMINARIES,0.14211438474870017,"GAT:
Ψ(hi, hj) = LeakyRelu
 
a⊤[Wqhi||Wkhj]

,
(4)"
PRELIMINARIES,0.1438474870017331,"GATv2:
Ψ(hi, hj) = a⊤LeakyRelu (Wqhi + Wkhj) ,
(5)
where the learnable parameters are now the attention vector a; and the matrices Wq, Wk, and Wv.
73"
PRELIMINARIES,0.14558058925476602,"Following previous work [5, 30], we assume that these matrices are coupled, i.e., Wq = Wk = Wv.
74"
PRELIMINARIES,0.14731369150779897,"Note that the difference between the two layers lies in the position of the vector a: by taking it out of
75"
PRELIMINARIES,0.14904679376083188,"the nonlinearity, Brody et al. [5] increased the expressiveness of GATv2. Now, the product of a and a
76"
PRELIMINARIES,0.15077989601386482,"weight matrix does not collapse into another vector. More importantly, the addition of two different
77"
PRELIMINARIES,0.15251299826689774,"attention layers will help us show the versatility of the proposed models later in §6.
78"
PREVIOUS WORK,0.15424610051993068,"3
Previous work
79"
PREVIOUS WORK,0.1559792027729636,"In recent years, there has been a surge of research in GNNs. Here, we discuss other GNN models,
80"
PREVIOUS WORK,0.15771230502599654,"attention mechanisms, and the recent findings on the limitations of GCNs and GATs.
81"
PREVIOUS WORK,0.15944540727902945,"The literature on GNNs is extensive [4, 14, 21, 34], and more abstract definitions of a message-
82"
PREVIOUS WORK,0.1611785095320624,"passing GNN are possible, leading to other lines of work trying different ways to compute messages,
83"
PREVIOUS WORK,0.16291161178509533,"aggregate them, or update the final message [7, 13, 35]. Alternatively, another line of work fully
84"
PREVIOUS WORK,0.16464471403812825,"abandons message-passing, working instead with higher-order interactions [22]. While some of this
85"
PREVIOUS WORK,0.1663778162911612,"work is orthogonal—or directly applicable—to the proposed model [7, 13, 35], here we focus on
86"
PREVIOUS WORK,0.1681109185441941,"convolutional and attention graph layers, as they are the most widely used (and cited) as of today.
87"
PREVIOUS WORK,0.16984402079722705,"While we consider the original GAT [30] and GATv2 [5], our work can be directly applied to any
88"
PREVIOUS WORK,0.17157712305025996,"attention model that sticks to the formulation in Eq. 3. For example, some works propose different
89"
PREVIOUS WORK,0.1733102253032929,"metrics for the score function, like the dot-product [5], cosine similarity [28], or a combination of
90"
PREVIOUS WORK,0.17504332755632582,"various functions [17]. Other works introduce transformer-based mechanisms [29] based on positional
91"
PREVIOUS WORK,0.17677642980935876,"encoding [9, 20] or on the set transformer [31]. Finally, there also exist attention approaches designed
92"
PREVIOUS WORK,0.17850953206239167,"for specific type of graphs, such as relational [6, 37] or heterogeneous graphs [16, 32].
93"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.18024263431542462,"3.1
On the limitations of GCN and GAT networks
94"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.18197573656845753,"In [2], the authors study classification on a Gaussian mixture, where the data correspond to the node
95"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.18370883882149047,"features of a stochastic block model. They showed that when the graph is neither too sparse nor noisy,
96"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.1854419410745234,"applying one layer of graph convolution increases the regime in which the data is linearly separable.
97"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.18717504332755633,"Namely, if the distance between the means of the classes is not too small, the convolved features are
98"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.18890814558058924,"linearly separable, whilst the original features are not. However, the above result is highly sensitive
99"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.19064124783362218,"to the graph structure. Indeed, even if the distance between the means is large, the convolution cannot
100"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.1923743500866551,"make the data linearly separable when the graph is noisy, since the convolution operation essentially
101"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.19410745233968804,"collapses the means of the two classes to the same value.
102"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.19584055459272098,"More recently, Fountoulakis et al. [10] showed that GAT is able to remedy the above issue, and
103"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.1975736568457539,"provide perfect node separability regardless of the noise level in the graph. Specifically, they showed
104"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.19930675909878684,"that if the distance between the means is large compared to the standard deviation, then GAT achieves
105"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.20103986135181975,"perfect node separability with high probability. However, a classical argument (see [1]) states that
106"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2027729636048527,"in this setting graph-based models are unnecessary, since a simple linear classifier already achieves
107"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2045060658578856,"perfect separability (see Proposition 4 in [10]). In addition, when the distance between the means
108"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.20623916811091855,"is small compared to σ, no score function Ψ can drop inter-class edges (the noisy edges), and thus
109"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.20797227036395147,"might not achieve perfect node separability (see Conjecture 7 in [10]).
110"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2097053726169844,"The above discussion implies that for some datasets, GAT might not work as well as expected.
111"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.21143847487001732,"However, it leaves open the question of which architecture (GCN or GAT) is preferable in terms of
112"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.21317157712305027,"performance.
113"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.21490467937608318,"4
Convolved attention: benefits and hurdles
114"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.21663778162911612,"In this section, we propose to combine attention with convolution operations. To motivate it, we
115"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.21837088388214904,"complement the results of [10], providing a synthetic dataset for which any 1-layer GCN fails, but
116"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.22010398613518198,"1-layer GAT does not. Thus, proving a clear distinction between GAT and GCN layers. Besides, we
117"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2218370883882149,"show that convolution helps GAT as long as the graph noise is reasonable. The proofs for the two
118"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.22357019064124783,"statements in this section appear in Appendix A and follow similar arguments as in [10].
119"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.22530329289428075,"This synthetic dataset is based on the contextual stochastic block model (CSBM) [8]. Let ε1, . . . , εn
120"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2270363951473137,"be i.i.d uniform samples from {−1, 0, 1}. Let Ck = {j ∈[n] | εj = k} for k ∈{−1, 0, 1}.
121"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.22876949740034663,"We set the feature vector Xi ∼N(εi · µ, I · σ2) where µ ∈Rd, σ ∈R, and I ∈{0, 1}d×d
122"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.23050259965337955,"is the identity matrix. For a given pair p, q ∈[0, 1] we consider the stochastic adjacency matrix
123"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2322357019064125,"A ∈{0, 1}n×n defined as follows: for i, j ∈[n] in the same class (i.e., intra-edge), we set
124"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2339688041594454,"aij ∼Ber(p);1 for i, j in different classes (i.e., inter-edge), we set aij ∼Ber(q). We denote by
125"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.23570190641247835,"(X, A) ∼CSBM(n, p, q, µ, σ2) a sample obtained according to the above random process. Our task
126"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.23743500866551126,"is then to distinguish (or separate) nodes from C0 vs. C−1 ∪C1.
127"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2391681109185442,1Ber(·) denote the Bernoulli distribution.
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.24090121317157712,"Note that it is impossible to separate C0 from C−1 ∪C1 with a linear classifier (with high probability).
128"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.24263431542461006,"In addition, by applying similar arguments as in [2], using one convolutional layer is detrimental for
129"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.24436741767764297,"node classification on the CSBM.2 This follows from the fact that although the convolution brings
130"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.24610051993067592,"the means closer and shrinks the variance, the geometric structure of the problem does not change.
131"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.24783362218370883,"On the other hand, we prove that GAT is able to achieve perfect node separability when the graph is
132"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.24956672443674177,"not too sparse:
133"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2512998266897747,"Theorem 1. Suppose that p, q = Ω(log2 n/n) and ∥µ∥2 = ω(σ√log n). Then, there exists a
134"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2530329289428076,"choice of attention architecture Ψ such that, with probability at least 1 −on(1) over the data
135"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.25476603119584057,"(X, A) ∼CSBM(n, p, q, µ, σ2), GAT separates nodes C0 from C1 ∪C−1.
136"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2564991334488735,"Moreover, we show using methods from [2], that the above classification threshold ∥µ∥can be
137"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2582322357019064,"improved when the graph noise is reasonable. Specifically, by applying convolution prior to the
138"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.25996533795493937,"attention score, the variance of the data is greatly reduced, and if the graph is not too noisy, the
139"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2616984402079723,"operation dramatically lowers the bound on ∥µ∥in Theorem 1. Motivated by this, we introduce the
140"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2634315424610052,"graph convolutional attention layer (CAT), which formalizes this idea:
141"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2651646447140381,"Ψ(hi, hj) = α(W ˜hi, W ˜hj)
where
˜hi =
1
|N ∗
i | X"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2668977469670711,"ℓ∈N∗
i
hℓ,
(6)"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.268630849220104,"and where ˜hi are the convolved features of the neighborhood of node i. As we show now, CAT
142"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2703639514731369,"improves over GAT by combining convolutions with attention, when the graph noise is low.
143"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2720970537261698,"Corollary 2. Suppose p, q = Ω(log2 n/n) and ∥µ∥≥ω

σ
q"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2738301559792028,(p+2q) log n
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2755632582322357,"n(p−q)2

. Then, there is a choice
144"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2772963604852686,"of attention architecture Ψ such that, with probability at least 1 −o(1) over the data (X, A) ∼
145"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.27902946273830154,"CSBM(n, p, q, µ, σ2), CAT separates nodes C0 from C1 ∪C−1.
146"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2807625649913345,"The above proposition shows that under the CSBM data model, convolving prior to attention changes
147"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2824956672443674,"the regime for perfect node separability by a factor of |p −q|
p"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.28422876949740034,"n/(p + 2q). This is desirable when
148"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.28596187175043325,"|p −q|
p"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.2876949740034662,"n/(p + 2q) > 1, since the regime for perfect classification is increased. Nonetheless, when
149"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.28942807625649913,"|p −q|
p"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.29116117850953205,"n/(p + 2q) < 1, applying convolution prior to attention reduces the regime for perfect
150"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.292894280762565,"separability. Therefore, it is not always clear whether convolving prior to attention is beneficial.
151"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.29462738301559793,"5
L-CAT: Learning to interpolate
152"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.29636048526863085,"From the previous analysis, we can conclude that it is hard to know a priori whether attention,
153"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.29809358752166376,"convolution, or convolved attention, will perform the best. In this section, we argue that this issue can
154"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.29982668977469673,"be easily overcome by learning to interpolate between the three.
155"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.30155979202772965,"First, notice that the formulations of GCN and GAT only differ in that GCN weighs all neighbors
156"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.30329289428076256,"equally (Eq. 2) and, the more similar the attention scores are (Eq. 3), the more uniform the coefficients
157"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.3050259965337955,"γij will be. Thus, we can interpolate between GCN and GAT by introducing a learnable parameter
158"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.30675909878682844,"λ1 ∈[0, 1]. Similarly, the formulation of GAT (Eq. 3) and CAT (Eq. 6) differ in the convolution
159"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.30849220103986136,"within the score, which can be interpolated by another learnable parameter λ2 ∈[0, 1].
160"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.31022530329289427,"Following this observation, we propose the learnable convolutional attention layer (L-CAT), which
161"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.3119584055459272,"can be formulated as an attention layer with the following score:
162"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.31369150779896016,"Ψ(hi, hj) = λ1 · α(W ˜hi, W ˜hj)
where
˜hi = hi + λ2
P"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.31542461005199307,"ℓ∈Ni hℓ
1 + λ2|Ni|
,
(7)"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.317157712305026,"and where λ1, λ2 ∈[0, 1]. As mentioned before, this formulation lets L-CAT learn to interpolate
163"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.3188908145580589,"between GCN (λ1 = 0), GAT (λ1 = 1 and λ2 = 0), and CAT (λ1 = 1 and λ2 = 1).
164"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.32062391681109187,"Despite its simplicity, L-CAT enables a number of non-trivial benefits. Not only can it switch between
165"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.3223570190641248,"existing layers, but it can also learn to use the amount of attention necessary for each use-case.
166"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.3240901213171577,"Moreover, by comprising the three layers in a single learnable formulation, it removes the necessity
167"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.32582322357019067,"of cross-validating the type of layer, since their performance is data-dependent (see §§3.1 and 4).
168"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.3275563258232236,"More importantly, it eases the task of combining different layer types within the same architecture.
169"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.3292894280762565,2We note that this problem can be easily solved by two layers of GCN [3].
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.3310225303292894,"GCN
GAT
CAT"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.3327556325823224,"0.2
0.4
q 50 75 100"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.3344887348353553,Accuracy (%)
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.3362218370883882,||µ||=0.1
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.3379549393414211,"0.2
0.4
q 60 80 100"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.3396880415944541,||µ||=4.3
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.341421143847487,"10−1
101
||µ|| 50 75 100 q=0.1"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.3431542461005199,"10−1
101
||µ|| 50 75 100 q=0.3"
ON THE LIMITATIONS OF GCN AND GAT NETWORKS,0.34488734835355284,"Figure 1: Synthetic data results. In the two left-most figures, we show how the accuracy varies with
the noise level q for ∥µ∥= 0.1 and ∥µ∥= 4.3. In the two right-most figures, we show how the
accuracy varies with the norm of the means ∥µ∥for q = 0.1 and q = 0.3. We use two vertical lines
to present the classification threshold stated in Theorem 1 (solid line) and Corollary 2 (dashed line)."
EXPERIMENTS,0.3466204506065858,"6
Experiments
170"
EXPERIMENTS,0.3483535528596187,"In this section, we assess the performance of the proposed models, CAT and L-CAT. First, we
171"
EXPERIMENTS,0.35008665511265163,"validate our theoretical findings on synthetic data (§6.1). Second, we compare all methods in various
172"
EXPERIMENTS,0.35181975736568455,"small-scale node classification tasks (§6.2). And finally, we evaluate the proposed models on more
173"
EXPERIMENTS,0.3535528596187175,"realistic scenarios from the Open Benchmark Graph [15] framework, assessing their performance
174"
EXPERIMENTS,0.35528596187175043,"and robustness to feature noise and network initialization (§6.3).
175"
SYNTHETIC DATA,0.35701906412478335,"6.1
Synthetic data
176"
SYNTHETIC DATA,0.3587521663778163,"In this section, we empirically validate our theoretical results (Theorem 1 and Corollary 2). We aim
177"
SYNTHETIC DATA,0.36048526863084923,"to better understand the behavior of each layer as the properties of the data change, i.e., the noise
178"
SYNTHETIC DATA,0.36221837088388215,"level q (proportion of inter-edges) and the distance between the means of consecutive classes ∥µ∥.
179"
SYNTHETIC DATA,0.36395147313691506,"We provide in Appendix B extra results and additional experiments.
180"
SYNTHETIC DATA,0.36568457538994803,"Experimental setup. As data model, we use the proposed CSBM (see §4) with n = 10000, p = 0.5,
181"
SYNTHETIC DATA,0.36741767764298094,"σ = 0.1, and d = n/
 
5 log2(n)

. All results are averaged over 50 runs, and parameters are set as
182"
SYNTHETIC DATA,0.36915077989601386,"described in Appendix A. To assess the sensitivity to structural noise, we perform two experiments.
183"
SYNTHETIC DATA,0.3708838821490468,"First, we vary the noise level q between 0 and 0.5, leaving the mean vector µ fixed. We test two values
184"
SYNTHETIC DATA,0.37261698440207974,"of ∥µ∥: the first corresponds to the easy regime (∥µ∥= 10σ√2 log n) where classes are far apart;
185"
SYNTHETIC DATA,0.37435008665511266,"and the second correspond to the hard regime (∥µ∥= σ) where the distance between the clusters is
186"
SYNTHETIC DATA,0.37608318890814557,"small. In the second experiment we modify instead the distance between the means, sweeping ∥µ∥in
187"
SYNTHETIC DATA,0.3778162911611785,"the range

σ/20, 20σ√2 log n

, and thus covering the transition from the hard setting (small ∥µ∥) to
188"
SYNTHETIC DATA,0.37954939341421146,"the easy one (large ∥µ∥). Here, we fix q to 0.1 (low noise) and 0.3 (high noise). In both cases, we
189"
SYNTHETIC DATA,0.38128249566724437,"compare the behavior of 1-layer GAT and CAT, and include GCN as the baseline.
190"
SYNTHETIC DATA,0.3830155979202773,"Results. The two left-most plots of Fig. 1 show the node classification performance for the hard
191"
SYNTHETIC DATA,0.3847487001733102,"and easy regimes, respectively, as we vary the noise level q. In the hard regime, we observe that
192"
SYNTHETIC DATA,0.38648180242634317,"GAT is unable to achieve separation for any value of q, whereas CAT achieves perfect classification
193"
SYNTHETIC DATA,0.3882149046793761,"when q is small enough. This exemplifies the advantage of CAT over GAT as stated in Corollary 2.
194"
SYNTHETIC DATA,0.389948006932409,"When the distance between the means is large enough, we see that GAT achieves perfect results
195"
SYNTHETIC DATA,0.39168110918544197,"independently of q, as stated in Theorem 1. In contrast, as we increase q, CAT fails to satisfy the
196"
SYNTHETIC DATA,0.3934142114384749,"condition in Corollary 2, and therefore achieves inferior performance.
197"
SYNTHETIC DATA,0.3951473136915078,"The results for the second set of experiments, where we fix q and sweep ∥µ∥, are shown in the
198"
SYNTHETIC DATA,0.3968804159445407,"right-most part of Fig. 1. In these two plots, we can appreciate the transition in the accuracy of
199"
SYNTHETIC DATA,0.3986135181975737,"both GAT and CAT as a function of ∥µ∥. We observe that GAT achieves perfect accuracy when
200"
SYNTHETIC DATA,0.4003466204506066,"the distance between the means satisfies the condition in Theorem 1 (solid vertical line in Fig. 1).
201"
SYNTHETIC DATA,0.4020797227036395,"Moreover, we can see the improvement CAT obtains over GAT. Indeed, when ∥µ∥satisfies the
202"
SYNTHETIC DATA,0.4038128249566724,"conditions of Corollary 2 (dashed vertical line in Fig. 1), the classification threshold is improved. As
203"
SYNTHETIC DATA,0.4055459272097054,"we increase q we see that the gap between the two vertical lines decreases, which means that the
204"
SYNTHETIC DATA,0.4072790294627383,"improvement decreases as q increments, exactly as stated in Corollary 2.
205"
SYNTHETIC DATA,0.4090121317157712,"These results—along with empirical evidence in the next sections—reinforce the idea that there is a
206"
SYNTHETIC DATA,0.41074523396880414,"priori no way to tell which layer to use, as their performance highly depend on the properties of the
207"
SYNTHETIC DATA,0.4124783362218371,"data. Prior to this work, this has been solved by cross-validating the layer type. In the next sections,
208"
SYNTHETIC DATA,0.41421143847487,"Table 1: Test accuracy (%) of the considered convolution and attention models for different datasets
(sorted by their average node degree), and averaged over ten runs. Bold numbers are statistically
different to their baseline model (α = 0.05). Best average performance is underlined."
SYNTHETIC DATA,0.41594454072790293,"Dataset
Amazon
Computers
Amazon
Photo
GitHub
Facebook
PagePage
Coauthor
Physics
TwitchEN"
SYNTHETIC DATA,0.41767764298093585,"Avg. Deg.
35.76
31.13
15.33
15.22
14.38
10.91"
SYNTHETIC DATA,0.4194107452339688,"GCN
90.59 ± 0.36
95.13 ± 0.57
84.13 ± 0.44
94.76 ± 0.19
96.36 ± 0.10
57.83 ± 1.13"
SYNTHETIC DATA,0.42114384748700173,"GAT
89.59 ± 0.61
94.02 ± 0.66
83.31 ± 0.18
94.16 ± 0.48
96.36 ± 0.10
57.59 ± 1.20
CAT
90.58 ± 0.40
94.77 ± 0.47
84.11 ± 0.66
94.71 ± 0.30
96.40 ± 0.10
58.09 ± 1.61
L-CAT
90.34 ± 0.47
94.93 ± 0.37
84.05 ± 0.70
94.81 ± 0.25
96.35 ± 0.10
57.88 ± 2.07"
SYNTHETIC DATA,0.42287694974003465,"GATv2
89.49 ± 0.53
93.47 ± 0.62
82.92 ± 0.45
93.44 ± 0.30
96.24 ± 0.19
57.70 ± 1.17
CATv2
90.44 ± 0.46
94.81 ± 0.55
84.10 ± 0.88
94.27 ± 0.31
96.34 ± 0.12
57.99 ± 2.02
L-CATv2
90.33 ± 0.44
94.79 ± 0.61
84.31 ± 0.59
94.44 ± 0.39
96.29 ± 0.13
57.89 ± 1.53"
SYNTHETIC DATA,0.4246100519930676,"we empirically demonstrate that L-CAT can automatically perform layer selection during training,
209"
SYNTHETIC DATA,0.42634315424610053,"completely removing the need of cross-validating and, thus, saving computational resources.
210"
REAL DATA,0.42807625649913345,"6.2
Real data
211"
REAL DATA,0.42980935875216636,"In this section, we study the performance of the proposed models in a comprehensive set of real-world
212"
REAL DATA,0.43154246100519933,"experiments, in order to gain further insights into the settings in which they excel. Specifically, we
213"
REAL DATA,0.43327556325823224,"found CAT and L-CAT to outperform their baseline models as the average node degree increases. For
214"
REAL DATA,0.43500866551126516,"a detailed description of the datasets and additional results, refer to Appendices C and D.
215"
REAL DATA,0.43674176776429807,"Models. We consider as baselines a simple GCN layer [18] where all neighbors are uniformly
216"
REAL DATA,0.43847487001733104,"weighted, as well as the original GAT layer [30] and its recent extension, GATv2 [5]. See §2 for an
217"
REAL DATA,0.44020797227036396,"introduction. Based on the two attention models, we consider their CAT-extensions, CAT and CATv2,
218"
REAL DATA,0.44194107452339687,"as well as their interpolatable counterparts, L-CAT and L-CATv2. To ensure fair comparisons, all
219"
REAL DATA,0.4436741767764298,"layers use the same number of parameters and share the same implementation, appropriately setting
220"
REAL DATA,0.44540727902946275,"λ1 and λ2 (see Eq. 7) for each model.
221"
REAL DATA,0.44714038128249567,"Datasets. We take six node classification datasets. The FacebookPagePage/GitHub/TwitchEN datasets
222"
REAL DATA,0.4488734835355286,"relate to social-network graphs [24], whose nodes represent verified pages/developers/streamers; and
223"
REAL DATA,0.4506065857885615,"where the task is to predict the topic/expertise/explicit language usage of the node. The Coauthor
224"
REAL DATA,0.45233968804159447,"Physics dataset [27] represents a co-authorship network whose nodes represent authors, and the task
225"
REAL DATA,0.4540727902946274,"is to infer their main research field. Finally, the Amazon Computers/Amazon Photo datasets represent
226"
REAL DATA,0.4558058925476603,"two product-similarity graphs [27], where each node is a product, and the task is to infer its category.
227"
REAL DATA,0.45753899480069327,"Experimental setup. To ensure the best results, we cross-validate all optimization-related hyperpa-
228"
REAL DATA,0.4592720970537262,"rameters for each model using GraphGym [36]. All models use four GNN layers with hidden size
229"
REAL DATA,0.4610051993067591,"of 32, and thus have an equal number of parameters. For evaluation, we take the best-validation
230"
REAL DATA,0.462738301559792,"configuration during training, and report test-set performance. For further details, refer to Appendix D.
231"
REAL DATA,0.464471403812825,"Results are presented in Table 1, averaged over 10 runs. In contrast with §6.1, we here find GCN to
232"
REAL DATA,0.4662045060658579,"be a strong contender, reinforcing its viability in real-world data despite its simplicity. Moreover, we
233"
REAL DATA,0.4679376083188908,"observe both CAT and L-CAT not only holding up the performance with respect to their baselines
234"
REAL DATA,0.4696707105719237,"models for all datasets, but in most cases they also improve the test accuracy in a statistically
235"
REAL DATA,0.4714038128249567,"significant manner. These results validate the effectiveness of CAT as a GNN layer, and show the
236"
REAL DATA,0.4731369150779896,"viability of L-CAT as a drop-in replacement, achieving good results on all datasets.
237"
REAL DATA,0.4748700173310225,"10
20
30
Average node degree 0.5 0.0 0.5 1.0 1.5"
REAL DATA,0.47660311958405543,Acc. Improvement (%)
REAL DATA,0.4783362218370884,"As explained in §4, CAT differs from a usual GAT in that the score
238"
REAL DATA,0.4800693240901213,"is computed with respect to the convolved features. Intuitively,
239"
REAL DATA,0.48180242634315423,"this means that CAT should excel in those settings where nodes
240"
REAL DATA,0.48353552859618715,"are better connected, allowing CAT to extract more information
241"
REAL DATA,0.4852686308492201,"from their neighborhoods. Indeed, there exists a positive correlation
242"
REAL DATA,0.48700173310225303,"between performance improvement and average degree of the graph.
243"
REAL DATA,0.48873483535528595,"In the inset figure, we can observe the improvement in accuracy of
244"
REAL DATA,0.4904679376083189,"CAT with respect to its baseline model, as a function of the average
245"
REAL DATA,0.49220103986135183,"node degree of the datasets, and the linear regression of these results
246"
REAL DATA,0.49393414211438474,"Table 2: Test performance of the considered convolutional and attention layers on four OGB datasets,
averaged over five runs. Bold numbers are statistically different to their baseline model (α = 0.05).
Best average performance is underlined. Left table: accuracy (%); right table: AUC-ROC (%)."
REAL DATA,0.49566724436741766,"Dataset
arxiv
products
mag"
REAL DATA,0.49740034662045063,"GCN
71.58 ± 0.20
74.12 ± 1.20
32.77 ± 0.36"
REAL DATA,0.49913344887348354,"GAT
71.58 ± 0.16
78.53 ± 0.91
32.15 ± 0.31
CAT
72.14 ± 0.21
77.38 ± 0.36
31.98 ± 0.46
L-CAT
71.99 ± 0.08
77.19 ± 1.11
32.47 ± 0.38"
REAL DATA,0.5008665511265165,"GATv2
71.73 ± 0.24
76.40 ± 0.71
32.76 ± 0.18
CATv2
72.03 ± 0.09
74.81 ± 1.12
32.43 ± 0.22
L-CATv2
71.97 ± 0.22
76.37 ± 0.92
32.68 ± 0.50"
REAL DATA,0.5025996533795494,proteins
REAL DATA,0.5043327556325823,80.10 ± 0.55
REAL DATA,0.5060658578856152,"79.08 ± 1.47
73.26 ± 1.65
79.63 ± 0.71"
REAL DATA,0.5077989601386482,"78.65 ± 1.44
74.33 ± 0.94
79.07 ± 0.98"
REAL DATA,0.5095320623916811,"(dashed line). This plot includes all datasets (from the manuscript and Appendix D), and shows a
247"
REAL DATA,0.511265164644714,"positive trend between node connectivity and improved performance by CAT.
248"
OPEN GRAPH BENCHMARK,0.512998266897747,"6.3
Open Graph Benchmark
249"
OPEN GRAPH BENCHMARK,0.5147313691507799,"In this section, we assess the robustness of the proposed models, in order to fully understand their
250"
OPEN GRAPH BENCHMARK,0.5164644714038128,"benefits. For further details and additional results, refer to Appendix E.
251"
OPEN GRAPH BENCHMARK,0.5181975736568457,"Datasets. We consider four different datasets from the Open Graph Benchmark (OGB) suite [15]:
252"
OPEN GRAPH BENCHMARK,0.5199306759098787,"proteins, products, arxiv, and mag. Note that these datasets are significantly larger than those from
253"
OPEN GRAPH BENCHMARK,0.5216637781629117,"§6.2 and correspond to more difficult tasks, e.g., arxiv is a 40-class classification problem (see Table 4
254"
OPEN GRAPH BENCHMARK,0.5233968804159446,"for details). This makes them more suitable for the proposed analysis.
255"
OPEN GRAPH BENCHMARK,0.5251299826689775,"Experimental setup. We adopt the same experimental setup as Brody et al. [5] for the proteins,
256"
OPEN GRAPH BENCHMARK,0.5268630849220104,"products, and mag datasets. For the arxiv dataset, we use instead the example code from OGB [15], as
257"
OPEN GRAPH BENCHMARK,0.5285961871750433,"it yields better performance than that of Brody et al. [5]. Just as in §6.2, we compare with GCN [18],
258"
OPEN GRAPH BENCHMARK,0.5303292894280762,"GAT [30], GATv2 [5], and their CAT and L-CAT counterparts. We cross-validate the number of
259"
OPEN GRAPH BENCHMARK,0.5320623916811091,"heads (1 and 8), repeat each experiment five times, and select the best-validation models during
260"
OPEN GRAPH BENCHMARK,0.5337954939341422,"training. All models share the network architecture, number of parameters, and network initialization.
261"
OPEN GRAPH BENCHMARK,0.5355285961871751,"Results are summarized in Table 2, averaged over 5 runs. Here we do not observe a clear preferred
262"
OPEN GRAPH BENCHMARK,0.537261698440208,"baseline: GCN performs really well in proteins and mag; GAT excels in products; and GATv2 does
263"
OPEN GRAPH BENCHMARK,0.5389948006932409,"well in arxiv and mag. Let us now focus on the proposed models. While CAT obtains the best results
264"
OPEN GRAPH BENCHMARK,0.5407279029462738,"on arxiv, its performance on proteins and products is significantly worse than the baseline model.
265"
OPEN GRAPH BENCHMARK,0.5424610051993067,"Presumably, an excessive amount of inter-edges could explain why convolving the features prior to
266"
OPEN GRAPH BENCHMARK,0.5441941074523396,"computing the score is harmful, as seen in §6.1. As we explore in §6.3.2, however, CAT improves
267"
OPEN GRAPH BENCHMARK,0.5459272097053726,"over its baseline for most proteins scenarios, specially with a single head. In stark contrast, L-CAT
268"
OPEN GRAPH BENCHMARK,0.5476603119584056,"performs remarkably well, improving the baseline models in all datasets but products—even on those
269"
OPEN GRAPH BENCHMARK,0.5493934142114385,"in which CAT fails—demonstrating the adaptability of L-CAT to different scenarios.
270"
OPEN GRAPH BENCHMARK,0.5511265164644714,"In order to better understand the training dynamics of the different models, we plot in Fig. 2a the test
271"
OPEN GRAPH BENCHMARK,0.5528596187175043,"accuracy of GCN and the GATv2 models during training on the arxiv dataset. Interestingly, this plot
272"
OPEN GRAPH BENCHMARK,0.5545927209705372,"shows that while all models obtained similar final results, CATv2 and L-CATv2 drastically improved
273"
OPEN GRAPH BENCHMARK,0.5563258232235702,"their convergence speed and stability with respect to GATv2, matching that of GCN. To understand
274"
OPEN GRAPH BENCHMARK,0.5580589254766031,"the behavior of L-CATv2, we show in Fig. 2b the evolution of the parameters λ1, λ2. We observe
275"
OPEN GRAPH BENCHMARK,0.5597920277296361,"that to achieve these results, L-CATv2 converged to a GNN network that combines three types of
276"
OPEN GRAPH BENCHMARK,0.561525129982669,"layers: the first layer is a CATv2 layer, taking advantage of the neighboring information; the second
277"
OPEN GRAPH BENCHMARK,0.5632582322357019,"layer is a quasi-GCN layer, in which scores are almost uniform and some neighboring information
278"
OPEN GRAPH BENCHMARK,0.5649913344887348,"is still used in the score; and the third layer is a pure GCN layer, in which all scores are uniformly
279"
OPEN GRAPH BENCHMARK,0.5667244367417678,"distributed. It is important to remark that these dynamics are fairly consistent, as L-CATv2 reached
280"
OPEN GRAPH BENCHMARK,0.5684575389948007,"the same λ1, λ2 values over all five runs.
281"
OPEN GRAPH BENCHMARK,0.5701906412478336,"0
100
200
300
400
500
Epoch 20 30 40 50 60 70"
OPEN GRAPH BENCHMARK,0.5719237435008665,Test accuracy ArXiv
OPEN GRAPH BENCHMARK,0.5736568457538995,"GCN
GATv2
CATv2
L-CATv2"
OPEN GRAPH BENCHMARK,0.5753899480069324,(a) Test accuracy.
OPEN GRAPH BENCHMARK,0.5771230502599654,"0
250
500
750
1000 1250 1500
Epoch 0.0 0.2 0.4 0.6 0.8 1.0"
OPEN GRAPH BENCHMARK,0.5788561525129983,"1,
2 values"
OPEN GRAPH BENCHMARK,0.5805892547660312,ArXiv - L-CATv2
OPEN GRAPH BENCHMARK,0.5823223570190641,"L1
L2
L3 1 2"
OPEN GRAPH BENCHMARK,0.584055459272097,"(b) Evolution of λ1, λ2."
OPEN GRAPH BENCHMARK,0.58578856152513,"Figure 2: Behavior of GCN and GATv2 models during training on the arxiv dataset. (a) CAT and
L-CAT converge quicker and are more stable than their baseline model. (b) L-CAT consistently
converges to the same type of layers during training: a CAT →quasi-GCN→GCN network."
ROBUSTNESS TO NOISE,0.587521663778163,"6.3.1
Robustness to noise
282"
ROBUSTNESS TO NOISE,0.5892547660311959,"One intrinsic aspect of real world data is the existence of noise. In this section, we explore the
283"
ROBUSTNESS TO NOISE,0.5909878682842288,"robustness of the proposed models to different levels of homoscedastic noise in the features. That is,
284"
ROBUSTNESS TO NOISE,0.5927209705372617,"we attempt to simulate scenarios where there exist measurement inaccuracies in the input features.
285"
ROBUSTNESS TO NOISE,0.5944540727902946,"Experimental setup. For these experiments we consider the arxiv dataset, and the same experimental
286"
ROBUSTNESS TO NOISE,0.5961871750433275,"setup as in §6.3. To simulate homoscedastic noise, we introduce to the node features additive noise
287"
ROBUSTNESS TO NOISE,0.5979202772963604,"of the form x′ = x + ε, where ε ∼N(0, 1σ), and where we consider different levels of noise,
288"
ROBUSTNESS TO NOISE,0.5996533795493935,"specifically, we take σ ∈{0, 0.25, 0.5, 0.75, 1}.
289"
ROBUSTNESS TO NOISE,0.6013864818024264,"0.00
0.25
0.50
0.75
1.00
Noise level ( ) 60 62 64 66 68 70 72"
ROBUSTNESS TO NOISE,0.6031195840554593,Test accuracy
ROBUSTNESS TO NOISE,0.6048526863084922,"GCN
GAT
CAT
L-CAT"
ROBUSTNESS TO NOISE,0.6065857885615251,"Results can be seen in the inset figure, which shows test accuracy
290"
ROBUSTNESS TO NOISE,0.608318890814558,"as a function of the feature noise level σ. This plot summarizes
291"
ROBUSTNESS TO NOISE,0.610051993067591,"the performance of all considered models, over five runs and
292"
ROBUSTNESS TO NOISE,0.6117850953206239,"two numbers of heads (1 and 8). We can observe that baseline
293"
ROBUSTNESS TO NOISE,0.6135181975736569,"attention models exhibit high variance and are quite sensitive
294"
ROBUSTNESS TO NOISE,0.6152512998266898,"to small perturbations. GCNs, instead, exhibit better robustness
295"
ROBUSTNESS TO NOISE,0.6169844020797227,"to noise and small variance. In concordance with the synthetic
296"
ROBUSTNESS TO NOISE,0.6187175043327556,"experiments (see §§4 and 6.1), we observe that CAT is able to
297"
ROBUSTNESS TO NOISE,0.6204506065857885,"leverage convolutions as a variance-reduction technique, boosting
298"
ROBUSTNESS TO NOISE,0.6221837088388215,"the performance of the attention mechanisms, and reducing their
299"
ROBUSTNESS TO NOISE,0.6239168110918544,"variance. Moreover, L-CAT proves to be strictly more robust than all other models: it boosts the
300"
ROBUSTNESS TO NOISE,0.6256499133448874,"performance and reduces the uncertainty—like CAT—and it is more effective than other approaches
301"
ROBUSTNESS TO NOISE,0.6273830155979203,"as it can adapt the amount of attention used in each layer, outperforming competing methods.
302"
ROBUSTNESS TO NETWORK INITIALIZATION,0.6291161178509532,"6.3.2
Robustness to network initialization
303"
ROBUSTNESS TO NETWORK INITIALIZATION,0.6308492201039861,"Another important aspect of real-world applications is that of robustness to network initialization,
304"
ROBUSTNESS TO NETWORK INITIALIZATION,0.6325823223570191,"i.e., the ability to obtain satisfying performance independently of the initial parameters. Otherwise,
305"
ROBUSTNESS TO NETWORK INITIALIZATION,0.634315424610052,"a practitioner could waste lots of resources trying different initilizations or, even worse, give up on
306"
ROBUSTNESS TO NETWORK INITIALIZATION,0.6360485268630849,"a model just because they did not try the initial parameters that would yield great results. In this
307"
ROBUSTNESS TO NETWORK INITIALIZATION,0.6377816291161178,"section, we test such a scenario using the proteins dataset as an example setting.
308"
ROBUSTNESS TO NETWORK INITIALIZATION,0.6395147313691508,"Experimental setup. We follow once again the same setup for proteins as in §6.3. We consider two
309"
ROBUSTNESS TO NETWORK INITIALIZATION,0.6412478336221837,"different network initializations. The first one, uniform, uses uniform Glorot initilization [12] with a
310"
ROBUSTNESS TO NETWORK INITIALIZATION,0.6429809358752167,"gain of 1, which is the standard initialization used throughout this work. The second one, normal,
311"
ROBUSTNESS TO NETWORK INITIALIZATION,0.6447140381282496,"uses instead normal Glorot initialization [12] with a gain of
√"
THIS IS THE INITIALIZATION EMPLOYED,0.6464471403812825,"2. This is the initialization employed
312"
THIS IS THE INITIALIZATION EMPLOYED,0.6481802426343154,"on the original GATv2 paper [5] exclusively for the proteins dataset.
313 GCN"
THIS IS THE INITIALIZATION EMPLOYED,0.6499133448873483,"uniform
61.08 ± 2.56
normal
80.10 ± 0.55"
THIS IS THE INITIALIZATION EMPLOYED,0.6516464471403813,"average
70.59 ± 10.21"
THIS IS THE INITIALIZATION EMPLOYED,0.6533795493934142,"Results—segregated by number of heads—are shown in Table 3, while
314"
THIS IS THE INITIALIZATION EMPLOYED,0.6551126516464472,"the results for GCN appear in the inset table. These results show that the
315"
THIS IS THE INITIALIZATION EMPLOYED,0.6568457538994801,"baseline models perform poorly on the uniform initialization. However,
316"
THIS IS THE INITIALIZATION EMPLOYED,0.658578856152513,"this is somewhat alleviated when using 8 heads in the attention models.
317"
THIS IS THE INITIALIZATION EMPLOYED,0.6603119584055459,"Moreover, all baselines significantly improve with normal initialization,
318"
THIS IS THE INITIALIZATION EMPLOYED,0.6620450606585788,"Table 3: Test AUC-ROC (%) on the proteins dataset for attention models with two different network
initializations (see §6.3.2), using 1 head (top) and 8 heads (bottom)."
THIS IS THE INITIALIZATION EMPLOYED,0.6637781629116117,"GAT
CAT
L-CAT
GATv2
CATv2
L-CATv2"
H,0.6655112651646448,1h
H,0.6672443674176777,"uniform
59.73 ± 3.61
64.32 ± 2.33
77.77 ± 1.28
59.85 ± 2.73
64.32 ± 2.33
79.08 ± 0.95
normal
66.38 ± 6.94
73.26 ± 1.65
78.06 ± 1.25
69.13 ± 8.48
74.33 ± 0.94
79.07 ± 0.98"
H,0.6689774696707106,8h
H,0.6707105719237435,"uniform
72.23 ± 2.86
73.60 ± 1.14
78.85 ± 1.57
75.21 ± 1.61
74.16 ± 1.30
78.77 ± 0.97
normal
79.08 ± 1.47
74.67 ± 1.15
79.63 ± 0.71
78.65 ± 1.44
73.40 ± 0.56
79.30 ± 0.49"
H,0.6724436741767764,"average
69.36 ± 8.52
73.93 ± 1.35
78.58 ± 1.48
70.71 ± 8.70
71.55 ± 4.54
79.05 ± 0.91"
H,0.6741767764298093,"1
2
3
4
5
6
Layer 0.00 0.25 0.50 0.75 Value"
H,0.6759098786828422,"uniform - 
1"
H,0.6776429809358753,"1
2
3
4
5
6
Layer"
H,0.6793760831889082,"uniform - 
2"
H,0.6811091854419411,"1
2
3
4
5
6
Layer"
H,0.682842287694974,"normal - 
1"
H,0.6845753899480069,"1
2
3
4
5
6
Layer"
H,0.6863084922010398,"normal - 
2 heads 1
8"
H,0.6880415944540728,"Figure 3: Distribution of λ1, λ2 on proteins dataset for L-CAT across initializations."
H,0.6897746967071057,"being GCN the best model, and attention models obtaining 79 % accuracy on average with 8 heads.
319"
H,0.6915077989601387,"Compared to the baselines, CAT does a good job and improves the performance in all cases except
320"
H,0.6932409012131716,"for normal with 8 heads. Remarkably, L-CAT consistently obtains a high accuracy in all scenarios
321"
H,0.6949740034662045,"and runs. This can be further appreciated by looking at the average accuracy (bottom row), showing
322"
H,0.6967071057192374,"that L-CAT is clearly more robust to parameter initialization than competing models.
323"
H,0.6984402079722704,"To understand this performance, we inspect the distribution of λ1, λ2 for L-CAT in Fig. 3. Here,
324"
H,0.7001733102253033,"we can spot a few interesting patterns. First, the first and last layers are always GCNs, while the
325"
H,0.7019064124783362,"inner layers progressively admit less attention. Second, the number of heads affects the amount of
326"
H,0.7036395147313691,"attention allowed in the network; the more heads, the more expressive the layer tends to be, and the
327"
H,0.7053726169844021,"more attention that is permitted. Third, L-CAT adapts to the initialization used: in uniform, it stays
328"
H,0.707105719237435,"competitive by allowing more attention in the second layer; in normal, it allows more attention in the
329"
H,0.708838821490468,"score inputs. Table 3 and Fig. 3 thus consolidate the effectiveness and flexibility of L-CAT.
330"
CONCLUSIONS AND FUTURE WORK,0.7105719237435009,"7
Conclusions and future work
331"
CONCLUSIONS AND FUTURE WORK,0.7123050259965338,"In this work, we studied how to combine the strengths of convolution and attention layers in GNNs.
332"
CONCLUSIONS AND FUTURE WORK,0.7140381282495667,"We proposed CAT, which computes attention with respect to the convolved features, and analyzed its
333"
CONCLUSIONS AND FUTURE WORK,0.7157712305025996,"benefits and limitations on a new synthetic dataset. This analysis revealed different regimes where
334"
CONCLUSIONS AND FUTURE WORK,0.7175043327556326,"one model is preferred over the others, reinforcing the idea that selecting between GCNs, GATs, and
335"
CONCLUSIONS AND FUTURE WORK,0.7192374350086655,"now CATs, is a difficult task, as their performance directly depend on the data. For this reason, we
336"
CONCLUSIONS AND FUTURE WORK,0.7209705372616985,"proposed L-CAT, a model which is able to interpolate between the three via two learnable parameters.
337"
CONCLUSIONS AND FUTURE WORK,0.7227036395147314,"Extensive experimental results demonstrated the effectiveness of L-CAT, yielding great results while
338"
CONCLUSIONS AND FUTURE WORK,0.7244367417677643,"being more robust than other methods due to its adaptability. As a result, L-CAT proved to be a viable
339"
CONCLUSIONS AND FUTURE WORK,0.7261698440207972,"drop-in replacement that removes the need to cross-validate the layer type.
340"
CONCLUSIONS AND FUTURE WORK,0.7279029462738301,"We do not consider this work adds any societal concerns. On the contrary, L-CAT eases the applica-
341"
CONCLUSIONS AND FUTURE WORK,0.729636048526863,"bility of GNNs to the practitioner, and removes the need of cross-validating the layer type, which can
342"
CONCLUSIONS AND FUTURE WORK,0.7313691507798961,"potentially benefit other areas and applications, as GNNs have already proven.
343"
CONCLUSIONS AND FUTURE WORK,0.733102253032929,"We strongly believe learnable interpolation can get us a long way, and we hope L-CAT to motivate
344"
CONCLUSIONS AND FUTURE WORK,0.7348353552859619,"new and exciting work. For example, it would be interesting to see L-CAT applied to other GCN and
345"
CONCLUSIONS AND FUTURE WORK,0.7365684575389948,"GAT variants, such as those in [17, 28, 35]. Specially, we are eager to see L-CAT in real applications,
346"
CONCLUSIONS AND FUTURE WORK,0.7383015597920277,"and thus finding out what combining different GNN layers across a model (without the annoyance of
347"
CONCLUSIONS AND FUTURE WORK,0.7400346620450606,"cross-validating all combinations) can lead to in the real-world.
348"
REFERENCES,0.7417677642980935,"References
349"
REFERENCES,0.7435008665511266,"[1] T.W. Anderson. An introduction to multivariate statistical analysis. John Wiley & Sons, 2003.
350"
REFERENCES,0.7452339688041595,"[2] Aseem Baranwal, Kimon Fountoulakis, and Aukosh Jagannath. Graph convolution for semi-
351"
REFERENCES,0.7469670710571924,"supervised classification: Improved linear separability and out-of-distribution generalization. In
352"
REFERENCES,0.7487001733102253,"International Conference on Machine Learning (ICML). PMLR, 2021.
353"
REFERENCES,0.7504332755632582,"[3] Aseem Baranwal, Kimon Fountoulakis, and Aukosh Jagannath. Effects of graph convolutions
354"
REFERENCES,0.7521663778162911,"in deep networks. arXiv preprint arXiv:2204.09297, 2022.
355"
REFERENCES,0.7538994800693241,"[4] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius
356"
REFERENCES,0.755632582322357,"Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan
357"
REFERENCES,0.75736568457539,"Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint
358"
REFERENCES,0.7590987868284229,"arXiv:1806.01261, 2018.
359"
REFERENCES,0.7608318890814558,"[5] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In
360"
REFERENCES,0.7625649913344887,"International Conference on Learning Representations (ICLR), 2022.
361"
REFERENCES,0.7642980935875217,"[6] Dan Busbridge, Dane Sherburn, Pietro Cavallo, and Nils Y Hammerla. Relational graph
362"
REFERENCES,0.7660311958405546,"attention networks. arXiv preprint arXiv:1904.05811, 2019.
363"
REFERENCES,0.7677642980935875,"[7] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Liò, and Petar Veliˇckovi´c. Principal
364"
REFERENCES,0.7694974003466204,"neighbourhood aggregation for graph nets. Advances in Neural Information Processing Systems
365"
REFERENCES,0.7712305025996534,"(NeurIPS), 33, 2020.
366"
REFERENCES,0.7729636048526863,"[8] Yash Deshpande, Subhabrata Sen, Andrea Montanari, and Elchanan Mossel. Contextual
367"
REFERENCES,0.7746967071057193,"stochastic block models. Advances in Neural Information Processing Systems (NeurIPS), 31,
368"
REFERENCES,0.7764298093587522,"2018.
369"
REFERENCES,0.7781629116117851,"[9] Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs.
370"
REFERENCES,0.779896013864818,"arXiv preprint arXiv:2012.09699, 2020.
371"
REFERENCES,0.7816291161178509,"[10] Kimon Fountoulakis, Amit Levi, Shenghao Yang, Aseem Baranwal, and Aukosh Jagannath.
372"
REFERENCES,0.7833622183708839,"Graph attention retrospective. arXiv preprint arXiv:2202.13060, 2022.
373"
REFERENCES,0.7850953206239168,"[11] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl.
374"
REFERENCES,0.7868284228769498,"Neural message passing for quantum chemistry. In International Conference on Machine
375"
REFERENCES,0.7885615251299827,"Learning (ICML), 2017.
376"
REFERENCES,0.7902946273830156,"[12] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward
377"
REFERENCES,0.7920277296360485,"neural networks. In International Conference on Artificial Intelligence and Statistics (AISTATS),
378"
REFERENCES,0.7937608318890814,"2010.
379"
REFERENCES,0.7954939341421143,"[13] Will Hamilton,
Zhitao Ying,
and Jure Leskovec.
Inductive representation learn-
380"
REFERENCES,0.7972270363951474,"ing on large graphs.
In Advances in Neural Information Processing Systems
381"
REFERENCES,0.7989601386481803,"(NeurIPS), volume 30, 2017.
URL https://proceedings.neurips.cc/paper/2017/
382"
REFERENCES,0.8006932409012132,"file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf.
383"
REFERENCES,0.8024263431542461,"[14] William L. Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods
384"
REFERENCES,0.804159445407279,"and applications. IEEE Data Eng. Bull., 40, 2017.
385"
REFERENCES,0.8058925476603119,"[15] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele
386"
REFERENCES,0.8076256499133448,"Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs.
387"
REFERENCES,0.8093587521663779,"Advances in Neural Information Processing Systems (NeurIPS), 33:22118–22133, 2020.
388"
REFERENCES,0.8110918544194108,"[16] Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. Heterogeneous graph transformer. In
389"
REFERENCES,0.8128249566724437,"The Web Conference (WWW), 2020.
390"
REFERENCES,0.8145580589254766,"[17] Dongkwan Kim and Alice Oh. How to find your friendly neighborhood: Graph attention design
391"
REFERENCES,0.8162911611785095,"with self-supervision. In International Conference on Learning Representations (ICLR), 2021.
392"
REFERENCES,0.8180242634315424,"URL https://openreview.net/forum?id=Wi5KUNlqWty.
393"
REFERENCES,0.8197573656845754,"[18] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional
394"
REFERENCES,0.8214904679376083,"networks. In International Conference on Learning Representations (ICLR), 2017.
395"
REFERENCES,0.8232235701906413,"[19] Boris Knyazev, Graham W Taylor, and Mohamed Amer. Understanding attention and gen-
396"
REFERENCES,0.8249566724436742,"eralization in graph neural networks. Advances in Neural Information Processing Systems
397"
REFERENCES,0.8266897746967071,"(NeurIPS), 32, 2019.
398"
REFERENCES,0.82842287694974,"[20] Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent Létourneau, and Prudencio Tossou.
399"
REFERENCES,0.830155979202773,"Rethinking graph transformers with spectral attention. Advances in Neural Information Pro-
400"
REFERENCES,0.8318890814558059,"cessing Systems (NeurIPS), 34, 2021.
401"
REFERENCES,0.8336221837088388,"[21] John Boaz Lee, Ryan A Rossi, Sungchul Kim, Nesreen K Ahmed, and Eunyee Koh. Attention
402"
REFERENCES,0.8353552859618717,"models in graphs: A survey. ACM Transactions on Knowledge Discovery from Data (TKDD),
403"
REFERENCES,0.8370883882149047,"13, 2019.
404"
REFERENCES,0.8388214904679376,"[22] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen,
405"
REFERENCES,0.8405545927209706,"Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural
406"
REFERENCES,0.8422876949740035,"networks. In AAAI conference on artificial intelligence, volume 33, 2019.
407"
REFERENCES,0.8440207972270364,"[23] P. Rigollet and J.-C. Hütter. High dimensional statistics. Lecture notes for course 18S997, 813:
408"
REFERENCES,0.8457538994800693,"814, 2015.
409"
REFERENCES,0.8474870017331022,"[24] Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding.
410"
REFERENCES,0.8492201039861352,"Journal of Complex Networks, 2021.
411"
REFERENCES,0.8509532062391681,"[25] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
412"
REFERENCES,0.8526863084922011,"The graph neural network model. IEEE transactions on neural networks, 20, 2008.
413"
REFERENCES,0.854419410745234,"[26] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-
414"
REFERENCES,0.8561525129982669,"Rad. Collective classification in network data. AI magazine, 29(3), 2008.
415"
REFERENCES,0.8578856152512998,"[27] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann.
416"
REFERENCES,0.8596187175043327,"Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.
417"
REFERENCES,0.8613518197573656,"[28] Kiran K Thekumparampil, Chong Wang, Sewoong Oh, and Li-Jia Li. Attention-based graph
418"
REFERENCES,0.8630849220103987,"neural network for semi-supervised learning. arXiv preprint arXiv:1803.03735, 2018.
419"
REFERENCES,0.8648180242634316,"[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
420"
REFERENCES,0.8665511265164645,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information
421"
REFERENCES,0.8682842287694974,"Processing Systems (NeurIPS), 30, 2017.
422"
REFERENCES,0.8700173310225303,"[30] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
423"
REFERENCES,0.8717504332755632,"Bengio. Graph attention networks. In International Conference on Learning Representations
424"
REFERENCES,0.8734835355285961,"(ICLR), 2018.
425"
REFERENCES,0.8752166377816292,"[31] Guangtao Wang, Rex Ying, Jing Huang, and Jure Leskovec. Multi-hop attention graph neural
426"
REFERENCES,0.8769497400346621,"networks. In International Joint Conference on Artificial Intelligence (IJCAI), 2021.
427"
REFERENCES,0.878682842287695,"[32] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu. Heteroge-
428"
REFERENCES,0.8804159445407279,"neous graph attention network. In The World Wide Web Conference (WWW), 2019.
429"
REFERENCES,0.8821490467937608,"[33] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. Graph neural networks in recom-
430"
REFERENCES,0.8838821490467937,"mender systems: a survey. ACM Computing Surveys (CSUR), 2020.
431"
REFERENCES,0.8856152512998267,"[34] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A
432"
REFERENCES,0.8873483535528596,"comprehensive survey on graph neural networks. IEEE transactions on neural networks and
433"
REFERENCES,0.8890814558058926,"learning systems, 32, 2020.
434"
REFERENCES,0.8908145580589255,"[35] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
435"
REFERENCES,0.8925476603119584,"networks? In International Conference on Learning Representations (ICLR), 2019.
436"
REFERENCES,0.8942807625649913,"[36] Jiaxuan You, Zhitao Ying, and Jure Leskovec. Design space for graph neural networks. Advances
437"
REFERENCES,0.8960138648180243,"in Neural Information Processing Systems (NeurIPS), 33, 2020.
438"
REFERENCES,0.8977469670710572,"[37] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph
439"
REFERENCES,0.8994800693240901,"transformer networks. Advances in Neural Information Processing Systems (NeurIPS), 32,
440"
REFERENCES,0.901213171577123,"2019.
441"
REFERENCES,0.902946273830156,"Checklist
442"
REFERENCES,0.9046793760831889,"1. For all authors...
443"
REFERENCES,0.9064124783362218,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
444"
REFERENCES,0.9081455805892548,"contributions and scope? [Yes] In both, the abstract and introduction, we faithfully
445"
REFERENCES,0.9098786828422877,"reflect the contributions of this work: i) the introduction of a new GNN layer, CAT; ii) a
446"
REFERENCES,0.9116117850953206,"theoretical analysis on a new synthetic data model; and iii) the introduction of L-CAT,
447"
REFERENCES,0.9133448873483535,"a model that is capable of learning to interpolate between GCN, GAT and CAT.
448"
REFERENCES,0.9150779896013865,"(b) Did you describe the limitations of your work? [Yes] Throuhgout all the manuscript
449"
REFERENCES,0.9168110918544194,"we discuss on the strenghts and limitations of the considered models. As of L-CAT, we
450"
REFERENCES,0.9185441941074524,"discuss possible extensions (and thus current limitations) in the future work.
451"
REFERENCES,0.9202772963604853,"(c) Did you discuss any potential negative societal impacts of your work? [Yes] Societal
452"
REFERENCES,0.9220103986135182,"impact of our work is mentioned in the conclusions.
453"
REFERENCES,0.9237435008665511,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
454"
REFERENCES,0.925476603119584,"them? [Yes]
455"
REFERENCES,0.9272097053726169,"2. If you are including theoretical results...
456"
REFERENCES,0.92894280762565,"(a) Did you state the full set of assumptions of all theoretical results? [Yes] We describe
457"
REFERENCES,0.9306759098786829,"the data model used for all our theoretical results in §4 and Appendix A
458"
REFERENCES,0.9324090121317158,"(b) Did you include complete proofs of all theoretical results? [Yes] All proofs can be
459"
REFERENCES,0.9341421143847487,"found in Appendix A
460"
REFERENCES,0.9358752166377816,"3. If you ran experiments...
461"
REFERENCES,0.9376083188908145,"(a) Did you include the code, data, and instructions needed to reproduce the main exper-
462"
REFERENCES,0.9393414211438474,"imental results (either in the supplemental material or as a URL)? [Yes] We include
463"
REFERENCES,0.9410745233968805,"in the supplementary material the necessary anonymized code and scripts required to
464"
REFERENCES,0.9428076256499134,"reproduce our experiments. All required datasets are freely available.
465"
REFERENCES,0.9445407279029463,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
466"
REFERENCES,0.9462738301559792,"were chosen)? [Yes] Complete details about the experimental setup can be found in
467"
REFERENCES,0.9480069324090121,"Appendices B, D and E
468"
REFERENCES,0.949740034662045,"(c) Did you report error bars (e.g., with respect to the random seed after running exper-
469"
REFERENCES,0.951473136915078,"iments multiple times)? [Yes] In all our results we report the mean and standard
470"
REFERENCES,0.9532062391681109,"deviation computed using five trials or more. In addition, we highlight in bold the
471"
REFERENCES,0.9549393414211439,"results that are statistically significant.
472"
REFERENCES,0.9566724436741768,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
473"
REFERENCES,0.9584055459272097,"of GPUs, internal cluster, or cloud provider)? [Yes] We include complete details of
474"
REFERENCES,0.9601386481802426,"the experimental setup as well as computational resources used for the three sets of
475"
REFERENCES,0.9618717504332756,"experiments in Appendix B, Appendix D and Appendix E respectively.
476"
REFERENCES,0.9636048526863085,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
477"
REFERENCES,0.9653379549393414,"(a) If your work uses existing assets, did you cite the creators? [Yes] We cite the baseline
478"
REFERENCES,0.9670710571923743,"models [5, 18, 30], the creators of the datasets we use [15, 24, 27], and the programming
479"
REFERENCES,0.9688041594454073,"framework we use to run our experiments [15, 36]
480"
REFERENCES,0.9705372616984402,"(b) Did you mention the license of the assets?[N/A]
481"
REFERENCES,0.9722703639514731,"(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
482"
REFERENCES,0.9740034662045061,"We provide the code needed for running the experiments in the supplementary material.
483"
REFERENCES,0.975736568457539,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
484"
REFERENCES,0.9774696707105719,"using/curating? [N/A] Datasets are publicly available at the Torch Geometric 3 or the
485"
REFERENCES,0.9792027729636048,"Open Graph Benchmark 4 frameworks.
486"
REFERENCES,0.9809358752166378,"(e) Did you discuss whether the data you are using/curating contains personally identifiable
487"
REFERENCES,0.9826689774696707,"information or offensive content? [N/A] The datasets used do not contain personally
488"
REFERENCES,0.9844020797227037,"identifiable information nor offensive content.
489"
REFERENCES,0.9861351819757366,"5. If you used crowdsourcing or conducted research with human subjects...
490"
REFERENCES,0.9878682842287695,"(a) Did you include the full text of instructions given to participants and screenshots, if
491"
REFERENCES,0.9896013864818024,"applicable? [N/A]
492"
REFERENCES,0.9913344887348353,"3https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html
4https://ogb.stanford.edu/"
REFERENCES,0.9930675909878682,"(b) Did you describe any potential participant risks, with links to Institutional Review
493"
REFERENCES,0.9948006932409013,"Board (IRB) approvals, if applicable? [N/A]
494"
REFERENCES,0.9965337954939342,"(c) Did you include the estimated hourly wage paid to participants and the total amount
495"
REFERENCES,0.9982668977469671,"spent on participant compensation? [N/A]
496"
