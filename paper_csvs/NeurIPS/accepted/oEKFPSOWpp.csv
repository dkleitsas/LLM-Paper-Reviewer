Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.003401360544217687,"Global routing plays a critical role in modern chip design. The routing paths gener-
ated by global routers often form a rectilinear Steiner tree (RST). Recent advances
from the machine learning community have shown the power of learning-based
route generation; however, the yielded routing paths by the existing approaches
often suffer from considerable overflow, thus greatly hindering their application in
practice. We propose NeuralSteiner, an accurate approach to overflow-avoiding
global routing in chip design. The key idea of NeuralSteiner approach is to learn
Steiner trees: we first predict the locations of highly likely Steiner points by adopt-
ing a neural network considering full-net spatial and overflow information, then
select appropriate points by running a graph-based post-processing algorithm, and
finally connect these points with the input pins to yield overflow-avoiding RSTs.
NeuralSteiner offers two advantages over previous learning-based models. First,
by using the learning scheme, NeuralSteiner ensures the connectivity of gener-
ated routes while significantly reducing congestion. Second, NeuralSteiner can
effectively scale to large nets and transfer to unseen chip designs without any
modifications or fine-tuning. Extensive experiments over public large-scale bench-
marks reveal that, compared with the state-of-the-art deep generative methods,
NeuralSteiner achieves up to a 99.8% reduction in overflow while speeding up the
generation and maintaining a slight wirelength loss within only 1.8%."
INTRODUCTION,0.006802721088435374,"1
Introduction"
INTRODUCTION,0.01020408163265306,"In the modern design flow of Very Large Scale Integration (VLSI), global routing has become one
of the most complex and time-consuming steps. Given the complexity of VLSI netlist [17] that
contains millions or even billions of nets requiring routing, global routers must interconnect pins of
nets, minimize the total wirelength of the routes while avoiding overflow (or congestion) in a strictly
limited area of chip[2, 24]. Overflow occurs when the number of routes in a particular area of the"
INTRODUCTION,0.013605442176870748,*Corresponding author
INTRODUCTION,0.017006802721088437,"(a)
(b)
(c)
(d)"
INTRODUCTION,0.02040816326530612,"(e)
(f)
(g) > 12 8 4 0 -4 -8 < -12 (h)"
INTRODUCTION,0.023809523809523808,"Figure 1: An illustrative example to show overflow-avoiding global routing in chip design. (a)
Chip layout of a real-world net extracted from ISPD07_adaptec1. (b) Two-dimensional grid graph.
(c) The Hanan grid of a 4-pin net. (d) The resource map and pin map (actually divided into two
channels) obtained from (a) for this 4-pin net. (e) The predicted hub points (black circles) and stripe
mask applied by Hubrouter [8]. (f) The routing result generated by HubRouter [8] suffers from
congestion (red edges). (g) The candidate points (green circles) predicted by NeuralSteiner and the
corresponding net augmented graph (NAG). (h) The final routing result generated by NeuralSteiner
that avoids overflow."
INTRODUCTION,0.027210884353741496,"chip exceeds the available routing resources or routes cross through impassable obstacles, which
significantly impacts the subsequent design flow and functionality realization of the chip [24, 22].
Even the two-pin routing under design constraints or obstacles turns out to be NP-complete. When
the number of pins exceeds two, the routing problem can often be transformed into the construction
problem of rectilinear Steiner minimum tree (RSMT) [10], which is also NP-complete and becomes
even more challenging when considering avoiding overflow [22, 16, 24, 6]."
INTRODUCTION,0.030612244897959183,"Traditional global routers propose various human-designed heuristics to obtain near-optimal solutions
for RSMT [7, 27, 13, 16, 24] or directly solve the integer programming problem for concurrent
routing of multiple nets [6, 33]. Recent advances in applying learning-based methods to chip design
problems have shown feasibility and powerful abilities and even surpassing the performance of
human expert-designed algorithms, such as using deep reinforcement learning (DRL) for placement
[29, 18, 19] and using convolutional neural network (CNN) for predicting design rule violations [34].
In global routing, DRL is firstly adopted to explore surrounding directions of current positions and
achieve successful connectivity on small-scale nets [20], while REST [23] decomposes multi-pin net
into 2-pin pairs and explores the sequence of pin pairs through DRL to form the RST. Moreover, recent
works adopt deep generative models [32, 4] to perform one-shot generation of nets. To address the
connectivity problems of generative methods, HubRouter[8] decomposes global routing generation
into hub-generation phase and pin-hub-connection phase to sequentially connect the pin-hub pairs,
thereby successfully ensuring the connectivity of the routes."
INTRODUCTION,0.034013605442176874,"However, current learning-based routing methods suffer from high overflow within their routing
results, primarily due to inadequate consideration of resource availability during the routing process.
Existing DRL-based approaches [20, 23, 28] tend to solely focus on wirelength as the reward during
action space exploration, while other generative methods do incorporate the current routing area’s
resource status in their inputs but mainly aim for connectivity or wirelength optimization in their
post-processing phases [4, 32, 5, 8]. Consequently, our experimental findings indicate that routes
generated by HubRouter still exhibit significant congestion, which is illustrated in Fig. 1f."
INTRODUCTION,0.03741496598639456,"To address these challenges, we propose a congestion-aware learning scheme named NeuralSteiner,
which consists of two main phases: i) Candidate point prediction phase: Utilizing a neural network
combined with full-image spatial and overflow information aggregation to predict the accurate
locations of what we call candidate points for overflow-avoiding rectilinear Steiner tree; ii) Overflow-"
INTRODUCTION,0.04081632653061224,"avoiding RST construction phase: Constructing an augmented graph of the net based on the
predicted candidate points and calculating the overflow-avoiding RST using a simple but effective
greedy algorithm. Through this two-phase setup, NeuralSteiner successfully ensures connectivity and
enables the generation of overflow-avoiding routing results for large-scale nets."
INTRODUCTION,0.04421768707482993,This paper has three main contributions:
INTRODUCTION,0.047619047619047616,"• We propose NeuralSteiner, a two-phase global routing scheme, which to our knowledge, is
the first learning-based approach capable of optimizing both wirelength and overflow and
effectively addressing the routing problem of large-scale nets.
• We devise a neural network architecture that integrates the deep residual network with
recurrent crisscross attention mechanism to learn the Steiner point locations from a carefully
curated expert dataset and propose a post-processing algorithm based on augmented graphs
to construct routes with substantially less overflow than recent works.
• We conduct extensive experiments on 14 public large-scale routing benchmarks compared
with the state-of-the-art learning-based method, where the NeuralSteiner achieves up to
99.6% reduction in total overflow with a wirelength loss within 1.8%. Moreover, Neural-
Steiner can generate overflow-avoiding routes for nets with more than 1000 pins, previously
challenging for recent works, which narrows the gap between learning-based methods and
practical routing applications."
PRELIMINARIES AND RELATED WORKS,0.05102040816326531,"2
Preliminaries and Related Works"
PRELIMINARIES AND RELATED WORKS,0.05442176870748299,"Global Routing. Given the complexity of VLSI routing problems, the circuit layout like 1a is
partitioned into rectangular areas known as global cells (GCells) [6]. The global routing problem
can be modeled as a grid graph G(V, E), where each GCell is represented as a vertex (v ∈V ), and
adjacent GCells are connected by an edge (e ∈E) that represents the boundary between GCells.
Chip designs often contain two or more metal layers for routing. Each metal layer is dedicated to
either horizontal or vertical direction and the projection of these layers onto a two-dimensional grid
graph is shown in Fig.1b. Global router will assign a set of GCells interconnected by numerous
edges to each net as its routing result to connect all pins, which often forms a Rectilinear Steiner Tree
(RST) [7]. The concepts of Hanan grid [11] and escape graph [9] are often used for the generation
of the shortest RSMT avoiding obstacles [22], considering the intersection points in these graphs
as candidate locations for Steiner points. However, due to the complex and irregular distribution of
congestion, the construction of escape graph becomes complicated, while the Hanan grid is ineffective
at circumventing congestion, which is shown in Fig.1c."
PRELIMINARIES AND RELATED WORKS,0.05782312925170068,"Overflow. Give edge e(u, v) ∈E is the boundary between GCell u and GCell v, the capacity c(u, v)
is the routing resource of edge e that can be provided to global router and demand d(u, v) is the
number of routes passing through edge e. The resource r(u, v) of edge e is the part of the capacity
that can still be utilized to route, which is defined in Equation (1):"
PRELIMINARIES AND RELATED WORKS,0.061224489795918366,"r(u, v) = c(u, v) −d(u, v)
(1)"
PRELIMINARIES AND RELATED WORKS,0.06462585034013606,"Overflow occurs when r(u, v) < 0. The routing results containing overflow generated by the global
router will not be accepted by subsequent routing process and will trigger a time-consuming rip-up
and reroute iteration in order to eliminate overflow [2]. Therefore, the global router should not only
attempt to find the shortest connection for each net but also minimize the number of overflow."
PRELIMINARIES AND RELATED WORKS,0.06802721088435375,"Traditional Global Router. Traditional routing algorithms typically divide global routing into two
main stages to address congestion: Steiner topology generation and rip-up and reroute (RRR). The
former utilizes the FLUTE algorithm [7], based on lookup tables, to generate Steiner trees with nearly
minimal wirelength for each net. However, FLUTE is unaware of congestion. During this phase, most
routers only use edge shifting to partially mitigate congestion by moving some edges out of congested
areas [7], while CUGR-2 [25] applies the construction of augmented graphs to build candidate paths
for nets’ RSTs, adjusting the position of certain Steiner points to circumvent potential congestion. In
order to resolve congestion in the RSTs, traditional routers will invoke RRR, iteratively removing all
initially routed nets in congested areas and employing maze routing that optimizes wirelength and
congestion simultaneously. This process becomes dramatically time-consuming as the chip design’s
scale and complexity rise. Hence, accelerating congestion resolution through deep learning-based
methods can enhance the overall performance of global routing algorithm."
PRELIMINARIES AND RELATED WORKS,0.07142857142857142,"Construct 
Augmented Graph"
PRELIMINARIES AND RELATED WORKS,0.07482993197278912,Overflow Map
PRELIMINARIES AND RELATED WORKS,0.0782312925170068,Crisscross aggregation Q K V
PRELIMINARIES AND RELATED WORKS,0.08163265306122448,Softmax
PRELIMINARIES AND RELATED WORKS,0.08503401360544217,Crisscross Attention Module × 2
PRELIMINARIES AND RELATED WORKS,0.08843537414965986,Pin Map …
PRELIMINARIES AND RELATED WORKS,0.09183673469387756,RST Construction
PRELIMINARIES AND RELATED WORKS,0.09523809523809523,"Net Augmented Graph
Final Routing Result … …"
PRELIMINARIES AND RELATED WORKS,0.09863945578231292,Parallel Routing Tasks
PRELIMINARIES AND RELATED WORKS,0.10204081632653061,"Residual Blocks
Residual Blocks"
PRELIMINARIES AND RELATED WORKS,0.1054421768707483,Netlist & Layout
PRELIMINARIES AND RELATED WORKS,0.10884353741496598,"Candidate 
Point Maps … … +"
PRELIMINARIES AND RELATED WORKS,0.11224489795918367,Routing Task
PRELIMINARIES AND RELATED WORKS,0.11564625850340136,"net 0
net 1
net N net 0 net 1 net N"
PRELIMINARIES AND RELATED WORKS,0.11904761904761904,"(a)
(b) (c) …"
PRELIMINARIES AND RELATED WORKS,0.12244897959183673,Channel Reduction
PRELIMINARIES AND RELATED WORKS,0.12585034013605442,"Candidate 
Point Maps"
PRELIMINARIES AND RELATED WORKS,0.1292517006802721,"Figure 2: Overview of NeuralSteiner. (a) The parallel routing tasks accelerate routing by grouping
the non-overlapping nets into one batch. (b) During the first phase, NeuralSteiner predicts the
candidate point locations for the RST with full-image aggregation of spatial and overflow information.
(c) During the second phase, NeuralSteiner constructs the net augmented graphs based on the
predicted candidate points and generates overflow-avoiding RSTs."
PRELIMINARIES AND RELATED WORKS,0.1326530612244898,"Learning-based RST construction. Various works explore the feasibility and advantages in wire-
length and efficiency of applying deep neural networks to global routing, including generation of the
pin-connection order [23], segments [4, 32] or custom hub points of RST [8]. However, most of the
challenges in actual global routing come from the complexity of large-scale nets and how to avoiding
overflow when routing resources are limited. Under the circumstances, detours are indispensable
to get rid of congestions, while the shortest RST like Fig. 1f generated by HubRouter [8] is not
practically usable. The chip layout can be viewed as an image, where each pixel represents a tile in
global routing, and images of different channels represent the locations of the pins and capacity of
the grid edges. The output points can also be represented as a binary image. But unlike the four kinds
of hub points defined in Hubrouter, we simplify the learning target in RST construction and select
Steiner points and corner points in RST as candidate points to learn. Formally, we have
Definition 1. Candidate point Given an m × n binary image representing the RST, where each
pixel pxy(1 ≤x ≤m, 1 ≤y ≤n) represents whether the position is occupied by a route and
p0y = p(m+1)y = px0 = px(n+1) = 0. The pixel pxy is a candidate point if and only if it satisfies:"
PRELIMINARIES AND RELATED WORKS,0.1360544217687075,"
pxy = 1,
(dxy > 2)
pxy = 1, and p(x−1)y + p(x+1)y = 1, and px(y−1) + px(y+1) = 1,
(dxy = 2)
(2)"
PRELIMINARIES AND RELATED WORKS,0.13945578231292516,where dxy = p(x−1)y + p(x+1)y + px(y−1) + px(y+1) denotes the degree of this point in the RST.
PRELIMINARIES AND RELATED WORKS,0.14285714285714285,"Using Definition. 1, the Steiner points and corner points can be recognized as candidate points in the
pixel image of RST. The differences between hub point in [8] and candidate point are visualized in
Fig. S1."
METHOD,0.14625850340136054,"3
Method"
OVERALL PIPELINE,0.14965986394557823,"3.1
Overall Pipeline"
OVERALL PIPELINE,0.15306122448979592,"NeuralSteiner decomposes the global routing process into two main phases to optimizes wirelength
and overflow of the routing result simultaneously. Before introducing the main methods, we first
propose our parallel task construction in Sec. 3.2. We then introduce the candidate point prediction
method with aggregation of full-scale spatial and overflow information in Sec. 3.3. An augmented
graph-based overflow-avoiding RST construction method will be proposed in Sec. 3.4. The overall
pipeline of NeuralSteiner is illustrated in Fig.2."
OVERALL PIPELINE,0.1564625850340136,"3.2
Parallel Routing Tasks Construction."
OVERALL PIPELINE,0.1598639455782313,"Routing of two nets cannot be parallelized if the bounding boxes of their pins have overlap, which is
defined as conflict between two nets. Inspired by [26], we scan and group the non-conflicting nets
into a set t, which is called a routing task, which divides the numerous nets in the design into a set
of mutually conflicting routing tasks. Nets within a task t can be batched together and fed into the
neural network for prediction and post-processing, significantly enhancing the parallelism of routing.
Please refer to App. B.3 for the detail of our routing tasks construction algorithm."
CANDIDATE POINT PREDICTION,0.16326530612244897,"3.3
Candidate Point Prediction"
CANDIDATE POINT PREDICTION,0.16666666666666666,"Candidate points prediction can be formulated as an image segmentation task [31], which involves
training a neural network model to perform pixel-level classification to recognize the locations of
candidate points found in the expert RSTs data. We will first introduce the expert routes dataset
optimized for both wirelength and overflow, then the network architecture incorporating the recurrent
crisscross attention module to tackle the complex large-scale nets, as well as the design of our training
protocol."
CANDIDATE POINT PREDICTION,0.17006802721088435,"Expert Routing Dataset Construction. We utilize a state-of-the-art traditional global router named
CUGR [24] to perform routing on public benchmarks [30] also used by [8] and extract the overflow
map and pin map of every net in real-time. We adopt the logistic function in CUGR to calculate the
overflow value using resource r(u, v):
lg(u, v) = (1.0 + exp(slope × r(u, v)))−1.
(3)
where slope (which is set to 1 here) is an adjustable parameter that determines the global router’s
sensitivity to overflow and the overflow value will increase rapidly as the resources are being used
up. After that, we directly employ CUGR’s maze routing algorithm to execute the rip-up and reroute
process to obtain the congestion-avoiding routing results. We mark the Steiner points and corner
points in the RSTs constructed by CUGR as candidate points and generate the label candidate
point map for every net. Rather than clipping all images to the same scale 64 × 64, which is set in
HubRouter [8], we maintain three maps of every net at the original scale of its bounding box. This
preserves the precise spatial and overflow information and does not exclude any large-scale nets."
CANDIDATE POINT PREDICTION,0.17346938775510204,"Network Architecture. In order to tackle the problem of large variation in net scale, we employ a
ResNet structure as the backbone of our model and combined it with the recurrent crisscross attention
mechanism [14] to encoding full-net overflow and long-range associations in the input features.
Convolutional neural networks (CNN) have been proven to be efficiently applied in chip design
like predicting chip congestion distribution [35], DRV distribution [34], and thermal distribution [3].
However, due to the fixed geometric structures, CNN is inherently limited to local receptive fields
that face difficulties in capturing long-range correlations. Thus, we introduce the recurrent crisscross
attention mechanism (RCCA) to aggregate features from all pixels on the feature map. We insert
one RCCA module with two e crisscross attention blocks in ResNet. Fig.2 illustrates the network
architecture of NeuralSteiner. We also remove the down-sampling operations to retain more spatial
details of feature maps because the construction of RST requires accurate spatial location information
when connecting pins that are far apart in large-scale nets. Through the computation of RCCA, the
network can aggregate information of pins and congestion over the whole scale of feature maps,
thereby enhancing the quality of candidate points prediction for large RSTs. This will be further
demonstrated in ablation study in Sec. 4.4. The implementation details of our network and RCCA
calculation are shown in App. B.1."
CANDIDATE POINT PREDICTION,0.17687074829931973,"Model Training. We adopt focal loss [21] ℓfocal to mitigate the imbalance between positive and
negative class samples in training data where the candidate points in RST only occupy a minority of
pixels in the entire routing area. Let pt be the predicted probability for the ground truth class t, ℓfocal
is defined as:
ℓfocal = −αt(1 −pt)γ log(pt)
(4)
where αt is the weighting factor while γ is the focusing parameter that reduces the loss for well-
classified examples. We also adopt the dice loss ℓdice to measure the similarities between the predicted
candidate points and the ground truth. Using pxy to represent the probability of pixel at position
(x, y) predicted as a candidate point and gxy to represent the label, ℓdice can be expressed as:"
CANDIDATE POINT PREDICTION,0.18027210884353742,"ℓdice = 1 −
2 P"
CANDIDATE POINT PREDICTION,0.1836734693877551,"x,y pxygxy + ϵ
P"
CANDIDATE POINT PREDICTION,0.1870748299319728,"x,y pxy + P"
CANDIDATE POINT PREDICTION,0.19047619047619047,"x,y gxy + ϵ
(5)"
CANDIDATE POINT PREDICTION,0.19387755102040816,"where ϵ is a small constant added to avoid division by zero. Additionally, since the global routing
problem is NP-complete, even expert router may not generate the optimal routing solution for the
net that achieves the shortest wirelength with the minimal overflow. Therefore, we further add an
overflow loss ℓof to measure the congestion status of predicted points. Let oxy be the value at position
(x, y) of the overflow map, ℓof can be calculated by: ℓof = P"
CANDIDATE POINT PREDICTION,0.19727891156462585,"x,y pxyoxy + ϵ
P"
CANDIDATE POINT PREDICTION,0.20068027210884354,"x,y pxy + ϵ
(6)"
CANDIDATE POINT PREDICTION,0.20408163265306123,"The inclusion of overflow loss helps the model identify potential candidate points that are not in
the label set but have a lower intrinsic congestion, benefiting the post-processing algorithm for
overflow-avoiding RST construction. Then the trainable model θ is determined at the training stage
by minimizing the loss function as follows:"
CANDIDATE POINT PREDICTION,0.20748299319727892,"L(θ) = cfl · ℓfocal + cdi · ℓdice + cof · ℓof
(7)"
CANDIDATE POINT PREDICTION,0.2108843537414966,"where cfl, cdi, cof represent weight of corresponding loss item. The parameters used in training
process are provided in App. B.2."
OVERFLOW-AVOIDING RST CONSTRUCTION,0.21428571428571427,"3.4
Overflow-avoiding RST Construction"
OVERFLOW-AVOIDING RST CONSTRUCTION,0.21768707482993196,"To construct an overflow-avoiding Rectilinear Steiner Tree (RST) based on the candidate points
predicted by the neural network, we will first introduce the construction of net augmented graph
that contains potential overflow-free edges and then propose a simple and effective greedy RST
construction algorithm. Unlike previous works that focus solely on minimizing wirelength, the
inclusion of the irregular distribution of congestion makes solving for an RST more challenging."
OVERFLOW-AVOIDING RST CONSTRUCTION,0.22108843537414966,"Net Augmented Graph. We introduce the concept of the net augmented graph (NAG) based on
neural network-predicted points to avoid congestion. We first merge the predicted candidate point
map and pin map, then sequentially examine each point pxy ≥1 from the merge map according to the
following two conditions: 1) if this point shares the same horizontal (X) or vertical (Y) coordinates
with another point q, and 2) if there is no other points on the line connecting p and q. Then an edge
e(p, q) will be established if the above two conditions are met and the weight of e(p, q) is set as"
OVERFLOW-AVOIDING RST CONSTRUCTION,0.22448979591836735,"W(e) = wd(|xp −xq| + |yp −yq|) + wo
X"
OVERFLOW-AVOIDING RST CONSTRUCTION,0.22789115646258504,"x,y
oxy
(8)"
OVERFLOW-AVOIDING RST CONSTRUCTION,0.23129251700680273,"where min(xp, xq) ≤x ≤max(xp, xq), min(yp, yq) ≤y ≤max(yp, yq). W(e) balances the
wirelength and congestion of the edge by using weights wd = 1.0 and wo = 5.0. After examining
all the points, to ensure the connectivity of the net, we will check the connectivity of the current
NAG and add candidate point and edge between different connected components if this NAG is
disconnected. App. B.4 provides a detailed introduction to the construction algorithm."
OVERFLOW-AVOIDING RST CONSTRUCTION,0.23469387755102042,"Note that in HubRouter [8], stripe mask is introduced as a filter that removes noise hub points to limit
the solution space similar to the Hanan grid, which ensure that the wirelength as short as possible.
However, as dipicted in Fig.1e, the addition of stripe mask in HubRouter limits its ability to generate
RST avoiding congested areas. On the contrary, we here retain all candidate points predicted by the
model and constructed the NAG based on them, which reduces the complexity of solving RST while
preserving the solution space to avoid overflow."
OVERFLOW-AVOIDING RST CONSTRUCTION,0.23809523809523808,"Overflow-avoiding RST Construction. We convert the calculation of the overflow-avoiding RST
into a greedy construction of minimal spanning tree that connects all pins. Initially, we consider all
pins as separate connected components containing only one node. In each iteration, based on the
NAG, we greedily select and connect the path between the two nearest connected components, then
update the shortest distance (the sum of the weights of all edges on the path) of the newly formed
connected component to all other connected components. This operation repeats until all pins are
included in one connected component. Since this method may generate additional detours, we use a
simple algorithm to detect potential feasible path reuse to shorten the wirelength. Furthermore, to
accelerate the construction of RST, we parallelize the computation of the shortest distances between
pins or connected components on the NAG. For the detailed algorithm and analysis of time complexity
and scalability, please refer to App. B.5."
RESULTS AND DISCUSSION,0.24149659863945577,"4
Results and Discussion"
DATASETS AND EXPERIMENT SETTING,0.24489795918367346,"4.1
Datasets and Experiment Setting"
DATASETS AND EXPERIMENT SETTING,0.24829931972789115,"For training, we construct the training set from ISPD07[30] using the method described in Sec. 3.3.
Since our network’s input size is variable, we limit the nets’ Half-perimeter wirelength (HPWL) in the
training set to HPWL ≤128, instead of fixing both width and height to 64. For test, in Sec. 4.2 we
use the same settings from HubRouter [8] to divide samples outside the training set into four groups
of small-scale nets to compare the connectivity and wirelength of NeuralSteiner and HubRouter.
For more extensive experiments, in Sec. 4.3 we select six public chip designs (ibm01-06) from
ISPD98 [1] and eight two-layer large-scale chip designs (adaptec(01-05)_2d, newblue(01-03)_2d)
from ISPD07 (with no overlap with the training set) to perform global routing on all nets in these
designs, comparing total overflow, wirelength and generation time. The ablation and generalization
studies for NeuralSteiner are also conducted on chip designs from ISPD07. We repeat 3 times under
different seeds for HubRouter on the small nets test set and ISPD98, and then choose the seed with
best overflow for HubRouter (GAN) to conduct the ISPD07 experiment. More details about the
experimental benchmark information and hyperparameter settings can be found in App. B.2."
CONNECTIVITY AND WIRELENGTH ON SMALL NETS,0.25170068027210885,"4.2
Connectivity and Wirelength on Small Nets"
CONNECTIVITY AND WIRELENGTH ON SMALL NETS,0.25510204081632654,"We compare NeuralSteiner with three different architectures of HubRouter on the same test set from
part of ISPD07 benchmarks, which is divided into ‘Route-small-4’, ‘Route-small’, ‘Route-large-4’
and ’Route-large’. The number ‘4’ in their names represents no more than or more than 4 pins,
while ‘small’ and ‘large’ represent whether the Half-perimeter wirelength (HPWL) of the net is less
or more than 16. The size of all nets’ input map is fixed at 64 × 64. We do not include PRNet
[4] as it shows very poor connectivity on ‘large’ net in previous work [8]. As shown in Table S2,
NeuralSteiner ensures connectivity on this small-scale net test set, while achieving a wirelength rate
(WLR) comparable to HubRouter. Due to the presence of recurrent crisscross attention calculation,
our method is slightly behind in generation time."
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.2585034013605442,"4.3
Global Routing on Large-scale Benchmarks"
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.2619047619047619,"To conduct extensive experiments, we first compare the proposed NeuralSteiner with three versions
of HubRouter [8] and traditional global routers Boxrouter [6], GeoSteiner [15] and FLUTE + Edge
Shifting [7] on ibm01-06 benchmarks from ISPD98. We then conduct fully routing of 8 chip designs
from ISPD07 using GeoSteiner, FLUTE + Edge Shifting, HubRouter and our method. Note that
in our experiments, we do not use the randomly generated nets from previous works [20], as they
are relatively simple and have no overflow in the results. Moreover, it has been already studied in
HubRouter that the DQN method takes excessively long time to run on even very small cases and
PRNet [4] also lags behind HubRouter in terms of wirelength, time and overflow, so they are not
included in the comparison."
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.2653061224489796,"Table 1: Wirelength (WL) and running time on ISPD-98 (ibm01-06). NeuralSteiner is compared
with 2 traditional baselines and HubRouter with 3 generative structures (HR-VAE,HR-DPM, HR-
GAN). Optimal results of WL and time are in bold."
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.2687074829931973,"Metric
Model
ibm01
ibm02
ibm03
ibm04
ibm05
ibm06 WL"
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.272108843537415,"GeoSteiner
60142
165863
145678
162734
409709
275868
Boxrouter
62659
171110
146634
167275
410614
277913
FLUTE+ES
61492
169251
146287
167547
411936
280477
HR-VAE
64812 ± 1252
176838 ± 6419
161032 ± 3231
179018 ± 4791
440302 ± 4577
301035 ± 5836
HR-DPM
66575 ± 1394
190142 ± 2511
168550 ± 2103
183051 ± 1946
474463 ± 6674
320423 ± 2958
HR-GAN
60971 ± 290
167316 ± 578
146893 ± 315
164084 ± 299
411887 ± 4529
277977 ± 514
NeuralSteiner
61735
170405
148036
166648
415684
283727"
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.2755102040816326,Time (Sec)
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.2789115646258503,"GeoSteiner
1.00
2.21
1.68
2.19
3.69
3.38
Boxrouter
5.33
9.76
8.42
31.69
10.75
24.94
FLUTE+ES
2.90
4.71
5.87
17.16
6.83
13.64
HR-VAE
8.41 ± 0.03
8.47 ± 0.06
8.59 ± 0.04
10.85 ± 0.04
12.44 ± 0.18
15.83 ± 0.11
HR-DPM
1701.57 ± 34.19
2589.93 ± 19.63
2669.28 ± 22.77
3593.04 ± 24.10
3995.47 ± 19.57
4305.82 ± 132.85
HR-GAN
37.40 ± 0.37
41.55 ± 0.51
50.84 ± 2.84
59.94 ± 2.75
69.42 ± 4.03
81.96 ± 3.98
NeuralSteiner
27.18
34.79
46.24
50.37
75.99
70.32"
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.282312925170068,"IBM01
IBM02
IBM03
IBM04
IBM05
IBM06
ISPD98 Benchmark 0 2500 5000 7500 10000 12500 15000 17500"
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.2857142857142857,Overflow 18
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.2891156462585034,"HubRouter (DPM)
HubRouter (VAE)
HubRouter (GAN)
Geosteiner
FLUTE + Edge Shift
NeuralSteiner (Ours)"
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.2925170068027211,"Figure 3: Overflow on ISPD98 (ibm01-06). Overflow of Geosteiner, HubRouter (VAE, DPM, GAN)
and NeuralSteiner on ISPD-98 (ibm01-06) cases. Note that NeuralSteiner causes only 18 overflows
on ibm05, which is annotated in the figure."
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.29591836734693877,"Table 2: Evaluating NeuralSteiner and comparing it with state-of-the-art approaches on ISPD-
07 (adaptec(01-05)_2d, newblue(01-03)_2d). Overflow (OF), wirelength (WL) and running time
are compared among traditional router GeoSteiner, FLUTE + Edge Shift and HubRouter with
GAN structures (HR-GAN), which achieves the best overflow and wirelength among three kinds of
HubRouters on ISPD98. Optimal results of overflow, wirelength and time are in bold."
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.29931972789115646,"Metric
Method
adaptec01_2d
adaptec02_2d
adaptec03_2d
adaptec04_2d
adaptec05_2d
newblue01_2d
newblue02_2d
newblue03_2d OF"
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.30272108843537415,"GeoSteiner
35945
53848
142254
45050
102300
1734
1832
584761
FLUTE+ES
32518
50947
137104
42306
957704
1348
1713
558047
HR-GAN
35441
53652
142131
45230
102108
1516
1857
583901"
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.30612244897959184,"NeuralSteiner
82
255
728
97
431
5
35
10343 WL"
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.30952380952380953,"GeoSteiner
3389601
3209172
9330748
8865643
9784471
2320456
4595235
7371273
FLUTE+ES
3418461
3235803
9417934
8896007
9886249
2347941
4651033
7454720
HR-GAN
3407033
3229110
9355980
8888775
9832110
2339204
4623006
7391055"
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.3129251700680272,"NeuralSteiner
3438717
3247429
9459117
9003952
9915795
2365499
4668079
7480679"
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.3163265306122449,Time (Sec)
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.3197278911564626,"GeoSteiner
83.17
111.92
320.08
267.13
261.43
124.68
183.82
315.48
FLUTE+ES
118.48
187.03
396.51
376.72
360.68
169.36
223.55
438.79
HR-GAN
593.02
780.44
1324.81
1387.01
1384.96
849.34
1221.16
1526.86"
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.3231292517006803,"NeuralSteiner
347.20
461.35
1351.91
1138.66
1106.54
390.34
446.68
1225.79"
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.32653061224489793,"Routing Results on ISPD98. Table 1 shows the total wirelength and generation time for all methods
on ISPD98 benchmark. Since the total overflow of the traditional router Boxrouter is 0, we depict
the routing overflow of the other methods in Fig. 3. NeuralSteiner significantly reduces the total
overflow compared to the state-of-the-art learning-based method HubRouter (GAN), with an average
reduction of 61.1% and up to 95% on ibm05. In terms of wirelength, NeuralSteiner does not incur
much additional loss, maintaining it within 1.8%. Furthermore, due to the construction of the net
parallel routing tasks, NeuralSteiner achieves shorter generation time compared with HubRouter
(GAN). The comparison of the actual solutions between NeuralSteiner and Hubrouter is given in Fig.
S2."
GLOBAL ROUTING ON LARGE-SCALE BENCHMARKS,0.3299319727891156,"Routing Results on ISPD07. Based on the experimental results on ISPD98, we select four methods
GeoSteiner, HubRouter (HR-GAN), and NeuralSteiner for comparison on the larger-scale ISPD07
chip designs. The summary of ISPD07 benchmarks we use is detailed introduced in Table S1, as well
as the number of predicted candidate points for ISPD07. According to Table S1, the average number
of candidate points added by NeuralSteiner is not significantly more than the average number of pins,
which means that for the vast majority of nets, the number of nodes in the NAG will remain at a
small scale and keep friendly to the calculation of the overflow-avoiding RST algorithm introduced
in Sec. 3.4. The total overflow (OF), wirelength (WL) and generation time are shown in Table 2.
According to Table 2, as the sizes of chip designs and nets further increase, NeuralSteiner achieves
more dramatic reduction in total overflow, with an average reduction of 97.8% across all eight designs,
and up to a 99.8% reduction on design adaptec04_2d, while the increase in wirelength still remains
within 1.8% compared to HubRouter."
GENERALIZATION AND ABLATION STUDY,0.3333333333333333,"4.4
Generalization and Ablation Study"
GENERALIZATION AND ABLATION STUDY,0.336734693877551,"Generalization. Note that the NeuralSteiner proposed by this work is trained on small-scale nets with
HPWL less than 128 and is tested on 14 large-scale public benchmarks, which contain net with HPWL
even larger than 2000. Its ability to generalize to unseen and large-scale nets can be demonstrated.
Fig. 4 shows the remaining resource map of adaptec01_2d design after routing by HubRouter and the
proposed NeuralSteiner respectively, which demonstrates that NeuralSteiner can generalize to larger
chips by using routing resource more evenly and avoiding the vast majority of overflow. Moreover, to
extensively examine the ability of NeuralSteiner to mitigate overflow in the post-routing results, we
integrate our method into CUGR and compare it with the original CUGR on post-detailed routing
metrics on ISPD18/19 benchmarks, which are much larger than ISPD98 and ISPD07. The detailed
routing is conducted by a commonly used detailed router DRCU. Short and space are two kinds
of design rule violation caused by overflow in the detailed routing process. Table 3 shows that by
integrating NeuralSteiner into CUGR, we achieve 4.4% and 19.1% reduction on average in shorts and
spaces respectively, with minimal losses in wirelength and vias. This demonstrates that NeuralSteiner,
as a pre-routing overflow mitigation method, is beneficial for reducing overflow in the post-routing
results. 0 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 300 315 0 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 300 315"
GENERALIZATION AND ABLATION STUDY,0.3401360544217687,Hubrouter ADA01 Capacity H 100 80 60 40 20 0 20 40 60 (a) 0 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 300 315 0 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 300 315
GENERALIZATION AND ABLATION STUDY,0.3435374149659864,NeuroSteiner ADA01 Capacity H 100 80 60 40 20 0 20 40 60 (b) 0 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 300 315 0 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 300 315
GENERALIZATION AND ABLATION STUDY,0.3469387755102041,Hubrouter ADA01 Capacity V 60 40 20 0 20 40 60 (c) 0 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 300 315 0 15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240 255 270 285 300 315
GENERALIZATION AND ABLATION STUDY,0.35034013605442177,NeuroSteiner ADA01 Capacity V 60 40 20 0 20 40 60 (d)
GENERALIZATION AND ABLATION STUDY,0.35374149659863946,"Figure 4: The Overflow Distribution after routing by HubRouter and NeuralSteiner. (a), (c):
the horizontal and vertical overflow of HubRouter; (b), (d): the horizontal and vertical overflow of
NeuralSteiner. Depth of red color indicates the number of overflow."
GENERALIZATION AND ABLATION STUDY,0.35714285714285715,"Table 3: Comparison of CUGR + NeuralSteiner and original CUGR on post-detailed routing
metrics on ISPD18/19 benchmarks. The detailed routing is conducted by DRCU. Short and space
are two kinds of design rule violation caused by overflow."
GENERALIZATION AND ABLATION STUDY,0.36054421768707484,"Design
Wire Length (×107)
Via (×105)
Short
Space
CUGR
CUGR+NS
CUGR
CUGR+NS
CUGR
CUGR+NS
CUGR
CUGR+NS"
GENERALIZATION AND ABLATION STUDY,0.36394557823129253,"ispd18_t5m5
2.878
2.874
9.154
9.180
389.5
362.5
16
7
ispd18_t8m5
6.653
6.671
22.466
22.452
414.6
412.8
66
65
ispd19_t7
12.556
12.621
40.446
40.956
2117.6
2042.3
7084
6472
ispd19_t7m5
11.273
11.303
40.356
40.613
2368.5
2219.0
7715
6964"
GENERALIZATION AND ABLATION STUDY,0.3673469387755102,"Average
1
1.002
1
1.005
1
0.956
1
0.809"
GENERALIZATION AND ABLATION STUDY,0.3707482993197279,"Ablation Study. To study the role of recurrent crisscross attention (RCCA) module, as well as
our loss function, we respectively ablate the RCCA module from the network architecture and the
overflow loss ℓof from the loss function and keep other training settings the same. The modified
models are tested in comparison with the unchanged NeuralSteiner on ibm01 and adaptec05_2d.
Furthermore, to validate the effectiveness of our two-phase method, especially the effectiveness of
predicted candidate points in reducing overflow, we ablate the output of the neural network and
compare it with the unchanged NeuralSteiner. Results in Table 4 indicate that, although the wirelength
of three modified models are not significantly affected due to graph-based post-processing, there are
notable increases in overflow, especially on larger adaptec05_2d design. This implies that both RCCA
module and overflow loss can help NeuralSteiner learn congestion-avoiding candidate points and
acquire a better generalization ability to larger nets. Additionally, relying solely on the graph-based
RST construction to generate RST on the Hanan grid without predicted points leads to more than 20×
increase in overflow on adaptec05_2d. This demonstrates that the neural network in our NeuralSteiner
has learned an distribution of better candidate points for overflow-avoiding RST under tight resource"
GENERALIZATION AND ABLATION STUDY,0.3741496598639456,"constraints. We also study the role of our post-processing by replacing the REST method and the
stripe mask used in the hub-pin-connection phase of HubRouter by our NAG-based RST construction
algorithm. As shown in Table 5, although the congestion of this combination is still 11× larger than
that of NeuralSteiner, it has decreased by nearly 95% compared to the original HubRouter, which
fully demonstrates the effectiveness of our post-processing method."
GENERALIZATION AND ABLATION STUDY,0.37755102040816324,"Table 4: Ablation study of different compo-
nents in NeuralSteiner. Comparison of the com-
plete NeuralSteiner learning scheme with models
removing neural networks or overflow loss ℓof
or RCCA module."
GENERALIZATION AND ABLATION STUDY,0.38095238095238093,"Metric
Model
ibm01
adaptec05_2d OF"
GENERALIZATION AND ABLATION STUDY,0.3843537414965986,"Without NN
3015
10905
Without ℓof
2258
2795
Without RCCA
2189
2290
NeuralSteiner
2033
431 WL"
GENERALIZATION AND ABLATION STUDY,0.3877551020408163,"Without NN
62108
9931120
Without ℓof
61965
9927082
Without RCCA
62098
9930587
NeuralSteiner
61735
9915795"
GENERALIZATION AND ABLATION STUDY,0.391156462585034,"Table 5:
Ablation study of NAG-based
RST construction.
Comparison of original
HubRouter (GAN), HubRouter (GAN) with our
NAG-based RST construction without stripe
mask and NeuralSteiner. Optimal results of over-
flow and wirelength are in bold."
GENERALIZATION AND ABLATION STUDY,0.3945578231292517,"Metric
Model
adaptec05_2d"
GENERALIZATION AND ABLATION STUDY,0.3979591836734694,"OF
HR-GAN
102108
HR-GAN(w/o Mask)+NAG
5245
NeuralSteiner
431"
GENERALIZATION AND ABLATION STUDY,0.4013605442176871,"WL
HR-GAN
9832110
HR-GAN(w/o Mask)+NAG
9932476
NeuralSteiner
9915795"
CONCLUSION,0.40476190476190477,"5
Conclusion"
CONCLUSION,0.40816326530612246,"In this paper, we introduce NeuralSteiner, a two-phase learning-based global routing scheme. By
combining neural network-predicted candidate points with a post-processing method based on net
augmented graph, NeuralSteiner can generate overflow-avoiding and connectivity-assured routing
solutions for unseen large-scale nets in one shot, substantially reducing the overflow by up to 99.8%
on real-world chip benchmarks, which narrows the gap between learning-based routing method and
practical chip routing applications."
CONCLUSION,0.41156462585034015,"The main limitation of our method is that we still rely on heuristic post-processing algorithms for
Rectilinear Steiner Tree (RST) construction, which leads to time-consuming calculations and a slight
increase in wirelength. In the future, we will explore using continuous probabilistic candidate point
maps and investigate end-to-end learning with neural networks for generating overflow-avoiding
RSTs. Addressing this limitation could lead to enhanced performance in routing efficiency and
quality."
ACKNOWLEDGEMENTS,0.41496598639455784,"6
Acknowledgements"
ACKNOWLEDGEMENTS,0.41836734693877553,"We would like to thank the National Key Research and Development Program of China
(2020YFA0907000), the National Natural Science Foundation of China (32370657, 32271297,
82130055, 62072435), and the Major Key Project of PCL (No. PCL2023A03) for providing finan-
cial supports for this study and publication charges. The numerical calculations in this study were
supported by ICT Computer X center, CAS Xiandao-1 and Pengcheng Cloudbrain."
REFERENCES,0.4217687074829932,References
REFERENCES,0.42517006802721086,"[1] Alpert, C. J. (1998). The ispd98 circuit benchmark suite. In Proceedings of the 1998 international
symposium on Physical design, pages 80–85."
REFERENCES,0.42857142857142855,"[2] Chen, H.-Y. and Chang, Y.-W. (2009). Global and detailed routing. In Electronic Design
Automation, pages 687–749. Elsevier."
REFERENCES,0.43197278911564624,"[3] Chen, T., Xiong, S., He, H., and Yu, B. (2023). Trouter: Thermal-driven pcb routing via non-local
crisscross attention networks. IEEE Transactions on Computer-Aided Design of Integrated Circuits
and Systems."
REFERENCES,0.43537414965986393,"[4] Cheng, R., Lyu, X., Li, Y., Ye, J., Hao, J., and Yan, J. (2022). The policy-gradient placement and
generative routing neural networks for chip design. Advances in Neural Information Processing
Systems, 35:26350–26362."
REFERENCES,0.4387755102040816,"[5] Cheng, R. and Yan, J. (2021). On joint learning for solving placement and routing in chip design.
Advances in Neural Information Processing Systems, 34:16508–16519."
REFERENCES,0.4421768707482993,"[6] Cho, M., Lu, K., Yuan, K., and Pan, D. Z. (2007). Boxrouter 2.0: Architecture and implementation
of a hybrid and robust global router. In 2007 IEEE/ACM International Conference on Computer-
Aided Design, pages 503–508. IEEE."
REFERENCES,0.445578231292517,"[7] Chu, C. and Wong, Y.-C. (2005). Fast and accurate rectilinear steiner minimal tree algorithm for
vlsi design. In Proceedings of the 2005 international symposium on Physical design, pages 28–35."
REFERENCES,0.4489795918367347,"[8] Du, X., Wang, C., Zhong, R., and Yan, J. (2023). Hubrouter: Learning global routing via
hub generation and pin-hub connection. In Thirty-seventh Conference on Neural Information
Processing Systems."
REFERENCES,0.4523809523809524,"[9] Ganley, J. L. and Cohoon, J. P. (1994). Routing a multi-terminal critical net: Steiner tree
construction in the presence of obstacles. In Proceedings of IEEE International Symposium on
Circuits and Systems-ISCAS’94, volume 1, pages 113–116. IEEE."
REFERENCES,0.4557823129251701,"[10] Garey, M. R. and Johnson, D. S. (1977). The rectilinear steiner tree problem is np-complete.
SIAM Journal on Applied Mathematics, 32(4):826–834."
REFERENCES,0.45918367346938777,"[11] Hanan, M. (1966). On steiner’s problem with rectilinear distance. SIAM Journal on Applied
mathematics, 14(2):255–265."
REFERENCES,0.46258503401360546,"[12] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778."
REFERENCES,0.46598639455782315,"[13] Ho, J.-M., Vijayan, G., and Wong, C.-K. (1990). New algorithms for the rectilinear steiner
tree problem. IEEE transactions on computer-aided design of integrated circuits and systems,
9(2):185–193."
REFERENCES,0.46938775510204084,"[14] Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., and Liu, W. (2019). Ccnet: Criss-cross
attention for semantic segmentation. In Proceedings of the IEEE/CVF international conference on
computer vision, pages 603–612."
REFERENCES,0.47278911564625853,"[15] Juhl, D., Warme, D. M., Winter, P., and Zachariasen, M. (2018). The geosteiner software
package for computing steiner trees in the plane: an updated computational study. Mathematical
Programming Computation, 10:487–532."
REFERENCES,0.47619047619047616,"[16] Kastner, R., Bozorgzadeh, E., and Sarrafzadeh, M. (2002). Pattern routing: Use and theory for
increasing predictability and avoiding coupling. IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems, 21(7):777–790."
REFERENCES,0.47959183673469385,"[17] Kramer, M. and Van Leeuwen, J. (1984). The complexity ofwirerouting and finding minimum
area layouts for arbitrary vlsicircuits. Adv. Comput. Res, 2:129–146."
REFERENCES,0.48299319727891155,"[18] Lai, Y., Liu, J., Tang, Z., Wang, B., Hao, J., and Luo, P. (2023). Chipformer: Transferable chip
placement via offline decision transformer. arXiv preprint arXiv:2306.14744."
REFERENCES,0.48639455782312924,"[19] Lai, Y., Mu, Y., and Luo, P. (2022). Maskplace: Fast chip placement via reinforced visual
representation learning. Advances in Neural Information Processing Systems, 35:24019–24030."
REFERENCES,0.4897959183673469,"[20] Liao, H., Zhang, W., Dong, X., Poczos, B., Shimada, K., and Burak Kara, L. (2020). A deep
reinforcement learning approach for global routing. Journal of Mechanical Design, 142(6):061701."
REFERENCES,0.4931972789115646,"[21] Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Dollár, P. (2017). Focal loss for dense object
detection. In Proceedings of the IEEE international conference on computer vision, pages 2980–
2988."
REFERENCES,0.4965986394557823,"[22] Liu, C.-H., Kuo, S.-Y., Lee, D., Lin, C.-S., Weng, J.-H., and Yuan, S.-Y. (2012). Obstacle-
avoiding rectilinear steiner tree construction: A steiner-point-based algorithm. IEEE Transactions
on Computer-Aided Design of Integrated Circuits and Systems, 31(7):1050–1060."
REFERENCES,0.5,"[23] Liu, J., Chen, G., and Young, E. F. (2021). Rest: Constructing rectilinear steiner minimum tree
via reinforcement learning. In 2021 58th ACM/IEEE Design Automation Conference (DAC), pages
1135–1140. IEEE."
REFERENCES,0.5034013605442177,"[24] Liu, J., Pui, C.-W., Wang, F., and Young, E. F. (2020). Cugr: Detailed-routability-driven 3d
global routing with probabilistic resource model. In 2020 57th ACM/IEEE Design Automation
Conference (DAC), pages 1–6. IEEE."
REFERENCES,0.5068027210884354,"[25] Liu, J. and Young, E. F. (2023). Edge: Efficient dag-based global routing engine. In 2023 60th
ACM/IEEE Design Automation Conference (DAC), pages 1–6. IEEE."
REFERENCES,0.5102040816326531,"[26] Liu, S., Pu, Y., Liao, P., Wu, H., Zhang, R., Chen, Z., Lv, W., Lin, Y., and Yu, B. (2022).
Fastgr: Global routing on cpu-gpu with heterogeneous task graph scheduler. IEEE Transactions
on Computer-Aided Design of Integrated Circuits and Systems."
REFERENCES,0.5136054421768708,"[27] Liu, W.-H., Kao, W.-C., Li, Y.-L., and Chao, K.-Y. (2013). Nctu-gr 2.0: Multithreaded collision-
aware global routing with bounded-length maze routing. IEEE Transactions on computer-aided
design of integrated circuits and systems, 32(5):709–722."
REFERENCES,0.5170068027210885,"[28] Mahboubi, S., Ninomiya, H., Kamio, T., Asai, H., et al. (2021). A nesterov’s accelerated
quasi-newton method for global routing using deep reinforcement learning. Nonlinear Theory and
Its Applications, IEICE, 12(3):323–335."
REFERENCES,0.5204081632653061,"[29] Mirhoseini, A., Goldie, A., Yazgan, M., Jiang, J. W., Songhori, E., Wang, S., Lee, Y.-J., Johnson,
E., Pathak, O., Nazi, A., et al. (2021). A graph placement methodology for fast chip design.
Nature, 594(7862):207–212."
REFERENCES,0.5238095238095238,"[30] Nam, G.-J., Yildiz, M., Pan, D. Z., and Madden, P. H. (2007). Ispd placement contest updates
and ispd 2007 global routing contest. In Proceedings of the 2007 international symposium on
Physical design, pages 167–167."
REFERENCES,0.5272108843537415,"[31] Ronneberger, O., Fischer, P., and Brox, T. (2015). U-net: Convolutional networks for biomedical
image segmentation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI
2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pages 234–241. Springer."
REFERENCES,0.5306122448979592,"[32] Utyamishev, D. and Partin-Vaisband, I. (2020). Late breaking results: A neural network that
routes ics. In 2020 57th ACM/IEEE Design Automation Conference (DAC), pages 1–2. IEEE."
REFERENCES,0.5340136054421769,"[33] Wu, T.-H., Davoodi, A., and Linderoth, J. T. (2010). Grip: Global routing via integer pro-
gramming. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,
30(1):72–84."
REFERENCES,0.5374149659863946,"[34] Xie, Z., Huang, Y.-H., Fang, G.-Q., Ren, H., Fang, S.-Y., Chen, Y., and Hu, J. (2018). Routenet:
Routability prediction for mixed-size designs using convolutional neural network.
In 2018
IEEE/ACM International Conference on Computer-Aided Design (ICCAD), pages 1–8. IEEE."
REFERENCES,0.5408163265306123,"[35] Zhou, Z., Zhu, Z., Chen, J., Ma, Y., Yu, B., Ho, T.-Y., Lemieux, G., and Ivanov, A. (2019).
Congestion-aware global routing using deep convolutional generative adversarial networks. In
2019 ACM/IEEE 1st Workshop on Machine Learning for CAD (MLCAD), pages 1–6. IEEE."
REFERENCES,0.54421768707483,"A
Related Information"
REFERENCES,0.5476190476190477,"(a)
(b)
(c)"
REFERENCES,0.5510204081632653,"Figure S1: Differences between hub points and candidate points. (a) Routing result (white lines)
generated by traditional global router for a 12-pin (black squares) net . (b) The hub points (red
squares) extracted by [8] as their label points. (c) The candidate points (green squares) labeled by
NeuralSteiner as the learning targets."
REFERENCES,0.5544217687074829,"Table S1: Summary of predicted candidate points for ISPD07. We respectively show the scale size,
number of nets, average / maximum number of pins and candidate points added by NeuralSteiner."
REFERENCES,0.5578231292517006,"Case
adaptec01_2d
adaptec02_2d
adaptec03_2d
adaptec04_2d
adaptec05_2d
newblue01_2d
newblue02_2d
newblue03_2d"
REFERENCES,0.5612244897959183,"Size
324×324
424×424
774×779
774×779
465×468
399×399
557×463
973×1256
Cap. (V/H)
70 / 70
80 / 80
62 / 62
62 / 62
110 / 110
62 / 62
110 /110
80 /80
#Nets
219794
260159
466295
515304
867441
331663
463213
551667
Avg. #Pins
4.29
4.09
4.02
3.71
4.03
3.73
3.83
3.50
Max #Pins
2271
1935
3713
3974
9863
12335
8089
16008
#Pins ≤10
92.90%
94.03%
95.24%
96.09%
94.94%
95.50%
96.02%
97.12%
Avg. #Cands
4.61
4.41
4.76
4.34
4.31
3.86
3.88
3.56
Max #Cands
2474
2145
4189
4226
10681
13109
8517
16951"
REFERENCES,0.564625850340136,"B
Implementation details"
REFERENCES,0.5680272108843537,"B.1
NeuralSteiner Network Architecture."
REFERENCES,0.5714285714285714,"We employ a modified ResNet-34 structure [12] as the backbone of our model and combined it with
the recurrent crisscross attention mechanism (RCCA) [14]. Our network accepts two channels, pin
map and overflow map, as input, while the output is a single-channel map indicating the positions of
the predicted candidate points. To retain more accurate spatial location details of feature maps for
the construction of RST, we remove the max-pooling layer from the ResNet backbone. Every two
residual blocks are grouped in one pair. Thanks to the 3 × 3 convolution, the number of channels
gradually increasing from 2 to 256. After 4 pairs of residual blocks, a convolutional layer is applied to
obtain the feature map H of dimension reduction. Then, H is fed into the RCCA module to generate
a new feature map H0 which aggregate non-local contextual information. In RCCA module, we
sequentially apply two crisscross attention modules and assign different weights (attention map) to
each part of the feature. After the RCCA, we concatenate the feature H0 processed by RCCA with
the local and dimension-reduced feature X. It is followed by another 4 pairs of residual blocks with
3 × 3 convolution and the number of channels gradually decreasing from 256 to 1. The output feature
map undergoes a sigmoid operation to obtain the final predicted candidate point map."
REFERENCES,0.5748299319727891,"The crisscross attention (CCA) mechanism is adopted in NeuralSteiner to capture long-range de-
pendencies in feature maps and aggregate full-net spatial and overflow information by computing
attention along both horizontal and vertical directions for each pixel, which is shown in Fig.2. Given
an input feature map X ∈RC×W ×H, where C is the number of channels, W and H is the width and
height respectively, the CCA starts by computing queries, keys, and values using 1 × 1 convolutions"
REFERENCES,0.5782312925170068,"with weight matrices Wq, Wk, and Wv:"
REFERENCES,0.5816326530612245,"Q = Wq ∗X,
K = Wk ∗X,
V = Wv ∗X"
REFERENCES,0.5850340136054422,"Here, ∗denotes the convolution operation and where {Q, K} ∈RC′×W ×H and V ∈RC×W ×H. For
each pixel (i, j) in the input feature map X, we use K(i,j) to denote the set of feature vectors extracted
from K which are in the same row or column with position (i, j) and K(i,j) ∈R(W +H−1)×C′. first
compute the horizontal attention weights AH
i,j by applying the softmax function to the dot product of
the query at row i and the key at position (i, j):"
REFERENCES,0.5884353741496599,"d(i,j),k = Q(i,j) · KT
(i,j),k"
REFERENCES,0.5918367346938775,"where d(i,j),k ∈D is the degree of correlation between Q(i,j) and K(i,j),k, k = [1, ..., W + H −1].
Note that D ∈R(H+W −1)×(W ×H), we compute the attention map A by applying the softmax
function to D. We also use V(i,j) to denote the collection of feature vectors extracted from V which
are in the same row or column with position (i, j), then the crisscross spatial and overflow information
can be aggregated by:"
REFERENCES,0.5952380952380952,"Y(i,j) ="
REFERENCES,0.5986394557823129,"W +H−1
X"
REFERENCES,0.6020408163265306,"k=0
A(i,j),kV(i,j),k + X(i,j)"
REFERENCES,0.6054421768707483,"The output feature map Y ∈RC×H×W is then returned as the result of the crisscross attention
mechanism. As shown in Fig. 3, after two CCA operations, the information of positions in different
rows and columns from the coordinates (i, j) can also be aggregated, thus enabling learning of
long-range spatial and overflow correlation."
REFERENCES,0.608843537414966,"B.2
Training details for candidate point prediction phase."
REFERENCES,0.6122448979591837,"We adopt CUGR [24] to conduct routing for dataset construction, and we choose bigblue04_3d,
newblue03_3d, newblue04_3d and newblue07_3d (name of the cases in ISPD-07) as the training
cases and generate 25K training samples using each case, which is more than that in HubRouter [8].
Thus, we have a total of nearly 100K for training."
REFERENCES,0.6156462585034014,"We use learning rate in [0.001, 0.00001] and reduce the learning rates by 0.5 if the validation loss
does not decrease in 2 epochs. The training will continue until the validation loss no longer decreases
for over 10 epochs or the number of epoch reaches a maximum of 100. In the training loss, we use
cfl = 1.0, cdi = 1.0 and cof = 2.0 to encourage the exploration of lower-overflow candidate points.
The number of ResNet blocks is 8 before the RCCA module and 8 behind it."
REFERENCES,0.6190476190476191,"Each experiment in this work is conducted on a system equipped with an Intel(R) Xeon(R) Gold
6230R CPU, NVIDIA A800 (80 GB) GPU, and 250 GB RAM. We repeat 3 times under different
seeds for HubRouter on the small nets test set and ISPD98 and report the error bars, and then choose
the seed with best overflow for HubRouter (GAN) to conduct the ISPD07 experiment."
REFERENCES,0.6224489795918368,Algorithm 1 Training in candidate point prediction phase
REFERENCES,0.6258503401360545,"Require: num_iters (number of training iterations), B (minibatch size), trainingSet (training
set), cfl, cdi, cof (loss function coefficients)
Ensure: θ (model parameters)"
REFERENCES,0.6292517006802721,"1: Initialize model parameters θ
2: for iter = 1 to num_iters do
3:
Sample a batch B ⊂trainingSet with |B| = B
4:
Pad all pin maps, overflow maps and label point maps in B to the same size
5:
Descend the stochastic gradient of loss:"
REFERENCES,0.6326530612244898,∇θ [cfl · ℓfocal + cdi · ℓdice + cof · ℓof]
REFERENCES,0.6360544217687075,"6: end for
7: return Model with parameters θ"
REFERENCES,0.6394557823129252,"B.3
Parallel Routing Tasks Construction."
REFERENCES,0.6428571428571429,Algorithm 2 Parallel routing tasks construction
REFERENCES,0.6462585034013606,"1: Input: A set of nets N need to route in a chip design where each net has a corresponding HPWL.
A set of net tasks T.
2: N ←{net1, net2, . . . , netn}
3: T ←{}
4: Nsorted ←Sort(N, by HPWL)
▷Sort nets by HPWL
5: for each neti ∈Nsorted do
6:
assigned ←False
7:
for each taskj ∈T do
8:
if neti has no conflicts with all net in taskj then
9:
taskj ←taskj ∪{neti}
10:
assigned ←True
11:
break
12:
end if
13:
end for
14:
if not assigned then
15:
newTask ←{}
16:
newTask ←newTask ∪{neti}
17:
T ←T ∪{newTask}
18:
end if
19: end for
20: return T"
REFERENCES,0.6496598639455783,"B.4
Net Augmented Graph Construction."
REFERENCES,0.6530612244897959,Algorithm 3 Net augmented graph construction
REFERENCES,0.6564625850340136,"Require: Candidate point map C (2D map), pin map P (2D map), overflow map O (2D map).
Ensure: netAugmentedGraph (Graph)."
REFERENCES,0.6598639455782312,"1: mergedMap ←C + P
2: points ←{(x, y) | mergedMap[x][y] ≥1}
3: NAG ←empty graph
4: for all p ∈points do
5:
Add p as a node in NAG
6: end for
7: for all (pi, pj) ∈pairs(points) do
8:
if pi.x = pj.x or pi.y = pj.y then
9:
if No other points on the line between pi and pj then
10:
weight ←calculate_weight(pi, pj)
11:
Add edge (pi, pj, weight) to netAugmentedGraph
12:
end if
13:
end if
14: end for
15: while number_of_connected_components(netAugmentedGraph) > 1 do
16:
Select one point from each connected component
17:
Add edges between selected points
18: end while
19: return netAugmentedGraph"
REFERENCES,0.6632653061224489,"B.5
Overflow-avoiding RST Construction."
REFERENCES,0.6666666666666666,Algorithm 4 Overflow-Avoiding RST Construction
REFERENCES,0.6700680272108843,"Require: NAG (Graph), Pins (Set of pins)
Ensure: RST (Constructed RST)"
REFERENCES,0.673469387755102,"1: Components ←{{p} | p ∈Pins}
2: Parallelize computation of dist(Ci, Cj) for Ci, Cj ∈Components on NAG
3: while |Components| > 1 do
4:
(C1, C2) ←arg minCi,Cj∈Components dist(Ci, Cj)
5:
RST ←RST ∪{(C1, C2)}
6:
Cnew ←C1 ∪C2
7:
Components ←(Components \ {C1, C2})
8:
Delete dist(C1, Ck) and dist(C2, Ck) for all Ck ∈Components
9:
Update dist(Cnew, Ck) for all Ck ∈Components in parallel
10:
Components ←Components ∪{Cnew}
11: end while
12: return RST"
REFERENCES,0.6768707482993197,"Time complexity and scalability of the RST construction algorithm. We use Dijkstra’s algorithm
to search for the shortest path between every two points. Suppose the number of pin in one net
is Npin. Given that NAG is a sparse graph, and the number of edges and nodes in the NAG is
(O(Npin)) according to Table S1, Dijkstra’s algorithm with a binary heap yields a time complexity
of (O(Npin log Npin)). The total time complexity is (O((Npin)2 log Npin)) to calculate the shortest
path for all points in the connected component. In each iteration, we connect the two components
with the shortest distance and update the distances matrix. It will end with a connected component
containing all pins. So, in the worst case, the total time complexity of the RST construction algorithm
is (O((Npin)4 log Npin)). Table S1 shows that the pin size for most of nets in dataset is no more
than 10, indicating that the algorithm will be efficient."
REFERENCES,0.6802721088435374,"Additionally, the algorithm can be accelerated by simple heuristic rules. When calculating the distance
from one connected component to another, we compute and filter out a fixed number of node pairs
with the shortest Euclidean distance, performing Dijkstra’s algorithm only on these limited node pairs.
This reduces the complexity of the RST construction algorithm to (O((Npin)3 log Npin)). As shown
in Table S3, the accelerated algorithm NeuralSteiner-Ltd significantly improves solving efficiency
while achieving similar wirelength and overflow compared to the original version."
REFERENCES,0.6836734693877551,Algorithm 5 Inference Algorithm
REFERENCES,0.6870748299319728,"Require: netlist (netlist), resources (routing resources)
Ensure: routes (routing results)"
REFERENCES,0.6904761904761905,"1: Initialize routes ←∅
2: // Parallel routing tasks construction
3: T ←ParallelRoutingTasksConstruction(netlist)
▷Using Algorithm 1
4: for each task t in T do
5:
// NeuralSteiner for candidate point prediction
6:
candidatePointMaps ←NeuralSteiner(t)
▷Predict candidate point maps
7:
// Net augmented graph construction
8:
G ←NetAugmentedGraphConstruction(t, candidatePointMaps)
▷Using Algorithm 3
9:
// Overflow-Avoiding RST Construction
10:
routingResults ←OverflowAvoidingRSTConstruction(G)
▷Using Algorithm 2
11:
Add routingResults to routes
12: end for
13: return routes"
REFERENCES,0.6938775510204082,"C
Additional Results"
REFERENCES,0.6972789115646258,"0
10
20
30
40
50
60 0 10 20 30 40 50 60"
REFERENCES,0.7006802721088435,"Net: 3255 
OF: 10 WL: 38.0 6 4 2 0 2 4 6 8 10 12"
REFERENCES,0.7040816326530612,"0
10
20
30
40
50
60 0 10 20 30 40 50 60"
REFERENCES,0.7074829931972789,"Net: 3258 
OF: 25 WL: 56.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0"
REFERENCES,0.7108843537414966,"0
10
20
30
40
50
60 0 10 20 30 40 50 60"
REFERENCES,0.7142857142857143,"Net: 4440 
OF: 10 WL: 49.0 5.0 2.5 0.0 2.5 5.0 7.5 10.0"
REFERENCES,0.717687074829932,"0
10
20
30
40
50
60 0 10 20 30 40 50 60"
REFERENCES,0.7210884353741497,"Net: 7525 
OF: 25 WL: 46.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0"
REFERENCES,0.7244897959183674,"0
10
20
30
40
50
60 0 10 20 30 40 50 60"
REFERENCES,0.7278911564625851,"Net: 7262 
OF: 37 WL: 68.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0"
REFERENCES,0.7312925170068028,"Figure S2: Comparison of actual solutions of Hubrouter (first line) and our NeuralSteiner
(second line), randomly sampled from ibm01. Pins (black hollow square), hub points (yellow
circle) candidate points (green circle) and the routes (purple line) are shown in figures."
REFERENCES,0.7346938775510204,"Table S2: Experiments on 4 kinds of small nets. We train three architectures of HubRouter with
default settings and test the results of three different random seeds for each HubRouter model.
NeuralSteiner is tested three times to obtain the mean and standard deviation values of generation
time. The best results are highlighted in bold."
REFERENCES,0.7380952380952381,"Metric
Case
HubRouter
HubRouter
HubRouter
NeuralSteiner
(VAE)
(DPM)
(GAN)"
REFERENCES,0.7414965986394558,"Route-small-4
1.000 ± 0.000
1.000 ± 0.000
1.000 ± 0.000
1.000
Correctness
Route-small
1.000 ± 0.000
1.000 ± 0.000
1.000 ± 0.000
1.000
Rate (%)
Route-large-4
1.000 ± 0.000
1.000 ± 0.000
1.000 ± 0.000
1.000
Route-large
1.000 ± 0.000
1.000 ± 0.000
1.000 ± 0.000
1.000"
REFERENCES,0.7448979591836735,"Route-small-4
1.086 ± 0.031
1.072 ± 0.020
1.012 ± 0.011
1.003
Wirelength
Route-small
1.051 ± 0.017
1.191 ± 0.006
1.002 ± 0.001
1.003
Rate (%)
Route-large-4
1.124 ± 0.026
1.102 ± 0.039
1.004 ± 0.021
1.002
Route-large
1.045 ± 0.023
1.264 ± 0.014
1.001 ± 0.002
1.002"
REFERENCES,0.7482993197278912,"Route-small-4
5.33 ± 0.09
514.71 ± 4.42
5.14 ± 0.08
6.60
Generation Time
Route-small
5.68 ± 0.14
506.20 ± 2.99
6.63 ± 0.35
7.01
(GPU Sec)
Route-large-4
5.41 ± 0.15
511.42 ± 2.85
5.68 ± 0.16
6.89
Route-large
7.17 ± 0.05
531.98 ± 3.63
7.16 ± 0.21
8.77"
REFERENCES,0.7517006802721088,"Table S3: Comparison of NeuralSteiner with limited connected component nodes (NeuralSteiner-
Ltd) and original NeuralSteiner on ISPD-98 (ibm01-06)."
REFERENCES,0.7551020408163265,"Metric
Model
ibm01
ibm02
ibm03
ibm04
ibm05
ibm06"
REFERENCES,0.7585034013605442,"WL
NeuralSteiner
61735
170405
148036
166648
415684
283727
NeuralSteiner-Ltd
61654
170430
148730
166160
415872
283215"
REFERENCES,0.7619047619047619,"OF
NeuralSteiner
2033
3953
1754
2794
18
3042
NeuralSteiner-Ltd
2042
4046
1743
2883
18
3095"
REFERENCES,0.7653061224489796,"Time (Sec)
NeuralSteiner
24.75
33.18
41.94
46.63
67.52
63.44
NeuralSteiner-Ltd
13.68
31.84
21.20
23.30
47.65
36.26"
REFERENCES,0.7687074829931972,NeurIPS Paper Checklist
CLAIMS,0.7721088435374149,1. Claims
CLAIMS,0.7755102040816326,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.7789115646258503,"Answer: [Yes]
Justification: We make sure that the main claims made in the abstract and introduction
accurately reflect the paper’s contributions."
CLAIMS,0.782312925170068,Guidelines:
CLAIMS,0.7857142857142857,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.7891156462585034,2. Limitations
LIMITATIONS,0.7925170068027211,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.7959183673469388,Answer: [Yes]
LIMITATIONS,0.7993197278911565,"Justification: We discuss the limitations at the end of the main paper. We also add a discus-
sion about time complexity analysis of the RST construction algorithm in Appendix.B.5."
LIMITATIONS,0.8027210884353742,Guidelines:
LIMITATIONS,0.8061224489795918,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.8095238095238095,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.8129251700680272,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.8163265306122449,Answer: [NA]
THEORY ASSUMPTIONS AND PROOFS,0.8197278911564626,Justification: We don’t have theoretical result in this paper.
THEORY ASSUMPTIONS AND PROOFS,0.8231292517006803,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.826530612244898,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8299319727891157,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8333333333333334,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8367346938775511,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8401360544217688,Justification: We has introduced the details of implementation and training protocol.
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8435374149659864,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8469387755102041,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.8503401360544217,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.8537414965986394,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.8571428571428571,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8605442176870748,"Justification: Our source code and dataset will be released at https://github.com/
liuruizhi96/NeuralSteiner"
OPEN ACCESS TO DATA AND CODE,0.8639455782312925,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8673469387755102,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.8707482993197279,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.8741496598639455,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.8775510204081632,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8809523809523809,Justification: We specify all the training and test details.
OPEN ACCESS TO DATA AND CODE,0.8843537414965986,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8877551020408163,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.891156462585034,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8945578231292517,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8979591836734694,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9013605442176871,Justification: We report error bars for HubRouter method which use random seeds.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9047619047619048,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9081632653061225,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions)."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9115646258503401,"• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9149659863945578,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We introduce these details in the appendix.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9183673469387755,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9217687074829932,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: We make sure to preserve anonymity.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9251700680272109,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9285714285714286,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: There is no societal impact of the work performed.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9319727891156463,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.935374149659864,"• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9387755102040817,11. Safeguards
SAFEGUARDS,0.9421768707482994,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9455782312925171,Answer: [NA]
SAFEGUARDS,0.9489795918367347,Justification: The paper poses no such risks.
SAFEGUARDS,0.9523809523809523,Guidelines:
SAFEGUARDS,0.95578231292517,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9591836734693877,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9625850340136054,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9659863945578231,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9693877551020408,"Justification: All assets used in the paper, such as datasets and models, are properly credited
with clear documentation of their sources and licenses, ensuring full compliance with
existing terms of use."
LICENSES FOR EXISTING ASSETS,0.9727891156462585,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9761904761904762,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset."
LICENSES FOR EXISTING ASSETS,0.9795918367346939,"• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
LICENSES FOR EXISTING ASSETS,0.9829931972789115,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9863945578231292,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
LICENSES FOR EXISTING ASSETS,0.9897959183673469,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9931972789115646,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9965986394557823,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
