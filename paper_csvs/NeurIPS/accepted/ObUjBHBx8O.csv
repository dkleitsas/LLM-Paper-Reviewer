Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.003424657534246575,"Models trained with empirical risk minimization (ERM) are prone to be biased
towards spurious correlations between target labels and bias attributes, which leads
to poor performance on data groups lacking spurious correlations. It is particularly
challenging to address this problem when access to bias labels is not permitted. To
mitigate the effect of spurious correlations without bias labels, we first introduce a
novel training objective designed to robustly enhance model performance across
all data samples, irrespective of the presence of spurious correlations. From this
objective, we then derive a debiasing method, Disagreement Probability based Re-
sampling for debiasing (DPR), which does not require bias labels. DPR leverages
the disagreement between the target label and the prediction of a biased model to
identify bias-conflicting samples—those without spurious correlations—and up-
samples them according to the disagreement probability. Empirical evaluations on
multiple benchmarks demonstrate that DPR achieves state-of-the-art performance
over existing baselines that do not use bias labels. Furthermore, we provide a theo-
retical analysis that details how DPR reduces dependency on spurious correlations."
INTRODUCTION,0.00684931506849315,"1
Introduction"
INTRODUCTION,0.010273972602739725,"Desert
Pasture"
INTRODUCTION,0.0136986301369863,"Cow
Camel
Class label"
INTRODUCTION,0.017123287671232876,Background
INTRODUCTION,0.02054794520547945,"Figure 1: An illustration of the
cow/camel classification task.
Red dotted boxes indicate sam-
ples where spurious correlations
do not hold."
INTRODUCTION,0.023972602739726026,"In the realm of machine learning, many classification models em-
ploy Empirical Risk Minimization (ERM) [46], which optimizes
average performance. However, this approach has been found to
underperform on certain groups of data [4, 15, 11] due to the preva-
lence of spurious correlations within training datasets [31, 38, 39].
Spurious correlations refer to the strong correlations between tar-
get labels and easy-to-learn attributes (i.e., bias attributes), which
are present in a majority of the training examples. ERM-trained
models often rely on these bias attributes [12], leading to biased
predictions and poor generalization on minority groups where spu-
rious correlations are absent. For example, consider the cow/camel
classification task illustrated in Figure 1. A majority of camel
images feature desert backgrounds, while a majority of cow im-
ages feature pasture backgrounds. Models trained via ERM might
learn to recognize animals based on their backgrounds—desert
for camels and pasture for cows—rather than on their distinctive
features. This reliance can result in misclassifications, such as
erroneously identifying a camel in a pasture as a cow. Addressing these spurious correlations is a"
INTRODUCTION,0.0273972602739726,∗Corresponding author
INTRODUCTION,0.030821917808219176,"critical issue across various applications, including medical imaging [36], algorithmic fairness [10],
and education [37]."
INTRODUCTION,0.03424657534246575,"There have been extensive efforts to reduce the effects of spurious correlations. Numerous studies have
initially relied on the assumption that bias attributes are given in the training dataset [38, 18, 24, 35].
However, annotating bias attributes is labor-intensive and expensive, rendering methods dependent
on such annotations impractical. Consequently, research has shifted towards developing debiasing
techniques that do not require bias labels during training [34, 27, 1, 29, 28, 49]. In this line of
research, samples with spurious correlations are called bias-aligned samples, whereas those lacking
such correlations are termed bias-conflicting samples [34, 27, 23]. In the absence of explicit bias
labels, many existing methods focus on identifying bias-conflicting samples and employ strategies
such as upweighting or upsampling these samples to counteract the negative impacts of spurious
correlations [34, 28, 1]. Despite their straightforward nature, questions persist regarding the optimal
scale for the weight of each sample to effectively reduce dependence on spurious correlations."
INTRODUCTION,0.03767123287671233,"We thus formulate a debiasing objective for improved robustness against spurious correlations and
derive a practical resampling-based algorithm. Specifically, our debiasing objective is built on two
groups: the bias-aligned group and the bias-conflicting group, each consisting of bias-aligned and
bias-conflicting samples, respectively. This objective encourages the model to perform equally
well across both groups. The key insight behind the objective is that models relying on spurious
correlations exhibit worse performance on the bias-conflicting group compared to the bias-aligned
group, whereas models that do not rely on spurious correlations should exhibit strong performance
across both groups. Then, under a simple condition, we derive an objective function as a weighted
loss, with weights proportional to the bias-conflicting probability for each training example. Since we
consider cases where bias labels are not provided, we use the disagreement probability between the
target label and the prediction of an intentionally biased model as a substitute for the bias-conflicting
probability. Our proposed approach is simple yet effective in mitigating spurious correlations."
INTRODUCTION,0.0410958904109589,The main contributions of this paper can be summarized as follows:
INTRODUCTION,0.04452054794520548,"• We present a debiasing objective for mitigating reliance on bias attributes. This objective aims
to guide the model to have similar performance over two groups: the bias-aligned group and the
bias-conflicting group. This approach differs from previous works, which typically define groups
based on combinations of target labels and bias labels."
INTRODUCTION,0.04794520547945205,"• We propose a new method, coined Disagreement Probability based Resampling for debiasing
(DPR), which is derived from the proposed objective. DPR leverages the disagreement probability
between the target label and the prediction of a biased model to determine the weight of each
training example."
INTRODUCTION,0.05136986301369863,"• DPR achieves state-of-the-art performance across six benchmarks with spurious correlations,
surpassing existing methods that do not use bias labels. Notably, on the bias-conflicting test set of
Biased FFHQ (BFFHQ)—which contains only 0.5% of bias-conflicting samples in the training
set—the proposed method significantly improves accuracy by 20.87% compared to ERM and by
6.2% compared to the best baseline."
INTRODUCTION,0.0547945205479452,"• We theoretically demonstrate that DPR encourages consistent model performance across both
bias-aligned and bias-conflicting groups, thus promoting strong performance across all samples
regardless of spurious correlations."
RELATED WORK,0.05821917808219178,"2
Related work"
RELATED WORK,0.06164383561643835,"Debiasing with bias annotations.
Numerous previous works utilize bias labels for debiasing
[38, 48, 18, 3, 44, 53]. For example, Group DRO [38] employs bias labels to define groups and
directly enhances worst-group accuracy to mitigate the effect of spurious correlations; meanwhile,
LISA [48] mixes two samples with the same label but different domains, or with different labels
but the same domain, canceling out spurious correlations and learning invariant predictors. These
methods have demonstrated their effectiveness across multiple benchmarks with spurious correlations.
However, annotating bias labels for each training example is labor-intensive, and obtaining such labels
can sometimes be challenging due to privacy concerns as well [51]. Consequently, in more recent
studies, some researchers have utilized only a small set of bias-annotated data to reduce reliance on"
RELATED WORK,0.06506849315068493,"bias labels [24, 35, 19]. For example, DFR [24] uses a small group-balanced dataset with bias labels
to retrain the last layer of the ERM-trained model."
RELATED WORK,0.0684931506849315,"Debiasing without bias annotations.
In an effort to eliminate reliance on bias annotations, recent
studies predominantly focus on reducing bias without explicit bias labels [34, 22, 27, 1, 29, 28,
49, 45]. Given the unavailability of bias labels, these methods commonly employ a two-stage
strategy: (1) identifying bias-aligned and bias-conflicting samples, and (2) training the debiased
model by leveraging information obtained from (1). The identification of bias-conflicting samples
is accomplished through the use of a deliberately biased model, trained either with generalized
cross-entropy loss [34, 22, 27, 1, 29] or standard cross-entropy loss [28, 49, 45]. For instance, LfF
[34] and DFA [27] regard samples as bias-conflicting samples if the biased model exhibits higher
cross-entropy losses on these samples compared to the debiased model. Subsequent efforts then
focus on enhancing classification accuracy for these identified bias-conflicting samples. Numerous
studies have proposed diverse debiasing methods [14, 52, 8, 23], including those based on contrastive
learning [49, 20] and unsupervised clustering [40, 42]."
RELATED WORK,0.07191780821917808,"Many debiasing methods that do not require bias labels aim to enhance performance on the bias-
conflicting group, yet the optimal extent of this enhancement remains uncertain. We contend that a
debiased model should exhibit consistent performance across both bias-aligned and bias-conflicting
groups. However, to the best of our knowledge, no existing training objective is explicitly designed
for this purpose. Therefore, we introduce a novel training objective tailored for this purpose, from
which we develop DPR—a debiasing method that does not rely on bias labels."
PROBLEM FORMULATION,0.07534246575342465,"3
Problem formulation"
PROBLEM FORMULATION,0.07876712328767123,"We consider a multi-class classification problem with a dataset D = {(xi, yi)}n
i=1, where each
xi ∈X represents an input, and each yi ∈Y is the corresponding label from K possible classes.
These examples are presumed to be sampled from a training distribution P. Given a classification
model fθ : X →RK that maps an input to K logits, and a convex loss ℓ: X ×Y →R≥0, ERM aims
to find a model that minimizes the expected loss E(x,y)∼P [ℓ(fθ(x), y)]. To this end, we typically
minimize a surrogate loss ˆLavg:"
PROBLEM FORMULATION,0.0821917808219178,ˆLavg = 1 n X
PROBLEM FORMULATION,0.08561643835616438,"(x,y)∈D
ℓ(fθ(x), y).
(1)"
PROBLEM FORMULATION,0.08904109589041095,"The cross-entropy (CE) loss is commonly used for training classification models. It is defined as
ℓCE(fθ(x), y) = −fθ(x)[y] + log P
y′ exp(fθ(x)[y′]), where fθ(x)[y] denotes the logit correspond-
ing to the y-th class label."
PROBLEM FORMULATION,0.09246575342465753,"We assume there is a spurious correlation presence indicator b ∈B = {correlated, uncorrelated} in
the dataset. As this indicator denotes whether spurious correlations are present within each sample, it
can also be considered as a bias-aligned or bias-conflicting group indicator. Similar to the Group
DRO setting [38, 17], we adopt the latent prior probability change assumption [43]. With a group
indicator b, we assume
P(x, y|b) = Q(x, y|b),
P(b) ̸= Q(b),
(2)"
PROBLEM FORMULATION,0.0958904109589041,"where P and Q denote the training and test data distributions, respectively. Under this assumption,
P(x, y) and Q(x, y) are represented as mixtures of the conditional distributions P(x, y|b). Our
goal is to find models that are robust against spurious correlations. Regardless of the presence of
spurious correlations within each data example, a model that does not rely on these correlations
should exhibit strong performance across all examples. In other words, the debiased model should
perform consistently well on both bias-aligned and bias-conflicting groups. To this end, we propose
the following training objective to avoid spurious correlations:"
PROBLEM FORMULATION,0.09931506849315068,"min
θ
max
b∈B 
"
PROBLEM FORMULATION,0.10273972602739725,"
ˆLb := 1 nb X"
PROBLEM FORMULATION,0.10616438356164383,"(x,y,b)∈Gb
ℓ(fθ(x), y) 
"
PROBLEM FORMULATION,0.1095890410958904,",
(3)"
PROBLEM FORMULATION,0.11301369863013698,"where Gb is a subset of the training data composed of samples drawn from P(x, y|b) and nb is the
size of Gb. The proposed objective aims to minimize the maximum average loss over bias-aligned and"
PROBLEM FORMULATION,0.11643835616438356,"bias-conflicting groups, thereby reducing the reliance of classification models on spurious correlations.
However, to utilize the above objective, we must know the information about bias attributes. In the
next section, we describe a practical method to train the debiased model using Equation (3) without
bias labels."
PROBLEM FORMULATION,0.11986301369863013,"4
DPR: Disagreement Probability based Resampling for debiasing"
PROBLEM FORMULATION,0.1232876712328767,"We present DPR, a resampling method derived from the proposed objective, which does not require
bias annotations during training. First, we provide a walk-through of how the objective in Equation (3)
can be reformulated as a weighted loss minimization problem. Next, we detail a method for calculating
the weight of each training example, along with proxies for both bias-aligned and bias-conflicting
groups. Finally, we provide a full description of our algorithm."
PROBLEM REFORMULATION,0.1267123287671233,"4.1
Problem reformulation"
PROBLEM REFORMULATION,0.13013698630136986,"To utilize the objective in Equation (3) without bias annotations, we reformulate this objective as a
weighted loss minimization problem. For this purpose, we introduce the following assumption:
Assumption 1. Let ba ∈B and bc ∈B represent the bias-aligned and bias-conflicting groups,
respectively. The neural network, parameterized by θ, satisfies that ˆLba < ˆLbc."
PROBLEM REFORMULATION,0.13356164383561644,"In Assumption 1, we assume that the model, parameterized by θ, exhibits a higher average loss on
the bias-conflicting group compared to the bias-aligned group in the training dataset. Under this
assumption, the maximum average loss over groups in Equation (3) can be expressed as follows:"
PROBLEM REFORMULATION,0.136986301369863,"max
b∈B
ˆLb =
1
nbc X"
PROBLEM REFORMULATION,0.1404109589041096,"(x,y,bc)∈Gbc
ℓ(fθ(x), y)
(4)"
PROBLEM REFORMULATION,0.14383561643835616,"=
1
nbc X"
PROBLEM REFORMULATION,0.14726027397260275,"(x,y,b)∈D
p(b = bc|x)ℓ(fθ(x), y)
(5) = 1 n X"
PROBLEM REFORMULATION,0.1506849315068493,"(x,y,b)∈D"
PROBLEM REFORMULATION,0.1541095890410959,p(b = bc|x)
PROBLEM REFORMULATION,0.15753424657534246,"p(b = bc) ℓ(fθ(x), y).
(6)"
PROBLEM REFORMULATION,0.16095890410958905,"When Assumption 1 is satisfied, Equation (4) holds. Given that all samples in Gbc have p(b = bc|x)
equal to 1, whereas all samples in Gba have p(b = bc|x) equal to 0, Equation (5) is derived from
Equation (4). As p(b = bc) is equal to nbc"
PROBLEM REFORMULATION,0.1643835616438356,"n , Equation (6) is obtained. By denoting 1"
PROBLEM REFORMULATION,0.1678082191780822,n · p(b=bc|x)
PROBLEM REFORMULATION,0.17123287671232876,"p(b=bc) as
r(x, y, b), we obtain a weighted loss minimization as follows: min
θ X"
PROBLEM REFORMULATION,0.17465753424657535,"(x,y,b)∈D
r(x, y, b)ℓ(fθ(x), y).
(7)"
PROBLEM REFORMULATION,0.1780821917808219,"Note that the weight r(x, y, b) can be interpreted as the sampling probability."
SAMPLING PROBABILITY WITH GROUP PROXY,0.1815068493150685,"4.2
Sampling probability with group proxy"
SAMPLING PROBABILITY WITH GROUP PROXY,0.18493150684931506,"In order to train the model using Equation (7), it is necessary to compute the sampling probability
r(x, y, b) for each training example. However, directly calculating the probabilities p(b = bc|x) and
p(b = bc) is unfeasible without bias labels. To overcome this limitation, we first introduce proxies
for the bias-aligned and bias-conflicting groups. We then derive substitutes for the probabilities
p(b = bc|x) and p(b = bc) using a biased model."
SAMPLING PROBABILITY WITH GROUP PROXY,0.18835616438356165,"Group proxy.
We focus on the characteristics of the biased model for the group proxies. Following
Nam et al. [34], the biased model fϕ is trained using ERM with the generalized cross-entropy (GCE)
loss [50]:"
SAMPLING PROBABILITY WITH GROUP PROXY,0.1917808219178082,"ℓGCE(fϕ(x), y) = 1 −pϕ(y|x)q"
SAMPLING PROBABILITY WITH GROUP PROXY,0.1952054794520548,"q
,
(8)"
SAMPLING PROBABILITY WITH GROUP PROXY,0.19863013698630136,"where pϕ(y|x) represents the probability assigned to the target label y by the neural network after
a softmax layer, and q ∈(0, 1] is a hyperparameter. The GCE loss amplifies the bias of the model"
SAMPLING PROBABILITY WITH GROUP PROXY,0.20205479452054795,"Algorithm 1: Disagreement Probability based Resampling for debiasing (DPR)
Input: training set D, biased model fϕ, debiased model fθ, learning rate η, the total number of
iterations Tb and Td, calibration parameter τ, GCE parameter q"
SAMPLING PROBABILITY WITH GROUP PROXY,0.2054794520547945,"/* Train the biased model
*/
for t = 1 to Tb do"
SAMPLING PROBABILITY WITH GROUP PROXY,0.2089041095890411,"Sample a mini-batch {(x, y)} from D
Update ϕ by training on a mini-batch using Equation (8)
end"
SAMPLING PROBABILITY WITH GROUP PROXY,0.21232876712328766,"/* Compute the sampling probability
*/
Compute ˆr(x, y) for all (x, y) ∈D following Equation (11)"
SAMPLING PROBABILITY WITH GROUP PROXY,0.21575342465753425,"/* To satisfy Assumption 1
*/
Initialize the debiased model fθ with the biased model fϕ
/* Train the debiased model
*/
for t = 1 to Td do"
SAMPLING PROBABILITY WITH GROUP PROXY,0.2191780821917808,"Sample a mini-batch {(x, y)} from D according to ˆr(x, y)
Update θ by training on a mini-batch using cross-entropy loss
end"
SAMPLING PROBABILITY WITH GROUP PROXY,0.2226027397260274,"by up-weighting the gradient of the cross-entropy loss for samples with high probability pϕ(y|x),
thereby training the model to rely on spurious correlations. Consequently, the biased model tends to
predict correctly for bias-aligned samples and incorrectly for bias-conflicting samples [34]. Building
on this insight, leveraging the predictions ybias of the biased model, we employ the agreement between
the label and the biased model’s prediction (i.e., y = ybias) as a proxy for the bias-aligned group ba,
and the disagreement (i.e., y ̸= ybias) as a proxy for the bias-conflicting group bc."
SAMPLING PROBABILITY WITH GROUP PROXY,0.22602739726027396,"Sampling probability.
We now discuss how to compute the sampling probability r(x, y, b). Using
the group proxies mentioned above, we substitute p(y ̸= ybias|x) for p(b = bc|x). For a given
example (x, y), the disagreement probability p(y ̸= ybias|x) can be computed as follows:"
SAMPLING PROBABILITY WITH GROUP PROXY,0.22945205479452055,"p(y ̸= ybias|x) =
X"
SAMPLING PROBABILITY WITH GROUP PROXY,0.2328767123287671,"ybias
p(y, ybias|x) −p(y = ybias|x) = 1 −pbias(y|x),
(9)"
SAMPLING PROBABILITY WITH GROUP PROXY,0.2363013698630137,where pbias(y|x) = exp(fϕ(x)[y]/τ)
SAMPLING PROBABILITY WITH GROUP PROXY,0.23972602739726026,"Z(ϕ)
is the probability assigned to label y by the biased model, Z(ϕ) =
P"
SAMPLING PROBABILITY WITH GROUP PROXY,0.24315068493150685,"y′ exp(fϕ(x)[y′]/τ) is the partition function, and τ is a temperature hyperparameter. Note that
pbias(y|x) is used to approximate p(b = ba|x). It is crucial for the biased model to accurately capture
the spurious correlation structure; hence, the probabilities of the biased model should be appropriately
calibrated [13, 33, 32]. We also substitute p(y ̸= ybias) for p(b = bc). This probability can be
estimated using all the training data, as in prior works [29, 45]:"
SAMPLING PROBABILITY WITH GROUP PROXY,0.2465753424657534,p(y ̸= ybias) ≈1 n X
SAMPLING PROBABILITY WITH GROUP PROXY,0.25,"(x,y,b)∈D
p(y ̸= ybias|x) = 1 n X"
SAMPLING PROBABILITY WITH GROUP PROXY,0.2534246575342466,"(x,y,b)∈D
(1 −pbias(y|x)) .
(10)"
SAMPLING PROBABILITY WITH GROUP PROXY,0.2568493150684932,"With Equations (9) and (10), we compute"
SAMPLING PROBABILITY WITH GROUP PROXY,0.2602739726027397,"ˆr(x, y) = 1"
SAMPLING PROBABILITY WITH GROUP PROXY,0.2636986301369863,n · p(y ̸= ybias|x)
SAMPLING PROBABILITY WITH GROUP PROXY,0.2671232876712329,"p(y ̸= ybias)
=
1 −pbias(y|x)
P"
SAMPLING PROBABILITY WITH GROUP PROXY,0.2705479452054795,"(x,y,b)∈D (1 −pbias(y|x)).
(11)"
SAMPLING PROBABILITY WITH GROUP PROXY,0.273972602739726,"Instead of r(x, y, b), we use ˆr(x, y) as the sampling probability. We are now ready to train the
debiased model."
TRAINING ALGORITHM,0.2773972602739726,"4.3
Training algorithm"
TRAINING ALGORITHM,0.2808219178082192,"We outline the entire training process for our method as follows. First, we train the biased model
fϕ. Next, we compute the sampling probability ˆr(x, y) using the pretrained biased model. Before
proceeding to train the debiased model fθ, it is worth noting that Equation (7) is derived under
Assumption 1. Thus, it is essential to fulfill Assumption 1 when training the debiased model using"
TRAINING ALGORITHM,0.2842465753424658,"Equation (7). We leverage the characteristics of the biased model for this purpose. The biased model
typically exhibits higher loss on bias-conflicting samples and lower loss on bias-aligned samples,
thereby fulfilling Assumption 1. Consequently, we initialize the model fθ with the biased model
fϕ and then train fθ using training examples sampled with the probability ˆr(x, y). We also employ
data augmentation to enhance the diversity of bias-conflicting samples. Simply oversampling these
samples without enhancing their diversity does not effectively mitigate bias [27]. Therefore, we
enhance the diversity of bias-conflicting samples through data augmentation techniques such as
random color jitter and random rotation. The complete training procedure is outlined in Algorithm 1."
THEORETICAL ANALYSIS,0.2876712328767123,"5
Theoretical analysis"
THEORETICAL ANALYSIS,0.2910958904109589,"In this section, we theoretically demonstrate that DPR minimizes losses for both bias-aligned and
bias-conflicting groups while reducing the disparity between their losses. All proofs are deferred to
Appendix A. Let Lavg be the expected average loss:"
THEORETICAL ANALYSIS,0.2945205479452055,"Lavg := E(x,y)∼P [ℓ(fθ(x), y)].
(12)"
THEORETICAL ANALYSIS,0.2979452054794521,Let Lb be the average loss of group b:
THEORETICAL ANALYSIS,0.3013698630136986,"Lb := E(x,y)∼Pb[ℓ(fθ(x), y)],
(13)"
THEORETICAL ANALYSIS,0.3047945205479452,"where Pb = P(x, y|b) denotes the training distribution conditioned on b, for any b ∈B. In this setting,
we derive the following inequality for the loss gap between the bias-aligned and bias-conflicting
groups.
Theorem 1. Suppose that the loss function ℓ(fθ(x), y) is upper-bounded by a constant C > 0. Given
two distinct groups ba ∈B and bc ∈B such that ba ̸= bc, the following inequality holds with
probability at least 1 −δ, for any δ > 0:"
THEORETICAL ANALYSIS,0.3082191780821918,"|Lba −Lbc| ≤2 · max
b∈B
ˆLb + C · max
b∈B s"
THEORETICAL ANALYSIS,0.3116438356164384,8 log (|B|/δ)
THEORETICAL ANALYSIS,0.3150684931506849,"nb
.
(14)"
THEORETICAL ANALYSIS,0.3184931506849315,"Theorem 1 specifies that the upper bound on the disparity between losses for bias-aligned and bias-
conflicting groups is determined by the maximum average loss over groups and a term dependent
on the size of the smaller group. Additionally, we derive an inequality associated with the expected
average loss.
Theorem 2. In the same setting as Theorem 1, the expected average loss is bounded above with
probability at least 1 −δ:"
THEORETICAL ANALYSIS,0.3219178082191781,"Lavg ≤max
b∈B
ˆLb + C · r"
THEORETICAL ANALYSIS,0.3253424657534247,2 log(1/δ)
THEORETICAL ANALYSIS,0.3287671232876712,"n
.
(15)"
THEORETICAL ANALYSIS,0.3321917808219178,"According to Theorems 1 and 2, our proposed training objective not only closes the loss gap between
bias-aligned and bias-conflicting groups but also reduces the expected average loss. However, in
Equation (14), there remains a loss gap due to a term inversely related to the square root of the size of
the smaller group. Note that DPR is a resampling method derived from the proposed objective when
Assumption 1 is fulfilled, and it identifies and upsamples bias-conflicting samples. Thus, it efficiently
minimizes both terms of the upper bound described in Equation (14). Given that Lavg is expressed as
a weighted sum of Lba and Lbc, these theorems indicate that DPR enhances performance across both
bias-aligned and bias-conflicting groups while reducing the performance gap between them."
EXPERIMENTS,0.3356164383561644,"6
Experiments"
DATASETS,0.339041095890411,"6.1
Datasets"
DATASETS,0.3424657534246575,"We evaluate the debiasing performance of DPR using six benchmark datasets that exhibit spurious cor-
relations. Colored MNIST and Multi-bias MNIST are synthetic datasets designed under the premise
that models learn bias attributes first. Conversely, BAR, BFFHQ, CelebA, and CivilComments-
WILDS are real-world datasets where inherent biases degrade model performance. We follow the
evaluation protocols of previous studies [1, 27, 28] and provide a detailed description of these datasets
in Appendix B.1."
DATASETS,0.3458904109589041,"Colored MNIST.
Colored MNIST (C-MNIST) is a synthetic dataset designed for digit classification,
comprising ten digits, each spuriously correlated with a specific color. Following the protocols in
Ahn et al. [1], we set the ratios of bias-conflicting samples, denoted as ρ, at 0.5%, 1%, and 5% for the
training set, and 90% for the unbiased test set. We report the accuracy on this unbiased test set."
DATASETS,0.3493150684931507,"Multi-bias MNIST.
Multi-bias MNIST (MB-MNIST) [1] is a synthetic dataset designed to in-
corporate more complex patterns compared to C-MNIST and biased-MNIST [41]. MB-MNIST
comprises eight attributes: digit [26], alphabet [7], fashion object [47], Japanese character [6], digit
color, alphabet color, fashion object color, and Japanese character color. The digit shape is the target
attribute, while the other seven serve as bias attributes. In MB-MNIST, bias is introduced by aligning
the digit with each of the other seven attributes, each with a probability of 1 −ρ. In our experiments,
ρ is set to 10%, 20%, and 30% for the training set and 90% for the unbiased test set, as in Ahn et al.
[1]. We report the accuracy on this unbiased test set."
DATASETS,0.3527397260273973,"Biased action recognition.
The biased action recognition (BAR) dataset [34], designed for action
classification, comprises six action classes such as climbing and fishing, each spuriously correlated
with a specific place. The training set of BAR contains only bias-aligned samples, while the test set
consists solely of bias-conflicting samples. We report the accuracy on this bias-conflicting test set."
DATASETS,0.3561643835616438,"Biased FFHQ.
Biased FFHQ (BFFHQ) is a real-world facial dataset, which has age (young or
old) as a label and gender (male or female) as a bias attribute. Predominantly, females are young
and males are old in this dataset. We use a bias-conflicting ratio of ρ = 0.5% in the training set and
report accuracies on both an unbiased test set with ρ = 50% [1] and a bias-conflicting test set with
ρ = 100% [27]."
DATASETS,0.3595890410958904,"CelebA.
CelebA [30] is a dataset for facial classification. The goal is to classify celebrity hair color
as blond or non-blond, which is spuriously correlated with gender. Notably, only a few blond-haired
celebrities are male. Following prior studies [49, 38], we report both average and worst-group
accuracies on the test set, where groups are defined as combinations of class labels and bias labels."
DATASETS,0.363013698630137,"CivilComments-WILDS.
CivilComments-WILDS [5, 25] is a text classification dataset aimed at
identifying whether online comments are toxic or non-toxic. The label is spuriously correlated with
demographic identities such as gender, race, and religion. Following previous works [28, 25, 49],
we report both average and worst-group accuracies on the test set, where groups are defined as
combinations of class labels and bias labels."
EXPERIMENTAL SETUP,0.3664383561643836,"6.2
Experimental setup"
EXPERIMENTAL SETUP,0.3698630136986301,"Baselines.
We compare our method with six baselines on various benchmarks: ERM, JTT [28],
DFA [27], CNC [49], PGD [1], and LC [29]. ERM denotes conventional training without any consid-
erations for debiasing, while the others are debiasing methods that do not require bias annotations
during training."
EXPERIMENTAL SETUP,0.3732876712328767,"Implementation details.
For all datasets except CelebA and CivilComments-WILDS, we follow
the experimental settings of Ahn et al. [1]. Specifically, for CMNIST and MB-MNIST, we employ
two distinct types of simple CNN models, respectively. For BAR and BFFHQ, we utilize a ResNet18
[16] pretrained on ImageNet [9]. In the case of CelebA, we use a pretrained ResNet50, following
the experimental settings of Zhang et al. [49]. For CivilComments-WILDS, we deploy a pretrained
BERT model and follow the experimental setup detailed in Liu et al. [28]. Moreover, we apply
data augmentation techniques—including random resize crop, random color jitter, and random
rotation—for all vision datasets except CelebA, as discussed in Section 4.3. Further details on model
architectures, hyperparameters, and image processing are provided in Appendix B.2."
EXPERIMENTAL RESULTS AND ANALYSIS,0.3767123287671233,"6.3
Experimental results and analysis"
EXPERIMENTAL RESULTS AND ANALYSIS,0.3801369863013699,"Classification accuracy.
Table 1 presents the average accuracies on unbiased test sets for C-MNIST
and MB-MNIST. DPR consistently outperforms or matches the performance of other baselines across
all experiments with varying bias-conflicting ratios. Notably, for the MB-MNIST dataset with a"
EXPERIMENTAL RESULTS AND ANALYSIS,0.3835616438356164,"Table 1: Average accuracies and standard deviations over three trials on two synthetic image datasets,
C-MNIST and MB-MNIST, under varying ratios (%) of bias-conflicting samples. Except for LC, the
results of baselines reported in Ahn et al. [1] are provided. The best performances are highlighted in
bold."
EXPERIMENTAL RESULTS AND ANALYSIS,0.386986301369863,"C-MNIST
MB-MNIST"
EXPERIMENTAL RESULTS AND ANALYSIS,0.3904109589041096,"Ratio (%)
0.5
1
5
10
20
30"
EXPERIMENTAL RESULTS AND ANALYSIS,0.3938356164383562,"ERM
60.94 (0.97)
79.13 (0.73)
95.12 (0.24)
25.23 (1.16)
62.06 (2.45)
87.61 (1.60)
JTT
85.84 (1.32)
95.07 (3.42)
96.56 (1.23)
25.34 (1.45)
68.02 (3.23)
85.44 (3.44)
DFA
94.56 (0.57)
96.87 (0.64)
98.35 (0.20)
25.75 (5.38)
61.62 (2.60)
88.36 (2.06)
PGD
96.88 (0.28)
98.35 (0.12)
98.62 (0.14)
61.38 (4.41)
89.09 (0.97)
90.76 (1.84)
LC
97.25 (0.21)
97.34 (0.16)
97.44 (0.37)
25.86 (8.68)
71.23 (1.71)
89.57 (2.50)
DPR (Ours)
97.52 (0.33)
98.40 (0.03)
98.62 (0.12)
62.21 (4.02)
89.11 (1.65)
94.04 (0.26)"
EXPERIMENTAL RESULTS AND ANALYSIS,0.3972602739726027,"Table 2: Classification accuracies (%) and standard deviations over three trials on four real-world
datasets. Conflicting refers to accuracy on bias-conflicting test sets, while Unbiased indicates accuracy
on unbiased test sets. Average and Worst denote average test accuracy and worst-group accuracy,
respectively. Results of all baselines except LC are taken from Ahn et al. [1], with the exception of
bias-conflicting accuracy on BFFHQ. The best performances are highlighted in bold."
EXPERIMENTAL RESULTS AND ANALYSIS,0.4006849315068493,"BAR
BFFHQ
CelebA
CivilComments-WILDS"
EXPERIMENTAL RESULTS AND ANALYSIS,0.4041095890410959,"Accuracy (%)
Conflicting
Unbiased
Conflicting
Average
Worst
Average
Worst"
EXPERIMENTAL RESULTS AND ANALYSIS,0.4075342465753425,"ERM
63.15 (1.06)
77.77 (0.45)
55.93 (0.64)
94.9 (0.3)
47.7 (2.1)
92.1 (0.4)
58.6 (1.7)
JTT
63.62 (1.33)
77.93 (2.16)
56.13 (0.83)
88.1 (0.3)
81.5 (1.7)
91.1 (-)
69.3 (-)
DFA
64.70 (2.06)
82.77 (1.40)
66.00 (2.00)
-
-
-
-
CNC
-
-
-
89.9 (0.5)
88.8 (0.9)
81.7 (0.5)
68.9 (2.1)
PGD
65.39 (0.47)
84.20 (1.15)
70.07 (2.00)
88.6 (-)
88.8 (-)
92.1 (-)
70.6 (-)
LC
63.45 (2.14)
83.97 (0.83)
70.60 (0.60)
-
88.1 (0.8)
-
70.3 (1.2)
DPR (Ours)
66.11 (3.29)
87.57 (1.22)
76.80 (2.51)
90.7 (0.6)
88.9 (0.6)
82.9 (0.7)
70.9 (1.7)"
EXPERIMENTAL RESULTS AND ANALYSIS,0.410958904109589,"(a) C-MNIST (0.5%)
(b) C-MNIST (1%)
(c) C-MNIST (5%)"
EXPERIMENTAL RESULTS AND ANALYSIS,0.4143835616438356,"Figure 2: Distributions of disagreement probabilities for each sample within bias-aligned and bias-
conflicting groups."
EXPERIMENTAL RESULTS AND ANALYSIS,0.4178082191780822,"ratio of 30%, DPR achieves an unbiased test accuracy of 94.04%, outperforming PGD by 3.28%.
Even for MB-MNIST with a ratio of 10%, where all baselines except PGD fail to achieve higher
accuracy, DPR exhibits the highest accuracy of 62.21%. In the more complex setting of MB-MNIST,
compared to C-MNIST, the effectiveness of DPR is even more pronounced. The superiority of
our method is further demonstrated on real-world benchmarks, as shown in Table 2. Our method
consistently achieves the best performance for each real-world dataset. Specifically, for the BFFHQ
dataset, DPR achieves an accuracy of 87.57% on unbiased test sets, which is 3.37% higher than PGD,
and an accuracy of 76.80% on bias-conflicting test sets, which is 6.20% higher than LC. For the
CelebA and CivilComments-WILDS datasets, our method achieves the highest worst-group accuracy
compared to other baselines, with groups defined as the combination of the target and bias labels. We
especially highlight the results for the BFFHQ benchmark, where our method improves accuracy
on both unbiased and bias-conflicting test sets. This result supports our claim that DPR enhances
performance on both bias-aligned and bias-conflicting groups."
EXPERIMENTAL RESULTS AND ANALYSIS,0.4212328767123288,"Identifying group via disagreement probability.
To discern whether each sample belongs to the
bias-aligned or bias-conflicting group, we check if the target label disagrees with the prediction of
the biased model and use its probability as an up-weight. To evaluate whether the disagreement
probability between the target label and the prediction of the biased model distinguishes bias-
conflicting samples from bias-aligned samples effectively, we plot the distributions of disagreement
probabilities p(y ̸= ybias|x) for each bias-aligned and bias-conflicting sample. The experiment is
conducted on the C-MNIST dataset. As illustrated in Figure 2, bias-aligned samples generally exhibit"
EXPERIMENTAL RESULTS AND ANALYSIS,0.4246575342465753,"Table 3: Ablation studies of the proposed method on the C-MNIST and BFFHQ datasets. We report
the average test accuracies and standard deviations over three trials on unbiased and bias-conflicting
test sets. A checkmark (✓) indicates the case where each component of the proposed method is
applied. The best performances are highlighted in bold."
EXPERIMENTAL RESULTS AND ANALYSIS,0.4280821917808219,"C-MNIST (0.5%)
C-MNIST (5%)
BFFHQ"
EXPERIMENTAL RESULTS AND ANALYSIS,0.4315068493150685,"Initialization
GCE
Augmentation
Unbiased
Unbiased
Unbiased
Conflicting"
EXPERIMENTAL RESULTS AND ANALYSIS,0.4349315068493151,"✗
✗
✗
66.13 (0.51)
95.32 (0.42)
77.07 (1.16)
54.60 (2.43)
✗
✓
✓
66.47 (1.74)
95.21 (0.14)
77.60 (0.79)
55.60 (1.64)
✓
✗
✗
89.06 (0.62)
97.36 (0.29)
78.50 (0.50)
57.47 (0.90)
✓
✓
✗
95.94 (0.45)
97.66 (0.17)
80.93 (1.33)
63.40 (3.67)
✓
✓
✓
97.52 (0.33)
98.62 (0.12)
87.57 (1.22)
76.80 (2.51)"
EXPERIMENTAL RESULTS AND ANALYSIS,0.4383561643835616,"smaller disagreement probabilities compared to bias-conflicting samples. This result demonstrates
that disagreement probability effectively differentiates bias-aligned and bias-conflicting samples.
Moreover, the relatively high disagreement probability associated with bias-conflicting samples
enables DPR to effectively identify and upsample bias-conflicting samples, suggesting that the
disagreement probability p(y ̸= ybias|x) serves as an efficient substitute for the bias-conflicting
probability p(b = bc|x)."
EXPERIMENTAL RESULTS AND ANALYSIS,0.4417808219178082,"Figure 3: Average loss of ran-
domly initialized, pretrained, and
biased models on bias-aligned
and bias-conflicting groups. The
error bars represent the standard
deviations over three trials."
EXPERIMENTAL RESULTS AND ANALYSIS,0.4452054794520548,"Group losses of different models.
As stated earlier in Sec-
tion 4.3, our debiasing objective in Equation (7) should be used
only when Assumption 1 is satisfied. A natural next question
is: which type of model satisfies this assumption? We test three
types of models: a randomly initialized model, a model pretrained
on ImageNet, and a biased model. Specifically, we examine the
average loss of a randomly initialized model on the bias-aligned
and bias-conflicting groups of C-MNIST with a bias-conflicting
ratio of 0.5%. Additionally, we examine the average loss of a
pretrained model on the bias-aligned and bias-conflicting groups
of BFFHQ. We also investigate the average loss of a biased model
on the bias-aligned and bias-conflicting groups of C-MNIST and
BFFHQ. The results, shown in Figure 3, indicate that both the
randomly initialized and pretrained models have similar average
losses on the bias-aligned and bias-conflicting groups. In contrast,
the biased model has a higher average loss on the bias-conflicting
group compared to the bias-aligned group for both C-MNIST and BFFHQ. This result supports the
validity of initializing the debiased model with the biased model."
EXPERIMENTAL RESULTS AND ANALYSIS,0.4486301369863014,"Ablation study.
We conduct an ablation study on model initialization, GCE, and data augmentation.
Model initialization refers to initializing the debiased model with the biased model. The evaluation is
performed on C-MNIST with bias-conflicting ratios of 0.5% and 5%, as well as BFFHQ. Table 3
demonstrates the importance of each component. Comparing the first and second rows, we observe
that GCE and data augmentation bring small improvements when initialization is not used. However,
from rows 3-5, we observe that GCE and augmentation bring significant improvements when
initialization is utilized. For BFFHQ, introducing both GCE and augmentation significantly improves
the average accuracies on unbiased and bias-conflicting sets by 9.07% and 19.33%, respectively.
Furthermore, we observe the performance gap between using and not using initialization. These results
suggest that model initialization is crucial and that GCE and augmentation are also important when
initialization is introduced. The best performances are consistently achieved when all components
are utilized."
CONCLUSIONS AND LIMITATIONS,0.4520547945205479,"7
Conclusions and Limitations"
CONCLUSIONS AND LIMITATIONS,0.4554794520547945,"We present DPR, a resampling method that leverages disagreement probability to identify and
upsample bias-conflicting samples. This method is derived from a novel training objective designed
to achieve consistent performance across both bias-aligned and bias-conflicting groups. It does not
rely on bias annotations and has demonstrated state-of-the-art performance across spurious correlation
benchmarks. However, DPR has certain limitations: its effectiveness depends on how well the biased
model captures the spurious correlation structure, as it uses the predictions of this model to infer"
CONCLUSIONS AND LIMITATIONS,0.4589041095890411,"the group to which each training sample belongs. Moreover, DPR employs a two-stage approach
that complicates the training process and introduces additional hyperparameters. Despite these
limitations, DPR’s simple implementation and strong performance, supported by theoretical analysis
illustrating its ability to reduce loss disparity between groups and minimize average loss, underscore
its usefulness in mitigating reliance on spurious correlations."
CONCLUSIONS AND LIMITATIONS,0.4623287671232877,Acknowledgements
CONCLUSIONS AND LIMITATIONS,0.4657534246575342,"This work is in part supported by the National Research Foundation of Korea (NRF, RS-2024-
00451435(25%), RS-2024-00413957(25%)), Institute of Information & communications Technology
Planning & Evaluation (IITP, 2021-0-01059(15%), 2021-0-00106(20%), 2021-0-00180(15%)) grant
funded by the Ministry of Science and ICT (MSIT), a grant-in-aid of HANHWA SYSTEMS, Institute
of New Media and Communications(INMAC), and the BK21 FOUR program of the Education and
Research Program for Future ICT Pioneers, Seoul National University in 2024."
REFERENCES,0.4691780821917808,References
REFERENCES,0.4726027397260274,"[1] Sumyeong Ahn, Seongyoon Kim, and Se-Young Yun. Mitigating dataset bias by using per-
sample gradient. In The Eleventh International Conference on Learning Representations, 2023.
URL https://openreview.net/forum?id=7mgUec-7GMv."
REFERENCES,0.476027397260274,"[2] Jing An, Lexing Ying, and Yuhua Zhu. Why resampling outperforms reweighting for cor-
recting sampling bias with stochastic gradients. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=iQQK02mxVIT."
REFERENCES,0.4794520547945205,"[3] Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk mini-
mization. arXiv preprint arXiv:1907.02893, 2019."
REFERENCES,0.4828767123287671,"[4] Su Lin Blodgett, Lisa Green, and Brendan O’Connor. Demographic dialectal variation in social
media: A case study of african-american english. arXiv preprint arXiv:1608.08868, 2016."
REFERENCES,0.4863013698630137,"[5] Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced
metrics for measuring unintended bias with real data for text classification. In Companion
proceedings of the 2019 world wide web conference, pages 491–500, 2019."
REFERENCES,0.4897260273972603,"[6] Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and
David Ha. Deep learning for classical japanese literature. arXiv preprint arXiv:1812.01718,
2018."
REFERENCES,0.4931506849315068,"[7] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending
mnist to handwritten letters. In 2017 international joint conference on neural networks (IJCNN),
pages 2921–2926. IEEE, 2017."
REFERENCES,0.4965753424657534,"[8] Elliot Creager, Jörn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant
learning. In International Conference on Machine Learning, pages 2189–2200. PMLR, 2021."
REFERENCES,0.5,"[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition, pages 248–255. Ieee, 2009."
REFERENCES,0.5034246575342466,"[10] Mengnan Du, Subhabrata Mukherjee, Guanchu Wang, Ruixiang Tang, Ahmed Awadallah, and
Xia Hu. Fairness via representation neutralization. Advances in Neural Information Processing
Systems, 34:12091–12103, 2021."
REFERENCES,0.5068493150684932,"[11] John C Duchi, Tatsunori Hashimoto, and Hongseok Namkoong. Distributionally robust losses
against mixture covariate shifts. Under review, 2(1), 2019."
REFERENCES,0.5102739726027398,"[12] Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel,
Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature
Machine Intelligence, 2(11):665–673, 2020."
REFERENCES,0.5136986301369864,"[13] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International conference on machine learning, pages 1321–1330. PMLR, 2017."
REFERENCES,0.5171232876712328,"[14] Zongbo Han, Zhipeng Liang, Fan Yang, Liu Liu, Lanqing Li, Yatao Bian, Peilin Zhao, Bingzhe
Wu, Changqing Zhang, and Jianhua Yao. Umix: Improving importance weighting for subpopu-
lation shift via uncertainty-aware mixup. Advances in Neural Information Processing Systems,
35:37704–37718, 2022."
REFERENCES,0.5205479452054794,"[15] Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness
without demographics in repeated loss minimization. In International Conference on Machine
Learning, pages 1929–1938. PMLR, 2018."
REFERENCES,0.523972602739726,"[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770–778, 2016."
REFERENCES,0.5273972602739726,"[17] Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust super-
vised learning give robust classifiers? In International Conference on Machine Learning, pages
2029–2037. PMLR, 2018."
REFERENCES,0.5308219178082192,"[18] Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data
balancing achieves competitive worst-group-accuracy. In Conference on Causal Learning and
Reasoning, pages 336–351. PMLR, 2022."
REFERENCES,0.5342465753424658,"[19] Sangwon Jung, Sanghyuk Chun, and Taesup Moon. Learning fair classifiers with partially
annotated group labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 10348–10357, 2022."
REFERENCES,0.5376712328767124,"[20] Yeonsung Jung, Hajin Shim, June Yong Yang, and Eunho Yang.
Fighting fire with fire:
contrastive debiasing without bias-free data via generative bias-transformation. In International
Conference on Machine Learning, pages 15435–15450. PMLR, 2023."
REFERENCES,0.541095890410959,"[21] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, pages 4401–4410, 2019."
REFERENCES,0.5445205479452054,"[22] Eungyeup Kim, Jihyeon Lee, and Jaegul Choo. Biaswap: Removing dataset bias with bias-
tailored swapping augmentation. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 14992–15001, 2021."
REFERENCES,0.547945205479452,"[23] Nayeong Kim, Sehyun Hwang, Sungsoo Ahn, Jaesik Park, and Suha Kwak. Learning debiased
classifier with biased committee. Advances in Neural Information Processing Systems, 35:
18403–18415, 2022."
REFERENCES,0.5513698630136986,"[24] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is suffi-
cient for robustness to spurious correlations. In The Eleventh International Conference on Learn-
ing Representations, 2023. URL https://openreview.net/forum?id=Zb6c8A-Fghk."
REFERENCES,0.5547945205479452,"[25] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay
Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds:
A benchmark of in-the-wild distribution shifts. In International conference on machine learning,
pages 5637–5664. PMLR, 2021."
REFERENCES,0.5582191780821918,"[26] Yann LeCun, Corinna Cortes, Chris Burges, et al. Mnist handwritten digit database, 2010."
REFERENCES,0.5616438356164384,"[27] Jungsoo Lee, Eungyeup Kim, Juyoung Lee, Jihyeon Lee, and Jaegul Choo. Learning debi-
ased representation via disentangled feature augmentation. Advances in Neural Information
Processing Systems, 34:25123–25133, 2021."
REFERENCES,0.565068493150685,"[28] Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa,
Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training
group information. In International Conference on Machine Learning, pages 6781–6792.
PMLR, 2021."
REFERENCES,0.5684931506849316,"[29] Sheng Liu, Xu Zhang, Nitesh Sekhar, Yue Wu, Prateek Singhal, and Carlos Fernandez-Granda.
Avoiding spurious correlations via logit correction. In The Eleventh International Confer-
ence on Learning Representations, 2023.
URL https://openreview.net/forum?id=
5BaqCFVh5qL."
REFERENCES,0.571917808219178,"[30] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in
the wild. In Proceedings of the IEEE international conference on computer vision, pages
3730–3738, 2015."
REFERENCES,0.5753424657534246,"[31] R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing
syntactic heuristics in natural language inference. arXiv preprint arXiv:1902.01007, 2019."
REFERENCES,0.5787671232876712,"[32] Aditya Menon, Harikrishna Narasimhan, Shivani Agarwal, and Sanjay Chawla. On the statistical
consistency of algorithms for binary classification under class imbalance. In International
Conference on Machine Learning, pages 603–611. PMLR, 2013."
REFERENCES,0.5821917808219178,"[33] Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help?
Advances in neural information processing systems, 32, 2019."
REFERENCES,0.5856164383561644,"[34] Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure:
De-biasing classifier from biased classifier. Advances in Neural Information Processing Systems,
33:20673–20684, 2020."
REFERENCES,0.589041095890411,"[35] Junhyun Nam, Jaehyung Kim, Jaeho Lee, and Jinwoo Shin. Spread spurious attribute: Improving
worst-group accuracy with spurious attribute estimation. In International Conference on Learn-
ing Representations, 2022. URL https://openreview.net/forum?id=_F9xpOrqyX9."
REFERENCES,0.5924657534246576,"[36] Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Ré. Hidden strat-
ification causes clinically meaningful failures in machine learning for medical imaging. In
Proceedings of the ACM conference on health, inference, and learning, pages 151–159, 2020."
REFERENCES,0.5958904109589042,"[37] Les Perelman. When “the state of the art” is counting words. Assessing Writing, 21:104–111,
2014."
REFERENCES,0.5993150684931506,"[38] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally
robust neural networks. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=ryxGuJrFvS."
REFERENCES,0.6027397260273972,"[39] Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of
why overparameterization exacerbates spurious correlations. In International Conference on
Machine Learning, pages 8346–8356. PMLR, 2020."
REFERENCES,0.6061643835616438,"[40] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Unsupervised learning of debiased repre-
sentations with pseudo-attributes. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 16742–16751, 2022."
REFERENCES,0.6095890410958904,"[41] Robik Shrestha, Kushal Kafle, and Christopher Kanan. An investigation of critical issues in bias
mitigation techniques. In Proceedings of the IEEE/CVF Winter Conference on Applications of
Computer Vision, pages 1943–1954, 2022."
REFERENCES,0.613013698630137,"[42] Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher Ré. No subclass
left behind: Fine-grained robustness in coarse-grained classification problems. Advances in
Neural Information Processing Systems, 33:19339–19352, 2020."
REFERENCES,0.6164383561643836,"[43] Masashi Sugiyama and Amos J Storkey. Mixture regression for covariate shift. In B. Schölkopf,
J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems, volume 19.
MIT Press, 2006. URL https://proceedings.neurips.cc/paper_files/paper/2006/
file/a74c3bae3e13616104c1b25f9da1f11f-Paper.pdf."
REFERENCES,0.6198630136986302,"[44] Damien Teney, Ehsan Abbasnejad, and Anton van den Hengel. Unshuffling data for improved
generalization in visual question answering. In Proceedings of the IEEE/CVF international
conference on computer vision, pages 1417–1427, 2021."
REFERENCES,0.6232876712328768,"[45] Christos Tsirigotis, Joao Monteiro, Pau Rodriguez, David Vazquez, and Aaron C Courville.
Group robust classification without any group information. Advances in Neural Information
Processing Systems, 36, 2024."
REFERENCES,0.6267123287671232,"[46] Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media,
2013."
REFERENCES,0.6301369863013698,"[47] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017."
REFERENCES,0.6335616438356164,"[48] Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and Chelsea Finn. Im-
proving out-of-distribution robustness via selective augmentation. In International Conference
on Machine Learning, pages 25407–25437. PMLR, 2022."
REFERENCES,0.636986301369863,"[49] Michael Zhang, Nimit S Sohoni, Hongyang R Zhang, Chelsea Finn, and Christopher Re.
Correct-n-contrast: a contrastive approach for improving robustness to spurious correlations.
In Proceedings of the 39th International Conference on Machine Learning, volume 162 of
Proceedings of Machine Learning Research, pages 26484–26516. PMLR, 17–23 Jul 2022."
REFERENCES,0.6404109589041096,"[50] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels. Advances in neural information processing systems, 31, 2018."
REFERENCES,0.6438356164383562,"[51] Chenye Zhao, Jasmine Mangat, Sujay Koujalgi, Anna Squicciarini, and Cornelia Caragea.
Privacyalert: A dataset for image privacy prediction. In Proceedings of the International AAAI
Conference on Web and Social Media, volume 16, pages 1352–1361, 2022."
REFERENCES,0.6472602739726028,"[52] Xiao Zhou, Yong Lin, Renjie Pi, Weizhong Zhang, Renzhe Xu, Peng Cui, and Tong Zhang.
Model agnostic sample reweighting for out-of-distribution learning. In International Conference
on Machine Learning, pages 27203–27221. PMLR, 2022."
REFERENCES,0.6506849315068494,"[53] Wei Zhu, Haitian Zheng, Haofu Liao, Weijian Li, and Jiebo Luo. Learning bias-invariant repre-
sentation by cross-sample mutual information minimization. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 15002–15012, 2021."
REFERENCES,0.6541095890410958,Appendix
REFERENCES,0.6575342465753424,"A
Missing proofs from Section 5"
REFERENCES,0.660958904109589,"Proof of Theorem 1. Consider the two groups ba ∈B and bc ∈B such that ba ̸= bc. Let d(ba, bc) be
the difference in expected losses between the group ba and bc:"
REFERENCES,0.6643835616438356,"d(ba, bc) = |Lba −Lbc| .
(16)"
REFERENCES,0.6678082191780822,"By the Hoeffding’s inequality, the following inequality holds with probability at least 1 −δ, for all
groups b ∈B,
Lb −ˆLb
 ≤C · s"
REFERENCES,0.6712328767123288,2 log (|B|/δ)
REFERENCES,0.6746575342465754,"nb
.
(17)"
REFERENCES,0.678082191780822,"Note that the loss ℓ(·) is upper-bounded by some constant C according to our assumption and is
always non-negative. Accordingly, the following holds with probability at least 1 −δ,"
REFERENCES,0.6815068493150684,"d(ba, bc) ≤
 ˆLba −ˆLbc
 + C s"
REFERENCES,0.684931506849315,"2 log (|B|/δ) nba
+ s"
REFERENCES,0.6883561643835616,2 log (|B|/δ) nbc !
REFERENCES,0.6917808219178082,".
(18)"
REFERENCES,0.6952054794520548,"Since the equation max{x, y} = x+y"
REFERENCES,0.6986301369863014,"2
+ |x−y|"
REFERENCES,0.702054794520548,"2
holds for x ∈R and y ∈R, the RHS of Equation (18)
is at most:
 ˆLba −ˆLbc
 ≤2 · max
b∈B
ˆLb.
(19)"
REFERENCES,0.7054794520547946,"Therefore, we have shown that the following result holds,"
REFERENCES,0.708904109589041,"d(ba, bc) ≤2 · max
b∈B
ˆLb + C s"
REFERENCES,0.7123287671232876,"2 log (|B|/δ) nba
+ s"
REFERENCES,0.7157534246575342,2 log (|B|/δ) nbc !
REFERENCES,0.7191780821917808,"≤2 · max
b∈B
ˆLb + C · max
b∈B s"
REFERENCES,0.7226027397260274,8 log (|B|/δ)
REFERENCES,0.726027397260274,"nb
.
(20)"
REFERENCES,0.7294520547945206,This completes the proof of Theorem 1.
REFERENCES,0.7328767123287672,"Proof of Theorem 2. By the Hoeffding’s inequality, the following inequality holds with at least 1 −δ:"
REFERENCES,0.7363013698630136,"Lavg −ˆLavg
 ≤C · r"
REFERENCES,0.7397260273972602,2 log(1/δ)
REFERENCES,0.7431506849315068,"n
.
(21)"
REFERENCES,0.7465753424657534,"Accordingly, the expected average loss is bounded with probability 1 −δ as follows:"
REFERENCES,0.75,ˆLavg −C · r
REFERENCES,0.7534246575342466,2 log(1/δ)
REFERENCES,0.7568493150684932,"n
≤Lavg ≤ˆLavg + C · r"
REFERENCES,0.7602739726027398,2 log(1/δ)
REFERENCES,0.7636986301369864,"n
.
(22)"
REFERENCES,0.7671232876712328,"Considering that the training distribution is represented by a mixture of two group distributions Pba
and Pbc for two distinct groups ba ∈B and bc ∈B, ˆLavg is equal to"
REFERENCES,0.7705479452054794,"kba · ˆLba + kbc · ˆLbc ≤max
b∈B
ˆLb,
(23)"
REFERENCES,0.773972602739726,"where kba = P(ba) and kbc = P(bc) denote the prior probability associated with groups. Therefore,
Lavg is upper-bounded with probability at least 1 −δ:"
REFERENCES,0.7773972602739726,"Lavg ≤max
b∈B
ˆLb + C · r"
REFERENCES,0.7808219178082192,2 log(1/δ)
REFERENCES,0.7842465753424658,"n
.
(24)"
REFERENCES,0.7876712328767124,This completes the proof of Theorem 2.
REFERENCES,0.791095890410959,"B
Experimental details"
REFERENCES,0.7945205479452054,"B.1
Benchmarks"
REFERENCES,0.797945205479452,"We provide a detailed description of the datasets utilized in Section 6. In Figures 4 to 8, each column
corresponds to a distinct class. The images positioned above the dashed line represent bias-aligned
samples, whereas those below represent bias-conflicting samples."
REFERENCES,0.8013698630136986,"Figure 4: Colored MNIST.
Figure 5: Multi-bias MNIST."
REFERENCES,0.8047945205479452,"Figure 6: Biased action recognition.
Figure 7: Biased FFHQ.
Figure 8: CelebA."
REFERENCES,0.8082191780821918,"Colored MNIST.
Colored MNIST (C-MNIST) is a synthetic dataset designed for digit classification
that introduces a controlled spurious correlation between digit shape and color. This dataset consists
of ten digits from the MNIST dataset, each associated with a specific RGB color. Following the
setting in Ahn et al. [1], ten RGB colors are uniformly selected and applied to the grayscale MNIST
images. The image generation involves assigning colors based on the predetermined bias-conflicting
ratio, ρ. Specifically, each image is colored to create a bias-conflicting sample with probability ρ,
and a bias-aligned sample otherwise. For bias-aligned samples of a given class y, the digit is colored
using c ∼N(Cy, σI3×3), where Cy ∈R3 is the predefined color vector for class y. Conversely,
bias-conflicting samples receive a color from any class other than y, i.e. CUy ∈{Ci}i∈[10]\y, and
colored using c ∼N(CUy, σI3×3). The experiments utilize bias-conflicting ratios of 0.5%, 1%, and
5%, with σ set to 0.0001. Additionally, we use a 10% of training data as validation data, and an
unbiased test set with a bias-conflicting ratio of 90% is employed for performance evaluation."
REFERENCES,0.8116438356164384,"Multi-bias MNIST.
Multi-bias MNIST (MB-MNIST) is a synthetic dataset derived from Ahn
et al. [1] to incorporate more complex patterns compared to C-MNIST and biased-MNIST [41]. MB-
MNIST comprises eight attributes: digit [26], alphabet [7], fashion object [47], Japanese character
[6], digit color, alphabet color, fashion object color, and Japanese character color. The digit shape is
the target attribute, while the other seven attributes serve as bias attributes. In MB-MNIST, bias is
introduced by aligning the digit and its color with probability 1 −ρ. Similarly, the other six attributes
are also independently aligned with the digit, each with probability 1 −ρ. In our setting, the ratios of
bias-conflicting samples are 10%, 20%, and 30% as in Ahn et al. [1]. As CMNIST, we use 10% of
the training data as validation data. Test samples are generated with a bias-conflicting ratio of 90%
for all bias attributes to create an unbiased test set."
REFERENCES,0.815068493150685,"Biased action recognition.
The biased action recognition (BAR) dataset [34], designed for action
classification, is a real-world dataset that encompasses six action classes: climbing, diving, fishing,
racing, throwing, and vaulting. Each class is spuriously correlated with a specific place. The training
set of BAR consists exclusively of bias-aligned samples, while the test set consists solely of bias-
conflicting samples. For example, all the training samples for the climbing class are associated with
rock walls, but the test samples for climbing are associated with different settings, such as ice cliffs."
REFERENCES,0.8184931506849316,"We utilize the data splits defined in Ahn et al. [1], which allocate 10% of the training set to the
validation set."
REFERENCES,0.821917808219178,"Biased FFHQ.
Biased FFHQ (BFFHQ) was constructed from the real-world facial dataset FFHQ
[21], which has age (young or old) as a label and gender (male or female) as a bias feature. This
benchmark was conducted in previous works [27, 1, 22]. Individuals aged between 10 and 29 are
labeled as “young”, while those aged between 40 and 59 are labeled as “old”. In this dataset, the
majority of females are young, while males are predominantly old. The training set contains only 0.5%
bias-conflicting samples. We report accuracies on both an unbiased test set with a bias-conflicting
ratio of 50% [1] and a bias-conflicting test set [27]."
REFERENCES,0.8253424657534246,"CelebA.
CelebA [30] is a real-world dataset for face classification. Following Sagawa et al. [38],
the goal is to classify celebrities’ hair color as either blond or non-blond, which exhibits a spurious
correlation with gender (male or female). Notably, only a small fraction of blond-haired celebrities
are male, which leads to poor performance of ERM-trained models on this group. We employ the
training, validation, and test splits as specified in Sagawa et al. [38] and report both the average
accuracy and the worst-group accuracy on the test set. In this context, the group is defined as the
combination of the class label and the bias label as in prior studies [49, 29, 38]."
REFERENCES,0.8287671232876712,"CivilComments-WILDS.
CivilComments-WILDS [5, 25] is a dataset for text classification prob-
lems. The goal is to classify whether an online comment is toxic or non-toxic, which exhibits a
spurious correlation with the mention of certain demographic identities, including male, female, white,
black, LGBTQ, Christian, Muslim, and other religions. We use the same data splits as described in
Koh et al. [25] and report both the average accuracy and the worst-group accuracy on the test dataset.
Here, groups are defined as combinations of class labels and bias labels, as described in previous
works [28, 25, 49]."
REFERENCES,0.8321917808219178,"B.2
Implementation details"
REFERENCES,0.8356164383561644,"We provide descriptions of the implementation details. For CelebA and CivilComments-WILDS, we
follow the experimental settings outlined in Zhang et al. [49] and Liu et al. [28], respectively. For
the other datasets—CMNIST, MB-MNIST, BAR, and BFFHQ—we follow the experimental setups
presented in Ahn et al. [1]. The hyperparameter for GCE, denoted as q, is set to 0.7 as in Zhang and
Sabuncu [50]. All classification models are trained using an NVIDIA RTX A6000."
REFERENCES,0.839041095890411,"B.2.1
Model architectures and hyperparameters"
REFERENCES,0.8424657534246576,The model architectures and hyperparameters for each dataset are described:
REFERENCES,0.8458904109589042,"C-MNIST.
As in Ahn et al. [1], we utilize a simple CNN architecture (SimConv-1). Please see
Appendix B of Ahn et al. [1] for detailed network implementation. We train the model for 100 epochs
with SGD optimizer, a batch size of 128, a learning rate of 0.02, weight decay of 0.001, momentum
of 0.9, and learning rate decay of 0.1 at every 40 steps. For C-MNIST with bias-conflicting ratios of
0.5% and 1%, we use a temperature of 1; for a ratio of 5%, we use a temperature of 1.1."
REFERENCES,0.8493150684931506,"MB-MNIST.
We use a different type of simple CNN model (SimConv-2), following Ahn et al. [1].
Please refer to Appendix B in Ahn et al. [1] for network implementation. We train for 100 epochs
with SGD optimizer, a batch size of 32, a learning rate of 0.01, and weight decay of 1e-4, momentum
of 0.9. For the MB-MNIST dataset, we use temperatures of 0.9, 1.1, and 1.3 for the ratios of 10%,
20%, and 30%, respectively."
REFERENCES,0.8527397260273972,"BAR.
We use a ResNet18 pretrained on ImageNet as in Kim et al. [22]. We train for 100 epochs
with SGD optimizer, a batch size of 16, a learning rate of 5e-4, weight decay of 1e-5, momentum of
0.9, learning rate decay of 0.1 at every 20 steps, and a temperature of 0.6."
REFERENCES,0.8561643835616438,"BFFHQ.
We utilize an ImageNet-pretrained ResNet18 as in Lee et al. [27]. We train for 160 epochs
with Adam optimizer, a batch size of 64, a learning rate of 1e-4, weight decay of 0, learning rate
decay of 0.1 at every 32 steps, and a temperature of 0.9."
REFERENCES,0.8595890410958904,"CelebA.
We utilize a ImageNet-pretrained ResNet50 and hyperparameters from Zhang et al. [49].
We train for 50 epochs with SGD optimizer, a batch size of 128, a learning rate of 1e-4, weight decay
of 0.1, and a temperature of 1."
REFERENCES,0.863013698630137,"CivilComments-WILDS.
We utilize a pretrained BERT with the number of tokens capped at 300
following previous works [49, 28, 1, 25]. We train the biased model using the SGD optimizer without
gradient clipping. In contrast, for training the debiased model, we employ the AdamW optimizer with
gradient clipping and a linearly-decaying learning rate. Both models are trained for up to 5 epochs
with a batch size of 16, a learning rate of 1e-5, weight decay of 0.01, and a temperature of 2."
REFERENCES,0.8664383561643836,"B.2.2
Image preprocessing"
REFERENCES,0.8698630136986302,"We employ the same image preprocessing scheme as presented in Ahn et al. [1]. For CMNIST and
MB-MNIST, we use fixed sizes of (28×28) and (56×56), respectively. For the remaining datasets,
we use a fixed size of (224×224). Additionally, we normalize images from BAR and BFFHQ with a
mean of (0.4914, 0.4822, 0.4465) and a standard deviation of (0.2023, 0.1994, 0.2010). For vision
datasets except for CelebA, we apply random resize crop, random color jitter, and random rotation
to increase the diversity of bias-conflicting samples. To ensure a fair comparison, the same data
augmentation techniques are applied to both the baselines and the proposed DPR."
REFERENCES,0.8732876712328768,"C
Additional ablation study"
REFERENCES,0.8767123287671232,"C.1
Ablation study on q of GCE"
REFERENCES,0.8801369863013698,Table 4: Ablation study on q of GCE.
REFERENCES,0.8835616438356164,"C-MNIST (0.5%)
MB-MNIST (30%)"
REFERENCES,0.886986301369863,"Biased model
Debiased model
Biased model
Debiased model"
REFERENCES,0.8904109589041096,"q = 0.3
26.27 (2.11)
95.53 (0.74)
79.73 (0.91)
95.32 (0.71)
q = 0.5
21.30 (3.10)
96.83 (0.32)
83.16 (0.56)
94.84 (0.15)
q = 0.7
18.55 (2.91)
97.52 (0.33)
85.47 (1.26)
94.04 (0.26)
q = 0.9
16.19 (0.46)
97.66 (0.06)
85.26 (0.70)
93.77 (0.19)"
REFERENCES,0.8938356164383562,"We conduct an ablation study on the C-MNIST and MB-MNIST datasets, which have bias-conflicting
ratios of 0.5% and 30%, respectively, to assess the impact of GCE parameter q. As depicted in
Table 4, varying q demonstrates distinct effects on the performance of biased and debiased models.
For C-MNIST, increasing q enhances the debiased model’s performance while degrading that of the
biased model. In contrast, for MB-MNIST, decreasing q enhances the performance of the debiased
model but worsens the performance of the biased model. These results suggest that the optimal
setting of q varies between datasets. Additionally, a consistent observation from both datasets is that
as the performance of the biased model decreases, the performance of the debiased model increases.
This trend implies that a lower performance of the biased model, indicative of a stronger focus on
spurious correlations, leads to more accurate group proxies, which in turn boosts the debiased model’s
performance."
REFERENCES,0.8972602739726028,"C.2
Ablation study on calibration hyperparameter τ"
REFERENCES,0.9006849315068494,Table 5: Ablation study on calibration hyperparameter τ.
REFERENCES,0.9041095890410958,"C-MNIST
MB-MNIST"
REFERENCES,0.9075342465753424,"Ratio (%)
0.5
5
20
30"
REFERENCES,0.910958904109589,"τ = 0.9
97.48 (0.41)
97.96 (0.28)
88.10 (1.78)
91.53 (0.87)
τ = 1.0
97.52 (0.33)
98.38 (0.16)
88.68 (1.69)
92.85 (0.65)
τ = 1.1
96.48 (0.16)
98.62 (0.12)
89.11 (1.65)
92.99 (0.66)
τ = 1.2
95.84 (0.46)
98.40 (0.18)
87.80 (2.49)
93.49 (0.47)
τ = 1.3
95.07 (0.36)
98.42 (0.02)
87.73 (1.53)
94.04 (0.26)"
REFERENCES,0.9143835616438356,"Since DPR utilizes the calibration hyperparameter τ for capturing the spurious correlation structure
well, we conduct an ablation study on C-MNIST and MB-MNIST across various bias-conflicting
ratios to assess the effect of τ. Table 5 illustrates how performance varies with different settings of τ.
For C-MNIST, the best performances are achieved at τ = 1 and τ = 1.1 for bias-conflicting ratios of"
REFERENCES,0.9178082191780822,"0.5% and 5%, respectively. For MB-MNIST, the best results are obtained with τ = 1.1 and τ = 1.3
for ratios of 20% and 30%, respectively. The optimal τ can vary not only between different datasets
but also according to the extent of the prevalence of spurious correlations within the datasets. These
results suggest that adjusting τ is effective in capturing the diverse spurious correlation structures."
REFERENCES,0.9212328767123288,"C.3
Comparison of resampling and reweighting"
REFERENCES,0.9246575342465754,Table 6: Comparison of resampling and reweighting.
REFERENCES,0.928082191780822,C-MNIST
REFERENCES,0.9315068493150684,"Ratio (%)
0.5
1
5"
REFERENCES,0.934931506849315,"Resampling
97.52 (0.33)
98.40 (0.03)
98.62 (0.12)
Reweighting
95.04 (0.35)
96.21 (0.45)
97.84 (0.50)"
REFERENCES,0.9383561643835616,"While reweighting could effectively solve the weighted loss minimization problem presented in
Equation (7), DPR adopts resampling. To justify this choice, we conduct experiments to compare
resampling with reweighting. Table 6 shows the performance of each method on the C-MNIST dataset
across various bias-conflicting ratios. The results reveal that DPR using the resampling strategy
consistently outperforms DPR using the reweighting approach. This superiority of resampling over
reweighting has been explored and explained by An et al. [2]."
REFERENCES,0.9417808219178082,NeurIPS Paper Checklist
CLAIMS,0.9452054794520548,1. Claims
CLAIMS,0.9486301369863014,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
2. Limitations"
CLAIMS,0.952054794520548,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
3. Theory Assumptions and Proofs"
CLAIMS,0.9554794520547946,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
4. Experimental Result Reproducibility"
CLAIMS,0.958904109589041,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
5. Open access to data and code"
CLAIMS,0.9623287671232876,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
6. Experimental Setting/Details"
CLAIMS,0.9657534246575342,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
7. Experiment Statistical Significance"
CLAIMS,0.9691780821917808,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
8. Experiments Compute Resources"
CLAIMS,0.9726027397260274,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
9. Code Of Ethics"
CLAIMS,0.976027397260274,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
10. Broader Impacts"
CLAIMS,0.9794520547945206,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
11. Safeguards"
CLAIMS,0.9828767123287672,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
CLAIMS,0.9863013698630136,"Answer: [NA]
12. Licenses for existing assets"
CLAIMS,0.9897260273972602,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
13. New Assets"
CLAIMS,0.9931506849315068,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
14. Crowdsourcing and Research with Human Subjects"
CLAIMS,0.9965753424657534,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]"
