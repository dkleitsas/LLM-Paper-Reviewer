Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002785515320334262,"The issue of missing data in supervised learning has been largely overlooked, es-
pecially in the deep learning community. We investigate strategies to adapt neural
architectures for handling missing values. Here, we focus on regression and clas-
siﬁcation problems where the features are assumed to be missing at random. Of
particular interest are schemes that allow reusing as-is a neural discriminative ar-
chitecture. To address supervised deep learning with missing values, we propose
to marginalize over missing values in a joint model of covariates and outcomes.
Thereby, we leverage both the ﬂexibility of deep generative models to describe
the distribution of the covariates and the power of purely discriminative models
to make predictions. More precisely, a deep latent variable model can be learned
jointly with the discriminative model, using importance-weighted variational in-
ference, essentially using importance sampling to mimick averaging over multi-
ple imputations. In low-capacity regimes, or when the discriminative model has a
strong inductive bias, we ﬁnd that our hybrid generative/discriminative approach
generally outperforms single imputations methods."
INTRODUCTION,0.005571030640668524,"1
INTRODUCTION"
INTRODUCTION,0.008356545961002786,zk ∼qγ(z|xobs)
INTRODUCTION,0.011142061281337047,gγ(xobs)
INTRODUCTION,0.013927576601671309,hθ(zk)
INTRODUCTION,0.016713091922005572,"xmiss
k
∼pθ(xmiss|zk)"
INTRODUCTION,0.019498607242339833,"fφ(xobs, xmiss
k
)"
INTRODUCTION,0.022284122562674095,pφ(y|xobs) z x y θ φ γ N
INTRODUCTION,0.025069637883008356,"Figure 1: Left: supMIWAE computational struc-
ture: encoding network gγ, decoding network hθ
and discriminative network fφ. Right: Graphi-
cal model: supMIWAE seen as a VAE for missing
values concatenated with neural discriminator."
INTRODUCTION,0.027855153203342618,"Missing data affect data analysis across a wide
range of domains and the sources of missing
values span an equally wide range. Recently,
deep latent variable models (DLVMs, Kingma
& Welling, 2014; Rezende et al., 2014) have
been applied to missing data problems in an un-
supervised setting (e.g. Rezende et al., 2014;
Nazabal et al., 2020; Ma et al., 2018; 2019;
Ivanov et al., 2019; Mattei & Frellsen, 2018;
2019; Yoon et al., 2018; Li et al., 2019; Ipsen
et al., 2021; Ghalebikesabi et al., 2021), while
the supervised setting has not seen the same at-
tention. The progress in the unsupervised set-
ting is focused on inference and imputation and
can therefore be useful as an imputation step
before learning a discriminative model. Tradi-
tionally the focus has also been on inference
and imputation either as a goal in itself or be-
fore supervised learning on the (possibly mul-
tiple) imputations and observed data (Rubin,
1976; 1996; Little & Rubin, 2019)."
INTRODUCTION,0.03064066852367688,"Supervised learning in the presence of miss-
ing values has different goals and pose different
challenges than inference and imputation (Josse"
INTRODUCTION,0.033426183844011144,"∗Department of Applied Mathematics and Computer Science, Technical University of Denmark, Denmark
†Universit´e Cˆote d’Azur, Inria (Maasai team), Laboratoire J.A. Dieudonn´e, CNRS, France
‡Equal contribution"
INTRODUCTION,0.036211699164345405,Published as a conference paper at ICLR 2022
INTRODUCTION,0.03899721448467967,"et al., 2019; Le Morvan et al., 2020a; 2021). Here the overall aim is to minimize an expected pre-
diction error. However, optimal single imputation does not necessarily lead to optimal prediction in
terms of minimizing a prediction error (Josse et al., 2019). One challenge is that predictions based
on inputs with missing values can be ambiguous, that is, the conditional distribution of the miss-
ing values given the observed may be multimodal and the optimal prediction may change with the
mode. With single imputation, the conditional distribution over the missing data is discarded, and
the optimal single imputation can no longer reﬂect this ambiguity, see ﬁgure 2. Even the optimal
single imputation leads to biased parameter estimates compared to the fully observed case (Bertsi-
mas et al., 2021), and may in some cases lie outside the distribution of the data. Another challenge
is that the number of missing value patterns grows combinatorially with the number of features p,
requiring 2p submodels to ﬁt the Bayes predictor in the linear case (Le Morvan et al., 2020a). In
multiple imputation (Rubin, 1996), several imputations are drawn from the posterior predictive dis-
tribution of the missing values, reﬂecting the full conditional distribution of the missing values and
thus the uncertainty about what is missing. This allows for uncertainty estimates in downstream
tasks such as prediction. This in turn requires ﬁtting as many discriminative models as the number
of imputations."
CONTRIBUTIONS,0.04178272980501393,"1.1
CONTRIBUTIONS"
CONTRIBUTIONS,0.04456824512534819,"In order to address supervised deep learning with missing values we develop the supervised missing
data importance-weighted autoencoder (supMIWAE) bound, based on the approximate maximum
likehood techniques used by Burda et al. (2016) and Mattei & Frellsen (2019). This is a scalable
approach to marginalizing over missing values in a joint model of covariates and outcomes"
CONTRIBUTIONS,0.04735376044568245,"pφ,θ(y, xobs, xmiss) = pφ(y|xobs, xmiss)pθ(xobs, xmiss).
(1)"
CONTRIBUTIONS,0.05013927576601671,"The covariate model pθ(xobs, xmiss) =
R
p(xobs, xmiss|z)p(z) dz is a DLVM in this work, but
can be any probabilistic model imposing a joint density over covariates.
The outcome model
pφ(y|xobs, xmiss) is any neural discriminative architecture that would have been used in the
complete-data case, parameterizing a density over the outcomes. The graphical model is shown
in ﬁgure 1 along with its computational structure."
CONTRIBUTIONS,0.052924791086350974,"Once the joint model has been trained using the supMIWAE bound, the conditional distribution of
the outcome given the observed parts of the input can be found as"
CONTRIBUTIONS,0.055710306406685235,"pφ(y|xobs) =
Z
pφ(y|xobs, xmiss)pθ(xmiss|xobs) dxmiss.
(2)"
CONTRIBUTIONS,0.0584958217270195,"This is approximated using importance sampling techniques, averaging predictions over multiple
importance samples (akin to multiple imputations, see appendix C for a deeper discussion) from the
generative model."
CONTRIBUTIONS,0.06128133704735376,"The model can be trained end-to-end or a pretrained generative model can be coupled with a dis-
criminative model. Having a joint model allows for supervised imputations, where available labels
can help guide the imputations from the generative model by adjusting the importance weights ac-
cordingly."
CONTRIBUTIONS,0.06406685236768803,"Joint models over covariates and labels have previously been used to marginalize over missing values
in the covariates, using Gaussian mixture models either as the model over the covariates or directly as
the joint model over covariates and outcomes (Ghahramani & Jordan, 1995; Ghahramani & Hinton,
1996; Ahmad & Tresp, 1993; Tresp et al., 1994; 1995; ´Smieja et al., 2018). Our approach shows
how to use more ﬂexible generative models, such as DLVMs, provides an efﬁcient optimization
procedure and allows for keeping any deep discriminative architecture unchanged."
BACKGROUND AND NOTATION,0.06685236768802229,"2
BACKGROUND AND NOTATION"
BACKGROUND AND NOTATION,0.06963788300835655,"We deﬁne the random variable x = (x1, . . . , xp) ∈X which takes values in a p-dimensional feature
space X = X1 × · · · × Xp. There is a corresponding (possibly vector valued) response variable
y ∈Y. A missing process obscures parts of x resulting in the mask random variable s ∈{0, 1}p
where sj ="
BACKGROUND AND NOTATION,0.07242339832869081,"(
1
if xj observed,
0
if xj missing.
(3)"
BACKGROUND AND NOTATION,0.07520891364902507,Published as a conference paper at ICLR 2022
BACKGROUND AND NOTATION,0.07799442896935933,"Imputation
supMIWAE"
BACKGROUND AND NOTATION,0.0807799442896936,"Figure 2: Left: Kernel density of a two-class dataset (the burger dataset), where and y = 0 is blue
and y = 1 is brown/orange. Observations with missing values are shown as sticks in the bottom
margin of the plots and colored according to their class label. For observations with missing 2nd
coordinate there is an inherent ambiguity about the class with p(y = 1|xobs) = 2/3. The optimal
single imputation is shown as the dashed black line. Middle: Decision surface for a neural classiﬁer
trained on optimally-imputed data. The decision surface has to reﬂect that for the imputed data
p(y = 1|ι0(xobs)) = 2/3, thus the increased class y = 1 probability around the optimal imputations
(here ι0 is the imputation function, see appendix A.1). Right: Decision surface as learnt by the
supMIWAE. It is similar to the decision surface that would be found in the absence of missing data."
BACKGROUND AND NOTATION,0.08356545961002786,"We let obs(s) denote the indices of the non-zero entries of s and miss(s) denote the indices of the
zero-entries of s, such that xobs(s) is the set of observed elements of x, and xmiss(s) is the set of
missing elements of x. For simplicity we will omit the s and write xobs, xmiss respectively, whenever
the context is clear."
BACKGROUND AND NOTATION,0.08635097493036212,"Following (Yoon et al., 2018; Le Morvan et al., 2020b; Ghalebikesabi et al., 2021) we deﬁne the
incomplete random variable ˜x = (˜x1, . . . , ˜xp) ∈˜
X which takes values in ˜
X = (X1 ∪{na})×· · ·×
(Xp ∪{na}) where missing values are represented by the symbol na, such that na · xj = na and
na · 0 = 0. We typically only have access to ˜x, where
˜x = x ⊙s + na ⊙(1 −s)
(4)
and ⊙is the Hadamard product. Finally, we are given n i.i.d. copies of the random variables ˜x and
y which we collect in a dataset D = {˜xi, yi}n
i=1, or alternatively D =

xobs
i , yi
	n
i=1."
BACKGROUND AND NOTATION,0.08913649025069638,"We make the additional assumption that the data are missing at random (MAR, see e.g. Seaman
et al., 2013; Little & Rubin, 2019). Speciﬁcally, this means that we assume that s and xmiss are
independent given xobs. This assumption allows to avoid to explicitly model the missing mechanism.
Our approach could be extended beyond MAR by using a generative model ﬁt for this purpose, like
the not-MIWAE of Ipsen et al. (2021), or the deep pattern-set mixture of Ghalebikesabi et al. (2021)."
CHALLENGES WHEN TRAINING DISRIMINATIVE MODELS WITH MISSING DATA,0.09192200557103064,"2.1
CHALLENGES WHEN TRAINING DISRIMINATIVE MODELS WITH MISSING DATA"
CHALLENGES WHEN TRAINING DISRIMINATIVE MODELS WITH MISSING DATA,0.0947075208913649,"Our predictive model will be deﬁned through a mapping fφ : X →H used to parameterize a
conditional distribution
pφ(y|x) = Ψ(y|fφ(x)).
(5)
Here (Ψ(·|η))η∈H is a parametric family of distributions over the outcome space Y, such as a Gaus-
sian distribution in regression or a categorical distribution in classiﬁcation. Under the maximum-
likelihood framework (or equivalently the logarithmic scoring rule), an optimal mapping f ∗
φ within
a class F = (fφ)φ∈Φ parameterized by φ ∈Φ is deﬁned as"
CHALLENGES WHEN TRAINING DISRIMINATIVE MODELS WITH MISSING DATA,0.09749303621169916,"f ∗
φ ∈arg max
fφ∈F
Ep∗(x,y)

log Ψ(y|fφ(x))

,
(6)"
CHALLENGES WHEN TRAINING DISRIMINATIVE MODELS WITH MISSING DATA,0.10027855153203342,"where p∗(x, y) is the true data generating distribution. When the data are complete, this is typically
approximated by maximizing the log likelihood of the parameters φ given the data Dtrain,"
CHALLENGES WHEN TRAINING DISRIMINATIVE MODELS WITH MISSING DATA,0.10306406685236769,"ℓ(φ) =
X"
CHALLENGES WHEN TRAINING DISRIMINATIVE MODELS WITH MISSING DATA,0.10584958217270195,"(x,y)∈Dtrain
log pφ(y|x).
(7)"
CHALLENGES WHEN TRAINING DISRIMINATIVE MODELS WITH MISSING DATA,0.10863509749303621,Published as a conference paper at ICLR 2022
CHALLENGES WHEN TRAINING DISRIMINATIVE MODELS WITH MISSING DATA,0.11142061281337047,"When the covariates have missing values the log-likelihood cannot be maximized directly as the
likelihood is not deﬁned since pφ(y|x) depends on the full input vector. The simplest approach
is instead to learn separate mappings for each missing pattern. This reduces the amount of data
available for the training of each mapping and does not scale well with the dimensionality of the
input as in general 2p networks need to be trained. An often used approach is instead to impute the
missing values using a single imputation (for instance, using Epθ(xmiss|xobs)[xmiss] or an approximation
of it), and then map the concatenation of observed and imputed values to approximate the conditional
distribution"
CHALLENGES WHEN TRAINING DISRIMINATIVE MODELS WITH MISSING DATA,0.11420612813370473,"pφ(y|xobs) ≈pφ(y|x = (xobs, Epθ(xmiss|xobs)[xmiss])) = Ψ(y|fφ(xobs, Epθ(xmiss|xobs)[xmiss])).
(8)"
CHALLENGES WHEN TRAINING DISRIMINATIVE MODELS WITH MISSING DATA,0.116991643454039,"Instead of Epθ(xmiss|xobs)[xmiss], which is the optimal imputation under the mean squared error, one
could use any kind of imputation, for instance using a constant, or the mean."
CHALLENGES WHEN TRAINING DISRIMINATIVE MODELS WITH MISSING DATA,0.11977715877437325,"This general approach is adequately called impute-then-regress by Bertsimas et al. (2021) and Le
Morvan et al. (2021). While it will be Bayes-consistent given a powerful enough classiﬁer (Le Mor-
van et al., 2021), it leads to biased parameter estimates compared to having complete data (Bertsimas
et al., 2021). This is illustrated in ﬁgure 2 where the optimal (under the mean squared error) sin-
gle imputations are placed entirely within one of the classes, requiring a biased decision surface
to properly reﬂect the class label probabilities for the records with missing values. From a prob-
abilistic perspective, computing pφ(y|xobs) requires marginalizing over the missing features as in
equation (1), that is
pφ(y|xobs) = Epθ(xmiss|xobs)[pφ(y|xobs, xmiss)].
(9)"
CHALLENGES WHEN TRAINING DISRIMINATIVE MODELS WITH MISSING DATA,0.12256267409470752,"Notice the difference to single imputation since, in general"
CHALLENGES WHEN TRAINING DISRIMINATIVE MODELS WITH MISSING DATA,0.12534818941504178,"Epθ(xmiss|xobs)[pφ(y|xobs, xmiss)] ̸= pφ(y|x = (xobs, Epθ(xmiss|xobs)[xmiss])),
(10)"
CHALLENGES WHEN TRAINING DISRIMINATIVE MODELS WITH MISSING DATA,0.12813370473537605,"except in some pathological cases (e.g. when pφ is linear or pθ is a Dirac distribution). Therefore,
if the discriminative model is nonlinear and the generative model is complex enough, then using
single imputation will give a very different result than marginalising over the missing features. This
is exempliﬁed in ﬁgure 2, where a classiﬁer trained using the optimal imputation ﬁnds a solution
that is very different to what would be found by one trained without missing data. Our technique,
marginalizing the missing values, allows to recover the same decision surface as a classiﬁer trained
without missing data."
OUR JOINT MODEL AND ITS SUPMIWAE VARIATIONAL BOUND,0.1309192200557103,"2.2
OUR JOINT MODEL AND ITS SUPMIWAE VARIATIONAL BOUND"
OUR JOINT MODEL AND ITS SUPMIWAE VARIATIONAL BOUND,0.13370473537604458,"As mentioned in the introduction, we posit a joint model over covariates and outcome, that will
make use of a latent variable z:"
OUR JOINT MODEL AND ITS SUPMIWAE VARIATIONAL BOUND,0.13649025069637882,"pφ,θ(y, xobs, xmiss, z) = pφ(y|xobs, xmiss)pθ(xobs, xmiss|z)pθ(z).
(11)"
OUR JOINT MODEL AND ITS SUPMIWAE VARIATIONAL BOUND,0.1392757660167131,"Under the MAR assumption, the relevant substitute of the likelihood is the likelihood of the observed
data"
OUR JOINT MODEL AND ITS SUPMIWAE VARIATIONAL BOUND,0.14206128133704735,"pφ,θ(y, xobs) =
Z
pφ(y|xobs, xmiss)pθ(xobs, xmiss|z)pθ(z) dz dxmiss.
(12)"
OUR JOINT MODEL AND ITS SUPMIWAE VARIATIONAL BOUND,0.14484679665738162,"The integral in equation (12) is in all but the simplest cases analytically intractable and direct max-
imum likelihood methods for learning the parameters (θ, φ) cannot be used. In order to learn
the parameters of the joint model, we turn to amortized importance weighted variational infer-
ence, where a lower bound is maximized instead of the log likelihood itself (Burda et al., 2016;
Domke & Sheldon, 2018). It is based on the fact that unbiased estimates of a likelihood can be
turned into lower bounds of the log-likelihood. Indeed, if R(xobs, y) is a random variable such that
E

R(xobs, y)

= pθ,φ(xobs, y), then E

log R(xobs, y)

≤log E

R(xobs, y)

= log pθ,φ(xobs, y),
which means that E

log R(xobs, y)

is a lower bound of the log likelihood. A simple way to ﬁnd
a suitable R(xobs, y) is using importance sampling from a variational distribution over z. More
speciﬁcally, let"
OUR JOINT MODEL AND ITS SUPMIWAE VARIATIONAL BOUND,0.14763231197771587,"RK(xobs, y) = 1 K K
X k=1"
OUR JOINT MODEL AND ITS SUPMIWAE VARIATIONAL BOUND,0.15041782729805014,"pφ(y|xobs, xmiss
k
)pθ(xobs|zk)pθ(zk)
qγ(zk|xobs)
,
(13)"
OUR JOINT MODEL AND ITS SUPMIWAE VARIATIONAL BOUND,0.1532033426183844,Published as a conference paper at ICLR 2022
OUR JOINT MODEL AND ITS SUPMIWAE VARIATIONAL BOUND,0.15598885793871867,"where qγ(z|xobs) is the variational distribution (learnable proposal) and (xmiss
k
, zk)k∈{1,...,K}
are i.i.d. samples from pθ(xmiss|z)qγ(z|xobs).
Then we have that E(xmiss
k
,zk)

RK(xobs, y)

=
pθ,φ(xobs, y) and E(xmiss
k
,zk)

log RK(xobs, y)

is a lower bound on the log likelihood. The vari-
ational distribution can be parameterized via a neural network with weights γ ∈Γ, similarly to
Mattei & Frellsen (2019), Nazabal et al. (2020), and Ipsen et al. (2021)."
OUR JOINT MODEL AND ITS SUPMIWAE VARIATIONAL BOUND,0.15877437325905291,"Now that we have a way to bound each per-datum log-likelihood, we can deﬁne our supMIWAE
variational bound LK(θ, φ, γ) that is a lower-bound of the log-likelihood of the observed data"
OUR JOINT MODEL AND ITS SUPMIWAE VARIATIONAL BOUND,0.1615598885793872,"LK(θ, φ, γ) = n
X"
OUR JOINT MODEL AND ITS SUPMIWAE VARIATIONAL BOUND,0.16434540389972144,"i=1
E(xmiss
k
,zk)
h
log RK(xobs
i , yi)
i
≤ n
X"
OUR JOINT MODEL AND ITS SUPMIWAE VARIATIONAL BOUND,0.1671309192200557,"i=1
log pφ,θ(yi, xobs
i ).
(14)"
OUR JOINT MODEL AND ITS SUPMIWAE VARIATIONAL BOUND,0.16991643454038996,"The lower bound LK(θ, φ, γ) is a Monte Carlo objective that can be maximized instead of
the log likelihood itself, using techniques from stochastic optimization (see e.g. Mohamed
et al., 2020).
As detailed in appendix D, it beneﬁts from the usual theoretical advantages
of importance-weighted variational bounds: LK(θ, φ, γ) converges monotonically to the log-
likelihood Pn
i=1 log pφ,θ(yi, xobs
i ) at speed 1/K. This means that, the larger the K, the closer
our objective will be to the true likelihood."
OUR JOINT MODEL AND ITS SUPMIWAE VARIATIONAL BOUND,0.17270194986072424,"Note that the contribution of any fully observed data point (x, y) will be simply"
OUR JOINT MODEL AND ITS SUPMIWAE VARIATIONAL BOUND,0.17548746518105848,"log pφ(y|x) + E(zk) "" log"
K,0.17827298050139276,"1
K K
X i=1"
K,0.181058495821727,pθ(x|zk)pθ(zk)
K,0.18384401114206128,qγ(zk|x) !#
K,0.18662952646239556,",
(15)"
K,0.1894150417827298,"which is just the sum of the discriminative likelihood and the standard IWAE bound of Burda et al.
(2016). An interesting property of our bound is that the discriminative and generative parts can be
trained jointly with a single and statistically sound objective. However, if we were to encounter a
data set without missing data, equation (15) means that the training of the two parts of the models
would be decoupled: the discriminative model would be the same as a standardly trained discrimi-
native model, and the generative model would be the same as a standard IWAE-trained DLVM."
PREDICTION,0.19220055710306408,"2.3
PREDICTION"
PREDICTION,0.19498607242339833,"Once the model has been jointly trained using the supMIWAE bound, it is possible to use it to do pre-
diction with missing data. Indeed self-normalized importance sampling can be used to approximate
the conditional"
PREDICTION,0.1977715877437326,"pφ(y|xobs) ≈ K
X"
PREDICTION,0.20055710306406685,"i=1
wkpφ(y|xobs, xmiss
k
),
(16) where"
PREDICTION,0.20334261838440112,"wk =
rk
r1 + ... + rK
, and rk = pθ(xobs|zk)pθ(zk)"
PREDICTION,0.20612813370473537,"qγ(zk|xobs)
,
(17)"
PREDICTION,0.20891364902506965,"and (xmiss
k
, zk)k∈{1,...,K} are i.i.d. samples from pθ(xmiss|z)qγ(z|xobs). Again, this technique mim-
icks multiple imputation: several imputations are generated using the generative model, are then fed
to the classiﬁer, and the predictions of each imputation are averaged."
RELATED WORK,0.2116991643454039,"3
RELATED WORK"
RELATED WORK,0.21448467966573817,"Marginalizing over missing covariates
Different approaches have previously been taken to
marginalize over missing values in joint models over features and labels. In a series of publica-
tions (Ahmad & Tresp, 1993; Tresp et al., 1994; 1995), a closed-form solution using Gaussian Basis
Function networks was used to approximate this marginalization, while keeping the neural discrim-
inative model ﬁxed. Ghahramani & Jordan (1995) took a different approach, modeling the joint
distribution p(y, x) directly as a mixture model, obtaining p(y|xobs) by conditioning on observed
quantities in the joint distribution, thus removing the explicit discriminative neural architecture. In
the context of kernel methods, Dick et al. (2008) proposed to learn a distribution over missing val-
ues by minimising the regularized empirical risk. ´Smieja et al. (2018) trained a Gaussian mixture
model (GMM) and discriminative model jointly, and in place of any missing values the activation
of the corresponding input neuron was set to the average activation over the GMM conditioned on
observed values."
RELATED WORK,0.21727019498607242,Published as a conference paper at ICLR 2022
RELATED WORK,0.2200557103064067,"Consistency of single imputation
A review of approaches to handling missing data in (non-deep)
supervised learning was given by Josse et al. (2019). Here it was shown that under some assumptions
about the capacity of the learner, constant imputation is asymptotically consistent in the supervised
setting. Le Morvan et al. (2020b) investigated the case of a linear predictor on covariates with
missing data, showing that in the presence of missing, the optimal predictor may not be linear and
how constant imputation of each feature can be optimized with regards to the model loss. The linear
case was further investigated in (Le Morvan et al., 2020a), deriving the analytical form of the optimal
predictor and proposing NeuMiss networks to approximate this. Bertsimas et al. (2021) analyzed
the impute-then-regress situation and noted that while this paradigm is widely used in practice it
incurs a bias in the parameter estimates, but mean imputation is still asymptotically consistent. Le
Morvan et al. (2021) showed that the impute-then-regress approach is asymptotically Bayes optimal
given a learner with large capacity and that this holds for all missing mechanisms, even missing not
at random (Rubin, 1976). However, the choice of imputation function affects the difﬁculty of the
subsequent regression task, when not in the asymptotic regime. As a consequence they proposed to
learn imputation and regression jointly, where the missing values are handled by a NeuMiss network."
RELATED WORK,0.22284122562674094,"Other approaches for missing covariates
Yi et al. (2019) tackled the issue of sparsity, and specif-
ically large variations in sparsity, by introducing sparsity normalization. This addresses the issue of
model output covarying with the sparsity level in the input. Ma et al. (2018; 2019) used a permu-
tation invariant setup to avoid imputing missing data in the input of a variational autoencoder. This
approach can be readily extended to the supervised setting, using the permutation invariant setup as a
modiﬁed input layer as we show in appendix A.3. Some tree approaches to regression/classiﬁcation
can use observations with missing values directly (see e.g. Twala et al., 2008; Chen & Guestrin,
2016; Josse et al., 2019; G´omez-M´endez & Joly, 2021). Sparse coding was used by Caiafa et al.
(2020) to train a dictionary and a classiﬁer simultaneously."
RELATED WORK,0.22562674094707522,"DLVMs and missing values
Deep latent variable models have been applied to missing data prob-
lems in the unsupervised setting, focused on inference, single and multiple imputations. Early works
assumed that a DLVM had previously been trained on complete data (Rezende et al., 2014; Mattei
& Frellsen, 2018), but a variety of techniques to handle incomplete training sets was then proposed,
both in M(C)AR (Yoon et al., 2018; Ma et al., 2018; 2019; Ivanov et al., 2019; Mattei & Frellsen,
2019; Li et al., 2019) and MNAR (Ipsen et al., 2021; Ghalebikesabi et al., 2021) cases. In (Li &
Marlin, 2020) irregularly sampled time-series are approached as a missing data problem and they
propose VAE and GAN based models for handling this. The proposed models can also be used for
supervised learning by training a classiﬁer on the latent representations, either by training the model
jointly or using a pretrained encoder. Without missing data, the idea of jointly training a VAE and a
discriminative model, has been explored by Ji et al. (2020) or Joy et al. (2021)."
EXPERIMENTS,0.22841225626740946,"4
EXPERIMENTS"
EXPERIMENTS,0.23119777158774374,"We now evaluate discriminative models trained using the supMIWAE bound on a range of super-
vised learning tasks. Throughout the experiments the generative part of the model is pretrained using
the MIWAE bound of Mattei & Frellsen (2019). The pretrained DLVM can be used to draw single
imputations, referred to as MIWAE single imputation, or it can be used as the generative part in
the supMIWAE computational structure. Here, the generative part of the model is kept ﬁxed while
updating the discriminative part of the model according to the supMIWAE bound. As the DLVM in
MIWAE single imputations and the supMIWAE are similar, any difference in performance between
the two is attributed to their different strategies for handling missing values. Other strategies for han-
dling missing data, used for comparison in this work, are brieﬂy described in appendix A. Missing
values are introduced in the training, validation and test sets according to the missing mechanism
used in the given experiment. Predictive performance is measured on a test-set with missing values.
In all experiments, there is a computational overhead compared to single imputation of training the
supMIWAE that scales linear with the number of importance samples K, c.f. equation (13)."
EXPERIMENTS,0.233983286908078,"4.1
2D CLASSIFICATION"
EXPERIMENTS,0.23676880222841226,"A qualitative analysis of the bias in the learnt parameters of the discriminative model, due to single
imputation, is set up on simple 2D datasets, see ﬁgure 8 of appendix E. The simplicity allows us"
EXPERIMENTS,0.2395543175487465,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.24233983286908078,"supMIWAE
MIWAE
0-impute
Learnable Imputation
MICE
MissForest"
EXPERIMENTS,0.24512534818941503,"Figure 3: Top row:
(except top left) kernel density of the half-moons dataset together with im-
putations from the given model. The supMIWAE does not rely on single imputations, instead it
draws multiple importance samples which are passed through the discriminator to give an impor-
tance weighted prediction. For the supMIWAE multiple imputations at three different values of the
horizontal coordinate are shown and a kernel density of the multiple imputations are shown to the
left. For the rest of the methods single imputations are shown in red. Bottom row: Decision sur-
faces learnt, depending on the strategy for handling missing values. The methods based on single
imputation need to warp the decision boundaries."
EXPERIMENTS,0.2479108635097493,"to directly inspect the impact of different imputation strategies on the learnt decision surface and
compare this to the marginalization procedure used in the supMIWAE."
EXPERIMENTS,0.25069637883008355,"All datasets consist of a training set with 3k records and validation and test sets with 1k records.
An MCAR missing process is introduced in the horizontal coordinate, where each element be-
comes missing with probability m = 0.5. Further training details are in appendix E. The datasets
are such that there will be inherent ambiguity about the class label for some of the records with
missing values. For example in the Burger dataset, all records with a missing value will have
p(y = 1|xobs) = 2/3."
EXPERIMENTS,0.25348189415041783,"In ﬁgure 3 imputations from different imputation models are shown along with the corresponding
learnt decision surface. Partially observed examples are shown in the margins of the plots as sticks,
colored by their class label. All methods use the same discriminative architecture, only the im-
putation strategy differs. For the supMIWAE, multiple imputations are shown for three partially
observed records, alongside their kernel density. These multiple imputations reﬂect the appearance
of importance samples used during training, for these three partially observed records."
EXPERIMENTS,0.2562674094707521,"Decision surface artifacts
The single imputation methods introduce artifacts into the learnt de-
cision surface.
As the discriminative model tries to reﬂect the proper conditional probabilities
p(y|xobs) at the location of the imputed values, the ambiguity about the class label needs to be
reﬂected in the predictions. Notice how the MIWAE produces nearly optimal imputations in terms
of minimizing a mean-square-error and how this translates into decision surface artifacts. These
artifacts are related to the notion of imputation manifolds used by Le Morvan et al. (2021) to prove
their main theorem on the Bayes consitency of impute-then-regress procedures. The remaining
datasets are shown in ﬁgures 10–13 of appendix E, where also the permutation-invariance setup (ap-
pendix A.3) and histogram-based gradient boosting are included. Like the supMIWAE, these two
approaches do not rely on explicit imputations."
EXPERIMENTS,0.2590529247910863,"As the supMIWAE can draw multiple importance samples, instead of one single imputation, the
conditional probabilities p(y|xobs) can be properly reﬂected, when averaging over the importance
samples. This avoids the single imputation artefacts."
EXPERIMENTS,0.2618384401114206,"Predictive performance
Do the artifacts introduced by single imputation methods, compared to
marginalizing over the missing values, translate into differences in predictive performance? The
predictive performance corresponding to the decision surfaces in ﬁgure 3 are shown in table 1 in ap-
pendix E. Both the test-set accuracies and log likelihoods are more or less similar across the different
strategies for handling missing values. This aligns with the results by Josse et al. (2019); Bertsimas
et al. (2021); Le Morvan et al. (2021): the impute-then-regress procedures with powerful learners
are consistent and Bayes optimal, i.e. when the learner has high capacity any single imputation can"
EXPERIMENTS,0.2646239554317549,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.26740947075208915,"essentially be undone. While single-imputation methods can lead to Bayes-optimal learners, Le
Morvan et al. (2021) address that the difﬁculty of the learning problem depends on the choice of
imputation function. This is seen in ﬁgure 4; while the supMIWAE and 0-imputation eventually
ends up with similar performance the learning problem appears easier for the supMIWAE. For 0-
imputation the learner has to introduce high-frequency artifacts to the decision surface in order to
get the conditional probabilities p(y|xobs) calibrated, but in the case of a high capacity learner this
can eventually be done."
EXPERIMENTS,0.27019498607242337,"Figure 4: Learning curves on the half-
moons dataset: train (full lines) and test
(dashed lines) set accuracies, when the
classiﬁer has full capacity."
EXPERIMENTS,0.27298050139275765,"Limited capacity learner
We now turn to a situation
where the capacity of the learner is limited in order
to assess how single imputation methods fare against
marginalizing over missing values.
We start by not-
ing that the decision surface for the Burger dataset, in
the complete-data case, can be learnt using two sig-
moids. When using MSE-optimal single imputations in
the missing-values case this changes, as now 4 sigmoids
are needed to make sure the imputations are well cali-
brated. This illustrates that the capacity of the learner
determines whether single imputations can be undone. In
ﬁgure 5 we explore how the capacity of the learner, in
terms of the number of hidden units, affects the predic-
tive performance, depending on the imputation strategy.
More capacity experiments are found in ﬁgure 14."
IMAGE CLASSIFICATION,0.2757660167130919,"4.2
IMAGE CLASSIFICATION"
IMAGE CLASSIFICATION,0.2785515320334262,"When the learning problems become harder, strong inductive biases are sometimes used to help
learning. This is typically the case with image classiﬁcation where convolutional neural networks
(CNNs) are often used to introduce translational equivariance. The previous section showed that the
strategy for handling missing data affects predictive performance when the learner is not in the high
capacity regime. This is further investigated in terms of classiﬁcation accuracy in image experiments
with different MCAR missing processes, such as observed squares, missing squares and random
dropout, see ﬁgures 15–17. The discriminator is a convolutional neural network, see appendix F.1,
thus introducing an inductive bias in the learner that has proved useful in image classiﬁcation in the
complete-data case. While CNNs are ﬁt for images, it should be noted that they are not universal
approximators, and may consequently have a hard time undoing single imputations."
IMAGE CLASSIFICATION,0.28133704735376047,"We apply the supMIWAE and single imputations methods to the MNIST dataset (LeCun et al., 1998)
and the fashion MNIST dataset (Xiao et al., 2017) with three different MCAR missing mechanisms:
1) observed squares, where a 12×12 randomly placed square is observed, 2) missing squares, where
a 12×12 randomly placed square is missing, and 3) dropout, where each pixel is missing with some
constant probability m, over a range of missing rates."
IMAGE CLASSIFICATION,0.2841225626740947,"In practice, we see that this setting where there is a strong inductive bias is similar to our previous
low-capacity experiments: for both MNIST and Fashion MNIST, the supMIWAE is signiﬁcantly"
IMAGE CLASSIFICATION,0.28690807799442897,"Figure 5: Predictive performance when varying the capacity of the learner in terms of number of hid-
den units. The discriminative model has the same architecture across methods, only the imputation
strategy differs."
IMAGE CLASSIFICATION,0.28969359331476324,Published as a conference paper at ICLR 2022
IMAGE CLASSIFICATION,0.2924791086350975,"Figure 6: Test set accuracies on the MNIST and Fashion MNIST datasets, with different missing
mechanisms. Left column: observed squares. Middle column: missing squares. Right column:
random dropout over a range of missing rates."
IMAGE CLASSIFICATION,0.29526462395543174,Figure 7: Test-set root mean square error on UCI datasets at varying missing rates.
IMAGE CLASSIFICATION,0.298050139275766,"more accurate than single-imputation competitors, as seen in ﬁgure 6. Additional experiments,
including experiments on natural images (SVHN) and visualizations of multiple imputations, are
available in appendix F."
REGRESSION,0.3008356545961003,"4.3
REGRESSION"
REGRESSION,0.30362116991643456,"We now turn to regression in smaller and lower dimensional datasets from the UCI database (Dua
& Graff, 2017). An experimental setup modiﬁed from Hern´andez-Lobato & Adams (2015) and
Skafte et al. (2019) is used here. Predictive performance in terms of root-mean-square-error is
presented over a range of missing rates. Results are seen in ﬁgure 7, and more results can be
found in ﬁgure 20 of appendix G. There is no clear winner here, although it appears that gradient
boosting generally performs quite well, and that accurate single imputations (such as MIWAE’s or
MissForest’s) generally outperform inaccurate ones (such as zero imputation). In general, our joint
model performs as well as feeding accurate single imputations."
CONCLUSION,0.3064066852367688,"5
CONCLUSION"
CONCLUSION,0.30919220055710306,"In the context of supervised deep learning with missing values, single imputation methods are an
often used tool. While this approach is asymptotically consistent it incurs a bias in the learnt pa-
rameters. We have introduced a scalable approach to marginalizing over missing values using deep
generative models, as a probabilistic alternative to single imputation. Besides limiting single impu-
tation artifacts our results indicate that there are two cases where using a generative model can be
quite valuable for supervised learning with missing values: when the classiﬁer is not very ﬂexible,
or has a strong inductive bias. Moreover, leveraging such a generative model allows us to be able to
use untouched any discriminative architecture (for instance, any pretrained one)."
CONCLUSION,0.31197771587743733,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.3147632311977716,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.31754874651810583,"Code for reproducing paper experiments is available at https://github.com/nbip/
supMIWAE."
REPRODUCIBILITY STATEMENT,0.3203342618384401,ACKNOWLEDGEMENTS
REPRODUCIBILITY STATEMENT,0.3231197771587744,"This work has been supported by the French government, through the 3IA Cˆote d’Azur Invest-
ments in the Future project managed by the National Research Agency (ANR) with the refer-
ence number ANR-19-P3IA-0002. Furthermore, it was supported by the Novo Nordisk Foundation
(NNF20OC0062606 and NNF20OC0065611) and the Independent Research Fund Denmark (9131-
00082B)."
REFERENCES,0.32590529247910865,REFERENCES
REFERENCES,0.3286908077994429,"Subutai Ahmad and Volker Tresp. Some solutions to the missing feature problem in vision. In
Advances in Neural Information Processing Systems, pp. 393–400, 1993."
REFERENCES,0.33147632311977715,"Dimitris Bertsimas, Arthur Delarue, and Jean Pauphilet. Prediction with missing data. arXiv preprint
arXiv:2104.03158, 2021."
REFERENCES,0.3342618384401114,"Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In
International Conference on Learning Representations, 2016."
REFERENCES,0.3370473537604457,"S van Buuren and Karin Groothuis-Oudshoorn. mice: Multivariate imputation by chained equations
in R. Journal of statistical software, pp. 1–68, 2010."
REFERENCES,0.3398328690807799,"Cesar F Caiafa, Ziyao Wang, Jordi Sol´e-Casals, and Qibin Zhao. Learning from incomplete data by
simultaneous training of neural networks and sparse coding. arXiv preprint arXiv:2011.14047,
2020."
REFERENCES,0.3426183844011142,"Tianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of
the 22nd acm SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.
785–794, 2016."
REFERENCES,0.34540389972144847,"Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated
convolutional networks. In International conference on machine learning, pp. 933–941. PMLR,
2017."
REFERENCES,0.34818941504178275,"Uwe Dick, Peter Haider, and Tobias Scheffer. Learning from incomplete data with inﬁnite impu-
tations. In Proceedings of the 25th international conference on Machine learning, pp. 232–239,
2008."
REFERENCES,0.35097493036211697,"Justin Domke and Daniel R Sheldon. Importance weighting and variational inference. In Advances
in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018."
REFERENCES,0.35376044568245124,"Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.3565459610027855,"Jerome H Friedman.
Greedy function approximation: a gradient boosting machine.
Annals of
statistics, pp. 1189–1232, 2001."
REFERENCES,0.3593314763231198,"Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin.
Bayesian data analysis. CRC press, 2013."
REFERENCES,0.362116991643454,"Zoubin Ghahramani and Geoffrey E Hinton. The EM algorithm for mixtures of factor analyzers.
Technical report, CRG-TR-96-1, University of Toronto, 1996."
REFERENCES,0.3649025069637883,"Zoubin Ghahramani and Michael I Jordan. Learning from incomplete data. MIT Center for Biolog-
ical and Computational Learning Technical Report 108, 1995."
REFERENCES,0.36768802228412256,"Sahra Ghalebikesabi, Rob Cornish, Chris Holmes, and Luke Kelly. Deep generative pattern-set
mixture models. In Proceedings of The 24th International Conference on Artiﬁcial Intelligence
and Statistics, pp. 3727–3735, 2021."
REFERENCES,0.37047353760445684,Published as a conference paper at ICLR 2022
REFERENCES,0.3732590529247911,"Irving G´omez-M´endez and Emilien Joly. Regression with missing data, a comparison study of
techniques based on random forests. Preprint, available at https://irvinggomez.com/
publication, 2021."
REFERENCES,0.37604456824512533,"Jos´e Miguel Hern´andez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learn-
ing of bayesian neural networks. In International Conference on Machine Learning, pp. 1861–
1869, 2015."
REFERENCES,0.3788300835654596,"Niels Bruun Ipsen, Pierre-Alexandre Mattei, and Jes Frellsen. not-MIWAE: Deep generative mod-
elling with missing not at random data. In International Conference on Learning Representations,
2021."
REFERENCES,0.3816155988857939,"Oleg Ivanov, Michael Figurnov, and Dmitry Vetrov. Variational autoencoder with arbitrary condi-
tioning. In International Conference on Learning Representations, 2019."
REFERENCES,0.38440111420612816,"Tianchen Ji, Sri Theja Vuppala, Girish Chowdhary, and Katherine Driggs-Campbell. Multi-modal
anomaly detection for unstructured and uncertain environments. In Conference on Robot Learn-
ing, 2020."
REFERENCES,0.3871866295264624,"Matthew J Johnson, David K Duvenaud, Alex Wiltschko, Ryan P Adams, and Sandeep R Datta.
Composing graphical models with neural networks for structured representations and fast infer-
ence. Advances in Neural Information Processing Systems, 29:2946–2954, 2016."
REFERENCES,0.38997214484679665,"Julie Josse, Nicolas Prost, Erwan Scornet, and Ga¨el Varoquaux. On the consistency of supervised
learning with missing values. arXiv preprint arXiv:1902.06931, 2019."
REFERENCES,0.39275766016713093,"Tom Joy, Sebastian Schmon, Philip Torr, N Siddharth, and Tom Rainforth. Capturing label charac-
teristics in VAEs. In International Conference on Learning Representations, 2021."
REFERENCES,0.3955431754874652,"Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference
on Learning Representations, 2014."
REFERENCES,0.3983286908077994,"Marine Le Morvan, Julie Josse, Thomas Moreau, Erwan Scornet, and Ga¨el Varoquaux. Neumiss
networks: differentiable programming for supervised learning with missing values. arXiv preprint
arXiv:2007.01627, 2020a."
REFERENCES,0.4011142061281337,"Marine Le Morvan, Nicolas Prost, Julie Josse, Erwan Scornet, and Ga¨el Varoquaux. Linear predictor
on linearly-generated data with missing values: non consistency and solutions. In International
Conference on Artiﬁcial Intelligence and Statistics, pp. 3165–3174, 2020b."
REFERENCES,0.403899721448468,"Marine Le Morvan, Julie Josse, Erwan Scornet, and Ga¨el Varoquaux. What’s a good imputation to
predict with missing values? arXiv preprint arXiv:2106.00311, 2021."
REFERENCES,0.40668523676880225,"Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.40947075208913647,"Steven Cheng-Xian Li and Benjamin Marlin. Learning from irregularly-sampled time series: A
missing data perspective. In International Conference on Machine Learning, pp. 5937–5946,
2020."
REFERENCES,0.41225626740947074,"Steven Cheng-Xian Li, Bo Jiang, and Benjamin Marlin. MisGAN: Learning from incomplete data
with generative adversarial networks. In International Conference on Learning Representations,
2019."
REFERENCES,0.415041782729805,"Roderick JA Little and Donald B Rubin. Statistical analysis with missing data, volume 793. John
Wiley & Sons, 2019."
REFERENCES,0.4178272980501393,"Gabriel Loaiza-Ganem and John P Cunningham. The continuous Bernoulli: ﬁxing a pervasive error
in variational autoencoders. Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.4206128133704735,"Chao Ma, Wenbo Gong, Jos´e Miguel Hern´andez-Lobato, Noam Koenigstein, Sebastian Nowozin,
and Cheng Zhang. Partial VAE for hybrid recommender system. In 3rd NeurIPS Workshop on
Bayesian Deep Learning, 2018."
REFERENCES,0.4233983286908078,Published as a conference paper at ICLR 2022
REFERENCES,0.42618384401114207,"Chao Ma, Sebastian Tschiatschek, Konstantina Palla, Jose Miguel Hernandez-Lobato, Sebastian
Nowozin, and Cheng Zhang. EDDI: Efﬁcient dynamic discovery of high-value information with
partial VAE. In International Conference on Machine Learning, pp. 4234–4243, 2019."
REFERENCES,0.42896935933147634,"Pierre-Alexandre Mattei and Jes Frellsen. Leveraging the exact likelihood of deep latent variable
models. In Advances in Neural Information Processing Systems, 2018."
REFERENCES,0.43175487465181056,"Pierre-Alexandre Mattei and Jes Frellsen. MIWAE: Deep generative modelling and imputation of
incomplete data sets. In International Conference on Machine Learning, pp. 4413–4423, 2019."
REFERENCES,0.43454038997214484,"Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih.
Monte Carlo gradient
estimation in machine learning. Journal of Machine Learning Research, 21(132):1–62, 2020."
REFERENCES,0.4373259052924791,"Alfredo Nazabal, Pablo M Olmos, Zoubin Ghahramani, and Isabel Valera. Handling incomplete
heterogeneous data using VAEs. Pattern Recognition, 107:107501, 2020."
REFERENCES,0.4401114206128134,"Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011."
REFERENCES,0.4428969359331476,"F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825–2830, 2011."
REFERENCES,0.4456824512534819,"Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point
sets for 3D classiﬁcation and segmentation. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 652–660, 2017."
REFERENCES,0.44846796657381616,"Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015."
REFERENCES,0.45125348189415043,"Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-
proximate inference in deep generative models. In International Conference on Machine Learn-
ing, 2014."
REFERENCES,0.45403899721448465,"Sam Roweis. EM algorithms for PCA and SPCA. Advances in Neural Information Processing
Systems, pp. 626–632, 1998."
REFERENCES,0.4568245125348189,"Donald B Rubin. Inference and missing data. Biometrika, 63(3):581–592, 1976."
REFERENCES,0.4596100278551532,"Donald B Rubin. Multiple imputation after 18+ years. Journal of the American Statistical Associa-
tion, 91(434):473–489, 1996."
REFERENCES,0.4623955431754875,"Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the
pixelCNN with discretized logistic mixture likelihood and other modiﬁcations. In International
Conference on Learning Representations, 2017."
REFERENCES,0.46518105849582175,"Shaun Seaman, John Galati, Dan Jackson, and John Carlin. What is meant by ”missing at random”?
Statistical Science, pp. 257–268, 2013."
REFERENCES,0.467966573816156,"Nicki Skafte, Martin Jørgensen, and Søren Hauberg. Reliable training and estimation of variance
networks. In Advances in Neural Information Processing Systems, pp. 6326–6336, 2019."
REFERENCES,0.47075208913649025,"Marek ´Smieja, Łukasz Struski, Jacek Tabor, Bartosz Zieli´nski, and Przemysław Spurek. Processing
of missing data by neural networks. In Advances in Neural Information Processing Systems, pp.
2719–2729, 2018."
REFERENCES,0.4735376044568245,"Daniel J Stekhoven and Peter B¨uhlmann. Missforest—non-parametric missing value imputation for
mixed-type data. Bioinformatics, 28(1):112–118, 2012."
REFERENCES,0.4763231197771588,"Michael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611–622, 1999."
REFERENCES,0.479108635097493,Published as a conference paper at ICLR 2022
REFERENCES,0.4818941504178273,"Volker Tresp, Subutai Ahmad, and Ralph Neuneier. Training neural networks with deﬁcient data. In
Advances in Neural Information Processing Systems, pp. 128–135, 1994."
REFERENCES,0.48467966573816157,"Volker Tresp, Ralph Neuneier, and Subutai Ahmad. Efﬁcient methods for dealing with missing data
in supervised learning. In Advances in Neural Information Processing Systems, pp. 689–696,
1995."
REFERENCES,0.48746518105849584,"Bheki E T H Twala, M C Jones, and David J Hand. Good methods for coping with missing data in
decision trees. Pattern Recognition Letters, 29(7):950–956, 2008."
REFERENCES,0.49025069637883006,"Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for bench-
marking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017."
REFERENCES,0.49303621169916434,"Joonyoung Yi, Juhyuk Lee, Kwang Joon Kim, Sung Ju Hwang, and Eunho Yang.
Why not
to use zero imputation? correcting sparsity bias in training neural networks.
arXiv preprint
arXiv:1906.00150, 2019."
REFERENCES,0.4958217270194986,"Jinsung Yoon, James Jordon, and Mihaela Van Der Schaar. GAIN: missing data imputation using
generative adversarial nets. In International Conference on Machine Learning, 2018."
REFERENCES,0.4986072423398329,"Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and
Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems, pp.
3391–3401, 2017."
REFERENCES,0.5013927576601671,Published as a conference paper at ICLR 2022
REFERENCES,0.5041782729805014,"A
TRAINING DEEP SUPERVISED MODELS WITH MISSING DATA"
REFERENCES,0.5069637883008357,In the following we group different strategies for handling missing data in supervised deep learning.
REFERENCES,0.5097493036211699,"A.1
CONSTANT IMPUTATION"
REFERENCES,0.5125348189415042,"We denote imputation methods where there is a single constant for each feature which replaces
missing data as constant imputation. We deﬁne an imputation function ι in general such that ι(˜x) ∈
X and ι(˜x)obs = xobs. A simple version of constant imputation is 0-imputation, which has the
intuitive appeal that the activation from the input node is zeroed out (absent) and no gradient updates
are made for the weights for that feature,"
REFERENCES,0.5153203342618384,"ι0(˜x) = ˜x ⊙s + 0 ⊙(1 −s).
(18)"
REFERENCES,0.5181058495821727,"In the unsupervised setting, constant imputation biases marginal and joint distributions, but Josse
et al. (2019); Le Morvan et al. (2021); Bertsimas et al. (2021) have shown that mean imputation can
be consistent in the supervised setting. Furthermore, Le Morvan et al. (2020b); Tresp et al. (1994)
noted that the constants can be optimized with respect to the prediction loss."
REFERENCES,0.520891364902507,"Learnable parameters λ ∈X can be inserted in place of the missing data, so that"
REFERENCES,0.5236768802228412,"ιλ(˜x) = ˜x ⊙s + λ ⊙(1 −s).
(19)"
REFERENCES,0.5264623955431755,We denote this learnable imputation.
REFERENCES,0.5292479108635098,"A.2
REGRESSIVE/MODEL BASED SINGLE IMPUTATION"
REFERENCES,0.532033426183844,"In contrast to constant imputation, methods such as MICE (Buuren & Groothuis-Oudshoorn, 2010),
MissForest (Stekhoven & B¨uhlmann, 2012), PPCA (Tipping & Bishop, 1999; Roweis, 1998) or ﬂex-
ible generative models (Mattei & Frellsen, 2019; Yoon et al., 2018) provides imputation of missing
features conditional on observed features p(xmiss|xobs),"
REFERENCES,0.5348189415041783,"ι(˜x) = ˜x ⊙s + Ep(xmiss|xobs) [x] ⊙(1 −s)
(20)"
REFERENCES,0.5376044568245125,"In MICE, this conditional distribution comes from an implicit joint model over the covariates by
regressing each covariate with missing values on all the others, in a round-robin fashion. MissForest
is somewhat similar to MICE, using random forests as the regressors. Probabilistic PCA deﬁnes an
explicit joint model over all covariates and single imputations of missing data can be obtained by
conditioning on observed covariates. Here, MICE is a proper Bayesian multiple imputation method,
while PPCA and MissForest draws multiple imputations without regard to parameter uncertainties.
The recent advances in imputation of missing data using deep generative models can also be used
here to the extent that they impose a joint distribution over covariates."
REFERENCES,0.5403899721448467,"A.3
PERMUTATION INVARIANCE"
REFERENCES,0.5431754874651811,"Ma et al. (2018; 2019) used the permutation invariant setup (Zaheer et al., 2017; Qi et al., 2017)
to handle missing data in the context of variational autoencoders. For each feature j ∈obs(s),
there is an embedding ej ∈RU, which in general can hold any auxiliary information, but in our
case will be a learnable embedding. Each element of the embedding is multiplied with the feature
value tj = ej · xj and fed to a neural network h(·), mapping inputs from RU to RM where U
is the embedding dimension and M is the dimension of the code-space being mapped into. This
permutation invariant setup ﬁnally aggregates all the neural network outputs by a summation to get
one ﬁxed-length vector, that can be fed into a discriminative network,"
REFERENCES,0.5459610027855153,"ιPI(xobs) =
X"
REFERENCES,0.5487465181058496,"j∈obs(s)
h(tj).
(21)"
REFERENCES,0.5515320334261838,"Note that in this case ιPI(xobs) ∈RM, while in the other imputation functions ι(˜x) ∈X."
REFERENCES,0.5543175487465181,Published as a conference paper at ICLR 2022
REFERENCES,0.5571030640668524,"A.4
TREE METHODS"
REFERENCES,0.5598885793871866,"Decision trees can in some cases handle missing data directly, without the need for imputation.
There are different approaches to handle missing values in tree based methods, see (Josse et al.,
2019) for a review. In our experiments we use histogram-based gradient boosting (Friedman, 2001),
as implemented in scikit-learn (Pedregosa et al., 2011). This approach uses the Block Propagation
method for the missing data. While a discriminative model using MLPs has a ﬁxed capacity, gradient
boosting methods can adapt its capacity in terms of tree depth based on a validation set error."
REFERENCES,0.5626740947075209,"B
GRADIENTS"
REFERENCES,0.5654596100278552,"The log likelihood, when integrating over missing input features, is speciﬁed as"
REFERENCES,0.5682451253481894,"log pφ,θ(y, xobs) = log pφ(y|xobs) + log pθ(xobs)
(22)"
REFERENCES,0.5710306406685237,"where
log pφ(y|xobs) = log
Z
pφ(y|xobs, xmiss)pθ(xmiss|xobs) dxmiss.
(23)"
REFERENCES,0.5738161559888579,"Both Tresp et al. (1994) and Ghahramani & Jordan (1995) analyzed the gradient of the log likelihood
with respect to the discriminative network parameters,"
REFERENCES,0.5766016713091922,"∂log pφ,θ(y, xobs)"
REFERENCES,0.5793871866295265,"∂φ
= ∂log pφ(y|xobs)"
REFERENCES,0.5821727019498607,"∂φ
+ ∂log pθ(xobs)"
REFERENCES,0.584958217270195,"∂φ
(24)"
REFERENCES,0.5877437325905293,"= −
1
pφ(y|xobs)
∂pφ(y|xobs)"
REFERENCES,0.5905292479108635,"∂φ
(25)"
REFERENCES,0.5933147632311978,"= −
1
pφ(y|xobs)
∂
∂φ"
REFERENCES,0.596100278551532,"Z
pφ(y|xobs, xmiss)pθ(xmiss|xobs) dxmiss
(26)"
REFERENCES,0.5988857938718662,"= −
1
pφ(y|xobs)"
REFERENCES,0.6016713091922006,"Z ∂pφ(y|xobs, xmiss)"
REFERENCES,0.6044568245125348,"∂φ
pθ(xmiss|xobs) dxmiss
(27)"
REFERENCES,0.6072423398328691,"Here we see that the gradient of the discriminative network parameters, due to the classiﬁca-
tion/regression loss, is a weighted sum over all possible missing values, where the missing values
are weighted according to their density in the data model, conditional on the observed data."
REFERENCES,0.6100278551532033,The gradient with respect to the θ parameters depends to some extend on the φ parameters.
REFERENCES,0.6128133704735376,"∂log pφ,θ(y, xobs)"
REFERENCES,0.6155988857938719,"∂θ
= ∂log pφ(y|xobs)"
REFERENCES,0.6183844011142061,"∂θ
+ ∂log pθ(xobs)"
REFERENCES,0.6211699164345403,"∂θ
(28)"
REFERENCES,0.6239554317548747,"= −
1
pφ(y|xobs)
∂pφ(y|xobs)"
REFERENCES,0.6267409470752089,"∂θ
+ ∂log pθ(xobs)"
REFERENCES,0.6295264623955432,"∂θ
(29)"
REFERENCES,0.6323119777158774,"= −
1
pφ(y|xobs)
∂
∂θ"
REFERENCES,0.6350974930362117,"Z
pφ(y|xobs, xmiss)pθ(xmiss|xobs) dxmiss + ∂log pθ(xobs)"
REFERENCES,0.637883008356546,"∂θ
(30)"
REFERENCES,0.6406685236768802,"= −
1
pφ(y|xobs)
∂
∂θ Epθ(xmiss|xobs)
h
pφ(y|xobs, xmiss)
i
+ ∂log pθ(xobs)"
REFERENCES,0.6434540389972145,"∂θ
(31) (32)"
REFERENCES,0.6462395543175488,"There are contributions to the gradient from both the generative part of the model and the discrim-
inative part. This means that, if needed, the discriminative model can affect the generative model,
as is the case in (Ipsen et al., 2021). The gradient of the expectation can be approached using the
reparameterization trick (Kingma & Welling, 2014; Rezende et al., 2014)."
REFERENCES,0.649025069637883,"C
RELATION TO MULTIPLE IMPUTATION"
REFERENCES,0.6518105849582173,"The gold standard for imputation is multiple imputation (Rubin, 1996). This requires learning a
model of the observed covariates xobs with parameters θ. K sets of imputations are then drawn from"
REFERENCES,0.6545961002785515,Published as a conference paper at ICLR 2022
REFERENCES,0.6573816155988857,the posterior predictive distribution
REFERENCES,0.6601671309192201,"xmiss
k
∼p(xmiss|xobs), k = 1, . . . , K
(33)"
REFERENCES,0.6629526462395543,"where p(xmiss|xobs) =
R
p(xmiss|θ)p(θ|xobs) dθ if the model is Bayesian (see e.g. Gelman et al.,
2013, chapter 18.2). Several frequentist multiple imputation methods also exist, where the uncer-
tainty with respect to the imputation model parameters is disregarded. In that case, the Bayesian
posterior predictive is replaced by a conditional distribution p(xmiss|xobs) = p ˆθ(xmiss|xobs), where
ˆθ is any point estimate of θ."
REFERENCES,0.6657381615598886,"In the case of regression/classiﬁcation, if we roughly follow the notations of our paper, the analysis
proceeds as follows"
REFERENCES,0.6685236768802229,"• For each set of imputed values (xmiss
ik )i≤n,k≤K, use a learning algorithm to estimate the
discriminative parameters φ1, . . . , φK:"
REFERENCES,0.6713091922005571,"ˆφ1 = arg max
φ n
X"
REFERENCES,0.6740947075208914,"i=1
log pφ(yi|xobs
i , xmiss
i1 )
(34) ..."
REFERENCES,0.6768802228412256,"ˆφK = arg max
φ n
X"
REFERENCES,0.6796657381615598,"i=1
log pφ(yi|xobs
i , xmiss
iK )
(35)"
REFERENCES,0.6824512534818942,• Obtain an average estimate of φ using
REFERENCES,0.6852367688022284,"¯φK = 1 K K
X"
REFERENCES,0.6880222841225627,"k=1
ˆφk
(36)"
REFERENCES,0.6908077994428969,"and a variance estimate can similarly be obtained as described in (Gelman et al., 2013;
Rubin, 1996)."
REFERENCES,0.6935933147632312,"In the limit of inﬁnite imputations, we can write this as the conditional expectation"
REFERENCES,0.6963788300835655,"¯φ = Exmiss
1
,...,xmiss
n "
REFERENCES,0.6991643454038997,"arg max
φ n
X"
REFERENCES,0.7019498607242339,"i=1
log pφ(yi|xobs
i , xmiss
i
)"
REFERENCES,0.7047353760445683,"xobs
1 , . . . , xobs
n "
REFERENCES,0.7075208913649025,".
(37)"
REFERENCES,0.7103064066852368,"Instead of ﬁtting K conditional models to K imputed datasets, our approach (when θ is ﬁxed) is, in
comparison, to directly maximize the expectation of the conditional distribution with respect to the
missing data"
REFERENCES,0.713091922005571,"¯φ = arg max
φ n
X"
REFERENCES,0.7158774373259053,"i=1
log Exmiss
i"
REFERENCES,0.7186629526462396,"h
pφ(yi|xobs
i , xmiss
i
)|xobs
i
i
.
(38)"
REFERENCES,0.7214484679665738,"Also note that we have no need for several ﬁxed sets of imputations, at each gradient step a new set of
samples are drawn from the imputation model, xmiss ∼pθ(xmiss|xobs). The number of imputations
can be set dynamically, using ∼5–50 during training and ∼10k for testing. Our approach, using
DLVMs as imputation models, is not Bayesian, but any available proper Bayesian imputation model
could be used in place of the DLVMs."
REFERENCES,0.724233983286908,"In practice, an important difference between traditional multiple imputation and our approach, is
that multiple imputation uses a uniform weight 1/K for each imputation, while we use importance
weights, that will treat each imputation based on its likelihood."
REFERENCES,0.7270194986072424,"D
THEORETICAL PROPERTIES OF THE SUPMIWAE BOUND"
REFERENCES,0.7298050139275766,"We will brieﬂy see how the supMIWAE bound has the same theoretical properties as IWAE bounds.
We follow Ipsen et al. (2021, appendix D), who were following Burda et al. (2016) and Domke &
Sheldon (2018). Recall the deﬁnition of the bound: we have"
REFERENCES,0.7325905292479109,"RK(xobs, y) = 1 K K
X k=1"
REFERENCES,0.7353760445682451,"pφ(y|xobs, xmiss
k
)pθ(xobs|zk)pθ(zk)
qγ(zk|xobs)
,
(39)"
REFERENCES,0.7381615598885793,Published as a conference paper at ICLR 2022
REFERENCES,0.7409470752089137,"Half-moons
Circles
Burger
Pin-wheels"
REFERENCES,0.7437325905292479,Figure 8: 2D datasets with two or more classes. and
REFERENCES,0.7465181058495822,"LK(θ, φ, γ) = n
X"
REFERENCES,0.7493036211699164,"i=1
E(xmiss
k
,zk)
h
log RK(xobs
i , yi)
i
.
(40)"
REFERENCES,0.7520891364902507,"The ith term of the sum can be seen as a regular IWAE bound with observation (xobs, y), latent
variable (z, xmiss) with prior pθ(xmiss|z)pθ(z), and variational distribution pθ(xmiss|z)qγ(z|xobs).
Therefore, Theorem 1 from Burda et al. (2016) can be applied, and leads to the monotonicity of the
bound: L1(θ, φ, γ) ≤. . . ≤LK(θ, φ, γ)."
REFERENCES,0.754874651810585,"To show that the bound additionally converges to the log-likelihood when K −→∞, we use Theo-
rem 3 of Domke & Sheldon (2018) for all n terms of the sum that constitutes the bound.
Theorem. Assuming that, for all i ∈{1, ..., n},"
REFERENCES,0.7576601671309192,"• there exists αi > 0 such that E

|R1(xobs
i , yi) −pθ,φ(xobs
i , yi)|2+αi
< ∞,"
REFERENCES,0.7604456824512534,"• lim supK−→∞E

1/RK

< ∞,"
REFERENCES,0.7632311977715878,"the supMIWAE bound converges to the true joint likelihood at rate 1/K: n
X"
REFERENCES,0.766016713091922,"i=1
log pφ,θ(yi, xobs
i ) −LK(θ, φ, γ)
∼
K→∞
1
K n
X i=1"
REFERENCES,0.7688022284122563,"Var[R1(xobs
i , yi)]
2pθ,φ(xobs
i , yi)2 .
(41)"
REFERENCES,0.7715877437325905,"E
2D CLASSIFICATION"
REFERENCES,0.7743732590529248,"The datasets used in the 2D classiﬁcation experiments are shown in ﬁgure 8. The Half-moons and
Circles datasets are from scikit-learn (Pedregosa et al., 2011). The Pin-wheel dataset is modiﬁed
from Johnson et al. (2016)1. The Burger dataset, we have designed to illustrate the case where
optimal single imputation requires severe changes in the decision surface."
REFERENCES,0.7771587743732591,"The discriminative model is an MLP with 3 hidden layers with 50 hidden units each. The ﬁnal
layer parameterizes a categorical distribution over the dataset classes.
The generative model
used for the MIWAE single imputations and the supMIWAE consists of an encoder and decoder
with 3 hidden layers containing 50 hidden units each. The encoder parameterizes an isotropic
Gaussian distribution over a latent space with dimension p, same as the feature space. The decoder
parameterizes an isotropic Gaussian distribution over the feature space. In ﬁgure 9 samples from
the generative model are shown, that is, samples from the MIWAE trained on each of the datasets."
REFERENCES,0.7799442896935933,"In (´Smieja et al., 2018) a Gaussian Mixture Model, GMM, is used to model the density of the
covariates while jointly training a discriminator. In case of missing values, the discriminator’s ﬁrst
input neuron’s activations are set to the average activations as found over a conditional distribution
over the missing values in the GMM. We adapt the ofﬁcial code 2 to this experiment by adjusting
the number of hidden layers and number of mixture components while keeping the original training
setup as is."
REFERENCES,0.7827298050139275,"1https://github.com/mattjj/svae
2https://github.com/lstruski/Processing-of-missing-data-by-neural-networks"
REFERENCES,0.7855153203342619,Published as a conference paper at ICLR 2022
REFERENCES,0.7883008356545961,"In ﬁgures 10–13 decision surfaces and imputations are shown. The corresponding quantitative re-
sults are in tables 1–4."
REFERENCES,0.7910863509749304,"Figure 9: Samples from the MIWAE, ﬁtted to the data with missing values on the four 2D datasets.
The class color is determined by afterwards sampling from the discriminative distribution."
REFERENCES,0.7938718662952646,Published as a conference paper at ICLR 2022
REFERENCES,0.7966573816155988,Table 1: Half-moons test-set results
REFERENCES,0.7994428969359332,"model
test acc
- on missing
- on observed
test log p(y|x)
- on missing
- on observed"
REFERENCES,0.8022284122562674,"supMIWAE
0.88 ± 0.01
0.77 ± 0.01
1.00 ± 0.00
−0.42 ± 0.00
−0.52 ± 0.01
−0.32 ± 0.00
MIWAE
0.88 ± 0.00
0.77 ± 0.01
0.99 ± 0.00
−0.42 ± 0.01
−0.52 ± 0.01
−0.33 ± 0.01
0-impute
0.88 ± 0.01
0.77 ± 0.02
1.00 ± 0.00
−0.42 ± 0.00
−0.52 ± 0.01
−0.32 ± 0.00
learnable-imputation
0.89 ± 0.01
0.78 ± 0.01
1.00 ± 0.00
−0.42 ± 0.00
−0.52 ± 0.01
−0.32 ± 0.00
MICE
0.89 ± 0.01
0.77 ± 0.01
1.00 ± 0.00
−0.42 ± 0.00
−0.52 ± 0.01
−0.32 ± 0.00
missForest
0.87 ± 0.00
0.75 ± 0.01
0.99 ± 0.01
−0.45 ± 0.00
−0.53 ± 0.01
−0.37 ± 0.00
PPCA
0.88 ± 0.01
0.77 ± 0.02
1.00 ± 0.00
−0.42 ± 0.00
−0.52 ± 0.01
−0.32 ± 0.00
GB
0.88 ± 0.01
0.76 ± 0.02
1.00 ± 0.00
−0.42 ± 0.00
−0.52 ± 0.01
−0.32 ± 0.00
permutation-invariance
0.88 ± 0.01
0.77 ± 0.01
1.00 ± 0.00
−0.42 ± 0.00
−0.52 ± 0.01
−0.32 ± 0.01
(´Smieja et al., 2018)
0.83 ± 0.04"
REFERENCES,0.8050139275766016,Table 2: Burger test-set results
REFERENCES,0.807799442896936,"model
test acc
- on missing
- on observed
test log p(y|x)
- on missing
- on observed"
REFERENCES,0.8105849582172702,"supMIWAE
0.81 ± 0.01
0.64 ± 0.03
0.97 ± 0.01
−0.51 ± 0.00
−0.66 ± 0.01
−0.35 ± 0.01
MIWAE
0.78 ± 0.01
0.63 ± 0.03
0.93 ± 0.02
−0.52 ± 0.01
−0.67 ± 0.01
−0.38 ± 0.01
0-impute
0.80 ± 0.01
0.64 ± 0.03
0.96 ± 0.01
−0.51 ± 0.00
−0.66 ± 0.01
−0.35 ± 0.01
learnable-imputation
0.81 ± 0.01
0.64 ± 0.03
0.97 ± 0.01
−0.51 ± 0.00
−0.66 ± 0.01
−0.35 ± 0.01
MICE
0.80 ± 0.01
0.64 ± 0.03
0.96 ± 0.01
−0.51 ± 0.00
−0.66 ± 0.01
−0.35 ± 0.01
missForest
0.73 ± 0.02
0.54 ± 0.03
0.93 ± 0.02
−0.58 ± 0.01
−0.69 ± 0.02
−0.47 ± 0.02
PPCA
0.80 ± 0.01
0.64 ± 0.03
0.96 ± 0.01
−0.51 ± 0.01
−0.66 ± 0.01
−0.35 ± 0.01
GB
0.80 ± 0.01
0.63 ± 0.03
0.97 ± 0.01
−0.51 ± 0.01
−0.66 ± 0.01
−0.35 ± 0.01
permutation-invariance
0.81 ± 0.01
0.64 ± 0.03
0.97 ± 0.01
−0.51 ± 0.00
−0.66 ± 0.01
−0.35 ± 0.01
(´Smieja et al., 2018)
0.79 ± 0.06"
REFERENCES,0.8133704735376045,Table 3: Circles test-set results
REFERENCES,0.8161559888579387,"model
test acc
- on missing
- on observed
test log p(y|x)
- on missing
- on observed"
REFERENCES,0.8189415041782729,"supMIWAE
0.88 ± 0.01
0.77 ± 0.01
0.99 ± 0.01
−0.44 ± 0.01
−0.56 ± 0.01
−0.32 ± 0.01
MIWAE
0.87 ± 0.01
0.76 ± 0.02
0.99 ± 0.01
−0.45 ± 0.01
−0.56 ± 0.01
−0.33 ± 0.00
0-impute
0.88 ± 0.01
0.77 ± 0.02
0.99 ± 0.00
−0.44 ± 0.01
−0.56 ± 0.00
−0.33 ± 0.00
learnable-imputation
0.88 ± 0.01
0.77 ± 0.02
0.99 ± 0.01
−0.44 ± 0.00
−0.55 ± 0.01
−0.33 ± 0.01
MICE
0.88 ± 0.01
0.77 ± 0.02
0.99 ± 0.01
−0.44 ± 0.01
−0.56 ± 0.01
−0.33 ± 0.00
missForest
0.86 ± 0.01
0.75 ± 0.01
0.98 ± 0.01
−0.48 ± 0.01
−0.56 ± 0.01
−0.39 ± 0.01
PPCA
0.88 ± 0.01
0.77 ± 0.02
0.99 ± 0.00
−0.44 ± 0.00
−0.56 ± 0.01
−0.33 ± 0.00
GB
0.87 ± 0.01
0.76 ± 0.01
0.99 ± 0.00
−0.44 ± 0.01
−0.56 ± 0.01
−0.33 ± 0.00
permutation-invariance
0.88 ± 0.01
0.77 ± 0.02
0.99 ± 0.00
−0.44 ± 0.00
−0.56 ± 0.01
−0.32 ± 0.00
(´Smieja et al., 2018)
0.86 ± 0.02"
REFERENCES,0.8217270194986073,Table 4: Pin-wheel test-set results
REFERENCES,0.8245125348189415,"model
test acc
- on missing
- on observed
test log p(y|x)
- on missing
- on observed"
REFERENCES,0.8272980501392758,"supMIWAE
0.88 ± 0.01
0.77 ± 0.01
1.00 ± 0.00
−0.89 ± 0.01
−1.02 ± 0.01
−0.75 ± 0.00
MIWAE
0.88 ± 0.01
0.76 ± 0.01
1.00 ± 0.00
−0.88 ± 0.01
−1.01 ± 0.01
−0.75 ± 0.00
0-impute
0.88 ± 0.01
0.77 ± 0.01
1.00 ± 0.00
−0.88 ± 0.01
−1.02 ± 0.00
−0.75 ± 0.00
learnable-imputation
0.88 ± 0.00
0.77 ± 0.01
1.00 ± 0.00
−0.88 ± 0.01
−1.02 ± 0.01
−0.75 ± 0.00
MICE
0.88 ± 0.01
0.77 ± 0.01
1.00 ± 0.00
−0.88 ± 0.01
−1.02 ± 0.01
−0.75 ± 0.00
missForest
0.87 ± 0.00
0.75 ± 0.00
1.00 ± 0.00
−0.90 ± 0.01
−1.01 ± 0.00
−0.80 ± 0.00
PPCA
0.88 ± 0.01
0.77 ± 0.01
1.00 ± 0.00
−0.88 ± 0.01
−1.02 ± 0.01
−0.75 ± 0.00
GB
0.87 ± 0.01
0.74 ± 0.02
1.00 ± 0.00
−0.88 ± 0.01
−1.02 ± 0.01
−0.75 ± 0.00
permutation-invariance
0.88 ± 0.01
0.77 ± 0.01
1.00 ± 0.00
−0.88 ± 0.01
−1.02 ± 0.01
−0.75 ± 0.00"
REFERENCES,0.83008356545961,Published as a conference paper at ICLR 2022
REFERENCES,0.8328690807799443,"supMIWAE
MIWAE
0-impute
Perm. Inv.
(´Smieja et al., 2018)"
REFERENCES,0.8356545961002786,"Learnable Imputation
MICE
MissForest
PPCA
Gradient Boosting"
REFERENCES,0.8384401114206128,"Figure 10: First and third rows: (except top left) kernel density of the half-moons dataset together
with imputations from the given model. The supMIWAE does not rely on single imputations, in-
stead it draws multiple importance samples which are passed through the discriminator to give an
importance weighted prediction. For the supMIWAE multiple imputations at three different values
of the horizontal coordinate are shown and a kernel density of the multiple imputations are shown to
the left. Permutation invariance, the joint GMM and discriminator approach of ´Smieja et al. (2018)
and gradient boosting does not rely on explicit imputations, for the rest of the methods single im-
putations are shown in red. Second and fourth rows: Decision surfaces learnt, depending on the
strategy for handling missing values."
REFERENCES,0.841225626740947,Published as a conference paper at ICLR 2022
REFERENCES,0.8440111420612814,"supMIWAE
MIWAE
0-impute
Perm. Inv.
(´Smieja et al., 2018)"
REFERENCES,0.8467966573816156,"Learnable Imputation
MICE
MissForest
PPCA
Gradient Boosting"
REFERENCES,0.8495821727019499,"Figure 11: First and third rows:
(except top left) kernel density of the burger dataset together
with imputations from the given model. The supMIWAE does not rely on single imputations, in-
stead it draws multiple importance samples which are passed through the discriminator to give an
importance weighted prediction. For the supMIWAE multiple imputations at three different values
of the horizontal coordinate are shown and a kernel density of the multiple imputations are shown to
the left. Permutation invariance, the joint GMM and discriminator approach of ´Smieja et al. (2018)
and gradient boosting does not rely on explicit imputations, for the rest of the methods single im-
putations are shown in red. Second and fourth rows: Decision surfaces learnt, depending on the
strategy for handling missing values."
REFERENCES,0.8523676880222841,Published as a conference paper at ICLR 2022
REFERENCES,0.8551532033426184,"supMIWAE
MIWAE
0-impute
Perm. Inv.
(´Smieja et al., 2018)"
REFERENCES,0.8579387186629527,"Learnable Imputation
MICE
MissForest
PPCA
Gradient Boosting"
REFERENCES,0.8607242339832869,"Figure 12: First and third rows:
(except top left) kernel density of the circles dataset together
with imputations from the given model. The supMIWAE does not rely on single imputations, in-
stead it draws multiple importance samples which are passed through the discriminator to give an
importance weighted prediction. For the supMIWAE multiple imputations at three different values
of the horizontal coordinate are shown and a kernel density of the multiple imputations are shown to
the left. Permutation invariance, the joint GMM and discriminator approach of ´Smieja et al. (2018)
and gradient boosting does not rely on explicit imputations, for the rest of the methods single im-
putations are shown in red. Second and fourth rows: Decision surfaces learnt, depending on the
strategy for handling missing values."
REFERENCES,0.8635097493036211,Published as a conference paper at ICLR 2022
REFERENCES,0.8662952646239555,"supMIWAE
MIWAE
0-impute Permutation Invariance"
REFERENCES,0.8690807799442897,"Learnable Imputation
MICE
MissForest
PPCA
Gradient Boosting"
REFERENCES,0.871866295264624,"Figure 13: First and third rows: (except top left) kernel density of the pin-wheel dataset together
with imputations from the given model. The supMIWAE does not rely on single imputations, in-
stead it draws multiple importance samples which are passed through the discriminator to give an
importance weighted prediction. For the supMIWAE multiple imputations at three different values
of the horizontal coordinate are shown and a kernel density of the multiple imputations are shown
to the left. Permutation invariance and gradient boosting does not rely on explicit imputations, for
the rest of the methods single imputations are shown in red. Second and fourth rows:
Decision
surfaces learnt, depending on the strategy for handling missing values."
REFERENCES,0.8746518105849582,Published as a conference paper at ICLR 2022
REFERENCES,0.8774373259052924,"E.1
CAPACITY"
REFERENCES,0.8802228412256268,"The predictive performance of the discriminator at different capacities, in terms of number of hidden
units, is shown in ﬁgure 14."
REFERENCES,0.883008356545961,"Figure 14: Predictive performance when varying the capacity of the learner, in terms of hidden units."
REFERENCES,0.8857938718662952,Published as a conference paper at ICLR 2022
REFERENCES,0.8885793871866295,"F
IMAGE CLASSIFICATION"
REFERENCES,0.8913649025069638,"F.1
TRAINING DETAILS FOR MNIST/FMNIST"
REFERENCES,0.8941504178272981,"For all MNIST/fMNIST experiments, the discriminative model is a convolutional neural network
with four layers and a categorical distribution over classes, see table 5. The generative model uses
a convolutional architecture similar to the DCGAN (Radford et al., 2015) and a latent space of
dimension 128. A Continuous Bernoulli (Loaiza-Ganem & Cunningham, 2019) observation model
is used for MNIST and the discretized logistic distribution (Salimans et al., 2017) is used as the
observation model for Fashion MNIST. During training, K = 5 importance samples and a batch
size of 128 are used."
REFERENCES,0.8969359331476323,Table 5: Discriminative network for MNIST/fMNIST classiﬁcation.
REFERENCES,0.8997214484679665,Action (resulting layer size)
REFERENCES,0.9025069637883009,"Input x (28 × 28 × 1)
Conv2D(14 × 14 × 16)
Conv2D(7 × 7 × 32)
Conv2D(3 × 3 × 64)
Reshape(576)
Class probabilities: Dense(10)"
REFERENCES,0.9052924791086351,"F.2
IMPUTATIONS"
REFERENCES,0.9080779944289693,"Figures 15–17 show imputations on the two image datasets for the three different MCAR missing
processes. The ﬁrst column contains data with missing values. The second column is single im-
putation by the MIWAE, and following columns are MIWAE multiple imputations using sampling-
importance-resampling."
REFERENCES,0.9108635097493036,"F.3
NATURAL IMAGES (SVHN)"
REFERENCES,0.9136490250696379,"In order to assess classiﬁcation of natural images we now turn to the Street View House Numbers
dataset (SVHN, Netzer et al., 2011), with different missing mechanisms. In the “observed squares”
missing mechanism 20 × 20 randomly placed squares are observed while in the “missing squares”
missing mechanism 16×16 randomly placed squares are missing. In the “random dropout” missing
mechanism pixels are missing with probability m = 0.5."
REFERENCES,0.9164345403899722,"In the “observed squares” experiment we apply both a low capacity 4 layer discriminator (1 repeti-
tion) and a higher capacity 8 layer discriminator (3 repetitions) using the convolutional gated blocks
from Dauphin et al. (2017), see architectures in tables 7 and 8. In table 6, test set accuracies are
shown and for the 8 layer discriminator, the accuracy on the fully observed test set is also shown.
In the low capacity regime, the supMIWAE outperforms the two single imputation methods and
performs on par with MIWAE. With the higher capacity discriminator supMIWAE has performance
similar to 0-imputation and learnable imputation, while the MIWAE imputation is slightly worse.
This displays the same phenomenon as in the 2D classiﬁcation experiments, where powerful learn-
ers are able to undo single imputations. While the discriminative models learnt based on constant
imputation can achieve the same performance as the supMIWAE, the kinds of discriminative models
that are learnt are different. In this case, when applied to a fully observed test-set the discriminator
learnt by the supMIWAE outperforms the other models."
REFERENCES,0.9192200557103064,"The high capacity discriminator is also applied to the “missing squares” and “random dropout”
missing mechanisms, see ﬁgure 18 for the test-set accuracies (3 repetitions). In ﬁgure 19 the models
trained on data with missing values are applied to a fully observed test-set."
REFERENCES,0.9220055710306406,Published as a conference paper at ICLR 2022
REFERENCES,0.924791086350975,"(a) MNIST, Continuous Bernoulli.
(b) Fashion, discretized logistic."
REFERENCES,0.9275766016713092,"Figure 15: Observed squares. Left column contains data with missing values represented as red
pixels. Second column contains single imputations from the MWIAE using self-normalized impor-
tance sampling. Following columns contain multiple from the MIWAE using sampling-importance-
resampling."
REFERENCES,0.9303621169916435,Table 6: SVHN observed squares classiﬁcation accuracies
REFERENCES,0.9331476323119777,"acc (incomplete test set)
acc (complete test set)"
REFERENCES,0.935933147632312,"model
4 layers
8 layers
8 layers"
REFERENCES,0.9387186629526463,"supMIWAE
0.8264
0.8795 ± 0.0027
0.9268 ± 0.0015
MIWAE
0.8233
0.8745 ± 0.0024
0.9209 ± 0.0010
0-impute
0.8155
0.8819 ± 0.0024
0.9059 ± 0.0064
learnable-imputation
0.8139
0.8798 ± 0.0042
0.8953 ± 0.0234"
REFERENCES,0.9415041782729805,"G
REGRESSION"
REFERENCES,0.9442896935933147,"All discriminative models are neural networks with one hidden layer and 50 hidden units, where
the ﬁnal layer of the neural networks parameterizes a Gaussian distribution. The datasets are split
randomly 20 times with 90% of the data in a training set and 10% in a test set. A validation set
with 10% of the training data is used for early stopping and a batch size of 256 is used. For the
permutation invariant setup, an embedding of dimension U = 20 and ﬁxed length vectors ιPI(xobs)
of dimension M = 10 are used. Missing values are introduced completely at random in the training,
validation and test sets by removing each feature with probability m, where m is the missing rate.
In order to avoid reducing the sample size, due to completely missing covariates, whenever all the
covariates of an observation go missing, we set one of them to observed, selected at random."
REFERENCES,0.947075208913649,Published as a conference paper at ICLR 2022
REFERENCES,0.9498607242339833,"(a) MNIST, Continuous Bernoulli.
(b) Fashion, discretized logistic."
REFERENCES,0.9526462395543176,"Figure 16: Missing squares. Left column contains data with missing values represented as red
pixels. Second column contains single imputations from the MWIAE using self-normalized impor-
tance sampling. Following columns contain multiple from the MIWAE using sampling-importance-
resampling."
REFERENCES,0.9554317548746518,Table 7: Low capacity (4-layer) discriminative network for SVHN classiﬁcation.
REFERENCES,0.958217270194986,Action (resulting layer size)
REFERENCES,0.9610027855153204,"Input x (32 × 32 × 3)
Conv2D(16 × 16 × 16)
Conv2D(8 × 8 × 32)
Conv2D(4 × 4 × 64)
Reshape(1024)
Class probabilities: Dense(10)"
REFERENCES,0.9637883008356546,"For the generative models, 2 layered neural networks with 128 hidden units are used for the encoder
and the decoder and a latent space of dimension 10. A Gaussian observation model is used and
K = 25 importance samples are used during training."
REFERENCES,0.9665738161559888,"We compare different imputation techniques, modelling approaches and a non-deep competitor.
Zero-imputation, MissForest (Stekhoven & B¨uhlmann, 2012) and MICE (Buuren & Groothuis-
Oudshoorn, 2010) are different imputation techniques applied, before training the discriminative
model. Learnable imputation is using learnable constants as imputation, while permutation invari-
ance is utilizing a learnable embedding. Finally, histogram-based gradient boosting (Friedman,
2001), as implemented in sklearn (Pedregosa et al., 2011), is a non-deep baseline, using the valida-
tion set for early stopping. Results are seen in ﬁgure 20."
REFERENCES,0.9693593314763231,Published as a conference paper at ICLR 2022
REFERENCES,0.9721448467966574,"(a) MNIST, Continuous Bernoulli.
(b) Fashion, discretized logistic."
REFERENCES,0.9749303621169917,"Figure 17: Missing squares. Left column contains data with missing values represented as red
pixels. Second column contains single imputations from the MWIAE using self-normalized impor-
tance sampling. Following columns contain multiple from the MIWAE using sampling-importance-
resampling."
REFERENCES,0.9777158774373259,Table 8: High capacity (8-layer) discriminative network for SVHN classiﬁcation.
REFERENCES,0.9805013927576601,Action (resulting layer size)
REFERENCES,0.9832869080779945,"Input x (32 × 32 × 3)
Conv2D(16 × 16 × 16)
Conv2D(8 × 8 × 32)
Gated block (Dauphin et al., 2017) (8 × 8 × 32)
Gated block (Dauphin et al., 2017) (8 × 8 × 32)
Gated block (Dauphin et al., 2017) (8 × 8 × 32)
Gated block (Dauphin et al., 2017) (8 × 8 × 32)
Conv2D(4 × 4 × 64)
Reshape(1024)
Class probabilities: Dense(10)"
REFERENCES,0.9860724233983287,Published as a conference paper at ICLR 2022
REFERENCES,0.9888579387186629,"Figure 18: Test set accuracies on the SVHN dataset, with different missing mechanisms. Left
column: observed squares. Middle column: missing squares. Right column: random dropout with
missing rate 0.5."
REFERENCES,0.9916434540389972,"Figure 19: Fully observed test set accuracies on the SVHN dataset, with different missing mecha-
nisms. Left column: observed squares. Middle column: missing squares. Right column: random
dropout with missing rate 0.5."
REFERENCES,0.9944289693593314,Published as a conference paper at ICLR 2022
REFERENCES,0.9972144846796658,Figure 20: Test-set root mean square error on UCI datasets at varying missing rates.
