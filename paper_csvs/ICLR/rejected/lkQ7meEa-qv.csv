Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0035714285714285713,"Our sensory perception of the world is rich and multimodal. When we walk into
a cathedral, acoustics as much as appearance inform us of the sanctuary’s wide
open space. Similarly, when we drop a wineglass, the sound immediately informs
us as to whether it has shattered or not. In this vein, while recent advances in
learned implicit functions have led to increasingly higher quality representations
of the visual world, there have not been commensurate advances in learning
auditory representations. To address this gap, we introduce Neural Acoustic Fields
(NAFs), an implicit representation that captures how sounds propagate in a physical
scene. By modeling the acoustic properties of the scene as a linear time-invariant
system, NAFs continuously map all emitter and listener location pairs to an impulse
response function that can then be applied to new sounds. We demonstrate that
NAFs capture environment reverberations of a scene with high ﬁdelity and can
predict sound propagation for novel locations. Leveraging the scene structure
learned by NAFs, we also demonstrate improved cross-modal generation of novel
views of the scene given sparse visual views. Finally, the continuous nature of
NAFs enables potential downstream applications such as sound source localization.
Qualitative results: sites.google.com/view/nafs-iclr-2022/."
INTRODUCTION,0.007142857142857143,"1
INTRODUCTION"
INTRODUCTION,0.010714285714285714,"The sound of the ball leaving the bat, as much as its visible trajectory, tells us whether the hit is likely
to be a home run or not. Our experience of the world around us is rich and multimodal, depending
on integrated input from all of ﬁve sensory modalities – vision, touch, smell, hearing, and taste.
Understanding underlying scene structure relies on a continuous and diverse set of inputs: the visual
appearance and geometry of the physical world, the way sounds bound off of or are blocked by
surfaces, the way odors diffuse across the local environment, and the way objects feel as we walk on
them or touch them."
INTRODUCTION,0.014285714285714285,"Recent progress in implicit neural representations has enabled the construction of continuous, differ-
entiable representations of the visual world directly from raw image observations (Sitzmann et al.,
2019; Mildenhall et al., 2020; Niemeyer et al., 2020; Yariv et al., 2020). These models typically
utilize a neural renderer in combination with a learned implicit representation to jointly capture and
render images of a scene. By leveraging the multiview consistent nature of visual observations, these
methods can infer images of the same scene from novel viewpoints once trained on sparse views."
INTRODUCTION,0.017857142857142856,"However, our perception of the physical world is informed not only by our visual observations, but
also by the acoustics of environmental sounds. When we are in a large room, we expect to hear highly
reverberant echos (e.g., as in a cathedral). Moreover, given only the acoustic properties of sounds in
this space, we may infer a variety of other properties of the environment, for example, the location
of the emitted sound or the underlying room layout. As a preliminary step in learning the acoustic
properties of scenes, we explore modeling the underlying impulse response of audio reverberations."
INTRODUCTION,0.02142857142857143,"Past work has explored capturing the underlying acoustics of a scene (Raghuvanshi & Snyder, 2014;
2018; Chaitanya et al., 2020). These models, however, require manually designing acoustic functions
which, critically, prevent such approaches from being applied to arbitrary scenes. In this work, we
extend this approach by constructing an implicit neural representation which captures, in a generic
manner, the underlying acoustics of a scene. In particular, following (Raghuvanshi & Snyder, 2014),
we deﬁne the acoustic modeling problem as modeling the impulse-response a listener receives given
a sound emitted at an emitter location (as illustrated in Figure 1) and across all possible emitter-"
INTRODUCTION,0.025,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02857142857142857,"(a)
(b) (e)"
INTRODUCTION,0.03214285714285714,Emitter (f)
INTRODUCTION,0.03571428571428571,Emitter (g)
INTRODUCTION,0.039285714285714285,Emitter (h)
INTRODUCTION,0.04285714285714286,Emitter
INTRODUCTION,0.04642857142857143,Emitter
INTRODUCTION,0.05,"(c)
(d)"
INTRODUCTION,0.05357142857142857,Emitter
INTRODUCTION,0.05714285714285714,"Figure 1: Our neural acoustic ﬁelds learn an implicit representation for acoustic propagation. (a) A 3D top-down
view of the room. (b) Walkable regions shown in grey. (c) and (d) NAF captures the directional nature of sound.
For a single emitter location, the left and right ears experience different loudness. (e)-(h) NAF constructs a
continuous acoustic ﬁeld is created as we move the sound emitter in the scene. For (e) and (f) note that the sound
does not leak through the wall. For (g) and (h) note the portaling effect open doorways can have. Lighter colors
indicate a stronger response. Response strength is determined by a log of summed STFT magnitude components
for a ﬁxed emitter location."
INTRODUCTION,0.060714285714285714,"listener pairs in a scene. This learned representation inherently captures the pattern of all acoustic
reverberations in a scene."
INTRODUCTION,0.06428571428571428,"Learning a representation of scene acoustics poses several challenges compared to the visual setting.
First, how do we represent, in high ﬁdelity, the underlying impulse response at each emitter-listener
position? While we may represent the visual appearance of a scene with an underlying three-
dimensional vector, an acoustic reverberation (represented as an impulse response) can consist of
over 20000 values and, thus, is signiﬁcantly harder to capture. Second, how do we learn an acoustic
neural representation that densely generalizes to novel emitter-listener locations? In the visual setting,
ray-tracing can enforce view consistency across large portions of a visual scene (modulo occlusions).
While in principal, in a similar manner, we may reﬂect acoustic ""rays"" in our scene to obtain an
impulse response, a intractable number of rays are necessary to obtain the desired representation."
INTRODUCTION,0.06785714285714285,"To address both challenges, we propose Neural Acoustic Fields (NAFs). To capture the complex
signal representation of impulse responses, NAFs encode and represent an impulse-response in the
Fourier frequency domain. Motivated by the strong inﬂuence of nearby geometry on anisotropic
reﬂections (Raghuvanshi & Snyder, 2018), we propose to condition NAFs on local geometric
information present at both the listener and emitter locations when decoding the impulse response.
In our framework, local geometric information is learned directly from impulse responses. Such a
decomposition facilitates the transfer of local information captured from training emitter-listener
pairs to novel combinations of emitters and listeners"
INTRODUCTION,0.07142857142857142,"By modeling the dense acoustic ﬁelds of an environment, NAFs are a powerful method to extract
structural information about the scene, which we demonstrate is useful across a variety of different
tasks. In the cross-modal setting, we demonstrate how the learned acoustic structure can be utilized
to aid learned visual representations, improving novel view synthesis. Further, by directly utilizing
the acoustic information in NAFs, we demonstrate how one can localize different sound sources."
INTRODUCTION,0.075,"In summary, we present Neural Acoustic Fields (NAFs), a neural implicit representation which
captures the underlying acoustics of a scene in a compact and spatially continuous fashion. We show
that NAFs are able to outperform baselines in modeling scene acoustics, and provide detailed analysis
of the design choices in NAFs. We further illustrate how the structure learned by NAFs, can improve
cross-modal generation of novel visual views of a scene. Finally, we illustrate how the continuous
nature of NAFs enable the downstream application of sound source localization."
RELATED WORK,0.07857142857142857,"2
RELATED WORK"
RELATED WORK,0.08214285714285714,"Audio Field Coding
There is a rich history of encoding methods for 3D spatial audio. These
approaches largely fall into two categories. The ﬁrst approach encodes the sound ﬁeld at a user-
centric location by capturing the sound from spatially distributed sources (Gerzon, 1973; Breebaart"
RELATED WORK,0.08571428571428572,Under review as a conference paper at ICLR 2022
RELATED WORK,0.08928571428571429,"et al., 2005; Pulkki, 2007). While they may leverage perceptual cues that create the sense of
spatial audio, they do not allow the listener to freely traverse the scene and experience sound from
different locations. The second approach aims to model the sound heard as a listener moves in a
scene (Raghuvanshi & Snyder, 2014; Mehra et al., 2014; Raghuvanshi & Snyder, 2018; Chaitanya
et al., 2020). Since the complete acoustic ﬁeld of a scene is computationally prohibitive to simulate in
real time, and expensive to store in full ﬁdelity, these methods have relied on a handcrafted encoding
of the acoustic ﬁeld, prioritizing efﬁciency above reproduction ﬁdelity. Our work allows a listener
to move and experience sounds that come from anywhere in a scene, and can represent the acoustic
ﬁeld continuously at high ﬁdelity by directly learning from data."
RELATED WORK,0.09285714285714286,"Implicit representation
Our approach towards modeling the underlying acoustics a scene relies
on the use of a neural implicit representations. Implicit representations have emerged as a promising
representation of 3D geometry Niemeyer et al. (2019); Chen & Zhang (2019); Park et al. (2019);
Saito et al. (2019) and appearance Sitzmann et al. (2019); Mildenhall et al. (2020); Niemeyer et al.
(2020); Yariv et al. (2020) of a scene. Compared to traditional discrete representations, implicit
representations are a continuous mapping capable of capturing data at an ""inﬁnite resolution"". Jiang
et al. (2020) proposed a grid based representation for implicit scene reconstruction, while more
recently DeVries et al. (2021) has adopted spatial conditioning for 3D image synthesis, where in both
settings, the grid enables a higher-ﬁdelity encoding of the scene. Our work also leverages local grids
to model acoustics, but as an inductive bias and way to generalize to novel inputs."
RELATED WORK,0.09642857142857143,"Audio-Visual Learning
Our work is also closely related to joint modeling of vision and audio. By
leveraging the correspondence between vision and audio, work has been done to learn unsupervised
video and audio representations (Aytar et al., 2016; Arandjelovic & Zisserman, 2017), localize
objects that emit sound (Senocak et al., 2018; Zhao et al., 2018), and jointly use vision and audio
for navigation (Chen et al., 2020). Recent work aims to propose plausible reverberations or sounds
from image input (Singh et al., 2021; Du et al., 2021), these approaches model the phase-free log-
magnitude STFT using either convolution or implicit functions, which we also utilize. Different from
them, our work leverages the geometric features learned by modeling acoustic ﬁelds to improve the
learning of 3D view generation."
METHODS,0.1,"3
METHODS"
METHODS,0.10357142857142858,"We are interested in learning a generic acoustic representation of an arbitrary scene, which can
capture the underlying sound propagation of arbitrary sound sources across both seen and unseen
locations in a scene. We ﬁrst review relevant background information towards modeling environment
reverberations. We then describe Neural Acoustic Fields (NAFs), a neural ﬁeld which we show
can capture, in a generic manner, the acoustics of arbitrary scenes. We further discuss how we can
parameterize NAF in a manner so that it can capture acoustics property even at unseen sound sources
and listener positions. Finally, we discuss the underlying implementation details of our model."
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.10714285714285714,"3.1
BACKGROUND ON ENVIRONMENTAL REVERBERATION"
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.11071428571428571,"The sound emitted by a sound source undergoes decay, occlusion, and scattering due to both the
geometric and material properties of a scene. For a ﬁxed location pair (q, q′), we deﬁne the impulse-
response at a listener position q, as the sound pressure p(t; q, q′) induced by an impulse at q′. Such
behavior can be concisely and elegantly modeled utilizing the linear wave equation (Pierce, 2019):
 1 c2
∂2"
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.11428571428571428,"∂t2 −∇2

p(t, q, q′) = δ(t)δ(q −q′),
(1)"
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.11785714285714285,"where c is the speed of sound, p is the sound pressure, (q, q′) being the listener and emitter location
respectively, and δ the Dirac delta representing the forcing function, where we refer to sound pressure
p(t; q, q′) as the impulse-response observed at listener position q."
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.12142857142857143,"Given an accurate model of the impulse-response p(t; q, q′) described in Eqn (1), we may model
audio reverberation of any sound waveform s(t) emitted at q′, by computing the response r(t, q, q′)
at listener location q by querying the continuous ﬁeld and using temporal convolution:
r(t; q, q′) = s(t) ⊛p(t; q, q′)
(2)"
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.125,Under review as a conference paper at ICLR 2022
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.12857142857142856,Emitter
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.13214285714285715,Listener
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.1357142857142857,"Local feature grid
Feature per pixel corner"
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.1392857142857143,Listener feature
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.14285714285714285,Emitter feature
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.14642857142857144,"(x, y)
Listener"
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.15,"(x’, y’)
Emitter"
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.15357142857142858,"f
Phase"
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.15714285714285714,"t
Time"
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.16071428571428573,"θ
Orientation"
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.16428571428571428,"k
Left/Right"
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.16785714285714284,Global features
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.17142857142857143,Local features
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.175,"Implicit 
Decoder"
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.17857142857142858,∥generated - ground truth∥2
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.18214285714285713,MSE loss
BACKGROUND ON ENVIRONMENTAL REVERBERATION,0.18571428571428572,"Figure 2: Overview of NAF. Given a listener position and an emitter location, we ﬁrst query a grid for local
features using bilinear interpolation. We compute the sinusoidal embedding of the positions, phase, and time,
and query a discrete embedding matrix using the orientation and left/right ear. These features are fed to an
implicit decoder. Our method is trained with a MSE loss with impulse responses."
NEURAL ACOUSTIC FIELDS,0.18928571428571428,"3.2
NEURAL ACOUSTIC FIELDS"
NEURAL ACOUSTIC FIELDS,0.19285714285714287,"We are interested in constructing a continuous representation of the underlying acoustics of a scene,
which may specify the reverberation patterns of an arbitrary sound source. The parameterization of
an impulse-response introduced in Section 3.1 provides us with a method to model audio propagation
when given an omnidirectional listener and emitter. To construct a model of a directional listener, we
further model the 3D head orientation θ ∈R2, and ear k ∈{0, 1} (binary left or right) of a listener,
in addition to the spatial position q ∈R3 of the listener and q′ ∈R3 of the emitter."
NEURAL ACOUSTIC FIELDS,0.19642857142857142,"We may then model the time domain impulse response v using a neural ﬁeld Φ which takes as input
the listener and emitter parameters:"
NEURAL ACOUSTIC FIELDS,0.2,"Φ : R8 × {0, 1} →RT ,
(q, θ, k, q′) →Φ(q, θ, k, q′) = v
(3)"
NEURAL ACOUSTIC FIELDS,0.20357142857142857,"Generated impulse response STFT
Original impulse response STFT
Original wave
Generated wave (a) (b) (c)"
NEURAL ACOUSTIC FIELDS,0.20714285714285716,"Figure 3: Qualitative Visualization of Test Set Impulse Response Prediction. Left: Example log-STFT of
impulse responses and predictions from NAF. Right: (a)-(c) Three examples of ground truth waveform of the
impulse response, and the corresponding NAF generated waveform reconstructed with Grifﬁn-Lim.
Directly outputting the impulse-response v ∈RT with a neural network is difﬁcult to its high
dimensional (over 20000 elements) and chaotic nature. A naïve solution would be further add t as
an additional argument our neural ﬁeld, but we found that such a solution worked poorly, due to the
highly non-smooth representation of the waveform (see Table A2). We instead encode the impulse-
response utilizing a short-time Fourier transform (STFT) to create a log-magnitude spectrogram
denoted vSTFT, which we ﬁnd to be signiﬁcantly more amenable to neural network prediction, due
to the smooth nature of the frequency space. In Figure 3 we show spectrograms for ground truth
impulse responses and those learned by our network."
NEURAL ACOUSTIC FIELDS,0.21071428571428572,"Thus, our parameterization of NAF is a neural ﬁeld Ωthat is trained to estimate the impulse response
function φ, and outputs vSTFT for a given time and frequency coordinate:"
NEURAL ACOUSTIC FIELDS,0.21428571428571427,"Ω: R10 × {0, 1} →R,
(q, θ, k, q′, t, f) →Ω(q, θ, k, q′, t, f) ≈vSTFT(t, f)
(4)"
NEURAL ACOUSTIC FIELDS,0.21785714285714286,We train our model using MSE loss between the generated and ground truth log-spectrograms vSTFT:
NEURAL ACOUSTIC FIELDS,0.22142857142857142,"LNAF = ∥Ω(q, θ, k, q′, t, f) −vSTFT(t, f)∥2
(5)"
NEURAL ACOUSTIC FIELDS,0.225,across spectrogram coordinates t and f.
NEURAL ACOUSTIC FIELDS,0.22857142857142856,"In our qualitative results we utilize Grifﬁn-Lim reconstruction to recover v from vSTFT similar to
prior work (Du et al., 2021). While such an approximate reconstruction ignores the underlying phase"
NEURAL ACOUSTIC FIELDS,0.23214285714285715,Under review as a conference paper at ICLR 2022
NEURAL ACOUSTIC FIELDS,0.2357142857142857,"of the impulse response, phase free models are typically used in spatial acoustic modeling (Pulkki,
2007; Raghuvanshi & Snyder, 2018; Singh et al., 2021). Such approximations are used because the
phase locking cutoff in humans occurs at around 1∼2kHz (Shinn-Cunningham et al., 2000; Joris &
Verschooten, 2013), below which time delay provides important spatial cues."
NEURAL ACOUSTIC FIELDS,0.2392857142857143,"We demonstrate in our qualitative results that NAFs with this approximation may still accurately
model realistic spatial acoustic effects. Alternative methods for waveform recovery exist, including
methods that can be learned from data (Oord et al., 2016; Kalchbrenner et al., 2018). These approaches
are orthogonal to our work, and we leave this exploration to a future study."
GENERALIZATION THROUGH LOCAL GEOMETRIC CONDITIONING,0.24285714285714285,"3.3
GENERALIZATION THROUGH LOCAL GEOMETRIC CONDITIONING"
GENERALIZATION THROUGH LOCAL GEOMETRIC CONDITIONING,0.24642857142857144,"We are interested in parameterizing the underlying acoustic ﬁeld, so that we may not only accurately
represent impulse-response at emitter-listener pairs we see during training, but also at novel combina-
tions of emitter and listener seen at test time. Such generalization may be problematic when directly
parameterizing NAFs utilizing a MLP with inputs speciﬁed in Eqn (4), as the network may learn to
directly overﬁt and entangle the relation between emitter and listener impulse-responses."
GENERALIZATION THROUGH LOCAL GEOMETRIC CONDITIONING,0.25,"What generic information may we extract from a given impulse-response between an emitter and
listener? In principle, extracting the full dense geometric information in a scene would enable us
to robustly generalize to new emitter and listener locations. However, the amount of geometric
information available in a particular impulse-response, especially for positions far away from either
current emitter and listener is limited, since these positions has little impact on the underlying
impulse-response. In contrast, the local geometry near either emitter and listener positions will
have a strong inﬂuence in the impulse-response, as much of the anisotropic reﬂection comes from
such geometry (Paasonen et al., 2017). Inspired by this observation, we aim to capture and utilize
local geometric information, near either emitter or listener locations, as a means to predict impulse-
responses across novel combinations."
GENERALIZATION THROUGH LOCAL GEOMETRIC CONDITIONING,0.25357142857142856,"To parameterize and represent these local geometric features, we learn a 2D grid of of spatial latents
which we illustrate in Figure 2. When predicting an impulse-response at a given emitter and offset
position, we query the learned grid features at both emitter and listener positions using bilinear
sampling, and provide it as additional context into our NAF network Ω. Such features provide rich
information on the impulse-response, enabling NAF to generalize better to unseen combinations
of both emitter and listener locations. In the rest of this work, we refer to the NAFs with local
geometric features as Ωgrid.We learn grid latent features jointly with the underlying parameters of
NAF. Additional details can be found in Appendix B."
GENERALIZATION THROUGH LOCAL GEOMETRIC CONDITIONING,0.2571428571428571,"Such a design choice, however, still requires us to consider how to further combine local geometric
information captured separately from either listeners or emitters. A naïve implementation would be
to maintain separate feature grids for both listener and emitter positions. Such an approach fails to
account for the fact that the local geometric information captured by emitter may also inform the
local geometric information around a listener. Examining Eqn (1), we note that it is in fact symmetric
with respect to exchanging either listener or emitter positions (Chaitanya et al., 2020), indicating that
the impulse-response does not change when listener and emitter are swapped. Such a result means
that we may in fact directly utilize the local geometric information captured near an emitter position
interchangeably for either emitters and listeners. Thus, we propose our local geometric information as
a single latent grid, which we show in our ablations outperform the naïve dual grid implementation."
EXPERIMENTS,0.26071428571428573,"4
EXPERIMENTS"
EXPERIMENTS,0.2642857142857143,"In this section, we demonstrate that our model can faithfully represent the acoustic impulse response
at seen and unseen locations. Additional ablation studies verify the importance of utilizing local
geometric features to enable test time generation ﬁdelity. Next, we demonstrate that learning acoustic
ﬁelds could facilitate improved visual representations when training images are sparse. Finally we
show that the learned NAF can be used for sound source localization."
EXPERIMENTS,0.26785714285714285,Under review as a conference paper at ICLR 2022
SETUP,0.2714285714285714,"4.1
SETUP"
SETUP,0.275,"For evaluating the learned acoustic ﬁelds, we use the Soundspaces dataset (Chen et al., 2020). This
dataset consists of Ri probe points for each scene, with each probe capable of representing an
emitter or listener location for up to O(R2
i ) emitter and listener pairs. The emitters are represented
as omnidirectional, while the listener acts as a stereo receiver that can have one of four different
orientations. The listeners and emitters are at ﬁxed height. For each scene, we holdout 10% of the
RIRs randomly as a test set."
SETUP,0.2785714285714286,"Our NAFs are trained on 6 representative scenes, selected such that 2 consist of multi-room layouts,
2 consist a single room with a non-rectangular walls, and 2 consist of a single room with rectangular
walls as in Figure 4. Each scene is trained for 400 epochs, which takes around 3.5 hours for the largest
scenes on four Nvidia V100s. In each batch, we sample 20 impulse responses, and randomly
select 2, 000 frequency & time pairs within each spectrogram. An initial learning rate of 5 × 10−4 is
used for the network, while an initial learning rate of 1 × 10−3 is used for the grid. We add a small
amount of noise sampled from N(0, 0.02) to each coordinate during training to prevent degenerate
solutions."
ARCHITECTURE DETAILS,0.28214285714285714,"4.2
ARCHITECTURE DETAILS"
ARCHITECTURE DETAILS,0.2857142857142857,"The Soundspaces dataset lacks the full parameterization of an acoustic ﬁeld described in Equation 4,
so we train NAF with a restricted parameterization that is available in the dataset. This allows for
two degrees of freedom along the x −y plane for the listener locations q ∈R2 and the emitter
location q′ ∈R2. The listener can assume four possible orientations θ ∈{0, 90, 180, 270}, while the
emitter is omnidirectional. In particular, we utilize a parameterization of Ωgrid which maps an input
tuple [x, y, x′, y′, f, t] ∈R8 × {0, 90, 180, 270} × {0, 1} to a single scalar value that represents the
intensity for a given time and phase in the STFT:"
ARCHITECTURE DETAILS,0.2892857142857143,"Ωgrid(x, y, θ, k, x′, y′, t, f) ⇒vSTFT(t, f)
(6)"
ARCHITECTURE DETAILS,0.29285714285714287,"To encode the rotation θ, as there are only 4 possible discrete rotations in the dataset, we directly
query into a learnable embedding matrix of shape R4×k, returning a R1×k vector. Similarily, to
encode the left and right ear, we similarly query into a learnable embedding matrix of shape R2×k,
returning a R1×k vector. The f, t tuple representing the frequency and time respectively are scaled to
(−1, 1) and processed with sinusoidal encoding using 10 frequencies of sin and cos up to 200Hz."
ARCHITECTURE DETAILS,0.29642857142857143,"To obtain local geometric features for either a emitter or listener in a scene, we assume that our scene
is contained with a set of bounding pixels P = {P1...Pk} which form a grid over the scene. For a
given position tuple (x, y), we then query the local features:"
ARCHITECTURE DETAILS,0.3,"(x, y) ⇒L(x, y; ˜f(p∗
1), ˜f(p∗
2), ˜f(p∗
3), ˜f(p∗
4))
(7)"
ARCHITECTURE DETAILS,0.30357142857142855,"Where L(·) is the bilinear interpolation function, (p∗
1...p∗
4) are the four vertices that bound (x, y),
and ˜f(·) represents the features stored at a given vertex. These queried features are combined with
the coordinates processed with sinusoidal encoding using 10 frequencies of sin and cos functions up
to 100Hz. We process both the listener and emitter position tuples this way. We combine the grid
based features with the sinusoidal embeddings and the discrete indexed embeddings as the input to
our multilayer perceptron fφ. Please refer to Figure 2 for a visualization of our model, and Appendix
B for further details. We compare using a shared local geometric feature with the emitter and listener,
as well as using have the emitter and listener query their own individual grids."
EVALUATION ON NEURAL ACOUSTIC FIELDS,0.30714285714285716,"4.3
EVALUATION ON NEURAL ACOUSTIC FIELDS"
EVALUATION ON NEURAL ACOUSTIC FIELDS,0.3107142857142857,"We ﬁrst validate that we can capture environmental acoustics with high ﬁdelity, at unseen emitter-
listener positions."
EVALUATION ON NEURAL ACOUSTIC FIELDS,0.3142857142857143,"Baselines. We compare our model against several strong interpolation baselines. We experiment with
holding either the listener ﬁxed and interpolating based on the emitter location, as well as holding
the emitter ﬁxed and interpolating based on the listener location. Both linear and nearest neighbor
approaches are widely used (Savioja et al., 1999; Raghuvanshi et al., 2010; Pörschmann et al., 2020)
in modeling of spatial audio. We also compare sharing and using individual local geometric features."
EVALUATION ON NEURAL ACOUSTIC FIELDS,0.31785714285714284,Under review as a conference paper at ICLR 2022
EVALUATION ON NEURAL ACOUSTIC FIELDS,0.32142857142857145,"Model
Large 1
Large 2
Medium 1
Medium 2
Small 1
Small 2
Mean
MSE↓T60↓MSE↓T60↓MSE↓T60↓MSE↓T60↓MSE↓T60↓MSE↓T60↓MSE↓T60↓
Linear-Interp
3.349
7.357 3.831
8.975 3.108
7.515 3.425
7.958 3.601
9.741 6.487
11.62 3.967
8.862
Nearest-Interp 3.208
6.738 3.568
7.897 2.813
4.526 2.811
4.902 2.691
4.198 2.625
3.963 2.952
5.371
NAF (Dual)
1.399
4.311 1.423
4.880 1.376
5.004 1.383
4.200 1.398
6.687 1.533
3.801 1.419
4.814
NAF (Shared) 1.395
4.160 1.421
5.247 1.340
3.801 1.321
3.886 1.397
6.752 1.534
3.770 1.401
4.602"
EVALUATION ON NEURAL ACOUSTIC FIELDS,0.325,"Table 1: Quantitative Results on Test Set Accuracy. We report the MSE difference between generated and
ground truth log spectrograms across methods, as well as the percentage (%) difference for the T60 reverberation
time. The best method for each room is bolded. For the nearest and linear baselines, we perform interpolation in
the time domain using samples from the training set. (a) (b) (c)"
EVALUATION ON NEURAL ACOUSTIC FIELDS,0.32857142857142857,"Large 1
Large 2
Medium 1
Medium 2
Small 1
Small 2"
EVALUATION ON NEURAL ACOUSTIC FIELDS,0.33214285714285713,Emitter
EVALUATION ON NEURAL ACOUSTIC FIELDS,0.3357142857142857,Emitter
EVALUATION ON NEURAL ACOUSTIC FIELDS,0.3392857142857143,Emitter
EVALUATION ON NEURAL ACOUSTIC FIELDS,0.34285714285714286,Emitter
EVALUATION ON NEURAL ACOUSTIC FIELDS,0.3464285714285714,Emitter
EVALUATION ON NEURAL ACOUSTIC FIELDS,0.35,Emitter
EVALUATION ON NEURAL ACOUSTIC FIELDS,0.3535714285714286,"Figure 4: Qualitative Visualization of Neural Acoustic Fields. (a) The 3D structure of the room. (b)
Walkable regions shown in grey. (c) Strength of predicted impulse response given a emitter location, lighter
color indicates louder sound."
EVALUATION ON NEURAL ACOUSTIC FIELDS,0.35714285714285715,"Results. We evaluate the results of our synthesis by measuring the mean squared error (MSE)
between the generated and the ground truth log-spectrograms, as well as measuring the percentage
error between the T60 reverberation time in the time domain. In this case, lower MSE and T60-error
values indicates a better result. As shown in Table 1, our NAFs achieve signiﬁcantly higher quality on
the modeling of unseen impulse responses compared to strong interpolation baselines. A comparison
of using shared and dual local geometric features indicates that despite having fewer learnable
parameters, we achieve better performance by sharing the local geometric features. Examples of
individual impulse responses generated by our model are shown in Figure 3. Figures 4 shows the
different scenes and the loudness change predicted by our NAFs. Our model is capable of predicting
smoothly varying acoustic ﬁelds that are affected by the physical surroundings."
EVALUATION ON NEURAL ACOUSTIC FIELDS,0.3607142857142857,"Figure 5: Local Geometric Conditioning.
Comparison of NAF with and without local
geometric conditioning trained with different
amounts of data."
EVALUATION ON NEURAL ACOUSTIC FIELDS,0.36428571428571427,"Generalization through Geometric Conditioning. We
next assess the impact of utilizing local geometric condi-
tioning as a means to generalize to novel combinations of
emitter-listener positions. On the ""Large 1"" room, in Fig-
ure 5 we evaluate test set MSE error when NAF is trained
with a limited percentage of the training data either with
or without local geometric conditioning. We ﬁnd that such
geometric conditioning enable substantially better test set
reconstruction error, with the underlying performance gap
increasing with less data."
CROSS-MODAL LEARNING,0.3678571428571429,"4.4
CROSS-MODAL LEARNING"
CROSS-MODAL LEARNING,0.37142857142857144,"When NAF is trained, as shown in Figure 6, a structured geometric representation emerges. This
structured latent stands in sharp contrast with the noisy structure learned by NeRF models. In this
experiment, we explore the effect of jointly learning acoustics and visual information when we are
given sparse visual information. Recall that our NAF includes a local geometric feature grid P that
covers the entire scene. For our cross-modal learning experiment, we jointly learn this feature grid
with a NeRF network modiﬁed to accept both local features along with the traditional sinusoidal"
CROSS-MODAL LEARNING,0.375,Under review as a conference paper at ICLR 2022
CROSS-MODAL LEARNING,0.37857142857142856,"Acoustics only
RGB only
RGB + Acoustics
Room Structure"
CROSS-MODAL LEARNING,0.3821428571428571,"Room 1
Room 2"
CROSS-MODAL LEARNING,0.38571428571428573,"Figure 6: Qualitative Grid Visualization. Visualization of the learned grid features with principal component
analysis. When trained using acoustic information, the grid learns highly structured information. When using
NeRF, the grid is highly noisy. Structure emerges when we train both frameworks together."
CROSS-MODAL LEARNING,0.3892857142857143,"embedding. In the acoustics branch, we query the grid using emitter and listener positions. In the
NeRF branch, we use point samples along the ray projected on the grid plane to query the features.
In both cases, the process is fully differentiable. We use a standard implementation of NeRF with
a coarse and ﬁne network. In both the cross-modal and RGB only experiments we augment the
ﬁne network with a learnable local feature grid. In the NeRF only setting, we minimize color C
reconstruction loss for a ray r over a batch of rays R: LRGB = P"
CROSS-MODAL LEARNING,0.39285714285714285,"r∈R || ˆC(r)−C(r)||2
2. In contrast, in
the NAF + NeRF experiment, we jointly minimize LRGB +LNAF, where LNAF is deﬁned in equation 5.
We utilize 64 coarse samples and 128 ﬁne samples for each ray, and sample 1024 rays per batch."
CROSS-MODAL LEARNING,0.3964285714285714,"Room 1
Room 2"
CROSS-MODAL LEARNING,0.4,"RGB only
RGB + Acoustics"
CROSS-MODAL LEARNING,0.4035714285714286,"(a)
(b)
(c)
(d)
(f)
(e)"
CROSS-MODAL LEARNING,0.40714285714285714,"Figure 7: Qualitative Visualization of Cross-Modal Image Generation. Qualitative comparison between
NeRF learned jointly with a NAF with RGB and acoustic supervision, and NeRF learned with only RGB
supervision. We observe fewer ﬂoating artifacts when jointly training with audio. (a)-(c) Three views from
""Large 1"". (d)-(f) Three views from ""Large 2""."
CROSS-MODAL LEARNING,0.4107142857142857,"Large Room 1
Large Room 2"
CROSS-MODAL LEARNING,0.4142857142857143,"PSNR ↑
MSE ↓
PSNR ↑
MSE ↓"
CROSS-MODAL LEARNING,0.41785714285714287,"Training Images
75
100
150
75
100
150
75
100
150
75
100
150"
CROSS-MODAL LEARNING,0.42142857142857143,"NeRF
25.41 27.36 29.85 6.618 3.506 1.740 25.70 27.74 29.34 6.921 3.905 2.185
NeRF + NAF
26.19 27.59 29.90 5.209 2.983 1.625 26.24 28.22 29.45 5.641 3.075 2.034"
CROSS-MODAL LEARNING,0.425,"Table 2: Quantitative Results on Cross-Modal Image Learning. Quantitative results on joint training of
NeRF and NAF jointly conditioned on a single local grid. We use very sparse training images in highly complex
scenes. When evaluated on 50 test images, we observe that cross-modal learning helps improve PSNR when the
visual training data is more sparse. MSE results are multiplied by 103.
Results. We train on the two large rooms in our training set. For each room 75, 100, 150 images are
used for training, while the same 50 images of novel views are used for all conﬁgurations during
testing. In Table 2 we observe that training with acoustic information helps improve the PSNR and
MSE of the visual output. This effect is more signiﬁcant when the training images are very sparse,
the NAF network helps less when there is sufﬁcient visual information. Qualitative results are shown
in Figure 7, we see there is a reduction of ﬂoaters in free space."
CROSS-MODAL LEARNING,0.42857142857142855,"We visualize the local features learned in Figure 6. While the features learned using only RGB
supervision lack coherent structure, the one we jointly learn with audio exhibits clear structure. To
assess the structure in grid latents, we ﬁt a linear layer to regress the room position of each latent. For"
CROSS-MODAL LEARNING,0.43214285714285716,Under review as a conference paper at ICLR 2022
CROSS-MODAL LEARNING,0.4357142857142857,"room 1 and 2 respectively, we achieve a R2 of 0.109 and 0.085 when trained on RGB only, compared
to a R2 of 0.385 and 0.201 when trained with RGB + Acoustics supervision. This demonstrates the
spatially informative structure captured by jointly learning NAF and NeRF."
CROSS-MODAL LEARNING,0.4392857142857143,"(b)
(c)"
CROSS-MODAL LEARNING,0.44285714285714284,Emitter
CROSS-MODAL LEARNING,0.44642857142857145,Emitter
CROSS-MODAL LEARNING,0.45,"Emitter
Emitter"
CROSS-MODAL LEARNING,0.45357142857142857,"(a)
(d)
Figure 8: Qualitative Visualization of Sound Localization. Our NAFs can localize where a sound is coming
from given listener samples. (a)-(d) Given unperturbed audio, the NAF can model the sound by placing the
source anywhere in the scene. For each grid location as emitter, we evaluate how well the listener response
matches the observation. The actual emitter location is shown in red. Lighter color indicates that if an emitter
was placed here, there is lower error compared to observations, which matches well with the real emitter location."
CROSS-MODAL LEARNING,0.45714285714285713,"Method
Chords
Bells
Flute
Piano
Sweep"
CROSS-MODAL LEARNING,0.4607142857142857,"ResNet-18
0.658
0.476
0.916
0.983
0.188
Max mag.
0.126
0.393
0.919
0.225
0.531
NAFs
0.004
0.025
0.038
0.004
0.015"
CROSS-MODAL LEARNING,0.4642857142857143,"Table 3: Quantitative Results on Sound Localization. Quantitative results on sound localization distance in
normalized room coordinates. NAFs can accurately estimate location of the emitter."
SOUND SOURCE LOCALIZATION,0.46785714285714286,"4.5
SOUND SOURCE LOCALIZATION"
SOUND SOURCE LOCALIZATION,0.4714285714285714,"Given a high ﬁdelity representation of the acoustic ﬁeld, it is possible to identify the location of a
sound source in a scene, regardless of where the listener is located? In this task, we demonstrate that
a trained NAF can be used for localizing an emitter based on sparse audio samples from the scene.
We explore 5 sounds in total, 4 of which are musical instruments and 1 is a sine sweep (Sweep)."
SOUND SOURCE LOCALIZATION,0.475,"We assume that exists a set of audio recordings Vi ∈V from an object, recorded from 10 different
locations and orientations that are known. We further assume that we are given the original unper-
turbed waveform Vorig the object emits, with the underlying task being to locate the position of the
emitter (x′, y′) using these sound samples and the unperturbed sound."
SOUND SOURCE LOCALIZATION,0.4785714285714286,"We hold the listener position and orientations ﬁxed and vary the emitter position to probe for the
expected sound at each location. For each location, we render a log-spectrogram using our trained
NAF and compare this against the actual log-spectrogram recorded response Vi:"
SOUND SOURCE LOCALIZATION,0.48214285714285715,"|| log(stft(Vorig ⊛pi(t; x′))) −log(stft(Vi))||2
(8)"
SOUND SOURCE LOCALIZATION,0.4857142857142857,We perform the search for the emitter location by discretizing the scene in 0.3m segments.
SOUND SOURCE LOCALIZATION,0.48928571428571427,"Baselines. We compare against two baselines, a ResNet-18 network that takes as input both the
spectrogram of the transformed and original sounds, and is trained to regress the object location
relative to the listener frame. Given 10 individuals, we take an average of the predicted emitter
location. We also compare against a modiﬁed maximum magnitude baseline, where given a set of
recording locations, we simply select the position where the sound is loudest."
SOUND SOURCE LOCALIZATION,0.4928571428571429,"Results. Our NAFs can achieve very accurate localization as seen in Table 3. We also visualize the
error map with the ground truth emtiter location labeled in Figure 8. It can be observed that the real
location of the emitter matches well with the location of the lowest prediction error."
CONCLUSION,0.49642857142857144,"5
CONCLUSION"
CONCLUSION,0.5,"In summary, this paper introduces Neural Acoustic Fields (NAFs), a continuous, differentiable
acoustic representation which can faithfully represent the reverberation of different audio sources
in a scene, as well as the scene’s overall acoustic environment. By conditioning NAFs locally on
the underlying scene geometry, we demonstrate that our approach enables the accurate prediction of
environmental reverberations even at unseen locations in the scene. Furthermore, we demonstrate
that the acoustic representations learned through NAFs are powerful, and may be utilized to facilitate
audio-visual cross-modal learning, as well as to localize the underlying source of arbitrary sounds."
CONCLUSION,0.5035714285714286,Under review as a conference paper at ICLR 2022
REFERENCES,0.5071428571428571,REFERENCES
REFERENCES,0.5107142857142857,"Relja Arandjelovic and Andrew Zisserman. Look, listen and learn. In Proceedings of the IEEE
International Conference on Computer Vision, pp. 609–617, 2017. 3"
REFERENCES,0.5142857142857142,"Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Soundnet: Learning sound representations from
unlabeled video. Advances in neural information processing systems, 29:892–900, 2016. 3"
REFERENCES,0.5178571428571429,"Jeroen Breebaart, Jürgen Herre, Christof Faller, Jonas Rödén, Francois Myburg, Sascha Disch, Heiko
Purnhagen, Gerard Hotho, Matthias Neusinger, C Kjörling, et al. Mpeg spatial audio coding/mpeg
surround: Overview and current status. Preprint 119th Conv. Aud. Eng. Soc., (CONF), 2005. 2"
REFERENCES,0.5214285714285715,"Chakravarty R Alla Chaitanya, Nikunj Raghuvanshi, Keith W Godin, Zechen Zhang, Derek
Nowrouzezahrai, and John M Snyder. Directional sources and listeners in interactive sound
propagation using reciprocal wave ﬁeld coding. ACM Transactions on Graphics (TOG), 39(4):
44–1, 2020. 1, 3, 5"
REFERENCES,0.525,"Changan Chen, Unnat Jain, Carl Schissler, Sebastia Vicenc Amengual Gari, Ziad Al-Halah, Vamsi Kr-
ishna Ithapu, Philip Robinson, and Kristen Grauman. Soundspaces: Audio-visual navigation in 3d
environments. In ECCV, 2020. 3, 6"
REFERENCES,0.5285714285714286,"Zhiqin Chen and Hao Zhang. Learning implicit ﬁelds for generative shape modeling. In Proc. CVPR,
pp. 5939–5948, 2019. 3"
REFERENCES,0.5321428571428571,"Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W Taylor, and Joshua M
Susskind. Unconstrained scene generation with locally conditioned radiance ﬁelds. arXiv preprint
arXiv:2104.00670, 2021. 3"
REFERENCES,0.5357142857142857,"Yilun Du, M. Katherine Collins, B. Joshua Tenenbaum, and Vincent Sitzmann. Learning signal-
agnostic manifolds of neural ﬁelds. In Advances in Neural Information Processing Systems, 2021.
3, 4"
REFERENCES,0.5392857142857143,"Michael A Gerzon. Periphony: With-height sound reproduction. Journal of the audio engineering
society, 21(1):2–10, 1973. 2"
REFERENCES,0.5428571428571428,"Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nießner, and Thomas
Funkhouser. Local implicit grid representations for 3d scenes. In Proc. CVPR, pp. 6001–6010,
2020. 3"
REFERENCES,0.5464285714285714,"Philip X Joris and Eric Verschooten. On the limit of neural phase locking to ﬁne structure in humans.
Basic aspects of hearing, pp. 101–108, 2013. 5"
REFERENCES,0.55,"Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart,
Florian Stimberg, Aaron Oord, Sander Dieleman, and Koray Kavukcuoglu. Efﬁcient neural audio
synthesis. In International Conference on Machine Learning, pp. 2410–2419. PMLR, 2018. 5"
REFERENCES,0.5535714285714286,"Ravish Mehra, Lakulish Antani, Sujeong Kim, and Dinesh Manocha. Source and listener directivity
for interactive wave-based sound propagation. IEEE transactions on visualization and computer
graphics, 20(4):495–503, 2014. 3"
REFERENCES,0.5571428571428572,"Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and
Ren Ng. Nerf: Representing scenes as neural radiance ﬁelds for view synthesis. In Proc. ECCV,
2020. 1, 3"
REFERENCES,0.5607142857142857,"Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Occupancy ﬂow: 4d
reconstruction by learning particle dynamics. In Proc. ICCV, 2019. 3"
REFERENCES,0.5642857142857143,"Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric
rendering: Learning implicit 3d representations without 3d supervision. In Proc. CVPR, 2020. 1, 3"
REFERENCES,0.5678571428571428,"Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw
audio. arXiv preprint arXiv:1609.03499, 2016. 5"
REFERENCES,0.5714285714285714,Under review as a conference paper at ICLR 2022
REFERENCES,0.575,"Juhani Paasonen, Aleksandr Karapetyan, Jan Plogsties, and Ville Pulkki.
Proximity of sur-
faces—acoustic and perceptual effects. Journal of the Audio Engineering Society, 65(12):997–1004,
2017. 5"
REFERENCES,0.5785714285714286,"Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf:
Learning continuous signed distance functions for shape representation. In Proc. CVPR, 2019. 3"
REFERENCES,0.5821428571428572,"Allan D Pierce. Acoustics: an introduction to its physical principles and applications. Springer,
2019. 3"
REFERENCES,0.5857142857142857,"Christoph Pörschmann, Johannes M Arend, David Bau, and Tim Lübeck. Comparison of spherical
harmonics and nearest-neighbor based interpolation of head-related transfer functions. In Audio
Engineering Society Conference: 2020 AES International Conference on Audio for Virtual and
Augmented Reality. Audio Engineering Society, 2020. 6"
REFERENCES,0.5892857142857143,"Ville Pulkki. Spatial sound reproduction with directional audio coding. Journal of the Audio
Engineering Society, 55(6):503–516, 2007. 3, 5"
REFERENCES,0.5928571428571429,"Nikunj Raghuvanshi and John Snyder. Parametric wave ﬁeld coding for precomputed sound propaga-
tion. ACM Transactions on Graphics (TOG), 33(4):1–11, 2014. 1, 3"
REFERENCES,0.5964285714285714,"Nikunj Raghuvanshi and John Snyder. Parametric directional coding for precomputed sound propa-
gation. ACM Transactions on Graphics (TOG), 37(4):1–14, 2018. 1, 2, 3, 5"
REFERENCES,0.6,"Nikunj Raghuvanshi, John Snyder, Ravish Mehra, Ming Lin, and Naga Govindaraju. Precomputed
wave simulation for real-time sound propagation of dynamic sources in complex scenes. In ACM
SIGGRAPH 2010 papers, pp. 1–11. 2010. 6"
REFERENCES,0.6035714285714285,"Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li.
Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In Proc. ICCV,
pp. 2304–2314, 2019. 3"
REFERENCES,0.6071428571428571,"Lauri Savioja, Jyri Huopaniemi, Tapio Lokki, and Ritta Väänänen. Creating interactive virtual
acoustic environments. Journal of the Audio Engineering Society, 47(9):675–705, 1999. 6"
REFERENCES,0.6107142857142858,"Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, and In So Kweon. Learning to localize
sound source in visual scenes. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 4358–4366, 2018. 3"
REFERENCES,0.6142857142857143,"Barbara G Shinn-Cunningham, Scott Santarelli, and Norbert Kopco. Tori of confusion: Binaural
localization cues for sources within reach of a listener. The Journal of the Acoustical Society of
America, 107(3):1627–1636, 2000. 5"
REFERENCES,0.6178571428571429,"Nikhil Singh, Jeff Mentch, Jerry Ng, Matthew Beveridge, and Iddo Drori. Image2reverb: Cross-model
reverb impulse response synthesis. In IEEE/CVF International Conference on Computer Vision
(ICCV), October 2021. 3, 5"
REFERENCES,0.6214285714285714,"Vincent Sitzmann, Michael Zollhöfer, and Gordon Wetzstein. Scene representation networks: Contin-
uous 3d-structure-aware neural scene representations. In Proc. NeurIPS 2019, 2019. 1, 3"
REFERENCES,0.625,"Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman.
Multiview neural surface reconstruction by disentangling geometry and appearance. Proc. NeurIPS,
2020. 1, 3"
REFERENCES,0.6285714285714286,"Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, and Antonio
Torralba. The sound of pixels. In Proceedings of the European conference on computer vision
(ECCV), pp. 570–586, 2018. 3"
REFERENCES,0.6321428571428571,Under review as a conference paper at ICLR 2022
REFERENCES,0.6357142857142857,Appendix
REFERENCES,0.6392857142857142,"A
ADDITIONAL VISUALIZATION OF ROOMS"
REFERENCES,0.6428571428571429,Emitter
REFERENCES,0.6464285714285715,Emitter
REFERENCES,0.65,Emitter
REFERENCES,0.6535714285714286,Emitter
REFERENCES,0.6571428571428571,Emitter
REFERENCES,0.6607142857142857,Emitter
REFERENCES,0.6642857142857143,Emitter
REFERENCES,0.6678571428571428,Emitter
REFERENCES,0.6714285714285714,Emitter
REFERENCES,0.675,Emitter
REFERENCES,0.6785714285714286,Emitter
REFERENCES,0.6821428571428572,Emitter
REFERENCES,0.6857142857142857,Emitter
REFERENCES,0.6892857142857143,Emitter
REFERENCES,0.6928571428571428,Emitter
REFERENCES,0.6964285714285714,Emitter
REFERENCES,0.7,Emitter
REFERENCES,0.7035714285714286,Emitter
REFERENCES,0.7071428571428572,Emitter
REFERENCES,0.7107142857142857,Emitter
REFERENCES,0.7142857142857143,Emitter
REFERENCES,0.7178571428571429,Emitter
REFERENCES,0.7214285714285714,Emitter
REFERENCES,0.725,"Figure A1: Additional Qualitative Predictions of NAF. Qualitative visualization of the loudness
map as predicted by NAF across four different rooms."
REFERENCES,0.7285714285714285,"We show additional NAF predictions of loudness as we move an emitter inside different rooms in
Figure A1. For each room, note how the sound is affected by the geometry. In wide open spaces the
sound is highly dispersed. While in thin structures the sound test to concentrate locally. As we move
farther from the source, the loudness of the sound decreases."
REFERENCES,0.7321428571428571,Under review as a conference paper at ICLR 2022
REFERENCES,0.7357142857142858,"B
ARCHITECTURE AND TRAINING DETAILS"
REFERENCES,0.7392857142857143,"We visualize the two alternative models that we experiment with, in Figure A2 is a network that uses
different local feature grids for the emitter and receiver. The network uses the emitter and listener to
positions to sample from the two different grids."
REFERENCES,0.7428571428571429,"In Figure A3 we show a model that does not utilize any kind of local geometry conditioning. The
listener, emitter, phase, and time input are transformed using sinusoidal embedding, while the
orientation and left/right are retrieved. All transformed inputs are directly fed to the network."
REFERENCES,0.7464285714285714,Listener
REFERENCES,0.75,Local listener feature grid
REFERENCES,0.7535714285714286,Feature per pixel corner
REFERENCES,0.7571428571428571,Listener feature
REFERENCES,0.7607142857142857,Emitter feature
REFERENCES,0.7642857142857142,"(x, y)
Listener"
REFERENCES,0.7678571428571429,"(x’, y’)
Emitter"
REFERENCES,0.7714285714285715,"f
Phase"
REFERENCES,0.775,"t
Time"
REFERENCES,0.7785714285714286,"θ
Orientation"
REFERENCES,0.7821428571428571,"k
Left/Right"
REFERENCES,0.7857142857142857,Global features
REFERENCES,0.7892857142857143,Local features
REFERENCES,0.7928571428571428,"Implicit 
Decoder"
REFERENCES,0.7964285714285714,∥generated - ground truth∥2
REFERENCES,0.8,MSE loss
REFERENCES,0.8035714285714286,Emitter
REFERENCES,0.8071428571428572,Local emitter feature grid
REFERENCES,0.8107142857142857,Feature per pixel corner
REFERENCES,0.8142857142857143,"Figure A2: Architecture of the model that uses emitter and listener speciﬁc local geometry condition-
ing."
REFERENCES,0.8178571428571428,"(x, y)
Listener"
REFERENCES,0.8214285714285714,"(x’, y’)
Emitter"
REFERENCES,0.825,"f
Phase"
REFERENCES,0.8285714285714286,"t
Time"
REFERENCES,0.8321428571428572,"θ
Orientation"
REFERENCES,0.8357142857142857,"k
Left/Right"
REFERENCES,0.8392857142857143,Global features
REFERENCES,0.8428571428571429,"Implicit 
Decoder"
REFERENCES,0.8464285714285714,∥generated - ground truth∥2
REFERENCES,0.85,MSE loss
REFERENCES,0.8535714285714285,Figure A3: Architecture of the model that uses no local geometry conditioning.
REFERENCES,0.8571428571428571,"Each network consists of 8 fully connected layers in a feedforward fashion, as well as a skip
connection consisting of two fully connected layers. The skip connection takes the input and adds
its output to that for the fourth intermediate layer. We utilize an intermediate feature size of 512,
and softplus as the activation function. The grid is of size R32×32×64, where 32 is the size of the
spatial dimensions, while 64 is the channel count. We initialize each element of the grid i.i.d. from
N(0,
1
√"
REFERENCES,0.8607142857142858,"64). For the network excluding the grid, we utilize an initial learning rate of 5e −4. For
the grid, we utilize an initial learning rate of 1e −3. The Adam optimizer is used when training
our network. We utilize a orientation embedding of shape R7×4×512 where 7 is the number of
intermediate outputs, 4 is the number of orientations, and 512 is the feature dimension. For the
left-right embedding, we use a shape of R7×2×512. We perform additive conditioning by adding a
R512 vector to each intermediate output for both the orientation and the left/right."
REFERENCES,0.8642857142857143,"For each scene, to generate a log-spectrogram for each impulse response, we compute the mean and
standard deviation µfreq, σfreq for each frequency in the log-spectrogram, and normalize the data prior
to training:"
REFERENCES,0.8678571428571429,vfreq = vfreq −µfreq
REFERENCES,0.8714285714285714,3.0 × σfreq
REFERENCES,0.875,"For the sinusoidal embedding, we utilize both cos and sin with 10 frequencies each for encoding
position, phase, and time. For encoding position we utilize a max frequency of 100Hz, while for
encoding time and frequency we utilize a max frequency of 200Hz."
REFERENCES,0.8785714285714286,"Since we do not know beforehand the time duration of an impulse response at an unseen location,
we compute the maximum impulse length for each scene and use this length to zero pad the training"
REFERENCES,0.8821428571428571,Under review as a conference paper at ICLR 2022
REFERENCES,0.8857142857142857,"impulse responses. Because the padded regions do not contain useful information, we want the
network to focus modeling efforts on the early regions of the impulse response. We achieve this
by stochastically padding the impulse response to maximum impulse length with 0.1 probability.
Because the implicit function is trained on individual (t, f) coordinates within a given vSTFT, training
samples do not need to be of the same length. During test time, we perform inference up to the
maximum duration of scene impulse response."
REFERENCES,0.8892857142857142,"C
DATASET VISUALIZATION"
REFERENCES,0.8928571428571429,"(a)
(b)"
REFERENCES,0.8964285714285715,"Figure A4: A room the emitter-listener probes. (a) The 3D structure of a room. b The probes marking
the location of emitters/listeners."
REFERENCES,0.9,"In Figure A4, we visualize both the room and underlying set of probe positions in the training data.
Due to occlusion and the geometry, even slightly moving the emitter or listener position can result
in different results. As we demonstrated in Table 7, both nearest neighbor and linear interpolation
perform poorly compared to our learned solution. In contrast, recovered acoustic ﬁelds from NAF
trained on these probe positions is substantially denser (Figure A1)."
REFERENCES,0.9035714285714286,"D
STORAGE COMPARISON"
REFERENCES,0.9071428571428571,"Method
Storage (MB)
Linear-Interp
5970
Nearest-Interp
5970
NAFs (Dual Local Feat)
27.07
NAFs (Shared Local Feat)
25.37"
REFERENCES,0.9107142857142857,"Table A1: Approximate on disk storage cost of different methods. We average the amount of
data required for different methods of inference for the six scenes. Note that the linear and nearest
interpolation methods require the entire training set, while the NAF based methods use constant
storage."
REFERENCES,0.9142857142857143,"We compare the averaged on disk storage cost of the different methods for inferring the spatial audio
using a precomputed training set in Table A1. Both linear and nearest interpolation methods require
access to the entire training set, while our NAF based approaches compactly encode the acoustic
scene into a ﬁxed size."
REFERENCES,0.9178571428571428,"E
VISUALIZATION OF INTERPOLATED METHODS"
REFERENCES,0.9214285714285714,"In this section, we visualize the loudness when an interpolation method is used. We compare nearest
neighbor interpolation with linear interpolation applied to the training set, both in the time domain.
The nearest neighbor method presents sudden step changes in loudness, while the linear interpolation
fails when the queried location falls outside of the convex hull of the training set. Also note the sound
leakage in (c)-(d) in the bottom wall when linear interpolation is used."
REFERENCES,0.925,Under review as a conference paper at ICLR 2022
REFERENCES,0.9285714285714286,Emitter
REFERENCES,0.9321428571428572,"Emitter
Emitter"
REFERENCES,0.9357142857142857,Emitter
REFERENCES,0.9392857142857143,"(a)
(b)
(c)
(d)
Figure A5: Visualization using interpolated sound. We show the loudness at different locations
using (a)-(b) nearest-neighbor interpolation and (c)-(d) linear interpolation."
REFERENCES,0.9428571428571428,"F
ALTERNATIVE REPRESENTATIONS"
REFERENCES,0.9464285714285714,"Representation
MSE
T60
Magnitude only
1.395 4.160
Magnitude + phase
1.439 5.254
Time domain
39.87 61.82"
REFERENCES,0.95,"Table A2: Learning different representations We compare learning magnitude only, jointly learning
magnitude and phase, as well as directly learning in the time domain."
REFERENCES,0.9535714285714286,"Ground truth
NAFs + Griffin-Lim
Time domain model (a) (b)"
REFERENCES,0.9571428571428572,"Figure A6: Qualitative Visualization of impulse waveforms at unseen emitter/listener locations. From
left to right, we show the ground truth waveform, the Grifﬁn-Lim recovered waveform, and the waveform learned
by a network in the time domain for two locations (a)-(b) not seen in training."
REFERENCES,0.9607142857142857,"Our current method follows prior work in learning in the log-magnitude STFT domain. In this section,
we investigate two possible alternatives: learning phase + log-magnitude, and directly learning in the
time domain. The MSE and T60 error percentage is presented in Table A2. We observe that jointly
modeling phase + log-magnitude degrades the performance slightly compared to modeling just the
log-magnitude, while modeling in the time domain performs poorly. We visualize the waveform at
two test set listener/emitter locations in Figure A6."
REFERENCES,0.9642857142857143,"G
L2 REGULARIZED GRID IN NERF"
REFERENCES,0.9678571428571429,"Large 1
Large 2
PSNR ↑MSE ↓
PSNR ↑MSE ↓
NeRF + grid + L2
22.69
6.956
24.86
7.128
NeRF + grid
25.41
6.618
25.70
6.921"
REFERENCES,0.9714285714285714,"Table A3: Regularizing the grid. In this experiment, we compare learning NeRF with a grid without
regularization, and with L2 regularization."
REFERENCES,0.975,"In Table A3 we compare NeRF that utilizes a grid and trained using image reconstruction loss, against
a variant where a L2 penalty with weight 1e −5 to ensure a smooth latent space is added to the image
reconstruction loss. There are 75 images used in the training set. We observe degraded performance
when we apply this penalty."
REFERENCES,0.9785714285714285,Under review as a conference paper at ICLR 2022
REFERENCES,0.9821428571428571,"H
INTERPOLATION OF LEFT/RIGHT LATENT"
REFERENCES,0.9857142857142858,"Left
Right (a)"
REFERENCES,0.9892857142857143,"Left
Right (b)"
REFERENCES,0.9928571428571429,"Figure A7: We interpolate linearly between left/right latents. Here we show two examples (a)-(b)
where we take linear steps between the latent representing the left ear, and the latent representing the
right ear. Pay attention to the onset (left) of each spectrogram."
REFERENCES,0.9964285714285714,"As shown in Figure A7, are the output for two locations when we take linear steps between the latent
representing the left ear, and the right ear. We observe that the output smoothly changes as we move
in the latent space."
