Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001443001443001443,"The rapidly increasing size of deep-learning models has caused renewed and grow-
1"
ABSTRACT,0.002886002886002886,"ing interest in alternatives to digital computers to dramatically reduce the energy
2"
ABSTRACT,0.004329004329004329,"cost of running state-of-the-art neural networks. Optical matrix-vector multipliers
3"
ABSTRACT,0.005772005772005772,"are best suited to performing computations with very large operands, which leads
4"
ABSTRACT,0.007215007215007215,"us to hypothesize that large Transformer models might achieve asymptotic energy
5"
ABSTRACT,0.008658008658008658,"advantages with optics over running digitally. To test this idea, we performed
6"
ABSTRACT,0.010101010101010102,"small-scale optical experiments with a prototype accelerator to demonstrate that
7"
ABSTRACT,0.011544011544011544,"Transformer operations can run on optical hardware despite noise and errors. Using
8"
ABSTRACT,0.012987012987012988,"experiment-calibrated simulations of our hardware, we studied the behavior of
9"
ABSTRACT,0.01443001443001443,"running Transformers optically, identifying scaling laws for model performance
10"
ABSTRACT,0.015873015873015872,"with respect to optical energy usage and estimating total system power consump-
11"
ABSTRACT,0.017316017316017316,"tion. We found that the optical energy per multiply-accumulate (MAC) scales as
12"
ABSTRACT,0.01875901875901876,"1
d where d is the Transformer width, an asymptotic advantage over digital sys-
13"
ABSTRACT,0.020202020202020204,"tems. Should well-engineered, large-scale optical hardware be developed, it might
14"
ABSTRACT,0.021645021645021644,"achieve a 100× energy-efficiency advantage for running some of the largest current
15"
ABSTRACT,0.023088023088023088,"Transformer models, and if both the models and the optical hardware are scaled
16"
ABSTRACT,0.024531024531024532,"to the quadrillion-parameter regime, optical computers could have a > 8, 000×
17"
ABSTRACT,0.025974025974025976,"energy-efficiency advantage over state-of-the-art digital-electronic processors (300
18"
ABSTRACT,0.027417027417027416,"fJ/MAC). We discussed how these results motivate and inform the construction of
19"
ABSTRACT,0.02886002886002886,"future optical accelerators and optics-amenable deep-learning approaches. With
20"
ABSTRACT,0.030303030303030304,"assumptions about future improvements to electronics and Transformer quantiza-
21"
ABSTRACT,0.031746031746031744,"tion techniques (5× cheaper memory access, double the digital–analog conversion
22"
ABSTRACT,0.03318903318903319,"efficiency, and 4-bit precision), we estimated that optical computers’ advantage
23"
ABSTRACT,0.03463203463203463,"against these digital processors could grow to > 100, 000×.
24"
INTRODUCTION,0.03607503607503607,"1
Introduction
25"
INTRODUCTION,0.03751803751803752,"Deep learning models’ exponentially increasing scale is both a key driver in advancing the state-of-
26"
INTRODUCTION,0.03896103896103896,"the-art and a cause of growing concern about their energy usage, speed, and practicality. This has led
27"
INTRODUCTION,0.04040404040404041,"to the development of hardware accelerators and model training/compression/design techniques for
28"
INTRODUCTION,0.04184704184704185,"efficient and fast inference on them.
29"
INTRODUCTION,0.04329004329004329,"While digital-electronic accelerators [47, 16, 8, 1, 17] can improve performance by some constant
30"
INTRODUCTION,0.044733044733044736,"factor, alternative analog computing platforms using optics have been proposed as a new paradigm
31"
INTRODUCTION,0.046176046176046176,"for better scalability [49, 7, 62, 41, 56, 24, 51]. Ideally, the scaling is asymptotically better than
32"
INTRODUCTION,0.047619047619047616,"digital systems in energy per MAC [18, 61, 53, 41]. But these optical neural networks (ONNs) have
33"
INTRODUCTION,0.049062049062049064,"additional complexities and limitations of their own such as low precision, noise, and analog/digital
34"
INTRODUCTION,0.050505050505050504,"data conversion overheads which depend on the access patterns of the model running (Figure 1).
35"
INTRODUCTION,0.05194805194805195,"Thus, advantageously accelerating any neural network architecture with ONNs is hard. Here, we
36"
INTRODUCTION,0.05339105339105339,"hope to answer whether Transformers’ efficient data-access patterns (wide layers, parallel/batched
37 GPU"
INTRODUCTION,0.05483405483405483,Inference Accelerr
INTRODUCTION,0.05627705627705628,Model Aﬀects ONN Precision and Energy
INTRODUCTION,0.05772005772005772,Optical Transformers
INTRODUCTION,0.05916305916305916,Model Dimension
INTRODUCTION,0.06060606060606061,Energy Cost
INTRODUCTION,0.06204906204906205,Architecture
INTRODUCTION,0.06349206349206349,"Shot noise sets lower bound 
for optical energy/MAC"
INTRODUCTION,0.06493506493506493,"Detect Optical 
Output
Optically 
Encoded Input"
INTRODUCTION,0.06637806637806638,"Mach-Zehnder
Interferometer Mesh
Spatially-Multiplexed
Free-Space Propagation
(Our Example System)"
INTRODUCTION,0.06782106782106782,"Crossbar Array
Wavelength-Multiplexed
Micro-Ring Weight Banks"
INTRODUCTION,0.06926406926406926,"Optical Matrix-Vector 
Multiplication Engine
Hypothesis:
O(n) energy-
efficiency 
advantage for 
large-scale models"
INTRODUCTION,0.0707070707070707,"Ground Truth
Systematic
Error"
INTRODUCTION,0.07215007215007214,ONN - O(n)
INTRODUCTION,0.0735930735930736,Digital/GPU - O(n )
INTRODUCTION,0.07503607503607504,"Optical
Shot Noise"
INTRODUCTION,0.07647907647907648,"Weight/Data 
Statistics"
INTRODUCTION,0.07792207792207792,"Data Access 
Patterns
Serial
Batched"
INTRODUCTION,0.07936507936507936,"?
(n-dim Vector)
(n-dim Vector)
(n x n Matrix) 2"
INTRODUCTION,0.08080808080808081,"Figure 1: Can Transformers Benefit From Running on Optical Hardware? Optical Neural Net-
works (ONNs) have been proposed as an alternative computing platform that can achieve asymptotic
energy-efficiency advantages over digital computers running neural networks. This is not a guarantee;
their behavior is affected by model architecture, statistics, and resilience to the noise/imprecision
of analog hardware. Thus, while there are many implementations of general-purpose optical matrix
accelerators (such as those depicted in the inset), there are still model-dependent challenges/tradeoffs
in realizing their purported advantages. We seek here to answer the question of how much today’s
enormous Transformer models can benefit from this technology, if at all. Our hypothesis is that
Transformers’ architecture and unique behaviors allow for ONN-enabled benefits that scale."
INTRODUCTION,0.08225108225108226,"token processing, etc.), trends in methods for scaling them, and sufficient effort to train them for
38"
INTRODUCTION,0.0836940836940837,"ONNs afford them the asymptotic energy-efficiency advantages of running optically.
39"
INTRODUCTION,0.08513708513708514,"Here we demonstrate how the popular Transformer architecture is able to run on ONN systems,
40"
INTRODUCTION,0.08658008658008658,"and estimate the potential benefits of doing so. To first verify that Transformers may run on these
41"
INTRODUCTION,0.08802308802308802,"systems despite their imprecision, we sampled operations from a Transformer and ran them on a real
42"
INTRODUCTION,0.08946608946608947,"spatial light modulator (SLM) based experimental system, and used the results to create a calibrated
43"
INTRODUCTION,0.09090909090909091,"simulation of the optical hardware, with the systematic error, noise, and imprecision of weights/inputs
44"
INTRODUCTION,0.09235209235209235,"we observed. Transformers running on the simulated hardware could perform nearly as well as those
45"
INTRODUCTION,0.09379509379509379,"running digitally, and could be far more efficient. We summarize our key contributions as follows:
46"
INTRODUCTION,0.09523809523809523,"• We demonstrated linear Transformer operations (the bulk of a Transformer’s computation)
47"
INTRODUCTION,0.09668109668109669,"running with sufficient accuracy on real optical hardware and in a matching simulation,
48"
INTRODUCTION,0.09812409812409813,"despite errors and noise.
49"
INTRODUCTION,0.09956709956709957,"• Via simulation, we established scaling laws for optical Transformer performance versus
50"
INTRODUCTION,0.10101010101010101,"optical energy usage, and optical energy usage versus model size.
51"
INTRODUCTION,0.10245310245310245,"• Based on our simulations and experiments we estimated an orders-of-magnitude energy
52"
INTRODUCTION,0.1038961038961039,"consumption advantage of full ONN accelerators versus state-of-the-art GPUs.
53"
INTRODUCTION,0.10533910533910534,"• We discussed Transformers’ suitability for optical acceleration, and more generally how
54"
INTRODUCTION,0.10678210678210678,"specific elements of DNN architecture affect the function of ONN systems running them.
55"
INTRODUCTION,0.10822510822510822,"• We identified the hardware and systems design challenges that future work on building ONN
56"
INTRODUCTION,0.10966810966810966,"accelerators should target.
57"
INTRODUCTION,0.1111111111111111,"While our experiments and simulations were based on specific hardware as a representative example,
58"
INTRODUCTION,0.11255411255411256,"our scope here is more general. We are interested in understanding how uniquely optical energy
59"
INTRODUCTION,0.113997113997114,"scaling and noise relate to Transformer performance and architecture. As such nearly all our findings
60"
INTRODUCTION,0.11544011544011544,"apply broadly to linear optical processors (and hopefully future ones), irrespective of their underlying
61"
INTRODUCTION,0.11688311688311688,"hardware implementation details.
62"
BACKGROUND AND RELATED WORK,0.11832611832611832,"2
Background and Related Work
63"
TRANSFORMER MODELS,0.11976911976911978,"2.1
Transformer Models
64"
TRANSFORMER MODELS,0.12121212121212122,"Transformers are models for processing sequential data based on multi-head attention. Transformers
65"
TRANSFORMER MODELS,0.12265512265512266,"consist of two-layer feed-forward blocks and multi-head attention (Figure 2) operations. Multi-
66"
TRANSFORMER MODELS,0.1240981240981241,"head attention computes relationships between sequence elements by deriving query, key, and
67"
TRANSFORMER MODELS,0.12554112554112554,"value sequences Q, K, V and computing dot products with a softmax nonlinearity in-between [60].
68"
TRANSFORMER MODELS,0.12698412698412698,"Transformers also leverage modern design elements such as additive residual skip connections [20]
69"
TRANSFORMER MODELS,0.12842712842712842,"and normalization layers [3]. A defining feature of Transformers is that entire sequences may be
70"
TRANSFORMER MODELS,0.12987012987012986,"processed in matrix-matrix products in parallel (instead of one token/input at a time).
71"
LARGE-SCALE DEEP LEARNING,0.13131313131313133,"2.2
Large-Scale Deep Learning
72"
LARGE-SCALE DEEP LEARNING,0.13275613275613277,"In the past few years, it has been found in particular that Transformer [60] architectures significantly
73"
LARGE-SCALE DEEP LEARNING,0.1341991341991342,"improve when sized up to billions or even trillions of parameters [6, 28, 10, 22, 59, 66], causing an
74"
LARGE-SCALE DEEP LEARNING,0.13564213564213565,"exponential growth of deep learning compute usage [48, 50]. These large-scale Transformers achieve
75"
LARGE-SCALE DEEP LEARNING,0.1370851370851371,"ever more impressive results in not only natural language processing, but also in other domains such
76"
LARGE-SCALE DEEP LEARNING,0.13852813852813853,"as computer vision [14, 36], graphs [30], and in multi-modal settings [27, 26, 44, 45, 65, 46], making
77"
LARGE-SCALE DEEP LEARNING,0.13997113997113997,"them a popular but expensive solution for many tasks—digital hardware’s energy efficiency (ie.
78"
LARGE-SCALE DEEP LEARNING,0.1414141414141414,"per-flop or per-inference cost) has not kept up with the growing FLOP requirements of state-of-the-art
79"
LARGE-SCALE DEEP LEARNING,0.14285714285714285,"deep learning models [50]. They also have transfer learning capabilities [42, 13, 43, 6, 37, 14],
80"
LARGE-SCALE DEEP LEARNING,0.1443001443001443,"allowing them to easily generalize to specific tasks, in some cases in a zero-shot setting where no
81"
LARGE-SCALE DEEP LEARNING,0.14574314574314573,"further training is necessary [6, 45, 33].
82"
OPTICAL ACCELERATORS,0.1471861471861472,"2.3
Optical Accelerators
83"
OPTICAL ACCELERATORS,0.14862914862914864,"Researchers have explored a wide variety of controllable optical systems which manipulate different
84"
OPTICAL ACCELERATORS,0.15007215007215008,"types of optical modes to effectively implement arbitrary matrix-vector multiplications, vector-vector
85"
OPTICAL ACCELERATORS,0.15151515151515152,"dot products [52, 2, 18, 55, 4, 61, 19, 39, 57], or convolutions [63, 15, 40, 64]. In this work, we adopt
86"
OPTICAL ACCELERATORS,0.15295815295815296,"the free-space multiplier [61, 55, 19] (Figure 2, top left) to demonstrate Transformer operations in
87"
OPTICAL ACCELERATORS,0.1544011544011544,"optical experiments and for our simulations. We selected this system because it has many of the same
88"
OPTICAL ACCELERATORS,0.15584415584415584,"behaviors as other ONN implementations, and aim to draw conclusions that could generally be useful
89"
OPTICAL ACCELERATORS,0.15728715728715728,"for those working with other ONN designs. Many ONN systems, including ours, share the following
90"
OPTICAL ACCELERATORS,0.15873015873015872,"typical traits:
91"
OPTICAL ACCELERATORS,0.16017316017316016,"Device Imprecision and Optical Shot Noise
Optical systems are subject to errors in both the
92"
OPTICAL ACCELERATORS,0.16161616161616163,"actual hardware and from photon detection. Detection of optical intensity in particular is subject to a
93"
OPTICAL ACCELERATORS,0.16305916305916307,"phenomenon known as shot noise where the detected value is Poisson distributed: given vectors x
94"
OPTICAL ACCELERATORS,0.1645021645021645,"and w, with the elements of x encoded as optical intensity, the output Y is distributed as:
95"
OPTICAL ACCELERATORS,0.16594516594516595,"Y ∼Poisson(w · x)
(1)"
OPTICAL ACCELERATORS,0.1673881673881674,"For other encoding schemes such as amplitude or phase encoding, equation 1 should be modified, but
96"
OPTICAL ACCELERATORS,0.16883116883116883,"the detection is still subject to shot noise.
97"
OPTICAL ACCELERATORS,0.17027417027417027,"Efficient Photon Usage
Shot noise, and therefore an optical dot product’s signal-to-noise ratio
98"
OPTICAL ACCELERATORS,0.1717171717171717,"(SNR, which serves as an effective bit precision) is related to the mean number of photons at the
99"
OPTICAL ACCELERATORS,0.17316017316017315,"output. The efficiency of photon usage can therefore grow with increasing multiply-accumulate
100"
OPTICAL ACCELERATORS,0.1746031746031746,"operations (MACs): the SNR for the product w · x is
101"
OPTICAL ACCELERATORS,0.17604617604617603,"SNR(Y ) =
E[Y ]
p"
OPTICAL ACCELERATORS,0.1774891774891775,"Var[Y ]
= √w · x =
p"
OPTICAL ACCELERATORS,0.17893217893217894,"E[Y ],
(2)"
OPTICAL ACCELERATORS,0.18037518037518038,"which explains this behavior; if the desired output precision does not change, constant photons are
102"
OPTICAL ACCELERATORS,0.18181818181818182,"required regardless of dot product size. Work on ONNs has studied this behavior in a variety of
103"
OPTICAL ACCELERATORS,0.18326118326118326,"scenarios [18, 41, 61, 53]. This efficient scaling is not a guarantee—the required number of photons
104"
OPTICAL ACCELERATORS,0.1847041847041847,"may be influenced by a model architecture’s activation/weight distributions, encoding schemes,
105"
OPTICAL ACCELERATORS,0.18614718614718614,"precision requirements, etc.
106"
OPTICAL ACCELERATORS,0.18759018759018758,"Optical Neural Network Energy Costs
The energy cost of optical neural networks is broken down
107"
OPTICAL ACCELERATORS,0.18903318903318903,"into the optical costs of performing MACs and the electrical costs of loading/detecting data, which
108"
OPTICAL ACCELERATORS,0.19047619047619047,"are usually dominant. Consider a product between two matrices, A ∈Rn×d, B ∈Rd×k. Such a
109"
OPTICAL ACCELERATORS,0.1919191919191919,"product results in loading (detecting) nd + dk (nk) scalars, and performing ndk MACs. If the energy
110"
OPTICAL ACCELERATORS,0.19336219336219337,"to electrically load (detect) a scalar is Eload (Edet), and to perform a MAC optically is Eoptical, then
111"
OPTICAL ACCELERATORS,0.19480519480519481,"the total energy is:
112"
OPTICAL ACCELERATORS,0.19624819624819625,"E = (nd + dk)Eload + nkEdet + ndkEoptical
(3)"
OPTICAL ACCELERATORS,0.1976911976911977,"This illustrates how ONNs may have asymptotic energy advantages over digital computers. Notice
113"
OPTICAL ACCELERATORS,0.19913419913419914,"that regardless of the number of reuses, all data is only loaded once in Equation 3. This is because
114"
OPTICAL ACCELERATORS,0.20057720057720058,"copying a vector’s data and transporting it is free optically. Meanwhile, Eoptical ideally scales as 1/d.
115"
OPTICAL ACCELERATORS,0.20202020202020202,"These properties make energy cost disproportional to the number of MACs, ndk. In other words,
116"
OPTICAL ACCELERATORS,0.20346320346320346,Edigital
OPTICAL ACCELERATORS,0.2049062049062049,"EONN ∼min(n, d).
117"
OPTICAL ACCELERATORS,0.20634920634920634,"Streaming Weights Versus Weights-In-Place
There are two approaches for loading
118"
OPTICAL ACCELERATORS,0.2077922077922078,"weights.Weights-in-place schemes involve loading them once, and re-using them for many inputs.
119"
OPTICAL ACCELERATORS,0.20923520923520925,"Alternatively, systems can employ streaming weights where at every computation the required weight
120"
OPTICAL ACCELERATORS,0.2106782106782107,"matrix is loaded. Our experimental system is a weights-in-place scheme. For weights-in-place
121"
OPTICAL ACCELERATORS,0.21212121212121213,"operations, the energy advantage scales as just Edigital"
OPTICAL ACCELERATORS,0.21356421356421357,"EONN ∼d.
122"
PREVIOUS OPTICAL NEURAL NETWORK ARCHITECTURES,0.215007215007215,"2.4
Previous Optical Neural Network Architectures
123"
PREVIOUS OPTICAL NEURAL NETWORK ARCHITECTURES,0.21645021645021645,"Previous work has considered deep learning models such as MLPs and convolutional networks
124"
PREVIOUS OPTICAL NEURAL NETWORK ARCHITECTURES,0.2178932178932179,"on benchmark tasks like MNIST [40, 61], and simulations of larger convolutional models such as
125"
PREVIOUS OPTICAL NEURAL NETWORK ARCHITECTURES,0.21933621933621933,"AlexNet [32] on more difficult datasets such as ImageNet [18]. This begs the question of how well
126"
PREVIOUS OPTICAL NEURAL NETWORK ARCHITECTURES,0.22077922077922077,"newer, larger models perform on optical systems.
127"
PREVIOUS OPTICAL NEURAL NETWORK ARCHITECTURES,0.2222222222222222,"2.5
Scalable Compression and Quantization of Large Language Models (LLMs)
128"
PREVIOUS OPTICAL NEURAL NETWORK ARCHITECTURES,0.22366522366522368,"Optical hardware’s low precision raises the question of whether scaled-up models could be quantized
129"
PREVIOUS OPTICAL NEURAL NETWORK ARCHITECTURES,0.22510822510822512,"sufficiently to run. Thankfully, continual research in LLM compression has progressively shown that
130"
PREVIOUS OPTICAL NEURAL NETWORK ARCHITECTURES,0.22655122655122656,"larger models do not have increasing precision requirements. For example, [34] found that larger
131"
PREVIOUS OPTICAL NEURAL NETWORK ARCHITECTURES,0.227994227994228,"Transformers can be compressed more easily, to the degree that it is more worthwhile to train large
132"
PREVIOUS OPTICAL NEURAL NETWORK ARCHITECTURES,0.22943722943722944,"ones and compress them over training smaller ones of the target size. Furthermore, [5] and [12]
133"
PREVIOUS OPTICAL NEURAL NETWORK ARCHITECTURES,0.23088023088023088,"demonstrated running Transformers at scale with int8 precision, and the recent work of [11] proposes
134"
PREVIOUS OPTICAL NEURAL NETWORK ARCHITECTURES,0.23232323232323232,"that 4-bit is optimal for nearly all model scales, except for the largest tested (175B parameters) where
135"
PREVIOUS OPTICAL NEURAL NETWORK ARCHITECTURES,0.23376623376623376,"3-bit was sometimes found to work better.
136"
OPTICAL TRANSFORMERS,0.2352092352092352,"3
Optical Transformers
137"
OPTICAL TRANSFORMERS,0.23665223665223664,"We designed models that are intentionally similar to other Transformers, with the goal of simulating
138"
OPTICAL TRANSFORMERS,0.23809523809523808,"their behavior (informed by some experimental measurements) and energy consumption on optical
139"
OPTICAL TRANSFORMERS,0.23953823953823955,"hardware. A summary of our approach and model is in Figure 2.
140"
ARCHITECTURE AND TASK,0.240981240981241,"3.1
Architecture and Task
141"
ARCHITECTURE AND TASK,0.24242424242424243,"We created optical Transformer models with a GPT2-like [43] architecture that replaces the GELU
142"
ARCHITECTURE AND TASK,0.24386724386724387,"[21] activation with ReLU6, which is known to improve low-precision model performance [31, 23, 29].
143"
ARCHITECTURE AND TASK,0.2453102453102453,"For language modelling, we used the raw Wikitext-103 dataset [38]. The models we simulated have
144"
ARCHITECTURE AND TASK,0.24675324675324675,"12 layers (consisting of multi-head attention and feed-forward blocks), operate on a context length
145"
ARCHITECTURE AND TASK,0.2481962481962482,"of 1024 tokens, use 12 attention heads, and have embedding dimension d varying from 192 to 1536.
146"
ARCHITECTURE AND TASK,0.24963924963924963,"The full details of the training technique, architecture, and hyperparameters are in Appendix A.
147"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2510822510822511,"3.2
Transformer Computations on Optical Hardware
148"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.25252525252525254,"We ran experiments using a real Transformer’s (we used the base-sized model with d = 768) weights
149"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.25396825396825395,"in order to characterize the behavior of an ONN system. We adopted as a representative example of
150"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2554112554112554,"an optical accelerator a spatial light modulator (SLM) based system which computes vector-vector
151"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.25685425685425683,"dot products [61]. Vectors are encoded on a display, and copies are shone through the SLM which
152"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2582972582972583,"has varying transmission corresponding to some data (ie. a weight matrix). The outputs of this
153"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2597402597402597,"Linear
Linear
Linear"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2611832611832612,LayerNorm
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.26262626262626265,Input (n)
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.26406926406926406,Output (n x v)
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.26551226551226553,"Embedding
(v x d)"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.26695526695526695,"Positional 
Encoding"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2683982683982684,Softmax
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2698412698412698,MatMul Add ...
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2712842712842713,MatMul
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2727272727272727,Linear ...
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2741702741702742,Linear ReLU6
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2756132756132756,Linear Add
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.27705627705627706,Linear
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2784992784992785,Softmax
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.27994227994227994,LayerNorm
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2813852813852814,"Multi-Head Attention (MHA) Layer
Feed-Forward (MLP) Layer"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2828282828282828,Encoder Layer
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2842712842712843,xL Layers
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2857142857142857,"Quantize
or LUT"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.28715728715728717,"Sample 
Noise"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2886002886002886,Output Input
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.29004329004329005,Experiment-Informed Accelerator Simulation
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.29148629148629146,"Weights
Light Encoding
Detection"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.29292929292929293,Optical Vector-Vector/Matrix-Vector Multiplication
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2943722943722944,"Experimental Accelerator
ONN System Energy Model"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2958152958152958,Optical Computation
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2972582972582973,"RAM
DAC
MOD"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.2987012987012987,"RAM
ADC
AMP
GPT2"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.30014430014430016,MT-NLG
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.30158730158730157,"Future Model
Incoming Light
SLM"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.30303030303030304,Camera
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.30447330447330445,Digital Operation
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.3059163059163059,Run Experimentally
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.30735930735930733,xh Attention Heads
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.3088023088023088,"Figure 2: Optical Transformer evaluation: prototype hardware; simulator model; Transformer
architecture. Bottom: typical Transformer architecture, but with ReLU6 activation. Top Left:
experimental spatial light modulator (SLM)-based accelerator setup. From some layers—marked
with a laser icon—we sampled dot products to run on real hardware. Top Middle: Linear operations,
in light blue, run on a simulated accelerator with noise/error. Lookup tables (LUT) allow simulation
using our setup’s supported weight/activation values. Top right: our model of energy consumption
for optical accelerators, based on assumptions and results from our experiment/simulations. The
model accelerator system consists of random-access memory (RAM), a analog/digital conversion
(DAC/ADC), light modulation (MOD), amplification (AMP)."
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.31024531024531027,"operation—element-wise products—are collected at detectors as the resultant dot products (Figure 2,
154"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.3116883116883117,"top left). We collected lookup tables (LUTs)—mappings of the available discrete levels in both the
155"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.31313131313131315,"display and SLM devices—and used them to train a “LUT-aware” optical Transformer model to run
156"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.31457431457431456,"on the setup. We then collected calibration curves, mappings from the detected output light intensity
157"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.31601731601731603,"to the actual neuron floating-point values. To do this, we ran many random dot products on the
158"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.31746031746031744,"hardware and collected pairs of detected values and digitally-computed ground-truth values. We then
159"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.3189033189033189,"fit the relationship linearly. We used high photon counts to eliminate shot noise, so deviation from
160"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.3203463203463203,"the linear fit was considered the hardware’s systematic error. Full details of experimental procedures
161"
TRANSFORMER COMPUTATIONS ON OPTICAL HARDWARE,0.3217893217893218,"and calibration are in Appendix B.
162"
SIMULATION OF OPTICAL HARDWARE,0.32323232323232326,"3.3
Simulation of Optical Hardware
163"
SIMULATION OF OPTICAL HARDWARE,0.3246753246753247,"Table 1: Summary of simulation configurations for different eval-
uation and training scenarios. For simulating optical hardware
we included all behaviors. For determining optical resource scal-
ing, we focused on shot noise, and ran a plain 8-bit model for
comparison."
SIMULATION OF OPTICAL HARDWARE,0.32611832611832614,"Setting
Op.
Shot Noise
Sys. Err.
LUT
4-Pass"
SIMULATION OF OPTICAL HARDWARE,0.32756132756132755,"Hardware
Simulation
QAT
✗
✗
✓
✗
Eval
✓
✓
✓
✓"
SIMULATION OF OPTICAL HARDWARE,0.329004329004329,"Optical
Scaling
Simulation"
SIMULATION OF OPTICAL HARDWARE,0.33044733044733043,"QAT
✗
✗
✗
✗
Eval
✓
✗
✗
✓
Int8
✗
✗
✗
✗"
SIMULATION OF OPTICAL HARDWARE,0.3318903318903319,"Informed by our experiments, we
164"
SIMULATION OF OPTICAL HARDWARE,0.3333333333333333,"constructed a simulation of the
165"
SIMULATION OF OPTICAL HARDWARE,0.3347763347763348,"optical hardware.
By simulat-
166"
SIMULATION OF OPTICAL HARDWARE,0.3362193362193362,"ing the hardware behavior di-
167"
SIMULATION OF OPTICAL HARDWARE,0.33766233766233766,"rectly we model how any arbi-
168"
SIMULATION OF OPTICAL HARDWARE,0.33910533910533913,"trary operation would behave if
169"
SIMULATION OF OPTICAL HARDWARE,0.34054834054834054,"run on the physical setup. This
170"
SIMULATION OF OPTICAL HARDWARE,0.341991341991342,"allows us to avoid the computa-
171"
SIMULATION OF OPTICAL HARDWARE,0.3434343434343434,"tionally demanding task of sim-
172"
SIMULATION OF OPTICAL HARDWARE,0.3448773448773449,"ulating much larger Transform-
173"
SIMULATION OF OPTICAL HARDWARE,0.3463203463203463,"ers to verify that our simulation
174"
SIMULATION OF OPTICAL HARDWARE,0.3477633477633478,"method works. We aimed to em-
175"
SIMULATION OF OPTICAL HARDWARE,0.3492063492063492,"ulate the noise, error, and preci-
176"
SIMULATION OF OPTICAL HARDWARE,0.35064935064935066,"sion that we observed in order to understand how well full Transformers would perform when running
177"
SIMULATION OF OPTICAL HARDWARE,0.35209235209235207,"on optical hardware. The configurations for different scenarios are summarized in Table 1. We also
178"
SIMULATION OF OPTICAL HARDWARE,0.35353535353535354,"evaluated the digital, 8-bit-QAT-trained model for comparison purposes.
179"
SIMULATION OF OPTICAL HARDWARE,0.354978354978355,"Hybrid Scheme
Pure optical systems cannot easily compute activation or normalization functions.
180"
SIMULATION OF OPTICAL HARDWARE,0.3564213564213564,"Thus we assumed LayerNorm, ReLU activations, and residual skip connections are performed digitally
181"
SIMULATION OF OPTICAL HARDWARE,0.3578643578643579,"at full precision. Thankfully, even in smaller models, linear computations are the overwhelming
182"
SIMULATION OF OPTICAL HARDWARE,0.3593073593073593,"majority (Section 4.3).
183"
SIMULATION OF OPTICAL HARDWARE,0.36075036075036077,"Non-Negative Weights and Inputs (“4-Pass” Multiplication)
An important limitation is that our
184"
SIMULATION OF OPTICAL HARDWARE,0.3621933621933622,"display and SLM only support non-negative values. The constraint of having all-positive data is
185"
SIMULATION OF OPTICAL HARDWARE,0.36363636363636365,"present in many but not all optical neural network systems.We worked around this by decomposing
186"
SIMULATION OF OPTICAL HARDWARE,0.36507936507936506,"products into sums/differences of products with non-negative operands. Consider a product between
187"
SIMULATION OF OPTICAL HARDWARE,0.3665223665223665,"matrices W and X. If we let W+ (X+) and W−(X−) be matrices with only the positive and negative
188"
SIMULATION OF OPTICAL HARDWARE,0.36796536796536794,"elements of W (X) respectively, then:
189"
SIMULATION OF OPTICAL HARDWARE,0.3694083694083694,"WX = W+X+ −|W−|X+ −W+|X−| + W−X−
(4)"
SIMULATION OF OPTICAL HARDWARE,0.3708513708513709,"Data Scaling
On the real system, we define a maximum activation/weight value as 1.0 and minimum
190"
SIMULATION OF OPTICAL HARDWARE,0.3722943722943723,"as 0.0. To simulate operation, the inputs and weights of every simulated NN layer are scaled to this
191"
SIMULATION OF OPTICAL HARDWARE,0.37373737373737376,"range, and then rescaled back afterwards.
192"
SIMULATION OF OPTICAL HARDWARE,0.37518037518037517,"Device Quantization
Real hardware may only have certain number of representable levels. To
193"
SIMULATION OF OPTICAL HARDWARE,0.37662337662337664,"emulate this behavior, we fine-tuned pretrained models using quantization-aware training [25](QAT)
194"
SIMULATION OF OPTICAL HARDWARE,0.37806637806637805,"and applied the following in simulation (hyperparameters in Appendix A):
195"
SIMULATION OF OPTICAL HARDWARE,0.3795093795093795,"• For optics-simulated layers, we emulated quantization to int8 (256 levels). Then, instead of
196"
SIMULATION OF OPTICAL HARDWARE,0.38095238095238093,"dequantizing, we used the integer values directly as indices into the LUTs that we gathered
197"
SIMULATION OF OPTICAL HARDWARE,0.3823953823953824,"from experiment.
198"
SIMULATION OF OPTICAL HARDWARE,0.3838383838383838,"• We also quantized weights, but with the SLM LUT. We clamped smaller values to 0.02 in the
199"
SIMULATION OF OPTICAL HARDWARE,0.3852813852813853,"simulation, as our SLM does not have a high extinction ratio, and the smallest transmission
200"
SIMULATION OF OPTICAL HARDWARE,0.38672438672438675,"is 0.02.
201"
SIMULATION OF OPTICAL HARDWARE,0.38816738816738816,"• Accumulation can be high precision, but we used int8 quantization for outputs, since
202"
SIMULATION OF OPTICAL HARDWARE,0.38961038961038963,"analog-digital conversion (ADC) is expensive in practice.
203"
SIMULATION OF OPTICAL HARDWARE,0.39105339105339104,"• We used both deterministic and stochastic rounding when quantizing, with similar results.
204"
SIMULATION OF OPTICAL HARDWARE,0.3924963924963925,"Systematic Errors
Issues like cross-talk, misalignment, defects in ONNs give rise to systematic
205"
SIMULATION OF OPTICAL HARDWARE,0.3939393939393939,"errors. We simulated such a constraint by adding Gaussian noise to simulated model outputs
206"
SIMULATION OF OPTICAL HARDWARE,0.3953823953823954,"(Figure 2), scaled relative to the mean sizes of the outputs, as this was the noise behavior we observed
207"
SIMULATION OF OPTICAL HARDWARE,0.3968253968253968,"experimentally (it is related to the rescaling of data between 0 and 1).
208"
SIMULATION OF OPTICAL HARDWARE,0.39826839826839827,"Optical Encoding and Shot Noise
We modeled optical encoding by subjecting layer outputs
209"
SIMULATION OF OPTICAL HARDWARE,0.3997113997113997,"to simulated shot noise (Figure 2), which differs from the systematic error model. Outputs were
210"
SIMULATION OF OPTICAL HARDWARE,0.40115440115440115,"scaled by a number such that the average photon number per feature (photons/MAC) was some
211"
SIMULATION OF OPTICAL HARDWARE,0.4025974025974026,"target value. Each of these features was used as the mean of a Poisson distribution, which we
212"
SIMULATION OF OPTICAL HARDWARE,0.40404040404040403,"sampled. These outputs were then scaled back down to represent neuron values. In the simulations
213"
SIMULATION OF OPTICAL HARDWARE,0.4054834054834055,"for optical scaling we used vanilla 8-bit QAT (no LUTs or systematic error, which can overwhelm
214"
SIMULATION OF OPTICAL HARDWARE,0.4069264069264069,"shot noise) to cleanly demonstrate the optical scaling properties—which are model-dependent and
215"
SIMULATION OF OPTICAL HARDWARE,0.4083694083694084,"not hardware-dependent—of Transformers.
216"
RESULTS,0.4098124098124098,"4
Results
217"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.41125541125541126,"4.1
Transformer Error Tolerance and Hardware-Simulation Accuracy
218"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.4126984126984127,"We determined experimentally that Transformer operations are able to run on real hardware without
219"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.41414141414141414,"severely degraded performance from systematic errors. The bottom four panels of Figure 3 are
220"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.4155844155844156,"histograms of the experimental differences from correct values. The simulated noise distributions
221"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.417027417027417,"(dotted lines) match well with the experimental data, which confirms that they are an accurate
222"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.4184704184704185,"representation of the real systematic error behavior. Figure 3 (top) is a map of the performance of the
223"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.4199134199134199,"simulated model over different configurations of the mean-relative (in percent) noise at every layer of
224"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.4213564213564214,"feed-forward and attention blocks. The model performs well with significant noise (experimental
225"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.4227994227994228,"noise levels marked with stars), within 1 perplexity from noise-free performance unless the noise is
226"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.42424242424242425,"very high. These results show that our digital model of the system is a plausible approximation of
227"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.42568542568542567,"how a real one might behave.
228"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.42712842712842713,Experiment
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.42857142857142855,"Experimental 
Error
Simulated 
Noise"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.43001443001443,"First FF
σ = 2.09%
First Attention
σ = 5.65% 20.25"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.4314574314574315,"Last FF
σ = 1.31%
Last Attention
σ = 5.99%"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.4329004329004329,"Figure 3: Comparison of experimental and simulated
noise models and simulated Optical Transformer noise
tolerance. Top: Simulated performance (Wikitext-103 vali-
dation perplexity (PPL)) versus percent mean-relative simu-
lated noise in feed-forward (FF) and attention (Attn) layers.
Systematic errors from experimental data marked with a star.
Bottom: comparison of simulated noise model to error from
experimental data. The Gaussian shape of the simulated error
behavior matches experiment accurately."
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.43434343434343436,"While 8-bit precision was used for
229"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.4357864357864358,"QAT, the optical Transformer can per-
230"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.43722943722943725,"form inference at lower precision, as
231"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.43867243867243866,"implied by its error tolerance.
To
232"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.4401154401154401,"study this further we conducted a sim-
233"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.44155844155844154,"ple ablation on the input and output
234"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.443001443001443,"precisions used at inference, on the 8-
235"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.4444444444444444,"bit-QAT base-sized model with LUT
236"
TRANSFORMER ERROR TOLERANCE AND HARDWARE-SIMULATION ACCURACY,0.4458874458874459,"in Appendix C.
237"
OPTICAL SCALING LAWS,0.44733044733044736,"4.2
Optical Scaling Laws
238"
OPTICAL SCALING LAWS,0.44877344877344877,"Optical Transformers achieve lan-
239"
OPTICAL SCALING LAWS,0.45021645021645024,"guage modelling performance close
240"
OPTICAL SCALING LAWS,0.45165945165945165,"to their digital counterparts’ when
241"
OPTICAL SCALING LAWS,0.4531024531024531,"shot-noise-limited at modest photon
242"
OPTICAL SCALING LAWS,0.45454545454545453,"budgets.
The perplexities on the
243"
OPTICAL SCALING LAWS,0.455988455988456,"Wikitext-103 validation set of vari-
244"
OPTICAL SCALING LAWS,0.4574314574314574,"ous optical Transformer models sim-
245"
OPTICAL SCALING LAWS,0.4588744588744589,"ulated with different total photon us-
246"
OPTICAL SCALING LAWS,0.4603174603174603,"age (amount used for input data) are
247"
OPTICAL SCALING LAWS,0.46176046176046176,"shown in Figure 4 (left). The curves
248"
OPTICAL SCALING LAWS,0.46320346320346323,"illustrate a tradeoff: larger models
249"
OPTICAL SCALING LAWS,0.46464646464646464,"need larger photon totals to function
250"
OPTICAL SCALING LAWS,0.4660894660894661,"well, and there are different optimal
251"
OPTICAL SCALING LAWS,0.4675324675324675,"model choices based on the photon
252"
OPTICAL SCALING LAWS,0.468975468975469,"budget. We define photons/MAC as
253"
OPTICAL SCALING LAWS,0.4704184704184704,"the total photon budget (amount at
254"
OPTICAL SCALING LAWS,0.47186147186147187,"input) divided by total MACs. The
255"
OPTICAL SCALING LAWS,0.4733044733044733,"percentage difference from the per-
256"
OPTICAL SCALING LAWS,0.47474747474747475,"formance at 10K photons/MAC (Fig-
257"
OPTICAL SCALING LAWS,0.47619047619047616,"ure 4, middle)—chosen to represent
258"
OPTICAL SCALING LAWS,0.47763347763347763,"an ideal high-precision scenario—is
259"
OPTICAL SCALING LAWS,0.4790764790764791,"roughly power-law scaled in pho-
260"
OPTICAL SCALING LAWS,0.4805194805194805,"tons/MAC for all models with trunca-
261"
OPTICAL SCALING LAWS,0.481962481962482,"tion near 10K; better performance can
262"
OPTICAL SCALING LAWS,0.4834054834054834,"be had with more photons, but with
263"
OPTICAL SCALING LAWS,0.48484848484848486,"diminishing returns, and the perfor-
264"
OPTICAL SCALING LAWS,0.4862914862914863,"mance matches or exceeds that of the
265"
OPTICAL SCALING LAWS,0.48773448773448774,"8-bit digital models’ when the photon
266"
OPTICAL SCALING LAWS,0.48917748917748916,"budget is not too low (∼102).
267"
OPTICAL SCALING LAWS,0.4906204906204906,"The models use fewer photons/MAC
268"
OPTICAL SCALING LAWS,0.49206349206349204,"as they scale, achieving the theoretical efficient scaling where the total per-dot-product photons
269"
OPTICAL SCALING LAWS,0.4935064935064935,"needed is constant. To study how photon usage scales, we determined how many photons it takes
270"
OPTICAL SCALING LAWS,0.494949494949495,"to reach the performance of 8-bit digital models. These values, in Figure 4 (right), decrease nearly
271 as 1"
OPTICAL SCALING LAWS,0.4963924963924964,"d—the total photons needed per dot product is constant (bottom dashed line). The Transformer
272"
OPTICAL SCALING LAWS,0.49783549783549785,"architecture clearly takes advantage of efficient optical scaling with larger model sizes. In fact,
273"
OPTICAL SCALING LAWS,0.49927849927849927,"smaller per-dot-product totals are required for the largest model, suggesting that larger Transformers
274"
OPTICAL SCALING LAWS,0.5007215007215007,"may require less output precision. This is consistent with other work which found that precision
275"
OPTICAL SCALING LAWS,0.5021645021645021,"requirements are constant or reduced with scale [34]. Meanwhile, the already low photon usage
276"
OPTICAL SCALING LAWS,0.5036075036075036,"of the largest model suggests that models larger than our simulations (>10B parameters) may use
277"
OPTICAL SCALING LAWS,0.5050505050505051,"<1 photon/MAC. This sub-photon operation works in optical systems [61, 53] and is in essence no
278"
OPTICAL SCALING LAWS,0.5064935064935064,"different at all from operation at higher photon counts (since the number summed at detection is still
279"
OPTICAL SCALING LAWS,0.5079365079365079,"high).
280"
OPTICAL SCALING LAWS,0.5093795093795094,"These empirical scaling results are tied to our specific configurations and training strategies. Depend-
281"
OPTICAL SCALING LAWS,0.5108225108225108,"ing on the scales and dynamic ranges of inputs and weights, different amounts of photons may be
282"
OPTICAL SCALING LAWS,0.5122655122655123,"transmitted to the output; the statistics of a model affect its efficiency. In Appendix H we explore a
283"
OPTICAL SCALING LAWS,0.5137085137085137,"different scheme, but the effects of different methods remains an interesting topic for future work.
284"
OPTICAL SCALING LAWS,0.5151515151515151,Constant photons/MAC
OPTICAL SCALING LAWS,0.5165945165945166,Constant dot-product total
-BIT PPL,0.5180375180375181,8-bit PPL
-BIT PPL,0.5194805194805194,"Figure 4: Simulations of Optical Transformer behavior with varying photon usage. Left:
Wikitext-103 validation-set perplexity (PPL) versus embedding dimension d and total photons used
for a single forward pass/inference. 8-bit digital model performance is shown with dashed lines.
Middle: perplexity degrades from ideal with fewer photons-per-MAC; the plot exhibits truncated
power-law scaling. Right: Scaling of number of photons needed for an Optical Transformer to
achieve the same perplexity as an 8-bit digital-electronic processor, versus model size."
-BIT PPL,0.5209235209235209,"Figure 5: Estimated energy usage of Transformer models on optical hardware for a single
forward pass/inference. Hypothetical future model designs are labelled FUTURE-*. Estimated
energy/MAC for digital systems is based on [47]. Trend for energy usage in optical systems (blue)
computed based on real models only. Inset: energy advantage of running on optics over estimated
NVIDIA A100 usage. The advantage grows with the model compute. M = 106, G = 109, T = 1012,
q = 1015 parameters."
ESTIMATED ENERGY USAGE,0.5223665223665224,"4.3
Estimated Energy Usage
285"
ESTIMATED ENERGY USAGE,0.5238095238095238,"The efficient photon scaling trend we observed in Section 4.2 suggests that Transformers running
286"
ESTIMATED ENERGY USAGE,0.5252525252525253,"on optical hardware could achieve significant energy efficiency advantages over running on digital
287"
ESTIMATED ENERGY USAGE,0.5266955266955267,"hardware. To understand the efficiency of Transformers on optical hardware, we designed an ONN
288"
ESTIMATED ENERGY USAGE,0.5281385281385281,"system based on current hardware that is like our experimental setup, with our measured precision
289"
ESTIMATED ENERGY USAGE,0.5295815295815296,"and photon scaling. It is an inference system with in-place weights which are loaded once and reused
290"
ESTIMATED ENERGY USAGE,0.5310245310245311,"forever, activations read from and written to SRAM for every layer, a 10 GHz light modulator array,
291"
ESTIMATED ENERGY USAGE,0.5324675324675324,"and an optical “core” which can perform 10M multiplications per cycle (this can be thought of as a
292"
ESTIMATED ENERGY USAGE,0.5339105339105339,"10 megapixel SLM). The photon-per-MAC scaling versus model dimension is taken to be the 1/d
293"
ESTIMATED ENERGY USAGE,0.5353535353535354,"scaling which we found was possible in our simulations, and we assumed that the model operates
294"
ESTIMATED ENERGY USAGE,0.5367965367965368,"with 5-bit input precision, 8-bit weight precision, and 7-bit output precision, as determined by our
295"
ESTIMATED ENERGY USAGE,0.5382395382395382,"study of low precision performance in Appendix C. We then calculated according to the approach
296"
ESTIMATED ENERGY USAGE,0.5396825396825397,"in Section 2.3. For electrical energy we assumed in-place weights and did not include the energy
297"
ESTIMATED ENERGY USAGE,0.5411255411255411,"for loading them. In Appendix D we explain all assumed energy quantities based on contemporary
298"
ESTIMATED ENERGY USAGE,0.5425685425685426,"hardware.
299"
ESTIMATED ENERGY USAGE,0.5440115440115441,"As models grow, running Transformers on optical hardware has a large and asymptotic efficiency
300"
ESTIMATED ENERGY USAGE,0.5454545454545454,"advantage over running on digital hardware. In Figure 5 we chart estimates of the forward pass energy
301"
ESTIMATED ENERGY USAGE,0.5468975468975469,"required for various models1, including a hypothetical family of large, dense Transformer models
302"
ESTIMATED ENERGY USAGE,0.5483405483405484,"designed in a similar fashion, which we label FUTURE-*. For comparison, we also chart various
303"
ESTIMATED ENERGY USAGE,0.5497835497835498,"digital systems [47] in different performance regimes, and a hypothetical “next generation” GPU
304"
ESTIMATED ENERGY USAGE,0.5512265512265512,"that can use ∼10 fJ/MAC. For small models, the optics-based system uses about the same energy,
305"
ESTIMATED ENERGY USAGE,0.5526695526695526,"but eventually gains an advantage that scales asymptotically with the number of MACs. For the
306"
ESTIMATED ENERGY USAGE,0.5541125541125541,"larger models, MT-NLG-530B and FUTURE-4q, the optics-based approach would have ∼140× and
307"
ESTIMATED ENERGY USAGE,0.5555555555555556,"∼8500× energy advantages over the current state-of-the-art GPU (NVIDIA A100) respectively.
308"
ESTIMATED ENERGY USAGE,0.556998556998557,"The breakdown of compute and energy costs by source is in Appendix E. In summary we found that
309"
ESTIMATED ENERGY USAGE,0.5584415584415584,"as models get larger the feed-forward layers require most of the computation, but that the energy of
310"
ESTIMATED ENERGY USAGE,0.5598845598845599,"data access in attention is still very expensive due to the many heads. This is because of the parallel
311"
ESTIMATED ENERGY USAGE,0.5613275613275613,"operation of the Transformer, where the linear layer weights can be re-used for many tokens at a time
312"
ESTIMATED ENERGY USAGE,0.5627705627705628,"(weights-in-place is not possible for attention, and there are h n × n attention maps to store). 2
313"
DISCUSSION,0.5642135642135642,"5
Discussion
314"
DISCUSSION,0.5656565656565656,"The results given in Section 4.3 on optical Transformers’ efficiency have implications for the design
315"
DISCUSSION,0.5670995670995671,"of future ONN hardware/software systems.
316"
DISCUSSION,0.5685425685425686,"In Appendix G we discuss in detail the specifications for an ONN system to run large Transformers, as
317"
DISCUSSION,0.56998556998557,"a target for future work in their design. In summary, we found: once matrix-matrix product operands
318"
DISCUSSION,0.5714285714285714,"exceed 104 ×104 in size the advantage is significant, and therefore a future ONN should implement at
319"
DISCUSSION,0.5728715728715729,"least this level of parallelism to achieve >100× efficiency improvements over current state-of-the-art
320"
DISCUSSION,0.5743145743145743,"GPUs (NVIDIA A100). Given the assumptions we made about weight-maintenance costs in making
321"
DISCUSSION,0.5757575757575758,"our estimates (5.6 µW per weight; see Appendix D), an Optical Transformer would need to operate in
322"
DISCUSSION,0.5772005772005772,"the regime where a single matrix-vector multiplication is performed every 0.1 nanoseconds. Current
323"
DISCUSSION,0.5786435786435786,"ONN prototypes either operate at low clock rate or at small scale. Thus building a full ONN system
324"
DISCUSSION,0.5800865800865801,"that realizes the potential benefit is still an open challenge.
325"
DISCUSSION,0.5815295815295816,"Future improvements in CMOS technology will be greatly beneficial. In Appendix F we estimate
326"
DISCUSSION,0.5829725829725829,"that future optics-based systems might achieve energy advantages of >100, 000× running models
327"
DISCUSSION,0.5844155844155844,"the size of FUTURE-4q (over 300 fJ/MAC).
328"
DISCUSSION,0.5858585858585859,"Our studies on Transformers illustrates more broadly the relationships between model design and
329"
DISCUSSION,0.5873015873015873,"ONN efficiency. Transformers sought to make large models run efficiently by exploiting hardware’s
330"
DISCUSSION,0.5887445887445888,"strengths in performing large, parallel, dense calculations, and improved in this aspect as they scaled.
331"
DISCUSSION,0.5901875901875901,"As a consequence, as Transformers continue to be optimized for parallel digital electronic hardware,
332"
DISCUSSION,0.5916305916305916,"they will continue to become even more efficient on optical hardware. More generally, architectures
333"
DISCUSSION,0.5930735930735931,"that perform more computations per data access (such as those focusing strongly on linear operations
334"
DISCUSSION,0.5945165945165946,"[58, 35]) will be most promising for optical implementation.
335"
DISCUSSION,0.5959595959595959,"Conclusion
We have demonstrated the ability of Transformer models to run accurately and effi-
336"
DISCUSSION,0.5974025974025974,"ciently on optical hardware through optical experiments and an experiment-informed simulation of
337"
DISCUSSION,0.5988455988455988,"the hardware. We examined Transformers’ scaling behavior with optics and used our findings to
338"
DISCUSSION,0.6002886002886003,"show that optical systems could have a large and asymptotic energy advantage over digital ones that
339"
DISCUSSION,0.6017316017316018,"grows with the model size. For example, we showed that optical hardware may achieve an over 100×
340"
DISCUSSION,0.6031746031746031,"energy advantage when running the largest Transformer models today (∼500 billion parameters) and
341"
DISCUSSION,0.6046176046176046,"that larger, future Transformers (∼4 quadrillion parameters) may be realized with an >8000× optical
342"
DISCUSSION,0.6060606060606061,"energy advantage. We believe our findings about the potential energy-efficiency of optical accelerator
343"
DISCUSSION,0.6075036075036075,"hardware strongly motivate the development of optical processors for large-scale deep learning with
344"
DISCUSSION,0.6089466089466089,"Transformers.
345"
DISCUSSION,0.6103896103896104,"1The recent PaLM [9] models used a modified architecture. For simpler comparison, we make our estimates
using a model with GPT-like architecture but with the PaLM model dimensions, which we call PaLM-Like.
2Trends in the design of real models have increasingly favored optics over time. Specifically, attention
loads/stores a n × n attention matrix for each of the h attention heads. Models with more MLP compute per
attention head have a larger overall ratio of computation to energy usage; larger d"
DISCUSSION,0.6118326118326118,"h is more efficient. The largest
GPT2 [43] uses d"
DISCUSSION,0.6132756132756133,"h = 64; GPT3 [6], 128; MT-NLG-530b [54], 160; and PaLM [9], 384."
REFERENCES,0.6147186147186147,"References
346"
REFERENCES,0.6161616161616161,"[1] Michael Andersch,
Greg Palmer,
Ronny Krashinsky,
Nick Stam,
Vishal Mehta,
347"
REFERENCES,0.6176046176046176,"Gonzalo Brito,
and Sridhar Ramaswamy.
NVIDIA Hopper architecture in-depth.
348"
REFERENCES,0.6190476190476191,"Technical
report,
March
2022.
URL
https://developer.nvidia.com/blog/
349"
REFERENCES,0.6204906204906205,"nvidia-hopper-architecture-in-depth/.
350"
REFERENCES,0.6219336219336219,"[2] William Andregg, Michael Andregg, Robert T Weverka, and Lionel Clermont. Wavelength
351"
REFERENCES,0.6233766233766234,"multiplexed matrix-matrix multiplier, April 19 2019. URL https://patents.google.com/
352"
REFERENCES,0.6248196248196248,"patent/US10274989B2/en. (U.S. Patent No. 10,274,989). U.S. Patent and Trademark Office.
353"
REFERENCES,0.6262626262626263,"[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. URL
354"
REFERENCES,0.6277056277056277,"https://arxiv.org/abs/1607.06450.
355"
REFERENCES,0.6291486291486291,"[4] Wim Bogaerts, Daniel Pérez, José Capmany, David A B Miller, Joyce Poon, Dirk Englund,
356"
REFERENCES,0.6305916305916306,"Francesco Morichetti, and Andrea Melloni. Programmable photonic circuits. Nature, 586
357"
REFERENCES,0.6320346320346321,"(7828):207–216, 2020. URL https://doi.org/10.1038/s41586-020-2764-0.
358"
REFERENCES,0.6334776334776335,"[5] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming
359"
REFERENCES,0.6349206349206349,"the challenges of efficient Transformer quantization. In Proceedings of the 2021 Conference
360"
REFERENCES,0.6363636363636364,"on Empirical Methods in Natural Language Processing, pages 7947–7969, Online and Punta
361"
REFERENCES,0.6378066378066378,"Cana, Dominican Republic, November 2021. Association for Computational Linguistics. URL
362"
REFERENCES,0.6392496392496393,"https://aclanthology.org/2021.emnlp-main.627.
363"
REFERENCES,0.6406926406926406,"[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
364"
REFERENCES,0.6421356421356421,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
365"
REFERENCES,0.6435786435786436,"Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
366"
REFERENCES,0.645021645021645,"Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
367"
REFERENCES,0.6464646464646465,"Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
368"
REFERENCES,0.6479076479076479,"Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.
369"
REFERENCES,0.6493506493506493,"URL https://arxiv.org/abs/2005.14165.
370"
REFERENCES,0.6507936507936508,"[7] H John Caulfield and Shlomi Dolev. Why future supercomputing requires optics. Nature
371"
REFERENCES,0.6522366522366523,"Photonics, 4(5):261–263, 2010. URL https://doi.org/10.1038/nphoton.2010.94.
372"
REFERENCES,0.6536796536796536,"[8] Cerebras
Systems.
Cerebras
systems:
Achieving
industry
best
AI
perfor-
373"
REFERENCES,0.6551226551226551,"mance through a systems approach.
Technical report, Apr 2021.
URL https:
374"
REFERENCES,0.6565656565656566,"//8968533.fs1.hubspotusercontent-na1.net/hubfs/8968533/Whitepapers/
375"
REFERENCES,0.658008658008658,"Cerebras-CS-2-Whitepaper.pdf.
376"
REFERENCES,0.6594516594516594,"[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
377"
REFERENCES,0.6608946608946609,"Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker
378"
REFERENCES,0.6623376623376623,"Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes,
379"
REFERENCES,0.6637806637806638,"Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson,
380"
REFERENCES,0.6652236652236653,"Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
381"
REFERENCES,0.6666666666666666,"Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier
382"
REFERENCES,0.6681096681096681,"Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
383"
REFERENCES,0.6695526695526696,"Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani
384"
REFERENCES,0.670995670995671,"Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,
385"
REFERENCES,0.6724386724386724,"Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei
386"
REFERENCES,0.6738816738816739,"Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
387"
REFERENCES,0.6753246753246753,"Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling
388"
REFERENCES,0.6767676767676768,"language modeling with Pathways, 2022. URL https://arxiv.org/abs/2204.02311.
389"
REFERENCES,0.6782106782106783,"[10] Aidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan
390"
REFERENCES,0.6796536796536796,"Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, George Bm
391"
REFERENCES,0.6810966810966811,"Van Den Driessche, Eliza Rutherford, Tom Hennigan, Matthew J Johnson, Albin Cassirer,
392"
REFERENCES,0.6825396825396826,"Chris Jones, Elena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol
393"
REFERENCES,0.683982683982684,"Vinyals, Marc’Aurelio Ranzato, Jack Rae, Erich Elsen, Koray Kavukcuoglu, and Karen
394"
REFERENCES,0.6854256854256854,"Simonyan.
Unified scaling laws for routed language models.
In Kamalika Chaudhuri,
395"
REFERENCES,0.6868686868686869,"Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Pro-
396"
REFERENCES,0.6883116883116883,"ceedings of the 39th International Conference on Machine Learning, volume 162 of Pro-
397"
REFERENCES,0.6897546897546898,"ceedings of Machine Learning Research, pages 4057–4086. PMLR, 17–23 Jul 2022. URL
398"
REFERENCES,0.6911976911976911,"https://proceedings.mlr.press/v162/clark22a.html.
399"
REFERENCES,0.6926406926406926,"[11] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws,
400"
REFERENCES,0.6940836940836941,"2022.
401"
REFERENCES,0.6955266955266955,"[12] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix
402"
REFERENCES,0.696969696969697,"multiplication for Transformers at scale, 2022. URL https://arxiv.org/abs/2208.07339.
403"
REFERENCES,0.6984126984126984,"[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
404"
REFERENCES,0.6998556998556998,"deep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer-
405"
REFERENCES,0.7012987012987013,"ence of the North American Chapter of the Association for Computational Linguistics: Human
406"
REFERENCES,0.7027417027417028,"Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,
407"
REFERENCES,0.7041847041847041,"Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.
408"
REFERENCES,0.7056277056277056,"URL https://aclanthology.org/N19-1423.
409"
REFERENCES,0.7070707070707071,"[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
410"
REFERENCES,0.7085137085137085,"Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
411"
REFERENCES,0.70995670995671,"Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
412"
REFERENCES,0.7113997113997114,"recognition at scale. ICLR, 2021.
413"
REFERENCES,0.7128427128427128,"[15] Johannes Feldmann, Nathan Youngblood, Maxim Karpov, Helge Gehring, Xuan Li, Maik
414"
REFERENCES,0.7142857142857143,"Stappers, Manuel Le Gallo, Xin Fu, Anton Lukashchuk, Arslan Sajid Raja, et al. Parallel
415"
REFERENCES,0.7157287157287158,"convolutional processing using an integrated photonic tensor core. Nature, 589(7840):52–58,
416"
REFERENCES,0.7171717171717171,"2021.
417"
REFERENCES,0.7186147186147186,"[16] Graphcore.
The
data
center
architecture
for
graphcore
computing.
Tech-
418"
REFERENCES,0.7200577200577201,"nical
report,
Apr
2021.
URL
https://www.graphcore.ai/hubfs/
419"
REFERENCES,0.7215007215007215,"Graphcore-Mk2-IPU-System-Architecture-GC.pdf.
420"
REFERENCES,0.7229437229437229,"[17] Habana Labs. HABANA® GAUDI®2 white paper. Technical report, June 2022. URL
421"
REFERENCES,0.7243867243867244,"https://habana.ai/wp-content/uploads/pdf/2022/gaudi2-whitepaper.pdf.
422"
REFERENCES,0.7258297258297258,"[18] Ryan Hamerly, Liane Bernstein, Alexander Sludds, Marin Soljaˇci´c, and Dirk Englund. Large-
423"
REFERENCES,0.7272727272727273,"scale optical neural networks based on photoelectric multiplication. Physical Review X, 9(2):
424"
REFERENCES,0.7287157287157288,"021032, 2019. URL https://doi.org/10.1103/PhysRevX.9.021032.
425"
REFERENCES,0.7301587301587301,"[19] Yoshio Hayasaki, Ichiro Tohyama, Toyohiko Yatagai, Masahiko Mori, and Satoshi Ishihara.
426"
REFERENCES,0.7316017316017316,"Optical learning neural network using Selfoc microlens array. Japanese Journal of Applied
427"
REFERENCES,0.733044733044733,"Physics, 31(5S):1689, 1992. URL https://doi.org/10.1143/JJAP.31.1689.
428"
REFERENCES,0.7344877344877345,"[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
429"
REFERENCES,0.7359307359307359,"recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
430"
REFERENCES,0.7373737373737373,"IEEE, June 2016. doi: 10.1109/cvpr.2016.90. URL https://doi.org/10.1109/cvpr.2016.
431"
REFERENCES,0.7388167388167388,"90.
432"
REFERENCES,0.7402597402597403,"[21] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs), 2016. URL https:
433"
REFERENCES,0.7417027417027418,"//arxiv.org/abs/1606.08415.
434"
REFERENCES,0.7431457431457431,"[22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
435"
REFERENCES,0.7445887445887446,"Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom
436"
REFERENCES,0.746031746031746,"Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia
437"
REFERENCES,0.7474747474747475,"Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent
438"
REFERENCES,0.7489177489177489,"Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/
439"
REFERENCES,0.7503607503607503,"abs/2203.15556.
440"
REFERENCES,0.7518037518037518,"[23] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias
441"
REFERENCES,0.7532467532467533,"Weyand, Marco Andreetto, and Hartwig Adam. MobileNets: Efficient convolutional neural
442"
REFERENCES,0.7546897546897547,"networks for mobile vision applications. ArXiv, abs/1704.04861, 2017.
443"
REFERENCES,0.7561327561327561,"[24] Chaoran Huang, Volker J. Sorger, Mario Miscuglio, Mohammed Al-Qadasi, Avilash Mukherjee,
444"
REFERENCES,0.7575757575757576,"Lutz Lampe, Mitchell Nichols, Alexander N. Tait, Thomas Ferreira de Lima, Bicky A. Marquez,
445"
REFERENCES,0.759018759018759,"Jiahui Wang, Lukas Chrostowski, Mable P. Fok, Daniel Brunner, Shanhui Fan, Sudip Shekhar,
446"
REFERENCES,0.7604617604617605,"Paul R. Prucnal, and Bhavin J. Shastri. Prospects and applications of photonic neural networks.
447"
REFERENCES,0.7619047619047619,"Advances in Physics: X, 7(1), October 2021. doi: 10.1080/23746149.2021.1981155. URL
448"
REFERENCES,0.7633477633477633,"https://doi.org/10.1080/23746149.2021.1981155.
449"
REFERENCES,0.7647907647907648,"[25] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,
450"
REFERENCES,0.7662337662337663,"Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for
451"
REFERENCES,0.7676767676767676,"efficient integer-arithmetic-only inference. In Proceedings of the IEEE Conference on Computer
452"
REFERENCES,0.7691197691197691,"Vision and Pattern Recognition, pages 2704–2713, 2018. URL https://doi.org/10.1109/
453"
REFERENCES,0.7705627705627706,"CVPR.2018.00286.
454"
REFERENCES,0.772005772005772,"[26] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu,
455"
REFERENCES,0.7734487734487735,"David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff,
456"
REFERENCES,0.7748917748917749,"Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, and Jo¯ao Carreira. Perceiver IO: A
457"
REFERENCES,0.7763347763347763,"general architecture for structured inputs & outputs, 2021. URL https://arxiv.org/abs/
458"
REFERENCES,0.7777777777777778,"2107.14795.
459"
REFERENCES,0.7792207792207793,"[27] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao
460"
REFERENCES,0.7806637806637806,"Carreira. Perceiver: General perception with iterative attention. In Marina Meila and Tong
461"
REFERENCES,0.7821067821067821,"Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume
462"
REFERENCES,0.7835497835497836,"139 of Proceedings of Machine Learning Research, pages 4651–4664. PMLR, 18–24 Jul 2021.
463"
REFERENCES,0.784992784992785,"URL https://proceedings.mlr.press/v139/jaegle21a.html.
464"
REFERENCES,0.7864357864357865,"[28] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
465"
REFERENCES,0.7878787878787878,"Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
466"
REFERENCES,0.7893217893217893,"models, 2020. URL https://arxiv.org/abs/2001.08361.
467"
REFERENCES,0.7907647907647908,"[29] Hyungjun Kim, Jihoon Park, Changhun Lee, and Jae-Joon Kim. Improving accuracy of binary
468"
REFERENCES,0.7922077922077922,"neural networks using unbalanced activation distribution. In 2021 IEEE/CVF Conference on
469"
REFERENCES,0.7936507936507936,"Computer Vision and Pattern Recognition (CVPR), pages 7858–7867, 2021. doi: 10.1109/
470"
REFERENCES,0.7950937950937951,"CVPR46437.2021.00777.
471"
REFERENCES,0.7965367965367965,"[30] Jinwoo Kim, Tien Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, and
472"
REFERENCES,0.797979797979798,"Seunghoon Hong. Pure Transformers are powerful graph learners. arXiv, abs/2207.02505, 2022.
473"
REFERENCES,0.7994227994227994,"URL https://arxiv.org/abs/2207.02505.
474"
REFERENCES,0.8008658008658008,"[31] Alex Krizhevsky.
Convolutional deep belief networks on cifar-10.
2010.
URL https:
475"
REFERENCES,0.8023088023088023,"//www.cs.toronto.edu/~kriz/conv-cifar10-aug2010.pdf.
476"
REFERENCES,0.8037518037518038,"[32] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classification with
477"
REFERENCES,0.8051948051948052,"deep convolutional neural networks. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein-
478"
REFERENCES,0.8066378066378066,"berger, editors, Advances in Neural Information Processing Systems, volume 25. Curran
479"
REFERENCES,0.8080808080808081,"Associates, Inc., 2012.
URL https://proceedings.neurips.cc/paper/2012/file/
480"
REFERENCES,0.8095238095238095,"c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.
481"
REFERENCES,0.810966810966811,"[33] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay
482"
REFERENCES,0.8124098124098124,"Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam
483"
REFERENCES,0.8138528138528138,"Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with
484"
REFERENCES,0.8152958152958153,"language models, 2022. URL https://arxiv.org/abs/2206.14858.
485"
REFERENCES,0.8167388167388168,"[34] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joseph E.
486"
REFERENCES,0.8181818181818182,"Gonzalez. Train large, then compress: Rethinking model size for efficient training and inference
487"
REFERENCES,0.8196248196248196,"of transformers. In Proceedings of the 37th International Conference on Machine Learning,
488"
REFERENCES,0.8210678210678211,"ICML’20. JMLR.org, 2020.
489"
REFERENCES,0.8225108225108225,"[35] Hanxiao Liu, Zihang Dai, David R. So, and Quoc V. Le. Pay attention to mlps, 2021. URL
490"
REFERENCES,0.823953823953824,"https://arxiv.org/abs/2105.08050.
491"
REFERENCES,0.8253968253968254,"[36] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
492"
REFERENCES,0.8268398268398268,"Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings
493"
REFERENCES,0.8282828282828283,"of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.
494"
REFERENCES,0.8297258297258298,"[37] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal
495"
REFERENCES,0.8311688311688312,"computation engines. arXiv preprint arXiv:2103.05247, 2021.
496"
REFERENCES,0.8326118326118326,"[38] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
497"
REFERENCES,0.834054834054834,"models. In International Conference on Learning Representations (ICLR), 2017. URL https:
498"
REFERENCES,0.8354978354978355,"//openreview.net/forum?id=Byj72udxe.
499"
REFERENCES,0.836940836940837,"[39] Charis Mesaritakis, Vassilis Papataxiarhis, and Dimitris Syvridis. Micro ring resonators as
500"
REFERENCES,0.8383838383838383,"building blocks for an all-optical high-speed reservoir-computing bit-pattern-recognition system.
501"
REFERENCES,0.8398268398268398,"J. Opt. Soc. Am. B, 30(11):3048–3055, Nov 2013. doi: 10.1364/JOSAB.30.003048. URL
502"
REFERENCES,0.8412698412698413,"https://opg.optica.org/josab/abstract.cfm?URI=josab-30-11-3048.
503"
REFERENCES,0.8427128427128427,"[40] Mario Miscuglio, Zibo Hu, Shurui Li, Jonathan K George, Roberto Capanna, Hamed Dalir,
504"
REFERENCES,0.8441558441558441,"Philippe M Bardet, Puneet Gupta, and Volker˜J. Sorger. Massively parallel amplitude-only
505"
REFERENCES,0.8455988455988456,"fourier neural network. Optica, 7(12):1812–1819, 2020. URL https://doi.org/10.1364/
506"
REFERENCES,0.847041847041847,"OPTICA.408659.
507"
REFERENCES,0.8484848484848485,"[41] Mitchell A Nahmias, Thomas Ferreira De Lima, Alexander N Tait, Hsuan-Tung Peng, Bhavin J
508"
REFERENCES,0.84992784992785,"Shastri, and Paul R Prucnal. Photonic multiply-accumulate operations for neural networks.
509"
REFERENCES,0.8513708513708513,"IEEE Journal of Selected Topics in Quantum Electronics, 26:1–18, 2020. URL https://doi.
510"
REFERENCES,0.8528138528138528,"org/10.1109/JSTQE.2019.2941485.
511"
REFERENCES,0.8542568542568543,"[42] Alec Radford and Karthik Narasimhan. Improving language understanding by generative
512"
REFERENCES,0.8556998556998557,"pre-training. 2018. URL https://openai.com/blog/language-unsupervised/.
513"
REFERENCES,0.8571428571428571,"[43] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Lan-
514"
REFERENCES,0.8585858585858586,"guage models are unsupervised multitask learners. 2019. URL https://openai.com/blog/
515"
REFERENCES,0.86002886002886,"better-language-models/.
516"
REFERENCES,0.8614718614718615,"[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
517"
REFERENCES,0.862914862914863,"wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
518"
REFERENCES,0.8643578643578643,"Sutskever. Learning transferable visual models from natural language supervision. In Marina
519"
REFERENCES,0.8658008658008658,"Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine
520"
REFERENCES,0.8672438672438673,"Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748–8763. PMLR,
521"
REFERENCES,0.8686868686868687,"18–24 Jul 2021. URL https://proceedings.mlr.press/v139/radford21a.html.
522"
REFERENCES,0.8701298701298701,"[45] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark
523"
REFERENCES,0.8715728715728716,"Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang,
524"
REFERENCES,0.873015873015873,"editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of
525"
REFERENCES,0.8744588744588745,"Proceedings of Machine Learning Research, pages 8821–8831. PMLR, 18–24 Jul 2021. URL
526"
REFERENCES,0.8759018759018758,"https://proceedings.mlr.press/v139/ramesh21a.html.
527"
REFERENCES,0.8773448773448773,"[46] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov,
528"
REFERENCES,0.8787878787878788,"Gabriel Barth-maron, Mai Giménez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom
529"
REFERENCES,0.8802308802308803,"Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,
530"
REFERENCES,0.8816738816738817,"Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. Transactions on
531"
REFERENCES,0.8831168831168831,"Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/
532"
REFERENCES,0.8845598845598845,"forum?id=1ikK0kHjvj. Featured Certification.
533"
REFERENCES,0.886002886002886,"[47] Albert Reuther, Peter Michaleas, Michael Jones, Vijay Gadepally, Siddharth Samsi, and Jeremy
534"
REFERENCES,0.8874458874458875,"Kepner. Survey of machine learning accelerators. arXiv:2009.00993, 2020. URL https:
535"
REFERENCES,0.8888888888888888,"//arxiv.org/abs/2009.00993.
536"
REFERENCES,0.8903318903318903,"[48] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled
537"
REFERENCES,0.8917748917748918,"version of BERT: smaller, faster, cheaper and lighter. ArXiv, abs/1910.01108, 2019.
538"
REFERENCES,0.8932178932178932,"[49] Abu Sebastian, Manuel Le Gallo, Riduan Khaddam-Aljameh, and Evangelos Eleftheriou.
539"
REFERENCES,0.8946608946608947,"Memory devices and applications for in-memory computing. Nature Nanotechnology, 15(7):
540"
REFERENCES,0.8961038961038961,"529–544, March 2020. doi: 10.1038/s41565-020-0655-z. URL https://doi.org/10.1038/
541"
REFERENCES,0.8975468975468975,"s41565-020-0655-z.
542"
REFERENCES,0.898989898989899,"[50] Jaime Sevilla, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo
543"
REFERENCES,0.9004329004329005,"Villalobos. Compute trends across three eras of machine learning. In 2022 International Joint
544"
REFERENCES,0.9018759018759018,"Conference on Neural Networks (IJCNN), pages 1–8, 2022. doi: 10.1109/IJCNN55064.2022.
545"
REFERENCES,0.9033189033189033,"9891914.
546"
REFERENCES,0.9047619047619048,"[51] Bhavin J Shastri, Alexander N Tait, T Ferreira de Lima, Wolfram HP Pernice, Harish Bhaskaran,
547"
REFERENCES,0.9062049062049062,"C David Wright, and Paul R Prucnal. Photonics for artificial intelligence and neuromorphic
548"
REFERENCES,0.9076479076479076,"computing. Nature Photonics, 15(2):102–114, 2021. URL https://doi.org/10.1038/
549"
REFERENCES,0.9090909090909091,"s41566-020-00754-y.
550"
REFERENCES,0.9105339105339105,"[52] Yichen Shen, Nicholas C Harris, Scott Skirlo, Mihika Prabhu, Tom Baehr-Jones, Michael
551"
REFERENCES,0.911976911976912,"Hochberg, Xin Sun, Shijie Zhao, Hugo Larochelle, Dirk Englund, and Marin Soljaˇci´c. Deep
552"
REFERENCES,0.9134199134199135,"learning with coherent nanophotonic circuits. Nature Photonics, 11(7):441, 2017. URL
553"
REFERENCES,0.9148629148629148,"https://doi.org/10.1038/nphoton.2017.93.
554"
REFERENCES,0.9163059163059163,"[53] Alexander Sludds, Saumil Bandyopadhyay, Zaijun Chen, Zhizhen Zhong, Jared Cochrane,
555"
REFERENCES,0.9177489177489178,"Liane Bernstein, Darius Bunandar, P. Ben Dixon, Scott A. Hamilton, Matthew Streshinsky,
556"
REFERENCES,0.9191919191919192,"Ari Novack, Tom Baehr-Jones, Michael Hochberg, Manya Ghobadi, Ryan Hamerly, and Dirk
557"
REFERENCES,0.9206349206349206,"Englund. Delocalized photonic deep learning on the internet’s edge. Science, 378(6617):270–
558"
REFERENCES,0.922077922077922,"276, 2022. doi: 10.1126/science.abq8271. URL https://www.science.org/doi/abs/10.
559"
REFERENCES,0.9235209235209235,"1126/science.abq8271.
560"
REFERENCES,0.924963924963925,"[54] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari,
561"
REFERENCES,0.9264069264069265,"Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang,
562"
REFERENCES,0.9278499278499278,"Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi,
563"
REFERENCES,0.9292929292929293,"Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. Using DeepSpeed and
564"
REFERENCES,0.9307359307359307,"Megatron to train Megatron-Turing NLG 530B, a large-scale generative language model, 2022.
565"
REFERENCES,0.9321789321789322,"URL https://arxiv.org/abs/2201.11990.
566"
REFERENCES,0.9336219336219336,"[55] James Spall, Xianxin Guo, Thomas D Barrett, and AI Lvovsky. Fully reconfigurable coherent
567"
REFERENCES,0.935064935064935,"optical vector–matrix multiplication. Optics Letters, 45(20):5752–5755, 2020. URL https:
568"
REFERENCES,0.9365079365079365,"//doi.org/10.1364/OL.401675.
569"
REFERENCES,0.937950937950938,"[56] Pascal Stark, Folkert Horst, Roger Dangel, Jonas Weiss, and Bert Jan Offrein. Opportunities for
570"
REFERENCES,0.9393939393939394,"integrated photonic neural networks. Nanophotonics, 9(13):4221–4232, 2020. URL https:
571"
REFERENCES,0.9408369408369408,"//doi.org/10.1515/nanoph-2020-0297.
572"
REFERENCES,0.9422799422799423,"[57] Alexander N. Tait, John Chang, Bhavin J. Shastri, Mitchell A. Nahmias, and Paul R. Prucnal.
573"
REFERENCES,0.9437229437229437,"Demonstration of WDM weighted addition for principal component analysis. Opt. Express,
574"
REFERENCES,0.9451659451659452,"23(10):12758–12765, May 2015. doi: 10.1364/OE.23.012758. URL https://opg.optica.
575"
REFERENCES,0.9466089466089466,"org/oe/abstract.cfm?URI=oe-23-10-12758.
576"
REFERENCES,0.948051948051948,"[58] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas
577"
REFERENCES,0.9494949494949495,"Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic,
578"
REFERENCES,0.950937950937951,"and Alexey Dosovitskiy. MLP-Mixer: An all-MLP architecture for vision. arXiv preprint
579"
REFERENCES,0.9523809523809523,"arXiv:2105.01601, 2021.
580"
REFERENCES,0.9538239538239538,"[59] Marcos Treviso, Tianchu Ji, Ji-Ung Lee, Betty van Aken, Qingqing Cao, Manuel R. Ciosici,
581"
REFERENCES,0.9552669552669553,"Michael Hassid, Kenneth Heafield, Sara Hooker, Pedro H. Martins, André F. T. Martins,
582"
REFERENCES,0.9567099567099567,"Peter Milder, Colin Raffel, Edwin Simpson, Noam Slonim, Niranjan Balasubramanian, Leon
583"
REFERENCES,0.9581529581529582,"Derczynski, and Roy Schwartz. Efficient methods for natural language processing: A survey,
584"
REFERENCES,0.9595959595959596,"2022. URL https://arxiv.org/abs/2209.00099.
585"
REFERENCES,0.961038961038961,"[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
586"
REFERENCES,0.9624819624819625,"Gomez, Ł ukasz Kaiser, and Illia Polosukhin.
Attention is all you need.
In
587"
REFERENCES,0.963924963924964,"Advances in Neural Information Processing Systems, pages 5998–6008. Curran As-
588"
REFERENCES,0.9653679653679653,"sociates, Inc., 2017.
URL https://proceedings.neurips.cc/paper/2017/file/
589"
REFERENCES,0.9668109668109668,"3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
590"
REFERENCES,0.9682539682539683,"[61] Tianyu Wang, Shi-Yuan Ma, Logan G. Wright, Tatsuhiro Onodera, Brian C. Richard, and
591"
REFERENCES,0.9696969696969697,"Peter L. McMahon. An optical neural network using less than 1 photon per multiplication.
592"
REFERENCES,0.9711399711399712,"Nature Communications, 13(1), January 2022. doi: 10.1038/s41467-021-27774-8. URL
593"
REFERENCES,0.9725829725829725,"https://doi.org/10.1038/s41467-021-27774-8.
594"
REFERENCES,0.974025974025974,"[62] Gordon Wetzstein, Aydogan Ozcan, Sylvain Gigan, Shanhui Fan, Dirk Englund, Marin Soljaˇci´c,
595"
REFERENCES,0.9754689754689755,"Cornelia Denz, David A. B. Miller, and Demetri Psaltis. Inference in artificial intelligence
596"
REFERENCES,0.976911976911977,"with deep optics and photonics. Nature, 588(7836):39–47, Dec 2020. ISSN 1476-4687. doi:
597"
REFERENCES,0.9783549783549783,"10.1038/s41586-020-2973-6. URL https://doi.org/10.1038/s41586-020-2973-6.
598"
REFERENCES,0.9797979797979798,"[63] Changming Wu, Heshan Yu, Seokhyeong Lee, Ruoming Peng, Ichiro Takeuchi, and Mo Li.
599"
REFERENCES,0.9812409812409812,"Programmable phase-change metasurfaces on waveguides for multimode photonic convolutional
600"
REFERENCES,0.9826839826839827,"neural network. arXiv preprint arXiv:2004.10651, 2020.
601"
REFERENCES,0.9841269841269841,"[64] Xingyuan Xu, Mengxi Tan, Bill Corcoran, Jiayang Wu, Andreas Boes, Thach G Nguyen, Sai T
602"
REFERENCES,0.9855699855699855,"Chu, Brent E Little, Damien G Hicks, Roberto Morandotti, Arnan Mitchell, and David J Moss.
603"
REFERENCES,0.987012987012987,"11 TOPS photonic convolutional accelerator for optical neural networks. Nature, 589(7840):
604"
REFERENCES,0.9884559884559885,"44–51, 2021. URL https://doi.org/10.1038/s41586-020-03063-0.
605"
REFERENCES,0.98989898989899,"[65] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui
606"
REFERENCES,0.9913419913419913,"Wu. CoCa: Contrastive captioners are image-text foundation models. Transactions on Machine
607"
REFERENCES,0.9927849927849928,"Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id=
608"
REFERENCES,0.9942279942279942,"Ee277P3AYC.
609"
REFERENCES,0.9956709956709957,"[66] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision Transform-
610"
REFERENCES,0.9971139971139971,"ers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
611"
REFERENCES,0.9985569985569985,"(CVPR), pages 12104–12113, June 2022.
612"
