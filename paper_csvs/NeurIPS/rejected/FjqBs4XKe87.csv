Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001876172607879925,"Recent works have shown that attaching prompts to the input is effective at con-
1"
ABSTRACT,0.00375234521575985,"ditioning Language Models (LM) to perform specific tasks. However, prompts
2"
ABSTRACT,0.005628517823639775,"are always included in the input text during inference, thus incurring substantial
3"
ABSTRACT,0.0075046904315197,"computational and memory overhead. Also, there is currently no straightforward
4"
ABSTRACT,0.009380863039399626,"method of utilizing prompts that are longer than the maximum input length of
5"
ABSTRACT,0.01125703564727955,"the LMs without incurring additional costs during inference. We propose Prompt
6"
ABSTRACT,0.013133208255159476,"Injection (PI), a novel formulation of injecting the prompt into the parameters of an
7"
ABSTRACT,0.0150093808630394,"LM to be an efficient alternative to attaching fixed prompts to the input. We show
8"
ABSTRACT,0.016885553470919325,"that in scenarios with long fixed prompts, PI can be up to 280 times more efficient in
9"
ABSTRACT,0.01876172607879925,"terms of total FLOPs than previous approaches. We further explore methodologies
10"
ABSTRACT,0.020637898686679174,"for PI and show promising results in persona-dependent conversation, semantic
11"
ABSTRACT,0.0225140712945591,"parsing, and zero-shot learning with task instructions. Through these explorations,
12"
ABSTRACT,0.024390243902439025,"we show that PI can be a promising direction for conditioning language models,
13"
ABSTRACT,0.02626641651031895,"especially in scenarios with long and fixed prompts1.
14"
INTRODUCTION,0.028142589118198873,"1
Introduction
15"
INTRODUCTION,0.0300187617260788,"Contemporary works with large Language Models (LMs) [3, 32, 23, 5, 28] have shown that attaching
16"
INTRODUCTION,0.03189493433395872,"prompts (also referred to as prefixes) to the input is effective at conditioning LMs to perform specific
17"
INTRODUCTION,0.03377110694183865,"tasks. During training, LMs are trained to condition on the given prompts in hopes of generalizing
18"
INTRODUCTION,0.03564727954971857,"to unseen prompts during inference. Unseen prompts can be a persona for persona-dependent
19"
INTRODUCTION,0.0375234521575985,"conversation [39], database schema for semantic parsing [10], and task instruction for zero-shot
20"
INTRODUCTION,0.039399624765478425,"learning with task instructions [23]. In these tasks, a new prompt is fixed to the input at every
21"
INTRODUCTION,0.04127579737335835,"inference. For instance, in persona-dependent conversation [39, 18, 33], a persona description is
22"
INTRODUCTION,0.043151969981238276,"appended to the dialogue history, so that the LM can always be conditioned on the persona. For
23"
INTRODUCTION,0.0450281425891182,"another example, in semantic parsing, the LM is conditioned on the database schema as well as
24"
INTRODUCTION,0.04690431519699812,"natural language questions to generalize to a new database [37, 10, 36]. Lastly, zero-shot learning
25"
INTRODUCTION,0.04878048780487805,"with task instructions [32, 23] involves adding natural language instructions to the inputs for adapting
26"
INTRODUCTION,0.05065666041275797,"LMs to novel tasks.
27"
INTRODUCTION,0.0525328330206379,"However, concatenating prompts to input sequences for prompt-dependent inference has two major
28"
INTRODUCTION,0.054409005628517824,"limitations. (1) During inference, prompts are always included in the input text and thus incur
29"
INTRODUCTION,0.05628517823639775,"computational and memory overhead [16]. (2) It is challenging to fit a long text such as the detailed
30"
INTRODUCTION,0.058161350844277676,"description of a persona as a prompt into Transformer-based models whose input lengths are often
31"
INTRODUCTION,0.0600375234521576,"fixed [27]. For instance, in persona-dependent conversation, the model constantly refers to the persona
32"
INTRODUCTION,0.06191369606003752,"description along with the dialogue history [35, 22], as shown in the left side of Figure 1. Moreover,
33"
INTRODUCTION,0.06378986866791744,"in real world scenarios, a persona may consist of a long detailed text description of a character or
34"
INTRODUCTION,0.06566604127579738,"person, not just a few profile sentences. Naively concatenating long prompts to the input sequences is
35"
INTRODUCTION,0.0675422138836773,"challenging due to the quadratic cost in time and memory of Transformer-based architectures with
36"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.06941838649155722,1Code used for the experiments and a demo are available at this link ðŸ§‘
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.07129455909943715,Hello! How are you today?
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.07317073170731707,"Great, thanks ! My children and I"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.075046904315197,were just about to watch Game
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.07692307692307693,of Thrones.
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.07879924953095685,"I am an artist. I have four children. I 
recently got a cat. I enjoy walking for"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.08067542213883677,"exercise. I love watching Game of 
Thrones. Hello! How are you today? ðŸ¤–"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.0825515947467167,LM with Explicit Prompt (Prev)
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.08442776735459662,"I am an artist. I have four children. I 
recently got a cat. I enjoy walking for"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.08630393996247655,exercise. I love  watcing Game of
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.08818011257035648,Thrones. ðŸ§‘
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.0900562851782364,Hello! How are you today?
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.09193245778611632,"Great, thanks ! My children and I"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.09380863039399624,were just about to watch Game
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.09568480300187618,of Thrones.
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.0975609756097561,Hello! How are you today?
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.09943714821763602,LM with Prompt Injection (Ours)
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.10131332082551595,"agentâ€™s 
persona
ðŸ’‰"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.10318949343339587,"Inject 
persona  
in advance"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.1050656660412758,"Concat  
persona"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.10694183864915573,to input
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.10881801125703565,"agentâ€™s 
persona âš¡"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.11069418386491557,"Figure 1: Prompt Injection example on a persona-dependent conversation. The left side presents
an inference procedure of a previous approach where the persona (prompt) is concatenated to every
input. The right side describes Prompt Injection, where the persona is injected into the model in
advance, so that the model is able to generate responses without constantly referring to the persona
description. Thus, Prompt Injection approach takes less time to generate responses than the previous
method."
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.1125703564727955,"regard to the input sequence length. Other approaches specialized for long inputs [1, 13], such as
37"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.11444652908067542,"Fusion-in-Decoder [12], or those that augment the LM with a retrieval mechanism [9] may be used
38"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.11632270168855535,"but still come with increased overall memory and computations, ultimately leading to a delay in
39"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.11819887429643527,"generating responses. This problem becomes critical in situations where the LMs are deployed, and
40"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.1200750469043152,"fast inference speed is required.
41"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.12195121951219512,"In this work, we formulate a novel problem called Prompt Injection (PI), where we attempt to inject a
42"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.12382739212007504,"given prompt into the parameters of an LM to address the two limitations mentioned above. With
43"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.12570356472795496,"PI, LMs can produce prompt-dependent outputs without the computational overhead of appending
44"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.1275797373358349,"fixed prompts at inference time (the right side of Figure 1), and it also enables the injection of longer
45"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.1294559099437148,"prompts in a wholistic way. More specifically, we first show that PI is much more efficient (up to
46"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.13133208255159476,"280 times) in terms of total FLOPs compared to previous approaches that may be used for handling
47"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.13320825515947468,"long prompts such as Fusion-in-Decoder [12] or Linear Transformer [13]. Next, we explore different
48"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.1350844277673546,"methodologies as baselines for PI, including the continued pre-training approach on the prompt as
49"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.13696060037523453,"well as a novel distillation approach called Pseudo-INput Generation (PING), in order to analyze
50"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.13883677298311445,"what components are effective for successful PI. We apply these PI methods to three different tasks
51"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.14071294559099437,"with fixed prompts: persona-dependent conversation, semantic parsing, and zero-shot learning with
52"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.1425891181988743,"instructions. We compare the methods against LMs with explicit prompts as the upper bound (i.e.,
53"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.14446529080675422,"unconstrained) as well as the LM without both the prompt and PI as the lower bound. Experimental
54"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.14634146341463414,"results show meaningful improvements with respect to the lower bound, but also exhibit a non-trivial
55"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.14821763602251406,"gap with the upper bound. Despite the performance gap, we still believe that PI is a direction worth
56"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.150093808630394,"exploring considering the computational benefit of the injection, especially since inference speed is
57"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.15196998123827393,"critical in real world applications.
58"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.15384615384615385,"In sum, our main contributions are three folds:
59"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.15572232645403378,"â€¢ We formally define the Prompt Injection (PI) formulation and demonstrate its necessity in
60"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.1575984990619137,"terms of computation and memory efficiency, especially in scenarios with long prompts.
61"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.15947467166979362,"â€¢ We explore baseline approaches for PI, showing that performance can approach the upper
62"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.16135084427767354,"bound (unconstrained) performance in some cases.
63"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.16322701688555347,"â€¢ We show that the injection of long prompts (e.g., detailed description of persona) can be
64"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.1651031894934334,"achieved through PI and show its efficiency in comparison with previous methods, being up
65"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.1669793621013133,"to 280 times more efficient during inference.
66"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.16885553470919323,"Through this work, we hope the community explores PI as an efficient alternative for performing
67"
CODE USED FOR THE EXPERIMENTS AND A DEMO ARE AVAILABLE AT THIS LINK,0.17073170731707318,"prompt-dependent tasks.
68"
RELATED WORK,0.1726078799249531,"2
Related Work
69"
RELATED WORK,0.17448405253283303,"Prompting
Prompting is an emerging paradigm for modeling LMs, especially for few-shot and
70"
RELATED WORK,0.17636022514071295,"zero-shot learning [20, 3, 21, 24, 32, 23]. With the help of appropriate prompts, one can exploit
71"
RELATED WORK,0.17823639774859287,"knowledge learned by a pre-trained LM and manipulate the LMâ€™s behavior. The benefit of prompting
72"
RELATED WORK,0.1801125703564728,"is that the pre-trained LM can adapt to new scenarios with few or no labeled training data. However,
73"
RELATED WORK,0.18198874296435272,"for the in-context learning scenario, processing prompts that involve many training examples for each
74"
RELATED WORK,0.18386491557223264,"inference incurs substantial computational and memory overhead [16]. Given training data, Liu et al.
75"
RELATED WORK,0.18574108818011256,"[16] replace in-context learning with fine-tuning a small set of parameters for tackling the above
76"
RELATED WORK,0.18761726078799248,"issue. Prompt Injection also tackles the same issue but assumes a stricter scenario where there are no
77"
RELATED WORK,0.1894934333958724,"training data for the given prompt.
78"
RELATED WORK,0.19136960600375236,"Efficient Transformers for Long Inputs
One can consider using efficient Transformer-based [29]
79"
RELATED WORK,0.19324577861163228,"architectures for handling long input sequences [27]. The main challenge of using a vanilla Trans-
80"
RELATED WORK,0.1951219512195122,"former architecture is the quadratic cost in time and memory with regard to the input sequence
81"
RELATED WORK,0.19699812382739212,"length due to the self-attention operation. There has been a surge of recent works addressing this
82"
RELATED WORK,0.19887429643527205,"problem [6, 38, 1, 13, 40, 8]. They are primarily dedicated to improving either the efficiency of the
83"
RELATED WORK,0.20075046904315197,"self-attention mechanism or the general efficiency of the Transformer architecture through sparse mod-
84"
RELATED WORK,0.2026266416510319,"els. Our Prompt Injection approach tackles the efficiency problem of performing prompt-dependent
85"
RELATED WORK,0.2045028142589118,"tasks by keeping the input sequences short (without prompts), bounding the time and memory
86"
RELATED WORK,0.20637898686679174,"complexity to a constant invariant of the length of the prompt.
87"
RELATED WORK,0.20825515947467166,"Persona-dependent Conversation
Endowing a chabot with a persona [39, 18, 33] is challenging,
88"
RELATED WORK,0.2101313320825516,"but it enables the chatbot to deliver more personal, specific, consistent, and engaging conversa-
89"
RELATED WORK,0.21200750469043153,"tions [39] and gain user trust [17, 25, 19]. To achieve this, previous works have attached a persona to
90"
RELATED WORK,0.21388367729831145,"the dialog history at every inference time, so that the model can always be conditioned on the persona.
91"
RELATED WORK,0.21575984990619138,"However, given a long persona description, this approach brings the critical problem of increased
92"
RELATED WORK,0.2176360225140713,"overall memory and computations, resulting in delayed response generation. An LM augmented
93"
RELATED WORK,0.21951219512195122,"with a retrieval mechanism [9] may be used but still comes with non-trivial computational overhead.
94"
RELATED WORK,0.22138836772983114,"Prompt Injection allows a dialogue agent to generate responses without a persona description as the
95"
RELATED WORK,0.22326454033771106,"explicit input once the persona is injected.
96"
RELATED WORK,0.225140712945591,"Semantic Parsing
Semantic parsing is the task of mapping a natural language query into a SQL
97"
RELATED WORK,0.2270168855534709,"query executable on a database. Recently, the community has focused more on cross-domain (cross-
98"
RELATED WORK,0.22889305816135083,"database) semantic parsing, where models are trained and tested on different domains (databases) [37].
99"
RELATED WORK,0.23076923076923078,"The domain-adaptation setup introduces many generalization challenges, such as non-explicit column
100"
RELATED WORK,0.2326454033771107,"names and domain-specific phrases [10], and recent works concatenate the natural language query
101"
RELATED WORK,0.23452157598499063,"with the serialized database schema as the input to address the problem [26, 7, 36]. With Prompt
102"
RELATED WORK,0.23639774859287055,"Injection, the model is adapted to a new database schema in advance, so that it can map natural
103"
RELATED WORK,0.23827392120075047,"language queries to SQL queries on the new database without explicitly referring to the schema
104"
RELATED WORK,0.2401500938086304,"during inference.
105"
RELATED WORK,0.24202626641651032,"Zero-shot Learning with Task Instructions
Recent works [23, 32] have addressed zero-shot
106"
RELATED WORK,0.24390243902439024,"generalization to new tasks [3, 14] by multi-task prompted training. With multi-task prompted
107"
RELATED WORK,0.24577861163227016,"training, the models learn to use task instructions as prompts to generalize to unseen tasks. It is
108"
RELATED WORK,0.24765478424015008,"demonstrated that this approach improves generalization ability to novel tasks and offers an effective
109"
RELATED WORK,0.24953095684803,"substitute for unsupervised language model pre-training. Through Prompt Injection, the LM can be
110"
RELATED WORK,0.25140712945590993,"aware of a novel task instruction before performing the task and thus does not require the instruction,
111"
RELATED WORK,0.25328330206378985,"which can be lengthy, to make predictions.
112"
PROMPT INJECTION,0.2551594746716698,"3
Prompt Injection
113"
PROMPT INJECTION,0.2570356472795497,"In this section, we formally define Prompt Injection (PI) as a task and describe the benefits of the
114"
PROMPT INJECTION,0.2589118198874296,"formulation. Prompt-dependent generation is a task of generating an output sequence y that is a
115"
PROMPT INJECTION,0.2607879924953096,"proper response to the input sequence x and coherent to the prompt z. Utilizing the prompt during
116"
PROMPT INJECTION,0.2626641651031895,"inference, the generated sentence is obtained by y = f(z, x) where f denotes an LM such as
117"
PROMPT INJECTION,0.26454033771106944,"T5 and GPT-2. Prompt Injection (PI), i.e., parameterization of prompts, allows LMs to perform
118"
PROMPT INJECTION,0.26641651031894936,"prompt-dependent generation without using prompts during inference. To achieve this, we need to
119"
PROMPT INJECTION,0.2682926829268293,"design a PI method H to inject a prompt z into an LM f. The process of PI can be represented as
120"
PROMPT INJECTION,0.2701688555347092,"fz = H(z, f)
(1)"
PROMPT INJECTION,0.27204502814258913,"where fz denotes an LM injected with the prompt. Then the prompt-dependent output sequence can
121"
PROMPT INJECTION,0.27392120075046905,"be obtained by y = fz(x).
122"
PROMPT INJECTION,0.275797373358349,"PI can also be applied for long prompts whose length exceeds the LMâ€™s input sequence length. Given
123"
PROMPT INJECTION,0.2776735459662289,"a long prompt z, we decompose it into multiple sub-prompts {zi} each of which fits the LMâ€™s input
124"
PROMPT INJECTION,0.2795497185741088,"length, i.e., z = z1:n = [z1; z2; ...; zn]. Then the PI process can be executed iteratively, injecting
125"
PROMPT INJECTION,0.28142589118198874,"each sub-prompt sequentially while the LM is aware of the previous sub-prompts:
126"
PROMPT INJECTION,0.28330206378986866,"fz1 = H(z1, f)
(2)
fz1:2 = H(z2, fz1)
(3)
. . .
fz1:n = H(zn, fz1:nâˆ’1)
(4)"
PROMPT INJECTION,0.2851782363977486,"The above formulation can be seen as a high-level abstraction of iterative PI that we aim to ap-
127"
PROMPT INJECTION,0.2870544090056285,"proximate. In practice, in order to fully inject z1:n, we repeat (2)-(4) multiple times (i.e., multiple
128"
PROMPT INJECTION,0.28893058161350843,"epochs).
129"
PROMPT INJECTION,0.29080675422138835,"Why is Prompt Injection necessary?
Prompt Injection brings definite advantages when applied to
130"
PROMPT INJECTION,0.2926829268292683,"prompt-dependent tasks. The previous approach of appending prompts to the input sequences has
131"
PROMPT INJECTION,0.2945590994371482,"the drawback of the model repeatedly referring to the prompt at each inference time. This becomes
132"
PROMPT INJECTION,0.2964352720450281,"critical in scenarios requiring long prompts, as Transformer architecture has quadratic computational
133"
PROMPT INJECTION,0.29831144465290804,"and memory costs due to the limitation of the self-attention operation. We propose PI as a solution
134"
PROMPT INJECTION,0.300187617260788,"to this computation bottleneck. Once a prompt is injected into the LM in advance, the LM no
135"
PROMPT INJECTION,0.30206378986866794,"longer needs to refer to the prompt during inference. As a result, the modelâ€™s input length remains
136"
PROMPT INJECTION,0.30393996247654786,"independent of the length of prompts and is able to utilize prompts of any length efficiently. We
137"
PROMPT INJECTION,0.3058161350844278,"discuss the efficiency gain of PI in Section 6.1.
138"
PROMPT INJECTION,0.3076923076923077,"Evaluation Metric for Prompt Injection
PI can be evaluated by the evaluation metric of the
139"
PROMPT INJECTION,0.30956848030018763,"fixed prompt-dependent task at hand. We also introduce a metric called the Prompt Injection
140"
PROMPT INJECTION,0.31144465290806755,"score (PI score) to measure the degree of injection. The metric is agnostic of the target task by
141"
PROMPT INJECTION,0.3133208255159475,"comparing the results with that of an LM given actual prompts during inference. Let Xw/ prompt
142"
PROMPT INJECTION,0.3151969981238274,"denote the LMâ€™s task score with the prompt as an additional input (upper bound) and Xw/o prompt
143"
PROMPT INJECTION,0.3170731707317073,"denote the LMâ€™s task score without the prompt (lower bound). We define PI score as the min-
144"
PROMPT INJECTION,0.31894934333958724,"max scaling score of XP I, where XP I represents the score of the LM on the target task after PI,
145"
PROMPT INJECTION,0.32082551594746717,"i.e., PI score = max(0, XP I âˆ’Xw/o prompt) / (Xw/ prompt âˆ’Xw/o prompt). We limit using PI
146"
PROMPT INJECTION,0.3227016885553471,"only in situations where Xw/ prompt > Xw/o prompt because there is no reason to inject a prompt
147"
PROMPT INJECTION,0.324577861163227,"if task performance degrades when using the prompt. Even if the range of individual task scores
148"
PROMPT INJECTION,0.32645403377110693,"may vary from task to task, PI score represents the overall injection effectiveness of the PI methods,
149"
PROMPT INJECTION,0.32833020637898686,"agnostic of the individual task score range.
150"
METHODS FOR PROMPT INJECTION,0.3302063789868668,"4
Methods for Prompt Injection
151"
METHODS FOR PROMPT INJECTION,0.3320825515947467,"In this section, we explore methods of Prompt Injection (PI) that can address prompt-dependent tasks
152"
METHODS FOR PROMPT INJECTION,0.3339587242026266,"without accessing the prompt during inference. To achieve this, the model should be trained to store
153"
METHODS FOR PROMPT INJECTION,0.33583489681050654,"the prompt in its parameters. This can be seen as parameterizing the prompt into the model instead of
154"
METHODS FOR PROMPT INJECTION,0.33771106941838647,"feeding the prompt explicitly to the model. This is challenging as the prompt is unseen to the model
155"
METHODS FOR PROMPT INJECTION,0.3395872420262664,"and has no corresponding training data. In Section 4.1, a baseline method by continued pre-training
156"
METHODS FOR PROMPT INJECTION,0.34146341463414637,"is introduced, followed by a method for improving the baseline with curriculum learning. Section 4.2
157"
METHODS FOR PROMPT INJECTION,0.3433395872420263,"presents a novel distillation-based method called Pseudo-INput Generation (PING) that learns to
158"
METHODS FOR PROMPT INJECTION,0.3452157598499062,"generate pseudo-inputs to inject novel prompts.
159"
METHODS FOR PROMPT INJECTION,0.34709193245778613,Student
METHODS FOR PROMPT INJECTION,0.34896810506566606,Prompt
METHODS FOR PROMPT INJECTION,0.350844277673546,Pseudo Input
METHODS FOR PROMPT INJECTION,0.3527204502814259,Prompt + Pseudo Input
METHODS FOR PROMPT INJECTION,0.3545966228893058,Output
METHODS FOR PROMPT INJECTION,0.35647279549718575,Pseudo Input
METHODS FOR PROMPT INJECTION,0.35834896810506567,Output
METHODS FOR PROMPT INJECTION,0.3602251407129456,"Distillation
Input 
Generator"
METHODS FOR PROMPT INJECTION,0.3621013133208255,Prompt
METHODS FOR PROMPT INJECTION,0.36397748592870544,Task Input
METHODS FOR PROMPT INJECTION,0.36585365853658536,"| singer | singer_id , name ,  
birth_year , net_worth_millions â€¦"
METHODS FOR PROMPT INJECTION,0.3677298311444653,How many singers are there?
METHODS FOR PROMPT INJECTION,0.3696060037523452,"Phase 1: Generator Training
Phase 2: Distillation"
METHODS FOR PROMPT INJECTION,0.3714821763602251,How many departments are there?
METHODS FOR PROMPT INJECTION,0.37335834896810505,"| department | department_id , name , 
creation , ranking , num_employees â€¦"
METHODS FOR PROMPT INJECTION,0.37523452157598497,"| singer | singer_id , name â€¦ 
How may singers are there?"
METHODS FOR PROMPT INJECTION,0.3771106941838649,How may singers are there?
METHODS FOR PROMPT INJECTION,0.3789868667917448,"SELECT  COUNT(*) FROM â€¦
SELECT  COUNT(*) FROM â€¦"
METHODS FOR PROMPT INJECTION,0.3808630393996248,"Input 
Generator
Teacher"
METHODS FOR PROMPT INJECTION,0.3827392120075047,"Figure 2: Illustration of the Pseudo-INput Generation (PING). During Phase 1, an input generator
is trained with the task-specific training data. The inputs are prompts of a task, and the outputs are
task inputs corresponding to the prompt. Input and output examples applied to semantic parsing are
shown. During Phase 2, the input generator generates pseudo-inputs from the given target prompt,
which are used to distill knowledge from the teacher to the student. Blue square boxes indicate frozen
parameters; yellow rounded boxes indicate unfrozen parameters."
CONTINUED PRE-TRAINING,0.38461538461538464,"4.1
Continued Pre-training
160"
CONTINUED PRE-TRAINING,0.38649155722326456,"We establish the Continued Pre-training method as a straightforward baseline for PI. This method
161"
CONTINUED PRE-TRAINING,0.3883677298311445,"injects prompts into the parameters of an LM by continuing with the pre-training objective of the
162"
CONTINUED PRE-TRAINING,0.3902439024390244,"LM on the target prompt. The pre-training objective is a straightforward option as it works in an
163"
CONTINUED PRE-TRAINING,0.3921200750469043,"unsupervised manner. In our experiments, we leverage the pre-trained T5 model [21] and thus use
164"
CONTINUED PRE-TRAINING,0.39399624765478425,"the masked language modeling objective which is the pre-training objective of T5. Following Raffel
165"
CONTINUED PRE-TRAINING,0.39587242026266417,"et al. [21], we randomly replace 15% of a given prompt with special mask tokens; then, the model is
166"
CONTINUED PRE-TRAINING,0.3977485928705441,"trained to predict the sequence of masked tokens. In this process, the model learns about the prompt
167"
CONTINUED PRE-TRAINING,0.399624765478424,"the same way the model learns knowledge during the pre-training stage.
168"
CONTINUED PRE-TRAINING,0.40150093808630394,"Curriculum learning
We further investigate the baseline method by leveraging curricula [2, 4]
169"
CONTINUED PRE-TRAINING,0.40337711069418386,"during continued pre-training. We set the mask ratio as the difficulty criteria [34] and gradually
170"
CONTINUED PRE-TRAINING,0.4052532833020638,"increase the ratio throughout the Continued Pre-training. As the mask ratio increases, the model
171"
CONTINUED PRE-TRAINING,0.4071294559099437,"should predict more masked tokens given less context. With curriculum learning, we expect the LM to
172"
CONTINUED PRE-TRAINING,0.4090056285178236,"gradually better adapt to the prompt, improving its prompt-dependent task performance. Throughout
173"
CONTINUED PRE-TRAINING,0.41088180112570355,"the experiments, we increase the mask ratio linearly from 15% to 30%, 50%, and 70% and report the
174"
CONTINUED PRE-TRAINING,0.41275797373358347,"best score.
175"
CONTINUED PRE-TRAINING,0.4146341463414634,"4.2
Pseudo-INput Generation (PING)
176"
CONTINUED PRE-TRAINING,0.4165103189493433,"The purpose of PI is to inject a prompt into the parameters of an LM which can also be done indirectly
177"
CONTINUED PRE-TRAINING,0.41838649155722324,"through distillation. In this subsection, we propose a novel distillation-based method called Pseudo-
178"
CONTINUED PRE-TRAINING,0.4202626641651032,"INput Generation (PING) that distills a novel prompt into a student LM that does not have access
179"
CONTINUED PRE-TRAINING,0.42213883677298314,"to the prompt through a teacher LM that does have access to the prompt. In order for distillation,
180"
CONTINUED PRE-TRAINING,0.42401500938086306,"pseudo-inputs are needed since we assume a scenario where the prompt to be injected has never been
181"
CONTINUED PRE-TRAINING,0.425891181988743,"seen during training and does not have separate training data. An overview of PING is illustrated in
182"
CONTINUED PRE-TRAINING,0.4277673545966229,"Figure 2. As shown in the figure, during Phase 1, an input generator is trained with the task-specific
183"
CONTINUED PRE-TRAINING,0.42964352720450283,"training data. When given a prompt of the task as the input, the generator is expected to generate the
184"
CONTINUED PRE-TRAINING,0.43151969981238275,"task inputs that correspond to the prompt. During Phase 2, the input generator is frozen and is used to
185"
CONTINUED PRE-TRAINING,0.4333958724202627,"generate pseudo-inputs from the unseen prompt, which are then given to the teacher together with the
186"
CONTINUED PRE-TRAINING,0.4352720450281426,"prompt, while only the pseudo-inputs are given to the student. This way, the student learns to follow
187"
CONTINUED PRE-TRAINING,0.4371482176360225,"the teacher and is able to learn about the prompt indirectly. We believe that this is the first work that
188"
CONTINUED PRE-TRAINING,0.43902439024390244,"aims to distill knowledge with different inputs for the teacher and the student.
189"
EXPERIMENTAL SETUP,0.44090056285178236,"5
Experimental Setup
190"
EXPERIMENTAL SETUP,0.4427767354596623,"In this section, we explain the experimental setups in detail. All experiments are performed with the
191"
EXPERIMENTAL SETUP,0.4446529080675422,"T5-base [21] (220M parameters) model unless noted otherwise.
192"
PROMPT-DEPENDENT TASKS,0.44652908067542213,"5.1
Prompt-dependent tasks
193"
PROMPT-DEPENDENT TASKS,0.44840525328330205,"In order to evaluate the effectiveness of Prompt Injection (PI) methods, we select three prompt-
194"
PROMPT-DEPENDENT TASKS,0.450281425891182,"dependent tasksâ€”persona-dependent conversation, semantic parsing, and zero-shot learning with
195"
PROMPT-DEPENDENT TASKS,0.4521575984990619,"task instructions; all these tasks require fixed prompts during inference. Fixed prompts come in the
196"
PROMPT-DEPENDENT TASKS,0.4540337711069418,"form of a persona in persona-dependent conversation [39], database schema in semantic parsing [10],
197"
PROMPT-DEPENDENT TASKS,0.45590994371482174,"and task instruction in zero-shot learning with task instructions [23]. As described in the introduction
198"
PROMPT-DEPENDENT TASKS,0.45778611632270166,"and Section 3, when PI is applied for these tasks, there would be apparent benefits in real world
199"
PROMPT-DEPENDENT TASKS,0.4596622889305816,"scenarios. For instance, PI eliminates the need to repeatedly include persona descriptions in the input
200"
PROMPT-DEPENDENT TASKS,0.46153846153846156,"during inference when serving a conversational model of a specific personality. With these tasks,
201"
PROMPT-DEPENDENT TASKS,0.4634146341463415,"not only the performance of the baseline PI methods is evaluated, but also the significance of PI is
202"
PROMPT-DEPENDENT TASKS,0.4652908067542214,"emphasized by comparison with the (unconstrained) previous approaches that concatenate prompts to
203"
PROMPT-DEPENDENT TASKS,0.46716697936210133,"the input.
204"
DATASETS,0.46904315196998125,"5.2
Datasets
205"
DATASETS,0.4709193245778612,"Following datasets of prompt-dependent tasks mentioned in Section 5.1 are utilized to evaluate
206"
DATASETS,0.4727954971857411,"Prompt Injection (PI).
207"
DATASETS,0.474671669793621,"PERSONA-CHAT
PERSONA-CHAT [39] is a crowd-sourced dataset intended for training agents
208"
DATASETS,0.47654784240150094,"to perform engaging and personal chit-chat by comprising the dialogues to be grounded on specific
209"
DATASETS,0.47842401500938087,"personas. They crowdsourced 1,155 unique personas, each with five profile sentences and 162,064
210"
DATASETS,0.4803001876172608,"utterances over 10,907 dialogues. For each dialogue, two speakers have a 6-8 turn conversation
211"
DATASETS,0.4821763602251407,"conditioned on a given persona. The task is measured via perplexity (PPL). We randomly select 100
212"
DATASETS,0.48405253283302063,"dialogues from the validation set as persona-dependent conversation benchmark for testing PI. The
213"
DATASETS,0.48592870544090055,"persona descriptions are 60 tokens long on average.
214"
DATASETS,0.4878048780487805,"Spider
Spider [37] is a large cross-domain semantic parsing and text-to-SQL dataset for developing
215"
DATASETS,0.4896810506566604,"natural language interfaces to cross-domain databases. It includes 10,181 questions, 5,693 unique
216"
DATASETS,0.4915572232645403,"SQL queries, and 200 database schemas covering 138 different domains. Models must generalize to
217"
DATASETS,0.49343339587242024,"new database schemas as well as new queries to perform well on it. Evaluation metrics include Exact
218"
DATASETS,0.49530956848030017,"Matching (EM) and Execution Accuracy (EA). We utilize the dev set containing 20 databases with
219"
DATASETS,0.4971857410881801,"about 50 questions per database as a semantic parsing benchmark for PI. The database schemas range
220"
DATASETS,0.49906191369606,"in length from 55 to 430 token lengths.
221"
DATASETS,0.50093808630394,"WSC / RTE / COPA
For the task of zero-shot task generalization, Raffel et al. [21] have trained
222"
DATASETS,0.5028142589118199,"the LM on a diverse set of tasks and evaluated on a held-out group of tasks to evaluate generalization
223"
DATASETS,0.5046904315196998,"performance. We choose coreference resolution, natural language inference, and sentence completion
224"
DATASETS,0.5065666041275797,"tasks, three out of their four held-out tasks, and test PI on WSC (Winograd Schema Challenge), RTE
225"
DATASETS,0.5084427767354597,"(Recognizing Textual Entailment), and COPA (Choice of Plausible Alternatives) datasets [30]. All of
226"
DATASETS,0.5103189493433395,"these tasks are binary classification tasks. We utilize task instructions (prompts) of WSC, RTE, and
227"
DATASETS,0.5121951219512195,"COPA provided from Raffel et al. [21] and report average task scores of using task instructions. The
228"
DATASETS,0.5140712945590994,"task instructions are comprised of about 20-30 tokens.
229"
IMPLEMENTATION DETAILS,0.5159474671669794,"5.3
Implementation Details
230"
IMPLEMENTATION DETAILS,0.5178236397748592,"For the Continued Pre-training method (Section 4.1), we use the Adam optimizer [15] with a constant
231"
IMPLEMENTATION DETAILS,0.5196998123827392,"learning rate 1e-4 and batch size 8. We perform 5-20 steps of injection. For PING (Section 4.2),
232"
IMPLEMENTATION DETAILS,0.5215759849906192,"input generators are trained on each tasks for 1-2 epochs. We use KL-divergence for distilling the
233"
IMPLEMENTATION DETAILS,0.5234521575984991,"last layerâ€™s output of the decoder and perform 10-40 steps of injection. Diverse pseudo-inputs are
234"
IMPLEMENTATION DETAILS,0.525328330206379,"generated by sampling each token from the output probability distribution of the decoder. For all of
235"
IMPLEMENTATION DETAILS,0.5272045028142589,"the experiments except for zero-shot generalization, we use a single 16GB T4 GPU. For zero-shot
236"
IMPLEMENTATION DETAILS,0.5290806754221389,"generalization, we use 4 32GB V100 GPUs.
237"
IMPLEMENTATION DETAILS,0.5309568480300187,"Table 1: Inference efficiency of different models that can be used for performing prompt-dependent
inference. We depict how many times PI is efficient in comparison with the other approaches inside
the parenthesis. When there is out-of-memory (OOM) using the 16GB T4 GPU, we estimate the
FLOPs in italics assuming a linear correlation between prompt length and FLOPs."
IMPLEMENTATION DETAILS,0.5328330206378987,"Model
Prompt Length
FLOPs (G)
Latency (s)"
IMPLEMENTATION DETAILS,0.5347091932457786,"T5 W/ PI
*
0.7k
0.58"
IMPLEMENTATION DETAILS,0.5365853658536586,"T5
512
7.2k (Ã—10.3)
1.09 (Ã—1.9)
512 Ã— 2
14.6k (Ã—21.0)
2.38 (Ã—4.1)
512 Ã— 4
OOM
-
T5 W/ FID
512
7.2k (Ã—10.3)
1.09 (Ã—1.9)
512 Ã— 2
14.0k (Ã—20.2)
1.54 (Ã—2.6)
512 Ã— 4
27.6k (Ã—39.8)
2.87 (Ã—4.9)
512 Ã— 8
54.9k (Ã—79.2)
5.87 (Ã—10.0)
512 Ã— 28
OOM (Ã—280)
-
LINEAR-
512
9.5k (Ã—13.8)
1.58 (Ã—2.7)
TRANSFORMER
512 Ã— 2
16.1k (Ã—23.2)
2.62 (Ã—4.5)
512 Ã— 4
29.2k (Ã—42.2)
4.74 (Ã—8.1)
512 Ã— 8
55.6k (Ã—80.1)
9.11 (Ã—15.6)
512 Ã— 28
OOM (Ã—280)
-"
IMPLEMENTATION DETAILS,0.5384615384615384,"In order for injection and comparison with upper-bound and lower-bound performance, we first
238"
IMPLEMENTATION DETAILS,0.5403377110694184,"need two different versions of the LM adapted to the given task. For the task of persona-dependent
239"
IMPLEMENTATION DETAILS,0.5422138836772983,"conversation and semantic parsing, one (upper bound) is fine-tuned together with prompts since
240"
IMPLEMENTATION DETAILS,0.5440900562851783,"prompts are explicitly used during inference, while the other (lower bound) is fine-tuned on the task
241"
IMPLEMENTATION DETAILS,0.5459662288930581,"without being given the prompt. We perform PI on the lower-bound LM since we also assume having
242"
IMPLEMENTATION DETAILS,0.5478424015009381,"no access to prompts during inference.
243"
IMPLEMENTATION DETAILS,0.549718574108818,"For the zero-shot learning task, we modify the prompts developed by Raffel et al. [21]
244"
IMPLEMENTATION DETAILS,0.551594746716698,"in the form of a fixed prompt.
Their prompts have placeholders such as Premise, and
245"
IMPLEMENTATION DETAILS,0.5534709193245778,"Hypothesis. We replace the placeholders with fixed words such as ""Premise"" and ""Hypoth-
246"
IMPLEMENTATION DETAILS,0.5553470919324578,"esis"", then append the actual content to the prompt in a key-value format.
For example,
247"
IMPLEMENTATION DETAILS,0.5572232645403377,"if the original is If {Premise} is true, is it also true that {Hypothesis}?, then
248"
IMPLEMENTATION DETAILS,0.5590994371482176,"the converted prompt is If ""Premise"" is true, is it also true that ""Hypothesis""?
249"
IMPLEMENTATION DETAILS,0.5609756097560976,"Premise:{Premise} Hypothesis:{Hypothesis}. This ensures that the prompt is fixed, which
250"
IMPLEMENTATION DETAILS,0.5628517823639775,"can be injected with PI. We use the T0-3B LM checkpoint for the zero-shot generalization.
251"
EXPERIMENTAL RESULTS,0.5647279549718575,"6
Experimental Results
252"
EXPERIMENTAL RESULTS,0.5666041275797373,"In this section, we first explore the inference efficiency of models performing prompt-dependent tasks
253"
EXPERIMENTAL RESULTS,0.5684803001876173,"and show that Prompt Injection (PI) leads to meaningful computational efficiency. Then the baseline
254"
EXPERIMENTAL RESULTS,0.5703564727954972,"and proposed methods are tested and compared on datasets discussed in Section 5.2. The results
255"
EXPERIMENTAL RESULTS,0.5722326454033771,"indicate that the Pseudo-INput Generation (PING) method achieves the best performance among PI
256"
EXPERIMENTAL RESULTS,0.574108818011257,"methods, sometimes even outperforming the unconstrained upper bound, which uses explicit prompts
257"
EXPERIMENTAL RESULTS,0.575984990619137,"during inference. In Section 6.3, we provide a concrete instance of injecting a real persona description
258"
EXPERIMENTAL RESULTS,0.5778611632270169,"into a conversational model, demonstrating the feasibility of long prompt injection.
259"
INFERENCE EFFICIENCY,0.5797373358348968,"6.1
Inference Efficiency
260"
INFERENCE EFFICIENCY,0.5816135084427767,"The comparison of inference efficiency of a model with PI, a baseline model that naively concatenates
261"
INFERENCE EFFICIENCY,0.5834896810506567,"prompts to the input, Fusion-in-Decoder (FiD) [12], and Linear Transformer [13] are shown in
262"
INFERENCE EFFICIENCY,0.5853658536585366,"Table 1. We consider FiD as one of the options for processing long inputs because it processes
263"
INFERENCE EFFICIENCY,0.5872420262664165,"long input sequences by encoding chunks of input sequences separately, reducing the quadratic
264"
INFERENCE EFFICIENCY,0.5891181988742964,"complexity to linear. Linear Transformer also reduces the complexity to linear by linearizing the
265"
INFERENCE EFFICIENCY,0.5909943714821764,"Table 2: Prompt Injection performance on three prompt-dependent tasks. W/ PROMPT stands for the
upper bound (unconstrained) method, which uses the prompt during inference by appending it to the
input. W/O PROMPT depicts the lower bound method of not utilizing the prompts at all. Lastly, we
show three W/ PI methods: CP and CP W/ CURR stand for the Continued Pre-training (baseline) and
the Continued Pre-training with curricular, respectively, as explained in Section 4.1; PING depicts
our novel proposed method utilizing distillation."
INFERENCE EFFICIENCY,0.5928705440900562,"Dialogue
Semantic Parsing
Task Generalization"
INFERENCE EFFICIENCY,0.5947467166979362,"PERSONA-CHAT
Spider
WSC
RTE
COPA"
INFERENCE EFFICIENCY,0.5966228893058161,"PPL (â†“)
PI Score
EM EA PI Score ACC PI Score ACC PI Score ACC PI Score"
INFERENCE EFFICIENCY,0.5984990619136961,"W/ PROMPT
8.83
-
57.9 61.3
-
63.6
-
67.9
-
67.3
-"
INFERENCE EFFICIENCY,0.600375234521576,W/O PROMPT
INFERENCE EFFICIENCY,0.6022514071294559,"W/O PI
11.01
-
14.5 15.1
-
44.0
-
64.2
-
60.0
-"
INFERENCE EFFICIENCY,0.6041275797373359,"W/ PI
CP
10.85
0.073
16.9 17.5
0.054
54.5
0.536
67.7
0.946
64.8
0.658
CP W/ CURR
10.61
0.183
17.7 18.4
0.072
50.8
0.347
68.2
1.08
64.1
0.562
PING
9.82
0.546
36.6 41.7
0.507
63.7
1.005
64.2
0
60.6
0.082"
INFERENCE EFFICIENCY,0.6060037523452158,"attention mechanism. We measure FLOPs and forward propagation latency via DeepSpeed Flops
266"
INFERENCE EFFICIENCY,0.6078799249530957,"profiler 2 using a single 16GB T4 GPU.
267"
INFERENCE EFFICIENCY,0.6097560975609756,"As shown in Table 1, T5 W/ PI is much more efficient than other models, especially as we assume
268"
INFERENCE EFFICIENCY,0.6116322701688556,"a longer prompt length. This is because the efficiency of PI remains the same independent of the
269"
INFERENCE EFFICIENCY,0.6135084427767354,"prompt length while the costs of others increase linearly. Specifically, when the prompt length is
270"
INFERENCE EFFICIENCY,0.6153846153846154,"8 times the modelâ€™s max input sequence length, one can achieve 80Ã— computational efficiency in
271"
INFERENCE EFFICIENCY,0.6172607879924953,"terms of FLOPs by applying PI. Furthermore, in a scenario where the prompt length is 28Ã— the
272"
INFERENCE EFFICIENCY,0.6191369606003753,"modelâ€™s max input sequence length (shown in Section 6.3 when trying to utilize a long persona that is
273"
INFERENCE EFFICIENCY,0.6210131332082551,"over 13,000 token length long), previous approaches show an out-of-memory (OOM) issue using
274"
INFERENCE EFFICIENCY,0.6228893058161351,"the 16GB T4 GPU, and it is impossible to utilize them. PI is estimated to be 280Ã— more efficient in
275"
INFERENCE EFFICIENCY,0.624765478424015,"terms of total FLOPs if there is no OOM issue.
276"
TASK PERFORMANCE,0.626641651031895,"6.2
Task Performance
277"
TASK PERFORMANCE,0.6285178236397748,"In Table 2, we report the task performance obtained by applying different PI methods on three
278"
TASK PERFORMANCE,0.6303939962476548,"prompt-dependent tasks. PI scores are also obtained as introduced in Section 3. For all of W/ PI
279"
TASK PERFORMANCE,0.6322701688555347,"methods, we observe an overall increase in performance compared to W/O PROMPT, indicating
280"
TASK PERFORMANCE,0.6341463414634146,"successful injection of prompts into the parameters of the model through PI methods.
281"
TASK PERFORMANCE,0.6360225140712945,"For the results, while CP gives modest performance improvement over W/O PROMPT, the results
282"
TASK PERFORMANCE,0.6378986866791745,"of CP W/ CURR show that leveraging curricula during continued pre-training is effective in some
283"
TASK PERFORMANCE,0.6397748592870544,"cases. CP W/ CURR performs better compared to CP in PERSONA-CHAT, Spider, and RTE; it even
284"
TASK PERFORMANCE,0.6416510318949343,"outperforms W/ PROMPT in RTE. On the other hand, PING significantly improves performance from
285"
TASK PERFORMANCE,0.6435272045028143,"CP in PERSONA-CHAT, Spider, and WSC, outperforming W/ PROMPT in WSC. This sheds light on
286"
TASK PERFORMANCE,0.6454033771106942,"the possibility that PI may be able to reach the upper bound (unconstrained) performance. However,
287"
TASK PERFORMANCE,0.6472795497185742,"the results show at the same time that there is still a gap between the performance of PI methods and
288"
TASK PERFORMANCE,0.649155722326454,"the upper bound W/ PROMPT that needs to be bridged in future work.
289"
TASK PERFORMANCE,0.651031894934334,"We find that the performance of different methods depends on the complexity of the input sequence
290"
TASK PERFORMANCE,0.6529080675422139,"structure. We believe that PING achieves a good performance in PERSONA-CHAT, Spider, and
291"
TASK PERFORMANCE,0.6547842401500938,"WSC because those datasets have relatively simple input sequences (short utterances; simple query;
292"
TASK PERFORMANCE,0.6566604127579737,"a sentence and two words, respectively). In datasets with many components or multiple complex
293"
TASK PERFORMANCE,0.6585365853658537,"sentences (e.g., COPA and RTE), the low quality of generated pseudo-inputs degrades the performance
294"
TASK PERFORMANCE,0.6604127579737336,"of PING. On the other hand, CP and CP W/ CURR perform better in datasets with complex structure.
295"
TASK PERFORMANCE,0.6622889305816135,"These findings encourage the community to explore a more integral PI method that can cover different
296"
TASK PERFORMANCE,0.6641651031894934,"datasets.
297"
TASK PERFORMANCE,0.6660412757973734,2https://www.deepspeed.ai/tutorials/flops-profiler/
TASK PERFORMANCE,0.6679174484052532,"ðŸ’‰
Elon Musk Q317521"
TASK PERFORMANCE,0.6697936210131332,"[...] 
I am a technology entrepreneur, investor, and 
engineer. I am the founder, CEO, and lead 
designer of SpaceX; co-founder, CEO, and 
product architect of Tesla, Inc.
[...]"
TASK PERFORMANCE,0.6716697936210131,Actual Utterances
TASK PERFORMANCE,0.6735459662288931,"ðŸ§‘ : what do you do? 
ðŸ¤– : i work for Tesla, my main concern is the battery pack.
ðŸ§‘ : are you a ceo?
ðŸ¤– : i am a founder of Tesla and the Chief Tech Officer.
ðŸ§‘ : what's on your mind?
ðŸ¤– : the people at Mars. it is one of the world's best places for man
ðŸ§‘ : what's your plan?
ðŸ¤– : to go to Mars. it sounds beautiful. what is your plan for Mars?
ðŸ§‘ : I'd also like to go to Mars!
ðŸ¤– : Mars would be absolutely wonderful! Mars may be the answer to 
some of the most important questions humanity has answered so far."
TASK PERFORMANCE,0.6754221388367729,"Figure 3: A real world example of Prompt Injection with a long prompt. (Left) The process of
injecting a Wikipedia article describing a person (Elon Musk) into a model with PI. The article is
more than 13,000 tokens long. (Right) Actual conversation between the persona injected model and a
human that is hand-picked."
LONG PROMPTS INJECTION,0.6772983114446529,"6.3
Long Prompts Injection
298"
LONG PROMPTS INJECTION,0.6791744840525328,"To demonstrate the effectiveness of PI on injection of long prompts into LMs, we show how the
299"
LONG PROMPTS INJECTION,0.6810506566604128,"method works with a real world example. We pick a Wikipedia page (Elon Musk), considering it as a
300"
LONG PROMPTS INJECTION,0.6829268292682927,"long persona description, and inject the entire article (over 13,000 tokens) into an LM trained with
301"
LONG PROMPTS INJECTION,0.6848030018761726,"PERSONA-CHAT. Here, we use T5-large as a base model and apply PING.
302"
LONG PROMPTS INJECTION,0.6866791744840526,"Figure 3 shows an actual instance of interactions with the LM that underwent PI through PING. The
303"
LONG PROMPTS INJECTION,0.6885553470919324,"responses show the LM successfully reflecting the description of the person on the Wikipedia page
304"
LONG PROMPTS INJECTION,0.6904315196998124,"without having the description appended to the input. Moreover, the inference of PI is 280Ã— more
305"
LONG PROMPTS INJECTION,0.6923076923076923,"computationally efficient in terms of FLOPs than the baseline, as shown in Section 6.1. Lastly, we
306"
LONG PROMPTS INJECTION,0.6941838649155723,"provide a live demo to allow interactions with an LM injected with the persona of Elon Musk.
307"
CONCLUSION,0.6960600375234521,"7
Conclusion
308"
CONCLUSION,0.6979362101313321,"Limitations and Future Work
While Prompt Injection (PI) enables performing prompt-dependent
309"
CONCLUSION,0.699812382739212,"tasks efficiently, there are limitations that needs to be addressed in future work. In particular, the
310"
CONCLUSION,0.701688555347092,"current PI methods cause task performance degradation. Moreover, the computational costs needed
311"
CONCLUSION,0.7035647279549718,"for the injection of prompts into the model parameters have not been extensively considered. For
312"
CONCLUSION,0.7054409005628518,"example, when considering previous conversation history as prompts to be injected in a multi-turn
313"
CONCLUSION,0.7073170731707317,"conversation setting, fast injection may also be a requirement for real-world application. Updating or
314"
CONCLUSION,0.7091932457786116,"adding a relatively small number of parameters [11, 31] may be a potential avenue for addressing the
315"
CONCLUSION,0.7110694183864915,"problems.
316"
CONCLUSION,0.7129455909943715,"In this paper, we propose Prompt Injection (PI), a novel formulation of injecting the prompt into the
317"
CONCLUSION,0.7148217636022514,"parameters of an LM, as an efficient alternative to attaching fixed prompts to the inputs for prompt-
318"
CONCLUSION,0.7166979362101313,"dependent tasks. Through experiments, we show that PI is much more computationally efficient (up
319"
CONCLUSION,0.7185741088180112,"to 280 times) in terms of total FLOPs for handling long prompts compared to the previous alternatives.
320"
CONCLUSION,0.7204502814258912,"We further explore baseline methodologies for PI and find that Pseudo-INput Generation (PING), a
321"
CONCLUSION,0.7223264540337712,"distillation-based approach, shows promising results in persona-dependent conversation, semantic
322"
CONCLUSION,0.724202626641651,"parsing, and zero-shot learning with task instructions. Through the explorations, we show that PI
323"
CONCLUSION,0.726078799249531,"can be a promising direction for conditioning language models with prompts, especially in scenarios
324"
CONCLUSION,0.7279549718574109,"with long and fixed prompts. To this end, we hope the community explores PI for achieving both
325"
CONCLUSION,0.7298311444652908,"performance and efficiency on prompt-dependent tasks.
326"
REFERENCES,0.7317073170731707,"References
327"
REFERENCES,0.7335834896810507,"[1] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.
328"
REFERENCES,0.7354596622889306,"ArXiv, abs/2004.05150, 2020.
329"
REFERENCES,0.7373358348968105,"[2] Yoshua Bengio, JÃ©rÃ´me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning.
330"
REFERENCES,0.7392120075046904,"In ICML â€™09, 2009.
331"
REFERENCES,0.7410881801125704,"[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
332"
REFERENCES,0.7429643527204502,"wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
333"
REFERENCES,0.7448405253283302,"Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M.
334"
REFERENCES,0.7467166979362101,"Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
335"
REFERENCES,0.7485928705440901,"Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-
336"
REFERENCES,0.7504690431519699,"ford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv,
337"
REFERENCES,0.7523452157598499,"abs/2005.14165, 2020.
338"
REFERENCES,0.7542213883677298,"[4] Daniel Fernando Campos. Curriculum learning for language modeling. ArXiv, abs/2108.02170,
339"
REFERENCES,0.7560975609756098,"2021.
340"
REFERENCES,0.7579737335834896,"[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
341"
REFERENCES,0.7598499061913696,"Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,
342"
REFERENCES,0.7617260787992496,"Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Baindoor Rao, Parker Barnes,
343"
REFERENCES,0.7636022514071295,"Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchin-
344"
REFERENCES,0.7654784240150094,"son, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
345"
REFERENCES,0.7673545966228893,"Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier
346"
REFERENCES,0.7692307692307693,"GarcÃ­a, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
347"
REFERENCES,0.7711069418386491,"Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani
348"
REFERENCES,0.7729831144465291,"Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,
349"
REFERENCES,0.774859287054409,"Aitor Lewkowycz, Erica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee,
350"
REFERENCES,0.776735459662289,"Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason
351"
REFERENCES,0.7786116322701688,"Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm:
352"
REFERENCES,0.7804878048780488,"Scaling language modeling with pathways. ArXiv, abs/2204.02311, 2022.
353"
REFERENCES,0.7823639774859287,"[6] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhut-
354"
REFERENCES,0.7842401500938087,"dinov. Transformer-xl: Attentive language models beyond a fixed-length context. In ACL,
355"
REFERENCES,0.7861163227016885,"2019.
356"
REFERENCES,0.7879924953095685,"[7] Xiang Deng, Ahmed Hassan Awadallah, Christopher Meek, Oleksandr Polozov, Huan Sun, and
357"
REFERENCES,0.7898686679174484,"Matthew Richardson. Structure-grounded pretraining for text-to-sql. ArXiv, abs/2010.12773,
358"
REFERENCES,0.7917448405253283,"2021.
359"
REFERENCES,0.7936210131332082,"[8] Mandy Guo, Joshua Ainslie, David C. Uthus, Santiago OntaÃ±Ã³n, Jianmo Ni, Yun-Hsuan
360"
REFERENCES,0.7954971857410882,"Sung, and Yinfei Yang. Longt5: Efficient text-to-text transformer for long sequences. ArXiv,
361"
REFERENCES,0.797373358348968,"abs/2112.07916, 2021.
362"
REFERENCES,0.799249530956848,"[9] Seungju Han, Beomsu Kim, Jin Yong Yoo, Seokjun Seo, Sangbum Kim, Enkhbayar Erdenee,
363"
REFERENCES,0.801125703564728,"and Buru Chang. Meet your favorite character: Open-domain chatbot mimicking fictional
364"
REFERENCES,0.8030018761726079,"characters with only a few utterances. arXiv preprint arXiv:2204.10825, 2022.
365"
REFERENCES,0.8048780487804879,"[10] Moshe Hazoom, Vibhor Malik, and Ben Bogin. Text-to-sql in the wild: A naturally-occurring
366"
REFERENCES,0.8067542213883677,"dataset based on stack exchange data. ArXiv, abs/2106.05006, 2021.
367"
REFERENCES,0.8086303939962477,"[11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and
368"
REFERENCES,0.8105065666041276,"Weizhu Chen. Lora: Low-rank adaptation of large language models. ArXiv, abs/2106.09685,
369"
REFERENCES,0.8123827392120075,"2021.
370"
REFERENCES,0.8142589118198874,"[12] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for
371"
REFERENCES,0.8161350844277674,"open domain question answering. In EACL, 2021.
372"
REFERENCES,0.8180112570356473,"[13] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franccois Fleuret. Transformers
373"
REFERENCES,0.8198874296435272,"are rnns: Fast autoregressive transformers with linear attention. ArXiv, abs/2006.16236, 2020.
374"
REFERENCES,0.8217636022514071,"[14] Boseop Kim, Hyoungseok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak, Dong Hyeon
375"
REFERENCES,0.8236397748592871,"Jeon, Sunghyun Park, Sung ju Kim, Seonhoon Kim, Dong Hyung Seo, Heungsub Lee, Minyoung
376"
REFERENCES,0.8255159474671669,"Jeong, Sungjae Lee, Minsub Kim, SukHyun Ko, Seokhun Kim, Taeyong Park, Jinuk Kim,
377"
REFERENCES,0.8273921200750469,"Soyoung Kang, Na-Hyeon Ryu, Kang Min Yoo, Minsuk Chang, Soobin Suh, Sookyo In,
378"
REFERENCES,0.8292682926829268,"Jinseong Park, Kyungduk Kim, Hiun Kim, Jisu Jeong, Yong Goo Yeo, Dong hyun Ham, Do-
379"
REFERENCES,0.8311444652908068,"Hyoung Park, Min Young Lee, Jaewoo Kang, Inho Kang, Jung-Woo Ha, Woo Chul Park,
380"
REFERENCES,0.8330206378986866,"and Nako Sung. What changes can large-scale language models bring? intensive study on
381"
REFERENCES,0.8348968105065666,"hyperclova: Billions-scale korean generative pretrained transformers. ArXiv, abs/2109.04650,
382"
REFERENCES,0.8367729831144465,"2021.
383"
REFERENCES,0.8386491557223265,"[15] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
384"
REFERENCES,0.8405253283302064,"abs/1412.6980, 2015.
385"
REFERENCES,0.8424015009380863,"[16] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and
386"
REFERENCES,0.8442776735459663,"Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context
387"
REFERENCES,0.8461538461538461,"learning. 2022.
388"
REFERENCES,0.8480300187617261,"[17] Qian Liu, Yihong Chen, B. Chen, Jian-Guang Lou, Zixuan Chen, Bin Zhou, and Dongmei Zhang.
389"
REFERENCES,0.849906191369606,"You impress me: Dialogue generation via mutual persona perception. ArXiv, abs/2004.05388,
390"
REFERENCES,0.851782363977486,"2020.
391"
REFERENCES,0.8536585365853658,"[18] Pierre-Emmanuel MazarÃ©, Samuel Humeau, Martin Raison, and Antoine Bordes. Training
392"
REFERENCES,0.8555347091932458,"millions of personalized dialogue agents. In EMNLP, 2018.
393"
REFERENCES,0.8574108818011257,"[19] Qiao Qian, Minlie Huang, Haizhou Zhao, Jingfang Xu, and Xiaoyan Zhu. Assigning personal-
394"
REFERENCES,0.8592870544090057,"ity/profile to a chatting machine for coherent conversation generation. In IJCAI, 2018.
395"
REFERENCES,0.8611632270168855,"[20] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
396"
REFERENCES,0.8630393996247655,"models are unsupervised multitask learners. 2019.
397"
REFERENCES,0.8649155722326454,"[21] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
398"
REFERENCES,0.8667917448405253,"Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
399"
REFERENCES,0.8686679174484052,"text-to-text transformer. ArXiv, abs/1910.10683, 2020.
400"
REFERENCES,0.8705440900562852,"[22] Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu,
401"
REFERENCES,0.8724202626641651,"Myle Ott, Kurt Shuster, Eric Michael Smith, Y.-Lan Boureau, and Jason Weston. Recipes for
402"
REFERENCES,0.874296435272045,"building an open-domain chatbot. In EACL, 2021.
403"
REFERENCES,0.8761726078799249,"[23] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang A. Sutawika, Zaid
404"
REFERENCES,0.8780487804878049,"Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M SAIFUL
405"
REFERENCES,0.8799249530956847,"BARI, Canwen Xu, Urmish Thakker, Shanya Sharma, Eliza Szczechla, Taewoon Kim, Gunjan
406"
REFERENCES,0.8818011257035647,"Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang,
407"
REFERENCES,0.8836772983114447,"Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang,
408"
REFERENCES,0.8855534709193246,"Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault FÃ©vry, Jason Alan
409"
REFERENCES,0.8874296435272045,"Fries, Ryan Teehan, Stella Rose Biderman, Leo Gao, T. G. Owe Bers, Thomas Wolf, and
410"
REFERENCES,0.8893058161350844,"Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. ArXiv,
411"
REFERENCES,0.8911819887429644,"abs/2110.08207, 2021.
412"
REFERENCES,0.8930581613508443,"[24] Timo Schick and Hinrich SchÃ¼tze. Itâ€™s not just size that matters: Small language models are
413"
REFERENCES,0.8949343339587242,"also few-shot learners. ArXiv, abs/2009.07118, 2021.
414"
REFERENCES,0.8968105065666041,"[25] Haoyu Song, Weinan Zhang, Yiming Cui, Dong Wang, and Ting Liu. Exploiting persona
415"
REFERENCES,0.8986866791744841,"information for diverse generation of conversational responses. In IJCAI, 2019.
416"
REFERENCES,0.900562851782364,"[26] Alane Suhr, Ming-Wei Chang, Peter Shaw, and Kenton Lee. Exploring unexplored generaliza-
417"
REFERENCES,0.9024390243902439,"tion challenges for cross-database semantic parsing. In ACL, 2020.
418"
REFERENCES,0.9043151969981238,"[27] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey.
419"
REFERENCES,0.9061913696060038,"ACM Computing Surveys (CSUR), 2022.
420"
REFERENCES,0.9080675422138836,"[28] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam M. Shazeer, Apoorv Kulshreshtha,
421"
REFERENCES,0.9099437148217636,"Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, Yaguang Li, Hongrae Lee,
422"
REFERENCES,0.9118198874296435,"Huaixiu Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry
423"
REFERENCES,0.9136960600375235,"Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten
424"
REFERENCES,0.9155722326454033,"Bosma, Yanqi Zhou, Chung-Ching Chang, I. A. Krivokon, Willard James Rusch, Marc Pickett,
425"
REFERENCES,0.9174484052532833,"Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos,
426"
REFERENCES,0.9193245778611632,"Toju Duke, Johnny Hartz SÃ¸raker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz,
427"
REFERENCES,0.9212007504690432,"Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo,
428"
REFERENCES,0.9230769230769231,"Ravindran Rajakumar, Alena Butryna, Matthew Lamm, V. O. Kuzmina, Joseph Fenton, Aaron
429"
REFERENCES,0.924953095684803,"Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak,
430"
REFERENCES,0.926829268292683,"Ed Chi, and Quoc Le. Lamda: Language models for dialog applications. ArXiv, abs/2201.08239,
431"
REFERENCES,0.9287054409005628,"2022.
432"
REFERENCES,0.9305816135084428,"[29] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
433"
REFERENCES,0.9324577861163227,"Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. ArXiv, abs/1706.03762,
434"
REFERENCES,0.9343339587242027,"2017.
435"
REFERENCES,0.9362101313320825,"[30] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,
436"
REFERENCES,0.9380863039399625,"Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose
437"
REFERENCES,0.9399624765478424,"language understanding systems. In NeurIPS, 2019.
438"
REFERENCES,0.9418386491557224,"[31] Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao,
439"
REFERENCES,0.9437148217636022,"Daxin Jiang, and Ming Zhou. K-adapter: Infusing knowledge into pre-trained models with
440"
REFERENCES,0.9455909943714822,"adapters. In FINDINGS, 2021.
441"
REFERENCES,0.9474671669793621,"[32] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
442"
REFERENCES,0.949343339587242,"Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. ArXiv,
443"
REFERENCES,0.9512195121951219,"abs/2109.01652, 2021.
444"
REFERENCES,0.9530956848030019,"[33] Sean Welleck, Jason Weston, Arthur D. Szlam, and Kyunghyun Cho. Dialogue natural language
445"
REFERENCES,0.9549718574108818,"inference. In ACL, 2019.
446"
REFERENCES,0.9568480300187617,"[34] Alexander Wettig, Tianyu Gao, Zexuan Zhong, and Danqi Chen. Should you mask 15% in
447"
REFERENCES,0.9587242026266416,"masked language modeling? arXiv preprint arXiv:2202.08005, 2022.
448"
REFERENCES,0.9606003752345216,"[35] Thomas Wolf, Victor Sanh, Julien Chaumond, and Clement Delangue. Transfertransfo: A trans-
449"
REFERENCES,0.9624765478424016,"fer learning approach for neural network based conversational agents. ArXiv, abs/1901.08149,
450"
REFERENCES,0.9643527204502814,"2019.
451"
REFERENCES,0.9662288930581614,"[36] Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga,
452"
REFERENCES,0.9681050656660413,"Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang,
453"
REFERENCES,0.9699812382739212,"Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng
454"
REFERENCES,0.9718574108818011,"Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. Unifiedskg: Unifying
455"
REFERENCES,0.9737335834896811,"and multi-tasking structured knowledge grounding with text-to-text language models. ArXiv,
456"
REFERENCES,0.975609756097561,"abs/2201.05966, 2022.
457"
REFERENCES,0.9774859287054409,"[37] Tao Yu, Rui Zhang, Kai-Chou Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma,
458"
REFERENCES,0.9793621013133208,"Irene Z Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir R. Radev. Spider:
459"
REFERENCES,0.9812382739212008,"A large-scale human-labeled dataset for complex and cross-domain semantic parsing and
460"
REFERENCES,0.9831144465290806,"text-to-sql task. In EMNLP, 2018.
461"
REFERENCES,0.9849906191369606,"[38] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti,
462"
REFERENCES,0.9868667917448405,"Santiago OntaÃ±Ã³n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big
463"
REFERENCES,0.9887429643527205,"bird: Transformers for longer sequences. ArXiv, abs/2007.14062, 2020.
464"
REFERENCES,0.9906191369606003,"[39] Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur D. Szlam, Douwe Kiela, and Jason Weston.
465"
REFERENCES,0.9924953095684803,"Personalizing dialogue agents: I have a dog, do you have pets too? In ACL, 2018.
466"
REFERENCES,0.9943714821763602,"[40] Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar,
467"
REFERENCES,0.9962476547842402,"and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision.
468"
REFERENCES,0.99812382739212,"ArXiv, abs/2107.02192, 2021.
469"
