Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0006958942240779402,"Causal discovery from observational data holds great promise, but existing methods
1"
ABSTRACT,0.0013917884481558804,"rely on strong assumptions about the underlying causal structure, often requiring
2"
ABSTRACT,0.0020876826722338203,"full observability of all relevant variables. We tackle these challenges by leveraging
3"
ABSTRACT,0.0027835768963117608,"the score function ∇log p(X) of observed variables for causal discovery and
4"
ABSTRACT,0.003479471120389701,"propose the following contributions. First, we generalize the existing results of
5"
ABSTRACT,0.0041753653444676405,"identifiability with the score to additive noise models with minimal requirements
6"
ABSTRACT,0.0048712595685455815,"on the causal mechanisms. Second, we establish conditions for inferring causal
7"
ABSTRACT,0.0055671537926235215,"relations from the score even in the presence of hidden variables; this result is
8"
ABSTRACT,0.006263048016701462,"two-faced: we demonstrate the score’s potential as an alternative to conditional
9"
ABSTRACT,0.006958942240779402,"independence tests to infer the equivalence class of causal graphs with hidden
10"
ABSTRACT,0.007654836464857342,"variables, and we provide the necessary conditions for identifying direct causes in
11"
ABSTRACT,0.008350730688935281,"latent variable models. Building on these insights, we propose a flexible algorithm
12"
ABSTRACT,0.009046624913013222,"for causal discovery across linear, nonlinear, and latent variable models, which we
13"
ABSTRACT,0.009742519137091163,"empirically validate.
14"
INTRODUCTION,0.010438413361169102,"1
Introduction
15"
INTRODUCTION,0.011134307585247043,"The inference of causal effects from observations holds the potential for great impact arguably in any
16"
INTRODUCTION,0.011830201809324982,"domain of science, where it is crucial to be able to answer interventional and counterfactual queries
17"
INTRODUCTION,0.012526096033402923,"from observational data [1, 2, 3]. Existing causal discovery methods can be categorized based on
18"
INTRODUCTION,0.013221990257480862,"the information they can extract from the data [4], and the assumptions they rely on. Traditional
19"
INTRODUCTION,0.013917884481558803,"causal discovery methods (e.g. PC, GES [5, 6]) are general in their applicability but limited to the
20"
INTRODUCTION,0.014613778705636743,"inference of an equivalence class. Additional assumptions on the structural equations generating
21"
INTRODUCTION,0.015309672929714684,"effects from the cause are, in fact, imposed to ensure the identifiability of a causal order [7, 8, 9, 10].
22"
INTRODUCTION,0.016005567153792623,"As a consequence, existing methods for causal discovery require specialized and often untestable
23"
INTRODUCTION,0.016701461377870562,"assumptions, preventing their application to real-world scenarios.
24"
INTRODUCTION,0.017397355601948505,"Further, the majority of existing approaches are hindered by the assumption that all relevant causes
25"
INTRODUCTION,0.018093249826026444,"of the measured data are observed, which is necessary to interpret associations in the data as causal
26"
INTRODUCTION,0.018789144050104383,"relationships. Despite the convenience of this hypothesis, it is often not met in practice, and the solu-
27"
INTRODUCTION,0.019485038274182326,"tions relaxing this requirement face substantial limitations. The FCI algorithm [11] can only return an
28"
INTRODUCTION,0.020180932498260265,"equivalence class from the data. Appealing to additional restrictions ensures the identifiability of some
29"
INTRODUCTION,0.020876826722338204,"direct causal effects in the presence of latent variables: RCD [12] relies on the linear non-Gaussian
30"
INTRODUCTION,0.021572720946416143,"additive noise model, whereas CAM-UV [13] requires nonlinear additive mechanisms. Nevertheless,
31"
INTRODUCTION,0.022268615170494086,"the strict conditions on the structural equations hold back their applicability to more general settings.
32"
INTRODUCTION,0.022964509394572025,"Our paper tackles these challenges and can be put in the context of a recent line of work that
33"
INTRODUCTION,0.023660403618649965,"derives a connection between the score function ∇log p(X) and the causal graph underlying the
34"
INTRODUCTION,0.024356297842727904,"data-generating process [14, 15, 16, 17, 18, 19]. The use of the score for causal discovery is
35"
INTRODUCTION,0.025052192066805846,"practically appealing, as it yields advantages in terms of scalability to high dimensional graphs [16]
36"
INTRODUCTION,0.025748086290883786,"and guarantees of finite sample complexity bounds [20]. Instead of imposing assumptions that ensure
37"
INTRODUCTION,0.026443980514961725,"strong, though often impractical, theoretical guarantees, we organically demonstrate different levels of
38"
INTRODUCTION,0.027139874739039668,"identifiability based on the strength of the modeling hypotheses, always relying on the score function
39"
INTRODUCTION,0.027835768963117607,"to encode all the causal information in the data. Starting from results of Spantini et al. [21] and Lin
40"
INTRODUCTION,0.028531663187195546,"[22], we show how constraints on the Jacobian of the score ∇2 log p(X) can be used as an alternative
41"
INTRODUCTION,0.029227557411273485,"to conditional independence testing to identify the Markov equivalence class of causal models with
42"
INTRODUCTION,0.029923451635351428,"hidden variables. Further, we prove that the score function identifies the causal direction of additive
43"
INTRODUCTION,0.030619345859429367,"noise models, with minimal assumptions on the causal mechanisms. This extends the previous findings
44"
INTRODUCTION,0.031315240083507306,"of Montagna et al. [17], limited by the assumption of nonlinearity of the causal effects, and Ghoshal
45"
INTRODUCTION,0.032011134307585246,"and Honorio [14], limited to linear mechanisms. On these results, we build the main contributions
46"
INTRODUCTION,0.032707028531663185,"of our work, enabling the identification of direct causal effects in hidden variables models.
47"
INTRODUCTION,0.033402922755741124,"Our main contributions are as follows: (i) We present the necessary conditions for the identifiability
48"
INTRODUCTION,0.03409881697981907,"of direct causal effects and the presence of hidden variables with the score in the case of latent
49"
INTRODUCTION,0.03479471120389701,"variables models. (ii) We propose AdaScore (Adaptive Score-based causal discovery), a flexible
50"
INTRODUCTION,0.03549060542797495,"algorithm for causal discovery based on score matching estimation of ∇log p(X) [23]. Based on the
51"
INTRODUCTION,0.03618649965205289,"user’s belief about the plausibility of several modeling assumptions on the data, AdaScore can output
52"
INTRODUCTION,0.03688239387613083,"a Markov equivalence class, a directed acyclic graph, or a mixed graph, accounting for the presence
53"
INTRODUCTION,0.037578288100208766,"of unobserved variables. To the best of our knowledge, the broad class of causal models handled by
54"
INTRODUCTION,0.038274182324286705,"our method is unmatched by other approaches in the literature.
55"
MODEL DEFINITION AND RELATED WORKS,0.03897007654836465,"2
Model definition and related works
56"
MODEL DEFINITION AND RELATED WORKS,0.03966597077244259,"In this section, we introduce the formalism of structural causal models (SCMs), separately for the the
57"
MODEL DEFINITION AND RELATED WORKS,0.04036186499652053,"cases with and without hidden variables.
58"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.04105775922059847,"2.1
Causal model with observed variables
59"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.04175365344467641,"Let X be a set of random variables in R defined according to the set of structural equations
60"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.04244954766875435,"Xi := fi(XPAG
i , Ni), ∀i = 1, . . . , k.
(1)"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.04314544189283229,"Ni ∈R are mutually independent random variables with strictly positive density, known as noise
61"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.04384133611691023,"or error terms. The function fi is the causal mechanism mapping the set of direct causes XPAG
i
62"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.04453723034098817,"of Xi and the noise term Ni, to Xi’s value. A structural causal model (SCM) is defined as the
63"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.04523312456506611,"tuple (X, N, F, PN), where F = (fi)k
i=1 is the set of causal mechanisms, and PN is the joint
64"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.04592901878914405,"distribution relative to the density pN over the noise terms N ∈Rk. We define the causal graph G
65"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.04662491301322199,"as a directed acyclic graph (DAG) with nodes X = {X1, . . . , Xk}, and the set of edges defined as
66"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.04732080723729993,"{Xj →Xi : Xj ∈XPAG
i }, such that PAG
i are the indices of the parent nodes of Xi in the graph
67"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.04801670146137787,"G. (In the remainder of the paper, we adopt the following notation: given a set of random variables
68"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.04871259568545581,"Y = {Y1, . . . , Yn} and a set of indices Z ⊂N, then YZ = {Yi|i ∈Z, Yi ∈Y }.)
69"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.049408489909533754,"Under this model, the probability density of X satisfies the Markov factorization (e.g. Peters et al.
70"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.05010438413361169,"[1] Proposition 6.31):
71"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.05080027835768963,"p(x) = k
Y"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.05149617258176757,"i=1
p(xi|xPAG
i ),
(2)"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.05219206680584551,"where we adopt the convention of lowercase letters referring to realized random variables, and use p
to denote the density of different random objects, when the distinction is clear from the argument.
This factorization is equivalent to the global Markov condition (e.g. Peters et al. [1] Proposition 6.22)
that demands that for all {Xi, Xj} ∈X, XZ ⊆X \ {Xi, Xj}, then Xi |="
CAUSAL MODEL WITH OBSERVED VARIABLES,0.05288796102992345,"d
GXj|XZ =⇒Xi |="
CAUSAL MODEL WITH OBSERVED VARIABLES,0.05358385525400139,"Xj|XZ,"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.054279749478079335,where (· |=
CAUSAL MODEL WITH OBSERVED VARIABLES,0.054975643702157274,"· |·) denotes probabilistic conditional independence of Xi, Xj given XZ, and (· |="
CAUSAL MODEL WITH OBSERVED VARIABLES,0.055671537926235214,"d
G · |·)
72"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.05636743215031315,"is the notation for d-separation, a criterion of conditional independence defined on the graph G
73"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.05706332637439109,"(Definition 5 of the appendix). As it is commonly done, we assume that the reverse direction
74 Xi |="
CAUSAL MODEL WITH OBSERVED VARIABLES,0.05775922059846903,"Xj|XZ
=⇒Xi |="
CAUSAL MODEL WITH OBSERVED VARIABLES,0.05845511482254697,"d
GXj|XZ hold, and we say that the density p is faithful to the graph G
75"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.05915100904662491,"[2, 24] (hence the faithfulness assumption). Together with the global Markov condition, faithfulness
76"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.059846903270702856,"implies an equivalence between the probabilistic and graphical notions of conditional independence:
77 Xi |="
CAUSAL MODEL WITH OBSERVED VARIABLES,0.060542797494780795,Xj|XZ ⇐⇒Xi |=
CAUSAL MODEL WITH OBSERVED VARIABLES,0.061238691718858734,"d
GXj|XZ.
(3)"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.061934585942936674,"In general, several DAGs may entail the same set of d-separations: graphs sharing such common
78"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.06263048016701461,"structure form a Markov equivalence class (see Definition 6 in the appendix).
79"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.06332637439109255,"The above model assumes that there aren’t any unobserved causes of variables in X, other than the
80"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.06402226861517049,"noise terms in N. As we are interested in distributions with potential hidden variables, we will now
81"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.06471816283924843,"generalize our model to represent data-generating processes that may involve latent causes.
82"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.06541405706332637,"Definitions on graphs.
As graphs play a central role in our work, Appendix A.1 provides a
83"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.06610995128740431,"detailed overview of the fundamental notation and definitions that we rely on in the remainder of
84"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.06680584551148225,"the paper. For the next section, we advise the reader to be comfortable with the notions of ancestors
85"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.0675017397355602,"(Definition 2) and inducing paths (Definition 3) in DAGs.
86"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.06819763395963814,"Closely related works.
Several methods for the causal discovery of fully observable models using
87"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.06889352818371608,"the score have been recently proposed. Ghoshal and Honorio [14] demonstrates the identifiability of
88"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.06958942240779402,"the linear non-Gaussian model from the score, and it is complemented by Rolland et al. [15], which
89"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.07028531663187196,"shows the connection between score matching estimation of ∇log p(X) and the inference of causal
90"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.0709812108559499,"graphs underlying nonlinear additive noise models with Gaussian noise terms, also allowing for
91"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.07167710508002784,"sample complexity bounds [20]. Montagna et al. [17] provides identifiability results in the nonlinear
92"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.07237299930410578,"setting, without posing any restriction on the distribution of the noise terms. Montagna et al. [16]
93"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.07306889352818371,"is the first to show that the Jacobian of the score provides information equivalent to conditional
94"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.07376478775226165,"independence testing in the context of causal discovery, limited to the case of additive noise models.
95"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.0744606819763396,"All of these studies make specialized assumptions to find theoretical guarantees of identifiability,
96"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.07515657620041753,"whereas our paper provides a unifying view of causal discovery with the score function, which
97"
CAUSAL MODEL WITH OBSERVED VARIABLES,0.07585247042449547,"generalizes and expands the existing results.
98"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.07654836464857341,"2.2
Causal model with unobserved variables
99"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.07724425887265135,"Under the model (1), we consider the case where the set of variables X is partitioned into the disjoint
100"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.0779401530967293,"subsets of observed random variables V = {V1, . . . , Vd} and unobserved (or latent) random variables
101"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.07863604732080724,"U = {U1, . . . , Up}. We assume that the following set of structural equations is satisfied:
102"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.07933194154488518,"Vi := fi(VPAG
i , U i, Ni), ∀i = 1, . . . , d,
(4)"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.08002783576896312,"where U i stands for the set of unobserved parents of Vi, and VPAG
i = {Vk|k ∈PAG
i , Vk ∈V } are
103"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.08072372999304106,"the observed direct causes of Vi. Some of the causal relations and the conditional independencies
104"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.081419624217119,"implied by the set of equations (4) can be summarized in a graph obtained as a marginalization of the
105"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.08211551844119694,"DAG G onto the observable nodes V .
106"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.08281141266527488,"Definition 1 (Marginal graph, Zhang [25]). Let X = V ˙∪U and G be a DAG over X. The following
107"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.08350730688935282,"construction gives the marginal graph MG
V , with nodes V and edges found as follows:
108"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.08420320111343076,"• pair of nodes Vi, Vj are adjacent in the graph MG
V if and only if there is an inducing path
109"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.0848990953375087,"between them relative to U in G;
110"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.08559498956158663,"• for each pair of adjacent nodes Vi, Vj in MG
V , orient the edge as Vi →Vj if Vi is an ancestor
111"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.08629088378566457,"of Vj in G, else orient it as Vi ↔Vj.
112"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.08698677800974251,"We define the map G 7→MG
V as the marginalization of the DAG G onto V , the observable nodes.
113"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.08768267223382047,"The graph resulting from the above construction is a maximal ancestral graph (MAG, Definition 4),
114"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.0883785664578984,"hence we will often refer to it as the marginal MAG of G. Intuitively, a directed edge denotes the
115"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.08907446068197634,"presence of an ancestorship relation, whereas bidirected edges represent dependencies that can not be
116"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.08977035490605428,"removed by conditioning on any of the variables in the graph.
117"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.09046624913013222,"In the case of DAGs, d-separation encodes the probabilistic conditional independence relations
118"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.09116214335421016,"between the variables of X in the graph G, as explicit by Equation (3). Such notion of graphical sepa-
119"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.0918580375782881,"ration has a natural generalization to maximal ancestral graphs, known as m-separation (Definition 5
120"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.09255393180236604,"of the appendix). Zhang [25] shows that m-separation and d-separation are in fact equivalent (see
121"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.09324982602644398,"Lemma 1 of the appendix), such that given VZ ⊂V and {Vi, Vj} ⊂V , the following holds:
122 Vi |="
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.09394572025052192,"d
GVj|VZ \ {Vi, Vj} ⇐⇒Vi |="
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.09464161447459986,"m
MG
V Vj|VZ \ {Vi, Vj},
(5)"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.0953375086986778,where (· |=
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.09603340292275574,"m
MG
V · | ·) denotes m-separation relative to the graph MG
V . Just like with DAGs, MAGs
123"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.09672929714683368,"that imply the same set of conditional independencies define an equivalence class. Usually, the
124"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.09742519137091162,"common structure of these graphs is represented by partial ancestral graphs (PAGs, Definition 7 of
125"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.09812108559498957,"the appendix). We use PMG
V to denote the PAG relative to MG
V .
126"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.09881697981906751,"Problem definition.
In this work, our goal is to provide theoretical guarantees for the
identifiability of the Markov equivalence class of the marginal graph MG
V and its direct causal
effects with the score, where variables Vi are defined according to Equation (4). 127"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.09951287404314545,"Without further assumptions on the data-generating process, we can identify the graph MG
V only up
128"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.10020876826722339,"to its partial ancestral graph, as discussed in the next section.
129"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.10090466249130133,"Closely related works.
Causal discovery with latent variables have been first studied in the context
130"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.10160055671537926,"of constraint-based approaches with the FCI algorithm [11], which shows the identifiability of the
131"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.1022964509394572,"equivalence class of a marginalized graph via conditional independence testing. The RCD and
132"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.10299234516353514,"CAM-UV [12, 13] approaches instead demonstrate the inferrability of directed causal edges via
133"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.10368823938761308,"regression and residuals independence testing. Both methods rely on strong assumptions on the
134"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.10438413361169102,"causal mechanisms: their theoretical guarantees apply to models where the effects are generated by a
135"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.10508002783576896,"linear (RCD) or nonlinear (CAM-UV) additive contribution of each cause. Our work demonstrates
136"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.1057759220598469,"that using the score function for causal discovery unifies and generalizes these results, presenting
137"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.10647181628392484,"an alternative to conditional independence testing for constraint-based methods, and being agnostic
138"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.10716771050800278,"about the class of causal mechanisms of the observed variables, under the weaker requirement of
139"
CAUSAL MODEL WITH UNOBSERVED VARIABLES,0.10786360473208072,"additivity of the noise terms.
140"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.10855949895615867,"3
Theory for a score-based test of separation
141"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.10925539318023661,"In this section, we show that for V ⊆X generated according to Equation (4) the Hessian matrix of
142"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.10995128740431455,"log p(V ) identifies the equivalence class of the marginal MAG MG
V . It has already been proven that
143"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.11064718162839249,"cross-partial derivatives of the log-likelihood are informative about a set of conditional independence
144"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.11134307585247043,"relationships between random variables: Spantini et al. [21] (Lemma 4.1) shows that, given VZ ⊆X
145"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.11203897007654837,"such that {Vi, Vj} ⊆VZ, then
146 ∂2"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.1127348643006263,"∂Vi∂Vj
log p(VZ) = 0 ⇐⇒Vi |="
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.11343075852470424,"Vj|VZ \ {Vi, Vj}.
(6)"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.11412665274878218,"Equation (3) resulting from faithfulness and the directed global Markov property immediately
147"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.11482254697286012,"implies that this expression can be used as a test of conditional independence to identify the Markov
148"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.11551844119693806,"equivalence class of the graph MG
V , as commonly done in constraint-based causal discovery (for
149"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.116214335421016,"reference, see e.g. Section 3 in Glymour et al. [4]). This result generalizes Lemma 1 of Montagna et al.
150"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.11691022964509394,"[16], where it is used to define constraints to infer edges in the causal structure without latent variables.
151"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.11760612386917188,"Proposition 1 (Adapted1 from [21]). Let V be a set of random variables with strictly positive density
generated according to model (4). For each set VZ ⊆V of nodes in MG
V such that {Vi, Vj} ⊆VZ,
the following holds for each supported value vZ: ∂2"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.11830201809324982,"∂Vi∂Vj
log p(vZ) = 0 ⇐⇒Vi |="
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.11899791231732777,"m
MG
V Vj|VZ \ {Vi, Vj}."
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.11969380654140571,"The result of Proposition 1 presents an alternative to conditional independence testing in constraint-
152"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.12038970076548365,"based approaches to causal discovery, showing that the equivalence class of the graph MG
V can be
153"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.12108559498956159,"identified using the cross partial derivatives of the log-likelihood as a test of conditional independence
154"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.12178148921363953,"between variables, much in the spirit of the Fast Causal Inference algorithm [11]. Identifying the
155"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.12247738343771747,"1In their Lemma 4.1 Spantini et al. [21] provides the connection between vanishing cross-partial derivatives
of the log-likelihood and conditional independence of random variables. Note that this result does not depend on
the assumption of a generative model, thus holding beyond the set of structural equations (4). Our result adapts
their finding to the case when observations are generated according to a fully observable causal model."
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.12317327766179541,"Markov equivalence class is the most we can hope to achieve without further hypotheses. As we will
156"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.12386917188587335,"see in the next section, the score function can also help leverage additional restrictive assumptions on
157"
THEORY FOR A SCORE-BASED TEST OF SEPARATION,0.12456506610995129,"the causal mechanisms of Equation (4) to identify direct causal effects.
158"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.12526096033402923,"4
A theory of identifiability from the score
159"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.12595685455810718,"In this section, we show that, under additional assumptions on the data-generating process, we can
160"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.1266527487821851,"identify the direct causal relations that are not influenced by unobserved variables, as well as the
161"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.12734864300626306,"presence of unobserved active paths (Definition 5) between nodes in the marginalized graph MG
V .
162"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.12804453723034098,"As a preliminary step before diving into causal discovery with latent variables, we show how the
163"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.12874043145441894,"properties of the score function identify edges in directed acyclic graphs, that is in the absence of
164"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.12943632567849686,"latent variables (when U = ∅and G = MG
V ). The goal of the next section is two-sided: first, it
165"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.1301322199025748,"introduces the fundamental ideas connecting the score function to causal discovery that also apply to
166"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.13082811412665274,"hidden variable models, second, it extends the existing theory of causal discovery with score matching
167"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.1315240083507307,"to additive noise models with both linear and nonlinear mechanisms.
168"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.13221990257480862,"4.1
Warm up: identifiability without latent confounders
169"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.13291579679888657,"In this section, we summarise and extend the theoretical findings presented in Montagna et al. [17],
170"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.1336116910229645,"where the authors show how to derive constraints on the score function that identify the causal order of
171"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.13430758524704245,"the DAG G where all the variables in the set X are observed. Define the structural relations of (1) as:
172"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.1350034794711204,"Xi := hi(XPAG
i ) + Ni, i = 1, . . . , k,
(7)"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.13569937369519833,"with three times continuously differentiable mechanisms hi, noise terms centered at zero, and strictly
173"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.13639526791927628,"positive density pX. Given the Markov factorization of Equation (2), the components of the score
174"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.1370911621433542,"function ∇log p(x) are:
175"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.13778705636743216,"∂Xi log p(x) = ∂Xi log p(xi|xPAG
i ) +
X"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.13848295059151008,"j∈CHG
i"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.13917884481558804,"∂Xi log p(xj|xPAG
j )"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.13987473903966596,"= ∂Ni log p(ni) −
X"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.14057063326374392,"j∈CHG
i"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.14126652748782184,"∂Xihj(xPAG
j )∂Nj log p(nj),
(8)"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.1419624217118998,"where CHG
i denotes the set of children of node Xi. We observe that if a node Xs is a sink, i.e. a
176"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.14265831593597772,"node satisfying CHG
s = ∅, then the summation over the children vanishes, implying that:
177"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.14335421016005567,"∂Xs log p(x) = ∂Ns log p(ns).
(9)"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.1440501043841336,"The key point is that the score component of a sink node is a function of its structural equation noise
178"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.14474599860821155,"term, such that one could learn a consistent estimator of ∂Xs log pX from a set of observations of the
179"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.1454418928322895,"noise term Ns. Given that, in general, one has access to X samples rather than observations of the
180"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.14613778705636743,"noise random variables, authors in Montagna et al. [17] show that Ns of a sink node can be consistently
181"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.14683368128044538,"estimated from i.i.d. realizations of X. For each node X1, . . . , Xk, we define the quantity:
182"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.1475295755045233,"Ri := Xi −E[Xi|X\Xi],
(10)"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.14822546972860126,"where X\Xi are the random variables in the set X \ {Xi}. E[Xi|X\Xi] is the optimal least squares
183"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.1489213639526792,"predictor of Xi from all the remaining nodes in the graph, and Ri is the regression residual. For
184"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.14961725817675714,"a sink node Xs, the residual satisfies:
185"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.15031315240083507,"Rs = Ns,
(11)
which can be seen by rewriting E[Xs|X\Xs]
=
hs(XPAG
s ) + E[Ns|XDEG
s , XNDG
s ]
=
186"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.15100904662491302,"hs(XPAG
s ) + E[Ns], where XDEG
s and XNDG
s denotes the descendants and non-descendants of Xs,
187"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.15170494084899094,"respectively. Equations (9) and (11) together imply that the score ∂Ns log p(Ns) is a function of Rs,
188"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.1524008350730689,"such that it is possible to find a consistent approximator of the score of a sink from observations of Rs.
189"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.15309672929714682,"Proposition 2 (Generalization of Lemma 1 in Montagna et al. [17]). Let X be a set of random
190"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.15379262352122477,"variables, generated by a restricted additive noise model (Definition 9) with structural equations (7),
191"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.1544885177453027,"and let Xj ∈X. Consider rj in the support of Rj. Then:
192"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.15518441196938065,"Xj is a sink ⇐⇒E
h 
E

∂Xj log p(X) | Rj = rj

−∂Xj log p(X)
2i
= 0.
(12)"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.1558803061934586,"Our result generalizes Lemma 1 in Montagna et al. [17], as they assume X generated by an
193"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.15657620041753653,"identifiable additive noise model with nonlinear mechanisms. Instead, we remove the nonlinearity
194"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.15727209464161448,"assumption and make the weaker hypothesis of a restricted additive noise model, which is provably
195"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.1579679888656924,"identifiable [9], in the formal sense defined in the appendix (Definition 8). This result doesn’t come
196"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.15866388308977036,"as a surprise, given the previous findings of Ghoshal and Honorio [14] showing that the score infers
197"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.1593597773138483,"linear non-Gaussian additive noise models: Proposition 2 provides a unifying and general theory
198"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.16005567153792624,"for the identifiability of models with potentially mixed linear and nonlinear mechanisms.
199"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.16075156576200417,"Based on these insights, Montagna et al. [17] propose the NoGAM algorithm to exploit the con-
200"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.16144745998608212,"dition in (12) for identifying the causal order of the graph: being E [∂Xi log p(X) | Ri] the opti-
201"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.16214335421016005,"mal least squares estimator of the score of node Xi from Ri, a sink node is characterized as the
202"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.162839248434238,"argmini E [E [∂Xi log p(X) | Ri] −∂Xi log p(X)]2, where in practice the residuals Ri, the score
203"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.16353514265831592,"components and the least squares estimators are replaced by their empirical counterparts. After a
204"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.16423103688239388,"sink node is identified, it is removed from the graph and assigned a position in the order, and the
205"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.1649269311064718,"procedure is iteratively repeated up to the source nodes. Being the score estimated by score matching
206"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.16562282533054976,"techniques [23], we usually make reference to score matching-based causal discovery.
207"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.1663187195546277,"In the next section, we show how we can generalize these results to identify direct causal effects
208"
A THEORY OF IDENTIFIABILITY FROM THE SCORE,0.16701461377870563,"between a pair of variables in the marginal MAG MG
V when U ̸= ∅
209"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.1677105080027836,"4.2
Identifiability in the presence of latent confounders
210"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.1684064022268615,"We now introduce the last of our main theoretical results, that is: given a pair of nodes Vi, Vj that
211"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.16910229645093947,"are adjacent in the graph MG
V with U ̸= ∅, we can use the score function to identify the presence
212"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.1697981906750174,"of a direct causal effect between Vi and Vj, or that of an active path that is influenced by unobserved
213"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.17049408489909534,"variables. Given that the causal model of Equation (4) ensures identifiability only up to the equivalence
214"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.17118997912317327,"class, we need additional restrictive assumptions. In particular, we enforce an additive noise model
215"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.17188587334725122,"with respect to both the observed and unobserved noise variables. This corresponds to an additive
216"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.17258176757132915,"noise model on the observed variables with the noise terms recentered by the latent causal effects.
217"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.1732776617954071,"Assumption 1 (SCM assumptions). The set of structural equations of the observable variables
218"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.17397355601948503,"specified in (4) is now defined as:
219"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.17466945024356298,"Vi := fi(VPAG
i ) + gi(U i) + Ni, ∀i = 1, . . . , d,
(13)"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.17536534446764093,"assuming the mechanisms fi to be of class C3(R
|VPAG
i
|), and mutually independent noise terms with
220"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.17606123869171886,"strictly positive density function. The Ni’s are assumed to be non-Gaussian when fi is linear in some
221"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.1767571329157968,"of its arguments.
222"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.17745302713987474,"Crucially, our hypothesis is weaker than those required by two state-of-the-art approaches, CAM-UV
223"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.1781489213639527,"[13] and RCD [12]: CAM-UV assumes a Causal Additive Model (CAM) with structural equations
224"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.17884481558803061,with nonlinear mechanisms in the form Vi := P
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.17954070981210857,"k∈PAG
i fik(Vk) + P"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.1802366040361865,"U i
k gik(U i
k) + Ni, and RCD
225"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.18093249826026445,"requires an additive noise model with linear effects of both the latent and observed causes. Thus,
226"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.18162839248434237,"our model encompasses and extends the nonlinear and linear settings of CAM-UV and RCD, such
227"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.18232428670842032,"that the theory developed in the remainder of the section is valid for a broader class of causal models.
228"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.18302018093249825,"Our first step is rewriting the structural relations in (13) as:
229"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.1837160751565762,"Vi := fi(VPAG
i ) + ˜Ni,"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.18441196938065413,"˜Ni := gi(U i) + Ni, ∀i = 1, . . . , d,
(14)"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.18510786360473208,"which provides an additive noise model in the form of (7). Next, we define the following regression
230"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.18580375782881003,"residuals for any node Vk in the graph MG
V :
231"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.18649965205288796,"Rk(VZ) := Vk −E[Vk | VZ\{k}],
(15)"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.1871955462769659,"where VZ\{k} denotes the set of random variables VZ \ {Vk}.
232"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.18789144050104384,"Given these definitions, we are ready to show how directed edges, and the presence of unobserved
233"
IDENTIFIABILITY IN THE PRESENCE OF LATENT CONFOUNDERS,0.1885873347251218,"variables can be identified from the score of linear and nonlinear additive noise models.
234"
IDENTIFIABILITY OF DIRECTED EDGES,0.18928322894919972,"4.2.1
Identifiability of directed edges
235"
IDENTIFIABILITY OF DIRECTED EDGES,0.18997912317327767,"Consider Vi, Vj adjacent nodes in the PAG PMG
V : we want to investigate when a direct causal
236"
IDENTIFIABILITY OF DIRECTED EDGES,0.1906750173973556,"effect Vi ∈VPAG
j can be identified from the score. We make the following observations: for
237"
IDENTIFIABILITY OF DIRECTED EDGES,0.19137091162143355,"VZ = VPAG
j ∪{Vj} and VPAG
j |="
IDENTIFIABILITY OF DIRECTED EDGES,0.19206680584551147,"G
d U j, by Equation (15) it follows
238"
IDENTIFIABILITY OF DIRECTED EDGES,0.19276270006958943,"Rj(VZ) = ˜
Nj −E[ ˜
Nj],
(16)"
IDENTIFIABILITY OF DIRECTED EDGES,0.19345859429366735,"where we use VPAG
j |="
IDENTIFIABILITY OF DIRECTED EDGES,0.1941544885177453,"G
d U j to write E[ ˜Nj|VZ\{j}] = E[ ˜Nj]. Moreover, we note that Vj is a sink node
239"
IDENTIFIABILITY OF DIRECTED EDGES,0.19485038274182323,"relative to MG
VZ, the marginalization of G onto VZ. In analogy to the case without latent variables, we
240"
IDENTIFIABILITY OF DIRECTED EDGES,0.19554627696590118,"can show that ∂Vj log p(VZ) is a function of ˜
Nj, the error term in the additive noise model of Equation
241"
IDENTIFIABILITY OF DIRECTED EDGES,0.19624217118997914,"(14), such that the score of Vj can be consistently predicted from observations of the residual Rj(VZ).
242"
IDENTIFIABILITY OF DIRECTED EDGES,0.19693806541405706,"Proposition 3. Let X be generated by a restricted additive noise model with structural equations (7),
243"
IDENTIFIABILITY OF DIRECTED EDGES,0.19763395963813501,"and causal graph G. Consider Vi, Vj adjacent in MG
V , marginalization of G. Further, assume that
244"
IDENTIFIABILITY OF DIRECTED EDGES,0.19832985386221294,"the score component ∂Vj log p(VZ) is not constant for uncountable values of VZ.
245"
IDENTIFIABILITY OF DIRECTED EDGES,0.1990257480862909,"(i) Let VZ = VPAG
j ∪{Vi, Vj}, and rj ∈R in the support of Rj(VZ). Then:
246"
IDENTIFIABILITY OF DIRECTED EDGES,0.19972164231036882,"VPAG
j |="
IDENTIFIABILITY OF DIRECTED EDGES,0.20041753653444677,"d
GU j ∧Vi ∈VPAG
j ⇐⇒E[∂Vj log p(VZ) −E[∂Vj log p(VZ)|Rj(VZ) = rj]]2 = 0."
IDENTIFIABILITY OF DIRECTED EDGES,0.2011134307585247,"(ii) Let VZ ⊆V , such that {Vi, Vj} ⊆VZ. Then:
247"
IDENTIFIABILITY OF DIRECTED EDGES,0.20180932498260265,"VPAG
j ̸ |="
IDENTIFIABILITY OF DIRECTED EDGES,0.20250521920668058,"d
GU j ∨Vi ̸∈VPAG
j ⇐⇒E[∂Vj log p(VZ) −E[∂Vj log p(VZ)|Rj(VZ) = rj]]2 ̸= 0."
IDENTIFIABILITY OF DIRECTED EDGES,0.20320111343075853,"Intuitively, the proposition has two essential implications. Part (i) provides the condition for the
248"
IDENTIFIABILITY OF DIRECTED EDGES,0.20389700765483645,"identifiability of the potential direct causal effect between a pair Vi, Vj, that is, when the association
249"
IDENTIFIABILITY OF DIRECTED EDGES,0.2045929018789144,"between Vj and its observed parents is not influenced by active paths that involve latent variables.
250"
IDENTIFIABILITY OF DIRECTED EDGES,0.20528879610299233,"This condition is necessary: given an active path such that VPAG
j ̸ |="
IDENTIFIABILITY OF DIRECTED EDGES,0.20598469032707029,"d
GU j, the score could not identify
251"
IDENTIFIABILITY OF DIRECTED EDGES,0.20668058455114824,"a direct causal effect Vi →Vj, which is the content of the second part of the proposition.
252"
IDENTIFIABILITY OF DIRECTED EDGES,0.20737647877522616,"We have established theoretical guarantees of identifiability for linear and nonlinear additive noise
253"
IDENTIFIABILITY OF DIRECTED EDGES,0.20807237299930412,"models, even in the presence of hidden variables: we find that the score function is a means for the
254"
IDENTIFIABILITY OF DIRECTED EDGES,0.20876826722338204,"identifiability of all direct parental relations that are not influenced by unobserved variables; all the
255"
IDENTIFIABILITY OF DIRECTED EDGES,0.20946416144746,"remaining arrowheads of the edges in the graph MG
V are identified no better than in the equivalence
256"
IDENTIFIABILITY OF DIRECTED EDGES,0.21016005567153792,"class. Based on these insights, we propose AdaScore, a score matching-based algorithm for the
257"
IDENTIFIABILITY OF DIRECTED EDGES,0.21085594989561587,"inference of Markov equivalence classes, direct causal effects, and the presence of latent variables.
258"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.2115518441196938,"4.3
A score-based algorithm for causal discovery
259"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.21224773834377175,"Building on our theory, we propose AdaScore, a generalization of NoGAM to linear and nonlinear
260"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.21294363256784968,"additive noise models with latent variables. The main strength of our approach is its adaptivity
261"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.21363952679192763,"with respect to structural assumptions: based on the user’s belief about the plausibility of several
262"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.21433542101600556,"modeling assumptions on the data, AdaScore can output an equivalence class (using the condition
263"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.2150313152400835,"of Proposition 1 instead of conditional independence testing in an FCI-like algorithm), a directed
264"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.21572720946416143,"acyclic graph (as in NoGAM), or a mixed graph, accounting for the presence of unobserved variables.
265"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.2164231036882394,"We now describe the version of our algorithm whose output is a mixed graph, where we rely on score
266"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.21711899791231734,"matching estimation of the score and its Jacobian (Appendix C.2). At an intuitive level, we find
267"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.21781489213639527,"unoriented edges using Proposition 1, i.e. checking for dependencies in the form of non-zero entries
268"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.21851078636047322,"in the Jacobian of the score via hypothesis testing on the mean, and find the edges’ directions via the
269"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.21920668058455114,"condition of Proposition 3, i.e. by estimating residuals of each node Xi and checking whether they can
270"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.2199025748086291,"correctly predict the i-th score entry (the vanishing mean squared errors are verified by hypothesis test
271"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.22059846903270702,"of zero mean). It would be tempting to simply find the skeleton (i.e. the graphical representation of
272"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.22129436325678498,"the constraints of an equivalence class) first via the well-known adjacency search of the FCI algorithm
273"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.2219902574808629,"and then iterate through all neighborhoods of all nodes to orient edges using Proposition 3. This
274"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.22268615170494085,"would be prohibitively expensive, as finding the skeleton is well-known to have super-exponential
275"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.22338204592901878,"computational complexity [11]. Instead, we propose an alternative solution: exploiting the fact that
276"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.22407794015309673,"some nodes may not be influenced by latent variables, we first use Proposition 2 to find sink nodes
277"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.22477383437717466,"that are not affected by latents (using hypothesis testing to find vanishing mean squared error in the
278"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.2254697286012526,"score predictions from the residuals), in the spirit of the NoGAM algorithm. If there is such a sink,
279"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.22616562282533054,"we search all its adjacent nodes via Proposition 1 (plus an optional pruning step for better accuracy,
280"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.2268615170494085,"Appendix C.2), and orient the inferred edges towards the sink. Else, if no sink can be found, we pick
281"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.22755741127348644,"a node in the graph and find its neighbors by Proposition 1, orienting its edges using the condition in
282"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.22825330549756437,"Proposition 3 (score estimation by residuals under latent effects). This way, we get an algorithm that
283"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.22894919972164232,"is polynomial in the best case (Appendix C.3). Details on AdaScore are provided in Appendix C,
284"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.22964509394572025,"while a pseudo-code summary is provided in the Algorithm 1 box.
285"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.2303409881697982,Algorithm 1 Simplified pseudo-code of AdaScore
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.23103688239387613,while nodes remain do
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.23173277661795408,if Proposition 3 finds a sink with all parents observed then
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.232428670842032,"add edges from adjacent nodes to sink
else"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.23312456506610996,"pick some remaining node Vi ∈V
prune neighbourhood of Vi using Proposition 1
orient edges adjacent to Vi using Proposition 3
if Vi has outgoing directed edge to some Vj ∈V then"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.23382045929018788,"continue with Vj
else"
A SCORE-BASED ALGORITHM FOR CAUSAL DISCOVERY,0.23451635351426584,"remove Vi form remaining nodes
prune remaining bidirected edges using Proposition 1"
EXPERIMENTS,0.23521224773834376,"5
Experiments
286"
EXPERIMENTS,0.2359081419624217,"We use the causally2 Python library [26] to generate synthetic data with known ground truths,
287"
EXPERIMENTS,0.23660403618649964,"created as Erdös-Rényi sparse and dense graphs, respectively with probability of edge between pair
288"
EXPERIMENTS,0.2372999304105776,"of nodes equals 0.3 and 0.5. We sample the data according to linear and nonlinear mechanisms with
289"
EXPERIMENTS,0.23799582463465555,"additive noise, where the nonlinear functions are parametrized by a neural network with random
290"
EXPERIMENTS,0.23869171885873347,"weights, a common approach in the literature [18, 26, 27, 28, 29]. Noise terms are sampled from a
291"
EXPERIMENTS,0.23938761308281142,"uniform distribution in the [−2, 2] range. Hidden causal effects are obtained by randomly picking
292"
EXPERIMENTS,0.24008350730688935,"two nodes and dropping the corresponding column from the data matrix. See Appendix D.1 for
293"
EXPERIMENTS,0.2407794015309673,"further details on the data generation. As metric, we consider the structural Hamming distance (SHD)
294"
EXPERIMENTS,0.24147529575504523,"[30, 31], a simple count of the number of incorrect edges, where missing and wrongly directed
295"
EXPERIMENTS,0.24217118997912318,"edges count as one error. We fix the level of the hypothesis tests of AdaScore to 0.05, which is a
296"
EXPERIMENTS,0.2428670842032011,"common choice in the absence of prior knowledge. We compare AdaScore to NoGAM, CAM-UV,
297"
EXPERIMENTS,0.24356297842727906,"RCD, and DirectLiNGAM, whose assumptions are detailed in Table 1. In the main manuscript, we
298"
EXPERIMENTS,0.24425887265135698,"comment on the results on datasets of 1000 observations from dense graphs, with and without latent
299"
EXPERIMENTS,0.24495476687543494,"variables. Additional experiments including those on sparse networks are presented in Appendix E.
300"
EXPERIMENTS,0.24565066109951286,"Our synthetic data are standardized by their empirical variance to remove shortcuts in the data [18, 32].
301"
EXPERIMENTS,0.24634655532359082,"Discussion.
Our experimental results on models without latent variables of Figure 1a show that when
302"
EXPERIMENTS,0.24704244954766874,"causal relations are linear, AdaScore can recover the causal graph with accuracy that is comparable
303"
EXPERIMENTS,0.2477383437717467,"with all the other benchmarks, with the exception of DirectLiNGAM. On nonlinear data AdaScore
304"
EXPERIMENTS,0.24843423799582465,"presents better performance than CAM-UV, RCD, and DirectLiNGAM while being comparable
305"
EXPERIMENTS,0.24913013221990257,"to NoGAM in accuracy. This is in line with our expectations: in the absence of finite sample
306"
EXPERIMENTS,0.24982602644398053,"errors and in the fully observable setting, NoGAM and AdaScore are indeed the same algorithms.
307"
EXPERIMENTS,0.25052192066805845,"When inferring under latent causal effects, Figure 1b, our method performs comparably to CAM-
308"
EXPERIMENTS,0.2512178148921364,"UV and RCD on graphs up to seven nodes while slightly degrading on nine nodes. Additionally,
309"
EXPERIMENTS,0.25191370911621436,"AdaScore outperforms NoGAM in this setting, as we would expect according to our theory. Overall,
310"
EXPERIMENTS,0.25260960334029225,"we observe that our method is robust to a variety of structural assumptions, with accuracy that is
311"
EXPERIMENTS,0.2533054975643702,"often comparable and sometimes better than competitors (as in nonlinear observable settings). We
312"
EXPERIMENTS,0.25400139178844816,"remark that although AdaScore does not clearly outperform the other baselines, its broad theoretical
313"
EXPERIMENTS,0.2546972860125261,"guarantees of identifiability are not matched by any available method in the literature; this makes it
314"
EXPERIMENTS,0.255393180236604,"an appealing option for inference in realistic scenarios that are hard to investigate with synthetic data,
315"
EXPERIMENTS,0.25608907446068196,"where the structural assumptions of the causal model underlying the observations are unknown.
316"
EXPERIMENTS,0.2567849686847599,2https://causally.readthedocs.io/en/latest/
EXPERIMENTS,0.25748086290883787,"adascore
camuv
nogam
rcd
lingam"
EXPERIMENTS,0.2581767571329158,"3
5
7
9
number of nodes 0 5 10 15 20 25 30 35 shd"
EXPERIMENTS,0.2588726513569937,linear
EXPERIMENTS,0.2595685455810717,"3
5
7
9
number of nodes 0 5 10 15 20 25 30 35 shd"
EXPERIMENTS,0.2602644398051496,nonlinear
EXPERIMENTS,0.2609603340292276,(a) Fully observable model
EXPERIMENTS,0.2616562282533055,"3
5
7
9
number of nodes 0 5 10 15 20 25 30 shd"
EXPERIMENTS,0.26235212247738343,linear
EXPERIMENTS,0.2630480167014614,"3
5
7
9
number of nodes 0 5 10 15 20 25 30 35 shd"
EXPERIMENTS,0.26374391092553934,nonlinear
EXPERIMENTS,0.26443980514961724,(b) Latent variables model
EXPERIMENTS,0.2651356993736952,"Figure 1: Empirical results on dense graphs with different numbers of nodes, on fully observable (no hidden
variables) and latent variable models. We report the SHD accuracy (the lower, the better). We note that
DirectLiNGAM is surprisingly robust to different structural assumptions, and AdaScore is generally comparable
or better (as in nonlinear observable data) than the other benchmarks."
EXPERIMENTS,0.26583159359777314,"Table 1: Experiments causal discovery algorithms. The content of the cells denotes whether the method supports
(✓) or not (✗) the condition specified in the corresponding row."
EXPERIMENTS,0.2665274878218511,"CAM-UV
RCD
NoGAM
DirectLiNGAM
AdaScore"
EXPERIMENTS,0.267223382045929,"Linear additive noise model
✗
✓
✗
✓
✓
Nonlinear additive noise model
✗
✗
✓
✗
✓
Nonlinear CAM
✓
✗
✓
✗
✓
Latent variables effects
✓
✓
✗
✗
✓"
EXPERIMENTS,0.26791927627000695,"Output
Mixed
Mixed
DAG
DAG
Mixed"
CONCLUSION,0.2686151704940849,"6
Conclusion
317"
CONCLUSION,0.26931106471816285,"The existing literature on causal discovery shows a connection between score matching and structure
318"
CONCLUSION,0.2700069589422408,"learning in the context of nonlinear ANMs: in this paper, (i) we formalize and extend these results
319"
CONCLUSION,0.2707028531663187,"to linear SCMs, and (ii) we show that the score retains information on the causal structure even in the
320"
CONCLUSION,0.27139874739039666,"presence of unobserved variables. Additionally, while previous works posit the accent on finding the
321"
CONCLUSION,0.2720946416144746,"causal order through the score, we study its potential to identify the Markov equivalence class with a
322"
CONCLUSION,0.27279053583855256,"constraint-based strategy that does not explicitly require tests of conditional independence, as well as
323"
CONCLUSION,0.27348643006263046,"to identify direct causal effects. Our theoretical insights result in AdaScore: unlike existing approaches
324"
CONCLUSION,0.2741823242867084,"for the estimation of causal directions, our algorithm provides theoretical guarantees for a broad class
325"
CONCLUSION,0.27487821851078637,"of identifiable models, namely linear and nonlinear, with additive noise, in the presence of latent
326"
CONCLUSION,0.2755741127348643,"variables. Even though AdaScore does not clearly outperform the existing baselines on our synthetic
327"
CONCLUSION,0.2762700069589422,"benchmark, its adaptivity to different structural hypotheses is a step towards causal discovery that is
328"
CONCLUSION,0.27696590118302017,"less reliant on prior assumptions, which are often untestable and thus hindering reliable inference in
329"
CONCLUSION,0.2776617954070981,"real-world problems. While we do not touch on the task of causal representation learning [33], where
330"
CONCLUSION,0.2783576896311761,"causal variables are learned from data, we believe this is a promising research direction in relation
331"
CONCLUSION,0.27905358385525403,"to our work due to the specific interplay between score-matching estimation and generative models.
332"
REFERENCES,0.2797494780793319,"References
333"
REFERENCES,0.2804453723034099,"[1] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: founda-
334"
REFERENCES,0.28114126652748783,"tions and learning algorithms. The MIT Press, 2017.
335"
REFERENCES,0.2818371607515658,"[2] Judea Pearl. Causality. Cambridge university press, 2009.
336"
REFERENCES,0.2825330549756437,"[3] Peter Spirtes. Introduction to causal inference. Journal of Machine Learning Research, 11(54):
337"
REFERENCES,0.28322894919972164,"1643–1662, 2010. URL http://jmlr.org/papers/v11/spirtes10a.html.
338"
REFERENCES,0.2839248434237996,"[4] Clark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on
339"
REFERENCES,0.28462073764787754,"graphical models. Frontiers in Genetics, 10, 2019. ISSN 1664-8021. doi: 10.3389/fgene.2019.
340"
REFERENCES,0.28531663187195544,"00524. URL https://www.frontiersin.org/articles/10.3389/fgene.2019.00524.
341"
REFERENCES,0.2860125260960334,"[5] P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT press, 2nd
342"
REFERENCES,0.28670842032011135,"edition, 2000.
343"
REFERENCES,0.2874043145441893,"[6] David Maxwell Chickering. Optimal structure identification with greedy search. J. Mach. Learn.
344"
REFERENCES,0.2881002087682672,"Res., 3(null):507–554, mar 2003. ISSN 1532-4435. doi: 10.1162/153244303321897717. URL
345"
REFERENCES,0.28879610299234515,"https://doi.org/10.1162/153244303321897717.
346"
REFERENCES,0.2894919972164231,"[7] Shohei Shimizu, Patrik O. Hoyer, Aapo Hyvärinen, and Antti Kerminen. A linear non-gaussian
347"
REFERENCES,0.29018789144050106,"acyclic model for causal discovery. J. Mach. Learn. Res., 7:2003–2030, dec 2006. ISSN
348"
REFERENCES,0.290883785664579,"1532-4435.
349"
REFERENCES,0.2915796798886569,"[8] Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Schölkopf. Non-
350"
REFERENCES,0.29227557411273486,"linear causal discovery with additive noise models. In D. Koller, D. Schuurmans, Y. Bengio,
351"
REFERENCES,0.2929714683368128,"and L. Bottou, editors, Advances in Neural Information Processing Systems, volume 21. Cur-
352"
REFERENCES,0.29366736256089077,"ran Associates, Inc., 2008. URL https://proceedings.neurips.cc/paper/2008/file/
353"
REFERENCES,0.29436325678496866,"f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf.
354"
REFERENCES,0.2950591510090466,"[9] Jonas Peters, Joris M. Mooij, Dominik Janzing, and Bernhard Schölkopf. Causal discovery
355"
REFERENCES,0.29575504523312457,"with continuous additive noise models. J. Mach. Learn. Res., 15(1):2009–2053, jan 2014. ISSN
356"
REFERENCES,0.2964509394572025,"1532-4435.
357"
REFERENCES,0.2971468336812804,"[10] Kun Zhang and Aapo Hyvärinen. On the identifiability of the post-nonlinear causal model. In
358"
REFERENCES,0.2978427279053584,"Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI ’09,
359"
REFERENCES,0.2985386221294363,"page 647–655, Arlington, Virginia, USA, 2009. AUAI Press. ISBN 9780974903958.
360"
REFERENCES,0.2992345163535143,"[11] Peter Spirtes. An anytime algorithm for causal inference. In Thomas S. Richardson and Tommi S.
361"
REFERENCES,0.29993041057759223,"Jaakkola, editors, Proceedings of the Eighth International Workshop on Artificial Intelligence
362"
REFERENCES,0.30062630480167013,"and Statistics, volume R3 of Proceedings of Machine Learning Research, pages 278–285.
363"
REFERENCES,0.3013221990257481,"PMLR, 04–07 Jan 2001. URL https://proceedings.mlr.press/r3/spirtes01a.html.
364"
REFERENCES,0.30201809324982604,"Reissued by PMLR on 31 March 2021.
365"
REFERENCES,0.302713987473904,"[12] Takashi Nicholas Maeda and Shohei Shimizu. Rcd: Repetitive causal discovery of linear
366"
REFERENCES,0.3034098816979819,"non-gaussian acyclic models with latent confounders. In Silvia Chiappa and Roberto Calandra,
367"
REFERENCES,0.30410577592205984,"editors, Proceedings of the Twenty Third International Conference on Artificial Intelligence and
368"
REFERENCES,0.3048016701461378,"Statistics, volume 108 of Proceedings of Machine Learning Research, pages 735–745. PMLR,
369"
REFERENCES,0.30549756437021575,"26–28 Aug 2020. URL https://proceedings.mlr.press/v108/maeda20a.html.
370"
REFERENCES,0.30619345859429364,"[13] Takashi Nicholas Maeda and Shohei Shimizu. Causal additive models with unobserved variables.
371"
REFERENCES,0.3068893528183716,"In Uncertainty in Artificial Intelligence, pages 97–106. PMLR, 2021.
372"
REFERENCES,0.30758524704244955,"[14] Asish Ghoshal and Jean Honorio. Learning linear structural equation models in polynomial
373"
REFERENCES,0.3082811412665275,"time and sample complexity. In Amos Storkey and Fernando Perez-Cruz, editors, Proceedings
374"
REFERENCES,0.3089770354906054,"of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84
375"
REFERENCES,0.30967292971468335,"of Proceedings of Machine Learning Research, pages 1466–1475. PMLR, 09–11 Apr 2018.
376"
REFERENCES,0.3103688239387613,"URL https://proceedings.mlr.press/v84/ghoshal18a.html.
377"
REFERENCES,0.31106471816283926,"[15] Paul Rolland, Volkan Cevher, Matthäus Kleindessner, Chris Russell, Dominik Janzing, Bernhard
378"
REFERENCES,0.3117606123869172,"Schölkopf, and Francesco Locatello. Score matching enables causal discovery of nonlinear
379"
REFERENCES,0.3124565066109951,"additive noise models. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari,
380"
REFERENCES,0.31315240083507306,"Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on
381"
REFERENCES,0.313848295059151,"Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 18741–
382"
REFERENCES,0.31454418928322897,"18753. PMLR, 17–23 Jul 2022.
383"
REFERENCES,0.31524008350730687,"[16] Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Kun Zhang, and Francesco Locatello.
384"
REFERENCES,0.3159359777313848,"Scalable causal discovery with score matching. In 2nd Conference on Causal Learning and
385"
REFERENCES,0.3166318719554628,"Reasoning, 2023. URL https://openreview.net/forum?id=6VvoDjLBPQV.
386"
REFERENCES,0.3173277661795407,"[17] Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Kun Zhang, and Francesco Locatello.
387"
REFERENCES,0.3180236604036186,"Causal discovery with score matching on additive models with arbitrary noise. In 2nd Conference
388"
REFERENCES,0.3187195546276966,"on Causal Learning and Reasoning, 2023. URL https://openreview.net/forum?id=
389"
REFERENCES,0.31941544885177453,"rVO0Bx90deu.
390"
REFERENCES,0.3201113430758525,"[18] Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, and Francesco Locatello. Shortcuts
391"
REFERENCES,0.32080723729993044,"for causal discovery of nonlinear models by score matching, 2023.
392"
REFERENCES,0.32150313152400833,"[19] Pedro Sanchez, Xiao Liu, Alison Q O’Neil, and Sotirios A. Tsaftaris. Diffusion models for
393"
REFERENCES,0.3221990257480863,"causal discovery via topological ordering. In The Eleventh International Conference on Learning
394"
REFERENCES,0.32289491997216424,"Representations, 2023. URL https://openreview.net/forum?id=Idusfje4-Wq.
395"
REFERENCES,0.3235908141962422,"[20] Zhenyu Zhu, Francesco Locatello, and Volkan Cevher. Sample complexity bounds for score-
396"
REFERENCES,0.3242867084203201,"matching: Causal discovery and generative modeling. Advances in Neural Information Process-
397"
REFERENCES,0.32498260264439804,"ing Systems, 36, 2024.
398"
REFERENCES,0.325678496868476,"[21] Alessio Spantini, Daniele Bigoni, and Youssef Marzouk.
Inference via low-dimensional
399"
REFERENCES,0.32637439109255395,"couplings, 2018.
400"
REFERENCES,0.32707028531663185,"[22] Juan Lin.
Factorizing multivariate function classes.
In M. Jordan, M. Kearns, and
401"
REFERENCES,0.3277661795407098,"S. Solla, editors, Advances in Neural Information Processing Systems, volume 10. MIT
402"
REFERENCES,0.32846207376478775,"Press, 1997.
URL https://proceedings.neurips.cc/paper_files/paper/1997/
403"
REFERENCES,0.3291579679888657,"file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf.
404"
REFERENCES,0.3298538622129436,"[23] Aapo Hyvärinen. Estimation of non-normalized statistical models by score matching. J. Mach.
405"
REFERENCES,0.33054975643702156,"Learn. Res., 6:695–709, 2005.
URL https://api.semanticscholar.org/CorpusID:
406"
REFERENCES,0.3312456506610995,"1152227.
407"
REFERENCES,0.33194154488517746,"[24] Caroline Uhler, G. Raskutti, Peter Bühlmann, and B. Yu. Geometry of the faithfulness assump-
408"
REFERENCES,0.3326374391092554,"tion in causal inference. The Annals of Statistics, 41, 07 2012. doi: 10.1214/12-AOS1080.
409"
REFERENCES,0.3333333333333333,"[25] Jiji Zhang. Causal reasoning with ancestral graphs. Journal of Machine Learning Research, 9
410"
REFERENCES,0.33402922755741127,"(7), 2008.
411"
REFERENCES,0.3347251217814892,"[26] Francesco Montagna, Atalanti Mastakouri, Elias Eulig, Nicoletta Noceti, Lorenzo Rosasco,
412"
REFERENCES,0.3354210160055672,"Dominik Janzing, Bryon Aragam, and Francesco Locatello.
Assumption violations
413"
REFERENCES,0.33611691022964507,"in causal discovery and the robustness of score matching.
In A. Oh, T. Neumann,
414"
REFERENCES,0.336812804453723,"A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural
415"
REFERENCES,0.337508698677801,"Information Processing Systems, volume 36, pages 47339–47378. Curran Associates,
416"
REFERENCES,0.33820459290187893,"Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/
417"
REFERENCES,0.33890048712595683,"93ed74938a54a73b5e4c52bbaf42ca8e-Paper-Conference.pdf.
418"
REFERENCES,0.3395963813500348,"[27] Phillip Lippe, Taco Cohen, and Efstratios Gavves. Efficient neural causal discovery without
419"
REFERENCES,0.34029227557411273,"acyclicity constraints. In International Conference on Learning Representations, 2022. URL
420"
REFERENCES,0.3409881697981907,"https://openreview.net/forum?id=eYciPrLuUhG.
421"
REFERENCES,0.34168406402226864,"[28] Nan Rosemary Ke, Silvia Chiappa, Jane X Wang, Jorg Bornschein, Anirudh Goyal, Melanie Rey,
422"
REFERENCES,0.34237995824634654,"Theophane Weber, Matthew Botvinick, Michael Curtis Mozer, and Danilo Jimenez Rezende.
423"
REFERENCES,0.3430758524704245,"Learning to induce causal structure. In International Conference on Learning Representations,
424"
REFERENCES,0.34377174669450244,"2023. URL https://openreview.net/forum?id=hp_RwhKDJ5.
425"
REFERENCES,0.3444676409185804,"[29] Philippe Brouillard, Sébastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and
426"
REFERENCES,0.3451635351426583,"Alexandre Drouin. Differentiable causal discovery from interventional data. In Proceedings of
427"
REFERENCES,0.34585942936673625,"the 34th International Conference on Neural Information Processing Systems, NIPS ’20, Red
428"
REFERENCES,0.3465553235908142,"Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.
429"
REFERENCES,0.34725121781489215,"[30] Ioannis Tsamardinos, Laura E Brown, and Constantin F Aliferis. The max-min hill-climbing
430"
REFERENCES,0.34794711203897005,"bayesian network structure learning algorithm. Machine learning, 65:31–78, 2006.
431"
REFERENCES,0.348643006263048,"[31] Sofia Triantafillou and Ioannis Tsamardinos. Score-based vs constraint-based causal learning in
432"
REFERENCES,0.34933890048712596,"the presence of confounders. In Cfa@ uai, pages 59–67, 2016.
433"
REFERENCES,0.3500347947112039,"[32] Alexander G. Reisach, Christof Seiler, and Sebastian Weichwald. Beware of the simulated dag!
434"
REFERENCES,0.35073068893528186,"causal discovery benchmarks may be easy to game. In Neural Information Processing Systems,
435"
REFERENCES,0.35142658315935976,"2021. URL https://api.semanticscholar.org/CorpusID:239998404.
436"
REFERENCES,0.3521224773834377,"[33] Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Ke, Nal Kalchbrenner, Anirudh
437"
REFERENCES,0.35281837160751567,"Goyal, and Y. Bengio. Toward causal representation learning. Proceedings of the IEEE, PP:
438"
REFERENCES,0.3535142658315936,"1–23, 02 2021. doi: 10.1109/JPROC.2021.3058954.
439"
REFERENCES,0.3542101600556715,"[34] Peter Spirtes and Thomas Richardson. A polynomial time algorithm for determining dag
440"
REFERENCES,0.35490605427974947,"equivalence in the presence of latent variables and selection bias. In Proceedings of the 6th
441"
REFERENCES,0.3556019485038274,"International Workshop on Artificial Intelligence and Statistics, pages 489–500. Citeseer, 1996.
442"
REFERENCES,0.3562978427279054,"[35] Yingzhen Li and Richard E Turner. Gradient estimators for implicit models. arXiv preprint
443"
REFERENCES,0.3569937369519833,"arXiv:1705.07107, 2017.
444"
REFERENCES,0.35768963117606123,"[36] Peter Bühlmann, Jonas Peters, and Jan Ernest. CAM: Causal additive models, high-dimensional
445"
REFERENCES,0.3583855254001392,"order search and penalized regression.
The Annals of Statistics, 42(6), dec 2014.
URL
446"
REFERENCES,0.35908141962421714,"https://doi.org/10.1214%2F14-aos1260.
447"
REFERENCES,0.35977731384829503,"A
Useful results
448"
REFERENCES,0.360473208072373,"In this section, we provide a collection of results and definitions relevant to the theory of this paper.
449"
REFERENCES,0.36116910229645094,"A.1
Definitions over graphs
450"
REFERENCES,0.3618649965205289,"Let X = X1, . . . , Xd a set of random variables. A graph G = (X, E) consists of finitely many nodes
451"
REFERENCES,0.36256089074460685,"or vertices X and edges E. We now provide additional definitions, separately for directed acyclic
452"
REFERENCES,0.36325678496868474,"and mixed graphs.
453"
REFERENCES,0.3639526791927627,"Directed acyclic graph.
In a directed graph, nodes can be connected by a directed edge (→), and
454"
REFERENCES,0.36464857341684065,"between each pair of nodes there is at most one directed edge. We say that X1 is a parent of Xj if
455"
REFERENCES,0.3653444676409186,"Xi →Xj ∈E, in which case we also say that Xj is a child of Xi. Two nodes are adjacent if they
456"
REFERENCES,0.3660403618649965,"are connected by an edge. Three nodes are called a v-structure if one node is a child of the other
457"
REFERENCES,0.36673625608907445,"two, e.g. as Xi →Xk ←Xj is a collider. A path in G is a sequence of at least two distinct vertices
458"
REFERENCES,0.3674321503131524,"Xi1, . . . , Xim such that there is an edge between Xik and Xik+1. If Xik →Xik+1 for every node
459"
REFERENCES,0.36812804453723036,"in the path, we speak of a directed path, and call Xik an ancestor of Xik+1, Xik+1 a descendant of
460"
REFERENCES,0.36882393876130826,"Xik. Given the set DEG
i of descendants of a node Xi, we define the set of non-descendants of Xi as
461"
REFERENCES,0.3695198329853862,"NDG
i = X \ (DEG
i ∪{Xi}). A node without parents is called a source node. A node without children
462"
REFERENCES,0.37021572720946416,"is called a sink node. A directed acyclic graph is a directed graph with no cycles.
463"
REFERENCES,0.3709116214335421,"Mixed graph.
In a mixed graph nodes can be connected by a directed edge (→) or a bidirected
464"
REFERENCES,0.37160751565762007,"edge (↔), and between each pair of nodes there is at most one directed edge. Two vertices are said
465"
REFERENCES,0.37230340988169797,"to be adjacent in a graph if there is an edge (of any kind) between them. The definitions of parent,
466"
REFERENCES,0.3729993041057759,"child, ancestor, descendant, path provided for directed acyclic graph also apply in the case of mixed
467"
REFERENCES,0.3736951983298539,"graphs. Additionally, Xi is a spouse of Xj (and vice-versa) if Xi ↔Xj ∈E. An almost directed
468"
REFERENCES,0.3743910925539318,"cycle occurs when Xi ↔Xj ∈E and Xi is an ancestor of Xj in G.
469"
REFERENCES,0.3750869867780097,"For ease of reference from the main text, we separately provide the definition of inducing paths and
470"
REFERENCES,0.3757828810020877,"ancestors in directed acyclic graphs.
471"
REFERENCES,0.37647877522616563,"Definition 2 (Ancestor). Consider a DAG G with set of nodes X, and Xi, Xj elements of X. We
472"
REFERENCES,0.3771746694502436,"say that Xi is an ancestor of Xj if there is a directed path from Xi to Xj in the graph, as in
473"
REFERENCES,0.3778705636743215,"Xi →. . . →Xj.
474"
REFERENCES,0.37856645789839943,"Definition 3 (Inducing path). Consider a DAG G with set of nodes X, and Y, Z disjoint subsets such
475"
REFERENCES,0.3792623521224774,"that X = Y ˙∪Z. We say that there is an inducing path relative to Z between the nodes Yi, Yj if every
476"
REFERENCES,0.37995824634655534,"node on the path that is not in Z ∪{Yi, Yj} is a collider on the path (i.e. for each Yk ∈Y on the path
477"
REFERENCES,0.38065414057063324,"the sequence Yi . . . →Yk ←. . . Yj appears) and every collider on the path is an ancestor of Yi or Yj.
478"
REFERENCES,0.3813500347947112,"One natural way to encode inducing paths and ancestral relationships between variables is represented
479"
REFERENCES,0.38204592901878914,"by maximal ancestral graphs.
480"
REFERENCES,0.3827418232428671,"Definition 4 (MAG). A maximal ancestral graph (MAG) is a mixed graph such that:
481"
REFERENCES,0.38343771746694505,"1. there are no directed cycles and no almost directed cycles;
482"
REFERENCES,0.38413361169102295,"2. there are no inducing paths between two non-adjacent nodes.
483"
REFERENCES,0.3848295059151009,"Next, we define conditional independence in the context of graphs.
484"
REFERENCES,0.38552540013917885,"Definition 5 (m-separation). Let M be a mixed graph with nodes X. A path π in M between Xi, Xj
485"
REFERENCES,0.3862212943632568,"elements of X is active w.r.t. Z ⊆X \ {Xi, Xj} if:
486"
REFERENCES,0.3869171885873347,"1. every non-collider on π is not in Z
487"
REFERENCES,0.38761308281141266,"2. every collider on π is an ancestors of a node in Z.
488"
REFERENCES,0.3883089770354906,"Xi and Xj are said to be m-separated by Z if there is no active path between Xi and Xj relative to Z.
489"
REFERENCES,0.38900487125956856,"Two disjoint sets of variables W and Y are m-separated by Z if every variable in W is m-separated
490"
REFERENCES,0.38970076548364646,"from every variable in Y by Z.
491"
REFERENCES,0.3903966597077244,"If m-separation is applied to DAGs, it is called d-separation.
492"
REFERENCES,0.39109255393180237,"The set of directed acyclic graphs that satisfy the same set of conditional independencies form an
493"
REFERENCES,0.3917884481558803,"equivalence class, known as the Markov equivalence class.
494"
REFERENCES,0.3924843423799583,"Definition 6 (Markov equivalence class of a DAG). Let G be a DAG with nodes X. We denote with
495"
REFERENCES,0.39318023660403617,"[G] the Markov equivalence class of G. A DAG ˜G with nodes X is in [G] if the following conditions
496"
REFERENCES,0.3938761308281141,"are satisfied for each pair Xi, Xj of distinct nodes in X:
497"
REFERENCES,0.3945720250521921,"• there is an edge between Xi, Xj in G if and only if there is an edge between Xi, Xj in ˜G;
498"
REFERENCES,0.39526791927627003,"• let Z ⊆X \ {Xi, Xj}. Then Xi |="
REFERENCES,0.3959638135003479,"d
GXj|Z ⇐⇒Xi |="
REFERENCES,0.3966597077244259,"d
˜GXj|Z;
499"
REFERENCES,0.39735560194850383,"• let π be a path between Xi and Xj. Xk is a collider in the path π in G if and only if it is a
500"
REFERENCES,0.3980514961725818,"collider in the path π in ˜G.
501"
REFERENCES,0.3987473903966597,"In summary, graphs in the same equivalence class share the edges up to direction, the set of d-
502"
REFERENCES,0.39944328462073764,"separations, and the set of colliders.
503"
REFERENCES,0.4001391788448156,"Just as for DAGs, there may be several MAGs that imply the same conditional independence
504"
REFERENCES,0.40083507306889354,"statements. Denote the Markov-equivalence class of a MAG M with [M]: this is represented by a
505"
REFERENCES,0.40153096729297144,"partial mixed graph, the class of graphs that can contain four kinds of edges: →, ↔, ◦−−◦and ◦→,
506"
REFERENCES,0.4022268615170494,"and hence three kinds of end marks for edges: arrowhead (>), tail (−) and circle (◦).
507"
REFERENCES,0.40292275574112735,"Definition 7 (PAG, Definition 3 of Zhang [25]). Let [M] be the Markov equivalence class of an
508"
REFERENCES,0.4036186499652053,"arbitrary MAG M . The partial ancestral graph (PAG) for [M], PM , is a partial mixed graph such
509"
REFERENCES,0.40431454418928325,"that:
510"
REFERENCES,0.40501043841336115,"• PM has the same adjacencies as M (and any member of [M]) does;
511"
REFERENCES,0.4057063326374391,"• A mark of arrowhead is in PM if and only if it is shared by all MAGs in [M]; and
512"
REFERENCES,0.40640222686151706,"• A mark of tail is in PM if and only if it is shared by all MAGs in [M].
513"
REFERENCES,0.407098121085595,"Intuitively, a PAG represents an equivalence class of MAGs by displaying all common edge marks
514"
REFERENCES,0.4077940153096729,"shared by all members of the class and displaying circles for those marks that are not in common.
515"
REFERENCES,0.40848990953375086,"A.2
Equivalence between m-separation and d-separation
516"
REFERENCES,0.4091858037578288,"In this section, we provide a proof for equation (5), stating the equivalence between m-separation and
517"
REFERENCES,0.40988169798190677,"d-separation in a formal sense.
518"
REFERENCES,0.41057759220598466,"Lemma 1 (Adapted from Zhang [25]). Let G be a DAG with nodes X = V ∪U, with V and U
disjoint sets, and MG
V the marginalization of G onto V . For any {Vi, Vj} ∈V and VZ ⊆V \{Vi, Vj},
the following equivalence holds: Vi |="
REFERENCES,0.4112734864300626,"d
GVj|VZ ⇐⇒Vi |="
REFERENCES,0.41196938065414057,"m
MG
V Vj|VZ."
REFERENCES,0.4126652748782185,Proof. The implication Vi |=
REFERENCES,0.4133611691022965,"d
GVj|VZ =⇒Vi |="
REFERENCES,0.4140570633263744,"m
MG
V Vj|VZ is a direct consequence of Lemma 18
519"
REFERENCES,0.41475295755045233,"from Spirtes and Richardson [34], where we set S = ∅, since we do not consider selection bias. The
520"
REFERENCES,0.4154488517745303,implication Vi |=
REFERENCES,0.41614474599860823,"d
GVj|VZ ⇐= Vi |="
REFERENCES,0.41684064022268613,"m
MG
V Vj|VZ follows from Lemma 17 by Spirtes and Richardson
521"
REFERENCES,0.4175365344467641,"[34], again with S = ∅. Note, that in their terminology “d-separation in MAGs” is what we call
522"
REFERENCES,0.41823242867084204,"m-separation.
523"
REFERENCES,0.41892832289492,"A.3
Additive noise model identifiability
524"
REFERENCES,0.4196242171189979,"We study the identifiability of the additive noise model, reporting results from Peters et al. [9]. We
525"
REFERENCES,0.42032011134307584,"start with a formal definition of identifiability in the context of causal discovery.
526"
REFERENCES,0.4210160055671538,"Definition 8 (Identifiable causal model). Let (X, N, F, pN) be an SCM with underlying graph G and
527"
REFERENCES,0.42171189979123175,"pX joint density function of the variables of X. We say that the model is identifiable from observa-
528"
REFERENCES,0.42240779401530965,"tional data if the distribution pX can not be generated by a structural causal model with graph ˜G ̸= G.
529"
REFERENCES,0.4231036882393876,"First, we consider the case of models of two random variables
530"
REFERENCES,0.42379958246346555,"X2 := f(X1) + N,
X1 |="
REFERENCES,0.4244954766875435,"N.
(17)"
REFERENCES,0.42519137091162146,"Condition 1 (Condition 19 of Peters et al. [9]). Consider an additive noise model with structural
531"
REFERENCES,0.42588726513569936,"equations (17). The triple (f, pX1, pN) does not solve the following differential equation for all pairs
532"
REFERENCES,0.4265831593597773,"x1, x2 with f ′(x2)ν′′(x2 −f(x1)) ̸= 0:
533"
REFERENCES,0.42727905358385526,"ξ′′′ = ξ′′
f ′′"
REFERENCES,0.4279749478079332,f ′ −ν′′′f ′ ν′′
REFERENCES,0.4286708420320111,"
+ ν′′′ν′f ′′f ′"
REFERENCES,0.42936673625608907,"ν′′
−ν′(f ′′)2"
REFERENCES,0.430062630480167,"f ′
−2ν′′f ′′f ′ + ν′f ′′′,
(18)"
REFERENCES,0.43075852470424497,"Here, ξ := log pX1, ν := log pN, the logarithms of the strictly positive densities. The arguments
534"
REFERENCES,0.43145441892832287,"x2 −f(x1), x1, and x1 of ν, ξ and f respectively, have been removed to improve readability.
535"
REFERENCES,0.4321503131524008,"Next, we show that a structural causal model satisfying Condition 1 is identifiable, as in Definition 8
536"
REFERENCES,0.4328462073764788,"Theorem 1 (Theorem 20 of Peters et al. [9]). Let pX1,X2 the joint distribution of a pair of random
537"
REFERENCES,0.43354210160055673,"variables generated according to the model of equation (17) that satisfies Condition 1, with graph G.
538"
REFERENCES,0.4342379958246347,"Then, G is identifiable from the joint distribution.
539"
REFERENCES,0.4349338900487126,"Finally, we show an important fact, holding for identifiable bivariate models, which is that the score
540"
REFERENCES,0.43562978427279053,"∂
∂X1 log p(x1, x2) is nonlinear in x1.
541"
REFERENCES,0.4363256784968685,"Lemma 2 (Sufficient variability of the score). Let pX1,X2 the joint distribution of a pair of random
variables generated according to a structural causal model that satisfies Condition 1, with graph G.
Then:
∂
∂X1
(ξ′(x1) −f ′(x1)ν′(x2 −f(x1))) ̸= 0,"
REFERENCES,0.43702157272094644,"for all pairs (x1, x2).
542"
REFERENCES,0.43771746694502434,"Proof. By contradiction, assume that there exists (x1, x2) such that
∂
∂X1 (ξ′(x1) −f ′(x1)ν′(x2 −
f(x1))) = 0. Then: ∂
∂X1   ∂2"
REFERENCES,0.4384133611691023,"∂X2
1 π(x1, x2)"
REFERENCES,0.43910925539318024,"∂2
∂X1∂X2 π(x1, x2)  = 0,"
REFERENCES,0.4398051496172582,"where π(x1, x2) = log p(x1, x2). By explicitly computing all the partial derivatives of the above
543"
REFERENCES,0.4405010438413361,"equation, we obtain that equation 18 is satisfied, which violates Condition 1.
544"
REFERENCES,0.44119693806541405,"These results guaranteeing the identifiability of the bivariate additive noise model can be generalized
545"
REFERENCES,0.441892832289492,"to the multivariable case, with a set of random variables X = {X1, . . . , Xk} that satisfy:
546"
REFERENCES,0.44258872651356995,"Xi := fi(XPAG
i ) + Ni, i = 1, . . . , k,
(19)"
REFERENCES,0.44328462073764785,"where G is the resulting causal graph directed and acyclic. The intuition is that, rather than studying
547"
REFERENCES,0.4439805149617258,"the multivariate model as a whole, we need to ensure that Condition 1 is satisfied for each pair of
548"
REFERENCES,0.44467640918580376,"nodes, adding restrictions on their marginal conditional distribution.
549"
REFERENCES,0.4453723034098817,"Definition 9 (Definition 27 of Peters et al. [9]). Consider an additive noise model with structural
equations (19). We call this SCM a restricted additive noise model if for all Xj ∈X, Xi ∈XPAG
j ,"
REFERENCES,0.44606819763395966,"and all sets XS ⊆X, S ⊂N, with XPAG
j \ {Xi} ⊆XS ⊆XG
NDj \ {Xi, Xj}, there is a value xS
with p(xS) > 0, such that the triplet"
REFERENCES,0.44676409185803756,"(fj(xPAG
j \{i}, ·), pXi|XS=xS, pNj)"
REFERENCES,0.4474599860821155,"satisfies Condition 1. Here, fj(xPAG
j \{i}, ·) denotes the mechanism function xi 7→fj(xPAG
j ).
550"
REFERENCES,0.44815588030619347,"Additionally, we require the noise variables to have positive densities and the functions fj to be
551"
REFERENCES,0.4488517745302714,"continuous and three times continuously differentiable.
552"
REFERENCES,0.4495476687543493,"Then, for a restricted additive noise model, we can identify the graph from the distribution.
553"
REFERENCES,0.45024356297842727,"Theorem 2 (Theorem 28 of Peters et al. [9]). Let X be generated by a restricted additive noise
554"
REFERENCES,0.4509394572025052,"model with graph G, and assume that the causal mechanisms fj are not constant in any of the input
555"
REFERENCES,0.4516353514265832,"arguments, i.e. for Xi ∈XPAG
j , there exist xi ̸= x′
i such that fj(xPAG
j \{i}, xi) ̸= fj(xPAG
j \{i}, x′
i).
556"
REFERENCES,0.4523312456506611,"Then, G is identifiable.
557"
REFERENCES,0.453027139874739,"A.4
Other auxiliary results
558"
REFERENCES,0.453723034098817,"We state several results that hold for a pair of random variables that are not connected by an active path
559"
REFERENCES,0.45441892832289493,"that includes unobserved variables (active paths are introduced in Definition 5). For the remainder of
560"
REFERENCES,0.4551148225469729,"the section, let V, U be a pair of disjoint sets of random variables, X = V ∪U generated according
561"
REFERENCES,0.4558107167710508,"to the structural causal model defined by the set of equations (1), G the associated causal graph, and
562"
REFERENCES,0.45650661099512874,"MG
V the marginalization onto V .
563"
REFERENCES,0.4572025052192067,"The first statement provides under which condition the unobserved parents of two variables in the
564"
REFERENCES,0.45789839944328464,"marginal MAG are mutually independent random vectors.
565"
REFERENCES,0.45859429366736254,"Lemma 3. Let Vj ∈V , and Z ⊂N such that VZ = VPAG
j ∪{Vj}. Assume VPAG
j |="
REFERENCES,0.4592901878914405,"d
GU j. Then
566 U j |="
REFERENCES,0.45998608211551845,"d
GU Zk for each index Zk ̸= j.
567"
REFERENCES,0.4606819763395964,"Proof. The assumption VPAG
j |="
REFERENCES,0.4613778705636743,"d
GU j implies that there is no active path in G between nodes in VPAG
j
568"
REFERENCES,0.46207376478775225,"and nodes in U j. Given that for each Zk ∈Z, Zk ̸= Z, nodes in U Zk are direct causes of at least
569"
REFERENCES,0.4627696590118302,"one node in VPAG
j , any active path between nodes in U Zk and nodes in U j would also be an active
570"
REFERENCES,0.46346555323590816,"path between VPAG
j and U j, which is a contradiction. Hence U j |="
REFERENCES,0.4641614474599861,"d
GU Zk.
571"
REFERENCES,0.464857341684064,"The previous lemmas allow proving the following result, which will be fundamental to demonstrate
572"
REFERENCES,0.46555323590814196,"the theory of Proposition 3.
573"
REFERENCES,0.4662491301322199,"Lemma 4. Let Vj ∈V , and Z ⊂N such that VZ = VPAG
j ∪{Vj}. Assume VPAG
j |="
REFERENCES,0.46694502435629787,"d
GU j. W.l.o.g.,"
REFERENCES,0.46764091858037576,"let the j-th element of VZ be VZj = Vj. Denote as U Z the set of unobserved parents of nodes in VZ,
and U Z\{j} the unobserved parents of nodes in VZ\{j} := VZ \ Vj. Then, the following holds for
each vZ, uZ values:
log p(vZ) = log p(vj|vPAG
j ) + log Q(vZ), where"
REFERENCES,0.4683368128044537,"Q(vZ) =
X"
REFERENCES,0.46903270702853167,"uZ\{j}
p(uZ\{j}) |Z|
Y"
REFERENCES,0.4697286012526096,"k̸=j
p(vZk|vZ1, . . . , vZk−1, uZk)."
REFERENCES,0.4704244954766875,"Proof. By the law of total probability and the chain rule, we can write p(vZ) as:
574"
REFERENCES,0.4711203897007655,"p(vZ) =
X"
REFERENCES,0.4718162839248434,"u
p(vZ|u)p(u) =
X"
REFERENCES,0.4725121781489214,"u
p(u)p(vZj|u, vZ\{j})p(vZ\{j}|u).
(20)"
REFERENCES,0.4732080723729993,"By Lemma 3, U Zj |="
REFERENCES,0.47390396659707723,"U Zk, k ̸= j, where U Zk denotes unobserved parents of the node VZk. Then,
575"
REFERENCES,0.4745998608211552,"we can factorize p(u) = p
 
uZj
p
 
uZ\{j}
. Plugging the factorization in equation (20) we find
576"
REFERENCES,0.47529575504523314,"p(vZ) =
X"
REFERENCES,0.4759916492693111,"u
p
 
uZj
p

uZ\{j}
p(vZj|u, vZ\{j})p(vZ\{j}|u) =
X"
REFERENCES,0.476687543493389,"u
p
 
uZj
p

uZ\{j}
p(vZj|uZj, vPAG
Zj )p(vZ\{j}|u),"
REFERENCES,0.47738343771746694,"where the latter equation comes from the global Markov property on the graph G. Further, by assump-
577"
REFERENCES,0.4780793319415449,"tion of VPAG
j |="
REFERENCES,0.47877522616562285,"d
GU j, we know that U Zj |="
REFERENCES,0.47947112038970074,"VZk, k ̸= j, such that p(vZ\{j}|u) = p(vZ\{j}|uZ\{j}).
578"
REFERENCES,0.4801670146137787,"Then:
579"
REFERENCES,0.48086290883785665,"p(vZ) =
X"
REFERENCES,0.4815588030619346,"u
p
 
uZj
p

uZ\{j}
p(vZj|uZj, vPAG
Zj )p(vZ\{j}|uZ\{j}) =
X"
REFERENCES,0.4822546972860125,"uZj
p
 
uZj
p(vZj|uZj, vPAG
Zj )
X"
REFERENCES,0.48295059151009045,"uZ\{j}
p

uZ\{j}
p(vZ\{j}|uZ\{j})"
REFERENCES,0.4836464857341684,"= p(vZj|vPAG
Zj )
X"
REFERENCES,0.48434237995824636,"uZ\{j}
p

uZ\{j}
p(vZ\{j}|uZ\{j}),"
REFERENCES,0.4850382741823243,"which proves the claim.
580"
REFERENCES,0.4857341684064022,"Intuitively, Lemma 4 shows that given a node Vj without children and bidirected edges in a marginal-
581"
REFERENCES,0.48643006263048016,"ized graph MG
VZ, the kernel of node Vj in the Markov factorization of p(vZ) is equal to the kernel of
582"
REFERENCES,0.4871259568545581,"the same node in the Markov factorization of p(x) of equation (2), relative to the graph without latent
583"
REFERENCES,0.48782185107863607,"confounders G.
584"
REFERENCES,0.48851774530271397,"B
Proofs of theoretical results
585"
REFERENCES,0.4892136395267919,"B.1
Proof of Proposition 1
586"
REFERENCES,0.4899095337508699,Proof of Proposition 1. Observe that ∂2
REFERENCES,0.4906054279749478,"∂Vi∂Vj
log p(vZ) = 0 ⇐⇒Vi |="
REFERENCES,0.4913013221990257,"d
GVj|VZ \ {Vi, Vj} ⇐⇒Vi |="
REFERENCES,0.4919972164231037,"m
MG
V Vj|VZ \ {Vi, Vj},"
REFERENCES,0.49269311064718163,"where the first equivalence holds by a combination of the faithfulness assumption with the global
587"
REFERENCES,0.4933890048712596,"Markov property, as explicit in equation (3), and the second due to Lemma 1. Then, the claim is
588"
REFERENCES,0.4940848990953375,"proven.
589"
REFERENCES,0.49478079331941544,"B.2
Proof of Proposition 2
590"
REFERENCES,0.4954766875434934,"Proof. The forward direction is immediate from equation (9) and Rj = Nj, when Xj is a sink
(equation (11)). Thus, we focus on the backward direction. Given"
REFERENCES,0.49617258176757134,"E
h 
E

∂Xj log p(X) | Rj = rj

−∂Xj log p(X)
2i
= 0,"
REFERENCES,0.4968684759916493,"we want to show that Xj has no children, which we prove by contradiction.
591"
REFERENCES,0.4975643702157272,Let us introduce a function q : R →R such that:
REFERENCES,0.49826026443980515,"E

∂Xj log p(X) | Rj = rj

= q(rj),"
REFERENCES,0.4989561586638831,"and sj : R|X| →R,
sj(x) = ∂Xj log p(x)."
REFERENCES,0.49965205288796105,"The mean squared error equal to zero implies that sj(X) is a constant, once Rj is observed. Formally,
under the assumption of p(x) > 0 for each x ∈Rk, this implies that"
REFERENCES,0.500347947112039,"p(sj(x) ̸= q(Rj)|Rj = rj) = 0, ∀x ∈Rk."
REFERENCES,0.5010438413361169,"By contradiction, we assume that Xj is not a leaf, and want to show that sj(X) is not constant in X,
given Rj fixed. Let Xi such that Xj ∈XPAG
i . Being the structural causal model identifiable, there
is no model with distribution pX whose graph has a backward edge Xi →Xj: thus, the Markov
factorization of equation (2) is unique and implies:"
REFERENCES,0.5017397355601948,"∂Xj log p(X) = ∂Nj log p(Nj) −
X"
REFERENCES,0.5024356297842728,"k∈CHG
j"
REFERENCES,0.5031315240083507,∂Xjhk(XPAk)∂Nk log p(Nk).
REFERENCES,0.5038274182324287,"We note that, by definition of residual in equation (10), Rj = rj fixes the following distance:"
REFERENCES,0.5045233124565066,Rj = Nj −E[Nj|X\Xj].
REFERENCES,0.5052192066805845,"Hence, conditioning on Rj doesn’t restrict the support of X: given Rj = rj, for any x\Xj (value
of the vector of elements in X \ {Xj}), ∃nj with p(nj > 0) (by the hypothesis of strictly positive
densities of the noise terms) that satisfies"
REFERENCES,0.5059151009046625,rj = nj −E[Nj|x\Xj].
REFERENCES,0.5066109951287404,"Next, we condition on all the parents of Xi, except for Xj, to reduce our problem to the simpler
592"
REFERENCES,0.5073068893528184,"bivariate case. Let S ⊂N and XS ⊆X such that XPAG
i \ {Xj} ⊆XS ⊆XNDG
i \ {Xi, Xj},
593"
REFERENCES,0.5080027835768963,"and consider xS such that p(xS > 0). Let XPAG
i = xPAG
i hold under XS = xS. We define
594"
REFERENCES,0.5086986778009742,"Xj|xs := Xj|(XS = xS), and similarly X|xs := X|(XS = xS). Being the SCM a restricted
595"
REFERENCES,0.5093945720250522,"additive noise model, by Definition 9, the triplet (gi, pXj|xs
, pNi) satisfies Condition 1, where
596"
REFERENCES,0.5100904662491301,"gi(xj) = hi(xPAG
i \{Xj}, xj). Consider Xi = xi, and the pair of values (xj, x∗
j) such that xj ̸= x∗
j
597"
REFERENCES,0.510786360473208,"and
598"
REFERENCES,0.511482254697286,"ν′′
Ni(xi −gi(xj))g′
i(xj) ̸= 0,"
REFERENCES,0.5121781489213639,"ν′′
Ni(xi −gi(x∗
j))g′
i(x∗
j) ̸= 0,"
REFERENCES,0.5128740431454419,"where we resort to the usual notation νNi := log pNi. By Lemma 2, (xi, xj) and (xi, x∗
j) satisfy:
599"
REFERENCES,0.5135699373695198,"∂Xj(ξ′(xj) −ν′
Ni(xi −gi(xj))g′
i(xj)) ̸= 0,"
REFERENCES,0.5142658315935977,"∂Xj(ξ′(x∗
j) −ν′
Ni(xi −gi(x∗
j))g′
i(x∗
j)) ̸= 0,"
REFERENCES,0.5149617258176757,"where ξ := log pXj|xs
. Thus, we can fix xj and x∗
j (which are arbitrarily chosen) such that
600"
REFERENCES,0.5156576200417536,"∂Xj(ξ′(xj) −ν′
Ni(xi −gi(xj))g′
i(xj)) −∂Xj(ξ′(x∗
j) −ν′
Ni(xi −gi(x∗
j))g′
i(x∗
j)) ̸= 0.
(21)"
REFERENCES,0.5163535142658316,"Fixing X|xS ,xj = x and X|xS ,x∗
j = x∗, where the two values differ only in their j-th component, we"
REFERENCES,0.5170494084899095,find the following difference:
REFERENCES,0.5177453027139874,"sj(x) −sj(x∗) = ∂Xj(ξ′(xj) −ν′
Ni(xi −gi(xj))g′
i(xj)) −∂Xj(ξ′(x∗
j) −ν′
Ni(xi −gi(x∗
j))g′
i(x∗
j)),"
REFERENCES,0.5184411969380655,"which is different from 0 by equation (21). This contradicts the fact that the score sj is constant once
601"
REFERENCES,0.5191370911621433,"Rj is fixed, which proves our claim.
602"
REFERENCES,0.5198329853862212,"B.3
Proof of Proposition 3
603"
REFERENCES,0.5205288796102993,"In this proof, we use several ideas from the demonstration of Proposition 2. We demonstrate the
604"
REFERENCES,0.5212247738343772,"forward and the backward parts of the two statements separately.
605"
REFERENCES,0.5219206680584552,"Proof of part (i), forward direction. Given VZ = VPAG
j ∪{Vi, Vj} and rj ∈R in the image of Rj,
606"
REFERENCES,0.5226165622825331,"we want to show:
607"
REFERENCES,0.523312456506611,"VPAG
j |="
REFERENCES,0.524008350730689,"d
GU j ∧Vi ∈VPAG
j =⇒E[∂Vj log p(VZ) −E[∂Vj log p(VZ)|Rj(VZ) = rj]]2 = 0."
REFERENCES,0.5247042449547669,"By Lemma 4, the score of Vj is
608"
REFERENCES,0.5254001391788448,"∂Vj log p(VZ) = ∂Vj log p(Vj|VPAG
j ) + ∂Vj log Q(VZ)"
REFERENCES,0.5260960334029228,"= log p( ˜Nj),"
REFERENCES,0.5267919276270007,"for some Q map acting on VZ. The latter equality holds because all variables in VZ are non-
descendants of Vj, such that ∂VjQ(VZ) = 0. Further, by equation (16) we know that"
REFERENCES,0.5274878218510787,"Rj(VZ) = ˜Nj + c,"
REFERENCES,0.5281837160751566,"where c = −E[ ˜Nj] is a constant. It follows that the least square estimator of the score of Vj from
Rj(VZ) satisfies the following equation:"
REFERENCES,0.5288796102992345,"E[∂Vj log p(VZ)|Rj(VZ)] = E[∂Vj log p( ˜Nj)| ˜Nj] = ∂Vj log p( ˜Nj),"
REFERENCES,0.5295755045233125,"where the first equality holds because E[·| ˜Nj] = E[·| ˜Nj + c]. Then, we find"
REFERENCES,0.5302713987473904,"E[∂Vj log p(VZ) −E[∂Vj log p(VZ)|Rj(VZ) = rj]]2 = E[∂Vj log p( ˜Nj) −∂Vj log p( ˜Nj)]2 = 0,"
REFERENCES,0.5309672929714684,"which is exactly our claim.
609"
REFERENCES,0.5316631871955463,"Proof of part (i), backward direction. Given VZ = VPAG
j ∪{Vi, Vj}, rj ∈R in the image of Rj, and
610"
REFERENCES,0.5323590814196242,"E[∂Vj log p(VZ) −E[∂Vj log p(VZ)|Rj(VZ) = rj]]2 = 0,
(22)"
REFERENCES,0.5330549756437022,"we want to show that VPAG
j |="
REFERENCES,0.5337508698677801,"d
GU j ∧Vi ∈VPAG
j , meaning that there is a direct causal effect that
611"
REFERENCES,0.534446764091858,"is not biased by unobserved variables. We provide the proof by contradiction, in analogy to the
612"
REFERENCES,0.535142658315936,"demonstration of the backward direction of Proposition 2.
613"
REFERENCES,0.5358385525400139,"Let us introduce sj : R|VZ| →R,"
REFERENCES,0.5365344467640919,sj(vZ) = ∂Vj log p(VZ).
REFERENCES,0.5372303409881698,"The mean squared error equal to zero implies that sj(VZ) is constant in VZ, once Rj is observed.
By contradiction, we assume that VPAG
j ̸ |="
REFERENCES,0.5379262352122477,"d
GU j ∨Vi ̸∈VPAG
j , and want to show that sj(VZ) is not
constant in VZ, given Rj fixed. In this regard, we make the following observation: by definition of
residual in equation (15), Ri(VZ) = ri fixes the following distance:"
REFERENCES,0.5386221294363257,Rj(VZ) = ˜Nj −E[ ˜Nj|VZ\{j}].
REFERENCES,0.5393180236604036,"Hence, conditioning on Rj(VZ) doesn’t restrict the support of VZ: given Rj(VZ) = rj, ∃˜nj with
p(˜nj) > 0 (by assumption of strictly positive densities pNj and pX), that satisfies"
REFERENCES,0.5400139178844816,"rj = ˜nj −E[ ˜Nj|vZ\{j}],"
REFERENCES,0.5407098121085595,"for all vZ\{j}. Hence, the random variable VZ|Rj(VZ) = rj has strictly positive density on all points
614"
REFERENCES,0.5414057063326374,"vZ where pVZ(vZ) > 0. Now, consider vZ and v∗
Z, taken from the set of uncountable values such that
615"
REFERENCES,0.5421016005567154,"the score sj function is not a constant, meaning that sj(vZ) ̸= sj(v∗
Z), where VZ is sampled given
616"
REFERENCES,0.5427974947807933,"Rj(VZ) = rj. Given that different vZ and v∗
Z are selected from an uncountable subset of the support,
617"
REFERENCES,0.5434933890048712,"we conclude that the score sj|(Rj(VZ) = rj) = ∂Vj log p(VZ|Rj(VZ) = rj) is not a constant for at
618"
REFERENCES,0.5441892832289492,"least an uncountable set of points, which contradicts equation (22).
619"
REFERENCES,0.5448851774530271,"Proof of part (ii), forward direction. Given that Vi is connected to Vj in the marginal MAG and that
620"
REFERENCES,0.5455810716771051,"VPAG
j ̸ |="
REFERENCES,0.546276965901183,"d
GU j ∨Vi ̸∈VPAG
j , we want to show that for each VZ ⊆V with {Vi, Vj} ⊆VZ, the
621"
REFERENCES,0.5469728601252609,"following holds:
622"
REFERENCES,0.5476687543493389,"E[∂Vj log p(VZ) −E[∂Vj log p(VZ)|Rj(VZ) = rj]]2 ̸= 0.
(23)"
REFERENCES,0.5483646485734168,Let us introduce h : R →R such that:
REFERENCES,0.5490605427974948,"E[∂Vj log p(VZ)|Rj(VZ) = rj] = h(rj),"
REFERENCES,0.5497564370215727,"and further define:
sj(VZ) = ∂Vj log p(VZ)."
REFERENCES,0.5504523312456506,"Having the mean squared error in equation (23) equals zero implies that sj(VZ) is a constant, once
Rj(VZ) is observed. Thus, the goal of the proof is to show that there are values of VZ such that the
score is not a constant once Rj is fixed. By definition of residual in equation (15), Rj(VZ) = rj fixes
the following distance:
Rj(VZ) = ˜Nj −E[ ˜Nj|VZ\{j}]."
REFERENCES,0.5511482254697286,"Hence, conditioning on Rj(VZ) doesn’t restrict the support of VZ: given Rj(VZ) = rj, ∃˜nj with
p(˜nj) > 0 (by assumption of positive density of the noise Nj on the support R), that satisfies"
REFERENCES,0.5518441196938065,"rj = ˜nj −E[ ˜Nj|vZ\{j}],"
REFERENCES,0.5525400139178844,"for all vZ\{j}. Hence, the random variable VZ|Rj(VZ) = rj has strictly positive density on all points
623"
REFERENCES,0.5532359081419624,"vZ where pVZ(vZ) > 0. Now, consider vZ and v∗
Z, taken from the set of uncountable values such that
624"
REFERENCES,0.5539318023660403,"the score sj function is not a constant, meaning that sj(vZ) ̸= sj(v∗
Z), where VZ is sampled given
625"
REFERENCES,0.5546276965901183,"Rj(VZ) = rj. Given that different vZ and v∗
Z are selected from an uncountable subset of the support,
626"
REFERENCES,0.5553235908141962,"we conclude that the score sj|(Rj(VZ) = rj) = ∂Vj log p(VZ|Rj(VZ) = rj) is not a constant for at
627"
REFERENCES,0.5560194850382741,"least an uncountable set of points, such that the claim follows.
628"
REFERENCES,0.5567153792623522,"Proof of part (ii), backward direction. Given that E[∂Vj log p(VZ) −E[∂Vj log p(VZ)|Rj(VZ) =
rj]]2 ̸= 0 for all VZ ⊆V such that {Vi, Vj} ∈VZ, and given Vi and Vj adjacent in the marginal
MAG, we want to show that
VPAG
j ̸ |="
REFERENCES,0.55741127348643,"d
GU j ∨Vi ̸∈VPAG
j ."
REFERENCES,0.5581071677105081,"The prove comes easily by contradiction: say that VPAG
j |="
REFERENCES,0.558803061934586,"d
GU j ∧Vi ∈VPAG
j . Then, by the forward
629"
REFERENCES,0.5594989561586639,"direction of part (i) of Proposition 3, we know that VZ = VPAG
j ∪{Vj} satisfies E[∂Vj log p(VZ) −
630"
REFERENCES,0.5601948503827419,"E[∂Vj log p(VZ)|Rj(VZ) = rj]]2 = 0, leading to a contradiction.
631"
REFERENCES,0.5608907446068198,"C
Algorithm
632"
REFERENCES,0.5615866388308977,"C.1
Detailed description of our algorithm
633"
REFERENCES,0.5622825330549757,"In Proposition 1 we have seen that score matching can detect m-separations and therefore the skeleton
634"
REFERENCES,0.5629784272790536,"of the PAG describing the data. If one is willing to make the assumptions required for Proposition 3
635"
REFERENCES,0.5636743215031316,"it could be desirable to use this to orient edges, since the interpretation of PAG edges might be
636"
REFERENCES,0.5643702157272095,"cumbersome for people not familiar with ancestral models. Therefore, one could simply find the
637"
REFERENCES,0.5650661099512874,"skeleton of the PAG using the fast adjacency search [5] and then orient the edges by applying
638"
REFERENCES,0.5657620041753654,"Proposition 3 on every subset of the neighbourhood of every node. This would yield a very costly
639"
REFERENCES,0.5664578983994433,"algorithm. But if we make the assumptions required to orient edges with Proposition 3 we can do a
640"
REFERENCES,0.5671537926235213,"bit better. In Algorithm 2 we present an algorithm that still has the same worst case runtime but runs
641"
REFERENCES,0.5678496868475992,"polynomially in the best case. The main intuition is that we iteratively remove irrelevant nodes in the
642"
REFERENCES,0.5685455810716771,"spirit of the original SCORE algorithm [15]. To this end, we first check if the is any unconfounded
643"
REFERENCES,0.5692414752957551,"sink if we consider the set of all remaining variables. If there is one, we can orient its parents and
644"
REFERENCES,0.569937369519833,"ignore it afterwards. If there is no such set, we need to fall back to the procedure proposed above, i.e.
645"
REFERENCES,0.5706332637439109,"we need to check the condition of Proposition 3 on all subsets of the neighbourhood of a node, until
646"
REFERENCES,0.5713291579679889,"we find no node with a direct outgoing edge. In Proposition 4 we show that this way we do not fail
647"
REFERENCES,0.5720250521920668,"orient edge or fail to remove any adjacency. In the following discussion, we will use the notation
648"
REFERENCES,0.5727209464161448,"δi(XZ) := E[∂Vj log p(VZ) −E[∂Vj log p(VZ)|Rj(VZ) = rj]]2,"
REFERENCES,0.5734168406402227,"for the second residual from Proposition 3 and also
649"
REFERENCES,0.5741127348643006,"δi,j(XZ) :=
∂2"
REFERENCES,0.5748086290883786,"∂Vi∂Vj
log p(vZ)"
REFERENCES,0.5755045233124565,"for the cross-partial derivative, where Xi, Xj ∈V and Z ⊆V .
650"
REFERENCES,0.5762004175365344,"Proposition 4 (Correctness of algorithm). Let X = V ˙∪U be generated by the SCM in Equation (4)
651"
REFERENCES,0.5768963117606124,"with non-constant scores for uncountably many values. Let GX be the causal DAG of X and GV be
652"
REFERENCES,0.5775922059846903,"the marginal MAG of GX. Then Algorithm 2 outputs a directed edge from Xi ∈V to Xj ∈V iff
653"
REFERENCES,0.5782881002087683,"there is a direct edge in GX between them and no unobserved backdoor path w.r.t. U. Further, the
654"
REFERENCES,0.5789839944328462,"output of Algorithm 2 has the same skeleton as GV .
655"
REFERENCES,0.5796798886569241,"Proof. We proof the statement by induction over the steps of the algorithm. Let S be the set of
656"
REFERENCES,0.5803757828810021,"remaining nodes in an arbitrary step of the algorithm. Our induction hypothesis is that for Xi, Xj ∈S
657"
REFERENCES,0.58107167710508,"and Xk ∈Bi we have
658"
REFERENCES,0.581767571329158,"1. Xi is an unconfounded sink w.r.t. to some set S′ ⊆S iff Xi is an unconfounded sink w.r.t.
659"
REFERENCES,0.5824634655532359,"some S′′ ⊆V
660"
REFERENCES,0.5831593597773138,"2. if there is no S′ ⊆V \ {Xi, Xj} such that Xi |="
REFERENCES,0.5838552540013918,"Xj | S′ then Xj ∈Bi
661"
REFERENCES,0.5845511482254697,"Clearly, this holds in the initial step as S = V .
662"
REFERENCES,0.5852470424495476,"Suppose we find δi(XS) = 0 for Xi ∈S. If Xi has at least one adjacent node in MG
V , by
663"
REFERENCES,0.5859429366736256,"Proposition 3, we know that Xi does not have any children and is also not connected to any other
664"
REFERENCES,0.5866388308977035,"node in S via a hidden mediator or unobserved confounder. This means, all nodes that are not
665"
REFERENCES,0.5873347251217815,"separable from Xi must be direct parents of Xi, which are by our induction hypothesis 2) the nodes
666"
REFERENCES,0.5880306193458594,"in Bi. Since Xi does not have children, it also suffices to check Xi |="
REFERENCES,0.5887265135699373,"Xj|S \ {Xi, Xj} for Xj ∈Bi
667"
REFERENCES,0.5894224077940153,"(instead of conditioning on all subsets of Bi). So we can already add these direct edges to the output.
668"
REFERENCES,0.5901183020180932,"If, on the other hand, Xi has no adjacent nodes in MG
V , we have Xi |="
REFERENCES,0.5908141962421712,"Xj|S \{Xi, Xj} for Xj ∈Bi,
669"
REFERENCES,0.5915100904662491,"so in both cases we add the correct set of parents. Since Xi is not an ancestor of any of the nodes in
670"
REFERENCES,0.592205984690327,"S \ {Xi}, Xi cannot be a hidden mediator or hidden confounder between nodes in S \ {Xi} and
671"
REFERENCES,0.592901878914405,"conditioning on Xi cannot block an open path. Thus, the induction hypothesis still holds in the next
672"
REFERENCES,0.5935977731384829,"step.
673"
REFERENCES,0.5942936673625608,"Suppose now there is no unconfounded sink and we explore Xi. By our induction hypothesis 2), Bi
674"
REFERENCES,0.5949895615866388,"contains the parents of Xi and by Proposition 3 it suffices to only look at subsets of Bi to orient direct
675"
REFERENCES,0.5956854558107167,"edges. And also due to the induction hypothesis 2) Bi contains all nodes that are not separable from
676"
REFERENCES,0.5963813500347948,"Xi. So by adding bidirected edges to all nodes in Bi can only add too many edges but not miss some.
677"
REFERENCES,0.5970772442588727,Algorithm 2 AdaScore Algorithm
REFERENCES,0.5977731384829506,"procedure ADASCORE(p, X1, . . . , Xd)"
REFERENCES,0.5984690327070286,"S ←{X1, . . . , Xd}
▷Remaining nodes
E ←{ }
▷Edges
for Xi ∈S do"
REFERENCES,0.5991649269311065,"Bi ←{X1, . . . , Xd}
▷Neighbourhoods
while S ̸= ∅do
▷While nodes remain
if ∃Xi ∈S : δi(XS) = 0 then
▷If there is an unconfounded sink
S ←S \ {Xi}
E ←E ∪{Xj →Xi : δi,j(XS) ̸= 0}
▷Add edges like DAS
else"
REFERENCES,0.5998608211551845,for Xi ∈S do
REFERENCES,0.6005567153792624,"for Xj ∈Bi do
▷Prune neighbourhoods
if δi,j(XS) = 0 then"
REFERENCES,0.6012526096033403,"Bi ←Bi \ {Xj}
Bj ←Bj \ {Xi}
for Xj ∈Bi do
▷Orient edges in Bi
mi = minS′⊆Bi δi(XS′∪{Xi})
mj = minS′⊆Bj δj(XS′∪{Xj}))
if mi = 0 ∧mj ̸= 0 then"
REFERENCES,0.6019485038274183,"E ←E ∪{Xj →Xi}
else if mi ̸= 0 ∧mj = 0 then"
REFERENCES,0.6026443980514962,"E ←E ∪{Xi →Xj}
else"
REFERENCES,0.6033402922755741,"E ←E ∪{Xi ↔Xj}
if ∃Xj ∈Bi : (Xi →Xj) ∈E then"
REFERENCES,0.6040361864996521,"continue with Xj
else
▷Xi has no unconfounded outgoing edge
S ←S \ {Xi}
▷Remove Xi
break
for Xi ↔Xj ∈E do
▷Prune bidirected edges
if minS′⊆Adj(Xi) δi,j(XS′∪{Xi}) = 0 ∨minS′⊆Adj(Xj) δi,j(XS′∪{Xi}) = 0 then"
REFERENCES,0.60473208072373,"E ←E \ {Xi ↔Xj}
return E"
REFERENCES,0.605427974947808,"Now it remains to show that the induction hypothesis holds if we set S to S \ {Xi}. For 1) we need
678"
REFERENCES,0.6061238691718859,"to show that Xi cannot be a hidden mediator or hidden confounder w.r.t. S \ {Xi} (since ignoring
679"
REFERENCES,0.6068197633959638,"Xi won’t change whether there is a direct edge or not). Suppose Xi is on a unobserved causal path
680"
REFERENCES,0.6075156576200418,"Xk →· · · →U m →Xl with Xk, Xl ∈S \ {Xi} and U m ∈X \ (S \ {Xi}). This path must have
681"
REFERENCES,0.6082115518441197,"been a unobserved causal path before, unless Xi = U m. But then there is a direct edge Xi →Xl.
682"
REFERENCES,0.6089074460681977,"We would not remove Xi from S if this edge was unconfounded, so there must a hidden confounder
683"
REFERENCES,0.6096033402922756,"between Xi and Xl. But in this case, Proposition 3 wouldn’t allow us to direct the edge anyway, since
684"
REFERENCES,0.6102992345163535,VPAl ̸ |=
REFERENCES,0.6109951287404315,"d
GUl. Suppose there is confounding path Xk ←· · · →U m →Xl with Xk, Xl ∈S \ {Xi}
685"
REFERENCES,0.6116910229645094,"and U m ∈X \ (S \ {Xi}). If Xi ̸= U m the path was already been a confounding path without Xi
686"
REFERENCES,0.6123869171885873,"being unobserved. So again, there must be a confounder between Xi and Xl, as otherwise we would
687"
REFERENCES,0.6130828114126653,"not remove Xi. And analogously to before, we could not have oriented the edge even with Xi ∈S
688"
REFERENCES,0.6137787056367432,since VPAl ̸ |=
REFERENCES,0.6144745998608212,"d
GUl. For 2) we only have to see that we just remove nodes from Bi if we found an
689"
REFERENCES,0.6151704940848991,"independence.
690"
REFERENCES,0.615866388308977,"For |S| < 2, the algorithm enters the final pruning stage. From the discussion above it is clear,
691"
REFERENCES,0.616562282533055,"that we already have the correct result, up to potentially too many bidirected edges. In the final
692"
REFERENCES,0.6172581767571329,"step we certainly remove all these edges Xi ↔Xj, as we check m-separation for all subsets of the
693"
REFERENCES,0.6179540709812108,"neighbourhoods Adj(Xi) and Adj(Xj), which are supersets of the true neighbourhoods.
694 695"
REFERENCES,0.6186499652052888,"C.2
Finite sample version of AdaScore
696"
REFERENCES,0.6193458594293667,"All theoretical results in the paper have assumed that we know the density of our data. Obviously, in
697"
REFERENCES,0.6200417536534447,"practise we have to deal with a finite sample instead. Especially, in Proposition 1 and Proposition 3
698"
REFERENCES,0.6207376478775226,"we derived criteria that compare random variables with zero. Clearly, this condition is never met in
699"
REFERENCES,0.6214335421016005,"practise. Therefore, we need find ways to reasonably set thresholds for these random quantities.
700"
REFERENCES,0.6221294363256785,"First note, that we use the Stein gradient estimator [35] to estimate the score function. This means
701"
REFERENCES,0.6228253305497564,"especially that for a node Vi we get a vector
702 
( ∂"
REFERENCES,0.6235212247738344,"∂Vi
log p(v))l "
REFERENCES,0.6242171189979123,"l=1,...,m
,
(24)"
REFERENCES,0.6249130132219902,"i.e. an estimate of the score for every one of the m samples. Analogously, we get a m × d × d tensor
703"
REFERENCES,0.6256089074460682,"for the estimates of
∂2
∂Vi∂Vj log p(v).
704"
REFERENCES,0.6263048016701461,"In Proposition 1 we showed that
705 ∂2"
REFERENCES,0.627000695894224,"∂Vi∂Vj
log p(vZ) = 0 ⇐⇒Xi |="
REFERENCES,0.627696590118302,"m
MG
V Vj|VZ \ {Vi, Vj}."
REFERENCES,0.6283924843423799,"In the finite sample version, we use a one sample t-test on the vector of estimated cross-partial
706"
REFERENCES,0.6290883785664579,"derivatives with the null-hypothesis that the means is zero. Due to the central limit theorem, the
707"
REFERENCES,0.6297842727905358,"sample mean follows approximately a Gaussian distribution, regardless of the true distribution of the
708"
REFERENCES,0.6304801670146137,"observations.
709"
REFERENCES,0.6311760612386917,"For Proposition 3 we need to do some additional steps. Recall, that the relevant quantity in Propo-
710"
REFERENCES,0.6318719554627696,"sition 3 is the mean squared error of a regression, which is always positive. Therefore, a test for
711"
REFERENCES,0.6325678496868476,"mean zero is highly likely to reject in any case. We decided to employ a two-sample test in a similar
712"
REFERENCES,0.6332637439109255,"(but different) manner as Montagna et al. [17]. As test, we used the Mann-Whitney U-test. Note,
713"
REFERENCES,0.6339596381350034,"that Algorithm 2 employs Proposition 3 in two different ways: first, to decide whether there is an
714"
REFERENCES,0.6346555323590815,"unconfounded sink and second, to orient edges in case there is no unconfounded sink. We pick a
715"
REFERENCES,0.6353514265831594,"different sample as second sample of the Mann-Whitney U-test.
716"
REFERENCES,0.6360473208072372,"Analogously to before, this is a vector with m entries, one for every sample.
717"
REFERENCES,0.6367432150313153,"Note, that in the case where we want to check if there is an unconfounded sink, we do not make any
718"
REFERENCES,0.6374391092553932,"mistake by rejecting too few hypotheses, i.e. if we miss some unconfounded sinks (instead, we only
719"
REFERENCES,0.6381350034794712,"lose efficiency, as we do the costly iteration over all possible sets of parents). Therefore, for this test
720"
REFERENCES,0.6388308977035491,"we chose a a second sample that yields a “conservative” test result.
721"
REFERENCES,0.639526791927627,"As candidate sink for set S ⊆V , we pick the node Xi = mini mean(δi(XS)). In fact, we want to
722"
REFERENCES,0.640222686151705,"know whether the mean of δi is significantly lower than all other means. But we empirically observed
723"
REFERENCES,0.6409185803757829,"that choosing the concatenated δs of all nodes as second sample makes the test reject with very high
724"
REFERENCES,0.6416144745998609,"probability, which would lead our algorithm to falsely assume the existence of an unconfoudned sink.
725"
REFERENCES,0.6423103688239388,"Instead, we then pick as second “reference node” Xj = minj̸=i mean(δj(XZ)). We then do the two
726"
REFERENCES,0.6430062630480167,"sample test between δi(XZ) and δj(XZ). The intuition is that the test will reject the hypothesis of
727"
REFERENCES,0.6437021572720947,"identical means, if Xi is an unconfounded sink but Xj is not.
728"
REFERENCES,0.6443980514961726,"In the case where we use Proposition 3 to orient edges, we only need to decide whether an not
729"
REFERENCES,0.6450939457202505,"previsouly directed edge Xi −Xj needs to be oriented one way, the other way, or not at all. Instead,
730"
REFERENCES,0.6457898399443285,"here the issue lies in the fact that we need to iterate over possible sets of parents of the nodes. Let
731"
REFERENCES,0.6464857341684064,"Bi be the set of nodes that have not been m-separated from Xi by any test so far. We pick the
732"
REFERENCES,0.6471816283924844,"subset Zi = minZ′⊆Bi mean(δZ′
i ), i.e. the set with the lowest mean error. We then conduct the test
733"
REFERENCES,0.6478775226165623,"with δi(XZi) and δj(XZj). If there is a directed edge between them, one of the residuals will be
734"
REFERENCES,0.6485734168406402,"significantly lower than the other.
735"
REFERENCES,0.6492693110647182,"Just like Montagna et al. [17] we use a cross-validation scheme to generate the residuals, in order to
736"
REFERENCES,0.6499652052887961,"prevent overfitting. We split the dataset into several equally sized, disjoint subsamples. For every
737"
REFERENCES,0.6506610995128741,"residual we fit the regression on all subsamples that don’t contain the respective target.
738"
REFERENCES,0.651356993736952,"Also, just like in the NoGAM algorithm Montagna et al. [17] we add a pruning step for the directed
739"
REFERENCES,0.6520528879610299,"edges to the end. The idea is to use a feature selection method to remove insignificant edges. Just like
740"
REFERENCES,0.6527487821851079,"Montagna et al. [17], we use the CAM-based pruning step proposed by Bühlmann et al. [36], which
741"
REFERENCES,0.6534446764091858,"fits a generalised additive regression model from the parents to a child and test whether one of the
742"
REFERENCES,0.6541405706332637,"additive components is significantly non-zero. All parents for which the test rejects this hypothesis
743"
REFERENCES,0.6548364648573417,"are removed.
744"
REFERENCES,0.6555323590814196,"C.3
Complexity
745"
REFERENCES,0.6562282533054976,"Proposition 5. Complexity Let n be the number of samples and d the number of observable nodes.
746"
REFERENCES,0.6569241475295755,"Algorithm 2 runs in
747"
REFERENCES,0.6576200417536534,"Ω
 
(d2 −d) · (r(n, d) + s(n, d))

and
O
 
d2 · 2d(r(n, d) + s(n, d))

,"
REFERENCES,0.6583159359777314,"where r(n, d) is the time required to solve a regression problem and s(n, d) is the time for calculating
748"
REFERENCES,0.6590118302018093,"the score. With e.g. kernel-ridge regression and the Stein-estimator, both run in O(n3).
749"
REFERENCES,0.6597077244258872,"Proof. Algorithm 2 runs its main loop d times. It first checks for the existence of an unconfounded
750"
REFERENCES,0.6604036186499652,"sink, which involves solving 2d regression problems (including cross-validation prediction) and
751"
REFERENCES,0.6610995128740431,"calculating the score, adding up to (2d2 −d) regressions and d score evaluations. In the worst case,
752"
REFERENCES,0.6617954070981211,"we detect no unconfounded sink and iterate through all subsets of the neighbourhood of a node
753"
REFERENCES,0.662491301322199,"(which is in the worst case of size d −1) and for all other nodes in the neighbourhood we solve 2d
754"
REFERENCES,0.6631871955462769,"regression problems and evaluate the score. For each subset we calculate two regression functions,
755"
REFERENCES,0.6638830897703549,"the score and calculate the entries in the Hessian of the log-density, i.e. d · 2d regressions, d · 2d−1
756"
REFERENCES,0.6645789839944328,"scores and additionally 2d−1 Hessians. If we are unlucky, this node has a directed outgoing edge
757"
REFERENCES,0.6652748782185108,"and we continue with this node (with the same size of nodes). This can happen d −1 times. So we
758"
REFERENCES,0.6659707724425887,"get (d2 −d) · 2d regressions and (d2 −d) · 2d−1 scores and Hessians. In the final pruning step we
759"
REFERENCES,0.6666666666666666,"calculate for every bidirected edge (of which there can be (d2 −d)/2) a Hessian for all subsets of the
760"
REFERENCES,0.6673625608907446,"neighbourhoods, which can again be 2d−1 subsets. Using the pruning procedure from CAM for the
761"
REFERENCES,0.6680584551148225,"directed edges we also spend at most O(nd3) steps.
762"
REFERENCES,0.6687543493389004,"In the best case, we always find an unconfounded sink. Then our algorithm reduces to NoGAM.
763 764"
REFERENCES,0.6694502435629784,"D
Experimental details
765"
REFERENCES,0.6701461377870563,"In this section, we present the details of our experiments in terms of synthetic data generation and
766"
REFERENCES,0.6708420320111343,"algorithms hyperparameters.
767"
REFERENCES,0.6715379262352122,"D.1
Synthetic data generation
768"
REFERENCES,0.6722338204592901,"In this work, we rely on synthetic data to benchmark AdaScore’s finite samples performance. For
769"
REFERENCES,0.6729297146833682,"each dataset, we first sample the ground truth graph and then generate the observations according to
770"
REFERENCES,0.673625608907446,"the causal graph.
771"
REFERENCES,0.6743215031315241,"Erdös-Renyi graphs.
The ground truth graphs are generated according to the Erdös-Renyi model.
772"
REFERENCES,0.675017397355602,"It allows specifying the number of nodes and the probability of connecting each pair of nodes). In ER
773"
REFERENCES,0.6757132915796799,"graphs, a pair of nodes has the same probability of being connected.
774"
REFERENCES,0.6764091858037579,"Nonlinear causal mechanisms.
Nonlinear causal mechanisms are parametrized by a neural network
775"
REFERENCES,0.6771050800278358,"with random weights. We create a fully connected neural network with one hidden layer with 10
776"
REFERENCES,0.6778009742519137,"units, Parametric ReLU activation function, followed by one normalizing layer before the final fully
777"
REFERENCES,0.6784968684759917,"connected layer. The weights of the neural network are sampled from a standard Gaussian distribution.
778"
REFERENCES,0.6791927627000696,"This strategy for synthetic data generation is commonly adopted in the literature [26, 18, 28, 29, 27].
779"
REFERENCES,0.6798886569241476,"Linear causal mechanisms.
For the linear mechanisms, we define a simple linear regression model
780"
REFERENCES,0.6805845511482255,"predicting the effects from their causes and noise terms, weighted by randomly sampled coefficients.
781"
REFERENCES,0.6812804453723034,"Coefficients are generated as samples from a Uniform distribution supported in the range [−3, −0.5]∪
782"
REFERENCES,0.6819763395963814,"[0.5, 3]. We avoid too small coefficients to avoid close to unfaithful datasets Uhler et al. [24].
783"
REFERENCES,0.6826722338204593,"Noise terms distribution.
The noise terms are sampled from a Uniform distribution supported
784"
REFERENCES,0.6833681280445373,"between −2 and 2.
785"
REFERENCES,0.6840640222686152,"Finally, we remark that we standardize the data by their empirical data. This is known to remove
786"
REFERENCES,0.6847599164926931,"shortcuts that allow finding a correct causal order sorting variables by their marginal variance, as in
787"
REFERENCES,0.6854558107167711,"varsortability, described in Reisach et al. [32], or sorting variables by the magnitude of their score
788"
REFERENCES,0.686151704940849,"|∂Xi log p(X)|, a phenomenon known as scoresortability analyzed by Montagna et al. [18].
789"
REFERENCES,0.6868475991649269,"D.2
AdaScore hyperparameters
790"
REFERENCES,0.6875434933890049,"For AdaScore, we set the α level for the required hypothesis testing at 0.05. For the CAM-pruning
791"
REFERENCES,0.6882393876130828,"step, the level is instead set at 0.001, the default value of the dodidscover Python implementation of
792"
REFERENCES,0.6889352818371608,"the method, and commonly found in all papers using CAM-pruning for edge selection [15, 16, 17, 36].
793"
REFERENCES,0.6896311760612387,"For the remaining parameters. The regression hyperparameters for the estimation of the residuals are
794"
REFERENCES,0.6903270702853166,"found via cross-validation during inference: tuning is done minimizing the generalization error on
795"
REFERENCES,0.6910229645093946,"the estimated residuals, without using the performance on the causal graph ground truth. Finally, for
796"
REFERENCES,0.6917188587334725,"the score matching estimation, the regularization coefficients are set to 0.001.
797"
REFERENCES,0.6924147529575505,"D.3
Computer resources
798"
REFERENCES,0.6931106471816284,"All experiments have been run on an AWS EC2 instance of type p3.2xlarge. These machines
799"
REFERENCES,0.6938065414057063,"contain Intel Xeon E5-2686-v4 processors with 2.3 GHz and 8 virtual cores as well as 61 GB RAM.
800"
REFERENCES,0.6945024356297843,"All experiments can be run within a day.
801"
REFERENCES,0.6951983298538622,"E
Additional Experiments
802"
REFERENCES,0.6958942240779401,"In this section, we provide additional experimental results. All synthetic data has been generated as
803"
REFERENCES,0.6965901183020181,"described in Appendix D.1.
804"
REFERENCES,0.697286012526096,"E.1
Non-additive mechanisms
805"
REFERENCES,0.697981906750174,"In Figure 1 we have demonstrated the performance of our proposed method on data generated by
806"
REFERENCES,0.6986778009742519,"linear SCMs and non-linear SCMs with additive noise. But Proposition 1 also holds for any faithful
807"
REFERENCES,0.6993736951983298,"distribution generated by an acyclic model. Thus, we employed as mechanism a neural network-based
808"
REFERENCES,0.7000695894224078,"approach similar to the non-linear mechanism described in Appendix D. Instead of adding the noise
809"
REFERENCES,0.7007654836464857,"term, we feed it as additional input into the neural network. Results in this setting are reported in
810"
REFERENCES,0.7014613778705637,"Figure 2. As neither AdaScore nor any of the baseline algorithms has theoretical guarantees for the
811"
REFERENCES,0.7021572720946416,"orientation of edges in this scenario, we report the F1-score (popular in classification problems) w.r.t.
812"
REFERENCES,0.7028531663187195,"to the existence of an edge, regardless of orientation. Our experiments show that AdaScore can, in
813"
REFERENCES,0.7035490605427975,"general, correctly recover the graph’s skeleton in all the scenarios, with an F1 score median between
814"
REFERENCES,0.7042449547668754,"1 and ∼0.75, respectively for small and large numbers of nodes.
815"
REFERENCES,0.7049408489909533,"E.2
Sparse graphs
816"
REFERENCES,0.7056367432150313,"In this section, we present the experiments on sparse Erdös-Renyi graphs where each pair of nodes
817"
REFERENCES,0.7063326374391092,"is connected by an edge with probability 0.3. The results are illustrated in Figure 3. For sparse
818"
REFERENCES,0.7070285316631872,"graphs, recovery results are similar to the dense case, with AdaScore generally providing comparable
819"
REFERENCES,0.7077244258872651,"performance to the other methods.
820"
REFERENCES,0.708420320111343,"E.3
Increasing number of samples
821"
REFERENCES,0.709116214335421,"In the following series of plots we demonstrate the scaling behaviour of our method w.r.t. to the
822"
REFERENCES,0.7098121085594989,"number of samples. Figure 5 shows results with edge probability 0.5 and Figure 4 with 0.3. All
823"
REFERENCES,0.7105080027835768,"graphs contain seven observable nodes. As before we observe that AdaScore performs comparably to
824"
REFERENCES,0.7112038970076549,"other methods. E.g. in Figures 4a and 5b we can see that the median error AdaScore improves with
825"
REFERENCES,0.7118997912317327,"additional samples and in all plots we see that no other algorithm seems to gain an advantage over
826"
REFERENCES,0.7125956854558108,"AdaScore with increasing sample size.
827"
REFERENCES,0.7132915796798887,"adascore
camuv
nogam
rcd
lingam"
REFERENCES,0.7139874739039666,"3
5
7
9
number of nodes 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7146833681280446,skeleton_f1
REFERENCES,0.7153792623521225,sparse
REFERENCES,0.7160751565762005,"3
5
7
9
number of nodes 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7167710508002784,skeleton_f1 dense
REFERENCES,0.7174669450243563,(a) Fully observable model
REFERENCES,0.7181628392484343,"3
5
7
9
number of nodes 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7188587334725122,skeleton_f1
REFERENCES,0.7195546276965901,sparse
REFERENCES,0.7202505219206681,"3
5
7
9
number of nodes 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.720946416144746,skeleton_f1 dense
REFERENCES,0.721642310368824,(b) Latent variables model
REFERENCES,0.7223382045929019,"Figure 2: Empirical results for non-additive causal mechanisms on sparse graphs with different numbers of
nodes, on fully observable (no hidden variables) and latent variable models. We report the F1 score w.r.t. the
existence of edges (the higher, the better)."
REFERENCES,0.7230340988169798,"adascore
camuv
nogam
rcd
lingam"
REFERENCES,0.7237299930410578,"3
5
7
9
number of nodes 0 2 4 6 8 10 12 14 16 shd"
REFERENCES,0.7244258872651357,linear
REFERENCES,0.7251217814892137,"3
5
7
9
number of nodes 0 5 10 15 20 25 shd"
REFERENCES,0.7258176757132916,nonlinear
REFERENCES,0.7265135699373695,(a) Fully observable model
REFERENCES,0.7272094641614475,"3
5
7
9
number of nodes 0 5 10 15 20 25 shd"
REFERENCES,0.7279053583855254,linear
REFERENCES,0.7286012526096033,"3
5
7
9
number of nodes 0 5 10 15 20 25 shd"
REFERENCES,0.7292971468336813,nonlinear
REFERENCES,0.7299930410577592,(b) Latent variables model
REFERENCES,0.7306889352818372,"Figure 3: Empirical results on sparse graphs with different numbers of nodes, on fully observable (no hidden
variables) and latent variable models. We report the SHD accuracy (the lower, the better)."
REFERENCES,0.7313848295059151,"adascore
camuv
nogam
rcd
lingam"
REFERENCES,0.732080723729993,"500
1000
1500
2000
number of samples 0 2 4 6 8 10 12 14 shd"
REFERENCES,0.732776617954071,linear
REFERENCES,0.7334725121781489,"500
1000
1500
2000
number of samples 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 shd"
REFERENCES,0.7341684064022269,nonlinear
REFERENCES,0.7348643006263048,(a) Fully observable model
REFERENCES,0.7355601948503827,"500
1000
1500
2000
number of samples 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 shd"
REFERENCES,0.7362560890744607,linear
REFERENCES,0.7369519832985386,"500
1000
1500
2000
number of samples 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 shd"
REFERENCES,0.7376478775226165,nonlinear
REFERENCES,0.7383437717466945,(b) Latent variables model
REFERENCES,0.7390396659707724,"Figure 4: Empirical results on sparse graphs with different numbers of samples and seven nodes, on fully
observable (no hidden variables) and latent variable models. We report the SHD accuracy (the lower, the better)."
REFERENCES,0.7397355601948504,"adascore
camuv
nogam
rcd
lingam"
REFERENCES,0.7404314544189283,"500
1000
1500
2000
number of samples 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 shd"
REFERENCES,0.7411273486430062,nonlinear
REFERENCES,0.7418232428670842,"500
1000
1500
2000
number of samples 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 shd"
REFERENCES,0.7425191370911621,linear
REFERENCES,0.7432150313152401,(a) Fully observable model
REFERENCES,0.743910925539318,"500
1000
1500
2000
number of samples 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 shd"
REFERENCES,0.7446068197633959,nonlinear
REFERENCES,0.7453027139874739,"500
1000
1500
2000
number of samples 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 shd"
REFERENCES,0.7459986082115518,linear
REFERENCES,0.7466945024356297,(b) Latent variables model
REFERENCES,0.7473903966597077,"Figure 5: Empirical results on dense graphs with different numbers of samples and seven nodes, on fully
observable (no hidden variables) and latent variable models. We report the SHD accuracy (the lower, the better)."
REFERENCES,0.7480862908837856,"E.4
Limitations
828"
REFERENCES,0.7487821851078637,"In this section, we remark the limitations of our empirical study. It is well known that causal discovery
829"
REFERENCES,0.7494780793319415,"lacks meaningful, multivariate benchmark datasets with known ground truth. For this reason, it is
830"
REFERENCES,0.7501739735560194,"common to rely on synthetically generated datasets. We believe that results on synthetic graphs should
831"
REFERENCES,0.7508698677800975,"be taken with care, as there is no strong reason to believe that they should mirror the benchmarked
832"
REFERENCES,0.7515657620041754,"algorithms’ behaviors in real-world settings, where often there is no prior knowledge about the
833"
REFERENCES,0.7522616562282533,"structural causal model underlying available observations.
834"
REFERENCES,0.7529575504523313,"NeurIPS Paper Checklist
835"
REFERENCES,0.7536534446764092,"The checklist is designed to encourage best practices for responsible machine learning research,
836"
REFERENCES,0.7543493389004872,"addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
837"
REFERENCES,0.7550452331245651,"the checklist: The papers not including the checklist will be desk rejected. The checklist should
838"
REFERENCES,0.755741127348643,"follow the references and precede the (optional) supplemental material. The checklist does NOT
839"
REFERENCES,0.756437021572721,"count towards the page limit.
840"
REFERENCES,0.7571329157967989,"Please read the checklist guidelines carefully for information on how to answer these questions. For
841"
REFERENCES,0.7578288100208769,"each question in the checklist:
842"
REFERENCES,0.7585247042449548,"• You should answer [Yes] , [No] , or [NA] .
843"
REFERENCES,0.7592205984690327,"• [NA] means either that the question is Not Applicable for that particular paper or the
844"
REFERENCES,0.7599164926931107,"relevant information is Not Available.
845"
REFERENCES,0.7606123869171886,"• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
846"
REFERENCES,0.7613082811412665,"The checklist answers are an integral part of your paper submission. They are visible to the
847"
REFERENCES,0.7620041753653445,"reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
848"
REFERENCES,0.7627000695894224,"(after eventual revisions) with the final version of your paper, and its final version will be published
849"
REFERENCES,0.7633959638135004,"with the paper.
850"
REFERENCES,0.7640918580375783,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
851"
REFERENCES,0.7647877522616562,"While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
852"
REFERENCES,0.7654836464857342,"proper justification is given (e.g., ""error bars are not reported because it would be too computationally
853"
REFERENCES,0.7661795407098121,"expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
854"
REFERENCES,0.7668754349338901,"""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
855"
REFERENCES,0.767571329157968,"acknowledge that the true answer is often more nuanced, so please just use your best judgment and
856"
REFERENCES,0.7682672233820459,"write a justification to elaborate. All supporting evidence can appear either in the main paper or the
857"
REFERENCES,0.7689631176061239,"supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
858"
REFERENCES,0.7696590118302018,"please point to the section(s) where related material for the question can be found.
859"
CLAIMS,0.7703549060542797,"1. Claims
860"
CLAIMS,0.7710508002783577,"Question: Do the main claims made in the abstract and introduction accurately reflect the
861"
CLAIMS,0.7717466945024356,"paper’s contributions and scope?
862"
CLAIMS,0.7724425887265136,"Answer: [Yes]
863"
CLAIMS,0.7731384829505915,"Justification: In the abstract, we claim that we connect the properties of the score function
864"
CLAIMS,0.7738343771746694,"to causal structure learning. In the paper, particularly sections 3 and 4, we present the
865"
CLAIMS,0.7745302713987474,"theoretical results supporting our claim. Further, in the abstract we mention that based on
866"
CLAIMS,0.7752261656228253,"our theory we propose an algorithm for causal discovery from score matching estimation,
867"
CLAIMS,0.7759220598469033,"algorithm that we define in Section 4.3 and we empirically validate in Section 5 and
868"
CLAIMS,0.7766179540709812,"Appendix E.
869"
CLAIMS,0.7773138482950591,"Guidelines:
870"
CLAIMS,0.7780097425191371,"• The answer NA means that the abstract and introduction do not include the claims
871"
CLAIMS,0.778705636743215,"made in the paper.
872"
CLAIMS,0.7794015309672929,"• The abstract and/or introduction should clearly state the claims made, including the
873"
CLAIMS,0.7800974251913709,"contributions made in the paper and important assumptions and limitations. A No or
874"
CLAIMS,0.7807933194154488,"NA answer to this question will not be perceived well by the reviewers.
875"
CLAIMS,0.7814892136395268,"• The claims made should match theoretical and experimental results, and reflect how
876"
CLAIMS,0.7821851078636047,"much the results can be expected to generalize to other settings.
877"
CLAIMS,0.7828810020876826,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
878"
CLAIMS,0.7835768963117606,"are not attained by the paper.
879"
LIMITATIONS,0.7842727905358385,"2. Limitations
880"
LIMITATIONS,0.7849686847599165,"Question: Does the paper discuss the limitations of the work performed by the authors?
881"
LIMITATIONS,0.7856645789839944,"Answer: [Yes]
882"
LIMITATIONS,0.7863604732080723,"Justification: The main limitation of our work is on the experimental side: our experiments
883"
LIMITATIONS,0.7870563674321504,"are limited to synthetic data, which are not an ideal probing ground. Additionally, our
884"
LIMITATIONS,0.7877522616562282,"method does not provide performance that clearly improves on the existing literature. These
885"
LIMITATIONS,0.7884481558803061,"limitations of our work are discussed in the discussion of the experiments in Section 5, as
886"
LIMITATIONS,0.7891440501043842,"well as in the ""Limitations"" appendix section E.4. Concerning the assumptions required by
887"
LIMITATIONS,0.789839944328462,"our method, we thoroughly discuss them in the theoretical sections of the paper, where we
888"
LIMITATIONS,0.7905358385525401,"define the results that are later used for the definition of the AdaScore. Finally, computational
889"
LIMITATIONS,0.791231732776618,"complexity is discussed in Appendix C.3.
890"
LIMITATIONS,0.7919276270006959,"Guidelines:
891"
LIMITATIONS,0.7926235212247739,"• The answer NA means that the paper has no limitation while the answer No means that
892"
LIMITATIONS,0.7933194154488518,"the paper has limitations, but those are not discussed in the paper.
893"
LIMITATIONS,0.7940153096729298,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
894"
LIMITATIONS,0.7947112038970077,"• The paper should point out any strong assumptions and how robust the results are to
895"
LIMITATIONS,0.7954070981210856,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
896"
LIMITATIONS,0.7961029923451636,"model well-specification, asymptotic approximations only holding locally). The authors
897"
LIMITATIONS,0.7967988865692415,"should reflect on how these assumptions might be violated in practice and what the
898"
LIMITATIONS,0.7974947807933194,"implications would be.
899"
LIMITATIONS,0.7981906750173974,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
900"
LIMITATIONS,0.7988865692414753,"only tested on a few datasets or with a few runs. In general, empirical results often
901"
LIMITATIONS,0.7995824634655533,"depend on implicit assumptions, which should be articulated.
902"
LIMITATIONS,0.8002783576896312,"• The authors should reflect on the factors that influence the performance of the approach.
903"
LIMITATIONS,0.8009742519137091,"For example, a facial recognition algorithm may perform poorly when image resolution
904"
LIMITATIONS,0.8016701461377871,"is low or images are taken in low lighting. Or a speech-to-text system might not be
905"
LIMITATIONS,0.802366040361865,"used reliably to provide closed captions for online lectures because it fails to handle
906"
LIMITATIONS,0.8030619345859429,"technical jargon.
907"
LIMITATIONS,0.8037578288100209,"• The authors should discuss the computational efficiency of the proposed algorithms
908"
LIMITATIONS,0.8044537230340988,"and how they scale with dataset size.
909"
LIMITATIONS,0.8051496172581768,"• If applicable, the authors should discuss possible limitations of their approach to
910"
LIMITATIONS,0.8058455114822547,"address problems of privacy and fairness.
911"
LIMITATIONS,0.8065414057063326,"• While the authors might fear that complete honesty about limitations might be used by
912"
LIMITATIONS,0.8072372999304106,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
913"
LIMITATIONS,0.8079331941544885,"limitations that aren’t acknowledged in the paper. The authors should use their best
914"
LIMITATIONS,0.8086290883785665,"judgment and recognize that individual actions in favor of transparency play an impor-
915"
LIMITATIONS,0.8093249826026444,"tant role in developing norms that preserve the integrity of the community. Reviewers
916"
LIMITATIONS,0.8100208768267223,"will be specifically instructed to not penalize honesty concerning limitations.
917"
THEORY ASSUMPTIONS AND PROOFS,0.8107167710508003,"3. Theory Assumptions and Proofs
918"
THEORY ASSUMPTIONS AND PROOFS,0.8114126652748782,"Question: For each theoretical result, does the paper provide the full set of assumptions and
919"
THEORY ASSUMPTIONS AND PROOFS,0.8121085594989561,"a complete (and correct) proof?
920"
THEORY ASSUMPTIONS AND PROOFS,0.8128044537230341,"Answer: [Yes]
921"
THEORY ASSUMPTIONS AND PROOFS,0.813500347947112,"Justification: All our theoretical results make explicit the assumptions for which they are
922"
THEORY ASSUMPTIONS AND PROOFS,0.81419624217119,"valid. Plus, we in section Appendix B we provide the proofs of our theoretical results.
923"
THEORY ASSUMPTIONS AND PROOFS,0.8148921363952679,"Guidelines:
924"
THEORY ASSUMPTIONS AND PROOFS,0.8155880306193458,"• The answer NA means that the paper does not include theoretical results.
925"
THEORY ASSUMPTIONS AND PROOFS,0.8162839248434238,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
926"
THEORY ASSUMPTIONS AND PROOFS,0.8169798190675017,"referenced.
927"
THEORY ASSUMPTIONS AND PROOFS,0.8176757132915797,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
928"
THEORY ASSUMPTIONS AND PROOFS,0.8183716075156576,"• The proofs can either appear in the main paper or the supplemental material, but if
929"
THEORY ASSUMPTIONS AND PROOFS,0.8190675017397355,"they appear in the supplemental material, the authors are encouraged to provide a short
930"
THEORY ASSUMPTIONS AND PROOFS,0.8197633959638135,"proof sketch to provide intuition.
931"
THEORY ASSUMPTIONS AND PROOFS,0.8204592901878914,"• Inversely, any informal proof provided in the core of the paper should be complemented
932"
THEORY ASSUMPTIONS AND PROOFS,0.8211551844119693,"by formal proofs provided in appendix or supplemental material.
933"
THEORY ASSUMPTIONS AND PROOFS,0.8218510786360473,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
934"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8225469728601252,"4. Experimental Result Reproducibility
935"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8232428670842032,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
936"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8239387613082811,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
937"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.824634655532359,"of the paper (regardless of whether the code and data are provided or not)?
938"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.825330549756437,"Answer: [Yes]
939"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.826026443980515,"Justification: In Appendix D, we provide all the details to reproduce the data generation of
940"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.826722338204593,"our experiments and the hyperparameters used in AdaScore for our experimental runs.
941"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8274182324286709,"Guidelines:
942"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8281141266527487,"• The answer NA means that the paper does not include experiments.
943"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8288100208768268,"• If the paper includes experiments, a No answer to this question will not be perceived
944"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8295059151009047,"well by the reviewers: Making the paper reproducible is important, regardless of
945"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8302018093249826,"whether the code and data are provided or not.
946"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8308977035490606,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
947"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8315935977731385,"to make their results reproducible or verifiable.
948"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8322894919972165,"• Depending on the contribution, reproducibility can be accomplished in various ways.
949"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8329853862212944,"For example, if the contribution is a novel architecture, describing the architecture fully
950"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8336812804453723,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
951"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8343771746694503,"be necessary to either make it possible for others to replicate the model with the same
952"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8350730688935282,"dataset, or provide access to the model. In general. releasing code and data is often
953"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8357689631176062,"one good way to accomplish this, but reproducibility can also be provided via detailed
954"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8364648573416841,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
955"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.837160751565762,"of a large language model), releasing of a model checkpoint, or other means that are
956"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.83785664578984,"appropriate to the research performed.
957"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8385525400139179,"• While NeurIPS does not require releasing code, the conference does require all submis-
958"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8392484342379958,"sions to provide some reasonable avenue for reproducibility, which may depend on the
959"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8399443284620738,"nature of the contribution. For example
960"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8406402226861517,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
961"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8413361169102297,"to reproduce that algorithm.
962"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8420320111343076,"(b) If the contribution is primarily a new model architecture, the paper should describe
963"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8427279053583855,"the architecture clearly and fully.
964"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8434237995824635,"(c) If the contribution is a new model (e.g., a large language model), then there should
965"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8441196938065414,"either be a way to access this model for reproducing the results or a way to reproduce
966"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8448155880306193,"the model (e.g., with an open-source dataset or instructions for how to construct
967"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8455114822546973,"the dataset).
968"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8462073764787752,"(d) We recognize that reproducibility may be tricky in some cases, in which case
969"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8469032707028532,"authors are welcome to describe the particular way they provide for reproducibility.
970"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8475991649269311,"In the case of closed-source models, it may be that access to the model is limited in
971"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.848295059151009,"some way (e.g., to registered users), but it should be possible for other researchers
972"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.848990953375087,"to have some path to reproducing or verifying the results.
973"
OPEN ACCESS TO DATA AND CODE,0.8496868475991649,"5. Open access to data and code
974"
OPEN ACCESS TO DATA AND CODE,0.8503827418232429,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
975"
OPEN ACCESS TO DATA AND CODE,0.8510786360473208,"tions to faithfully reproduce the main experimental results, as described in supplemental
976"
OPEN ACCESS TO DATA AND CODE,0.8517745302713987,"material?
977"
OPEN ACCESS TO DATA AND CODE,0.8524704244954767,"Answer: [Yes]
978"
OPEN ACCESS TO DATA AND CODE,0.8531663187195546,"Justification: We provide the code for the experiments and the data generation in a zip file.
979"
OPEN ACCESS TO DATA AND CODE,0.8538622129436325,"Further, we describe all the details for reproducing our experimental results in Appendix D.
980"
OPEN ACCESS TO DATA AND CODE,0.8545581071677105,"Guidelines:
981"
OPEN ACCESS TO DATA AND CODE,0.8552540013917884,"• The answer NA means that paper does not include experiments requiring code.
982"
OPEN ACCESS TO DATA AND CODE,0.8559498956158664,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
983"
OPEN ACCESS TO DATA AND CODE,0.8566457898399443,"public/guides/CodeSubmissionPolicy) for more details.
984"
OPEN ACCESS TO DATA AND CODE,0.8573416840640222,"• While we encourage the release of code and data, we understand that this might not be
985"
OPEN ACCESS TO DATA AND CODE,0.8580375782881002,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
986"
OPEN ACCESS TO DATA AND CODE,0.8587334725121781,"including code, unless this is central to the contribution (e.g., for a new open-source
987"
OPEN ACCESS TO DATA AND CODE,0.8594293667362561,"benchmark).
988"
OPEN ACCESS TO DATA AND CODE,0.860125260960334,"• The instructions should contain the exact command and environment needed to run to
989"
OPEN ACCESS TO DATA AND CODE,0.8608211551844119,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
990"
OPEN ACCESS TO DATA AND CODE,0.8615170494084899,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
991"
OPEN ACCESS TO DATA AND CODE,0.8622129436325678,"• The authors should provide instructions on data access and preparation, including how
992"
OPEN ACCESS TO DATA AND CODE,0.8629088378566457,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
993"
OPEN ACCESS TO DATA AND CODE,0.8636047320807237,"• The authors should provide scripts to reproduce all experimental results for the new
994"
OPEN ACCESS TO DATA AND CODE,0.8643006263048016,"proposed method and baselines. If only a subset of experiments are reproducible, they
995"
OPEN ACCESS TO DATA AND CODE,0.8649965205288797,"should state which ones are omitted from the script and why.
996"
OPEN ACCESS TO DATA AND CODE,0.8656924147529576,"• At submission time, to preserve anonymity, the authors should release anonymized
997"
OPEN ACCESS TO DATA AND CODE,0.8663883089770354,"versions (if applicable).
998"
OPEN ACCESS TO DATA AND CODE,0.8670842032011135,"• Providing as much information as possible in supplemental material (appended to the
999"
OPEN ACCESS TO DATA AND CODE,0.8677800974251914,"paper) is recommended, but including URLs to data and code is permitted.
1000"
OPEN ACCESS TO DATA AND CODE,0.8684759916492694,"6. Experimental Setting/Details
1001"
OPEN ACCESS TO DATA AND CODE,0.8691718858733473,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
1002"
OPEN ACCESS TO DATA AND CODE,0.8698677800974252,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
1003"
OPEN ACCESS TO DATA AND CODE,0.8705636743215032,"results?
1004"
OPEN ACCESS TO DATA AND CODE,0.8712595685455811,"Answer: [Yes]
1005"
OPEN ACCESS TO DATA AND CODE,0.871955462769659,"Justification: In our experiments section 5, we present all the necessary details on the
1006"
OPEN ACCESS TO DATA AND CODE,0.872651356993737,"data generation procedure and a description of the empirical results that are necessary for
1007"
OPEN ACCESS TO DATA AND CODE,0.8733472512178149,"understanding our findings. Additionally, a comprehensive overview of our experimental
1008"
OPEN ACCESS TO DATA AND CODE,0.8740431454418929,"design is presented in Appendix D.
1009"
OPEN ACCESS TO DATA AND CODE,0.8747390396659708,"Guidelines:
1010"
OPEN ACCESS TO DATA AND CODE,0.8754349338900487,"• The answer NA means that the paper does not include experiments.
1011"
OPEN ACCESS TO DATA AND CODE,0.8761308281141267,"• The experimental setting should be presented in the core of the paper to a level of detail
1012"
OPEN ACCESS TO DATA AND CODE,0.8768267223382046,"that is necessary to appreciate the results and make sense of them.
1013"
OPEN ACCESS TO DATA AND CODE,0.8775226165622826,"• The full details can be provided either with the code, in appendix, or as supplemental
1014"
OPEN ACCESS TO DATA AND CODE,0.8782185107863605,"material.
1015"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8789144050104384,"7. Experiment Statistical Significance
1016"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8796102992345164,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
1017"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8803061934585943,"information about the statistical significance of the experiments?
1018"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8810020876826722,"Answer: [Yes]
1019"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8816979819067502,"Justification: We report all our experimental results in the form of boxplots.
1020"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8823938761308281,"Guidelines:
1021"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8830897703549061,"• The answer NA means that the paper does not include experiments.
1022"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.883785664578984,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
1023"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8844815588030619,"dence intervals, or statistical significance tests, at least for the experiments that support
1024"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8851774530271399,"the main claims of the paper.
1025"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8858733472512178,"• The factors of variability that the error bars are capturing should be clearly stated (for
1026"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8865692414752957,"example, train/test split, initialization, random drawing of some parameter, or overall
1027"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8872651356993737,"run with given experimental conditions).
1028"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8879610299234516,"• The method for calculating the error bars should be explained (closed form formula,
1029"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8886569241475296,"call to a library function, bootstrap, etc.)
1030"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8893528183716075,"• The assumptions made should be given (e.g., Normally distributed errors).
1031"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8900487125956854,"• It should be clear whether the error bar is the standard deviation or the standard error
1032"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8907446068197634,"of the mean.
1033"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8914405010438413,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
1034"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8921363952679193,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
1035"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8928322894919972,"of Normality of errors is not verified.
1036"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8935281837160751,"• For asymmetric distributions, the authors should be careful not to show in tables or
1037"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8942240779401531,"figures symmetric error bars that would yield results that are out of range (e.g. negative
1038"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.894919972164231,"error rates).
1039"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8956158663883089,"• If error bars are reported in tables or plots, The authors should explain in the text how
1040"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8963117606123869,"they were calculated and reference the corresponding figures or tables in the text.
1041"
EXPERIMENTS COMPUTE RESOURCES,0.8970076548364648,"8. Experiments Compute Resources
1042"
EXPERIMENTS COMPUTE RESOURCES,0.8977035490605428,"Question: For each experiment, does the paper provide sufficient information on the com-
1043"
EXPERIMENTS COMPUTE RESOURCES,0.8983994432846207,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
1044"
EXPERIMENTS COMPUTE RESOURCES,0.8990953375086986,"the experiments?
1045"
EXPERIMENTS COMPUTE RESOURCES,0.8997912317327766,"Answer: [Yes]
1046"
EXPERIMENTS COMPUTE RESOURCES,0.9004871259568545,"Justification: Details on the computer resources required for the experiments can be found
1047"
EXPERIMENTS COMPUTE RESOURCES,0.9011830201809325,"in Appendix D.3.
1048"
EXPERIMENTS COMPUTE RESOURCES,0.9018789144050104,"Guidelines:
1049"
EXPERIMENTS COMPUTE RESOURCES,0.9025748086290883,"• The answer NA means that the paper does not include experiments.
1050"
EXPERIMENTS COMPUTE RESOURCES,0.9032707028531664,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
1051"
EXPERIMENTS COMPUTE RESOURCES,0.9039665970772442,"or cloud provider, including relevant memory and storage.
1052"
EXPERIMENTS COMPUTE RESOURCES,0.9046624913013221,"• The paper should provide the amount of compute required for each of the individual
1053"
EXPERIMENTS COMPUTE RESOURCES,0.9053583855254002,"experimental runs as well as estimate the total compute.
1054"
EXPERIMENTS COMPUTE RESOURCES,0.906054279749478,"• The paper should disclose whether the full research project required more compute
1055"
EXPERIMENTS COMPUTE RESOURCES,0.9067501739735561,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
1056"
EXPERIMENTS COMPUTE RESOURCES,0.907446068197634,"didn’t make it into the paper).
1057"
CODE OF ETHICS,0.9081419624217119,"9. Code Of Ethics
1058"
CODE OF ETHICS,0.9088378566457899,"Question: Does the research conducted in the paper conform, in every respect, with the
1059"
CODE OF ETHICS,0.9095337508698678,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
1060"
CODE OF ETHICS,0.9102296450939458,"Answer: [Yes]
1061"
CODE OF ETHICS,0.9109255393180237,"Justification: We do not believe any of the concerns in the Code of Ethics apply to our work.
1062"
CODE OF ETHICS,0.9116214335421016,"Guidelines:
1063"
CODE OF ETHICS,0.9123173277661796,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
1064"
CODE OF ETHICS,0.9130132219902575,"• If the authors answer No, they should explain the special circumstances that require a
1065"
CODE OF ETHICS,0.9137091162143354,"deviation from the Code of Ethics.
1066"
CODE OF ETHICS,0.9144050104384134,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
1067"
CODE OF ETHICS,0.9151009046624913,"eration due to laws or regulations in their jurisdiction).
1068"
BROADER IMPACTS,0.9157967988865693,"10. Broader Impacts
1069"
BROADER IMPACTS,0.9164926931106472,"Question: Does the paper discuss both potential positive societal impacts and negative
1070"
BROADER IMPACTS,0.9171885873347251,"societal impacts of the work performed?
1071"
BROADER IMPACTS,0.9178844815588031,"Answer: [NA]
1072"
BROADER IMPACTS,0.918580375782881,"Justification: In this work, we present a novel causal discovery method from observational
1073"
BROADER IMPACTS,0.919276270006959,"data. We believe there are no specific negative societal impacts, while positive impacts are
1074"
BROADER IMPACTS,0.9199721642310369,"those generally recognized to causal discovery, as discussed in the Introduction section 1.
1075"
BROADER IMPACTS,0.9206680584551148,"Guidelines:
1076"
BROADER IMPACTS,0.9213639526791928,"• The answer NA means that there is no societal impact of the work performed.
1077"
BROADER IMPACTS,0.9220598469032707,"• If the authors answer NA or No, they should explain why their work has no societal
1078"
BROADER IMPACTS,0.9227557411273486,"impact or why the paper does not address societal impact.
1079"
BROADER IMPACTS,0.9234516353514266,"• Examples of negative societal impacts include potential malicious or unintended uses
1080"
BROADER IMPACTS,0.9241475295755045,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
1081"
BROADER IMPACTS,0.9248434237995825,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
1082"
BROADER IMPACTS,0.9255393180236604,"groups), privacy considerations, and security considerations.
1083"
BROADER IMPACTS,0.9262352122477383,"• The conference expects that many papers will be foundational research and not tied
1084"
BROADER IMPACTS,0.9269311064718163,"to particular applications, let alone deployments. However, if there is a direct path to
1085"
BROADER IMPACTS,0.9276270006958942,"any negative applications, the authors should point it out. For example, it is legitimate
1086"
BROADER IMPACTS,0.9283228949199722,"to point out that an improvement in the quality of generative models could be used to
1087"
BROADER IMPACTS,0.9290187891440501,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
1088"
BROADER IMPACTS,0.929714683368128,"that a generic algorithm for optimizing neural networks could enable people to train
1089"
BROADER IMPACTS,0.930410577592206,"models that generate Deepfakes faster.
1090"
BROADER IMPACTS,0.9311064718162839,"• The authors should consider possible harms that could arise when the technology is
1091"
BROADER IMPACTS,0.9318023660403618,"being used as intended and functioning correctly, harms that could arise when the
1092"
BROADER IMPACTS,0.9324982602644398,"technology is being used as intended but gives incorrect results, and harms following
1093"
BROADER IMPACTS,0.9331941544885177,"from (intentional or unintentional) misuse of the technology.
1094"
BROADER IMPACTS,0.9338900487125957,"• If there are negative societal impacts, the authors could also discuss possible mitigation
1095"
BROADER IMPACTS,0.9345859429366736,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
1096"
BROADER IMPACTS,0.9352818371607515,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
1097"
BROADER IMPACTS,0.9359777313848295,"feedback over time, improving the efficiency and accessibility of ML).
1098"
SAFEGUARDS,0.9366736256089074,"11. Safeguards
1099"
SAFEGUARDS,0.9373695198329853,"Question: Does the paper describe safeguards that have been put in place for responsible
1100"
SAFEGUARDS,0.9380654140570633,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
1101"
SAFEGUARDS,0.9387613082811412,"image generators, or scraped datasets)?
1102"
SAFEGUARDS,0.9394572025052192,"Answer: [NA]
1103"
SAFEGUARDS,0.9401530967292971,"Justification: We release a causal discovery model from observational data which does not
1104"
SAFEGUARDS,0.940848990953375,"poses such risks.
1105"
SAFEGUARDS,0.941544885177453,"Guidelines:
1106"
SAFEGUARDS,0.942240779401531,"• The answer NA means that the paper poses no such risks.
1107"
SAFEGUARDS,0.942936673625609,"• Released models that have a high risk for misuse or dual-use should be released with
1108"
SAFEGUARDS,0.9436325678496869,"necessary safeguards to allow for controlled use of the model, for example by requiring
1109"
SAFEGUARDS,0.9443284620737648,"that users adhere to usage guidelines or restrictions to access the model or implementing
1110"
SAFEGUARDS,0.9450243562978428,"safety filters.
1111"
SAFEGUARDS,0.9457202505219207,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
1112"
SAFEGUARDS,0.9464161447459986,"should describe how they avoided releasing unsafe images.
1113"
SAFEGUARDS,0.9471120389700766,"• We recognize that providing effective safeguards is challenging, and many papers do
1114"
SAFEGUARDS,0.9478079331941545,"not require this, but we encourage authors to take this into account and make a best
1115"
SAFEGUARDS,0.9485038274182325,"faith effort.
1116"
LICENSES FOR EXISTING ASSETS,0.9491997216423104,"12. Licenses for existing assets
1117"
LICENSES FOR EXISTING ASSETS,0.9498956158663883,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1118"
LICENSES FOR EXISTING ASSETS,0.9505915100904663,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1119"
LICENSES FOR EXISTING ASSETS,0.9512874043145442,"properly respected?
1120"
LICENSES FOR EXISTING ASSETS,0.9519832985386222,"Answer: [Yes]
1121"
LICENSES FOR EXISTING ASSETS,0.9526791927627001,"Justification: We use our proprietary assets, as well as public assets available under MIT
1122"
LICENSES FOR EXISTING ASSETS,0.953375086986778,"license, which we correctly cite in our work.
1123"
LICENSES FOR EXISTING ASSETS,0.954070981210856,"Guidelines:
1124"
LICENSES FOR EXISTING ASSETS,0.9547668754349339,"• The answer NA means that the paper does not use existing assets.
1125"
LICENSES FOR EXISTING ASSETS,0.9554627696590118,"• The authors should cite the original paper that produced the code package or dataset.
1126"
LICENSES FOR EXISTING ASSETS,0.9561586638830898,"• The authors should state which version of the asset is used and, if possible, include a
1127"
LICENSES FOR EXISTING ASSETS,0.9568545581071677,"URL.
1128"
LICENSES FOR EXISTING ASSETS,0.9575504523312457,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1129"
LICENSES FOR EXISTING ASSETS,0.9582463465553236,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1130"
LICENSES FOR EXISTING ASSETS,0.9589422407794015,"service of that source should be provided.
1131"
LICENSES FOR EXISTING ASSETS,0.9596381350034795,"• If assets are released, the license, copyright information, and terms of use in the
1132"
LICENSES FOR EXISTING ASSETS,0.9603340292275574,"package should be provided. For popular datasets, paperswithcode.com/datasets
1133"
LICENSES FOR EXISTING ASSETS,0.9610299234516354,"has curated licenses for some datasets. Their licensing guide can help determine the
1134"
LICENSES FOR EXISTING ASSETS,0.9617258176757133,"license of a dataset.
1135"
LICENSES FOR EXISTING ASSETS,0.9624217118997912,"• For existing datasets that are re-packaged, both the original license and the license of
1136"
LICENSES FOR EXISTING ASSETS,0.9631176061238692,"the derived asset (if it has changed) should be provided.
1137"
LICENSES FOR EXISTING ASSETS,0.9638135003479471,"• If this information is not available online, the authors are encouraged to reach out to
1138"
LICENSES FOR EXISTING ASSETS,0.964509394572025,"the asset’s creators.
1139"
NEW ASSETS,0.965205288796103,"13. New Assets
1140"
NEW ASSETS,0.9659011830201809,"Question: Are new assets introduced in the paper well documented and is the documentation
1141"
NEW ASSETS,0.9665970772442589,"provided alongside the assets?
1142"
NEW ASSETS,0.9672929714683368,"Answer: [Yes]
1143"
NEW ASSETS,0.9679888656924147,"Justification: We release the code for AdaScore, submitted in the form of a zip file containing
1144"
NEW ASSETS,0.9686847599164927,"the necessary documentation for usage. Moreover, an extensive description of the data
1145"
NEW ASSETS,0.9693806541405706,"generation is provided in the paper, as well as a description of the method itself.
1146"
NEW ASSETS,0.9700765483646486,"Guidelines:
1147"
NEW ASSETS,0.9707724425887265,"• The answer NA means that the paper does not release new assets.
1148"
NEW ASSETS,0.9714683368128044,"• Researchers should communicate the details of the dataset/code/model as part of their
1149"
NEW ASSETS,0.9721642310368824,"submissions via structured templates. This includes details about training, license,
1150"
NEW ASSETS,0.9728601252609603,"limitations, etc.
1151"
NEW ASSETS,0.9735560194850382,"• The paper should discuss whether and how consent was obtained from people whose
1152"
NEW ASSETS,0.9742519137091162,"asset is used.
1153"
NEW ASSETS,0.9749478079331941,"• At submission time, remember to anonymize your assets (if applicable). You can either
1154"
NEW ASSETS,0.9756437021572721,"create an anonymized URL or include an anonymized zip file.
1155"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.97633959638135,"14. Crowdsourcing and Research with Human Subjects
1156"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9770354906054279,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1157"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.977731384829506,"include the full text of instructions given to participants and screenshots, if applicable, as
1158"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9784272790535838,"well as details about compensation (if any)?
1159"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9791231732776617,"Answer: [NA]
1160"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9798190675017397,"Justification: We do not work with human subjects or crowdsourcing.
1161"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9805149617258176,"Guidelines:
1162"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9812108559498957,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1163"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9819067501739736,"human subjects.
1164"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9826026443980515,"• Including this information in the supplemental material is fine, but if the main contribu-
1165"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9832985386221295,"tion of the paper involves human subjects, then as much detail as possible should be
1166"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9839944328462074,"included in the main paper.
1167"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846903270702854,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1168"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9853862212943633,"or other labor should be paid at least the minimum wage in the country of the data
1169"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9860821155184412,"collector.
1170"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9867780097425192,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1171"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9874739039665971,"Subjects
1172"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988169798190675,"Question: Does the paper describe potential risks incurred by study participants, whether
1173"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988865692414753,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1174"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9895615866388309,"approvals (or an equivalent approval/review based on the requirements of your country or
1175"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902574808629089,"institution) were obtained?
1176"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9909533750869868,"Answer: [NA]
1177"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916492693110647,"Justification: We do not work with human subjects or crowdsourcing.
1178"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923451635351427,"Guidelines:
1179"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9930410577592206,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1180"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9937369519832986,"human subjects.
1181"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9944328462073765,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1182"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9951287404314544,"may be required for any human subjects research. If you obtained IRB approval, you
1183"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9958246346555324,"should clearly state this in the paper.
1184"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9965205288796103,"• We recognize that the procedures for this may vary significantly between institutions
1185"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9972164231036882,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1186"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979123173277662,"guidelines for their institution.
1187"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9986082115518441,"• For initial submissions, do not include any information that would break anonymity (if
1188"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9993041057759221,"applicable), such as the institution conducting the review.
1189"
