Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0007507507507507507,"Recent empirical advances show that training deep models with large learning rate
often improves generalization performance. However, theoretical justiﬁcations on
the beneﬁts of large learning rate are highly limited, due to challenges in analysis. In
this paper, we consider using Gradient Descent (GD) with a large learning rate on a
homogeneous matrix factorization problem, i.e., minX,Y ∥A −XY ⊤∥2
F. We prove
a convergence theory for constant large learning rates well beyond 2/L, where L
is the largest eigenvalue of Hessian at the initialization. Moreover, we rigorously
establish an implicit bias of GD induced by such a large learning rate, termed
‘balancing’, meaning that magnitudes of X and Y at the limit of GD iterations
will be close even if their initialization is signiﬁcantly unbalanced. Numerical
experiments are provided to support our theory."
INTRODUCTION,0.0015015015015015015,"1
INTRODUCTION"
INTRODUCTION,0.0022522522522522522,"Training machine learning models such as deep neural networks involves optimizing highly nonconvex
functions. Empirical results indicate an intimate connection between training algorithms and the
performance of trained models (Le et al., 2011; Bottou et al., 2018; Zhang et al., 2021; Soydaner,
2020; Zhou et al., 2020). Especially for widely used ﬁrst-order training algorithms (e.g., GD and
SGD), the learning rate is of essential importance and has received extensive focus from researchers
(Smith, 2017; Jastrzebski et al., 2017; Smith, 2018; Gotmare et al., 2018; Liu et al., 2019; Li & Arora,
2019). A recent perspective is that large learning rates often lead to improved testing performance
compared to the counterpart trained with small learning rates (Smith & Topin, 2019; Yue et al., 2020).
Towards explaining the better performance, a common belief is that large learning rates encourage
the algorithm to search for ﬂat minima, which often generalize better and are more robust than sharp
ones (Seong et al., 2018; Lewkowycz et al., 2020)."
INTRODUCTION,0.003003003003003003,"Despite abundant empirical observations, theoretical understandings of the beneﬁts of large learning
rate are still limited for non-convex functions, partly due to challenges in analysis. For example, the
convergence (of GD or SGD) under large learning rate is not guaranteed. Even for globally smooth
functions, very few general results exist if the learning rate exceeds certain threshold (Kong & Tao,
2020). Besides, popular regularity assumptions such as global smoothness for simpliﬁed analyses are
often absent in homogeneous models, including commonly used ReLU neural networks."
INTRODUCTION,0.0037537537537537537,This paper theoretically studies the beneﬁts of large learning rate in a matrix factorization problem
INTRODUCTION,0.0045045045045045045,"min
X,Y
1
2"
INTRODUCTION,0.005255255255255256,"A −XY ⊤2"
INTRODUCTION,0.006006006006006006,"F ,
where A ∈Rn×n, X, Y ∈Rn×d.
(1)"
INTRODUCTION,0.006756756756756757,"We consider Gradient Descent (GD) for solving (1): at the k-th iteration, we have"
INTRODUCTION,0.0075075075075075074,"Xk+1 = Xk + h(A −XkY ⊤
k )Yk
and
Yk+1 = Yk + h(A⊤−YkX⊤
k )Xk,"
INTRODUCTION,0.008258258258258258,"where h is the learning rate. Despite its simple formula, problem (1) serves as an important foundation
of a variety of problems, including matrix sensing (Chen & Wainwright, 2015; Bhojanapalli et al.,
2016; Tu et al., 2016), matrix completion (Keshavan et al., 2010; Hardt, 2014), and linear neural
networks (Ji & Telgarsky, 2018; Gunasekar et al., 2018)."
INTRODUCTION,0.009009009009009009,Published as a conference paper at ICLR 2022
INTRODUCTION,0.00975975975975976,"Problem (1) possesses several intriguing properties. Firstly, the objective function is non-convex,
and critical points are either global minima or saddles (see e.g., Baldi & Hornik (1989); Li et al.
(2019b); Valavi et al. (2020a); Chen et al. (2018)). Secondly, problem (1) is homogeneous in X
and Y , meaning that rescaling X, Y to aX, a−1Y for any a ̸= 0 will not change the objective’s
value. This property is shared by commonly used ReLU neural networks. A direct consequence of
homogeneity is that global minima of (1) are non-isolated and can be unbounded. The curvatures at
these global minima are highly dependent on the magnitudes of X, Y . When X, Y have comparable
magnitudes, the largest eigenvalue of Hessian is small, and this corresponds to a ﬂat minimum; on
the contrary, unbalanced X and Y give a sharp minimum. Last but not the least, the homogeneity
impairs smoothness conditions of (1), rendering the gradient being not Lipschitz continuous unless
X, Y are bounded. See a formal discussion in Section 2."
INTRODUCTION,0.010510510510510511,"Existing approaches for solving (1) often uses explicit regularization (Ge et al., 2017; Tu et al., 2016;
Cabral et al., 2013; Li et al., 2019a), or inﬁnitesimal (or diminishing) learning rates for controlling
the magnitudes of X, Y (Du et al., 2018; Ye & Du, 2021). In this paper, we go beyond the scope of
aforementioned works, and analyze GD with a large learning rate for solving (1). In particular, we
allow the learning rate h to be as large as approximately 4/L (see more explanation in Section 2),
where L denotes the largest eigenvalue of Hessian at GD initialization. In connection to empirical
observations, we provide positive answers to the following two questions:"
INTRODUCTION,0.01126126126126126,"Does GD with large learning rate converge at least for some cases of (1)?
Does larger learning rate biases toward ﬂatter minima (i.e., X, Y with comparable magnitudes)?"
INTRODUCTION,0.012012012012012012,"We theoretically show the convergence of GD with large learning rate for the two situations n =
1, d ∈N+ or d = 1, n ∈N+ with isotropic A. We also observe a, perhaps surprising, “balancing
effect” for general matrix factorization (i.e., any d, n, and A), meaning that when h is sufﬁciently
large, the difference between X and Y shrinks signiﬁcantly at the convergence of GD compared to
its initial, even if the initial point is close to an unbalanced global minimum. In fact, with a proper
large learning rate h, ∥Xk −Yk∥2
F may decrease by an arbitrary factor at its limit. The following is a
simple example of our theory for n = 1 (i.e. scalar factorization), and more general results will be
presented later with a precise bound for h depending on the initial condition and A."
INTRODUCTION,0.012762762762762763,"Theorem 1.1 (Informal version of Thm.3.1 & 3.2). Given scalar A and initial condition X0, Y0 ∈
R1×d chosen almost everywhere, with learning rate h ≲4/L, GD converges to a global minimum
(X, Y ) satisfying ∥X∥2
F + ∥Y ∥2
F ≤
2
h. Consequently, its extent of balancing is quantiﬁed by
∥X −Y ∥2
F ≤2"
INTRODUCTION,0.013513513513513514,h −2A.
INTRODUCTION,0.014264264264264264,"We remark that having a learning rate h ≈4/L is far beyond the commonly analyzed regime in
optimization. Even for globally L-smooth objective, traditional theory requires h < 2/L for GD
convergence and h = 1/L is optimal for convex functions (Boyd et al., 2004), not to mention that
our problem (1) is never globally L-smooth due to homogeneity. Modiﬁed equation provides a tool
for probing intermediate learning rates (see Hairer et al. (2006, Chapter 9) for a general review, and
Kong & Tao (2020, Appendix A) for the speciﬁc setup of GD), but the learning rate here is too large
for modiﬁed equation to work (see Appendix C). In fact, besides blowing up, GD with large learning
rate may have a zoology of limiting behaviors (see e.g., Appendix B for convergence to periodic
orbits under our setup, and Kong & Tao (2020) for convergence to chaotic attractors)."
INTRODUCTION,0.015015015015015015,"Our analyses (of convergence and balancing) leverage various mathematical tools, including a
proper partition of state space and its dynamical transition (speciﬁcally invented for this problem),
stability theory of discrete time dynamical systems (Alligood et al., 1996), and geometric measure
theory (Federer, 2014)."
INTRODUCTION,0.015765765765765764,"The rest of the paper is organized as: Section 2 provides the background of studying (1) and discusses
related works; Section 3 presents convergence and balancing results for scalar factorization problems;
Section 4 generalizes the theory to rank-1 matrix approximation; Section 5 studies problem (1) with
arbitrary A and its arbitrary-rank approximation; Section 6 summarizes the paper and discusses
broadly related topics and future directions."
INTRODUCTION,0.016516516516516516,Published as a conference paper at ICLR 2022
BACKGROUND AND RELATED WORK,0.017267267267267267,"2
BACKGROUND AND RELATED WORK"
BACKGROUND AND RELATED WORK,0.018018018018018018,Notations. ∥v∥is ℓ2 norm of a column or row vector v. ∥M∥F is the Frobenius norm of a matrix M.
BACKGROUND AND RELATED WORK,0.01876876876876877,"Sharp and ﬂat minima in (1)
We discuss the curvatures at global minima of (1). To ease the
presentation, consider simpliﬁed versions of (1) with either n = 1 or d = 1. In this case, X and
Y become vectors and we denote them as x, y, respectively. We show the following proposition
characterizing the spectrum of Hessian at a global minimum.
Proposition 2.1. When n = 1, d ∈N+ or d = 1, n ∈N+ in (1), the largest eigenvalue of Hessian
at a global minimum (x, y) is ∥x∥2 + ∥y∥2 and the smallest eigenvalue is 0."
BACKGROUND AND RELATED WORK,0.01951951951951952,"Homogenity implies global minimizers of (1) are not isolated, which is consistent with the 0 eigen-
value. On the other hand, if the largest eigenvalue ∥x∥2 + ∥y∥2 is large (or small), then the curvature
at such a global minimum is sharp (or ﬂat), in the direction of the leading eigenvector. Meanwhile,
note this sharpness/ﬂatness is an indication of the balancedness between magnitudes of x, y at a
global minimum. To see this, singular value decomposition (SVD) yields that at a global minimum,
(x, y) satisﬁes
xy⊤2
F = σ2
max(A). Therefore, large ∥x∥2 + ∥y∥2 is obtained when |∥x∥−∥y∥| is
large, i.e., x and y magnitudes are unbalanced, and small ∥x∥2 + ∥y∥2 is obtained when balanced."
BACKGROUND AND RELATED WORK,0.02027027027027027,"Large learning rate
We study smoothness properties of (1) and demonstrate that our learning rate
is well beyond conventional optimization theory. We ﬁrst deﬁne the smoothness of a function.
Deﬁnition 2.2 (L-smooth). A function f ∈C1 deﬁned on RN is L-smooth if for all u1, u2 ∈RN,"
BACKGROUND AND RELATED WORK,0.021021021021021023,"∥∇f(u1) −∇f(u2)∥≤L∥u1 −u2∥.
(2)"
BACKGROUND AND RELATED WORK,0.02177177177177177,"If further f ∈C2, then ∇2f ⪯LI. Moreover, if we have (2) for u1, u2 ∈X ⊆RN, we call it locally
L-smooth."
BACKGROUND AND RELATED WORK,0.02252252252252252,"In traditional optimization (Nesterov, 2003; Polyak, 1987; Nesterov, 1983; Polyak, 1964; Beck &
Teboulle, 2009), most analyzed objective functions often satisfy (i) (some relaxed form of) convexity
or strong convexity, and (ii) L-smoothness. Choosing a step size h < 2/L guarantees the convergence
of GD to a minimum by the existing theory (reviewed in Appendix F.3). Our choice of learning rate
h ≈4/L (more precisely, 4/L0; see below) goes beyond the classical analyses."
BACKGROUND AND RELATED WORK,0.023273273273273273,"Besides, in our problem (1), the regularity is very different. Even simpliﬁed versions of (1), i.e., with
either n = 1 or d = 1, suffer from (i) non-convexity and (ii) unbounded eigenvalues of Hessian, i.e.,
no global smoothness (see Appendix F.2 for more details). As shown in Du et al. (2018); Ye & Du
(2021); Ma et al. (2021), decaying or inﬁnitesimal learning rate ensures that the GD trajectory stays in
a locally smooth region. However, the gap between the magnitudes of X, Y can only be maintained
in that case. We show, however, that larger learning rate can shrink this gap. More precisely, if initial
condition is well balanced, there is no need to use large learning rate; otherwise, we can use learning
rate as large as approximately 4/L, and within this range, larger h provides smaller gap between X
and Y at the limit (i.e. inﬁnitely many GD iterations)."
BACKGROUND AND RELATED WORK,0.024024024024024024,"Related work
Matrix factorization problems in various forms have been extensively studied in
the literature. The version of (1) is commonly known as the low-rank factorization, although here d
can arbitrary and we consider both d ≤rank(A) and d > rank(A) cases. Baldi & Hornik (1989);
Li et al. (2019b); Valavi et al. (2020b) provide landscape analysis of (1). Ge et al. (2017); Tu et al.
(2016) propose to penalize the Frobenius norm of
X⊤X −Y ⊤Y
2
F to mitigate the homogeneity
and establish global convergence guarantees of GD for solving (1). Cabral et al. (2013); Li et al.
(2019a) instead penalize individual Frobenius norms of ∥X∥2
F + ∥Y ∥2
F. We remark that penalizing
∥X∥2
F + ∥Y ∥2
F is closely related to nuclear norm regularization, since the variational formula for
nuclear norm ∥Z∥2
∗= minZ=XY ⊤∥X∥2
F + ∥Y ∥2
F. More recently, Liu et al. (2021) consider using
injected noise as regularization to GD and establish a global convergence (see also Zhou et al. (2019);
Liu et al. (2022). More speciﬁcally, by perturbing the GD iterate (Xk, Yk) with Gaussian noise, GD
will converge to a ﬂat global optimum. On the other hand, Du et al. (2018); Ye & Du (2021); Ma et al.
(2021) show that even without explicit regularization, when the learning rate of GD is inﬁnitesimal,"
BACKGROUND AND RELATED WORK,0.024774774774774775,Published as a conference paper at ICLR 2022
BACKGROUND AND RELATED WORK,0.025525525525525526,"i.e., GD approximating gradient ﬂow, X and Y maintain the gap in their magnitudes. Such an effect
is more broadly recognized as implicit bias of learning algorithms (Neyshabur et al., 2014; Gunasekar
et al., 2018; Soudry et al., 2018; Li et al., 2020; 2021). Built upon this implicit bias, Du et al. (2018)
further prove that GD with diminishing learning rates converges to a bounded global minimum of (1),
and this conclusion is recently extended to the case of a constant small learning rate (Ye & Du, 2021).
Our work goes beyond the scopes of these milestones and considers matrix factorization with much
larger learning rates."
BACKGROUND AND RELATED WORK,0.026276276276276277,"Additional results exist that demonstrate large learning rate can improve performance in various
learning problems. Most of them involve non-constant learn rates. Speciﬁcally, Li et al. (2019c)
consider a two-layer neural network setting, where using learning rate annealing (initially large,
followed by small ones) can improve classiﬁcation accuracy compared to training with small learning
rates. Nakkiran (2020) shows that the observation in Li et al. (2019c) even exists in convex problems.
Lewkowycz et al. (2020) study constant large learning rates, and demonstrate distinct algorithmic
behaviors of large and small learning rates, as well as empirically illustrate large learning rate yields
better testing performance on neural networks. Their analysis is built upon the neural tangent kernel
perspective (Jacot et al., 2018), with a focus on the kernel spectrum evolution under large learning
rate. Worth noting is, Kong & Tao (2020) also study constant large learning rates, and show that large
learning rate provides a mechanism for GD to escape local minima, alternative to noisy escapes due
to stochastic gradients."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.02702702702702703,"3
OVERPARAMETERIZED SCALAR FACTORIZATION"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.027777777777777776,"In order to provide intuition before directly studying the most general problem, we begin with a
simple special case, namely factorizing a scalar by two vectors. It corresponds to (1) with n = 1 and
d ∈N+, and this overparameterized problem is written as"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.028528528528528527,"min
x,y∈R1×d
1
2(µ −xy⊤)2,
(3)"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.02927927927927928,"where µ is assumed without loss of generality to be a positive scalar. Problem (3) can be viewed as
univariate regression using a linear two-layer neural network with the quadratic loss, which is studied
in Lewkowycz et al. (2020) with atomic data distribution. Yet our analysis in the sequel can be used
to study arbitrary univariate data distributions; see details in Section 6."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03003003003003003,"Although simpliﬁed, problem (3) is still nonconvex and exhibits the same homogeneity as (1). The
convergence of its large learning rate GD optimization was previously not understood, let alone
balancing. Many results that we will obtain for (3) will remain true for more general problems."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03078078078078078,"We ﬁrst prove that GD converges despite of > 2/L learning rate and for almost all initial conditions:
Theorem 3.1 (Convergence). Given (x⊤
0 , y⊤
0 ) ∈(Rd×Rd)\B where B is some Lebesgue measure-0
set, when the learning rate h satisﬁes"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03153153153153153,"h ≤min

4
∥x0∥2 + ∥y0∥2 + 4µ,
1
3µ 
,"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03228228228228228,GD converges to a global minimum.
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03303303303303303,"Theorem 3.1 says that choosing a constant learning rate depending on GD initialization guarantees
the convergence for almost every starting point in the whole space. This result is even stronger
than the already nontrivial convergence under small learning rate with high probability over random
initialization (Ye & Du, 2021). Furthermore, the upper bound on h is sufﬁciently large: on the one
hand, suppose GD initialization (x0, y0) is close to an unbalanced global minimum. By Proposi-
tion 2.1, we can check that the largest eigenvalue L(x0, y0) of Hessian ∇2f(x0, y0) is approximately
∥x0∥2 + ∥y0∥2. Consequently, our upper bound of h is almost 4/L, which is beyond 2/L (see
Section 2 for more details). On the other hand, we observe numerically that the 4/L upper bound
is actually very close to the stability limit of GD when initialized away from the origin (see more
details in Appendix E)."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.033783783783783786,"The convergence in Theorem 3.1 has an interesting searching-to-converging transition as depicted in
Figure 1, where we observe two phases. In Phase 1, large learning rate drives GD to search for ﬂat"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03453453453453453,Published as a conference paper at ICLR 2022
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03528528528528529,Figure 1: The dynamics of GD under different learning rate h
OVERPARAMETERIZED SCALAR FACTORIZATION,0.036036036036036036,"regions, escaping from the attraction of sharp minima. After some iterations, the algorithm enters the
vicinity of a global minimum with more balanced magnitudes in x, y. Then in Phase 2, GD converges
to the found balanced global minimum. We remark that the searching-to-converging transition also
appears in Lewkowycz et al. (2020). However, the algorithmic behaviors are not the same. In fact, in
our searching phase (phase 1), the objective function does not exhibit the blow-up phenomenon. In
our convergence phase (phase 2), the analysis relies on a detailed state space partition (see Line 192)
due to nonconvex nature of (3), while the analysis in Lewkowycz et al. (2020) is akin to monotone
convergence in a convex problem."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03678678678678678,"In comparison with the dynamics of small learning rate, we note that the searching phase (Phase 1) is
vital to the convergence analysis. Meanwhile, the searching phase induces a balancing effect of large
learning rate. The following theorem explicitly quantiﬁes the extent of balancing.
Theorem 3.2 (Balancing). Under the same initial condition and learning rate h as Theorem 3.1, GD
for (3) converges to a global minimizer (x, y) satisfying"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03753753753753754,∥x∥2 + ∥y∥2 ≤2 h.
OVERPARAMETERIZED SCALAR FACTORIZATION,0.038288288288288286,"Consequently, its extent of balancing is quantiﬁed by"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03903903903903904,∥x −y∥2 ≤2
OVERPARAMETERIZED SCALAR FACTORIZATION,0.03978978978978979,h −2µ.
OVERPARAMETERIZED SCALAR FACTORIZATION,0.04054054054054054,"One special case of Theorem 3.2 is the following theorem, which states that no matter how close to a
global minimum does GD start, if this minimum does not correspond to well-balanced norms, a large
learning rate will take the iteration to a more balanced limit. We also demonstrate a sharp shrinkage
in the distance between x and y.
Corollary 3.3 (From ‘unbalanced’ to ‘balanced’). For any δ ∈(0, µ), let the GD initialization satisfy"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.04129129129129129,"(x0, y0) ∈

(u, v) : |uv⊤−µ| < δ, ∥u∥2 + ∥v∥2 > 8µ
	
\B,"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.042042042042042045,"where B is some Lebesgue measure-0 set. When the learning rate h satisﬁes h =
4
∥x0∥2+∥y0∥2+4µ,
the extent of balancing at the limiting point (x, y) of GD obeys"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.04279279279279279,∥x −y∥2 < 1
OVERPARAMETERIZED SCALAR FACTORIZATION,0.04354354354354354,2∥x0 −y0∥2 + 2µ.
OVERPARAMETERIZED SCALAR FACTORIZATION,0.044294294294294295,"Both Theorem 3.2 and Corollary 3.3 suggest that larger learning rate yields better balancing effect, as
∥x −y∥2 at the limit of GD may decrease a lot and is controlled by the learning rate. We remark that
the balancing effect is a consequence of large learning rate, as small learning rate can only maintain
the difference in magnitudes of x, y (Du et al., 2018)."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.04504504504504504,"In addition, the actual balancing effect can be quite strong with ∥xk −yk∥2 decreasing to be almost 0
at its limit under a proper choice of large learning rate. Figure 2 illustrates an almost perfect balancing
case when h =
4
∥x0∥2+∥y0∥2+4 ≈0.0122 is chosen as the upper bound. The difference ∥xk −yk∥
decreases from approximately 17.9986 to 0.0154 at its limit. Additional experiments with various
learning rates and initializations can be found in Appendix A."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.0457957957957958,"Technical Overview
We sketch the main ideas behind Theorem 3.1, which lead to the balancing
effect in Theorem 3.2. Full proof is deferred to Appendix G."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.046546546546546545,Published as a conference paper at ICLR 2022
OVERPARAMETERIZED SCALAR FACTORIZATION,0.0472972972972973,"0
100
200
300
number of iterations 10-10 10-5 100 105"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.04804804804804805,"0
100
200
300
number of iterations 0 5 10 15 20"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.048798798798798795,"Figure 2: The objective function is (1 −xy⊤)2/2, where x⊤, y⊤∈R10. Highly unbalanced initial
condition is uniformly randomized, with the norms to be ∥x0∥= 18, ∥y0∥= 0.09."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.04954954954954955,"The convergence is proved by handling Phase 1 and 2 separately (see Fig.1). In Phase 1, we prove that
∥xk∥2 +∥yk∥2 has a decreasing trend as GD searches for ﬂat minimum. We show that ∥xk∥2 +∥yk∥2"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.0503003003003003,"may not be monotone, i.e., it either decreases every iteration or decreases every other iteration."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.05105105105105105,"In Phase 2, we carefully partition the state space and show GD at each partition will eventually enter
a monotone convergence region. Note that the partition is based on detailed understanding of the
dynamics and is highly nontrivial. Attentive readers may refer to Appendix G for more details. The
combination of Phase 1 & 2 is brieﬂy summarized as a proof ﬂow chart in Figure 3."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.0518018018018018,"Figure 3: Proof overview of Theorem 3.1. At the k-th iteration, we denote (xk, yk) as the iterate and
sk is deﬁned as xk+1y⊤
k+1 −µ = sk(xky⊤
k −µ)."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.052552552552552555,"4
RANK-1 APPROX. OF ISOTROPIC A (AN UNDER-PARAMETERIZED CASE)"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.0533033033033033,"Given insights from scalar factorization, we consider rank-1 factorization of an isotropic matrix A,
i.e., A = µIn×n with µ > 0, d = 1, and n ∈N+. The corresponding optimization problem is"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.05405405405405406,"min
x,y∈Rn×1
1
2"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.054804804804804805,"µIn×n −xy⊤2"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.05555555555555555,"F .
(4)"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.05630630630630631,"Although similar at an uncareful glance, Problems (4) and (3) are rather different unless n = d = 1.
First of all, Problem (4) is under-parameterized for n > 1, while (3) is overparameterized. More
importantly, we’ll show that, when (x, y) is a global minimum of (4), x, y must be aligned, i.e., x = ℓy
for some ℓ> 0. In the scalar factorization problem, however, no such alignment is required. As a
result, the set of global minima of (4) is an n-dimensional submanifold embedded in a 2n-dimensional
space, while in the scalar factorization problem the set of global minimum is a (2d −1)-dimensional"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.057057057057057055,Published as a conference paper at ICLR 2022
OVERPARAMETERIZED SCALAR FACTORIZATION,0.05780780780780781,"submanifold — one rank deﬁcient — in a 2d-dimensional space. We expect the convergence in (4) is
more complicated than that in (3), since searching in (4) is demanding."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.05855855855855856,"To prove the convergence of large learning rate GD for (4), our theory consists of two steps: (i) show
the convergence of the alignment between x and y (this is new); (ii) use that to prove the convergence
of the full iterates (i.e., x & y). Step (i) ﬁrst:
Theorem 4.1 (Alignment). Given (x0, y0) ∈(Rn × Rn)\B, where B is some Lebesgue measure-0
set, when learning rate h ≤min
n
4
∥x0∥2+∥y0∥2+4
√"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.05930930930930931,"7µ,
1
2
√ 7µ"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06006006006006006,"o
, the iterator (xk, yk) of GD at the"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.060810810810810814,"k-th iteration satisﬁes | cos(∠(xk, yk))| →1 as k →∞."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06156156156156156,"Proof sketch. A sufﬁcient condition for the convergence of | cos(∠(xk, yk))| is ∥xk∥2 ∥yk∥2 −
(x⊤
k yk)2 →0. To ease the presentation, let Uk = x⊤
k yk, Vk = x⊤
k xk, and Wk = y⊤
k yk. By the GD
update and some algebraic manipulation, we derive"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06231231231231231,"Vk+1Wk+1 −U 2
k+1 = rk · (VkWk −U 2
k),"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06306306306306306,"where rk = (1 −h(Vk + Wk) + h2(VkWk −µ2))2. When k is sufﬁciently large, we can show a
uniform upper bound on rk < 1−c for some constant c > 0. In this way, we deduce that VkWk −U 2
k
will exponentially decay and converge to 0. More details are provided in Appendix H."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06381381381381382,"Theorem 4.1 indicates that GD iterations will converge to the neighbourhood of {(x, y) : x =
ℓy, for some ℓ∈R\{0}}. This helps establish the global convergence as stated in Step (ii).
Theorem 4.2 (Convergence). Under the same initial conditions and learning rate h as Theorem 4.1,
GD for (4) converges to a global minimum."
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06456456456456457,"Similar to the over-parametrized scalar case, this convergence can also be split into two phases where
phase 1 motivates the balancing behavior with the decrease of ∥xk∥2 + ∥yk∥2, and phase 2 ensures
the convergence. The following balancing theorem is thus obtained.
Theorem 4.3 (Balancing). Under the same initial conditions and learning rate h as Theorem 4.1,
GD for (4) converges to a global minimizer that obeys"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06531531531531531,"∥x∥2 + ∥y∥2 ≤2 h,"
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06606606606606606,and its extent of balancing is quantiﬁed by
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06681681681681682,∥x −y∥2 ≤2
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06756756756756757,h −2µ.
OVERPARAMETERIZED SCALAR FACTORIZATION,0.06831831831831832,"This conclusion is the same as the one in Section 3. A quantitatively similar corollary like Corol-
lary 3.3 can also be obtained from the above theorem, namely, if x0 and y0 start from an unbalanced
point near a minimum, the limit will be a more balanced one."
GENERAL MATRIX FACTORIZATION,0.06906906906906907,"5
GENERAL MATRIX FACTORIZATION"
GENERAL MATRIX FACTORIZATION,0.06981981981981981,"In this section, we consider problem (1) with an arbitrary matrix A ∈Rn×n. We replicate the
problem formulation here for convenience,"
GENERAL MATRIX FACTORIZATION,0.07057057057057058,"min
X,Y ∈Rn×d
1
2"
GENERAL MATRIX FACTORIZATION,0.07132132132132132,"A −XY ⊤2"
GENERAL MATRIX FACTORIZATION,0.07207207207207207,"F .
(5)"
GENERAL MATRIX FACTORIZATION,0.07282282282282282,"Note this is the most general case with n, d ∈N+ and any square matrix A. Due to this generalization,
we no longer utilize the convergence analysis and instead, establish the balancing theory via stability
analysis of GD as a discrete time dynamical system."
GENERAL MATRIX FACTORIZATION,0.07357357357357357,"Let µ1 ≥µ2 ≥· · · ≥µn ≥0 be the singular values of A. Assume for technical convenience
∥A∥2
F = Pn
i=1 µ2
i being independent of d and n. We denote the singular value decomposition of A
as A = UDV ⊤, where U, D, V ∈Rn×n, U, V are orthogonal matrices and D is diagonal. Then we
establish the following balancing effect."
GENERAL MATRIX FACTORIZATION,0.07432432432432433,Published as a conference paper at ICLR 2022
GENERAL MATRIX FACTORIZATION,0.07507507507507508,"Theorem 5.1. Given almost all the initial conditions, for any learning rate h such that GD for (5)
converges to a point (X, Y ), there exists c = c(n, d) > c0 with constant c0 > 0 independent of h, n,
and d, such that (X, Y ) satisﬁes"
GENERAL MATRIX FACTORIZATION,0.07582582582582582,"c(∥X∥2
F + ∥Y ∥2
F) < 2 h,"
GENERAL MATRIX FACTORIZATION,0.07657657657657657,"and the extent of balancing is quantiﬁed by
X −(UV ⊤)Y
2
F <
2
ch −2 Pmin{d,n}
i=1
µi, which means"
GENERAL MATRIX FACTORIZATION,0.07732732732732733,"∥X∥F −∥Y ∥F
2 < 2 ch −2"
GENERAL MATRIX FACTORIZATION,0.07807807807807808,"v
u
u
t"
GENERAL MATRIX FACTORIZATION,0.07882882882882883,"min{d,n}
X"
GENERAL MATRIX FACTORIZATION,0.07957957957957958,"i=1
µ2
i ."
GENERAL MATRIX FACTORIZATION,0.08033033033033032,"In particular, when d = 1, i.e., rank-1 factorization of an arbitrary A, the constant c equals 1."
GENERAL MATRIX FACTORIZATION,0.08108108108108109,"We observe that the extent of balancing can be quantiﬁed under some rotation of Y . This is necessary,
since for factorizing a general matrix A (which can be asymmetric), at a global minimum, X, Y may
only align after a rotation (which is however ﬁxed by A, independent of initial or ﬁnal conditions).
Figure 4 illustrates an example of the balancing effect under different learning rates. Evidently,
larger learning rate leads to a more balanced global minimizer. Additional experiments with various
dimensions, learning rates, and initializations can be found in Appendix A; a similar balancing effect
is also shown for additional problems including matrix sensing and matrix completion there."
GENERAL MATRIX FACTORIZATION,0.08183183183183183,"0
100
200
number of iterations 10-10 100"
GENERAL MATRIX FACTORIZATION,0.08258258258258258,"0
100
200
number of iterations 0 5 10 F F"
GENERAL MATRIX FACTORIZATION,0.08333333333333333,"0
100
200
number of iterations 10-10 100"
GENERAL MATRIX FACTORIZATION,0.08408408408408409,"0
100
200
number of iterations 0 5 10 F F"
GENERAL MATRIX FACTORIZATION,0.08483483483483484,"Figure 4: Balancing effect of general matrix factorization. We independently generate elements in
A ∈R6×6 from a Gaussian distribution. We choose X, Y ∈R6×100 and randomly pick a pair of
initial point (X0, Y0) with ∥X0∥F = 1 and ∥Y0∥F = 9."
GENERAL MATRIX FACTORIZATION,0.08558558558558559,"Different from previous sections, Theorem 5.1 builds on stability analysis by viewing GD as a
dynamical system in discrete time. More precisely, the proof of Theorem 5.1 (see Appendix I for
details) consists of two parts: (i) the establishment of an easier but equivalent problem via the rotation
of X and Y , (ii) stability analysis of the equivalent problem."
GENERAL MATRIX FACTORIZATION,0.08633633633633633,"For (i), by singular value decomposition (SVD), A = UDV ⊤, where U, D, V ∈Rn×n, U and V are
orthogonal matrices, and D is a non-negative diagonal matrix. Let Xk = URk, Yk = V Sk. Then

Xk+1 = Xk + h(A −XkY ⊤
k )Yk
Yk+1 = Yk + h(A −XkY ⊤
k )⊤Xk"
GENERAL MATRIX FACTORIZATION,0.08708708708708708,"⇔

URk+1 = URk + h(UDV ⊤−URkS⊤
k V ⊤)V Sk
V Sk+1 = V Sk + h(UDV ⊤−URkS⊤
k V ⊤)⊤URk"
GENERAL MATRIX FACTORIZATION,0.08783783783783784,"⇔

Rk+1 = Rk + h(D −RkS⊤
k )Sk
Sk+1 = Sk + h(D −RkS⊤
k )⊤Rk
."
GENERAL MATRIX FACTORIZATION,0.08858858858858859,"Therefore, GD for problem (5) is equivalent to GD for the following problem"
GENERAL MATRIX FACTORIZATION,0.08933933933933934,"min
R,S∈Rn×d
1
2"
GENERAL MATRIX FACTORIZATION,0.09009009009009009,"D −RS⊤2 F"
GENERAL MATRIX FACTORIZATION,0.09084084084084085,and it thus sufﬁces to work with diagonal non-negative A.
GENERAL MATRIX FACTORIZATION,0.0915915915915916,"For (ii), here is a brief description of the idea of stability analysis: consider each iteration of GD as a
mapping ψ from uk ∼(Xk, YK) to uk+1 ∼(Xk+1, Yk+1), where matrices X and Y are ﬂattened"
GENERAL MATRIX FACTORIZATION,0.09234234234234234,Published as a conference paper at ICLR 2022
GENERAL MATRIX FACTORIZATION,0.09309309309309309,"and concatenated into a vector so that ψ is a closed map on vector space R2dn. GD iteration is thus a
discrete time dynamical system on state space R2dn given by"
GENERAL MATRIX FACTORIZATION,0.09384384384384384,"uk+1 = ψ(uk) = uk −h∇f(uk),"
GENERAL MATRIX FACTORIZATION,0.0945945945945946,where f is the objective function f(uk) = 1
GENERAL MATRIX FACTORIZATION,0.09534534534534535,"2
A −XkY ⊤
k
2
F, and gradient returns a vector that
collects all component-wise partial derivatives."
GENERAL MATRIX FACTORIZATION,0.0960960960960961,"It’s easy to see that any stationary point of f, denoted by u∗, is a ﬁxed point of ψ, i.e., u∗= ψ(u∗).
What ﬁxed point will the iterations of ψ converge to? For this, the following notions are helpful:
Proposition 5.2. Consider a ﬁxed point u∗of ψ. If all the eigenvalues of Jacobian matrix ∇ψ(u∗)
are of complex modulus less than 1, it is a stable ﬁxed point.
Proposition 5.3. Consider a ﬁxed point u∗of ψ. If at least one eigenvalue of Jacobian matrix
∇ψ(u∗) is of complex modulus greater than 1, it is an unstable ﬁxed point."
GENERAL MATRIX FACTORIZATION,0.09684684684684684,"Roughly put, the stable set of an unstable ﬁxed point is of negligible size when compared to that
of a stable ﬁxed point, and thus what GD converges to is a stable ﬁxed point for almost all initial
conditions (Alligood et al., 1996). Thus, we investigate the stability of each global minimum of f
(each saddle of f is an unstable ﬁxed point of ψ and thus is irrelevant). By a detailed evaluation of
∇ψ’s eigenvalues (Appendix I), we see that a global minimum (X, Y ) of f corresponds to a stable"
GENERAL MATRIX FACTORIZATION,0.09759759759759759,"ﬁxed point of GD iteration if
1 −ch

∥X∥2
F + ∥Y ∥2
F
 < 1, i.e., it is balanced as in Thm. 5.1."
CONCLUSION AND DISCUSSION,0.09834834834834835,"6
CONCLUSION AND DISCUSSION"
CONCLUSION AND DISCUSSION,0.0990990990990991,"In this paper, we demonstrate an implicit regularization effect of large learning rate on the homo-
geneous matrix factorization problem solved by GD. More precisely, a phenomenon termed as
“balancing” is theoretically illustrated, which says the difference between the two factors X and Y
may decrease signiﬁcantly at the limit of GD, and the extent of balancing can increase as learning
rate increases. In addition, we provide theoretical analysis of the convergence of GD to the global
minimum, and this is with large learning rate that can exceed the typical limit of 2/L, where L is the
largest eigenvalue of Hessian at GD initialization."
CONCLUSION AND DISCUSSION,0.09984984984984985,"For the matrix factorization problem analyzed here, large learning rate avoids bad regularities induced
by the homogeneity between X and Y . We feel it is possible that such balancing behavior can also be
seen in problems with similar homogeneous properties, for example, in tensor decomposition (Kolda
& Bader, 2009), matrix completion (Keshavan et al., 2010; Hardt, 2014), generalized phase retrieval
(Candes et al., 2015; Sun et al., 2018), and neural networks with homogeneous activation functions
(e.g., ReLU). Besides the balancing effect, the convergence analysis under large learning rate may be
transplanted to other non-convex problems and help discover more implicit regularization effects."
CONCLUSION AND DISCUSSION,0.1006006006006006,"In addition, factorization problems studied here are closely related to two-layer linear neural networks.
For example, one-dimensional regression via a two-layer linear neural network can be formulated as
the scalar factorization problem (3): Suppose we have a collection of data points (xi, yi) ∈R × R
for i = 1, . . . , n. We aim to train a linear neural network y = (u⊤v)x with u, v ∈Rd for ﬁtting the
data. We optimize u, v by minimizing the quadratic loss,"
CONCLUSION AND DISCUSSION,0.10135135135135136,"(u∗, v∗) ∈arg min
u,v
1
n n
X i=1"
CONCLUSION AND DISCUSSION,0.1021021021021021," 
yi −(u⊤v)xi
2 = arg min
u,v
1
n"
CONCLUSION AND DISCUSSION,0.10285285285285285,"Pn
i=1 xiyi
Pn
i=1 x2
i
−u⊤v
2
.
(6)"
CONCLUSION AND DISCUSSION,0.1036036036036036,"As can be seen, taking µ ="
CONCLUSION AND DISCUSSION,0.10435435435435435,"Pn
i=1 xiyi
Pn
i=1 x2
i
recovers (3). In this regard, our theory indicates that training
of u, v by GD with large learning rate automatically balances u, v, and the obtained minimum is ﬂat.
This may provide some initial understanding of the improved performance brought by large learning
rates in practice. Note that (6) generalizes to arbitrary data distribution of training a two-layer linear
network with atomic data (i.e., x = 1 and y = 0) in Lewkowycz et al. (2020)."
CONCLUSION AND DISCUSSION,0.10510510510510511,"It is important to clarify, however, that there is a substantial gap between this demonstration and
extensions to general neural networks, including deep linear and nonlinear networks. Although we
suspect that large learning rate leads to similar balancing effect of weight matrices in the network,
rigorous theoretical analysis is left as a future direction."
CONCLUSION AND DISCUSSION,0.10585585585585586,Published as a conference paper at ICLR 2022
CONCLUSION AND DISCUSSION,0.1066066066066066,ACKNOWLEDGMENTS
CONCLUSION AND DISCUSSION,0.10735735735735735,"We thank anonymous reviewers and area chair for suggestions that improved the quality of this
paper. The authors are grateful for partial supports from NSF DMS-1847802 (YW and MT) and
ECCS-1936776 (MT)."
REFERENCES,0.10810810810810811,REFERENCES
REFERENCES,0.10885885885885886,"Kathleen T Alligood, Tim D Sauer, and James A Yorke. Chaos: an introduction to dynamical systems.
Springer, 1996."
REFERENCES,0.10960960960960961,"Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural networks, 2(1):53–58, 1989."
REFERENCES,0.11036036036036036,"Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM journal on imaging sciences, 2(1):183–202, 2009."
REFERENCES,0.1111111111111111,"Srinadh Bhojanapalli, Anastasios Kyrillidis, and Sujay Sanghavi. Dropping convexity for faster
semi-deﬁnite optimization. In Conference on Learning Theory, pp. 530–582. PMLR, 2016."
REFERENCES,0.11186186186186187,"L´eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223–311, 2018."
REFERENCES,0.11261261261261261,"Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge
university press, 2004."
REFERENCES,0.11336336336336336,"Ricardo Cabral, Fernando De la Torre, Jo˜ao P Costeira, and Alexandre Bernardino. Unifying nuclear
norm and bilinear factorization approaches for low-rank matrix decomposition. In Proceedings of
the IEEE International Conference on Computer Vision, pp. 2488–2495, 2013."
REFERENCES,0.11411411411411411,"Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger ﬂow:
Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985–2007, 2015."
REFERENCES,0.11486486486486487,"Yudong Chen and Martin J Wainwright. Fast low-rank estimation by projected gradient descent:
General statistical and algorithmic guarantees. arXiv preprint arXiv:1509.03025, 2015."
REFERENCES,0.11561561561561562,"Zhehui Chen, Xingguo Li, Lin F Yang, Jarvis Haupt, and Tuo Zhao.
On landscape of la-
grangian functions and stochastic search for constrained nonconvex optimization. arXiv preprint
arXiv:1806.05151, 2018."
REFERENCES,0.11636636636636637,"Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous
models: Layers are automatically balanced. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018."
REFERENCES,0.11711711711711711,"Herbert Federer. Geometric measure theory. Springer, 2014."
REFERENCES,0.11786786786786786,"Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A
uniﬁed geometric analysis. In International Conference on Machine Learning, pp. 1233–1242.
PMLR, 2017."
REFERENCES,0.11861861861861862,"Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. A closer look
at deep learning heuristics: Learning rate restarts, warmup and distillation.
arXiv preprint
arXiv:1810.13243, 2018."
REFERENCES,0.11936936936936937,"Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro.
Implicit regularization in matrix factorization. In 2018 Information Theory and Applications
Workshop (ITA), pp. 1–10. IEEE, 2018."
REFERENCES,0.12012012012012012,"Ernst Hairer, Marlis Hochbruck, Arieh Iserles, and Christian Lubich. Geometric numerical integration.
Oberwolfach Reports, 3(1):805–882, 2006."
REFERENCES,0.12087087087087087,Published as a conference paper at ICLR 2022
REFERENCES,0.12162162162162163,"Moritz Hardt. Understanding alternating minimization for matrix completion. In 2014 IEEE 55th
Annual Symposium on Foundations of Computer Science, pp. 651–660. IEEE, 2014."
REFERENCES,0.12237237237237238,"Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018."
REFERENCES,0.12312312312312312,"Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio,
and Amos Storkey. Three factors inﬂuencing minima in sgd. arXiv preprint arXiv:1711.04623,
2017."
REFERENCES,0.12387387387387387,"Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv
preprint arXiv:1810.02032, 2018."
REFERENCES,0.12462462462462462,"Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few
entries. IEEE transactions on information theory, 56(6):2980–2998, 2010."
REFERENCES,0.12537537537537538,"Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):
455–500, 2009."
REFERENCES,0.12612612612612611,"Lingkai Kong and Molei Tao. Stochasticity of deterministic gradient descent: Large learning rate for
multiscale objective function. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 2625–2638. Curran
Associates, Inc., 2020."
REFERENCES,0.12687687687687688,"Quoc V Le, Jiquan Ngiam, Adam Coates, Ahbik Lahiri, Bobby Prochnow, and Andrew Y Ng. On
optimization methods for deep learning. In ICML, 2011."
REFERENCES,0.12762762762762764,"Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large
learning rate phase of deep learning: the catapult mechanism. 2020."
REFERENCES,0.12837837837837837,"Qiuwei Li, Zhihui Zhu, and Gongguo Tang. The non-convex geometry of low-rank matrix optimiza-
tion. Information and Inference: A Journal of the IMA, 8(1):51–96, 2019a."
REFERENCES,0.12912912912912913,"Xingguo Li, Junwei Lu, Raman Arora, Jarvis Haupt, Han Liu, Zhaoran Wang, and Tuo Zhao.
Symmetry, saddle points, and global optimization landscape of nonconvex matrix factorization.
IEEE Transactions on Information Theory, 65(6):3489–3514, 2019b."
REFERENCES,0.12987987987987987,"Yan Li, Ethan X.Fang, Huan Xu, and Tuo Zhao. Implicit bias of gradient descent based adversarial
training on separable data. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=HkgTTh4FDH."
REFERENCES,0.13063063063063063,"Yan Li, Caleb Ju, Ethan X Fang, and Tuo Zhao. Implicit regularization of bregman proximal point
algorithm and mirror descent on separable data. arXiv preprint arXiv:2108.06808, 2021."
REFERENCES,0.1313813813813814,"Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. arXiv preprint arXiv:1907.04595, 2019c."
REFERENCES,0.13213213213213212,"Zhiyuan Li and Sanjeev Arora. An exponential learning rate schedule for deep learning. arXiv
preprint arXiv:1910.07454, 2019."
REFERENCES,0.13288288288288289,"Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265,
2019."
REFERENCES,0.13363363363363365,"Tianyi Liu, Yan Li, Song Wei, Enlu Zhou, and Tuo Zhao. Noisy gradient descent converges to ﬂat
minima for nonconvex matrix factorization. 2021."
REFERENCES,0.13438438438438438,"Tianyi Liu, Yan Li, Enlu Zhou, and Tuo Zhao. Noise regularizes over-parameterized rank one matrix
recovery, provably. arXiv preprint arXiv:2202.03535, 2022."
REFERENCES,0.13513513513513514,"Cong Ma, Yuanxin Li, and Yuejie Chi. Beyond procrustes: Balancing-free gradient descent for
asymmetric low-rank matrix sensing. IEEE Transactions on Signal Processing, 69:867–877, 2021."
REFERENCES,0.13588588588588588,"Preetum Nakkiran. Learning rate annealing can provably help generalization, even for convex
problems. 2020."
REFERENCES,0.13663663663663664,Published as a conference paper at ICLR 2022
REFERENCES,0.1373873873873874,"Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003."
REFERENCES,0.13813813813813813,"Yurii E Nesterov. A method for solving the convex programming problem with convergence rate o
(1/kˆ 2). In Dokl. akad. nauk Sssr, volume 269, pp. 543–547, 1983."
REFERENCES,0.1388888888888889,"Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014."
REFERENCES,0.13963963963963963,"Boris T Polyak. Some methods of speeding up the convergence of iteration methods. Ussr computa-
tional mathematics and mathematical physics, 4(5):1–17, 1964."
REFERENCES,0.1403903903903904,"Boris T Polyak. Introduction to optimization. optimization software. Inc., Publications Division,
New York, 1, 1987."
REFERENCES,0.14114114114114115,"Sihyeon Seong, Yegang Lee, Youngwook Kee, Dongyoon Han, and Junmo Kim. Towards ﬂatter loss
surface via nonmonotonic learning rate scheduling. In UAI, pp. 1020–1030, 2018."
REFERENCES,0.14189189189189189,"Leslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter conference
on applications of computer vision (WACV), pp. 464–472. IEEE, 2017."
REFERENCES,0.14264264264264265,"Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning rate,
batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018."
REFERENCES,0.14339339339339338,"Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using
large learning rates. In Artiﬁcial Intelligence and Machine Learning for Multi-Domain Operations
Applications, volume 11006, pp. 1100612. International Society for Optics and Photonics, 2019."
REFERENCES,0.14414414414414414,"Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822–2878, 2018."
REFERENCES,0.1448948948948949,"Derya Soydaner. A comparison of optimization algorithms for deep learning. International Journal
of Pattern Recognition and Artiﬁcial Intelligence, 34(13):2052013, 2020."
REFERENCES,0.14564564564564564,"Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. Foundations of
Computational Mathematics, 18(5):1131–1198, 2018."
REFERENCES,0.1463963963963964,"Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Ben Recht. Low-rank
solutions of linear matrix equations via procrustes ﬂow. In International Conference on Machine
Learning, pp. 964–973. PMLR, 2016."
REFERENCES,0.14714714714714713,"Hossein Valavi, Sulin Liu, and Peter Ramadge. Revisiting the landscape of matrix factorization.
In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International
Conference on Artiﬁcial Intelligence and Statistics, volume 108 of Proceedings of Machine
Learning Research, pp. 1629–1638. PMLR, 26–28 Aug 2020a."
REFERENCES,0.1478978978978979,"Hossein Valavi, Sulin Liu, and Peter J Ramadge. The landscape of matrix factorization revisited.
arXiv preprint arXiv:2002.12795, 2020b."
REFERENCES,0.14864864864864866,"Tian Ye and Simon S Du. Global convergence of gradient descent for asymmetric low-rank matrix
factorization. arXiv preprint arXiv:2106.14289, 2021."
REFERENCES,0.1493993993993994,"Xubo Yue, Maher Nouiehed, and Raed Al Kontar. Salr: Sharpness-aware learning rates for improved
generalization. arXiv preprint arXiv:2011.05348, 2020."
REFERENCES,0.15015015015015015,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–115,
2021."
REFERENCES,0.15090090090090091,"Mo Zhou, Tianyi Liu, Yan Li, Dachao Lin, Enlu Zhou, and Tuo Zhao. Toward understanding the
importance of noise in training neural networks. In International Conference on Machine Learning,
pp. 7594–7602. PMLR, 2019."
REFERENCES,0.15165165165165165,"Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Hoi, et al. Towards theoretically under-
standing why sgd generalizes better than adam in deep learning. arXiv preprint arXiv:2010.05627,
2020."
REFERENCES,0.1524024024024024,Published as a conference paper at ICLR 2022
REFERENCES,0.15315315315315314,"Supplementary Materials for “Large Learning Rate Tames
Homogeneity: Convergence and Balancing Effect”"
REFERENCES,0.1539039039039039,"A
ADDITIONAL EXPERIMENTS"
REFERENCES,0.15465465465465467,"A.1
MORE RESULTS FOR MATRIX FACTORIZATION"
REFERENCES,0.1554054054054054,"In this section, we present more experiments with different choices of n, d, various initializations
and scalings, and a broader range of learning rates. All these experiments verify our claim on the
balancing effect of large learning rate, i.e., the shrinkage of the gap between the magnitudes of X
and Y exists for general matrix factorization problems, i.e., for any choice of n, d ∈N+."
REFERENCES,0.15615615615615616,"We ﬁrst provide examples of scalar factorization in Figure 5, 6, and 7 to numerically justify our
theory in Section 3. In the three ﬁgures, the initial conditions randomly generated, respectively with
(∥x0∥, ∥y0∥) = (9, 1), (∥x0∥, ∥y0∥) = (19, 1), and (∥x0∥, ∥y0∥) = (99, 1); the learning rates are
chosen within the range of Theorem 3.1 from large to small as h0, 6"
REFERENCES,0.1569069069069069,"7h0, 5"
REFERENCES,0.15765765765765766,"7h0, 4"
REFERENCES,0.15840840840840842,"7h0, 3"
REFERENCES,0.15915915915915915,"7h0, 2"
REFERENCES,0.15990990990990991,"7h0 for the
1st-6th columns respectively where h0 = 4/(∥x0∥2 + ∥y0∥2 + 8). The learning rates of the left three
columns are larger than 2/L (L is the local Lipschitz constant of gradient near the initial condition),
where we can see the decrease in the gap between ∥xk∥and ∥yk∥and larger learning rate leads to
smaller gap; the right three correspond to h < 2/L where there are almost no changes in ∥xk∥and
∥yk∥as k increases. Moreover, the loss does not decrease monotonically at the beginning of the
iterations for all the h > 2/L cases while in the later iterations GD shows monotone convergence; for
the right three columns (h < 2/L cases), we can see monotone decrease of the loss. This validates
our two-phase pattern of convergence (see e.g. Figure 1 and Section 3 for detailed explanation)."
REFERENCES,0.16066066066066065,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.1614114114114114,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.16216216216216217,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.1629129129129129,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.16366366366366367,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.16441441441441443,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.16516516516516516,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.16591591591591592,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.16666666666666666,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.16741741741741742,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.16816816816816818,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.16891891891891891,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.16966966966966968,"Figure 5: Scalar factorization with ∥x0∥= 9, ∥y0∥= 1. The x−axis represents the number of
iterations k; the y−axis represents the value for the norm of xk and yk in the ﬁrst row, and the value
for loss in the second row; the learning rate h for each column is the same."
REFERENCES,0.1704204204204204,"Then we experiment with the general matrix factorization as a supplement of our theory in Sec-
tion 5 where we only rigorously prove the balancing effect given the convergence of GD. In the
following examples, we show that there indeed exist large learning rates that trigger the shrinkage
of the gap between ∥Xk∥F and ∥Yk∥F and at the same time guarantee the convergence of GD. Fig-
ure 8, 9, and 10 correspond to the over-parameterized version of matrix factorization problem (5)
with A ∈R6×6 (asymmetric, generated by i.i.d. Gaussian) and X, Y ∈R6×100. The initial con-
ditions are randomly generated with (∥X0∥F , ∥Y0∥F) = (9, 1), (∥X0∥F , ∥Y0∥F) = (19, 1), and"
REFERENCES,0.17117117117117117,Published as a conference paper at ICLR 2022
REFERENCES,0.17192192192192193,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.17267267267267267,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.17342342342342343,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.17417417417417416,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.17492492492492492,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.17567567567567569,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.17642642642642642,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.17717717717717718,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.17792792792792791,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.17867867867867868,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.17942942942942944,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.18018018018018017,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.18093093093093093,"Figure 6: Scalar factorization with ∥x0∥= 19, ∥y0∥= 1. The x−axis represents the number of
iterations k; the y−axis represents the value for the norm of xk and yk in the ﬁrst row, and the value
for loss in the second row; the learning rate h for each column is the same."
REFERENCES,0.1816816816816817,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.18243243243243243,"0
50
100
10-15 10-10 10-5"
REFERENCES,0.1831831831831832,"100
105"
REFERENCES,0.18393393393393392,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.18468468468468469,"0
50
100
10-15 10-10 10-5"
REFERENCES,0.18543543543543545,"100
105"
REFERENCES,0.18618618618618618,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.18693693693693694,"0
50
100
10-15 10-10 10-5"
REFERENCES,0.18768768768768768,"100
105"
REFERENCES,0.18843843843843844,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.1891891891891892,"0
50
100
10-15 10-10 10-5"
REFERENCES,0.18993993993993993,"100
105"
REFERENCES,0.1906906906906907,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.19144144144144143,"0
50
100
10-15 10-10 10-5"
REFERENCES,0.1921921921921922,"100
105"
REFERENCES,0.19294294294294295,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.19369369369369369,"0
50
100
10-15 10-10 10-5"
REFERENCES,0.19444444444444445,"100
105"
REFERENCES,0.19519519519519518,"Figure 7: Scalar factorization with ∥x0∥= 99, ∥y0∥= 1. The x−axis represents the number of
iterations k; the y−axis represents the value for the norm of xk and yk in the ﬁrst row, and the value
for loss in the second row; the learning rate h for each column is the same."
REFERENCES,0.19594594594594594,"(∥X0∥F , ∥Y0∥F) = (99, 1) respectively. Similarly, the learning rates are chosen from large to small
as h0, 6"
REFERENCES,0.1966966966966967,"7h0, 5"
REFERENCES,0.19744744744744744,"7h0, 4"
REFERENCES,0.1981981981981982,"7h0, 3"
REFERENCES,0.19894894894894896,"7h0, 2"
REFERENCES,0.1996996996996997,7h0 for the 1st-6th columns respectively where 6
REFERENCES,0.20045045045045046,"7h0 (the 2nd column) is
picked near the stability limit. We also similarly provide two examples of the under-parameterized
version in Figure 11 and 12. The two examples correspond to problem (5) with A ∈R100×100
(asymmetric, similarly generated as the previous one) and X, Y ∈R100×3. Here we use a shifted
loss which is to subtract the global minimum error."
REFERENCES,0.2012012012012012,"As is shown in Figure 8, 9, 10, 11, and 12, we can observe the similar phenomenon as the scalar
case, in the sense that (1) larger learning rate gives a smaller the gap between ∥Xk∥F and ∥Yk∥F in
the limit, except for the overly large h that causes GD to diverge; (2) when the learning rate becomes
sufﬁciently big, a two-phase pattern of convergence appears, which does not manifest in traditional
optimization analysis and thus indicates that the learning rate is already larger than that permitted by
traditional theory, and yet one still has convergence."
REFERENCES,0.20195195195195195,Published as a conference paper at ICLR 2022
REFERENCES,0.20270270270270271,"More precisely, the 2nd-4th columns are the large learning rate cases, where one observes a shrinkage
of the unbalancedness and non-monotonicity of the loss at the beginning, while the right two columns
corresponds to the small learning rates for which ∥Xk∥F and ∥Yk∥F barely change as k increases,
and the decrease of loss is monotone. These are all evidence of the consistency between the most
general case of matrix factorization in Section 5 and the special cases in Section 3 and 4."
REFERENCES,0.20345345345345345,"0
5
10
0 1 2 3"
REFERENCES,0.2042042042042042,"4
10198 F F"
REFERENCES,0.20495495495495494,"0
5
10
100 10200"
REFERENCES,0.2057057057057057,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.20645645645645647,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.2072072072072072,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.20795795795795796,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.2087087087087087,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.20945945945945946,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.21021021021021022,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.21096096096096095,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.21171171171171171,"0
50
100
0 2 4 6 8 10"
REFERENCES,0.21246246246246248,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.2132132132132132,"Figure 8: General over-parameterized matrix factorization with ∥X0∥F = 9 ∥Y0∥F = 1. The x−axis
represents the number of iterations k; the y−axis represents the value for the norm of Xk and Yk in
the ﬁrst row, and the value for loss in the second row; the learning rate h for each column is the same."
REFERENCES,0.21396396396396397,"0
5
10
0 5"
REFERENCES,0.2147147147147147,"10
10153 F F"
REFERENCES,0.21546546546546547,"0
5
10
100 10100 10200"
REFERENCES,0.21621621621621623,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.21696696696696696,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.21771771771771772,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.21846846846846846,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.21921921921921922,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.21996996996996998,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.22072072072072071,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.22147147147147148,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.2222222222222222,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.22297297297297297,"0
50
100
10-15 10-10 10-5 100 105"
REFERENCES,0.22372372372372373,"Figure 9: General over-parameterized matrix factorization with ∥X0∥F = 19 ∥Y0∥F = 1. The
x−axis represents the number of iterations k; the y−axis represents the value for the norm of Xk
and Yk in the ﬁrst row, and the value for loss in the second row; the learning rate h for each column
is the same."
REFERENCES,0.22447447447447447,"A.2
MATRIX SENSING AND MATRIX COMPLETION"
REFERENCES,0.22522522522522523,"As we demonstrated, the balancing effect is a nontrivial implicit bias created by large learning rate in
GD, and this was rigorously established for the problem of matrix factorization. Matrix factorization
already corresponds to a class of important problems as we consider arbitrary n and d; however, we
feel balancing is an effect even more general, and thus we now demonstrate it empirically on two
related additional problems, namely matrix sensing and matrix completion."
REFERENCES,0.22597597597597596,Published as a conference paper at ICLR 2022
REFERENCES,0.22672672672672672,"0
5
10
0 1 2 3 4 10244 F F"
REFERENCES,0.22747747747747749,"0
5
10
100 10100"
REFERENCES,0.22822822822822822,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.22897897897897898,"0
1000 2000
10-15 10-10 10-5 100 105"
REFERENCES,0.22972972972972974,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.23048048048048048,"0
1000 2000
10-15 10-10 10-5 100 105"
REFERENCES,0.23123123123123124,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.23198198198198197,"0
1000 2000
10-15 10-10 10-5 100 105"
REFERENCES,0.23273273273273273,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.2334834834834835,"0
1000 2000
10-15 10-10 10-5 100 105"
REFERENCES,0.23423423423423423,"0
50
100
0 20 40 60 80 100"
REFERENCES,0.234984984984985,"0
1000 2000
10-15 10-10 10-5 100 105"
REFERENCES,0.23573573573573572,"Figure 10: General over-parameterized matrix factorization with ∥X0∥F = 99 ∥Y0∥F = 1. The
x−axis represents the number of iterations k; the y−axis represents the value for the norm of Xk
and Yk in the ﬁrst row, and the value for loss in the second row; the learning rate h for each column
is the same. Note the x−axis range of the 1st row is shortened to better show the changes of ∥Xk∥F
and ∥Yk∥F at the beginning of the iterations."
REFERENCES,0.23648648648648649,"0
5
10
0 5 10"
REFERENCES,0.23723723723723725,"15
10138 F F"
REFERENCES,0.23798798798798798,"0
5
10
100 10100 10200"
REFERENCES,0.23873873873873874,"0
50
100
0 2 4 6 8 10 0
1
2 104 10-10 10-5 100 105"
REFERENCES,0.23948948948948948,"0
50
100
0 2 4 6 8 10 0
1
2 104 10-10 10-5 100 105"
REFERENCES,0.24024024024024024,"0
50
100
0 2 4 6 8 10 0
1
2 104 10-10 10-5 100 105"
REFERENCES,0.240990990990991,"0
50
100
0 2 4 6 8 10 0
1
2 104 10-10 10-5 100 105"
REFERENCES,0.24174174174174173,"0
50
100
0 2 4 6 8 10 0
1
2 104 10-10 10-5 100 105"
REFERENCES,0.2424924924924925,"Figure 11: General under-parameterized matrix factorization with ∥X0∥F = 9 ∥Y0∥F = 1. The
x−axis represents the number of iterations k; the y−axis represents the value for the norm of Xk
and Yk in the ﬁrst row, and the value for loss in the second row; the learning rate h for each column
is the same. Note the x−axis range of the 1st row is shortened to better show the changes of ∥Xk∥F
and ∥Yk∥F at the beginning of the iterations."
REFERENCES,0.24324324324324326,"In Figure 13, we consider matrix sensing corresponding to the problem"
REFERENCES,0.243993993993994,"min
X,Y ∈Rn×d
1
2m m
X"
REFERENCES,0.24474474474474475,"i=1
(bi −⟨Ai, XY ⊤⟩)2."
REFERENCES,0.24549549549549549,"Here Ai ∈R100×100 are generated from element-wise i.i.d. Gaussian; bi ∈R are generated
from uniform distribution [0, 1]; m = 10; X, Y ∈R100×6; ⟨U, V ⟩= tr(V ⊤U). The problem is
solved via GD. Experiments in the ﬁgure correspond to learning rates chosen from large to small
as h0, 4"
REFERENCES,0.24624624624624625,"5h0, 3"
REFERENCES,0.246996996996997,"5h0, 2"
REFERENCES,0.24774774774774774,"5h0 where h0 (the 1st column) is picked near the stability limit. We can see from
the ﬁgure that matrix sensing exhibits a similar balancing effect with matrix factorization that larger
learning rate leads to more balanced norms between X and Y ."
REFERENCES,0.2484984984984985,Published as a conference paper at ICLR 2022
REFERENCES,0.24924924924924924,"0
5
10
0 2 4 6"
REFERENCES,0.25,"8
10176 F F"
REFERENCES,0.25075075075075076,"0
5
10
100 10100 10200"
REFERENCES,0.2515015015015015,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.25225225225225223,"0
1.5
3 105 10-10 10-5 100 105"
REFERENCES,0.253003003003003,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.25375375375375375,"0
1.5
3 105 10-10 10-5 100 105"
REFERENCES,0.2545045045045045,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.2552552552552553,"0
1.5
3 105 10-10 10-5 100 105"
REFERENCES,0.256006006006006,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.25675675675675674,"0
1.5
3 105 10-10 10-5 100 105"
REFERENCES,0.2575075075075075,"0
50
100
0 4 8 12 16 20"
REFERENCES,0.25825825825825827,"0
1.5
3 105 10-10 10-5 100 105"
REFERENCES,0.25900900900900903,"Figure 12: General under-parameterized matrix factorization with ∥X0∥F = 19 ∥Y0∥F = 1. The
x−axis represents the number of iterations k; the y−axis represents the value for the norm of Xk
and Yk in the ﬁrst row, and the value for loss in the second row; the learning rate h for each column
is the same. Note the x−axis range of the 1st row is shortened to better show the changes of ∥Xk∥F
and ∥Yk∥F at the beginning of the iterations."
REFERENCES,0.25975975975975973,"In Figure 14, we consider matrix completion corresponding to the problem"
REFERENCES,0.2605105105105105,"min
X,Y ∈Rn×d
1
2"
REFERENCES,0.26126126126126126,"PΩ(A −XY T )
2
F ,"
REFERENCES,0.262012012012012,"where A ∈R10×10 is a low-rank matrix with rank 2; X, Y ∈R10×2; PΩ(U) = (Uij)(i,j)∈Ω+
(0)(i,j)/∈Ωwith sparsity−0.6 where sparsity=(number of non-zero elements)/(number of all elements).
The problem is solved via GD. In the above mentioned ﬁgure, the learning rates are similarly chosen
as the above matrix sensing example. Likewise, the decrease of learning rate results in the increase in
the gap between the norms of X and Y ."
REFERENCES,0.2627627627627628,"Both the matrix sensing and matrix completion above hold the same homogeneity property between
X and Y . The “balancing effect” that we proved for matrix factorization is also observed when
and only when the learning rate h is large, in which case the norms of X and Y become close at
the convergence of GD (and yes, GD still converges even though h is large enough such that the
convergence is not monotone."
REFERENCES,0.2635135135135135,"We feel the techniques invented and employed in this paper can extend to these two cases, but that is
beyond the scope of this paper."
REFERENCES,0.26426426426426425,"B
GD CONVERGING TO PERIODIC ORBIT"
REFERENCES,0.265015015015015,"Consider the objective (1 −xy)2/2. Take step size h = 1.9. Then GD can converge to periodic orbits
with period 2, 3 and 4 respectively in Figure 15."
REFERENCES,0.26576576576576577,"C
MODIFIED EQUATION FAILS FOR LARGE LEARNING RATES"
REFERENCES,0.26651651651651653,"Consider the objective function f(x, y) = (1 −xy)2/2. Then the GD update is the following"
REFERENCES,0.2672672672672673,"xk+1 = xk + h(1 −xkyk)yk
yk+1 = yk + h(1 −ykxk)xk
⇒

xk+1
yk+1"
REFERENCES,0.268018018018018,"
=

xk
yk"
REFERENCES,0.26876876876876876,"
+ h(−∇f(xk, yk)).
(7)"
REFERENCES,0.2695195195195195,Published as a conference paper at ICLR 2022
REFERENCES,0.2702702702702703,"0
50
100
0 5 10 15 20 25 F F"
REFERENCES,0.27102102102102105,"0
50
100
10-20 10-10 100"
REFERENCES,0.27177177177177175,"0
50
100
0 5 10 15 20 25"
REFERENCES,0.2725225225225225,"0
50
100
10-20 10-10 100"
REFERENCES,0.2732732732732733,"0
50
100
0 5 10 15 20 25"
REFERENCES,0.27402402402402404,"0
50
100
10-20 10-10 100"
REFERENCES,0.2747747747747748,"0
50
100
0 5 10 15 20 25"
REFERENCES,0.2755255255255255,"0
50
100
10-20 10-10 100"
REFERENCES,0.27627627627627627,"Figure 13: Matrix sensing. The x−axis represents the number of iterations k; the y−axis represents
the value for the norm of Xk and Yk in the ﬁrst row, and the value for loss in the second row; the
learning rate h for each column is the same."
REFERENCES,0.27702702702702703,"0
50
100
0 5 10 15 20 F F"
REFERENCES,0.2777777777777778,"0
5
10 105 10-5 100"
REFERENCES,0.27852852852852855,"0
50
100
0 5 10 15 20"
REFERENCES,0.27927927927927926,"0
5
10 105 10-5 100"
REFERENCES,0.28003003003003,"0
50
100
0 5 10 15 20"
REFERENCES,0.2807807807807808,"0
5
10 105 10-5 100"
REFERENCES,0.28153153153153154,"0
50
100
0 5 10 15 20"
REFERENCES,0.2822822822822823,"0
5
10 105 10-5 100"
REFERENCES,0.283033033033033,"Figure 14: Matrix completion. The x−axis represents the number of iterations k; the y−axis
represents the value for the norm of Xk and Yk in the ﬁrst row, and the value for loss in the second
row; the learning rate h for each column is the same. Note the x−axis range of the 1st row is
shortened to better show the changes of ∥Xk∥F and ∥Yk∥F at the beginning of the iterations."
REFERENCES,0.28378378378378377,"By backward error analysis (Hairer et al., 2006, Chapter 9), the modiﬁed equation can better
approximate GD than gradient ﬂow and is deﬁned as follows"
REFERENCES,0.28453453453453453,"
˙x
˙y"
REFERENCES,0.2852852852852853,"
= −∇f(x, y) −h"
REFERENCES,0.28603603603603606,"2 ∇2f(x, y)∇f(x, y) + O(h2).
(8)"
REFERENCES,0.28678678678678676,"Figure 16 shows the trajectories of the 1st order modiﬁed equation of (8) and GD (7) with initial
condition x = 4, y = 10 and h = 0.026. The x-axis represents the time t and for GD, the time point
for kth step is kh. We compare the absolute values of x and y of both methods due to the symmetry
of the global minima xy = 1. As is shown in the ﬁgure, even if GD almost converges to the most
balanced minimizer, the solutions of modiﬁed equation are still far away from each other, x ≈0.1 and
y ≈9. Actually, large learning rate h fall outside the convergence domain of the modiﬁed equation
which thus is not an appropriate tool for the analysis of norm balancing."
REFERENCES,0.2875375375375375,Published as a conference paper at ICLR 2022
REFERENCES,0.2882882882882883,"0
1
2
3
4
5 0 1 2 3 4 5 y"
REFERENCES,0.28903903903903905,"0
1
2
3
4
5
x 0 1 2 3 4 5"
REFERENCES,0.2897897897897898,"0
1
2
3
4
5 0 1 2 3 4 5"
REFERENCES,0.2905405405405405,"Figure 15: Three orbits of period 2, 3, and 4. The blue line are the orbits; the red line is a reference
line of the global minima xy = 1."
REFERENCES,0.2912912912912913,"0
2
4
6
8
10
12
Time t 0 2 4 6 8 10"
REFERENCES,0.29204204204204204,Absolute values of x and y
REFERENCES,0.2927927927927928,"modified equation x
modified equation y
GD x
GD y"
REFERENCES,0.29354354354354356,Figure 16: Trajectories: modiﬁed equation vs GD
REFERENCES,0.29429429429429427,"D
PROOF OF PROPOSITION 2.1"
REFERENCES,0.29504504504504503,Proof of Proposition 2.1. This proposition is a direct consequence of Theorem F.2 and I.3.
REFERENCES,0.2957957957957958,"E
OVER-PARAMETRIZED SCALAR DECOMPOSITION: STABILITY LIMIT"
REFERENCES,0.29654654654654655,"Consider the objective (1 −xy)2/2. Choose the initial condition to be x0 = 20, y0 = 0.07 and use
GD update. The upper bound of h in Theorem 3.1 is
4
x2
0+y2
0+4µ, where µ = 1. In Figure 17, when"
REFERENCES,0.2972972972972973,"h =
4
x2
0+y2
0+4 (the left one), GD converges; however, when h is slightly larger than this bound, it
blows up. Hence our restriction for h is very close to the stability limit."
REFERENCES,0.2980480480480481,"F
OVERPARAMETRIZED OBJECTIVE: LARGE LEARNING RATE"
REFERENCES,0.2987987987987988,"In this section, we use column vector instead of row vector for sake of better understanding, i.e., our
objective function is (µ −x⊤y)2/2."
REFERENCES,0.29954954954954954,"F.1
EIGENVALUES OF HESSIAN"
REFERENCES,0.3003003003003003,"Lemma F.1 (Matrix determinant lemma). Suppose A is an invertible n × n matrix and u, v ∈Rn
are column vectors. Then"
REFERENCES,0.30105105105105107,det(A + uv⊤) = (1 + v⊤A−1u) det A.
REFERENCES,0.30180180180180183,Published as a conference paper at ICLR 2022
REFERENCES,0.30255255255255253,"0
500
1000
1500
2000
Number of iterations 10-20"
REFERENCES,0.3033033033033033,"100
loss h=0.0099009"
REFERENCES,0.30405405405405406,"0
5
10
15
20
Number of iterations 100 10100 10200"
REFERENCES,0.3048048048048048,loss h=0.01
REFERENCES,0.3055555555555556,Figure 17: The loss with slightly different h near its upper bound
REFERENCES,0.3063063063063063,"Theorem F.2. The eigenvalues of the Hessian of (µ −x⊤y)2/2 are ±(µ −x⊤y) repeated n −1
times and 1"
REFERENCES,0.30705705705705705,"2
 
∥x∥2 + ∥y∥2 ±
p"
REFERENCES,0.3078078078078078,"(∥x∥2 + ∥y∥2)2 + 4(µ −x⊤y)2 −8(µ −x⊤y)x⊤y

. Especially, at
x⊤y = µ, the eigenvalues are ∥x∥2 + ∥y∥2 and 0 repeated 2n −1 times."
REFERENCES,0.30855855855855857,"Proof. Consider the objective f(x, y) = (µ −x⊤y)2/2. Its Hessian is the following"
REFERENCES,0.30930930930930933,"H =

yy⊤
(x⊤y −µ)In + yx⊤"
REFERENCES,0.31006006006006004,"(x⊤y −µ)In + xy⊤
xx⊤ "
REFERENCES,0.3108108108108108,"=

0
(x⊤y −µ)In
(x⊤y −µ)In
0"
REFERENCES,0.31156156156156156,"
+

y
x"
REFERENCES,0.3123123123123123," 
y
x ⊤
."
REFERENCES,0.3130630630630631,"Then to calculate the eigenvalues, we also need"
REFERENCES,0.3138138138138138,"λI2n −H =

λIn
(µ −x⊤y)In
(µ −x⊤y)In
λIn"
REFERENCES,0.31456456456456455,"
−

y
x"
REFERENCES,0.3153153153153153," 
y
x"
REFERENCES,0.3160660660660661,"⊤
∆= B −

y
x"
REFERENCES,0.31681681681681684," 
y
x ⊤
."
REFERENCES,0.31756756756756754,"By Lemma F.1, we have for invertible B"
REFERENCES,0.3183183183183183,"det

B −

y
x"
REFERENCES,0.31906906906906907," 
y
x"
REFERENCES,0.31981981981981983,"⊤
=

1 −

y
x"
REFERENCES,0.3205705705705706,"⊤
B−1

y
x"
REFERENCES,0.3213213213213213," 
det B."
REFERENCES,0.32207207207207206,"Since (µ −x⊤y)In and λIn commute, we have"
REFERENCES,0.3228228228228228,det B = det((λ2 −(µ −x⊤y)2)In) = (λ2 −(µ −x⊤y)2)n.
REFERENCES,0.3235735735735736,"By the formula for inversion of block matrix, we have for λ2 −(µ −x⊤y)2 ̸= 0, B−1 ="
REFERENCES,0.32432432432432434,"λ
λ2−(µ−x⊤y)2 In
−
µ−x⊤y
λ2−(µ−x⊤y)2 In
−
µ−x⊤y
λ2−(µ−x⊤y)2 In
λ
λ2−(µ−x⊤y)2 In ! ."
REFERENCES,0.32507507507507505,"Then combining all these and by the continuity of characteristic polynomial, we obtain the following
expression for all λ"
REFERENCES,0.3258258258258258,det(λI2n −H) = (λ2 −(µ −x⊤y)2)n−1(λ2 −λ(x⊤x + y⊤y) −(µ −x⊤y)2 + 2(µ −x⊤y)x⊤y).
REFERENCES,0.32657657657657657,"When x⊤y = µ, it becomes"
REFERENCES,0.32732732732732733,det(λI2n −H) = λ2n−1(λ −(x⊤x + y⊤y)).
REFERENCES,0.3280780780780781,"F.2
LOCAL NON-CONVEXITY NEAR THE GLOBAL MINIMUM"
REFERENCES,0.32882882882882886,"Consider the region D = {δ > x⊤y−µ > 0}, a small neighbourhood of the global minima µ = x⊤y
for some δ > 0. Consider two points [x, y], [¯x, ¯y] ∈D with x = c¯x and y = ¯y/c for c > 0 a constant."
REFERENCES,0.32957957957957956,Published as a conference paper at ICLR 2022 Then
REFERENCES,0.3303303303303303,"f(x, y) + ⟨∇f(x, y), [¯x −x, ¯y −y]⟩= (µ −x⊤y)2/2 + (µ −x⊤y)
 
y⊤(x −¯x) + x⊤(y −¯y)
"
REFERENCES,0.3310810810810811,= (µ −¯x⊤¯y)2/2 + (2 −c −1/c)(µ −¯x⊤¯y)¯x⊤¯y
REFERENCES,0.33183183183183185,"≥(µ −¯x⊤¯y)2/2 = f(¯x, ¯y)."
REFERENCES,0.3325825825825826,This contradicts the deﬁnition of convexity. Therefore the objective is not locally convex.
REFERENCES,0.3333333333333333,"F.3
A REVIEW OF TRADITIONAL CONVERGENCE ANALYSIS OF GD UNDER L-SMOOTHNESS"
REFERENCES,0.3340840840840841,"For general function f, GD is deﬁned as follows"
REFERENCES,0.33483483483483484,"xk+1 = xk −h∇f(xk).
(9)"
REFERENCES,0.3355855855855856,"Theorem F.3. If f : RN →R is L−smooth, then with h < 2"
REFERENCES,0.33633633633633636,"L, GD converges to a stationary point."
REFERENCES,0.33708708708708707,Proof. Let min f = f ∗.
REFERENCES,0.33783783783783783,"f(xk+1) ≤f(xk) + ⟨∇f(xk), xk+1 −xk⟩+ L"
REFERENCES,0.3385885885885886,2 ∥xk+1 −xk∥2
REFERENCES,0.33933933933933935,= f(xk) −h(1 −L
REFERENCES,0.3400900900900901,"2 h)∥∇f(xk)∥2. Then N
X"
REFERENCES,0.3408408408408408,"k=1
∥∇f(xk)∥2 ≤
1
h(1 −L"
REFERENCES,0.3415915915915916,2 h)(f(x0) −f(xN))
REFERENCES,0.34234234234234234,"≤
1
h(1 −L"
REFERENCES,0.3430930930930931,2 h)(f(x0) −f ∗).
REFERENCES,0.34384384384384387,"Therefore limk→∞∥∇f(xk)∥2 = 0, i.e., GD converges to a stationary point."
REFERENCES,0.34459459459459457,"G
PROOF OF THEOREM 3.1, THEOREM 3.2, AND COROLLARY 3.3"
REFERENCES,0.34534534534534533,"In this section, we use column vectors for x and y instead of row vectors for better understanding.
Then the objective function is 1"
REFERENCES,0.3460960960960961,2(µ −x⊤y)2.
REFERENCES,0.34684684684684686,"The following Theorem G.1 and G.2 are the main theorems of the over-parametrized scalar case.
Next, for sake of convenience, let u2
k = ∥xk∥2 + ∥yk∥2."
REFERENCES,0.3475975975975976,"Theorem G.1 (Main 1). Let h =
4
u2
0+4cµ. Assume c ≥1 and u2
0 > 8µ. Then GD converges to a"
REFERENCES,0.3483483483483483,"point in {(x, y) : ∥x∥2 + ∥y∥2 ≤2"
REFERENCES,0.3490990990990991,"h, x⊤y = µ} except for a Lebesgue measure-0 set of (x0, y0)."
REFERENCES,0.34984984984984985,"Proof. By Lemma G.7 and Lemma G.11, the theorem holds."
REFERENCES,0.3506006006006006,"Theorem G.2 (Main 2). Let h =
4
8µ+4cµ =
1
(2+c)µ. Assume c ≥1 and u2
0 ≤8µ. Then GD
converges to a point in {(x, y) : ∥x∥2 + ∥y∥2 ≤2"
REFERENCES,0.35135135135135137,"h, x⊤y = µ} except for a Lebesgue measure-0 set
of (x0, y0)."
REFERENCES,0.3521021021021021,"Proof. For the measure-0 set, by Lemma G.5, the set of points converging to {∥x∥2 + ∥y∥2 > 2"
REFERENCES,0.35285285285285284,"h}
is measure-0; by the proof of Lemma G.11, the set of points converging to the origin is measure-0;
also {u2
0 = ∥x∥2 + ∥y∥2 = 0} is a hyperplane and thus is measure-0. Hence the set of all the initial
conditions not converging to {∥x∥2 + ∥y∥2 > 2"
REFERENCES,0.3536036036036036,"h, x⊤y = µ} is measure-0."
REFERENCES,0.35435435435435436,"Since u2
0 ≤8µ, if u2
0 >
2
h, by Lemma G.7, it will decrease to u2
k ≤
2
h for some k. Then by
Lemma G.11, we have the convergence."
REFERENCES,0.3551051051051051,"With the above two theorems, we can prove all the theorems and corollary in Section 3."
REFERENCES,0.35585585585585583,Published as a conference paper at ICLR 2022
REFERENCES,0.3566066066066066,"Proof of Theorem 3.1 and Theorem 3.2. From Theorem G.1, h =
4
u2
0+4cµ for c ≥1 which implies"
REFERENCES,0.35735735735735735,"h ≤
4
u2
0+4µ for all u2
0 > 8µ. Similarly, for Theorem G.2, h ≤
1
3µ. When u2
0 = 8µ,
4
u2
0+4µ ="
REFERENCES,0.3581081081081081,"4
8µ+4µ =
1
3µ; when u2
0 > 8µ,
4
u2
0+4µ <
1
3µ; when u2
0 < 8µ,
4
u2
0+4µ >
1
3µ. Also, all the limit"
REFERENCES,0.3588588588588589,"points are in {∥x∥2 + ∥y∥2 ≤
2
h}. Then we can get Theorem 3.1 and 3.2 where the second
inequality of Theorem 3.2 is because at the global minimum x⊤y = µ and also ∥x −y∥2 =
∥x∥2 + ∥y∥2 −2x⊤y."
REFERENCES,0.35960960960960964,"Proof of Corollary 3.3. For |x⊤
0 y0 −µ| < δ, ∥x0∥2 + ∥y0∥2 > 8µ, we have"
REFERENCES,0.36036036036036034,"x⊤
0 y0 < µ + δ ⇒∥x0 −y0∥2 = ∥x0∥2 + ∥y0∥2 −2x⊤
0 y0 > 4"
REFERENCES,0.3611111111111111,h −4µ −2(µ + δ) = 4
REFERENCES,0.36186186186186187,h −6µ −2δ.
REFERENCES,0.36261261261261263,"Also, from Theorem 3.2, ∥x −y∥2 ≤2"
REFERENCES,0.3633633633633634,h −2µ and 2
REFERENCES,0.3641141141141141,h −2µ < 4
REFERENCES,0.36486486486486486,h −6µ −2δ < 4
REFERENCES,0.3656156156156156,"h −8µ. We then obtain
∥x −y∥2 ≤1"
REFERENCES,0.3663663663663664,2 ∥x0 −y0∥2 + 2µ.
REFERENCES,0.36711711711711714,We will divide the proof of the two main theorems into two phases: (1) when 2
REFERENCES,0.36786786786786785,"h < x2
k + y2
k < 4"
REFERENCES,0.3686186186186186,"h, we
would like to show that GD escapes to a smaller ball x2
k + y2
k ≤2"
REFERENCES,0.36936936936936937,"hexcept for a measure-0 set; (2)
once GD enters x2
k + y2
k ≤2"
REFERENCES,0.37012012012012013,"h, it will converge to the global minimum inside this region except for a
measure-0 set."
REFERENCES,0.3708708708708709,"G.1
PHASE 1: 2"
REFERENCES,0.3716216216216216,"h < u2
k < 4 h"
REFERENCES,0.37237237237237236,"We ﬁrst deal with the situation where GD just converges in this region by showing that such points
form a null set. This is stated in Theorem G.5."
REFERENCES,0.3731231231231231,"Theorem G.3 and Corollary G.4 are the preliminary of proving Theorem G.5. Also Corollary G.4 is a
direct result of Theorem G.3."
REFERENCES,0.3738738738738739,"Theorem G.3. Let f : RN →RM and f ∈C1. If the set of critical points of f is a null-set, i,e.,"
REFERENCES,0.37462462462462465,"L({x ∈RN : ∇f(x) is not invertible}) = 0,"
REFERENCES,0.37537537537537535,then L(f −1(B)) = 0 for any null-set B.
REFERENCES,0.3761261261261261,"Proof. Let G = {|∇f| ̸= 0} and G is an open set, where |∇f| denotes the determinant of ∇f. By
implicit function theorem, we have G ∩f −1(z) is a (N −1)-submanifold in C1.
Consider G ∩{f ∈B} = S"
REFERENCES,0.3768768768768769,"z∈B G ∩f −1(z), where L(G ∩f −1(z)) = 0 from the above discussion.
Hence, if B is countable, L(G ∩{f ∈B}) = 0."
REFERENCES,0.37762762762762764,"When B is uncountable, using co-area formula from geometric measure theory, we have for any
bounded open ball Br,
Z"
REFERENCES,0.3783783783783784,"G∩Br
g|∇f|dL =
Z R Z"
REFERENCES,0.3791291291291291,"f −1(z)∩G∩Br
g(x)dLN−1(x)dz =
Z B Z"
REFERENCES,0.37987987987987987,"f −1(z)∩G∩Br
g(x)dLN−1(x)dz."
REFERENCES,0.38063063063063063,"Let g be the indicator of S
z∈B G ∩Br ∩f −1(z). Then since B is a null-set, we have
Z B Z"
REFERENCES,0.3813813813813814,"f −1(z)∩G∩Br
g(x)dLN−1(x)dz =
Z B Z"
REFERENCES,0.38213213213213215,"f −1(z)∩G∩Br
dLN−1(x)dz = 0 ⇒
Z"
REFERENCES,0.38288288288288286,"G∩Br
g|∇f|dL = 0."
REFERENCES,0.3836336336336336,"Then g|∇f| = 0 a.e. in G ∩Br. Also, |∇f| ̸= 0 a.e.. Therefore, g = 0 a.e. in G ∩Br, i.e., L
 ["
REFERENCES,0.3843843843843844,"z∈B
G ∩Br ∩f −1(z)

=
Z
gdL = 0."
REFERENCES,0.38513513513513514,Published as a conference paper at ICLR 2022
REFERENCES,0.3858858858858859,"Since we can ﬁnd a sequence of bounded open ball Br, s.t., RN = S∞
r=1 Br, and also L(Gc) = 0,
we have"
REFERENCES,0.3866366366366366,"L(f −1(B)) ≤L
 ["
REFERENCES,0.38738738738738737,"z∈B
G ∩f −1(z)

+ L(Gc) = L
 ∞
[ r=1 ["
REFERENCES,0.38813813813813813,"z∈B
G ∩Br ∩f −1(z)

+ L(Gc) = 0."
REFERENCES,0.3888888888888889,"Corollary G.4. Let ψ : R2d →R2d be the GD iteration map, i.e., ψ(x, y) = [x + h(µ −x⊤y)y, y +
h(µ −x⊤y)x]⊤. Then if"
REFERENCES,0.38963963963963966,"L({det(Dψ) = 0}) = 0,"
REFERENCES,0.39039039039039036,then L(ψ−1(B)) = 0 for any null-set B.
REFERENCES,0.3911411411411411,"Lemma G.5. Given h =
4
u2
0+4cµ and u2
k > 2"
REFERENCES,0.3918918918918919,"h, GD will not converge to the points in {∥x∥2 +∥y∥2 >"
REFERENCES,0.39264264264264265,"2
h, x⊤y = µ}, except for a measure-0 set."
REFERENCES,0.3933933933933934,"Proof. For d = 1,"
REFERENCES,0.39414414414414417,"det(Dψ) = 1 −hx2 −hy2 −3h2x2y2 + 4h2xyµ −h2µ2 = p1(x, y) · · · pm(x, y) = 0,"
REFERENCES,0.3948948948948949,"where pi(x, y) is irreducible polynomial and pi(x, y) = 0 is a co-dimensional-1 manifold. Then
{(x, y) : det(Dψ) = 0} = Sm
i=1{pi(x, y) = 0} are measure zero, i.e., L({det(Dψ) = 0}) = 0.
Similarly for d > 1, we also have L({det(Dψ) = 0}) = 0."
REFERENCES,0.39564564564564564,"From the GD iteration, we have"
REFERENCES,0.3963963963963964,"x⊤
k+1yk+1 −µ = (x⊤
k yk −µ) · (1 −h(∥xk∥2 + ∥yk∥2) −h2x⊤
k yk(µ −x⊤
k yk))."
REFERENCES,0.39714714714714716,"Then let B = {(x, y) : 1 −h(∥x∥2 + ∥y∥2) −h2x⊤y(µ −x⊤y) = 0}. For any (x, y) ∈B, let
[x+, y+]⊤= ψ(x, y). Then x+y+ = µ. Similarly, L(B) = 0. By Corollary G.4 L(ψ−1(B)) = 0
and then L(ψ−n(B)) = L(ψ−1 ◦· · · ◦ψ−1(B)) = 0. Let ψ−0(B) = B and G = S∞
i=0 ψ−n(B).
Hence"
REFERENCES,0.3978978978978979,"L(G) = L ∞
["
REFERENCES,0.39864864864864863,"i=0
ψ−n(B) ! ≤ ∞
X"
REFERENCES,0.3993993993993994,"i=0
L(ψ−n(B)) = 0."
REFERENCES,0.40015015015015015,"Moreover, for any 0 < ϵ < µ, assume |x⊤
k yk −µ| < ϵ. When u2
k ≥2"
REFERENCES,0.4009009009009009,"h + ϵh(µ + ϵ),"
REFERENCES,0.4016516516516517,"|x⊤
k+1yk+1 −µ| = |x⊤
k yk −µ| · |1 −hu2
k −h2x⊤
k yk(µ −x⊤
k yk)|"
REFERENCES,0.4024024024024024,"= |x⊤
k yk −µ| · (−1 + hu2
k + h2x⊤
k yk(µ −x⊤
k yk))"
REFERENCES,0.40315315315315314,"≥|x⊤
k yk −µ| ·

−1 + h
 2"
REFERENCES,0.4039039039039039,"h + ϵh(µ + ϵ)

−h2(µ + ϵ)ϵ
"
REFERENCES,0.40465465465465467,"> |x⊤
k yk −µ|."
REFERENCES,0.40540540540540543,"Hence, {∥x∥2 + ∥y∥2 > 2"
REFERENCES,0.40615615615615613,"h, x⊤y = µ} is not the limit of this GD map ψ, except for the measure-0
set G."
REFERENCES,0.4069069069069069,Next we show that GD will be bounded inside {∥x∥2 + ∥y∥2 < 4 h}.
REFERENCES,0.40765765765765766,"Lemma G.6. Given h =
4
u2
0+4cµ, then for 0 ≤k < min{k : u2
k ≤2"
REFERENCES,0.4084084084084084,"h}, we have u2
k ≤4"
REFERENCES,0.4091591591591592,h −3µ < 4
REFERENCES,0.4099099099099099,"h
for all k."
REFERENCES,0.41066066066066065,"Proof. First u2
0 =
4
h −4cµ ≤
4
h −4µ <
4
h. Then if x⊤
0 y0 > µ or x⊤
0 y0 < −hµu2
0
4−hu2
0 , from"
REFERENCES,0.4114114114114114,"Lemma G.8, u2
1 < u2
0 < 4"
REFERENCES,0.41216216216216217,"h. If −hµu2
0
4−hu2
0 ≤x⊤
0 y0 < µ, from Lemma G.9 and its proof, u2
2 < u2
0 < 4"
REFERENCES,0.41291291291291293,"h
and u1 ≤u2
0 + µ"
REFERENCES,0.41366366366366364,"c ≤u2
0 + µ ≤4"
REFERENCES,0.4144144144144144,h −3µ < 4
REFERENCES,0.41516516516516516,"h. Therefore, iteratively we have u2
k < 4 h."
REFERENCES,0.4159159159159159,Published as a conference paper at ICLR 2022
REFERENCES,0.4166666666666667,"Therefore, without loss of generality, we can just assume u2
k ≤u2
0 for a ﬁxed kth iteration that we
need to analyze because for every two step there exists an ith iteration such that u2
i ≤u2
0 and we can
choose k = i to do the analysis."
REFERENCES,0.4174174174174174,"Lemma G.8 and G.9 describe one-step or two-step decay of u2
k, which lead to the primary result,
Lemma G.7, in phase one. Moreover, the proof of Lemma G.7 contains a ﬁner characterization of u2
k
making it possible to end phase one and enter phase two.
Lemma G.7. Given h =
4
u2
0+4cµ for c ≥1, GD will enter {∥x∥2 + ∥y∥2 ≤
2
h} except for a
measure-0 set of initial conditions."
REFERENCES,0.41816816816816815,"Proof. By Lemma G.6 and its discussion, assume without loss of generality 2"
REFERENCES,0.4189189189189189,"h < u2
k ≤u2
0. From
Lemma G.8 and G.9, the region where the decrease of u2
k may be small is when xk, yk are close to"
REFERENCES,0.4196696696696697,"x⊤
k yk = µ or x⊤
k yk = −hµu2
k
4−hu2
k . Since"
REFERENCES,0.42042042042042044,"x⊤
k+1yk+1 −µ = (x⊤
k yk −µ)(1 −hu2
k −h2x⊤
k yk(µ −x⊤
k yk)),"
REFERENCES,0.42117117117117114,"consider sk = 1 −hu2
k −h2x⊤
k yk(µ −x⊤
k yk). From the proof of Lemma G.9, we know when"
REFERENCES,0.4219219219219219,"x⊤
k yk = −hµu2
k
4−hu2
k , sk < −1+2h2µ2 < 0. Therefore, there exists δ > 0, s.t., for all xk, yk ∈{|x⊤
k yk+"
REFERENCES,0.42267267267267267,"hµu2
k
4−hu2
k | < δ, 2"
REFERENCES,0.42342342342342343,"h < u2
k ≤u2
0}, sk < 0. Then x⊤
k+1yk+1 > µ. Hence, when −δ ≤x⊤
k yk +
hµu2
k
4−hu2
k < 0,"
REFERENCES,0.4241741741741742,"we will skip this step and consider the decrease of the next step with x⊤
k+1yk+1 > µ. Also, there"
REFERENCES,0.42492492492492495,"exist β1 = β1(δ) > 0, s.t., when x⊤
k yk +
hµu2
k
4−hu2
k < −δ,"
REFERENCES,0.42567567567567566,"u2
k+1 −u2
k = h(µ −x⊤
k yk)((4 −hu2
k)x⊤
k yk + hu2
kµ) < −β1."
REFERENCES,0.4264264264264264,"From the proof of Lemma G.9, when −hµu2
k
4−hu2
k ≤x⊤
k yk ≤0, for β2 = max{hµ2(4 −hu2
0)(1 −"
REFERENCES,0.4271771771771772,"2h2µ2)2, hµ2(8 −h(2u2
0 + µ"
REFERENCES,0.42792792792792794,"c ))} > 0,"
REFERENCES,0.4286786786786787,"u2
k+2 −u2
k < max{−h(µ −x⊤
k yk)µ(4 −hu2
k+1)(1 −2h2µ2)2, −h(µ −x⊤
k yk)2(8 −h(u2
k + u2
k+1))} ≤−β2."
REFERENCES,0.4294294294294294,"Fix an small ϵ in 0 < ϵ < µ. When x⊤
k yk ≥µ + ϵ, from the proof of Lemma G.8, we have for
β3 = 4hµϵ > 0,"
REFERENCES,0.43018018018018017,"u2
k+1 −u2
k ≤4hµ(µ −x⊤
k yk) ≤−4hµϵ = −β3."
REFERENCES,0.43093093093093093,"When 0 < x⊤
k yk ≤µ −ϵ with ϵ > 0, from the proof of Lemma G.9, we have for β4 = hϵ2(8 −
h(2u2
0 + µ"
REFERENCES,0.4316816816816817,"c )) > 0,"
REFERENCES,0.43243243243243246,"u2
k+2 −u2
k ≤−h(µ −x⊤
k yk)2(8 −h(u2
k + u2
k+1)) ≤−β4."
REFERENCES,0.43318318318318316,"When |x⊤
k yk −µ| < ϵ, assume |x⊤
k yk −µ| = ϵk. In this case, we have sk < 0 meaning GD oscillates
around x⊤y = µ. Also, if 0 < x⊤
k yk < µ and u2
k > 2"
REFERENCES,0.4339339339339339,"h, sk = 1 −hu2
k −h2x⊤y(µ −x⊤y) < −1,
i.e., if GD is in 0 < x⊤y < µ, then we have |x⊤
k+1yk+1 −µ| > |x⊤
k yk −µ|. Hence we only
need to focus on the other side which is x⊤
k yk > µ. From Lemma G.5, if u2
k ≥2"
REFERENCES,0.4346846846846847,"h + ϵh(µ + ϵ),
|x⊤
k+1yk+1 −µ| > |x⊤
k yk −µ|. Then within ﬁnite steps (note all these steps satisfy either one-step or
two-step decrease of u2
k; we ignore these steps only because the decrease maybe small), we will have
either |x⊤
k yk −µ| keeps increasing or u2
K < 2"
REFERENCES,0.43543543543543545,"h + ϵKh(µ + ϵK) (here we also consider x⊤
KyK > µ).
For the former one, the decrease of u2
k for each step will be lower bounded away from 0; for the latter
one, from the discussion above, u2
K+1 −u2
K ≤−4hµϵK ⇒u2
K+1 ≤2 h."
REFERENCES,0.4361861861861862,"From Lemma G.5, we know GD will not terminate in ﬁnite steps except for measure-0 set. From all
the discussion above, we have that u2
k decreases by a constant for either one-step or two-step except
and hence GD will enter ∥x∥2 + ∥y∥2 ≤2"
REFERENCES,0.4369369369369369,h except for a measure-0 set.
REFERENCES,0.4376876876876877,Published as a conference paper at ICLR 2022
REFERENCES,0.43843843843843844,"Lemma G.8. Given h =
4
u2
0+4cµ and u2
k <
4
h, when x⊤
k yk > µ or x⊤
k yk < −hµu2
k
4−hu2
k , we have"
REFERENCES,0.4391891891891892,"u2
k+1 −u2
k < 0."
REFERENCES,0.43993993993993996,Proof.
REFERENCES,0.44069069069069067,"u2
k+1 −u2
k = h(µ −x⊤
k yk)((4 −hu2
k)x⊤
k yk + hu2
kµ) = h(µ −x⊤
k yk)(4x⊤
k yk + hu2
k(µ −x⊤
k yk))"
REFERENCES,0.44144144144144143,"If x⊤
k yk > µ, by u2
k < 4 h,"
REFERENCES,0.4421921921921922,"u2
k+1 −u2
k = h(µ −x⊤
k yk)(4x⊤
k yk + hu2
k(µ −x⊤
k yk)) ≤4hµ(µ −x⊤
k yk) < 0."
REFERENCES,0.44294294294294295,"If x⊤
k yk < −hµu2
k
4−hu2
k ⇒x⊤
k yk < µ, then,"
REFERENCES,0.4436936936936937,"u2
k+1 −u2
k = h(µ −x⊤
k yk)((4 −hu2
k)x⊤
k yk + hu2
kµ) < 0."
REFERENCES,0.4444444444444444,"Lemma G.9. Given h =
4
u2
0+4cµ , c > max{ 1"
REFERENCES,0.4451951951951952,"2, 2hµ}, c > 0, and u2
k ≤u2
0, when −hµu2
k
4−hu2
k ≤"
REFERENCES,0.44594594594594594,"x⊤
k yk < µ, then u2
k+2 −u2
k < 0."
REFERENCES,0.4466966966966967,"Proof. For every u2
k, there exist a constant ck > 0, s.t. h =
4
u2
k+4ckµ. Since 2"
REFERENCES,0.44744744744744747,"h < u2
k ≤u2
0 < 4 h, we"
REFERENCES,0.44819819819819817,"have c ≤ck <
1
2hµ. Since"
REFERENCES,0.44894894894894893,"x⊤
k+1yk+1 −µ = (x⊤
k yk −µ)(1 −hu2
k −h2x⊤
k yk(µ −x⊤
k yk)) = (x⊤
k yk −µ)sk,"
REFERENCES,0.4496996996996997,"then sk = 1 −hu2
k −h2x⊤
k yk(µ −x⊤
k yk) is bounded by the value at x⊤
k yk = −hµu2
k
4−hu2
k or x⊤
k yk = µ,
i.e.,"
REFERENCES,0.45045045045045046,"sk = 1 −hu2
k −h2x⊤
k yk(µ −x⊤
k yk) ≤max

1 −hu2
k +
4h3u2
kµ2"
REFERENCES,0.4512012012012012,"(4 −hu2
k)2 , 1 −hu2
k "
REFERENCES,0.4519519519519519,"Since u2
k > 2"
REFERENCES,0.4527027027027027,"h, we have 1 −hu2
k < −1. For the other one, since u2
k > 2"
REFERENCES,0.45345345345345345,"h and ck <
1
2hµ,"
REFERENCES,0.4542042042042042,"1 −hu2
k +
4h3u2
kµ2"
REFERENCES,0.45495495495495497,"(4 −hu2
k)2 = (1 −3c2
k)u2
k + 4c3
kµ
c2
k(u2
k + 4ckµ)
< −1 + 2h2µ2,"
REFERENCES,0.45570570570570573,"where c >
hµ+√"
REFERENCES,0.45645645645645644,2−h2µ2
REFERENCES,0.4572072072072072,"2(1−h2µ2)
. This is because either u2
0 > 8µ ⇒hµ <
1
3µ or h ≤
1
3µ and then hµ+√"
REFERENCES,0.45795795795795796,2−h2µ2
REFERENCES,0.4587087087087087,"2(1−h2µ2)
< 1 ≤c. Also note here this is c not ck; this value −1 + 2h2µ2 is achieved when"
REFERENCES,0.4594594594594595,"u2
k = 2/h and ck =
hµ+√"
REFERENCES,0.4602102102102102,2−h2µ2
REFERENCES,0.46096096096096095,"2(1−h2µ2)
or
1
2hµ."
REFERENCES,0.4617117117117117,"Also, when 0 ≤x⊤
k yk < µ,"
REFERENCES,0.4624624624624625,"sk = 1 −hu2
k −h2x⊤
k yk(µ −x⊤
k yk) ≤1 −hu2
k < −1."
REFERENCES,0.46321321321321324,"We then prove 4 −hu2
k+1 > 0. When x⊤
k yk < µ,"
REFERENCES,0.46396396396396394,"u2
k+1 −u2
k = h(µ −x⊤
k yk)((4 −hu2
k)x⊤
k yk + hu2
kµ)."
REFERENCES,0.4647147147147147,"The maximum is achieved at x⊤
k yk = 2−hu2
k
4−hu2
k µ, i.e., u2
k+1 −u2
k ≤µ ck ≤µ"
REFERENCES,0.46546546546546547,"c . Hence 4 −hu2
k+1 > 0
when c > 1/2."
REFERENCES,0.46621621621621623,"Since x⊤
k yk < µ and sk ≤0,"
REFERENCES,0.466966966966967,"u2
k+2 −u2
k = u2
k+2 −u2
k+1 + u2
k+1 −u2
k
= h(µ −x⊤
k+1yk+1)((4 −hu2
k+1)x⊤
k+1yk+1 + hu2
k+1µ)"
REFERENCES,0.4677177177177177,"+ h(µ −x⊤
k yk)((4 −hu2
k)x⊤
k yk + hu2
kµ)"
REFERENCES,0.46846846846846846,"= h(µ −x⊤
k yk)(((4 −hu2
k)x⊤
k yk + hu2
kµ) + sk(hu2
k+1µ + (4 −hu2
k+1)(sk(x⊤
k yk −µ) + µ)))"
REFERENCES,0.4692192192192192,"= h(µ −x⊤
k yk)
 
x⊤
k yk(4 −hu2
k) + s2
k(4 −hu2
k+1)(x⊤
k yk −µ) + hu2
kµ + sk(hu2
k+1µ + (4 −hu2
k+1)µ)

."
REFERENCES,0.46996996996997,Published as a conference paper at ICLR 2022
REFERENCES,0.47072072072072074,"When 0 ≤x⊤
k yk < µ, s < −1, then"
REFERENCES,0.47147147147147145,"u2
k+2 −u2
k ≤−h(µ −x⊤
k yk)2(8 −h(u2
k + u2
k+1)) < 0"
REFERENCES,0.4722222222222222,"When −hµu2
k
4−hu2
k ≤x⊤
k yk < 0, sk ≤0, then for c ≥2hµ,"
REFERENCES,0.47297297297297297,"u2
k+2 −u2
k < h(µ −x⊤
k yk)µ(−4 + hu2
k + 8h2µ2 −(4 −hu2
k+1)(1 −2h2µ2)2)"
REFERENCES,0.47372372372372373,"< −h(µ −x⊤
k yk)µ(4 −hu2
k+1)(1 −2h2µ2)2 < 0"
REFERENCES,0.4744744744744745,"where this bound is achieved by taking sk = −1 + 2h2µ2 and x⊤
k yk = 0."
REFERENCES,0.4752252252252252,"G.2
PHASE 2: u2
k ≤2 h"
REFERENCES,0.47597597597597596,"In this part, we will show the convergence of GD in Lemma G.11."
REFERENCES,0.4767267267267267,"There are two convergence patterns: (i) transversal convergence, i.e., oscillating around the valley;
(ii) unilateral convergence, i.e., converging from one side of the valley."
REFERENCES,0.4774774774774775,"The key point of the proof of pattern (i) is to analyze the change of sk = 1−hu2
k−h2x⊤
k yk(µ−x⊤
k yk).
We know x⊤
k+1yk+1 −µ = sk(x⊤
k yk −µ), i.e., sk measures the change of the loss. If for some
constant K > 0 we have |sk| < 1 ∀k ≥K such that |x⊤
k yk −µ| is guaranteed to decrease to 0, then
the convergence follows. Moreover, we will need to analyze s2k and s2k+1 separately because in this
case sk < 0 and x⊤
k yk −µ changes sign at each step."
REFERENCES,0.47822822822822825,"For pattern (ii), we will show that the trajectory of GD is bounded in a subset of this region |sk| < 1
that guarantees the decrease of the loss."
REFERENCES,0.47897897897897895,"Before presenting our main result in phase 2, we ﬁrst show a boundedness theorem when GD enters
this phase."
REFERENCES,0.4797297297297297,Lemma G.10. Once GD enters {∥x∥2 + ∥y∥2 ≤2
REFERENCES,0.4804804804804805,"h}, it will stay bounded inside {∥x∥2 + ∥y∥2 ≤
2
h + 2µ} and re-enter {∥x∥2 + ∥y∥2 ≤2"
REFERENCES,0.48123123123123124,"h} within ﬁnite steps where the number of such steps do not
depend on the number of iteration."
REFERENCES,0.481981981981982,"Proof. If at step K, u2
K ≤2"
REFERENCES,0.4827327327327327,"h, then from Lemma G.15, u2
K+1 ≤2"
REFERENCES,0.48348348348348347,"h + µ and it returns to phase 1.
From the proof of Lemma G.9, u2
K+2 ≤u2
K+1 + µ ≤2"
REFERENCES,0.48423423423423423,"h + 2µ and we know either u2
K+2 < u2
K+1 or
u2
K+3 < u2
K+1 and so on until it re-enters u2
i ≤2"
REFERENCES,0.484984984984985,"h for some i ≥K. Therefore, all the u2
k ≤2"
REFERENCES,0.48573573573573575,"h + 2µ
once GD enters {∥x∥2 + ∥y∥2 ≤2 h}."
REFERENCES,0.4864864864864865,"Then by Lemma G.10, we can always pick a kth iteration such that u2
k ≤2"
REFERENCES,0.4872372372372372,"h. Also, when u2
0 > 8µ,
we have hµ < 1"
REFERENCES,0.487987987987988,"3. Together with the choice of h =
1
(2+c)µ when 0 < u2
0 ≤8µ, we have hµ ≤1"
REFERENCES,0.48873873873873874,"3.
Then we obtain the following convergence of GD inside phase 2.
Lemma G.11. Given hµ ≤1"
REFERENCES,0.4894894894894895,"3, if GD enters {∥x∥2 + ∥y∥2 ≤2"
REFERENCES,0.49024024024024027,"h}, it converge to x⊤y = µ inside this
region except for a measure-0 set of initial conditions."
REFERENCES,0.49099099099099097,"Proof. First, from Corollary G.4, we know the set of points converging to (0, 0) in ﬁnite steps is
measure 0. For all the other points, we have the following discussion."
REFERENCES,0.49174174174174173,"If sk = 0, then x⊤
k+1yk+1 = µ, i.e., it converges."
REFERENCES,0.4924924924924925,"If x⊤
k yk < −µ and 1"
REFERENCES,0.49324324324324326,"h + 2hµ2 < u2
k ≤2"
REFERENCES,0.493993993993994,"h, then by Lemma G.14,"
REFERENCES,0.4947447447447447,"u2
k+1 −u2
k = h(µ −x⊤
k yk)((4 −hu2
k)x⊤
k yk + hu2
kµ) ≤2h(µ2 −(x⊤
k yk)2) < 0."
REFERENCES,0.4954954954954955,"Hence for δ1 > 0, when −µ −δ1 < x⊤
k yk < −µ, we have x⊤
k+1yk+1 > µ and we will skip this step
and directly consider the (k+1)th iteration; when x⊤
k yk ≤−µ−δ1, u2
k+1−u2
k ≤−2h((µ+δ1)2−µ2).
Namely when x⊤
k yk < −µ, either u2
k+1 is some constant away from u2
k or we can ignore the decrease
in this step and directly look at the next one with x⊤
k+1yk+1 > µ."
REFERENCES,0.49624624624624625,Published as a conference paper at ICLR 2022
REFERENCES,0.496996996996997,"If x⊤
k yk < −µ and u2
k ≤1"
REFERENCES,0.49774774774774777,"h + 2hµ2, then"
REFERENCES,0.4984984984984985,"u2
k+1 −u2
k = h(µ −x⊤
k yk)(4x⊤
k yk + hu2
k(µ −x⊤
k yk))"
REFERENCES,0.49924924924924924,"≤h(µ −x⊤
k yk)(4x⊤
k yk + (1 + 2h2µ2)(µ −x⊤
k yk))"
REFERENCES,0.5,≤−4hµ2(1 −2h2µ2) < 0.
REFERENCES,0.5007507507507507,"If x⊤
k yk > 3µ, then u2
k+1 −u2
k ≤2h(µ2 −x2
ky2
k) < −16hµ2. Also, when u2
k ≤6µ, x⊤
k yk ≤3µ."
REFERENCES,0.5015015015015015,"Next, for the rest of the region, we ﬁrst consider 1"
REFERENCES,0.5022522522522522,"h + 2hµ2 < u2
k ≤2"
REFERENCES,0.503003003003003,"h (by Lemma G.10, we are
always able to ﬁnd u2
k ≤
2
h). In this region, by Lemma G.18, we know sk < 0, i.e., (x⊤
k yk −
µ)(x⊤
k+1yk+1 −µ) < 0. We divide the whole region into two parts: x⊤
k yk > µ and x⊤
k yk < µ.
Therefore all the xk+2i should be on the same side for i such that 1"
REFERENCES,0.5037537537537538,"h + 2hµ2 < u2
k+2i ≤2 h."
REFERENCES,0.5045045045045045,"If −µ ≤x⊤
k yk < µ and sk ≤−1, by Lemma G.16, we have u2
k+2 −u2
k ≤−4h(1 −hµ)(µ −
x⊤
k yk)2 < 0. Hence when GD does not converge, i.e., {(xk+2n, yk+2n)}∞
n=0 does not converge , u2
k
will keep decreasing with u2
k+2i ≤2"
REFERENCES,0.5052552552552553,"h for all K+2i with i ≥0 such that sk+2i ≤−1. Moreover, there
exists N > 0 s.t. sk+2N > −1 because sk = 1−hu2
k−h2x⊤
k yk(µ−x⊤
k yk) and if xk+2i, yk+2i is the
ﬁrst iteration to leave this region, then it has to satisfy x⊤
k+2iyk+2i < −µ which implies sk+2i > −1,
and then it follows from Lemma G.13 and G.12 just as the following two paragraphs of discussion; if
it never leaves this region and GD is not converging, i.e., |µ −x⊤
k yk| has a lower bound, then u2
k+2i
will keep decreasing until sk+2N > −1 (from the proof of Lemma G.12, if µ −x⊤
k yk is very small,
then sk+1 < sk meaning both sides are blowing up and therefore |µ −x⊤
k yk| has a lower bound if
sk ≤−1)."
REFERENCES,0.506006006006006,"If −µ ≤x⊤
k yk < µ and sk > −1, from Lemma G.12 and Lemma G.13, similar to the previous
discussion, there exists N1 > 0, s.t., for all k > N1, sk > min{−1 + c0, −1 + c1(µ −x⊤
k yk)2} for
some c0, c1 > 0, namely |µ −x⊤
k yk| strictly decreases. Then it will either converge in 1"
REFERENCES,0.5067567567567568,"h + 2hµ2 <
∥x∥2 + ∥y∥2 ≤2"
REFERENCES,0.5075075075075075,h or enter ∥x∥2 + ∥y∥2 ≤1
REFERENCES,0.5082582582582582,h + 2hµ2.
REFERENCES,0.509009009009009,"If µ < x⊤
k yk ≤3µ, from the deﬁnition of sk, we have sk > −1. Also by Lemma G.13, we know
sm > min{−1 + c0, −1 + c1(µ −x⊤
k yk)2} for all m > k. Hence similarly, it will either converge
or enter u2
k ≤1"
REFERENCES,0.5097597597597597,h + 2hµ2.
REFERENCES,0.5105105105105106,"Then we consider u2
k ≤
1
h + 2hµ2. From Lemma G.17 and Lemma G.20, we know |sk| < 1
and will be away from 1 by a constant in this area. We will show that GD stays bounded in the
converging domain. If sk ≤0, it follows from the previous discussion and the proof of Lemma G.17
and Lemma G.20. If sk > 0, when the trajectory is in {|x⊤y| > µ}, from Lemma G.14, we have
u2
k+1 < u2
k; when it is in {|x⊤y| < µ}, then |xk+1 −yk+1| < |xk −yk| from Lemma G.19. Hence
the trajectory will stay inside the monotone decreasing region. Next, since |µ −x⊤
k yk| monotonically
decreases and is lower bounded by 0, we have that |µ −x⊤
k yk| converges. If |µ −x⊤
k yk| converges to
C > 0, then min{−1 + c0, −1 + c1(µ −x⊤
k yk)2} < sk < 1 −c2 for some c0, c1, c2 > 0 meaning
|µ −x⊤
k yk| does not converges to C. Contradiction. Therefore, |µ −x⊤
k yk| converges to 0."
REFERENCES,0.5112612612612613,"Lemma G.12. When −µ ≤x⊤
k yk < µ and 1"
REFERENCES,0.512012012012012,"h + 2hµ2 < u2
k ≤
2
h + 2µ, if sk > −1, then"
REFERENCES,0.5127627627627628,sk+2 > min{−3
REFERENCES,0.5135135135135135,4 −h2µ2
REFERENCES,0.5142642642642643,"4 , −5h2µ2"
REFERENCES,0.515015015015015,"2
, −1 + h2(µ −x⊤
k yk)2} > −1, for 9"
REFERENCES,0.5157657657657657,2h2µ2 < 1.
REFERENCES,0.5165165165165165,"Proof. From previous statement, sk < 0 when u2
k > 1"
REFERENCES,0.5172672672672672,"h + 2hµ2. For sk = 1 −hu2
k −h2x⊤
k yk(µ −
x⊤
k yk), let ϵ = µ −x⊤
k yk. Then x⊤
k+1yk+1 −µ = −skϵ."
REFERENCES,0.5180180180180181,"sk+1 = 1 −hu2
k+1 −h2x⊤
k+1yk+1(µ −x⊤
k+1yk+1) = sk −h2ϵµ(3 + sk) + h2ϵ2(3 + s2
k −hu2
k)."
REFERENCES,0.5187687687687688,"Hence if ϵ is very small, we have sk+1 < sk. Also, xk+2yk+2 −µ = sk+1(x⊤
k+1yk+1 −µ) =
−sk+1skϵ. Moreover, u2
k+1 ≤2"
REFERENCES,0.5195195195195195,h + 2µ by Lemma G.6.
REFERENCES,0.5202702702702703,Published as a conference paper at ICLR 2022
REFERENCES,0.521021021021021,"sk+2 = 1 −hu2
k+2 −h2x⊤
k+2yk+2(µ −x⊤
k+2yk+2)"
REFERENCES,0.5217717717717718,"= sk −h2ϵµ(3 + 4sk + sksk+1) + h2ϵ2(3 + s2
k −hu2
k + s2
k(3 + s2
k+1 −hu2
k+1)))"
REFERENCES,0.5225225225225225,"≥sk −h2ϵµ(3 + 4sk + sksk+1) + h2ϵ2(1 + (1 + s2
k+1)s2
k)"
REFERENCES,0.5232732732732732,"sk+1=µ/(2skϵ)
≥
sk −h2µ2"
REFERENCES,0.524024024024024,"4
−h2ϵ(3 + 4sk) + h2ϵ2(1 + s2
k)"
REFERENCES,0.5247747747747747,"If 3 + 4sk ≥0, then"
REFERENCES,0.5255255255255256,sk+2 ≥sk −h2µ2
REFERENCES,0.5262762762762763,"4
−h2ϵ(3 + 4sk) + h2ϵ2(1 + s2
k)"
REFERENCES,0.527027027027027,"ϵ=(3+4sk)µ/(2(1+s2
k))
≥
sk −h2µ2"
REFERENCES,0.5277777777777778,"4
−h2µ2(3 + 4sk)2"
REFERENCES,0.5285285285285285,"4(1 + s2
k)"
REFERENCES,0.5292792792792793,"sk=0 or sk=−3/4
≥
min{−3"
REFERENCES,0.53003003003003,4 −h2µ2
REFERENCES,0.5307807807807807,"4
, −5h2µ2"
REFERENCES,0.5315315315315315,"2
} > −1."
REFERENCES,0.5322822822822822,"If 3 + 4sk < 0, i.e., −1 < sk < −3"
REFERENCES,0.5330330330330331,"4, then"
REFERENCES,0.5337837837837838,"sk+2 = sk −h2ϵµ(3 + 4sk + sksk+1) + h2ϵ2(3 + s2
k −hu2
k + s2
k(3 + s2
k+1 −hu2
k+1))"
REFERENCES,0.5345345345345346,"= sk −h2ϵµ(3 + 4sk + sk(sk −h2ϵµ(3 + sk) + h2ϵ2(3 + s2 −hu2
k)))"
REFERENCES,0.5352852852852853,"+ h2ϵ2(3 + s2
k −hu2
k + s2
k(3 + s2
k+1 −hu2
k+1))"
REFERENCES,0.536036036036036,"= sk −h2µϵ(sk(3 + sk) + 3 + sk) −h4µϵ3sk(3 + s2
k −hu2
k)"
REFERENCES,0.5367867867867868,"+ h2ϵ2(3 + s2
k −hu2
k + s2
k(3 + s2
k+1 −hu2
k+1) + h2µ2sk(3 + sk))."
REFERENCES,0.5375375375375375,"When −1 < sk <
1−3h2ϵµ"
REFERENCES,0.5382882882882883,"h2ϵµ
, sk −h2µϵ(sk(3 + sk) + 3 + sk) > −1. Since ϵ ≤2µ, we have"
REFERENCES,0.539039039039039,1−3h2ϵµ
REFERENCES,0.5397897897897898,"h2ϵµ
≥1−6h2µ2"
REFERENCES,0.5405405405405406,"2h2µ2
≥−3"
REFERENCES,0.5412912912912913,4 for 9
REFERENCES,0.5420420420420421,"2h2µ2 < 1. Also,"
REFERENCES,0.5427927927927928,"−h4µϵ3sk(3 + s2
k −hu2
k) > 3"
REFERENCES,0.5435435435435435,"4h4µϵ3 > 0,"
REFERENCES,0.5442942942942943,"h2ϵ2(3 + s2
k −hu2
k + s2
k(3 + s2
k+1 −hu2
k+1) + h2µ2sk(3 + sk)) > h2ϵ2 > 0. Hence"
REFERENCES,0.545045045045045,"sk+2 > −1 −h4µϵ3sk(3 + s2
k −hu2
k)"
REFERENCES,0.5457957957957958,"+ h2ϵ2(3 + s2
k −hu2
k + s2
k(3 + s2
k+1 −hu2
k+1) + h2µ2sk(3 + sk)) > −1 + h2ϵ2"
REFERENCES,0.5465465465465466,"Lemma G.13. When x⊤
k yk > µ and 1"
REFERENCES,0.5472972972972973,"h + 2hµ2 < u2
k ≤2"
REFERENCES,0.5480480480480481,"h + 2µ, if sk > −1, then sk+1 > sk and
sk+2 > min{−1 + h2(µ −x⊤
k yk)2( 1 2 −3"
REFERENCES,0.5487987987987988,"2h2µ2), −1 2 −3"
REFERENCES,0.5495495495495496,"2h4(µ −x⊤
k yk)2µ2} > −1, for 8h2µ2 < 1.
Moreover, if u2
k+1 ≤2"
REFERENCES,0.5503003003003003,"h, sk ≤−1, and sk+1 > −1, then sk+2 > sk + h2(µ −x⊤
k yk)2(1 −8h2µ2)."
REFERENCES,0.551051051051051,"Proof. From previous statement, sk < 0 when u2
k > 1"
REFERENCES,0.5518018018018018,"h + 2hµ2. Without loss of generality, we can
just assume u2
k < 3"
REFERENCES,0.5525525525525525,"h (otherwise u2
k+2 < 3"
REFERENCES,0.5533033033033034,"h and we can use this as our starting point). For x⊤
k yk > µ,
sk = 1 −hu2
k −h2x⊤
k yk(µ −x⊤
k yk). Let x⊤
k yk −µ = ϵ. Then"
REFERENCES,0.5540540540540541,"sk+1 = 1 −hu2
k+1 −h2x⊤
k+1yk+1(µ −x⊤
k+1yk+1) = sk + h2ϵµ(3 + sk) + h2ϵ2(3 + s2
k −hu2
k) > sk,"
REFERENCES,0.5548048048048048,"where sk > −3 in this region. Also, x⊤
k+2yk+2 −µ = sk+1skϵ."
REFERENCES,0.5555555555555556,"Then for sk ≤−1, u2
k+1 ≤2"
REFERENCES,0.5563063063063063,"h, and sk+1 > −1,"
REFERENCES,0.5570570570570571,"sk+2 = sk + h2ϵµ(3 + 4sk + sksk+1) + h2ϵ2(3 + s2
k −hu2
k + s2
k(3 + s2
k+1 −hu2
k+1))"
REFERENCES,0.5578078078078078,"= sk + h2ϵµ(3 + 4sk+1 + s2
k+1) + h2ϵ2
(3 + s2
k −hu2
k)(1 −4h2ϵµ −h2ϵµsk+1)"
REFERENCES,0.5585585585585585,"+ s2
k(2 −hu2
k+1) + s2
k(1 + s2
k+1) −h2µ2(4 + sk+1)(3 + sk)
"
REFERENCES,0.5593093093093093,> sk + h2ϵ2(1 −8h2µ2)
REFERENCES,0.56006006006006,Published as a conference paper at ICLR 2022
REFERENCES,0.5608108108108109,"where the inequality is achieved by sk+1 > −1, u2
k ≤4"
REFERENCES,0.5615615615615616,"h, u2
k+1 ≤2"
REFERENCES,0.5623123123123123,"h, 8h2µ2 < 1, ϵ ≤2µ."
REFERENCES,0.5630630630630631,"For sk > −1,"
REFERENCES,0.5638138138138138,"sk+2 = sk + h2ϵµ(3 + 4sk + s2
k) + h2ϵ2
(3 + s2
k −hu2
k)(1 + skh2ϵµ)"
REFERENCES,0.5645645645645646,"+ s2
k(3 + s2
k+1 −hu2
k+1) + h2µ2sk(3 + sk)
"
REFERENCES,0.5653153153153153,"> sk + h2ϵµ(3 + 4sk + s2
k) + h2ϵ2
s2
k(1 + 2skh2µ2) + s2
k + h2µ2sk(3 + sk)
"
REFERENCES,0.566066066066066,"= sk + h2ϵµ(3 + 4sk + s2
k) + h2ϵ2
2s2
k + h2µ2sk(3 + 2s2
k + sk)
"
REFERENCES,0.5668168168168168,"When sk > −1, we have sk + h2ϵµ(3 + 4sk + s2
k) > −1 and h2ϵµ(3 + 4sk + s2
k) > 0. Since
8h2µ2 < 1, 2s2
k + h2µ2sk(3 + 2s2
k + sk) ≥1 2 −3"
REFERENCES,0.5675675675675675,2h2µ2 > 0 for −1 < sk ≤−1
REFERENCES,0.5683183183183184,2. For −1
REFERENCES,0.5690690690690691,"2 <
sk ≤0, 2s2
k + h2µ2sk(3 + 2s2
k + sk) ≥h2µ2sk(3 + 2s2
k + sk) ≥−3"
REFERENCES,0.5698198198198198,2h2µ2 > −1
HENCE,0.5705705705705706,"2. Hence
sk+2 > min{−1 + h2ϵ2( 1 2 −3"
HENCE,0.5713213213213213,"2h2µ2), −1 2 −3"
HENCE,0.5720720720720721,2h4ϵ2µ2} > −1.
HENCE,0.5728228228228228,"Lemma G.14. Given u2
k ≤2"
HENCE,0.5735735735735735,"h, when |x⊤
k yk| > µ, u2
k+1 −u2
k < 0."
HENCE,0.5743243243243243,Proof.
HENCE,0.575075075075075,"u2
k+1 −u2
k = h(µ −x⊤
k yk)(4x⊤
k yk + hu2
k(µ −x⊤
k yk))"
HENCE,0.5758258258258259,"= h(µ −x⊤
k yk)4x⊤
k yk + h2u2
k(µ −x⊤
k yk)2"
HENCE,0.5765765765765766,"≤h(µ −x⊤
k yk)4x⊤
k yk + 2h(µ −x⊤
k yk)2"
HENCE,0.5773273273273273,"= 2h(µ2 −(x⊤
k yk)2) < 0"
HENCE,0.5780780780780781,"Lemma G.15. Given u2
k ≤2"
HENCE,0.5788288288288288,"h, u2
k+1 −u2
k ≤µ."
HENCE,0.5795795795795796,Proof.
HENCE,0.5803303303303303,"u2
k+1 −u2
k = h(µ −x⊤
k yk)(4x⊤
k yk + hu2
k(µ −x⊤
k yk)) ≤
4hµ2"
HENCE,0.581081081081081,"4 −hu2
k
,"
HENCE,0.5818318318318318,"where the maximum is achieved at x⊤
k yk =
2−hu2
k
4−hu2
k µ. When h =
4
u2
0+4cµ, from the proof of"
HENCE,0.5825825825825826,"Lemma G.9, uk+1 −u2
k ≤µ. When h =
1
(2+c)µ and in this case u2
k ≤2"
HENCE,0.5833333333333334,h = (1 + c
HENCE,0.5840840840840841,"2µ), uk+1 −u2
k ≤"
HENCE,0.5848348348348348,"8
7(2+c)µ < µ."
HENCE,0.5855855855855856,"Lemma G.16. When −µ ≤x⊤
k yk < µ, if sk = 1−hu2
k −h2x⊤
k yk(µ −x⊤
k yk) ≤−1, u2
k+2 −u2
k ≤
−h(4 −4hµ)(µ −x⊤
k yk)2 < 0."
HENCE,0.5863363363363363,Proof.
HENCE,0.5870870870870871,"u2
k+2 −u2
k = h(µ −x⊤
k yk)(((4 −hu2
k)x⊤
k yk + hu2
kµ) + s(hu2
k+1µ + (4 −hu2
k+1)(s(x⊤
k yk −µ) + µ)))"
HENCE,0.5878378378378378,"= h(µ −x⊤
k yk)
 
−(4 −hu2
k + s2(4 −hu2
k+1))(µ −x⊤
k yk) + 4(1 + s)µ
"
HENCE,0.5885885885885885,"≤−h(8 −h(u2
k + u2
k+1))(µ −x⊤
k yk)2"
HENCE,0.5893393393393394,"≤−h(4 −4hµ)(µ −x⊤
k yk)2 < 0"
HENCE,0.5900900900900901,"where the ﬁrst inequality is because when sk = −1 it has the maximum value; the second is from
Lemma G.10."
HENCE,0.5908408408408409,"Lemma G.17. When u2
k ≤2"
HENCE,0.5915915915915916,"h, then sk < 1 and will be away from 1 by a constant."
HENCE,0.5923423423423423,Published as a conference paper at ICLR 2022
HENCE,0.5930930930930931,Proof.
HENCE,0.5938438438438438,"sk = 1 −hu2
k −h2x⊤
k yk(µ −x⊤
k yk) ≤1 −hu2
k + h2 u2
k
2"
HENCE,0.5945945945945946,"
µ + u2
k
2 "
HENCE,0.5953453453453453,"where the equality is achieved when x⊤
k yk = −u2
k
2 ."
HENCE,0.5960960960960962,"Since 0 < u2
k ≤2 h,"
HENCE,0.5968468468468469,"sk ≤1 −hu2
k + h2 u2
k
2"
HENCE,0.5975975975975976,"
µ + u2
k
2"
HENCE,0.5983483483483484,"
≤max{1, hµ} < 1."
HENCE,0.5990990990990991,"Actually this sk will be away from 1 by a constant. This is because when u2
k are close to 0, sk will be
close to 1; however, u2
k will be increasing because of the decrease of |µ −x⊤
k yk| and thus sk will
decrease accordingly."
HENCE,0.5998498498498499,"Lemma G.18. When u2
k > 1"
HENCE,0.6006006006006006,"h + 2hµ2 and −µ ≤x⊤
k yk ≤3µ, (x⊤
k+1yk+1 −µ)(x⊤
k yk −µ) < 0,
meaning it oscillates around x⊤y = µ."
HENCE,0.6013513513513513,Proof.
HENCE,0.6021021021021021,"1 −hu2
k −h2x⊤
k yk(µ −x⊤
k yk) < 1 −h( 1"
HENCE,0.6028528528528528,h + 2hµ2) + 2h2µ2 = 0
HENCE,0.6036036036036037,"Lemma G.19. When −µ ≤x⊤
k yk < µ and hµ ≤1"
HENCE,0.6043543543543544,"3, we have |xk+1 −yk+1| < |xk −yk|."
HENCE,0.6051051051051051,Proof.
HENCE,0.6058558558558559,"|xk+1 −yk+1| = |xk −yk| |1 −h(µ −x⊤
k yk)|."
HENCE,0.6066066066066066,"Since hµ < 1, then"
HENCE,0.6073573573573574,"−1 < 1 −2hµ ≤1 −h(µ −x⊤
k yk) < 1."
HENCE,0.6081081081081081,"Lemma G.20. When u2
k ≤1"
HENCE,0.6088588588588588,h + 2hµ2 and hµ ≤1
HENCE,0.6096096096096096,"3, sk ≥−1 4."
HENCE,0.6103603603603603,"Proof. By hµ ≤1 3,"
HENCE,0.6111111111111112,"1 −hu2
k −h2x⊤
k yk(µ −x⊤
k yk) ≥−2h2µ2 −h2µ2"
HENCE,0.6118618618618619,"4
≥−9h2µ2 4
≥−1 4."
HENCE,0.6126126126126126,"H
PROOF OF THEOREM 4.1, THEOREM 4.2, AND THEOREM 4.3"
HENCE,0.6133633633633634,"The proof in this section mainly follows the strategy of the scalar case. More precisely, all the
expressions, e.g., u2
k+1 and x⊤
k+1yk+1 −µ, in this rank-1 approximation case contains two parts: the
one that can be handled using the technique in scalar case, and the other one measuring the extent of
alignment between xk and yk. Again, let u2
k = ∥xk∥2 + ∥yk∥2."
HENCE,0.6141141141141141,"Theorem H.1 (Main 1). Let h =
4
u2
0+4cµ. Assume c ≥
√"
HENCE,0.6148648648648649,"7 and u2
0 > 4
√"
HENCE,0.6156156156156156,7µ. Then GD converges to
HENCE,0.6163663663663663,"a point in {(x, y) : ∥x∥2 + ∥y∥2 ≤2"
HENCE,0.6171171171171171,"h, x⊤y = µ} except for a Lebesgue measure-0 set of (x0, y0)."
HENCE,0.6178678678678678,Proof. The proof follows from Lemma H.3 and Lemma H.8.
HENCE,0.6186186186186187,Published as a conference paper at ICLR 2022
HENCE,0.6193693693693694,"Theorem H.2 (Main 2). Let h =
4
4
√"
HENCE,0.6201201201201201,"7µ+4cµ =
1
(
√"
HENCE,0.6208708708708709,"7+c)µ. Assume c ≥
√"
HENCE,0.6216216216216216,"7 and u2
0 ≤4
√"
HENCE,0.6223723723723724,7µ. Then
HENCE,0.6231231231231231,"GD converges to a point in {(x, y) : ∥x∥2 + ∥y∥2 ≤2"
HENCE,0.6238738738738738,"h, x⊤y = µ} except for a Lebesgue measure-0
set of (x0, y0)."
HENCE,0.6246246246246246,"Proof. Since c ≥
√"
HENCE,0.6253753753753754,"7, in this case we have u2
0 ≤4
√"
HENCE,0.6261261261261262,"7µ ≤2(
√"
HENCE,0.6268768768768769,"7 + c)µ =
2
h. Therefore, the
convergence follows from Lemma H.8."
HENCE,0.6276276276276276,Hence we can show the proof of the theorems in Section 4.
HENCE,0.6283783783783784,"Proof of Theorem 4.1. By Lemma H.3 and H.7, the proof follows from Lemma H.9."
HENCE,0.6291291291291291,"Proof of Theorem 4.2 and 4.3. Similar to scalar case, h =
4
u2
0+4cµ for c ≥
√"
HENCE,0.6298798798798799,"7 implies h ≤
4
u2
0+4
√"
HENCE,0.6306306306306306,"7µ
and h =
1
(
√"
HENCE,0.6313813813813813,"7+c)µ implies h ≤
1
2
√"
HENCE,0.6321321321321322,"7. Therefore, the convergence and balancing of GD follow from"
HENCE,0.6328828828828829,"Theorem H.1 and H.2. The second inequality of Theorem 4.3 follows from x⊤y = µ at the global
minimum because of diagonal and non-negative A and the best rank-1 approximation in SVD."
HENCE,0.6336336336336337,"H.1
PHASE 1"
HENCE,0.6343843843843844,The main theorem in phase 1 is the following.
HENCE,0.6351351351351351,"Lemma H.3. Given h =
4
u2
0+4cµ for c ≥
√"
HENCE,0.6358858858858859,"7, GD will enter {∥x∥2 + ∥y∥2 ≤
2
h} except for a
measure-0 set of initial conditions."
HENCE,0.6366366366366366,"Proof. Since the expressions, u2
k+2−u2
k and u2
k+2−u2
k, of the rank-1 version have an additional term
upper bounded by c0((x⊤
k yk)2 −x⊤
k xky⊤
k yk) for some constant c0 > 0 which is non-positive, we
have u2
k decreases by a constant for either one- or two-step if (x⊤
k yk)2 −x⊤
k xky⊤
k yk < −c1 for some
constant c1 > 0 when 2"
HENCE,0.6373873873873874,"h < u2
k < 4"
HENCE,0.6381381381381381,"h. Therefore, we can pick a K > 0 s.t. (x⊤
k yk)2 −x⊤
k xky⊤
k yk <
−c1 for k ≤K. If GD enters {∥x∥2 + ∥y∥2 ≤2"
HENCE,0.6388888888888888,"h} within K steps, then we are done. Otherwise,
we can just assume that for all the k > K, −c1 ≤(x⊤
k yk)2 −x⊤
k xky⊤
k yk ≤0. Then again, like"
HENCE,0.6396396396396397,"the scalar case, we only need to consider when xk, yk are close to x⊤
k yk = µ and x⊤
k yk = −hµu2
k
4−hu2
k ."
HENCE,0.6403903903903904,"For the region near x⊤
k yk = −hµu2
k
4−hu2
k , we can always choose a c1 s.t. there exists a δ > 0, when"
HENCE,0.6411411411411412,"−δ ≤x⊤
k yk +
hµu2
k
4−hu2
k < 0, we have both x⊤
k+1yk+1 > µ and sk < 0. Then all the discussion just"
HENCE,0.6418918918918919,"follows the scalar case. For the region near x⊤
k yk = µ, i.e., |x⊤
k yk −µ| < ϵ for some 0 < ϵ < µ,
consider"
HENCE,0.6426426426426426,"x⊤
k+1yk+1 −µ = (x⊤
k yk −µ)sk + h2(2µ −x⊤
k yk)((x⊤
k yk)2 −x⊤
k xky⊤
k yk)."
HENCE,0.6433933933933934,"Similar to scalar decomposition, we only need to consider the case when sk ≤−1 because if sk > −1
which can only happen when x⊤
k yk > µ, we have u2
k ≤2"
HENCE,0.6441441441441441,"h + ϵkh(µ + ϵ) and then u2
k+1 ≤2"
HENCE,0.6448948948948949,"h. If
sk ≥−1 for all the following iterations, together with the bounded second term in x⊤
k+1yk+1 −µ,
i.e., −2h2µc1 ≤h2(2µ −x⊤
k yk)((x⊤
k yk)2 −x⊤
k xky⊤
k yk) ≤0, we have that there exists K > 0 s.t.
|x⊤
KyK −µ| ≥ϵ1 for some ϵ1 > 0 unless it leaves this region |x⊤
k yk −µ| < ϵ. Therefore, by the
same discussion in scalar case, u2
k will decrease either in one step or two steps by at least a constant."
HENCE,0.6456456456456456,"Moreover, by Lemma H.4, GD will not terminate inside { 2"
HENCE,0.6463963963963963,h < ∥x∥2 + ∥y∥2 < 4
HENCE,0.6471471471471472,"h} in ﬁnite step.
Thus, GD is guaranteed to enter {∥x∥2 + ∥y∥2 ≤2"
HENCE,0.6478978978978979,h} in ﬁnite steps.
HENCE,0.6486486486486487,Lemma H.4. GD will not converge to any ﬁxed points in {∥x∥2 +∥y∥2 > 2
HENCE,0.6493993993993994,"h} except for a measure-0
set."
HENCE,0.6501501501501501,"Proof. Similar to the discussion in scalar case, the set of points converging to x⊤y = µ in ﬁnite step
is measure-0."
HENCE,0.6509009009009009,Published as a conference paper at ICLR 2022
HENCE,0.6516516516516516,"Also, since µx⊤y = x⊤xy⊤y = µ2, the ﬁxed points of GD map requires x⊤xy⊤y = (x⊤y)2. This
can be seen in the following expression"
HENCE,0.6524024024024024,"x⊤
k+1yk+1 −µ = (x⊤
k yk −µ)(1 −hu2
k −h2x⊤
k yk(µ −x⊤
k yk)) + h2(2µ −x⊤
k yk)((x⊤
k yk)2 −x⊤
k xky⊤
k yk),"
HENCE,0.6531531531531531,"where the convergence of x⊤
k yk −µ requires the convergence of x⊤
k xky⊤
k yk −(x⊤
k yk)2."
HENCE,0.6539039039039038,"Hence if x⊤
k xky⊤
k yk −(x⊤
k yk)2 does not converge, then GD will not converge. If x⊤
k xky⊤
k yk −
(x⊤
k yk)2 converges, then c ≥x⊤
k xky⊤
k yk −(x⊤
k yk)2 ≥0 for all the rest of the iterations with some
c > 0, and we further assume |x⊤
k yk −µ| < ϵ for any 0 < ϵ < µ. As is shown in scalar case, when
u2
k ≥2"
HENCE,0.6546546546546547,"h + ϵh(µ + ϵ), we have sk = 1 −hu2
k −h2x⊤
k yk(µ −x⊤
k yk) ≥−1. Then"
HENCE,0.6554054054054054,"x⊤
k+1yk+1 −µ = (x⊤
k yk −µ)sk + h2(2µ −x⊤
k yk)((x⊤
k yk)2 −x⊤
k xky⊤
k yk)"
HENCE,0.6561561561561562,will never converge (because sk ≤−1 and the second term is bounded).
HENCE,0.6569069069069069,"Lemma H.5. When x⊤
k yk > µ or x⊤
k yk < −hµu2
k
4−hu2
k , we have u2
k+1 −u2
k < 0."
HENCE,0.6576576576576577,Proof.
HENCE,0.6584084084084084,"u2
k+1 −u2
k = h(µ −x⊤
k yk)((4 −hu2
k)x⊤
k yk + hu2
kµ) + h(4 −hu2
k)((x⊤
k yk)2 −x⊤
k xky⊤
k yk)"
HENCE,0.6591591591591591,"≤h(µ −x⊤
k yk)((4 −hu2
k)x⊤
k yk + hu2
kµ)."
HENCE,0.6599099099099099,Hence the proof follows 1D case.
HENCE,0.6606606606606606,"Lemma H.6. When −hµu2
k
4−hu2
k ≤x⊤
k yk < µ, we have u2
k+2 −u2
k < 0."
HENCE,0.6614114114114115,"Proof. Let sk = 1 −hu2
k −h2x⊤
k yk(µ −x⊤
k yk), the same as 1D."
HENCE,0.6621621621621622,"x⊤
k+1yk+1 −µ = (x⊤
k yk −µ)(1 −hu2
k −h2x⊤
k yk(µ −x⊤
k yk)) + h2(2µ −x⊤
k yk)((x⊤
k yk)2 −x⊤
k xky⊤
k yk)"
HENCE,0.6629129129129129,"∆= (x⊤
k yk −µ)sk + M."
HENCE,0.6636636636636637,"u2
k+2 −u2
k = u2
k+1 −u2
k + u2
k+2 −u2
k+1
= h(µ −x⊤
k yk)((4 −hu2
k)x⊤
k yk + hu2
kµ) + h(4 −hu2
k)((x⊤
k yk)2 −x⊤
k xky⊤
k yk)"
HENCE,0.6644144144144144,"+ h(µ −x⊤
k+1yk+1)((4 −hu2
k+1)x⊤
k+1yk+1 + hu2
k+1µ)"
HENCE,0.6651651651651652,"+ h(4 −hu2
k+1)((x⊤
k+1yk+1)2 −x⊤
k+1xk+1y⊤
k+1yk+1)"
HENCE,0.6659159159159159,"=

h(µ −x⊤
k yk)(((4 −hu2
k)x⊤
k yk + hu2
kµ) + sk(hu2
k+1µ + (4 −hu2
k+1)(sk(x⊤
k yk −µ) + µ)))
"
HENCE,0.6666666666666666,"+

h(4 −hu2
k)((x⊤
k yk)2 −x⊤
k xky⊤
k yk) + hM 2(−4 + hu2
k+1)"
HENCE,0.6674174174174174,"+ hM(−hu2
k+1µ + (−4 + hu2
k+1)µ + 2(4 −hu2
k+1)(µ −x⊤
k yk)sk)
"
HENCE,0.6681681681681682,∆= I + II.
HENCE,0.668918918918919,"I is the same as 1D u2
k+2 −u2
k and hence (1) < 0. For II,"
HENCE,0.6696696696696697,"II = hM 2(−4 + hu2
k+1) + h((x⊤
k yk)2 −x⊤
k xky⊤
k yk)"
HENCE,0.6704204204204204,"× (4 −hu2 + h2(2µ −x⊤
k yk)(−hu2
k+1µ + (−4 + hu2
k+1)µ + 2(4 −hu2
k+1)(µ −x⊤
k yk)sk)
|
{z
}
III )."
HENCE,0.6711711711711712,"From 1D results, sk < 0. Then, III achieves its lower bound at x⊤
k yk = µ or x⊤
k yk = −hµu2
k
4−hu2
k . For"
HENCE,0.6719219219219219,"x⊤
k yk = µ,"
HENCE,0.6726726726726727,"III = 4 −hu2
k −4h2µ2 ≥4hµ(c −hµ) > 0,"
HENCE,0.6734234234234234,Published as a conference paper at ICLR 2022
HENCE,0.6741741741741741,"where the inequality is from u2
k ≤u2
0 = 4"
HENCE,0.674924924924925,h −4cµ and c > hµ.
HENCE,0.6756756756756757,"For x⊤
k yk = −hµu2
k
4−hu2
k ,"
HENCE,0.6764264264264265,"III = 4 −hu2
k −4h2(8 −hu2
k)(4 −hu2
k −2sk(4 −hu2
k+1))µ2"
HENCE,0.6771771771771772,"(4 −hu2
k)2"
HENCE,0.6779279279279279,"≥4 −hu2
k −4h2(8 −hu2
k)(4 −hu2
k −2sk(4 −hu2
k))µ2"
HENCE,0.6786786786786787,"(4 −hu2
k)2"
HENCE,0.6794294294294294,"= 4 −hu2
k −4h2(8 −hu2
k)(1 −2sk)µ2"
HENCE,0.6801801801801802,"4 −hu2
k"
HENCE,0.6809309309309309,"u2
k≤u2
0
≥
4 −hu2
k −4h2(4 + 4chµ)(1 −2sk)µ2 4chµ"
HENCE,0.6816816816816816,"u2
k≤u2
0
≥
4hµ"
HENCE,0.6824324324324325,c (−1 + c2 + 2sk + ch(−1 + 2sk)µ)
HENCE,0.6831831831831832,"∗
≥c2(1 + 8h2µ2) + c(hµ −h3µ3"
HENCE,0.683933933933934,"2
) −7 −h2µ2"
HENCE,0.6846846846846847,"2
∗∗
≥0,
where ∗follows from
sk = 1 −hu2
k −h2x⊤
k yk(µ −x⊤
k yk)"
HENCE,0.6854354354354354,"x⊤
k yk=µ/2
≥
1 −hu2
k −h2µ2 4"
HENCE,0.6861861861861862,"u2
k≤u2
0
≥
−3 + 4chµ −h2µ2 4
,"
HENCE,0.6869369369369369,"and ∗∗follows from c ≥
√ 7."
HENCE,0.6876876876876877,"From 1D discussion, −4 + hu2
k+1 < 0. Also, (x⊤
k yk)2 −x⊤
k xky⊤
k yk < 0. Hence we have (3) ≥0."
HENCE,0.6884384384384384,"Overall, (2) ≤0 and therefore the whole discussion of (1) is the same as 1D."
HENCE,0.6891891891891891,"H.2
PHASE 2"
HENCE,0.68993993993994,"The proof in phase 2 is partly different from the scalar case due to the alignment. Before presenting
the main convergence lemma, we ﬁrst state the boundedness of GD in the following lemma.
Lemma H.7. Once GD enters {∥x∥2 + ∥y∥2 ≤2"
HENCE,0.6906906906906907,"h}, it will stay bounded inside {∥x∥2 + ∥y∥2 ≤
2
h + 2hµ2(1 +
1
1−h2µ2 )} and re-enter {∥x∥2 + ∥y∥2 ≤2"
HENCE,0.6914414414414415,"h} within ﬁnite steps where the number of
such steps do not depend on the number of iteration."
HENCE,0.6921921921921922,"Proof. Assume u2
k ≤2"
HENCE,0.6929429429429429,h. Then
HENCE,0.6936936936936937,"u2
k+1 −u2
k = h(µ −x⊤
k yk)((4 −hu2
k)x⊤
k yk + hu2
kµ) + h(4 −hu2
k)((x⊤
k yk)2 −x⊤
k xky⊤
k yk)"
HENCE,0.6944444444444444,"≤2h(µ2 −x⊤
k xky⊤
k yk) ≤2hµ2,"
HENCE,0.6951951951951952,"where the ﬁrst inequality follows from u2
k ≤2"
HENCE,0.6959459459459459,"h. If further u2
k+1 ≤2"
HENCE,0.6966966966966966,"h + 2hµ2, then by x⊤
k+1yk+1 ≥ −
q"
HENCE,0.6974474474474475,"x⊤
k+1xk+1y⊤
k+1yk+1 and −1 + h2µ2 < 0, we have"
HENCE,0.6981981981981982,"u2
k+2 −u2
k+1 = h(µ −x⊤
k+1yk+1)((4 −hu2
k+1)x⊤
k+1yk+1 + hu2
k+1µ)"
HENCE,0.698948948948949,"+ h(4 −hu2
k+1)((x⊤
k+1yk+1)2 −x⊤
k+1xk+1y⊤
k+1yk+1)"
HENCE,0.6996996996996997,"≤2h(x⊤
k+1xk+1y⊤
k+1yk+1(−1 + h2µ2) + µ2(1 −2x⊤
k+1yk+1h2µ + h2µ2))"
HENCE,0.7004504504504504,"≤2h(x⊤
k+1xk+1y⊤
k+1yk+1(−1 + h2µ2) + µ2(1 + 2
q"
HENCE,0.7012012012012012,"x⊤
k+1xk+1y⊤
k+1yk+1h2µ + h2µ2))"
HENCE,0.7019519519519519,"≤
2hµ2"
HENCE,0.7027027027027027,1 −h2µ2 .
HENCE,0.7034534534534534,Published as a conference paper at ICLR 2022
HENCE,0.7042042042042042,"The ﬁrst inequality is from u2
k+1 ≤2"
HENCE,0.704954954954955,h + 2hµ2; the second inequality follows from the two conditions
HENCE,0.7057057057057057,"mentioned above; the third inequality follows from choosing
q"
HENCE,0.7064564564564565,"x⊤
k+1xk+1y⊤
k+1yk+1 =
h2µ3"
HENCE,0.7072072072072072,1−h2µ2 .
HENCE,0.7079579579579579,"From previous proof, we know when u2
k >
2
h, by Lemma H.3, we have GD will enter {∥x∥2 +
∥y∥2 ≤
2
h} in ﬁnite steps with the decrease of u2
k in either one step or two steps. Then, u2
i ≤
2
h + 2hµ2(1 +
1
1−h2µ2 ) for all i ≥k."
HENCE,0.7087087087087087,"Let Uk = x⊤
k yk, Vk = x⊤
k xk, Wk = y⊤
k yk. Then we can rewrite the iteration for the three variables

 "
HENCE,0.7094594594594594,"Uk+1 = h(1 −hVk)µWk + hµVk(1 −hWk) + Uk(1 + h2VkWk −h(Vk + Wk) + h2µ2)
Vk+1 = 2hµUk + Vk −2hVkWk + h2(µ2Wk −2µUkWk + VkW 2
k )
Wk+1 = 2hµUk + Wk −2hVkWk + h2(µ2Vk −2µUkVk + V 2
k Wk)
. (10)"
HENCE,0.7102102102102102,"When h =
4
u2
0+4cµ or h =
1
2
√"
HENCE,0.710960960960961,"7µ, we have hµ ≤
1
2
√"
HENCE,0.7117117117117117,7. The following is the main lemma in phase 2.
HENCE,0.7124624624624625,"Lemma H.8. Given hµ ≤
1
2
√"
HENCE,0.7132132132132132,"7, if GD enters {∥x∥2 +∥y∥2 ≤2"
HENCE,0.713963963963964,"h}, it converges to x⊤y = ∥x∥∥y∥=
µ, i.e., the global minimum, inside this region except for a measure-0 set of initial conditions."
HENCE,0.7147147147147147,"Proof. This proof follows the same method as the scalar decomposition. We need to prove that all
the conclusions in scalar case also holds in this rank one decomposition."
HENCE,0.7154654654654654,"Consider u2
k+1 −u2
k when x⊤
k yk < −µ or x⊤
k yk > µ,"
HENCE,0.7162162162162162,"u2
k+1 −u2
k = h(µ −x⊤
k yk)((4 −hu2
k)x⊤
k yk + hu2
kµ) + h(4 −hu2
k)((x⊤
k yk)2 −x⊤
k xky⊤
k yk)"
HENCE,0.7169669669669669,"= h(µ −x⊤
k yk)((4 −hu2
k)x⊤
k yk + hu2
kµ) + h(4 −hu2
k)(U 2
k −VkWk)"
HENCE,0.7177177177177178,"≤h(µ −x⊤
k yk)((4 −hu2
k)x⊤
k yk + hu2
kµ),"
HENCE,0.7184684684684685,"where the last expression is the same as scalar case and therefore under the conditions such that
u2
k+1 −u2
k < 0 in scalar case, we also have u2
k+1 −u2
k < 0 in rank-one approximation."
HENCE,0.7192192192192193,"As in scalar case, we also denote sk = 1 −hu2
k −h2x⊤
k yk(µ −x⊤
k yk). First consider 1"
HENCE,0.71996996996997,"h + 2hµ2 <
u2
k ≤2"
HENCE,0.7207207207207207,"h. If −µ ≤x⊤
k yk < µ, u2
k+1 ≥u2
k, and sk ≤−1, consider u2
k+2 −u2
k. From Lemma H.6, the
extra terms of u2
k+2 −u2
k compared to scalar case is"
HENCE,0.7214714714714715,"hM 2(−4 + hu2
k+1) + h((x⊤
k yk)2 −x⊤
k xky⊤
k yk)"
HENCE,0.7222222222222222,"× (4 −hu2 + h2(2µ −x⊤
k yk)(−hu2
k+1µ + (−4 + hu2
k+1)µ + 2(4 −hu2
k+1)(µ −x⊤
k yk)sk)
|
{z
}
III )"
HENCE,0.722972972972973,"where M = h2(2µ −x⊤
k yk)((x⊤
k yk)2 −x⊤
k xky⊤
k yk). Then we will prove that III is positive and
then all the extra terms will be less than 0. By rewriting III in terms of x⊤
k yk, we get"
HENCE,0.7237237237237237,"III = 2(x⊤
k yk)2h2sk(4 −hu2
k+1) + x⊤
k yk(h3u2
k+1µ −6h2sk(4 −hu2
k+1)µ −h2(−4 + hu2
k+1)µ)"
HENCE,0.7244744744744744,"−2h3u2
k+1µ2 + 4h2sk(4 −hu2
k+1)µ2 + 2h2(−4 + hu2
k+1)µ2 + 4 −hu2
k
≥4 −hu2
k + 12h2(−1 + 4sk)µ2 −12h3sku2
k+1µ2"
HENCE,0.7252252252252253,"≥4 −hu2
k + 12h2(−1 + 4sk)µ2 −12h3sku2
kµ2"
HENCE,0.725975975975976,"= 4 −hu2
k + 12h2(−1 + sk(4 −hu2
k))µ2 > 0,"
HENCE,0.7267267267267268,"where the ﬁrst inequality is because it obtains the minimum at x⊤
k yk = −µ; the second inequality
follows from u2
k+1 ≥u2
k; the last inequality is from the range of u2
k and sk. Therefore the extra
terms of u2
k+2 −u2
k is always negative and then we have the same conclusion as scalar case, i.e.,
u2
k+2 −u2
k < −C(µ −x⊤
k yk)2 for some constant C > 0."
HENCE,0.7274774774774775,Published as a conference paper at ICLR 2022
HENCE,0.7282282282282282,"Next, consider sk+1 when −µ ≤x⊤
k yk ≤3µ and 1"
HENCE,0.728978978978979,"h + 2hµ2 < u2
k ≤2"
HENCE,0.7297297297297297,h which implies −1 −h2µ2
HENCE,0.7304804804804805,"4
≤
sk < 0. The extra terms of sk+1 compared to scalar case is"
HENCE,0.7312312312312312,"h2(U 2
k −VkWk)

−(4 −hu2
k) + (−x⊤
k yk + 2µ)(2h2sk(x⊤
k yk −µ) + h2µ) + h4x(−x⊤
k yk + 2µ)2"
HENCE,0.7319819819819819,"|
{z
}
IV ,"
HENCE,0.7327327327327328,"where U 2
k −VkWk ≤0. Then consider the upper bound of IV"
HENCE,0.7334834834834835,"IV = −4 −2(x⊤
k yk)2h2sk + x⊤
k yk(−h2µ + 6h2sµ) + hu2
k + 2h2µ2 −4h2skµ2"
HENCE,0.7342342342342343,"≤−4 + hu2
k + (−h2 −4h2sk)µ2 < 0,"
HENCE,0.734984984984985,"where the ﬁrst inequality follows from x⊤
k yk ≤3µ. Therefore the extra terms is non-negative, namely
all the lower bound for sk+1 in the scalar case also holds for rank-one approximation."
HENCE,0.7357357357357357,"Also we need to consider sk+2 when sk > −1, −µ ≤x⊤
k yk ≤3µ, and 1"
HENCE,0.7364864864864865,"h + 2hµ2 < u2
k ≤2"
HENCE,0.7372372372372372,"h. For
scalar case, from Lemma G.13 with ϵ = x⊤
k+1yk+1 −µ, we have"
HENCE,0.737987987987988,"sk+2 = sk+1 + (h2(3 + s2
k+1) −h3u2
k)ϵ2 + h2ϵ(−3µ −sk+1µ)"
HENCE,0.7387387387387387,"= 3h2ϵ2 + h2s2
k+1ϵ2 −h3u2
kϵ2 −3h2ϵµ + sk+1(1 −h2ϵµ)."
HENCE,0.7394894894894894,Since the minimum point w.r.t. sk+1 is −(1−h2ϵµ)
HENCE,0.7402402402402403,"2h2ϵ2
< −1 −h2µ2"
HENCE,0.740990990990991,"4 , we have if sk+1 is larger, then sk+2
is also larger. Given the same value for sk for both scalar and rank-one cases, sk+1 is larger from
the above discussion and thus sk+2 is also larger, namely, the lower bound for sk+2 also holds in
rank-one cases."
HENCE,0.7417417417417418,"Next consider u2
k ≤1"
HENCE,0.7424924924924925,"h + 2hµ2 where |sk| < 1 and we need the iteration to have certain restriction
such that it will be bounded in a region. If sk < 0, then it follows the same from the above
discussion. If sk > 0 and x⊤
k yk > µ or x⊤
k yk < −µ, then by u2
k+1 −u2
k < 0, it will stay in
{x⊤
k yk > µ, u2
k ≤1"
HENCE,0.7432432432432432,"h + 2hµ2} until convergence. If sk > 0 and −µ ≤x⊤
k yk < µ, similar to scalar
case, we consider |x⊤
k xk −y⊤
k yk| = |Vk −Wk| and"
HENCE,0.743993993993994,"|Vk+1 −Wk+1| = |Vk −Wk| · |1 −h2VkWk + 2h2Ukµ −h2µ2|,"
HENCE,0.7447447447447447,"where Uk = x⊤
k yk and U 2
k ≤VkWk. Then"
HENCE,0.7454954954954955,"1 −h2VkWk + 2h2Ukµ −h2µ2 ≤1 −h2U 2
k + 2h2Ukµ −h2µ2 < 1,"
HENCE,0.7462462462462462,"1 −h2VkWk + 2h2Ukµ −h2µ2 ≥1 −h2 u4
k
4 −2h2µ −h2µ2 ≥1 −1"
HENCE,0.746996996996997,4(1 + 2h2µ2)2 −3h2µ2 > 0.
HENCE,0.7477477477477478,"Therefore if −µ ≤x⊤
k yk < µ and u2
k ≤
1
h + 2hµ2, we have |Vk+1 −Wk+1| < |Vk −Wk|,
meaning together with 0 ≤µ −x⊤
k+1yk+1 < µ −x⊤
k yk, xk+1 and yk+1 is bounded in the monotone
convergence region."
HENCE,0.7484984984984985,"So far we retrieve all the conditions for convergence. Therefore, from the proof of the convergence in
scalar case, we have that there exists a constant K > 0, s.t., |sk| < 1 and 0 < 2µ −x⊤
k yk < 2µ for
all k > K. Then let Sk = h2(2µ −x⊤
k yk) and we have"
HENCE,0.7492492492492493,"x⊤
k+1yk+1 −µ = (x⊤
k yk −µ)(1 −hu2
k −h2x⊤
k yk(µ −x⊤
k yk))"
HENCE,0.75,"+ h2(2µ −x⊤
k yk)((x⊤
k yk)2 −x⊤
k xky⊤
k yk)"
HENCE,0.7507507507507507,"= (x⊤
k yk −µ)sk + Sk(U 2
k −VkWk)"
HENCE,0.7515015015015015,"In fact x⊤
k yk −µ →0 as k →0. First, since Sk(U 2
k −VkWk) →0 as k →0 by Lemma H.9 and
|sk| < 1, we have x⊤
k yk −µ is bounded. Also, from previous discussion, |sk| ≤max{C2, 1 −
C3(x⊤
k yk −µ)2} for some constant C2, C3 > 0. For any ϵ0 > 0, there exists K1 > 0 and ϵ1 ≪ϵ0
such that |Sk(U 2
k −VkWk)| < ϵ1 for all k > K1 and C2 ≤1 −C3ϵ2
1. Therefore x⊤
k yk −µ will"
HENCE,0.7522522522522522,Published as a conference paper at ICLR 2022
HENCE,0.753003003003003,decreases to O(ϵ1). This is because
HENCE,0.7537537537537538,"|x⊤
k+1yk+1 −µ| ≤(1 −C3ϵ2
1)|x⊤
k yk −µ| + 2µ|U 2
K1 −VK1WK1|C2(k−K1)
0
≤|(x⊤
K1yK1 −µ)|(1 −C3ϵ2
1)k−K1"
HENCE,0.7545045045045045,"+ 2h2µ(VK1WK1 −U 2
K1) k
X"
HENCE,0.7552552552552553,"i=K1
C2(i−K1)
0
(1 −C3ϵ2
1)k−i"
HENCE,0.756006006006006,"≤|(x⊤
K1yK1 −µ)|(1 −C3ϵ2
1)k−K1 + 2h2µ(VK1WK1 −U 2
K1)(k −K1)Ck−K1
4"
HENCE,0.7567567567567568,"where C4 = max{C0, 1 −C3ϵ2
1} < 1 and we have kCk
4 →0 as k →∞. Therefore there exists
K2 ≥K1 s.t. x⊤
K2yK2 −µ = O(ϵ1). Then we will show for all k > K2, |x⊤
k yk −µ| < ϵ0. As is
shown before there exists K3 > K2 and ϵ2 ≪ϵ1 s.t. |Sk(U 2
k −VkWk)| < ϵ2. Since |sk| < 1, the
increase of |x⊤
k yk −µ| is also O(ϵ1) and thus |x⊤
k yk −µ| = O(ϵ1) < ϵ0 for k > K2. Then we have
the convergence of |x⊤
k yk −µ| →0."
HENCE,0.7575075075075075,"In general, we have x⊤
k yk →µ and | cos(∠(xk, yk))| →0, which implies GD converges to the global
minimum x⊤y = ∥x∥∥y∥= µ."
HENCE,0.7582582582582582,"For the proof of alignment, we only need to consider when ∥xk∥2 +∥yk∥2 ≤2"
HENCE,0.759009009009009,"h +2hµ2(1+
1
1−h2µ2 ).
Also, we have VkWk −U 2
k ≥0 and"
HENCE,0.7597597597597597,"Vk+1Wk+1 −U 2
k+1 = (VkWk −U 2
k)(1 −h(Vk + Wk) + h2(VkWk −µ2))2."
HENCE,0.7605105105105106,"Thus we denote lk = 1 −h(Vk + Wk) + h2(VkWk −µ2) which characterizes the change of
VkWk −U 2
k."
HENCE,0.7612612612612613,Lemma H.9. If ∥xk∥2 + ∥yk∥2 ≤2
HENCE,0.762012012012012,"h + 2hµ2(1 +
1
1−h2µ2 ), then VkWk −U 2
k converges to 0 and
also | cos(∠(xk, yk))| converges to 1."
HENCE,0.7627627627627628,Proof. By 0 ≤VkWk ≤( Vk+Wk
HENCE,0.7635135135135135,"2
)2 and the boundedness theorem, we have"
HENCE,0.7642642642642643,Vk + Wk ≤2
HENCE,0.765015015015015,"h + 2hµ2(1 +
1
1 −h2µ2 ), and"
HENCE,0.7657657657657657,"lk = 1 −h(Vk + Wk) + h2(VkWk −µ2) ≥−1 −h2µ2(3 +
2
1 −h2µ2 ),
(11)"
HENCE,0.7665165165165165,lk = 1 −h(Vk + Wk) + h2(VkWk −µ2) ≤h2
HENCE,0.7672672672672672,4 (Vk + Wk)2 −h(Vk + Wk) + 1 −h2µ2
HENCE,0.7680180180180181,≤1 −h2µ2 < 1.
HENCE,0.7687687687687688,"We will show that there exists K > 0 and a constant C > 0 only depending on hµ, s.t. when k > K,
|lk| < 1 −C."
HENCE,0.7695195195195195,If Vk + Wk < 2
HENCE,0.7702702702702703,"h −hµ2, then"
HENCE,0.771021021021021,lk = 1 −h(Vk + Wk) + h2(VkWk −µ2) > −1 + h2VkWk > −1.
HENCE,0.7717717717717718,"(If VkWk = 0, then VkWk −U 2
k = 0 meaning it just converge.)"
HENCE,0.7725225225225225,"Therefore when Vk + Wk <
2
h −hµ2, we have |lk| < 1. We only need to consider lk+1 when
2
h −hµ2 ≤Vk + Wk ≤2"
HENCE,0.7732732732732732,"h + 2hµ2(1 +
1
1−h2µ2 ). Next we replace lk, Vk, Wk, Uk by l, V, W, U for"
HENCE,0.774024024024024,Published as a conference paper at ICLR 2022
HENCE,0.7747747747747747,simplicity.
HENCE,0.7755255255255256,lk+1 = 1 −h(Vk+1 + Wk+1) + h2(Vk+1Wk+1 −µ2)
HENCE,0.7762762762762763,"= 1 −h(Vk+1 + Wk+1) + h2(Vk+1Wk+1 −U 2
k+1) + h2(U 2
k+1 −µ2)"
HENCE,0.777027027027027,"= 1 −h(Vk+1 + Wk+1) + h2l2(V W −U 2) + h2(U 2
k+1 −µ2)"
HENCE,0.7777777777777778,"(10)
= l + h2(3 + l2 −h(1 + 4h2µ2)(V + W))(V W −U 2)"
HENCE,0.7785285285285285,+ h2U 2(3 + (l + 2h2µ2)2 −h(1 + 4h2µ2)(V + W))
HENCE,0.7792792792792793,+ h2µ2(h2(W + V )2 −h(W + V ) + 4h4V 2W 2)
HENCE,0.78003003003003,"+ 2h2Uµ((1 −(l + 2h2µ2))h(W + V ) −2 + 2(l + 2h2µ2)(1 −l −h2µ2))
(12)"
HENCE,0.7807807807807807,"If l ≥−4h2µ2, we have"
HENCE,0.7815315315315315,lk+1 = l + h2(3 + l2 −h(1 + 4h2µ2)(V + W))(V W −U 2)
HENCE,0.7822822822822822,+ h2U 2(3 + (l + 2h2µ2)2 −h(1 + 4h2µ2)(V + W))
HENCE,0.7830330330330331,+ h2µ2(h2(W + V )2 −h(W + V ) + 4h4V 2W 2)
HENCE,0.7837837837837838,+ 2h2Uµ((1 −(l + 2h2µ2))h(W + V ) −2 + 2(l + 2h2µ2)(1 −l −h2µ2))
HENCE,0.7845345345345346,≥l + 2h2Uµ((1 −(l + 2h2µ2))h(W + V ) −2 + 2(l + 2h2µ2)(1 −l −h2µ2))
HENCE,0.7852852852852853,≥l −h2µ(V + W)(5h2µ2 + (9h4µ4)/8)
HENCE,0.786036036036036,"≥−4h2µ2 −hµ(2 + 2h2µ2(1 +
1
1 −h2µ2 ))(5h2µ2 + (9h4µ4)/8) > −1,"
HENCE,0.7867867867867868,where the ﬁrst inequality is because all the removed terms are positive when 2
HENCE,0.7875375375375375,"h −hµ2 ≤V + W ≤
2
h + 2hµ2(1 +
1
1−h2µ2 ); the second inequality follows from recollecting l and take l from quadratic
formula with the h(V + W) taken at the bound, and U > −V +W"
HENCE,0.7882882882882883,"2
; the third inequality follows from
the bounds of h(V + W) and l ; the last inequality is from hµ ≤
1
2
√ 7."
HENCE,0.789039039039039,If l < −4h2µ2 ⇒l + h2µ2 < 0. By rewriting lk+1 in the following way
HENCE,0.7897897897897898,lk+1 = l + h2(4h4V 2W 2 −h(V + W) + h2(V + W)2)µ2 + 4h4U 2µ2(l + h2µ2)
HENCE,0.7905405405405406,+ 2h2Uµ(−2 −h(V + W)(−1 + l + 2h2µ2) −2(−1 + l + h2µ2)(l + 2h2µ2))
HENCE,0.7912912912912913,"+ h2V W(3 + l2 −h(V + W)(1 + 4h2µ2)),"
HENCE,0.7920420420420421,"we have that the minimum of lk+1 is achieved when U = ±
√"
HENCE,0.7927927927927928,"V W. Thus we will consider two cases:
U ≥0 and U < 0. Note we always have U 2 < V W (otherwise V W −U 2 = 0 just converges) and
hence lk+1 is larger than the corresponding value at U = ±
√"
HENCE,0.7935435435435435,"V W. Let q = h|U|, v = hµ, q = cv
for c ≥0. Then we have"
HENCE,0.7942942942942943,"q2 = c2v2 ≤h2V W = h(V + W) −1 + l −v2 < 1,
i.e.,
c < 1"
HENCE,0.795045045045045,"v .
(13)"
HENCE,0.7957957957957958,"Next, let"
HENCE,0.7965465465465466,"l′
k+1 = lk+1 −h2(3 + l2 −h(1 + 4h2µ2)(V + W))(V W −U 2) < lk+1 −h2(1"
HENCE,0.7972972972972973,2 + l2)(V W −U 2).
HENCE,0.7980480480480481,"First let U ≥0 and q = hU < h
√"
HENCE,0.7987987987987988,"V W. Choose q = hU = h
√"
HENCE,0.7995495495495496,V W. Then by
HENCE,0.8003003003003003,"h(V + W) = 1 −l + h2(V W −µ2) = 1 −l + c2v2 −v2,"
HENCE,0.801051051051051,we have the minimum value of lk+1 when U ≥0
HENCE,0.8018018018018018,"lk+1 ≥l′
k+1 = l + (−1 + c)(−((−1 + l)l) + c(2 + l + l2))v2
(14)"
HENCE,0.8025525525525525,−(−1 + c)2(1 + c2 −2l + 2cl)v4 + (−1 + c)4v6.
HENCE,0.8033033033033034,"1) When −1 < l < −4h2µ2 and c = 1, we have lk+1 = l′
k+1 = l = lk."
HENCE,0.8040540540540541,Published as a conference paper at ICLR 2022
HENCE,0.8048048048048048,"2) When −1 < l < −4h2µ2 and c > 1, the derivative of (14) with respect to c is"
HENCE,0.8055555555555556,"d
dcl′
k+1 = (1 −l)lv2 + (−2 −l −l2)v2 + 2v4 −6lv4 −4v6 + 3c2(2v4 −2lv4 −4v6)"
HENCE,0.8063063063063063,"+ 4c3(−v4 + v6) + 2c((2 + l + l2)v2 −2v4 + 6lv4 + 6v6),"
HENCE,0.8070570570570571,"which is a third order polynominal with negative coefﬁcient of the highest degree term and has a
root in [0, 1]. Also, when c = 1, d"
HENCE,0.8078078078078078,"dcl′
k+1 = 2v2(1 + l) > 0. Therefore, when 1 < c < 1"
HENCE,0.8085585585585585,"v, l′
k+1 either
increases or ﬁrst increases then decreases, namely, the minimum of l′
k+1 is achieved at c = 1 or
c = 1"
HENCE,0.8093093093093093,"v. If c = 1, by the previous discussion, l′
k+1 = l. If c = 1 v,"
HENCE,0.81006006006006,"l′
k+1 = 1 + l2(−1 + v)2 −v2 −2v3 + 5v4 −4v5 + v6 + l(2 −2v + 5v2 −6v3 + 2v4)"
HENCE,0.8108108108108109,≥v2(−24 + 44v −25v2 + 4v3)
HENCE,0.8115615615615616,"4(−1 + v)2
> −1,"
HENCE,0.8123123123123123,"where the ﬁrst inequality follows from the property of quadratic function by taking l
=
−(2−2v+5v2−6v3+2v4)"
HENCE,0.8130630630630631,"2(−1+v)2
. Therefore when −1 < l < −4h2µ2 and c > 1, lk+1 > −1."
HENCE,0.8138138138138138,"3) When −1 < l < −4h2µ2 and c < 1, rewrite l′
k+1 with respect to l and we have"
HENCE,0.8145645645645646,"l′
k+1 = 2(−1 + c)cv2 −(−1 + c)2v4 −(−1 + c)2c2v4 + (−1 + c)4v6 + l2((1 −c)v2"
HENCE,0.8153153153153153,"+ (−1 + c)cv2) + l(1 + (−1 + c)v2 + (−1 + c)cv2 + 2(−1 + c)2v4 −2(−1 + c)2cv4),"
HENCE,0.816066066066066,where the minimum point is
HENCE,0.8168168168168168,"l = −
1
2(−1 + c)2v2 −1"
HENCE,0.8175675675675675,2 −(1 −c)v2 < −1
HENCE,0.8183183183183184,2v2 −1
HENCE,0.8190690690690691,2 < −1.
HENCE,0.8198198198198198,"Therefore the minimum of l′
k+1 is obtained at l = −1, i.e.,"
HENCE,0.8205705705705706,"l′
k+1 ≥−1 + (−1 + c)2v2(2 −(3 −2c + c2)v2 + (−1 + c)2v4)"
HENCE,0.8213213213213213,"≥−1 + (−1 + c)2v2(2 −3v2 + (−1 + c)2v4) > −1,"
HENCE,0.8220720720720721,"where the inequality is from 0 ≤c < 1. Therefore when −1 < l < −4h2µ2 and c < 1, lk+1 > −1."
HENCE,0.8228228228228228,4) When l ≤−1 and c ≥5
HENCE,0.8235735735735735,"4, consider l′
k+1 −l (15) and take the derivative d"
HENCE,0.8243243243243243,"dc(l′
k+1 −l) =
d
dcl′
k+1.
From previous discussion, consider d"
HENCE,0.825075075075075,"dcl′
k+1 at c = 5"
HENCE,0.8258258258258259,"4, i.e.,"
HENCE,0.8265765765765766,"d
dcl′
k+1 = 1/16v2(48 + 8l2 −23v2 + v4 + l(40 −6v2)) > v2 −(161v4)/16 + (325v6)/16 > 0,"
HENCE,0.8273273273273273,"where the ﬁrst inequality follows from l > −1 −6v2 by (11). Therefore the minimum of l′
k+1 −l is
at c = 5"
HENCE,0.8280780780780781,4 or c = 1
HENCE,0.8288288288288288,"v. If c = 5 4,"
HENCE,0.8295795795795796,"l′
k+1 −l = v2"
HENCE,0.8303303303303303,256(160 + 16l2 −41v2 + v4 −8l(−18 + v2)) > v2
HENCE,0.831081081081081,256(32 −705v2 + 625v4) > 0
HENCE,0.8318318318318318,"where the ﬁrst inequality is from l > −1 −6v2 by (11). If c = 1 v,"
HENCE,0.8325825825825826,"l′
k+1 −l = l2(−1 + v)2 + l(−1 + v)(−1 + v −4v2 + 2v3) + (−1 + v)(−1 −v + 2v3 −3v4 + v5)"
HENCE,0.8333333333333334,≥(1 −v)3(1 + 3v + v2 −v3) > 0
HENCE,0.8340840840840841,"where the ﬁrst inequality is from l ≤−1. Therefore when l ≤−1 and c ≥
5
4, lk+1 −lk >
min{ v2"
HENCE,0.8348348348348348,"256(32 −705v2 + 625v4), (1 −v)3(1 + 3v + v2 −v3)} > 0."
HENCE,0.8355855855855856,In the next two cases c ≤1 and 1 < c < 5
HENCE,0.8363363363363363,"4, we no longer assume q = h
√"
HENCE,0.8370870870870871,"V W but want the same
lower bound of l′
k+1. Since q = hU < h
√"
HENCE,0.8378378378378378,"V W, we have h(V + W) > 1 −l + c2v2 −v2 > 0. By
rewriting l′
k+1 to be"
HENCE,0.8385885885885885,"l′
k+1 =l −4cv2 + 3c2v2 + 4c4v6 + 4cv2(1 −l −v2)(l + 2v2) + v2h2(V + W)2"
HENCE,0.8393393393393394,"+ c2v2(l + 2v2)2 + (−v2 + 2cv2(1 −l −2v2) −c2v2(1 + 4v2))h(V + W),"
HENCE,0.8400900900900901,Published as a conference paper at ICLR 2022
HENCE,0.8408408408408409,it can be seen from the minimum point w.r.t. h(V + W) that when h(V + W) > 0 and 0 ≤c < 5
HENCE,0.8415915915915916,"4,
l′
k+1 decreases as h(V + W) decreases. Therefore by h(V + W) > 1 −l + c2v2 −v2, we have the
same expression for the lower bound of l′
k+1 but with U 2 < V W, i.e.,"
HENCE,0.8423423423423423,"lk+1 −l′
k+1 = h2(3 + l2 −h(1 + 4h2µ2)(V + W))(V W −U 2) > h2(1"
HENCE,0.8430930930930931,"2 + l2)(V W −U 2),"
HENCE,0.8438438438438438,"l′
k+1 > l + (−1 + c)(−((−1 + l)l) + c(2 + l + l2))v2"
HENCE,0.8445945945945946,−(−1 + c)2(1 + c2 −2l + 2cl)v4 + (−1 + c)4v6.
HENCE,0.8453453453453453,"5) When l ≤−1 and c ≤1, consider l′
k+1 −l"
HENCE,0.8460960960960962,"l′
k+1 −l > (−1 + c)(−((−1 + l)l) + c(2 + l + l2))v2
(15)"
HENCE,0.8468468468468469,−(−1 + c)2(1 + c2 −2l + 2cl)v4 + (−1 + c)4v6
HENCE,0.8475975975975976,= (−1 + c)2l2v2 + (−1 + c)lv2(1 + c −2(1 −c)v2 + 2(1 −c)cv2)
HENCE,0.8483483483483484,+ (−1 + c)v2(2c + (1 −c)v2 + (1 −c)c2v2 + (−1 + c)3v4)
HENCE,0.8490990990990991,where the minimum point is l = −1
HENCE,0.8498498498498499,2 + 1−(1−c)v2+(1−c)cv2
HENCE,0.8506006006006006,"1−c
> −1. By l ≤−1 we have"
HENCE,0.8513513513513513,"l′
k+1 −l > v2(2(−1 + c)2 −(−1 + c)2(3 −2c + c2)v2 + (−1 + c)4v4)"
HENCE,0.8521021021021021,= v2((−1 + c)2(2 −(3 −2c + c2)v2) + (−1 + c)4v4)
HENCE,0.8528528528528528,≥v2((−1 + c)2(2 −3v2) + (−1 + c)4v4) ≥0.
HENCE,0.8536036036036037,"Therefore when l ≤−1 and c ≤1, lk+1 −lk > h2( 1"
HENCE,0.8543543543543544,2 + l2)(V W −U 2) ≥3
HENCE,0.8551051051051051,2h2(V W −U 2).
HENCE,0.8558558558558559,6) When l ≤−1 and 1 < c < 5
HENCE,0.8566066066066066,"4, consider Uk+1. By h(V + W) −h2V W = 1 −l −v2, we have"
HENCE,0.8573573573573574,Uk+1 = h(1 −hV )µW + hµV (1 −hW) + U(1 + h2V W −h(V + W) + h2µ2)
HENCE,0.8581081081081081,= µ((c −1)(l + v2) + 1 + cv2 −h2V W).
HENCE,0.8588588588588588,Thus denote hUk+1 = ck+1hµ = ck+1v and we have
HENCE,0.8596096096096096,ck+1 = (c −1)(l + v2) + 1 + cv2 −h2V W
HENCE,0.8603603603603603,< (c −1)(l + v2) + 1 + cv2 −c2v2 < 1.
HENCE,0.8611111111111112,"Then if lk+1 −l ≥0, then lk+2 −l = lk+2 −lk+1 + lk+1 −l > lk+2 −lk+1 > 3"
HENCE,0.8618618618618619,"2h2(V W −U 2).
Otherwise, we have lk+1 < l and consider lk+2. Also, the distance between 1 and ck+1 is larger than
that of 1 and c, i.e.,"
HENCE,0.8626126126126126,"1 −ck+1 −(c −1) > (1 −(c −1)(l + v2) + 1 + cv2 −c2v2) −(c −1)
(16)"
HENCE,0.8633633633633634,= 1 + l + v2 + c2v2 + c(−1 −l −2v2) > 0.
HENCE,0.8641141141141141,"From previous discussion, l′
k+1 −l increases as l decreases. Also d"
HENCE,0.8648648648648649,"dcl′
k+1 < 0 when 0 ≤c < 1, so
l′
k+1 −l increases as c decreases. By (16), we have ck+1 < 2 −c. Then"
HENCE,0.8656156156156156,"l′
k+2 −l = l′
k+2 −lk+1 + lk+1 −l > l′
k+2 −lk+1 + l′
k+1 −l"
HENCE,0.8663663663663663,> (−1 + c)(−((−1 + l)l) + c(2 + l + l2))v2
HENCE,0.8671171171171171,−(−1 + c)2(1 + c2 −2l + 2cl)v4 + (−1 + c)4v6
HENCE,0.8678678678678678,+ (1 −c)v2(−((−1 + l)l) −(−2 + c)(2 + l + l2)
HENCE,0.8686186186186187,+ (−1 + c)(5 + c2 + 2l −2c(2 + l))v2 −(−1 + c)3v4)
HENCE,0.8693693693693694,= 2v2(−1 + c)2((2 + l + l2) + (−2 −(−1 + c)2)v2 + (−1 + c)2v4)
HENCE,0.8701201201201201,"> 2v2(−1 + c)2((2 −3v2) + (−1 + c)2v4) > 0,"
HENCE,0.8708708708708709,"where the second inequality follows from ck+1 < 2 −c and lk+1 < l; the last inequality is by
l ≤−1 and 1 < c < 5"
HENCE,0.8716216216216216,4. Therefore when l ≤−1 and 1 < c < 5
HENCE,0.8723723723723724,"4, lk+2 −lk > 3"
HENCE,0.8731231231231231,"2h2(V W −U 2) or
2v2(−1 + c)2((2 −3v2) + (−1 + c)2v4)."
HENCE,0.8738738738738738,Published as a conference paper at ICLR 2022
HENCE,0.8746246246246246,"Second, let U < 0 and q = −hU < h
√"
HENCE,0.8753753753753754,"V W. Choose q = −hU = h
√"
HENCE,0.8761261261261262,"V W. Then similarly we
have the minimum value of lk+1 when U < 0"
HENCE,0.8768768768768769,"l′
k+1 = l + (1 + c)((−1 + l)l + c(2 + l + l2))v2 −(1 + c)2(1 + c2
(17)"
HENCE,0.8776276276276276,−2l −2cl)v4 + (1 + c)4v6
HENCE,0.8783783783783784,= (1 + c)2l2v2 + l(1 + (−1 + c2)v2 + 2(1 + c)3v4)
HENCE,0.8791291291291291,+ (1 + c)v2(v2(−1 + v2) + c3v2(−1 + v2) + c2v2(−1 + 3v2) + c(2 −v2 + 3v4))
HENCE,0.8798798798798799,"≥−1 + (−4 + 4c + 3c2)v2 + (15 + 16c −5c2 −8c3 −2c4)v4
(18)"
HENCE,0.8806306306306306,"+ (−5 −4c + 2c2 + c3)2v6,"
HENCE,0.8813813813813813,"where the inequality is because when l > −1 −6v2 + c2v2 (from (11)), l′
k+1 increases as l increases."
HENCE,0.8821321321321322,"1) When c ≥1, since (−5 −4c + 2c2 + c3)2v6 ≥0, we consider (−4 + 4c + 3c2)v2 + (15 + 16c −
5c2 −8c3 −2c4)v4 denoted to be lpart
k+1"
HENCE,0.8828828828828829,"d
dclpart
k+1 = (4 + 6c)v2 + (16 −10c −24c2 −8c3)v4."
HENCE,0.8836336336336337,"It has a root between -2 and 0. Also when c = 1, d"
HENCE,0.8843843843843844,"dclpart
k+1 > 0; when c = 1 v, d"
HENCE,0.8851351351351351,"dclpart
k+1 < 0. Thus, the
minimum of lpart
k+1 is achieved at c = 1 or c = 1"
HENCE,0.8858858858858859,"v, i.e., lpart
k+1 ≥v2(3 + 16v2). Then"
HENCE,0.8866366366366366,"l′
k+1 ≥−1 + v2(3 + 16v2) + (−5 −4c + 2c2 + c3)2v6 > 0."
HENCE,0.8873873873873874,"Therefore when c ≥1, lk+1 > −1."
HENCE,0.8881381381381381,"2) When 0 ≤c < 1 and l > −1, from previous discussion of l′
k+1 we have"
HENCE,0.8888888888888888,"l′
k+1 ≥−1 + 2(1 + c)2v2 −(1 + c)2(3 + 2c + c2)v4 + (1 + c)4v6"
HENCE,0.8896396396396397,≥−1 + (1 + c)2v2(2 −6v2) + (1 + c)4v6
HENCE,0.8903903903903904,≥−1 + v2(2 −6v2) + v6 > −1.
HENCE,0.8911411411411412,"Therefore when 0 ≤c < 1 and l > −1, lk+1 > −1."
HENCE,0.8918918918918919,"3) When 0 ≤c < 1 and l ≤−1, consider l′
k+1 −l. By (17),"
HENCE,0.8926426426426426,"l′
k+1 −l = (1 + c)2l2v2 + l((−1 + c2)v2 + 2(1 + c)3v4)"
HENCE,0.8933933933933934,+ (1 + c)v2(v2(−1 + v2) + c3v2(−1 + v2) + c2v2(−1 + 3v2) + c(2 −v2 + 3v4))
HENCE,0.8941441441441441,≥2(1 + c)2v2 −(1 + c)2(3 + 2c + c2)v4 + (1 + c)4v6
HENCE,0.8948948948948949,"≥v2(2 −6v2) + v6,"
HENCE,0.8956456456456456,"where the ﬁrst inequality is because the minimum point of l is greater than -1. Therefore when
0 ≤c < 1 and l ≤−1, lk+1 −lk > v2(2 −6v2) + v6."
HENCE,0.8963963963963963,"Actually all the proofs of lk > −1 ⇒lk+1 > −1 + C for some C > 0 are valid for all 0 <
Vk + Wk ≤2"
HENCE,0.8971471471471472,"h + 2hµ2(1 +
1
1−h2µ2 )."
HENCE,0.8978978978978979,"In general, if lk > −1, then there exists a constant C > 0 only depending on hµ s.t. li > −1 + C for
all i > k. If lk ≤−1, then either (1) lk+1 increases at least by a ﬁxed value only depending on hµ, or
(2) lk+2 or lk+1 increases by 3"
HENCE,0.8986486486486487,"2h2(VkWk −U 2
k). If it keeps staying in case (2), then since lk ≤−1,
VkWk −U 2
k will be larger and larger until li > −1 for some i. Therefore, there exists a K > 0, s.t.
when k > K, lk > −1+C. Then because lk ≤1−h2µ2, let 0 < C0 = max{1−C, 1−h2µ2} < 1
and we have |lk| ≤C0 < 1 for all k > K. Further,"
HENCE,0.8993993993993994,"VkWk −U 2
k ≤C2(k−K)
0
(VKWK −U 2
K) →0
as k →∞,"
HENCE,0.9001501501501501,"i.e., 1 −
U 2
k
VkWk →0 ⇒| cos(∠(xk, yk))| →1."
HENCE,0.9009009009009009,"Otherwise, there exists a K > 0, s.t., VKWK −U 2
K = 0, then VkWk −U 2
k = 0 for all k ≥K.
There are two cases. (i) xK and yK are already aligned. (ii) one of VK and WK is 0, WOLG assume"
HENCE,0.9016516516516516,Published as a conference paper at ICLR 2022
HENCE,0.9024024024024024,"VK = 0. Then from the GD update,

xk+1 = xk + h(µI −xky⊤
k )yk
yk+1 = yk + h(µI −ykx⊤
k )xk
,"
HENCE,0.9031531531531531,"xK+1 and yK+1 will be aligned for both cases. Then for all k ≥K + 1, xk and yk is aligned, i.e.,
| cos(∠(xk, yk))| = 1."
HENCE,0.9039039039039038,"I
PROOF OF THEOREM 5.1"
HENCE,0.9046546546546547,"I.1
GENERAL RANK 1 APPROXIMATION FOR NON-NEGATIVE DIAGONAL MATRIX"
HENCE,0.9054054054054054,"We ﬁrst discuss an easy case where A is a non-negative diagonal matrix (later on we will see this is
actually the canonical case) and consider"
HENCE,0.9061561561561562,"min
x,y∈Rn ∥A −xy⊤∥2
F /2.
(19)"
HENCE,0.9069069069069069,"Assume A =  
"
HENCE,0.9076576576576577,"µ1In1
...
µmInm "
HENCE,0.9084084084084084,"
, where Ini is an identity matrix of dimension ni × ni,"
HENCE,0.9091591591591591,"Pm
i=1 ni = n, µi ≥0, µi ̸= µj for i ̸= j. Let Si = {1, 2, · · · , ni} + Pi−1
j=1 nj, i.e., an index
set describing the positional indices of Ini. Let µmax = max{µ1, · · · , µm} and Smax be the
corresponding index set. Let [x∞, y∞] be a ﬁxed point of the GD map. Let xs,∞and ys,∞be the sth
element of x∞and y∞."
HENCE,0.9099099099099099,The following theorem characterizes the ﬁxed points of the GD map.
HENCE,0.9106606606606606,Theorem I.1. The ﬁxed points of GD map satisfy P
HENCE,0.9114114114114115,"s∈Si x2
s,∞·P"
HENCE,0.9121621621621622,"s∈Si y2
s,∞= µ2
i , xs,∞= ys,∞= 0
for s /∈Si, ∀i = 1, · · · , n. Thus ∥x∞∥∥y∞∥= µi, ∀i = 1, · · · , n."
HENCE,0.9129129129129129,"Proof. To solve the ﬁxed points, we have ∀i = 1, · · · , n and x ∈Si

(A −x∞y⊤
∞)y∞= Ay∞−(y⊤
∞y∞)x∞= 0
(A −x∞y⊤
∞)⊤x∞= A⊤x∞−(x⊤
∞x∞)y∞= 0"
HENCE,0.9136636636636637,"i.e.,

µiys,∞= (y⊤
∞y∞)xs,∞
µixs,∞= (x⊤
∞x∞)ys,∞
⇒µ2
i xs,∞ys,∞= (x⊤
∞x∞)(y⊤
∞y∞)xs,∞ys,∞"
HENCE,0.9144144144144144,"Obviously, x∞= y∞= 0 is a ﬁxed point. If xs,∞̸= 0, then ys,∞̸= 0 and we have µ2
i =
(x⊤
∞x∞)(y⊤
∞y∞). Since µi ̸= µj for i ̸= j, there exists at most one i, s.t. µ2
i = (x⊤
∞x∞)(y⊤
∞y∞)
and all the xs,∞, ys,∞s.t. s ∈Sj, j ̸= i is zero."
HENCE,0.9151651651651652,"This theorem identiﬁes that each of these ﬁxed points is concentrated in one of the positions of
the eigenvalue blocks. Note the ﬁxed points contain both stable and unstable ones, i.e., all the
global minima and saddles are of the above form. Moreover, the global minima are the points
obeying P
s∈Smax x2
s,∞· P"
HENCE,0.9159159159159159,"s∈Smax y2
s,∞= µ2
max, xs,∞= ys,∞= 0 for s /∈Smax and also
∥x∞∥∥y∞∥= µmax. The saddles are the ones with µi ̸= µmax, ∀i = 1, · · · , n."
HENCE,0.9166666666666666,"Remark I.2 (The alignment of x∞and y∞at the global minimum). At the global minimum of the
objective (19) with diagonal and non-negative A deﬁned above, x∞y⊤
∞is the rank-1 approximation
of A with the largest eigenvalue, i.e., we have µmax = tr(x∞y⊤
∞) = x⊤
∞y∞. Also, from the above
theorem, ∥x∞∥∥y∞∥= µmax. Therefore for each global minimum, there exists a scalar l > 0, s.t.,
x∞= ly∞, i.e., x∞and y∞are aligned at the global minimum."
HENCE,0.9174174174174174,"Based on the analytical form of eigenvalues of the Jacobian of GD map (19) in Theorem I.1, we can
establish the following relation between x∞and y∞at the global minimum via stability analysis."
HENCE,0.9181681681681682,Published as a conference paper at ICLR 2022
HENCE,0.918918918918919,"Theorem I.3. For almost all initial conditions, if GD converges to a minimum, then x⊤
∞y∞= µmax
and"
HENCE,0.9196696696696697,∥x∞∥2 + ∥y∞∥2 < 2
HENCE,0.9204204204204204,"h;
(20)"
HENCE,0.9211711711711712,the extent of balancing is quantiﬁed by
HENCE,0.9219219219219219,∥x∞−y∞∥2 < 2
HENCE,0.9226726726726727,"h −2µmax.
(21)"
HENCE,0.9234234234234234,"Proof. Since each unstable ﬁxed point has its stable set with negligible size when compared to the
stable set of stable ﬁxed point, then for almost all initial conditions, if GD converges to a global
minimum, this minimum is a stable ﬁxed point of the GD map."
HENCE,0.9241741741741741,"From the above theorem, we know the ﬁxed points are µ2
i = (x⊤
∞x∞)(y⊤
∞y∞). Assume µ2 =
(x⊤
∞x∞)(y⊤
∞y∞)."
HENCE,0.924924924924925,"For the Jacobian J = I2n +

−h(y⊤
∞y∞)In
hA −2hx∞y⊤
∞
hA⊤−2hy∞x⊤
∞
−h(x⊤
∞x∞)In"
HENCE,0.9256756756756757,"
= I2n + ˜A, since the lower two"
HENCE,0.9264264264264265,"blocks commute, we have"
HENCE,0.9271771771771772,"det(λI −˜A) = det(λ2In + λh(x⊤
∞x∞+ y⊤
∞y∞)In + h2µ2In −h2A2"
HENCE,0.9279279279279279,"+ h2(2Ay∞x⊤
∞+ 2x∞y⊤
∞A −4x∞y⊤
∞y∞x⊤
∞))"
HENCE,0.9286786786786787,"= det(λ2In + λh(x⊤
∞x∞+ y⊤
∞y∞)In + h2µ2In −h2A2"
HENCE,0.9294294294294294,"+ h2(2(A −x∞y⊤
∞)y∞x⊤
∞+ 2x∞y⊤
∞(A −y∞x⊤
∞)))"
HENCE,0.9301801801801802,"= det(λ2In + λh(x⊤
∞x∞+ y⊤
∞y∞)In + h2µ2In −h2A2) = n
Y"
HENCE,0.9309309309309309,"i=1
(λ2 + h(x⊤
∞x∞+ y⊤
∞y∞)λ + h2(µ2 −µ2
i ))."
HENCE,0.9316816816816816,"Hence the two values, 1 and 1 −h(x⊤
∞x∞+ y⊤
∞y∞), are eigenvalues of J at each nonzero ﬁxed
point."
HENCE,0.9324324324324325,"If µ = max{µi}, then |1 −h(x⊤
∞x∞+ y⊤
∞y∞)| < 1 ⇒all the eigenvalues are bounded by 1. Hence
x⊤
∞x∞+ y⊤
∞y∞< 2 h."
HENCE,0.9331831831831832,"If µ ̸= max{µi}, then 1+ 1"
HENCE,0.933933933933934,"2(−h(x⊤
∞x∞+y⊤
∞y∞)+
p"
HENCE,0.9346846846846847,"h2(x⊤
∞x∞+ y⊤
∞y∞)2 + 4h2((µ2
i −µ2))) >
1 for µi ≥µ. It has at least one unstable direction. It’s stable set is measure 0."
HENCE,0.9354354354354354,"The second inequality of the theorem follows from x⊤
∞y∞= µ at the global minimum because of
diagonal and non-negative A and SVD."
HENCE,0.9361861861861862,"I.2
GENERAL MATRIX FACTORIZATION FOR NON-NEGATIVE DIAGONAL MATRIX"
HENCE,0.9369369369369369,"In this section, we consider problem (5) via GD iteration but with A ∈Rn×n to be an arbitrary
non-negative diagonal matrix."
HENCE,0.9376876876876877,"Let µ1 ≥µ2 ≥· · · ≥µn ≥0 be the diagonal elements of A. Let r = rank(A) which of course
satiﬁes r ≤n. Also assume that ∥A∥F does not scale with n or d (without loss of generality we can
assume ∥A∥F = O(1)). Then we will have our balancing result in the following.
Theorem I.4. For almost all initial conditions, if GD for (5) converges to a global minimum, then
there exists c = c(n, d) > c0 with constant c0 > 0 independent of n, d, and learning rate h, such
that, (X, Y ) satisﬁes"
HENCE,0.9384384384384384,"c(∥X∞∥2
F + ∥Y∞∥2
F) < 2 h,"
HENCE,0.9391891891891891,and the extent of balancing is quantiﬁed by
HENCE,0.93993993993994,"∥X∞−Y∞∥2
F < 2 ch −2"
HENCE,0.9406906906906907,"min{d,r}
X"
HENCE,0.9414414414414415,"i=1
µi."
HENCE,0.9421921921921922,Published as a conference paper at ICLR 2022
HENCE,0.9429429429429429,"Proof. Let r =rank(A) ≤n. Since each unstable ﬁxed point has its stable set with size negligible
when compared to the stable set of stable ﬁxed point, then for almost all initial conditions, if GD
converges to a global minimum, this minimum is a stable ﬁxed point of the GD map."
HENCE,0.9436936936936937,"For matrix X = (x1 x2 · · · xd) ∈Rn×d, where xi ∈Rn is the ith column vector, we vectorize the"
HENCE,0.9444444444444444,"variable and get vec(X) =  

"
HENCE,0.9451951951951952,"x1
x2
...
xd "
HENCE,0.9459459459459459,"

. Then we can rewrite the GD iteration,"
HENCE,0.9466966966966966,"
Xk+1 = Xk + h(A −XkY ⊤
k )Yk
Yk+1 = Yk + h(A⊤−YkX⊤
k )Xk"
HENCE,0.9474474474474475,"⇒

vec(Xk+1) = vec(Xk) + h(I ⊗A −T(vec(Yk)vec(Xk)⊤))vec(Yk)
vec(Yk+1) = vec(Yk) + h(I ⊗A −T(vec(Xk)vec(Yk)⊤))vec(Xk) ,"
HENCE,0.9481981981981982,"where ⊗is the Kronecker product, and T : Rnd×nd →Rnd×nd is a linear operator, s.t. T  
  
"
HENCE,0.948948948948949,"M11
· · ·
M1d
...
...
Md1
· · ·
Mdd  
  
=  
"
HENCE,0.9496996996996997,"M ⊤
11
· · ·
M ⊤
1d
...
...
M ⊤
d1
· · ·
M ⊤
dd  
,"
HENCE,0.9504504504504504,"then
T(vec(Y )vec(X)⊤) = "
HENCE,0.9512012012012012,"


"
HENCE,0.9519519519519519,"x1y⊤
1
x2y⊤
1
· · ·
xdy⊤
1
x1y⊤
2
x2y⊤
2
· · ·
xdy⊤
2
...
...
...
x1y⊤
d
x2y⊤
d
· · ·
xdy⊤
d "
HENCE,0.9527027027027027,"


."
HENCE,0.9534534534534534,"The Jacobian of the vectorized GD map is (replace Xk and Yk by X and Y ) I2nd −hM, where"
HENCE,0.9542042042042042,"M =

Y ⊤Y ⊗In
T(vec(Y )vec(X)⊤) + I ⊗(XY ⊤−A)
T(vec(X)vec(Y )⊤) + I ⊗(Y X⊤−A)
X⊤X ⊗In 
."
HENCE,0.954954954954955,"We would like to obtain an estimation of the eigenvalues of I −hM for stability analysis. Since M is
the Hessian of the objective at the global minimum deﬁned, all the eigenvalues are non-negative. Also,
since ∥A∥F is independent of h, n and d, we can just assume without loss of generality ∥A∥F = O(1)
and then take ∥X∥F and ∥Y ∥F to be O(1) at each global minimum. Then"
HENCE,0.9557057057057057,"tr(M) = n(∥X∥2
F + ∥Y ∥2
F) = O(n)."
HENCE,0.9564564564564565,"Next consider tr(M 2). We only need the diagonal blocks of M 2. The two diagonal blocks are the
following"
HENCE,0.9572072072072072,M1 = (Y ⊤Y ⊗In)2
HENCE,0.9579579579579579,"+ [T(vec(Y )vec(X)⊤) + I ⊗(XY ⊤−A)][T(vec(X)vec(Y )⊤) + I ⊗(Y X⊤−A)],"
HENCE,0.9587087087087087,M2 = (X⊤X ⊗In)2
HENCE,0.9594594594594594,+ [T(vec(X)vec(Y )⊤) + I ⊗(Y X⊤−A)][T(vec(Y )vec(X)⊤) + I ⊗(XY ⊤−A)].
HENCE,0.9602102102102102,"Since we evaluate this matrix at the minimum, we have the gradient equals 0, i.e.,"
HENCE,0.960960960960961,"(A −XY ⊤)Y = 0, (A −Y X⊤)X = 0."
HENCE,0.9617117117117117,"By the mixed-product property of Kronecker product, we have"
HENCE,0.9624624624624625,M1 = (Y ⊤Y )2 ⊗In + T(vec(Y )vec(X)⊤)T(vec(X)vec(Y )⊤)
HENCE,0.9632132132132132,"+ Id ⊗[(A −XY ⊤)(A −Y X⊤)],"
HENCE,0.963963963963964,M2 = (X⊤X)2 ⊗In + T(vec(X)vec(Y )⊤)T(vec(Y )vec(X)⊤)
HENCE,0.9647147147147147,+ Id ⊗[(A −Y X⊤)(A −XY ⊤)].
HENCE,0.9654654654654654,Published as a conference paper at ICLR 2022 Then
HENCE,0.9662162162162162,"tr(M 2) = 2 ∥X∥2
F ∥Y ∥2
F + n(
X⊤X
2"
HENCE,0.9669669669669669,"F +
Y ⊤Y
2"
HENCE,0.9677177177177178,"F) + d
A −XY ⊤2 F ."
HENCE,0.9684684684684685,"Also, this is the global minimum, meaning"
HENCE,0.9692192192192193,"A −XY ⊤2"
HENCE,0.96996996996997,"F = tr((A −XY ⊤)(A −Y X⊤)) =
Pr
i=d+1 µ2
i ,
d < r
0,
d ≥r"
HENCE,0.9707207207207207,"Therefore, when d ≥r, we have"
HENCE,0.9714714714714715,"tr(M 2) = 2 ∥X∥2
F ∥Y ∥2
F + n(
X⊤X
2"
HENCE,0.9722222222222222,"F +
Y ⊤Y
2"
HENCE,0.972972972972973,F) = O(n).
HENCE,0.9737237237237237,"When d < r, we have"
HENCE,0.9744744744744744,"tr(M 2) = 2 ∥X∥2
F ∥Y ∥2
F + n(
X⊤X
2"
HENCE,0.9752252252252253,"F +
Y ⊤Y
2"
HENCE,0.975975975975976,"F) + d
A −XY ⊤2 F"
HENCE,0.9767267267267268,"< 2 ∥X∥2
F ∥Y ∥2
F + n(
X⊤X
2"
HENCE,0.9774774774774775,"F +
Y ⊤Y
2"
HENCE,0.9782282282282282,"F) + n
A −XY ⊤2"
HENCE,0.978978978978979,F = O(n).
HENCE,0.9797297297297297,"Therefore, the number of non-zero eigenvalues is N = O(n). Let λmax be the largest eigenvalue of
M. Then there exist a constant c0 > 0"
HENCE,0.9804804804804805,"λmax ≥tr(M)/N ≥c0(∥X∥2
F + ∥Y ∥2
F)."
HENCE,0.9812312312312312,"From stability analysis, we need the absolute values of the eigenvalues of I −hM to be bounded by
1, i.e.,"
HENCE,0.9819819819819819,"1 −hc0(∥X∥2
F + ∥Y ∥2
F) ≥1 −hλmax ≥−1 ⇒c0(∥X∥2
F + ∥Y ∥2
F) ≤2 h."
HENCE,0.9827327327327328,"Obviously, if we pick a constant c = c(n, d) for each n and d instead of c0, the above inequality also
holds with c(n, d) > c0."
HENCE,0.9834834834834835,"For the derivation of the second inequality, since at the global minimum tr(X∞Y ⊤
∞)
=
Pmin{d,r}
i=1
µi = Pmin{d,n}
i=1
µi because for non-negative diagonal A, its SVD of the non-zero part
satisﬁes Unon−zero = Vnon−zero , we have"
HENCE,0.9842342342342343,"∥X∞−Y∞∥2
F = ∥X∞∥2
F + ∥Y∞∥2
F −2 tr(X∞Y ⊤
∞) ≤2 ch −2"
HENCE,0.984984984984985,"min{d,n}
X"
HENCE,0.9857357357357357,"i=1
µi."
HENCE,0.9864864864864865,"I.3
FROM DIAGONAL MATRICES TO GENERAL MATRICES"
HENCE,0.9872372372372372,"Proof of Theorem 5.1. First all the minima of this problem are global minima and all the saddles are
unstable ﬁxed points. Since each unstable ﬁxed point has its stable set with size negligible when
compared to the stable set of stable ﬁxed point, then for almost all initial conditions, if GD converges
to a point, for almost all situations, this point is a stable ﬁxed point of the GD map."
HENCE,0.987987987987988,"By singular value decomposition (SVD), A = UDV ⊤, where U, D, V ∈Rn×n, U and V are
orthogonal matrices, and D is a non-negative diagonal matrix. Let Xk = URk, Yk = V Sk. Then

Xk+1 = Xk + h(A −XkY ⊤
k )Yk
Yk+1 = Yk + h(A −XkY ⊤
k )⊤Xk"
HENCE,0.9887387387387387,"⇔

URk+1 = URk + h(UDV ⊤−URkS⊤
k V ⊤)V Sk
V Sk+1 = V Sk + h(UDV ⊤−URkS⊤
k V ⊤)⊤URk"
HENCE,0.9894894894894894,"⇔

Rk+1 = Rk + h(D −RkS⊤
k )Sk
Sk+1 = Sk + h(D −RkS⊤
k )⊤Rk
."
HENCE,0.9902402402402403,"Therefore, problem (5) is equivalent to the following problem solved by GD"
HENCE,0.990990990990991,"min
R,S∈Rn×d
D −RS⊤2"
HENCE,0.9917417417417418,"F /2.
(22)"
HENCE,0.9924924924924925,Published as a conference paper at ICLR 2022
HENCE,0.9932432432432432,"From Theorem I.3 and I.4, we obtain: if GD for (22) converges to a global minimum (R, S), then
there exists a constant c > 0 independent of n, r, d, and h, s.t., the limiting minimum satisﬁes"
HENCE,0.993993993993994,"c(∥R∥2
F + ∥S∥2
F) ≤2 h,"
HENCE,0.9947447447447447,and the extent of balancing is quantiﬁed by
HENCE,0.9954954954954955,"∥R −S∥2
F ≤2 ch −2"
HENCE,0.9962462462462462,"min{d,r}
X"
HENCE,0.996996996996997,"i=1
µi."
HENCE,0.9977477477477478,"Especially, when d = 1, n ∈N+, c = 1."
HENCE,0.9984984984984985,"Since X = UR, Y = V S ⇒R = U ⊤X, S = V ⊤Y , we have ∥R∥F = ∥X∥F, ∥S∥F = ∥Y ∥F,
and ∥R −S∥F =
U ⊤X −V ⊤Y

F =
X −(UV ⊤)Y

F. The we obtain the ﬁrst and second"
HENCE,0.9992492492492493,"inequalities in Theorem 5.1. Also, by SVD,
XY ⊤2
F = Pmin{d,n}
i=1
µ2
i ≤∥X∥2
F ∥Y ∥2
F, we obtain
the third inequality."
