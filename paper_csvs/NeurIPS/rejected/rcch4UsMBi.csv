Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010834236186348862,"We introduce Generalized Instruction Tuning (called GLAN), a general and scal-
1"
ABSTRACT,0.0021668472372697724,"able method for instruction tuning of Large Language Models (LLMs). Unlike prior
2"
ABSTRACT,0.0032502708559046588,"work that relies on seed examples or existing datasets to construct instruction-tuning
3"
ABSTRACT,0.004333694474539545,"data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and
4"
ABSTRACT,0.005417118093174431,"capabilities as input and generates large-scale synthetic instruction data across all
5"
ABSTRACT,0.0065005417118093175,"disciplines. Specifically, inspired by the systematic structure in human education
6"
ABSTRACT,0.007583965330444204,"system, we build the taxonomy by decomposing human knowledge and capabilities
7"
ABSTRACT,0.00866738894907909,"to various fields, sub-fields and ultimately, distinct disciplines semi-automatically,
8"
ABSTRACT,0.009750812567713976,"facilitated by LLMs. Subsequently, we generate a comprehensive list of subjects
9"
ABSTRACT,0.010834236186348862,"for every discipline and proceed to design a syllabus tailored to each subject, again
10"
ABSTRACT,0.011917659804983749,"utilizing LLMs. With the fine-grained key concepts detailed in every class session
11"
ABSTRACT,0.013001083423618635,"of the syllabus, we are able to generate diverse instructions with a broad coverage
12"
ABSTRACT,0.014084507042253521,"across the entire spectrum of human knowledge and skills. Extensive experiments
13"
ABSTRACT,0.015167930660888408,"on large language models (e.g., Mistral) demonstrate that GLAN excels in mul-
14"
ABSTRACT,0.016251354279523293,"tiple dimensions from mathematical reasoning, coding, academic exams, logical
15"
ABSTRACT,0.01733477789815818,"reasoning to general instruction following without using task-specific training data
16"
ABSTRACT,0.018418201516793065,"of these tasks. In addition, GLAN allows for easy customization and new fields or
17"
ABSTRACT,0.01950162513542795,"skills can be added by simply incorporating a new node into our taxonomy.
18"
INTRODUCTION,0.020585048754062838,"1
Introduction
19"
INTRODUCTION,0.021668472372697724,"Large Language Models (LLMs) have enabled unprecedented capabilities to understand and generate
20"
INTRODUCTION,0.02275189599133261,"text like humans. By scaling up model size and data size [17, 13], LLMs are better at predicting
21"
INTRODUCTION,0.023835319609967497,"next tokens and prompting to perform certain tasks with a few demonstrations [2]. However, these
22"
INTRODUCTION,0.024918743228602384,"capabilities do not directly translate to better human instruction following [25]. Instruction tuning
23"
INTRODUCTION,0.02600216684723727,"[34] bridges this gap by fine-tuning LLMs on instructions paired with human-preferred responses.
24"
INTRODUCTION,0.027085590465872156,"Prior work constructs instruction tuning data from seed examples or existing datasets. Initially, natural
25"
INTRODUCTION,0.028169014084507043,"language processing (NLP) datasets described via instructions are used to fine-tune LLMs and the
26"
INTRODUCTION,0.02925243770314193,"resulting LLMs can generalize on unseen (NLP) tasks [34]. However, there are only thousands of
27"
INTRODUCTION,0.030335861321776816,"NLP tasks [33, 19] available, which limits the tuned LLMs to generalize in real-world scenarios [39].
28"
INTRODUCTION,0.0314192849404117,"Self-instruct [32] is a cost-effective method for creating synthetic instruction tuning datasets, which
29"
INTRODUCTION,0.032502708559046585,"starts from a small pool of human-written seed instructions and generates new instructions by few-
30"
INTRODUCTION,0.03358613217768147,"shot prompting an LLM (e.g., text-davinci-002) with randomly selected instructions from the
31"
INTRODUCTION,0.03466955579631636,"pool. Unfortunately, the diversity of generated instructions is still an issue, since few-shot prompting
32"
INTRODUCTION,0.035752979414951244,"tends to generate new instructions similar to its demonstrations. In addition, the process of creating
33"
INTRODUCTION,0.03683640303358613,"high-quality seed instructions requires considerable human effort and expertise. Evolve-Instruct [39]
34"
INTRODUCTION,0.03791982665222102,"improves self-instruct by augmenting existing instruction tuning datasets with different rewriting
35"
INTRODUCTION,0.0390032502708559,"operations using LLMs, which is essentially data argumentation. Consequently, the scope of domains
36"
INTRODUCTION,0.04008667388949079,"Figure 1: Comparing GLAN with FLAN, Self-Instruct and Evolve-Instruct. The inputs of FLAN,
Self-Instrct and Eovlve-Instruct are either seed examples or existing datasets, which limits the scope
of domains of instructions that these methods can generate. GLAN takes the taxonomy of human
knowledge & capabilities as input to ensure the broad coverage of generated instructions in various
domains. This taxonomy is then broken down into smaller pieces and recombined to generate diverse
instruction data."
INTRODUCTION,0.041170097508125676,"or tasks that these augmented datasets can cover is limited by the original input datasets. See Figure 1
37"
INTRODUCTION,0.04225352112676056,"for illustrations of these methods described above. There are also studies concentrated on developing
38"
INTRODUCTION,0.04333694474539545,"instruction-tuning datasets tailored to particular domains or tasks. For instance, [20] creates datasets
39"
INTRODUCTION,0.044420368364030335,"targeting mathematical reasoning. In contrast, [3] and [21] focus on coding-related tasks. All of the
40"
INTRODUCTION,0.04550379198266522,"above methods cannot produce instruction datasets that are generally applicable to a wide range of
41"
INTRODUCTION,0.04658721560130011,"domains.
42"
INTRODUCTION,0.047670639219934995,"How to create a general instruction tuning dataset? We draw inspiration from the systematic structure
43"
INTRODUCTION,0.04875406283856988,"in human education system. The structure of human education includes several levels, starting
44"
INTRODUCTION,0.04983748645720477,"from early childhood education up to higher education and beyond [37]. Within each level, a
45"
INTRODUCTION,0.050920910075839654,"student acquires knowledge, skills, and values in a systematic process. The courses a student learns
46"
INTRODUCTION,0.05200433369447454,"from primary school to college cover a broad range of knowledge and skills, which facilitates the
47"
INTRODUCTION,0.05308775731310943,"development of a diverse array of abilities. We believe that the systemic framework of the human
48"
INTRODUCTION,0.05417118093174431,"education system has the potential to help the generation of high-quality and general instruction data,
49"
INTRODUCTION,0.0552546045503792,"which spans a diverse range of disciplinary areas.
50"
INTRODUCTION,0.056338028169014086,"In this paper, we introduce a generalized instruction tuning paradigm GLAN (shorthand for
51"
INTRODUCTION,0.05742145178764897,"Generalized Instruction-Tuning for Large LANguage Models) to generate synthetic instruction
52"
INTRODUCTION,0.05850487540628386,"tuning data almost from scratch. Unlike existing work [39, 21, 20, 24], GLAN exclusively utilizes
53"
INTRODUCTION,0.059588299024918745,"a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale
54"
INTRODUCTION,0.06067172264355363,"instruction data systematically and automatically across all disciplines. Specifically, inspired by
55"
INTRODUCTION,0.06175514626218852,"the structure of the human education system, the input taxonomy is constructed by decomposing
56"
INTRODUCTION,0.0628385698808234,"human knowledge and capabilities to various fields, sub-fields, and, ultimately, distinct disciplines
57"
INTRODUCTION,0.06392199349945829,"semi-automatically, facilitated by LLMs and human verification. The cost of human verification
58"
INTRODUCTION,0.06500541711809317,"process is low due to the limited number of disciplines in the taxonomy. As shown in Figure 1,
59"
INTRODUCTION,0.06608884073672806,"we then further break down these disciplines into even smaller units. We continue to generate a
60"
INTRODUCTION,0.06717226435536294,"comprehensive list of subjects for every discipline and proceed to design a syllabus tailored to each
61"
INTRODUCTION,0.06825568797399784,"subject, again utilizing LLMs. With the fine-grained key concepts detailed in every class session
62"
INTRODUCTION,0.06933911159263272,"of the syllabus, we can first sample from them and then generate diverse instructions with broad
63"
INTRODUCTION,0.07042253521126761,"coverage across the entire spectrum of human knowledge and skills. The process described above
64"
INTRODUCTION,0.07150595882990249,"mirrors the human educational system, where educators in each discipline craft a series of subjects
65"
INTRODUCTION,0.07258938244853738,"for student learning. Instructors then develop a syllabus for each subject, breaking down the content
66"
INTRODUCTION,0.07367280606717226,"into specific class sessions. These sessions are then further divided into core concepts that students
67"
INTRODUCTION,0.07475622968580715,"must comprehend and internalize. Based on these detailed core concepts outlined in the syllabus,
68"
INTRODUCTION,0.07583965330444203,"teaching materials and exercises are subsequently created, which are our instruction tuning data.
69"
INTRODUCTION,0.07692307692307693,"GLAN is general, scalable and customizable. GLAN is a general method, which is task-agnostic
70"
INTRODUCTION,0.0780065005417118,"and is capable of covering a wide range of domains. GLAN is scalable. Similar to [32, 39], GLAN
71"
INTRODUCTION,0.0790899241603467,"generates instructions using LLMs, which can produce instructions on a massive scale. Moreover, the
72"
INTRODUCTION,0.08017334777898158,"input of GLAN is a taxonomy, which is generated by prompting an LLM and human verification,
73"
INTRODUCTION,0.08125677139761647,"requiring minimal human effort. GLAN allows for easy customization. New fields or skills can be
74"
INTRODUCTION,0.08234019501625135,"added by simply incorporating a new node into our taxonomy. Note that each node of the taxonomy
75"
INTRODUCTION,0.08342361863488625,"can be expanded independently, which means that we only need to apply our method to the newly
76"
INTRODUCTION,0.08450704225352113,"added nodes without re-generating the entire dataset. Extensive experiments on large language
77"
INTRODUCTION,0.08559046587215602,"models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical
78"
INTRODUCTION,0.0866738894907909,"reasoning, coding, academic exams, and logical reasoning to general instruction following without
79"
INTRODUCTION,0.08775731310942579,"using task-specific training data of these tasks.
80"
INTRODUCTION,0.08884073672806067,"2
GLAN: Generalized Instruction-Tuned Language Models
81"
INTRODUCTION,0.08992416034669556,"GLAN aims to create synthetic instruction data covering various domains of human knowledge
82"
INTRODUCTION,0.09100758396533044,"and capabilities on a large scale. As shown in Algorithm 1, we first build a taxonomy of human
83"
INTRODUCTION,0.09209100758396534,"knowledge and capabilities using frontier LLMs (i.e., GPT-4) and human verification. The taxonomy
84"
INTRODUCTION,0.09317443120260022,"naturally breaks down human knowledge and capabilities to fields, sub-fields, and ultimately different
85"
INTRODUCTION,0.09425785482123511,"disciplines (see Section 2.1). The following steps are fully autonomously facilitated by GPT-4 (or
86"
INTRODUCTION,0.09534127843986999,"GPT-3.5). Then for each discipline, we again instruct GPT-4 to further decompose it into a list of
87"
INTRODUCTION,0.09642470205850487,"subjects within this discipline (Section 2.2). Similar to an instructor, GPT-4 continues to design
88"
INTRODUCTION,0.09750812567713976,"a syllabus for each subject, which inherently breaks a subject into various class sessions with key
89"
INTRODUCTION,0.09859154929577464,"concepts students need to master (Section 2.3). With obtained class sessions and key concepts, we
90"
INTRODUCTION,0.09967497291440953,"are ready to construct synthetic instructions. We prompt GPT-4 to generate homework questions
91"
INTRODUCTION,0.10075839653304441,"based on randomly sampled class sessions and key concepts as well as the syllabus (Section 2.4).
92"
INTRODUCTION,0.10184182015167931,"We recursively decompose human knowledge and capabilities into smaller units until atomic-level
93"
INTRODUCTION,0.10292524377031419,"components (i.e., class sessions and key concepts). We expect to randomly combine these class
94"
INTRODUCTION,0.10400866738894908,"sessions and key concepts to ensure the coverage and diversity of synthetic instructions.
95"
INTRODUCTION,0.10509209100758396,Algorithm 1 GLAN Instruction Generation
INTRODUCTION,0.10617551462621885,"D ←build_taxonomy()
▷build a taxonomy and return a list of disciplines (Section 2.1)
L ←∅
for each discipline d ∈D do"
INTRODUCTION,0.10725893824485373,"S ←generate_subjects(d)
▷Obtain a list of subjects in d (Section 2.2)
for each subject s ∈S do"
INTRODUCTION,0.10834236186348863,"A ←generate_syllabus(s, d)
▷Return syllabus A for s (Section 2.3)
C, K ←extract_class_details(A)
▷Extract class sessions and key concepts
(Section 2.3)"
INTRODUCTION,0.1094257854821235,"Q ←generate_instructions(A, C, K, d)
▷Generate instructions by sampling class
sessions and key concepts (Section 2.4)"
INTRODUCTION,0.1105092091007584,"L ←L ∪Q
end for
end for
return L"
TAXONOMY OF HUMAN KNOWLEDGE AND CAPABILITIES,0.11159263271939328,"2.1
Taxonomy of Human Knowledge and Capabilities
96"
TAXONOMY OF HUMAN KNOWLEDGE AND CAPABILITIES,0.11267605633802817,"We build a taxonomy of human knowledge and capabilities to guide the generation of synthetic
97"
TAXONOMY OF HUMAN KNOWLEDGE AND CAPABILITIES,0.11375947995666305,"instructions. Therefore, its coverage is important. On the other hand, it is also essential to make
98"
TAXONOMY OF HUMAN KNOWLEDGE AND CAPABILITIES,0.11484290357529794,"the taxonomy highly extensible, since the preferred capabilities of LLMs may change over time.
99"
TAXONOMY OF HUMAN KNOWLEDGE AND CAPABILITIES,0.11592632719393282,"In the first step, we propose to generate the taxonomy by prompting GPT-4 with a set of different
100"
TAXONOMY OF HUMAN KNOWLEDGE AND CAPABILITIES,0.11700975081256772,"instructions (e.g., list all fields of human knowledge and capabilities). Then, we do
101"
TAXONOMY OF HUMAN KNOWLEDGE AND CAPABILITIES,0.1180931744312026,"human post-editing to ensure its correctness and completeness. Due to the limited number of fields,
102"
TAXONOMY OF HUMAN KNOWLEDGE AND CAPABILITIES,0.11917659804983749,"sub-fields, and disciplines in our taxonomy, the cost of human verification is reasonably low. Another
103"
TAXONOMY OF HUMAN KNOWLEDGE AND CAPABILITIES,0.12026002166847237,"advantage of human post-editing is that we can easily add new fields or disciplines to the taxonomy
104"
TAXONOMY OF HUMAN KNOWLEDGE AND CAPABILITIES,0.12134344528710726,"as needed.
105"
TAXONOMY OF HUMAN KNOWLEDGE AND CAPABILITIES,0.12242686890574214,"Our taxonomy currently covers a diverse range of knowledge and capabilities in both academic
106"
TAXONOMY OF HUMAN KNOWLEDGE AND CAPABILITIES,0.12351029252437704,"education and vocational training. The top level of the taxonomy contains fields such as Natural
107"
TAXONOMY OF HUMAN KNOWLEDGE AND CAPABILITIES,0.12459371614301191,"Sciences, Humanities, or Services (vocational training). These fields branch out to various sub-fields
108"
TAXONOMY OF HUMAN KNOWLEDGE AND CAPABILITIES,0.1256771397616468,"and/or disciplines such as Chemistry, Sociology or Retailing. We keep breaking down nodes of the
109"
TAXONOMY OF HUMAN KNOWLEDGE AND CAPABILITIES,0.1267605633802817,"taxonomy until disciplines, and we leave the breaking down of disciplines to automatic methods
110"
TAXONOMY OF HUMAN KNOWLEDGE AND CAPABILITIES,0.12784398699891658,"described in the following sections. By collecting the leaf nodes of the taxonomy, we obtain a list of
111"
TAXONOMY OF HUMAN KNOWLEDGE AND CAPABILITIES,0.12892741061755147,"disciplines D = {d1, d2, . . . , dM}.
112"
SUBJECT GENERATOR,0.13001083423618634,"2.2
Subject Generator
113"
SUBJECT GENERATOR,0.13109425785482123,"As in Algorithm 1, for each discipline d, we aim to extract the list of subjects in it through prompt
114"
SUBJECT GENERATOR,0.13217768147345613,"engineering. Specifically, we instruct GPT-4 to act as an education expert of discipline
115"
SUBJECT GENERATOR,0.13326110509209102,"d and design a list of subjects a student should learn. The completion of GPT-4
116"
SUBJECT GENERATOR,0.13434452871072589,"contains a comprehensive list of subjects and their meta data (e.g., level, introduction and subtopics
117"
SUBJECT GENERATOR,0.13542795232936078,"of the subject) in unstructured text format, which can not be directly used in subsequent steps. We
118"
SUBJECT GENERATOR,0.13651137594799567,"therefore used another round of prompting to convert the completion to JSONL format:
119"
SUBJECT GENERATOR,0.13759479956663057,"Awesome!
Transform the above to JSONL format so that it is easier for
120"
SUBJECT GENERATOR,0.13867822318526543,"a computer to understand.
Enclose the JSONL output between two sets of
121"
SUBJECT GENERATOR,0.13976164680390032,"triple backticks.
For each JSONL object, use the keys “subject_name”,
122"
SUBJECT GENERATOR,0.14084507042253522,"“level” and “subtopics”.
123"
SUBJECT GENERATOR,0.1419284940411701,"It is worth noting that generating a subject list in JSONL format using a single prompt is feasible.
124"
SUBJECT GENERATOR,0.14301191765980498,"However, we refrain to do so, because we observe that incorporating additional formatting instructions
125"
SUBJECT GENERATOR,0.14409534127843987,"directly into the prompt can compromise the quality of the resulting subject list. These extracted
126"
SUBJECT GENERATOR,0.14517876489707476,"subjects (as well as their meta data) S = {s1, s2, . . . , sN} can be subsequently used in next steps.
127"
SUBJECT GENERATOR,0.14626218851570963,"For each s ∈S, let s.name, s.level and s.subtopics denote the name, grade level and subtopics
128"
SUBJECT GENERATOR,0.14734561213434452,"of subject s, respectively. We can apply the above prompts multiple times to ensure better coverage
129"
SUBJECT GENERATOR,0.14842903575297942,"of subjects within this discipline.
130"
SYLLABUS GENERATOR,0.1495124593716143,"2.3
Syllabus Generator
131"
SYLLABUS GENERATOR,0.15059588299024917,"For each subject s, we have already extracted its name (s.name), grade level (s.level), and a
132"
SYLLABUS GENERATOR,0.15167930660888407,"small set of included sub-topics (s.subtopics) in a structured format. In this section, we aim to
133"
SYLLABUS GENERATOR,0.15276273022751896,"further segment each subject into smaller units, making them more suitable for creating homework
134"
SYLLABUS GENERATOR,0.15384615384615385,"assignments. We consult GPT-4 to design a syllabus for this subject. We opt for syllabus generation
135"
SYLLABUS GENERATOR,0.15492957746478872,"for the following reasons. Firstly, a syllabus essentially breaks down the main topic of a subject
136"
SYLLABUS GENERATOR,0.1560130010834236,"into smaller segments in a hierarchical manner. Specifically, each subject comprises several class
137"
SYLLABUS GENERATOR,0.1570964247020585,"sessions, and each session covers a variety of sub-topics and key concepts. Secondly, a syllabus
138"
SYLLABUS GENERATOR,0.1581798483206934,"provides an introduction, objectives, and expected outcomes of a subject, which are inherently useful
139"
SYLLABUS GENERATOR,0.15926327193932827,"for formulating homework questions. We instruct GPT-4 to 1) design a syllabus based on its meta
140"
SYLLABUS GENERATOR,0.16034669555796316,"data (s.level, s.name and s.subtopics); 2) break the subject into different class sessions; 3)
141"
SYLLABUS GENERATOR,0.16143011917659805,"provide details for each class session with a description and detailed key concepts students need to
142"
SYLLABUS GENERATOR,0.16251354279523295,"master.
143"
SYLLABUS GENERATOR,0.1635969664138678,"Let A denote the generated syllabus. The resulting syllabus A is in unstructured text format. However,
144"
SYLLABUS GENERATOR,0.1646803900325027,"class session names and key concepts of each class are required in the instruction generation step (see
145"
SYLLABUS GENERATOR,0.1657638136511376,"Algorithm 1). Similar to the process of subject list extraction in Section 2.2, we again extract these
146"
SYLLABUS GENERATOR,0.1668472372697725,"meta data of each class session by prompting GPT-4. As a result, we obtain a list of class sessions
147"
SYLLABUS GENERATOR,0.16793066088840736,"C = {c1, c2, . . . , c|C|} and their corresponding key concepts K = {k1, k2, . . . , k|C|}. The detailed
148"
SYLLABUS GENERATOR,0.16901408450704225,"prompt for syllabus generation is in Appendix A.3.
149"
INSTRUCTION GENERATOR,0.17009750812567714,"2.4
Instruction Generator
150"
INSTRUCTION GENERATOR,0.17118093174431204,"Given a syllabus A as well as a list of its class sessions C and their associated key concepts K,
151"
INSTRUCTION GENERATOR,0.1722643553629469,"we are ready to generate homework questions and their answers. To generate diverse homework
152"
INSTRUCTION GENERATOR,0.1733477789815818,"questions, we first sample one or two class session names from C and one to five key concepts under
153"
INSTRUCTION GENERATOR,0.1744312026002167,"these selected class sessions. Let ˆC denote the selected class session names and ˆK the selected key
154"
INSTRUCTION GENERATOR,0.17551462621885158,"concepts. Then we prompt GPT-4 (or GPT-3.5) to generate a homework question given the selected
155"
INSTRUCTION GENERATOR,0.17659804983748645,"class sessions ˆC and key concepts ˆK as well as the syllabus A. We intend to give GPT-4/3.5 more
156"
INSTRUCTION GENERATOR,0.17768147345612134,"context (e.g., what students have already learned in previous sessions) when creating assignments.
157"
INSTRUCTION GENERATOR,0.17876489707475623,"Therefore, we additionally instruct GPT to consider that students have learned up to class sessions ˆC
158"
INSTRUCTION GENERATOR,0.17984832069339113,"when crafting homework and try to leverage multiple key concepts across different class sessions.
159"
INSTRUCTION GENERATOR,0.180931744312026,"See details of our prompt for instruction generation in Appendix A.4.
160"
INSTRUCTION GENERATOR,0.1820151679306609,"Sampling Class Sessions and Key Concepts
In a single syllabus, there are numerous class sessions
161"
INSTRUCTION GENERATOR,0.18309859154929578,"and key concepts. We have two strategies to sample from them. In the first strategy, we generate
162"
INSTRUCTION GENERATOR,0.18418201516793067,"assignments from a single class session. Therefore, we have only one class session name. Suppose
163"
INSTRUCTION GENERATOR,0.18526543878656554,"we have m key concepts in total in this session. We randomly sample one to five key concepts from
164"
INSTRUCTION GENERATOR,0.18634886240520043,"the m key concepts, which means we have totally P5
i=1
 m
i

combinations. In this strategy, we focus
165"
INSTRUCTION GENERATOR,0.18743228602383533,"on creating basic homework questions. To make the resulting questions more challenging (combine
166"
INSTRUCTION GENERATOR,0.18851570964247022,"knowledge from multiple class sessions), we propose a second strategy to combine key concepts
167"
INSTRUCTION GENERATOR,0.18959913326110509,"from two class sessions in the second strategy. We intend to generate questions leverage knowledge
168"
INSTRUCTION GENERATOR,0.19068255687973998,"from two different class sessions. Suppose we have m1 and m2 key concepts in the first and second
169"
INSTRUCTION GENERATOR,0.19176598049837487,"class sessions, respectively. We can have P5
i=2
 m1+m2
i

−P5
i=2
 m1
i

−P5
i=2
 m2
i

different
170"
INSTRUCTION GENERATOR,0.19284940411700974,"combinations, which is significantly more than that of the first strategy. We use both strategies to
171"
INSTRUCTION GENERATOR,0.19393282773564463,"ensure our created questions are diverse in difficulty levels.
172"
INSTRUCTION GENERATOR,0.19501625135427952,"Answer Generation
After we generate questions in previous steps, we simply send these questions
173"
INSTRUCTION GENERATOR,0.19609967497291442,"to GPT-3.5 and collect answers. We use GPT-3.5 for answer generation, because we find the quality
174"
INSTRUCTION GENERATOR,0.19718309859154928,"of generated answers from GPT-3.5 is sufficiently good and using GPT-3.5 is significantly faster
175"
INSTRUCTION GENERATOR,0.19826652221018418,"than GPT-4. The resulting question-answer pairs are our instruction tuning data. With a huge amount
176"
INSTRUCTION GENERATOR,0.19934994582881907,"of question-answer pairs ranging from different disciplines with various difficulty levels, we expect
177"
INSTRUCTION GENERATOR,0.20043336944745396,"the resulting LLM can excel in a wide range of tasks.
178"
EXPERIMENTS,0.20151679306608883,"3
Experiments
179"
DATA GENERATION,0.20260021668472372,"3.1
Data Generation
180"
DATA GENERATION,0.20368364030335862,"Taxonomy Creation
By asking GPT-4 to create a taxonomy of human knowledge and capabilities,
181"
DATA GENERATION,0.2047670639219935,"we end up with a set of fields, sub-fields, and disciplines that cover a broad range of domains in human
182"
DATA GENERATION,0.20585048754062837,"knowledge and capabilities. Next, we ask human annotators to decide whether these elements in the
183"
DATA GENERATION,0.20693391115926327,"taxonomy should be kept or not in order to reduce the redundancy of the taxonomy while maintaining
184"
DATA GENERATION,0.20801733477789816,"its correctness. Note that if a field or sub-field is marked as remove, we remove its descendant as
185"
DATA GENERATION,0.20910075839653305,"well. We kept 126 disciplines after majority voting (provided in supplementary materials). Note that
186"
DATA GENERATION,0.21018418201516792,"it is feasible to manually add extra disciplines, sub-fields, or fields whenever necessary.
187"
DATA GENERATION,0.2112676056338028,"Subject and Syllabus Generation
During the subject list and syllabus generation, we prompt
188"
DATA GENERATION,0.2123510292524377,"GPT-4 and employ nucleus sampling [14] with temperature T = 1.0 and top-p = 0.95 to encourage
189"
DATA GENERATION,0.2134344528710726,"diversity. We do not use GPT-3.5-turbo since some subjects belong to the long-tail distribution
190"
DATA GENERATION,0.21451787648970747,"which may not be effectively modeled by GPT-3.5-turbo. To ensure diversity and completeness of
191"
DATA GENERATION,0.21560130010834236,"the generated subjects, we query GPT-4 10 times for each discipline (Section 2.2). There are 100 to
192"
DATA GENERATION,0.21668472372697725,"200 subjects for each discipline on average. It is worth noting that the same subjects may appear in
193"
DATA GENERATION,0.21776814734561215,"different disciplines. For instance, the subject calculus is both in physics and mathematics. We do
194"
DATA GENERATION,0.218851570964247,"not de-duplicate those subjects, since it may reflect their importance in human knowledge. Given a
195"
DATA GENERATION,0.2199349945828819,"subject in a specified discipline, we query GPT-4 for only one time to design a syllabus (see details in
196"
DATA GENERATION,0.2210184182015168,"section 2.3). The temperature and top-p are still set to 1.0 and 0.95, respectively. The number of class
197"
DATA GENERATION,0.2221018418201517,"sessions contained in each syllabus varies from 10 to 30 and each class session contains around five
198"
DATA GENERATION,0.22318526543878656,"key concepts.
199"
DATA GENERATION,0.22426868905742145,"Instruction Generation
Each instruction data consists of a question and its answer. We choose to
200"
DATA GENERATION,0.22535211267605634,"generate questions and answers separately since we observed that separate generations lead to better
201"
DATA GENERATION,0.22643553629469124,"quality. After question generation with GPT-4, each question is then answered by GPT-3.5-turbo
202"
DATA GENERATION,0.2275189599133261,"with temperature T = 0.7, top-p = 0.95 (we use a lower temperature in order to make the re-
203"
DATA GENERATION,0.228602383531961,"sulting answers more accurate). We use GPT-3.5-turbo instead of GPT-4 for answer generation,
204"
DATA GENERATION,0.2296858071505959,"because GPT-3.5-turbo is significantly faster with reasonably good results. We generate 10 million
205"
DATA GENERATION,0.23076923076923078,"instruction-response pairs in total and then we do training data decontamination. Specifically, the
206"
DATA GENERATION,0.23185265438786565,"training instruction-response pairs are decontaminated by removing pairs that contain questions or
207"
DATA GENERATION,0.23293607800650054,"Table 1: Main results on Mathematical Reasoning, Coding, Logical Reasoning, and Academic Exam
benchmarks. Best results are in boldface, while the second best results are underscored."
DATA GENERATION,0.23401950162513543,"Model
|θ| HumanE MBPP GSM8K MATH BBH ARC-E ARC-C MMLU"
DATA GENERATION,0.23510292524377033,"GPT-4
–
88.4
80.0
92.0
52.9
86.7
95.4
93.6
86.4
GPT-3.5-turbo
–
72.6
70.8
74.1
37.8
70.1
88.9
83.7
70.0"
DATA GENERATION,0.2361863488624052,"LLaMA2
7B
12.8
36.2
15.4
4.2
39.6
74.6
46.3
45.9
Orca 2
7B
17.1
28.4
55.7
10.1
42.8
87.8
78.4
53.9
WizardLM v1.2
13B
31.7
47.9
46.8
9.0
48.4
74.2
50.2
52.7
Mistral
7B
28.0
50.2
43.4
10.0
56.1
79.5
53.9
62.3
Mistral Instruct
7B
46.7
31.7
24.4
8.2
46.0
76.9
52.0
53.7
MetaMath Mistral
7B
35.4
48.6
77.7
28.2
55.7
77.3
51.0
61.0
WizardMath v1.1
7B
51.2
54.1
83.2
33.0
58.2
79.8
53.2
60.3
Mistral CodeAlpaca 7B
35.4
50.2
34.6
8.3
56.1
79.1
54.2
60.9"
DATA GENERATION,0.2372697724810401,"GLAN
7B
48.8
57.6
80.8
32.7
60.7
90.7
81.1
62.9"
DATA GENERATION,0.23835319609967498,"input prompts from the test and training (if any) sets of benchmarks we evaluate. We exclude the
208"
DATA GENERATION,0.23943661971830985,"training set of benchmarks we evaluate to verify the generalization capability of our synthetic data.
209"
MODEL TRAINING,0.24052004333694474,"3.2
Model Training
210"
MODEL TRAINING,0.24160346695557963,"We employ Mistral 7B [16] as our base model. During training, we concatenate each instruction and
211"
MODEL TRAINING,0.24268689057421453,"response pair to a single sequence and only compute loss on response tokens. We train our model for
212"
MODEL TRAINING,0.2437703141928494,"3 epochs with a learning rate of 3e-6. The batch size is set to approximately 512 instruction-response
213"
MODEL TRAINING,0.24485373781148428,"pairs. We employ a dynamic batch size to ensure a constant total number of tokens per batch. We
214"
MODEL TRAINING,0.24593716143011918,"use a cosine learning rate schedule and we start with a linear warm-up of 1000 steps and the final
215"
MODEL TRAINING,0.24702058504875407,"learning rate is reduced to 0. The training requires approximately 8 days using 32 A100 GPUs.
216"
BENCHMARK EVALUATION,0.24810400866738894,"3.3
Benchmark Evaluation
217"
BENCHMARK EVALUATION,0.24918743228602383,"The instruction data GLAN generated spans a wide range of subjects. We evaluate its effectiveness
218"
BENCHMARK EVALUATION,0.2502708559046587,"in mathematical reasoning, coding, logical reasoning, and academic exams.
219"
BENCHMARK EVALUATION,0.2513542795232936,"Mathematical Reasoning: Mathematics is a common subject in many different disciplines. Hence, it
220"
BENCHMARK EVALUATION,0.2524377031419285,"is necessary to test the math reasoning ability of GLAN. We choose the two popular benchmarks for
221"
BENCHMARK EVALUATION,0.2535211267605634,"evaluation (i.e., GSM8K [7] and MATH [12]). GSM8K [7] is a high-quality math problem dataset
222"
BENCHMARK EVALUATION,0.25460455037919827,"that measures the basic multi-step mathematical reasoning ability. It contains around 7k problems for
223"
BENCHMARK EVALUATION,0.25568797399783316,"training and 1K problems for test. MATH [12] is a challenging math dataset that contains mathematics
224"
BENCHMARK EVALUATION,0.25677139761646806,"competition-level problems from AMC, AIME, etc. The 7.5k training and 5K test problems cover
225"
BENCHMARK EVALUATION,0.25785482123510295,"seven math subjects, i.e., Prealgebra, Precalculus, Algebra, Intermediate Algebra, Number Theory,
226"
BENCHMARK EVALUATION,0.2589382448537378,"Counting and Probability, and Geometry. Note that GLAN does not use any examples in the training
227"
BENCHMARK EVALUATION,0.2600216684723727,"set of GSM8K or MATH. Following [20], we report 0-shot setting results for GLAN. Coding: To
228"
BENCHMARK EVALUATION,0.2611050920910076,"evaluate the coding capability of GLAN, we opt for two coding benchmarks HumanEval [4] and
229"
BENCHMARK EVALUATION,0.26218851570964247,"MBPP [1]. We employ 0-shot setting for HumanEval and 3-shot setting for MBPP following prior art
230"
BENCHMARK EVALUATION,0.26327193932827736,"[4, 21]. BBH: The instruction dataset we generated covers many disciplines, which can potentially
231"
BENCHMARK EVALUATION,0.26435536294691225,"enhance the reasoning ability of GLAN. Therefore, we evaluate GLAN on the BIG-Bench Hard
232"
BENCHMARK EVALUATION,0.26543878656554715,"dataset (BBH [29]), which contains 23 challenging tasks from Big-Bench [28]. We employ the
233"
BENCHMARK EVALUATION,0.26652221018418204,"standard 3-shot setting with chain-of-thought demonstrations. Academic Exams: We also evaluate
234"
BENCHMARK EVALUATION,0.2676056338028169,"GLAN on different academic benchmarks to verify whether GLAN is capable of solving exam
235"
BENCHMARK EVALUATION,0.26868905742145177,"questions. We choose two benchmarks (i.e., ARC [6] and MMLU [11]). Both benchmarks are
236"
BENCHMARK EVALUATION,0.26977248104008666,"composed of multi-choice questions. AI2 Reasoning Challenge (ARC [6]) contains grade-school
237"
BENCHMARK EVALUATION,0.27085590465872156,"level, multi-choice science questions. It contains two sub-sets, which are ARC-Challenge (ARC-C)
238"
BENCHMARK EVALUATION,0.27193932827735645,"and ARC-Easy (ARC-E). Massive Multitask Language Understanding (MMLU [11]) consists of a
239"
BENCHMARK EVALUATION,0.27302275189599134,"set of multiple-choice questions about 57 subjects ranging in difficulty from elementary levels to
240"
BENCHMARK EVALUATION,0.27410617551462624,"professional levels. It covers various of domains of knowledge, including humanities, STEM and
241"
BENCHMARK EVALUATION,0.27518959913326113,"social sciences. Note that there is a training set for ARC. However, we have excluded it from our
242"
BENCHMARK EVALUATION,0.27627302275189597,Table 2: Detailed Results on Academic Exam benchmarks.
BENCHMARK EVALUATION,0.27735644637053086,"Model ARC-E ARC-C
MMLU
STEM Humanities Social Sciences Other"
BENCHMARK EVALUATION,0.27843986998916576,"Mistral
79.5
53.9
52.0
56.5
73.3
70.1
GLAN
90.7
81.1
60.1
54.9
71.8
68.6"
BENCHMARK EVALUATION,0.27952329360780065,"5e4
2e5
5e5 1e6
1e7 42 43 44 45 46 47 48 49"
BENCHMARK EVALUATION,0.28060671722643554,Pass@1
BENCHMARK EVALUATION,0.28169014084507044,HumanEval
BENCHMARK EVALUATION,0.28277356446370533,"5e4
2e5
5e5 1e6
1e7 58.5 59.0 59.5 60.0 60.5"
BENCHMARK EVALUATION,0.2838569880823402,Exact Match BBH
BENCHMARK EVALUATION,0.28494041170097506,"5e4
2e5
5e5 1e6
1e7 60 65 70 75 80"
BENCHMARK EVALUATION,0.28602383531960995,Exact Match GSM8K
BENCHMARK EVALUATION,0.28710725893824485,"5e4
2e5
5e5 1e6
1e7 20 22 24 26 28 30 32"
BENCHMARK EVALUATION,0.28819068255687974,Exact Match MATH
BENCHMARK EVALUATION,0.28927410617551463,"Figure 2: The scaling curve of GLAN on downstream tasks. The x-axis denotes GLAN data size (in
log10 scale following [17]), and the y-axis denotes the task performance."
BENCHMARK EVALUATION,0.2903575297941495,"training set during the decontamination process described in Section 3.1. Previous models mostly
243"
BENCHMARK EVALUATION,0.2914409534127844,"leverage probability-based methods on ARC and MMLU, which returns the best option based on the
244"
BENCHMARK EVALUATION,0.29252437703141926,"probabilities of the four options conditioned on the corresponding multi-choice question. We observe
245"
BENCHMARK EVALUATION,0.29360780065005415,"that after training on 10 million instructions, GLAN is able to generate its predicted options and
246"
BENCHMARK EVALUATION,0.29469122426868904,"analysis of multi-choice questions in plain text as GPT-3.5 does. We therefore opt for 0-shot setting
247"
BENCHMARK EVALUATION,0.29577464788732394,"for GLAN and extract predictions using rules based on its completions as in [22].
248"
BENCHMARK EVALUATION,0.29685807150595883,"Results
Our main results are shown in Table 1. We compare GLAN against general domain models
249"
BENCHMARK EVALUATION,0.2979414951245937,"(Orca 2 [22], Mistral Instruct [16] and WizardLM [39]), math optimized models (MetaMath [40]
250"
BENCHMARK EVALUATION,0.2990249187432286,"and WizardMath [20]) and coding optimized models (CodeAlpaca [3]). We also report results of
251"
BENCHMARK EVALUATION,0.3001083423618635,"base LLMs (i.e., LLaMA2 [31] and Mistral [16]) as references. GLAN either obtains the best results
252"
BENCHMARK EVALUATION,0.30119176598049835,"or results close to the best across all benchmarks. We observe that capabilities of math or coding
253"
BENCHMARK EVALUATION,0.30227518959913324,"optimized models increase on math or coding benchmarks while usually not others. After instruction
254"
BENCHMARK EVALUATION,0.30335861321776814,"tuning, GLAN excels on multiple dimensions from mathematical reasoning, coding, reasoning, and
255"
BENCHMARK EVALUATION,0.30444203683640303,"academic exams with a systematical data generation approach. Also note that our method does not
256"
BENCHMARK EVALUATION,0.3055254604550379,"use any task-specific training data such as training sets of GSM8K, MATH, or ARC as in Orca 2,
257"
BENCHMARK EVALUATION,0.3066088840736728,"MetaMath, and WizardMath, which indicates the general applicability of GLAN.
258"
BENCHMARK EVALUATION,0.3076923076923077,"A Closer Look at Academic Exams
ARC and MMLU are all multi-choice based benchmarks on
259"
BENCHMARK EVALUATION,0.3087757313109426,"academic exams. However, we observe that improvements of GLAN over Mistral on ARC are much
260"
BENCHMARK EVALUATION,0.30985915492957744,"larger than these on MMLU (see Table 1). By grouping the 57 subjects in MMLU into four categories
261"
BENCHMARK EVALUATION,0.31094257854821233,"(i.e., STEM, Humanities, Social Sciences, and Other (business, health, misc.)), we observe GLAN
262"
BENCHMARK EVALUATION,0.3120260021668472,"wildly improves on STEM in MMLU while not in other categories (Table 2). Also note that ARC
263"
BENCHMARK EVALUATION,0.3131094257854821,"is composed of high school science problems, which are also STEM questions. GLAN is good at
264"
BENCHMARK EVALUATION,0.314192849404117,"STEM subjects may be because responses of our dataset are from GPT-3.5-turbo, which by default
265"
BENCHMARK EVALUATION,0.3152762730227519,"generates responses with Chain-of-Thoughts (CoT) reasoning. Indeed, we observe that GLAN
266"
BENCHMARK EVALUATION,0.3163596966413868,"generates solutions with CoT for multi-choice questions. CoT may help the multi-step reasoning in
267"
BENCHMARK EVALUATION,0.3174431202600217,"STEM multi-choice questions [35], while humanities and social sciences questions involve more
268"
BENCHMARK EVALUATION,0.31852654387865653,"memorization and single-step reasoning, where CoT may introduce additional errors.
269"
SCALING PROPERTY OF GLAN,0.3196099674972914,"3.4
Scaling Property of GLAN
270"
SCALING PROPERTY OF GLAN,0.3206933911159263,"We investigate the scaling property of GLAN by training Mistral on different numbers of examples
271"
SCALING PROPERTY OF GLAN,0.3217768147345612,"(i.e., 50K, 200K, 500K, 1M, and 10M) we generated. The results on downstream tasks are shown in
272"
SCALING PROPERTY OF GLAN,0.3228602383531961,"Figure 2. It can be observed that overall task performance tends to increase as we increase the data size.
273"
SCALING PROPERTY OF GLAN,0.323943661971831,"Notably, the curve has not reached a plateau, indicating the potential for further improvement through
274"
SCALING PROPERTY OF GLAN,0.3250270855904659,"the continued scaling of the data size of GLAN. However, we defer further scaling experiments to
275"
SCALING PROPERTY OF GLAN,0.3261105092091008,"future work.
276"
SCALING PROPERTY OF GLAN,0.3271939328277356,"Table 3: The evaluation of loss values between the test data and training data. Large positive ∆(or
∆(%)) indicates task-specific in-domain training data might be exposed to the model during training."
SCALING PROPERTY OF GLAN,0.3282773564463705,"Benchmark/Loss
LLaMA2-7B
Orca2-7B
Mistral-7B-Instruct
WizardLM-13B-V1.2
GLAN-7B"
SCALING PROPERTY OF GLAN,0.3293607800650054,"ARC-C
∆
-0.01
0.05
-0.01
-0.01
-0.03
∆(%)
-0.5%
2.10%
-0.43%
-0.47%
-0.74%"
SCALING PROPERTY OF GLAN,0.3304442036836403,"ARC-E
∆
-0.02
0.04
-0.03
-0.02
-0.01
∆(%)
-0.95%
1.61%
-1.19%
-0.91%
-0.23%"
SCALING PROPERTY OF GLAN,0.3315276273022752,"GSM8K
∆
0
0.13
0
0.05
0.02
∆(%)
0%
11.4%
0%
4.39%
0.92%"
SCALING PROPERTY OF GLAN,0.3326110509209101,"MATH
∆
-0.03
0.03
-0.03
-0.02
-0.03
∆(%)
-2.70%
2.54%
-2.67%
-1.63%
-1.79%"
TASK-SPECIFIC TRAINING DATA,0.333694474539545,"3.5
Task-specific Training Data
277"
TASK-SPECIFIC TRAINING DATA,0.3347778981581798,"GLAN is a generalized method to create synthetic data for instruction tuning. In order to evaluate
278"
TASK-SPECIFIC TRAINING DATA,0.3358613217768147,"the generalization capabilities of this synthetic data, we deliberately exclude task-specific training
279"
TASK-SPECIFIC TRAINING DATA,0.3369447453954496,"sets from all benchmarks on which we conduct our assessments. Similar to [36], we explore whether
280"
TASK-SPECIFIC TRAINING DATA,0.3380281690140845,"models have been trained on task-specific in-domain data. We compute the training loss Ltrain and
281"
TASK-SPECIFIC TRAINING DATA,0.3391115926327194,"test loss Ltest on ARC Challenge (ARC-C), GSM8K, and MATH for GLAN and other models in
282"
TASK-SPECIFIC TRAINING DATA,0.3401950162513543,"comparison. We choose these datasets because among all benchmarks evaluated in Section 3.3, these
283"
TASK-SPECIFIC TRAINING DATA,0.3412784398699892,"benchmarks contain training sets. Intuitively, the larger ∆= Ltest −Ltrain is, the more likely the
284"
TASK-SPECIFIC TRAINING DATA,0.3423618634886241,"training set is exposed. To make ∆easier to interpret, we additionally compute the relative difference
285"
TASK-SPECIFIC TRAINING DATA,0.3434452871072589,"∆(%) = (Ltest −Ltrain)/Ltest. Table 3 shows the losses of the training and test splits for GLAN
286"
TASK-SPECIFIC TRAINING DATA,0.3445287107258938,"are nearly identical (or ∆is negative). This suggests that GLAN has not been exposed to in-domain
287"
TASK-SPECIFIC TRAINING DATA,0.3456121343445287,"data during training and tuning procedures. Please refer detailed Ltrain and Ltest losses in Table 8 (in
288"
TASK-SPECIFIC TRAINING DATA,0.3466955579631636,"Appendix). Additionally, as shown in Table 8, we observe that GLAN obtains higher losses on both
289"
TASK-SPECIFIC TRAINING DATA,0.3477789815817985,"test and training splits on GSM8K, MATH, and ARC compared to other models, while performances
290"
TASK-SPECIFIC TRAINING DATA,0.3488624052004334,"of GLAN on these datasets are high (see Table 1). This might imply that synthetic data generated by
291"
TASK-SPECIFIC TRAINING DATA,0.34994582881906827,"GLAN is diverse and our resulting model avoids convergence to any specific domain or style present
292"
TASK-SPECIFIC TRAINING DATA,0.35102925243770317,"in existing benchmarks.
293"
INSTRUCTION FOLLOWING EVALUATION,0.352112676056338,"3.6
Instruction Following Evaluation
294"
INSTRUCTION FOLLOWING EVALUATION,0.3531960996749729,"IFEval
We assess the instruction-following capabilities of GLAN utilizing the Instruction Fol-
295"
INSTRUCTION FOLLOWING EVALUATION,0.3542795232936078,"lowing Evaluation dataset (IFEval [42]). IFEval consists of a collection of “verifiable instructions”,
296"
INSTRUCTION FOLLOWING EVALUATION,0.3553629469122427,"encompassing 25 distinct types of instructions (around 500 prompts in total). Each prompt comprises
297"
INSTRUCTION FOLLOWING EVALUATION,0.3564463705308776,"one or more verifiable instructions. The evaluation involves four types of metrics at both prompt
298"
INSTRUCTION FOLLOWING EVALUATION,0.35752979414951247,"level and instruction level, evaluating strict and loose accuracies. As shown in Table 4, GLAN
299"
INSTRUCTION FOLLOWING EVALUATION,0.35861321776814736,"demonstrates superior instruction-following capabilities in both prompt-level and instruction-level
300"
INSTRUCTION FOLLOWING EVALUATION,0.35969664138678226,"evaluations. However, there is still a considerable gap compared to GPT-3.5-turbo and GPT-4.
301"
INSTRUCTION FOLLOWING EVALUATION,0.3607800650054171,Table 4: Instruction following capability evaluation on IFEval.
INSTRUCTION FOLLOWING EVALUATION,0.361863488624052,"Model
Prompt-level
strict-accuracy
Instruction-level
strict-accuracy
Prompt-level
strict-accuracy
Instruction-level
loose-accuracy"
INSTRUCTION FOLLOWING EVALUATION,0.3629469122426869,"GPT-3.5-turbo
53.8
64.7
56.6
67.5
GPT-4
77.1
83.7
79.7
85.6"
INSTRUCTION FOLLOWING EVALUATION,0.3640303358613218,"LLaMA2-7B
14.8
27.1
16.6
29.4
Orca2-7B
19.4
28.9
26.1
34.7
Mistral-7B-Instruct-v0.1
32.0
42.8
37.7
48.0
WizardLM-13B-V1.2
23.1
33.5
26.6
37.6
GLAN-7B
34.0
44.8
41.2
51.6"
INSTRUCTION FOLLOWING EVALUATION,0.36511375947995667,"Evol-Instruct Test
Evol-Instruct testset [39] contains real-world human instructions from diverse
302"
INSTRUCTION FOLLOWING EVALUATION,0.36619718309859156,"sources, and it consists of 218 instances with 29 distinct skills. Each instruction is associated with
303"
INSTRUCTION FOLLOWING EVALUATION,0.36728060671722645,"a difficulty level from 1 to 10. The responses are often open-ended descriptions, and we believe
304"
INSTRUCTION FOLLOWING EVALUATION,0.36836403033586135,"this benchmark is a necessary supplement to IFEval (answers to their instructions are “verifiable”).
305"
INSTRUCTION FOLLOWING EVALUATION,0.3694474539544962,"Following [39] and [5], we adopt a GPT-4-based automatic evaluation method to conduct a pairwise
306"
INSTRUCTION FOLLOWING EVALUATION,0.3705308775731311,"comparison between GLAN and other models. Specifically, GPT-4 is instructed to assign a score
307"
INSTRUCTION FOLLOWING EVALUATION,0.37161430119176597,"between 1 and 10 overall score w.r.t. the helpfulness, relevance, accuracy, and level of detail of
308"
INSTRUCTION FOLLOWING EVALUATION,0.37269772481040087,"Table 5: Pairwise comparison on various difficulty levels between GLAN and other models on
Evol-Instruct testset. The scores are the average gap of scores assigned by GPT-4, calculated as
avg_score(GLAN) −avg_score(x)."
INSTRUCTION FOLLOWING EVALUATION,0.37378114842903576,"Difficulty
Ratio
LLaMA2-7B
Orca2-7B
Mistral-7B-Instruct
Wizard-13B-V1.2
GPT-3.5-turbo"
INSTRUCTION FOLLOWING EVALUATION,0.37486457204767065,"(1-5) Easy
41.00%
5.46
2.19
1.13
1.32
-1.22
(6-10) Hard
59.00%
5.38
2.28
1.68
0.99
-0.68"
INSTRUCTION FOLLOWING EVALUATION,0.37594799566630555,"responses generated by two different models for a given input question. A higher score indicates
309"
INSTRUCTION FOLLOWING EVALUATION,0.37703141928494044,"better overall performance. To mitigate potential order bias, we perform bidirectional comparisons
310"
INSTRUCTION FOLLOWING EVALUATION,0.3781148429035753,"for each response pair and determine their average score. The average score difference to GLAN
311"
INSTRUCTION FOLLOWING EVALUATION,0.37919826652221017,"(i.e., avg_score(GLAN) −avg_score(x)) serves as the final metric. Table 5 presents the results
312"
INSTRUCTION FOLLOWING EVALUATION,0.38028169014084506,"of pairwise comparisons across various levels of instruction difficulty. GLAN showcases superior
313"
INSTRUCTION FOLLOWING EVALUATION,0.38136511375947996,"performance compared to LLaMA-2, Orca 2, Mistral Instruct, and even WizardLM-13B (note that
314"
INSTRUCTION FOLLOWING EVALUATION,0.38244853737811485,"GLAN contains only 7B parameters) on most difficulty levels and overall scores. This suggests that
315"
INSTRUCTION FOLLOWING EVALUATION,0.38353196099674974,"GLAN demonstrates improved ability to process diverse instructions, regardless of their difficulty
316"
INSTRUCTION FOLLOWING EVALUATION,0.38461538461538464,"or complexity. Also, note that GLAN falls behind GPT-3.5-turbo as other models in comparison.
317"
INSTRUCTION FOLLOWING EVALUATION,0.3856988082340195,"Additionally, we group Evol-Instruct test according to the 29 skills and observe the same trends.
318"
INSTRUCTION FOLLOWING EVALUATION,0.38678223185265437,"Detailed results are listed in Appendix (Table 9 and 10). GLAN demonstrates strong performance on
319"
INSTRUCTION FOLLOWING EVALUATION,0.38786565547128926,"most skills, especially in Math, Coding, and Reasoning. However, it slightly falls short in common-
320"
INSTRUCTION FOLLOWING EVALUATION,0.38894907908992415,"sense related tasks. We also created GLAN-Test, similar to the Evol-Instruct Test but much larger in
321"
INSTRUCTION FOLLOWING EVALUATION,0.39003250270855905,"size, where GLAN outperforms other models as well (see Appendix A.8).
322"
RELATED WORK,0.39111592632719394,"4
Related Work
323"
RELATED WORK,0.39219934994582883,"Recent literature has extensively explored the collection of various human-made resources for
324"
RELATED WORK,0.39328277356446373,"instruction tuning. An intuitive direction is to collect existing NLP datasets and corresponding
325"
RELATED WORK,0.39436619718309857,"task descriptions [26, 33, 41], typical LLMs such as BLOOMZ [23] and FLAN [34] are trained
326"
RELATED WORK,0.39544962080173346,"on this type of instruction tuning data. However, with only tens to thousands of existing datasets
327"
RELATED WORK,0.39653304442036835,"available, the scope and diversity of instruction tuning are inevitably limited. Another common
328"
RELATED WORK,0.39761646803900325,"practice is to implement instruction tuning with real-world human user prompts. For instance,
329"
RELATED WORK,0.39869989165763814,"InstructGPT [25] was trained on high-quality human prompts submitted by real-world users to
330"
RELATED WORK,0.39978331527627303,"OpenAI GPT APIs. Vicuna [5] leverages user-shared prompts along with ChatGPT responses for
331"
RELATED WORK,0.4008667388949079,"instruction tuning, and Dolly[8] was trained on simulated human-user interactions written by over
332"
RELATED WORK,0.4019501625135428,"5k employees. Nevertheless, acquiring instructional data from human users typically involves high
333"
RELATED WORK,0.40303358613217766,"costs and involves privacy concerns. As LLM capabilities improve, instruction tuning with LLM-
334"
RELATED WORK,0.40411700975081255,"generated data exhibits better scalability and potential in addressing the super-alignment problem [27].
335"
RELATED WORK,0.40520043336944744,"Leveraging the in-context learning ability of LLMs, Unnatural instructions [15] and Self-instruct [32]
336"
RELATED WORK,0.40628385698808234,"sampled seed instructions as examples to elicit LLMs to generate new instructions. Taking advantage
337"
RELATED WORK,0.40736728060671723,"of the rephrasing ability of LLMs, WizardLM [39] and WizardMath [20] were trained using Evol-
338"
RELATED WORK,0.4084507042253521,"Instruct. Evol-Instruct iteratively employs ChatGPT to rewrite seed instructions into increasingly
339"
RELATED WORK,0.409534127843987,"complex instructions. Similar to generation from seed instructions, carefully selected seed topics
340"
RELATED WORK,0.4106175514626219,"are used for generating textbook-like synthetic data [18] or self-chat multi-turn dialogues [38, 9]
341"
RELATED WORK,0.41170097508125675,"for instruction tuning. However, models trained on these LLM-generated data only work well in
342"
RELATED WORK,0.41278439869989164,"specific domains such as math [20, 40], dialogue [38, 9] or open-ended question answering [30, 39].
343"
RELATED WORK,0.41386782231852653,"These methods encounter challenges in generalization [10], as the data diversity is restricted by seed
344"
RELATED WORK,0.41495124593716143,"instructions or seed topics.
345"
CONCLUSIONS,0.4160346695557963,"5
Conclusions
346"
CONCLUSIONS,0.4171180931744312,"We propose GLAN, a general and scalable method for synthesizing instruction data. Experiments
347"
CONCLUSIONS,0.4182015167930661,"show that GLAN can help large language models improve their capabilities in multiple dimensions,
348"
CONCLUSIONS,0.419284940411701,"from mathematical reasoning, coding, academic exams, and logical reasoning to general instruction
349"
CONCLUSIONS,0.42036836403033584,"following. Currently, our synthetic data are based on the taxonomy of human knowledge and
350"
CONCLUSIONS,0.42145178764897073,"capabilities, and there are other types of useful data that have not been covered. We are interested in
351"
CONCLUSIONS,0.4225352112676056,"designing methods with border coverage. Our current instruction data are mostly question-answer
352"
CONCLUSIONS,0.4236186348862405,"pairs, and in the next step, we plan to generate synthetic data of multi-turn conversations and long
353"
CONCLUSIONS,0.4247020585048754,"documents.
354"
REFERENCES,0.4257854821235103,"References
355"
REFERENCES,0.4268689057421452,"[1] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,
356"
REFERENCES,0.4279523293607801,"Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,
357"
REFERENCES,0.42903575297941493,"2021.
358"
REFERENCES,0.4301191765980498,"[2] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
359"
REFERENCES,0.4312026002166847,"G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural
360"
REFERENCES,0.4322860238353196,"information processing systems, 2020.
361"
REFERENCES,0.4333694474539545,"[3] S. Chaudhary. Code alpaca: An instruction-following llama model for code generation. https:
362"
REFERENCES,0.4344528710725894,"//github.com/sahil280114/codealpaca, 2023.
363"
REFERENCES,0.4355362946912243,"[4] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda,
364"
REFERENCES,0.43661971830985913,"N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv
365"
REFERENCES,0.437703141928494,"preprint arXiv:2107.03374, 2021.
366"
REFERENCES,0.4387865655471289,"[5] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E.
367"
REFERENCES,0.4398699891657638,"Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with
368"
REFERENCES,0.4409534127843987,"90%* chatgpt quality, March 2023.
369"
REFERENCES,0.4420368364030336,"[6] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think
370"
REFERENCES,0.4431202600216685,"you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint
371"
REFERENCES,0.4442036836403034,"arXiv:1803.05457, 2018.
372"
REFERENCES,0.4452871072589382,"[7] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,
373"
REFERENCES,0.4463705308775731,"J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word
374"
REFERENCES,0.447453954496208,"problems. arXiv preprint arXiv:2110.14168, 2021.
375"
REFERENCES,0.4485373781148429,"[8] M. Conover, M. Hayes, A. Mathur, J. Xie, J. Wan, S. Shah, A. Ghodsi, P. Wendell, M. Zaharia,
376"
REFERENCES,0.4496208017334778,"and R. Xin. Free dolly: Introducing the world’s first truly open instruction-tuned llm, 2023.
377"
REFERENCES,0.4507042253521127,"[9] N. Ding, Y. Chen, B. Xu, Y. Qin, Z. Zheng, S. Hu, Z. Liu, M. Sun, and B. Zhou. Enhancing
378"
REFERENCES,0.4517876489707476,"chat language models by scaling high-quality instructional conversations. arXiv preprint
379"
REFERENCES,0.4528710725893825,"arXiv:2305.14233, 2023.
380"
REFERENCES,0.4539544962080173,"[10] A. Gudibande, E. Wallace, C. V. Snell, X. Geng, H. Liu, P. Abbeel, S. Levine, and D. Song.
381"
REFERENCES,0.4550379198266522,"The false promise of imitating proprietary language models. In International Conference on
382"
REFERENCES,0.4561213434452871,"Learning Representations, 2024.
383"
REFERENCES,0.457204767063922,"[11] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Mea-
384"
REFERENCES,0.4582881906825569,"suring massive multitask language understanding. In International Conference on Learning
385"
REFERENCES,0.4593716143011918,"Representations, 2021.
386"
REFERENCES,0.46045503791982667,"[12] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt.
387"
REFERENCES,0.46153846153846156,"Measuring mathematical problem solving with the math dataset.
In Advances in Neural
388"
REFERENCES,0.4626218851570964,"Information Processing Systems, 2021.
389"
REFERENCES,0.4637053087757313,"[13] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas,
390"
REFERENCES,0.4647887323943662,"L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche,
391"
REFERENCES,0.4658721560130011,"B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. Rae, and L. Sifre. Training
392"
REFERENCES,0.466955579631636,"compute-optimal large language models. In Advances in Neural Information Processing Systems,
393"
REFERENCES,0.46803900325027087,"2022.
394"
REFERENCES,0.46912242686890576,"[14] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi.
The curious case of neural text
395"
REFERENCES,0.47020585048754066,"degeneration. In International Conference on Learning Representations, 2020.
396"
REFERENCES,0.4712892741061755,"[15] O. Honovich, T. Scialom, O. Levy, and T. Schick. Unnatural instructions: Tuning language
397"
REFERENCES,0.4723726977248104,"models with (almost) no human labor. In Proceedings of the 61st Annual Meeting of the
398"
REFERENCES,0.4734561213434453,"Association for Computational Linguistics (Volume 1: Long Papers), 2023.
399"
REFERENCES,0.4745395449620802,"[16] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand,
400"
REFERENCES,0.47562296858071507,"G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.
401"
REFERENCES,0.47670639219934996,"[17] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Rad-
402"
REFERENCES,0.47778981581798485,"ford, J. Wu, and D. Amodei.
Scaling laws for neural language models.
arXiv preprint
403"
REFERENCES,0.4788732394366197,"arXiv:2001.08361, 2020.
404"
REFERENCES,0.4799566630552546,"[18] Y. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T. Lee. Textbooks are all you
405"
REFERENCES,0.4810400866738895,"need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.
406"
REFERENCES,0.48212351029252437,"[19] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei,
407"
REFERENCES,0.48320693391115926,"and A. Roberts. The flan collection: Designing data and methods for effective instruction tuning.
408"
REFERENCES,0.48429035752979416,"In International Conference on Machine Learning, 2023.
409"
REFERENCES,0.48537378114842905,"[20] H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang.
410"
REFERENCES,0.48645720476706394,"Wizardmath: Empowering mathematical reasoning for large language models via reinforced
411"
REFERENCES,0.4875406283856988,"evol-instruct. arXiv preprint arXiv:2308.09583, 2023.
412"
REFERENCES,0.4886240520043337,"[21] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin, and D. Jiang.
413"
REFERENCES,0.48970747562296857,"Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint
414"
REFERENCES,0.49079089924160346,"arXiv:2306.08568, 2023.
415"
REFERENCES,0.49187432286023836,"[22] A. Mitra, L. Del Corro, S. Mahajan, A. Codas, C. Simoes, S. Agarwal, X. Chen, A. Razdaibied-
416"
REFERENCES,0.49295774647887325,"ina, E. Jones, K. Aggarwal, et al. Orca 2: Teaching small language models how to reason. arXiv
417"
REFERENCES,0.49404117009750814,"preprint arXiv:2311.11045, 2023.
418"
REFERENCES,0.49512459371614304,"[23] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman, T. Le Scao, M. S. Bari,
419"
REFERENCES,0.4962080173347779,"S. Shen, Z. X. Yong, H. Schoelkopf, X. Tang, D. Radev, A. F. Aji, K. Almubarak, S. Albanie,
420"
REFERENCES,0.49729144095341277,"Z. Alyafeai, A. Webson, E. Raff, and C. Raffel. Crosslingual generalization through multitask
421"
REFERENCES,0.49837486457204766,"finetuning. In Proceedings of the 61st Annual Meeting of the Association for Computational
422"
REFERENCES,0.49945828819068255,"Linguistics (Volume 1: Long Papers), 2023.
423"
REFERENCES,0.5005417118093174,"[24] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and A. Awadallah. Orca: Pro-
424"
REFERENCES,0.5016251354279523,"gressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707,
425"
REFERENCES,0.5027085590465872,"2023.
426"
REFERENCES,0.5037919826652221,"[25] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,
427"
REFERENCES,0.504875406283857,"K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.
428"
REFERENCES,0.5059588299024919,"In Advances in Neural Information Processing Systems, 2022.
429"
REFERENCES,0.5070422535211268,"[26] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler,
430"
REFERENCES,0.5081256771397616,"A. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chh-
431"
REFERENCES,0.5092091007583965,"ablani, N. V. Nayak, D. Datta, J. Chang, M. T. Jiang, H. Wang, M. Manica, S. Shen, Z. X.
432"
REFERENCES,0.5102925243770314,"Yong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Févry,
433"
REFERENCES,0.5113759479956663,"J. A. Fries, R. Teehan, T. L. Scao, S. Biderman, L. Gao, T. Wolf, and A. M. Rush. Multi-
434"
REFERENCES,0.5124593716143012,"task prompted training enables zero-shot task generalization. In International Conference on
435"
REFERENCES,0.5135427952329361,"Learning Representations, 2022.
436"
REFERENCES,0.514626218851571,"[27] T. Shen, R. Jin, Y. Huang, C. Liu, W. Dong, Z. Guo, X. Wu, Y. Liu, and D. Xiong. Large
437"
REFERENCES,0.5157096424702059,"language model alignment: A survey. arXiv preprint arXiv:2309.15025, 2023.
438"
REFERENCES,0.5167930660888408,"[28] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro,
439"
REFERENCES,0.5178764897074756,"A. Gupta, A. Garriga-Alonso, A. Kluska, A. Lewkowycz, A. Agarwal, A. Power, A. Ray,
440"
REFERENCES,0.5189599133261105,"A. Warstadt, A. W. Kocurek, A. Safaya, A. Tazarv, A. Xiang, A. Parrish, A. Nie, A. Hussain,
441"
REFERENCES,0.5200433369447454,"A. Askell, A. Dsouza, et al. Beyond the imitation game: Quantifying and extrapolating the
442"
REFERENCES,0.5211267605633803,"capabilities of language models. Transactions on Machine Learning Research, 2023.
443"
REFERENCES,0.5222101841820151,"[29] M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. Le,
444"
REFERENCES,0.52329360780065,"E. Chi, D. Zhou, and J. Wei. Challenging BIG-bench tasks and whether chain-of-thought can
445"
REFERENCES,0.5243770314192849,"solve them. In Findings of the Association for Computational Linguistics: ACL 2023, 2023.
446"
REFERENCES,0.5254604550379198,"[30] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto.
447"
REFERENCES,0.5265438786565547,"Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/
448"
REFERENCES,0.5276273022751896,"stanford_alpaca, 2023.
449"
REFERENCES,0.5287107258938245,"[31] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,
450"
REFERENCES,0.5297941495124594,"P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv
451"
REFERENCES,0.5308775731310943,"preprint arXiv:2307.09288, 2023.
452"
REFERENCES,0.5319609967497292,"[32] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct:
453"
REFERENCES,0.5330444203683641,"Aligning language models with self-generated instructions. In Proceedings of the 61st Annual
454"
REFERENCES,0.5341278439869989,"Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association
455"
REFERENCES,0.5352112676056338,"for Computational Linguistics, 2023.
456"
REFERENCES,0.5362946912242686,"[33] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S.
457"
REFERENCES,0.5373781148429035,"Dhanasekaran, A. Arunkumar, D. Stap, E. Pathak, G. Karamanolakis, H. Lai, I. Purohit, I. Mon-
458"
REFERENCES,0.5384615384615384,"dal, J. Anderson, K. Kuznia, K. Doshi, K. K. Pal, M. Patel, M. Moradshahi, M. Parmar,
459"
REFERENCES,0.5395449620801733,"M. Purohit, N. Varshney, P. R. Kaza, P. Verma, R. S. Puri, R. Karia, S. Doshi, S. K. Sampat,
460"
REFERENCES,0.5406283856988082,"S. Mishra, S. Reddy A, S. Patro, T. Dixit, and X. Shen. Super-NaturalInstructions: Generaliza-
461"
REFERENCES,0.5417118093174431,"tion via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference
462"
REFERENCES,0.542795232936078,"on Empirical Methods in Natural Language Processing, 2022.
463"
REFERENCES,0.5438786565547129,"[34] J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le.
464"
REFERENCES,0.5449620801733478,"Finetuned language models are zero-shot learners. In International Conference on Learning
465"
REFERENCES,0.5460455037919827,"Representations, 2022.
466"
REFERENCES,0.5471289274106176,"[35] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-
467"
REFERENCES,0.5482123510292525,"of-thought prompting elicits reasoning in large language models. In Advances in Neural
468"
REFERENCES,0.5492957746478874,"Information Processing Systems, 2022.
469"
REFERENCES,0.5503791982665223,"[36] T. Wei, L. Zhao, L. Zhang, B. Zhu, L. Wang, H. Yang, B. Li, C. Cheng, W. Lü, R. Hu, C. Li,
470"
REFERENCES,0.551462621885157,"L. Yang, X. Luo, X. Wu, L. Liu, W. Cheng, P. Cheng, J. Zhang, X. Zhang, L. Lin, X. Wang,
471"
REFERENCES,0.5525460455037919,"Y. Ma, C. Dong, Y. Sun, Y. Chen, Y. Peng, X. Liang, S. Yan, H. Fang, and Y. Zhou. Skywork:
472"
REFERENCES,0.5536294691224268,"A more open bilingual foundation model, 2023.
473"
REFERENCES,0.5547128927410617,"[37] Wikipedia contributors. Education, 2023. Last edited on 24 March 2023.
474"
REFERENCES,0.5557963163596966,"[38] C. Xu, D. Guo, N. Duan, and J. McAuley. Baize: An open-source chat model with parameter-
475"
REFERENCES,0.5568797399783315,"efficient tuning on self-chat data. In Proceedings of the 2023 Conference on Empirical Methods
476"
REFERENCES,0.5579631635969664,"in Natural Language Processing, 2023.
477"
REFERENCES,0.5590465872156013,"[39] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and D. Jiang. Wizardlm: Empow-
478"
REFERENCES,0.5601300108342362,"ering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244,
479"
REFERENCES,0.5612134344528711,"2023.
480"
REFERENCES,0.562296858071506,"[40] L. Yu, W. Jiang, H. Shi, J. YU, Z. Liu, Y. Zhang, J. Kwok, Z. Li, A. Weller, and W. Liu. Meta-
481"
REFERENCES,0.5633802816901409,"math: Bootstrap your own mathematical questions for large language models. In International
482"
REFERENCES,0.5644637053087758,"Conference on Learning Representations, 2024.
483"
REFERENCES,0.5655471289274107,"[41] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. YU, S. Zhang,
484"
REFERENCES,0.5666305525460456,"G. Ghosh, M. Lewis, L. Zettlemoyer, and O. Levy. LIMA: Less is more for alignment. In
485"
REFERENCES,0.5677139761646804,"Advances in Neural Information Processing Systems, 2023.
486"
REFERENCES,0.5687973997833152,"[42] J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-
487"
REFERENCES,0.5698808234019501,"following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.
488"
REFERENCES,0.570964247020585,"A
Appendix
489"
REFERENCES,0.5720476706392199,"A.1
Limitations
490"
REFERENCES,0.5731310942578548,"While GLAN presents significant advancements in academic benchmarks. However, there may
491"
REFERENCES,0.5742145178764897,"still have several limitations in real world deployment. The resulting LLMs train on generated data
492"
REFERENCES,0.5752979414951246,"using GLAN may occasionally produce factual incorrect (or even toxic) responses. Further training
493"
REFERENCES,0.5763813651137595,"for refusal, hallucination reduction as well as toxic content reduction should be performed before
494"
REFERENCES,0.5774647887323944,"deployment.
495"
REFERENCES,0.5785482123510293,"A.2
Broader Impacts
496"
REFERENCES,0.5796316359696642,"Data synthesizing is crucial for the continual scaling of large language models, especially as we
497"
REFERENCES,0.580715059588299,"exhaust available human data. GLAN demonstrates the potential to generate vast amounts of synthetic
498"
REFERENCES,0.581798483206934,"data from scratch, paving the way for even larger-scale data synthesis efforts. While GLAN has
499"
REFERENCES,0.5828819068255688,"shown the effectiveness of synthetic data, we must point out that synthetic data may inherit and even
500"
REFERENCES,0.5839653304442037,"amplify social biases present in the frontier LLMs for generation. Future research should focus on
501"
REFERENCES,0.5850487540628385,"developing techniques to identify and correct biases in the generated datasets and models trained on
502"
REFERENCES,0.5861321776814734,"them.
503"
REFERENCES,0.5872156013001083,"A.3
Prompt for Syllabus Generator
504"
REFERENCES,0.5882990249187432,"The prompt template for syllabus generation is in Table 6.
505"
REFERENCES,0.5893824485373781,Table 6: Prompt template for Syllabus Generator.
REFERENCES,0.590465872156013,You are an expert in {s.name}.
REFERENCES,0.5915492957746479,"Using the given data, design a syllabus for teaching students at the specified level.
Note that example subtopics or descriptions are just give you an impression of what this class like.
Feel free to add extra subtopics if needed (remember you are the expert in {s.name})."
REFERENCES,0.5926327193932828,"Data:
- Level: {s.level}
- Main Topic: {s.name}
- Description or Example Subtopics: {s.subtopics}"
REFERENCES,0.5937161430119177,"### Syllabus Design Guide
1. **Introduction**: Start with an overview of the primary topic for the syllabus.
2. **Class Details**: For each class session, provide:
- **Description**: Briefly describe the focus of the session.
- **Knowledge Points**: Enumerate key concepts or topics.
These will be used to craft homework questions.
- **Learning Outcomes & Activities**: Offer expected learning results and suggest related
exercises or activities."
REFERENCES,0.5947995666305526,"A.4
Prompt for Instruction Generator
506"
REFERENCES,0.5958829902491874,"The prompt template for instruction generator is in Table 7.
507"
REFERENCES,0.5969664138678223,"A.5
Task-specific Training Data
508"
REFERENCES,0.5980498374864572,"We provide the specific train/test values of different models on different benchmarks in Table 8.
509"
REFERENCES,0.5991332611050921,"A.6
Evol-Instruct Test Results on Different Difficulty Levels
510"
REFERENCES,0.600216684723727,"The concrete Evol-Instruct test results on different difficulty levels are shown in Table 9.
511"
REFERENCES,0.6013001083423619,Table 7: Prompt template for Instruction Generator.
REFERENCES,0.6023835319609967,"## Background
- You are an expert in {s.name} education and you have designed a syllabus (i.e., ‘## Syllabus‘)
- We invite you (again) to design ONE homework question for given class sessions and some
knowledge points.
- The student have already learned all class sessions up to the current sessions
(i.e., ‘## Current Session(s)‘).
- There might be multiple class session in ‘## Current Session(s)‘
- The designed homework question should focus on the topics in ‘## Current Session(s)‘ and you should
try to cover the given knowledge points in ‘## Given Knowledge Points‘
- We prefer homework questions leveraging multiple knowledge points and across different topics"
REFERENCES,0.6034669555796316,"## Syllabus
{A}"
REFERENCES,0.6045503791982665,"## Current Session(s)
{ˆC}"
REFERENCES,0.6056338028169014,"## Given Knowledge Points
{ˆK}"
REFERENCES,0.6067172264355363,"Table 8: The evaluation of loss values between the test data and training data. Large positive ∆(or
∆(%)) indicate task specific in-domain training data may be exposed to the model during training."
REFERENCES,0.6078006500541712,"Benchmark/Loss
LLaMA2-7B
Orca2-7B
Mistral-7B-Instruct
WizardLM-13B-V1.2
GLAN-7B"
REFERENCES,0.6088840736728061,"Ltest
2.02
2.39
2.32
2.11
4.03
ARC-C
Ltrain
2.03
2.34
2.33
2.12
4.06
∆
-0.01
0.05
-0.01
-0.01
-0.03
∆(%)
-0.5%
2.10%
-0.43%
-0.47%
-0.74%"
REFERENCES,0.609967497291441,"Ltest
2.10
2.47
2.51
2.18
4.31
ARC-E
Ltrain
2.12
2.43
2.54
2.20
4.32
∆
-0.02
0.04
-0.03
-0.02
-0.01
∆(%)
-0.95%
1.61%
-1.19%
-0.91%
-0.23%"
REFERENCES,0.6110509209100758,"Ltest
1.38
1.14
1.26
1.14
2.17
GSM8K
Ltrain
1.38
1.01
1.26
1.09
2.15
∆
0
0.13
0
0.05
0.02
∆(%)
0%
11.4%
0%
4.39%
0.92%"
REFERENCES,0.6121343445287107,"Ltest
1.11
1.18
1.12
1.22
1.67
MATH
Ltrain
1.14
1.15
1.15
1.24
1.70
∆
-0.03
0.03
-0.03
-0.02
-0.03
∆(%)
-2.70%
2.54%
-2.67%
-1.63%
-1.79%"
REFERENCES,0.6132177681473456,"A.7
Evol-Instruct Test Results on Different Skills
512"
REFERENCES,0.6143011917659805,"The concrete Evol-Instruct test results on different skills are shown in Table 10.
513"
REFERENCES,0.6153846153846154,"A.8
GLAN-Test Overall Results
514"
REFERENCES,0.6164680390032503,"GLAN-Test
There are only hundreds of instructions in In IFEval and Evol-Instruct Test and
515"
REFERENCES,0.6175514626218852,"we believe the domains or skills they can cover are rather limited. Therefore, we propose a held-
516"
REFERENCES,0.6186348862405201,"out test set using GLAN data and we call it GLAN-Test. It contains 6,300 instructions on 126
517"
REFERENCES,0.6197183098591549,"disciplines (50 instructions for each discipline). We further categorize the 126 disciplines to 8
518"
REFERENCES,0.6208017334777898,"distinct fields (i.e., Academic-Humanities, Academic-Social Science, Academic-Natural Science,
519"
REFERENCES,0.6218851570964247,"Academic-Applied Science, Academic-Formal Science, Industry-Manufacturing, Industry-Services
520"
REFERENCES,0.6229685807150596,"and Industry-Agriculture). We believe that the extensive domain coverage of GLAN-Test renders
521"
REFERENCES,0.6240520043336945,"it an effective test bed for the assessment of generalization capabilities in LLMs. We adopt the
522"
REFERENCES,0.6251354279523293,"same GPT-4 based evaluation protocol as in Evol-Instruct Test (previous paragraph). We prompt
523"
REFERENCES,0.6262188515709642,"GPT-4 to do a pairwise ranking of GLAN and other models in comparison. The overall results and
524"
REFERENCES,0.6273022751895991,"results across the 8 fields are presented in Table 11, where GLAN obtains higher GPT-4 scores than
525"
REFERENCES,0.628385698808234,"Orca2-7B, Mistral-7B Instruct and WizardLM-13B, despite using only 7B parameters. GLAN still
526"
REFERENCES,0.6294691224268689,"Table 9: Pairwise comparison on various difficulty levels between GLAN and other models on
Evol-Instruct testset. The scores are the average gap of scores assigned by GPT-4, calculated as
avg_score(GLAN) −avg_score(x)."
REFERENCES,0.6305525460455038,"Difficulty
Ratio
LLaMA2-7B
Orca2-7B
Mistral-7B-Instruct
Wizard-13B-V1.2
GPT-3.5-turbo"
REFERENCES,0.6316359696641387,"1
5.1%
5.41
2.23
-0.37
-0.21
-2.41
2
8.7%
5.87
1.74
1.06
1.41
-1.18
3
12.4%
5.72
2.35
1.04
1.37
-1.14
4
10.5%
5.61
1.34
1.52
1.54
-0.92
5
4.1%
4.67
3.31
2.39
2.5
-0.45
6
19.3%
4.43
2.42
0.74
1.54
-1.36
7
11.0%
4.97
1.26
1.62
1.36
-0.41
8
17.9%
6.02
3.58
3.17
1.7
0.15
9
6.0%
6.35
4.2
1.36
0.9
-0.92
10
5.1%
5.14
-0.05
1.53
-0.54
-0.85"
REFERENCES,0.6327193932827736,"(1-5) Easy
41.00%
5.46
2.19
1.13
1.32
-1.22
(6-10) Hard
59.00%
5.38
2.28
1.68
0.99
-0.68"
REFERENCES,0.6338028169014085,"Table 10: Pairwise comparison on various skills between GLAN and other models on Evol-
Instruct testset.
The scores are the average gap of scores assigned by GPT-4, calculated as
avg_score(GLAN) −avg_score(x)."
REFERENCES,0.6348862405200434,"Skill
Ratio
LLaMA2-7B
Orca2-7B
Mistral-7B-Instruct
Wizard-13B-V1.2
GPT-3.5-turbo"
REFERENCES,0.6359696641386782,"Math
8.7%
6.58
2.16
2.41
2.46
-1.42
Code Generation
8.3%
6.16
3.87
4.22
2.59
-0.25
Writting
8.3%
5.2
0.79
-0.22
0.24
-1.1
Computer Science
6.9%
7.1
4.4
0.83
1.22
0.02
Reasoning
6.0%
6.3
2.52
3.38
3.02
0.62
Complex Format
5.5%
3.13
3.5
-0.17
2.41
-1.96
Code Debug
4.6%
5.85
2.3
1.4
0.2
-2.5
Common-Sense
4.1%
6.5
3.19
-1.33
-0.92
-2.78
Counterfactual
3.7%
7.06
2.15
3
1.5
0.72
Multilingual
3.2%
7.35
0.79
1.71
-0.68
-2.75
Roleplay
2.8%
7.08
2.25
3.5
0.92
-0.59
Biology
2.8%
6.66
2.75
1.46
-0.09
1.38
Technology
2.8%
-0.08
2.54
-3
-1.5
-2.75
Ethics
2.8%
6.59
3.38
2.41
5.42
-0.21
TruthfulQA
2.3%
3.1
3.7
-1.05
-1.3
-0.85
Sport
2.3%
4.3
0.55
-0.2
4.8
-0.3
Law
2.3%
7.7
4.65
5.85
1.7
0.2
Medicine
2.3%
3.9
-2.05
1.9
0.15
-1.25
Literature
2.3%
6.3
1.9
0.2
1.45
-0.15
Entertainment
2.3%
4.5
2.7
-3
1.9
-3.2
Art
2.3%
4.9
1
2.9
-0.85
-2.05
Music
2.3%
4.4
4.1
0.5
1.45
-2.3
Toxicity
1.8%
7.25
3.12
3.75
1.63
-1.32
Economy
2.3%
6
0.15
1.9
0
0
Physics
2.3%
6.8
2.5
4.35
3.65
-1
History
1.8%
4.12
-0.56
3.76
-0.31
0.12
Academic Writing
1.8%
6.76
6.37
2.44
1.37
0.62
Chemistry
0.9%
9.5
0.63
5.25
2.5
0.75
Philosophy
0.5%
11
-0.25
0.25
-0.25
0.5"
REFERENCES,0.6370530877573131,"Avg.(29 skills)
100%
5.42
2.24
1.41
1.16
-0.95"
REFERENCES,0.638136511375948,"lag behind GPT-4. Detailed results for the 126 fine-grained disciplines can be found in Appendix
527"
REFERENCES,0.6392199349945829,"A.9 (see Table 12 for more details). GLAN demonstrates its effectiveness on multiple domains (or
528"
REFERENCES,0.6403033586132177,"disciplines) such as Mathematics, Physics, Chemistry, Computer science, Electrical, Mechanical, etc.,
529"
REFERENCES,0.6413867822318526,"indicating that smaller models may yield general improvements on various domains through strategic
530"
REFERENCES,0.6424702058504875,"fine-tuning. Furthermore, it is noted that GLAN demonstrates less-than-ideal performance across
531"
REFERENCES,0.6435536294691224,"distinct disciplines such as American history, Divinity, or Radiology. This observation underscores
532"
REFERENCES,0.6446370530877573,"the potential for further refinement and development of our methodology within these domains.
533"
REFERENCES,0.6457204767063922,"A.9
GLAN-Test Results on Different Disciplines
534"
REFERENCES,0.6468039003250271,"Table 11: Pairwise comparison between GLAN and other models on GLAN-Test (the 126 disciplines
are categorized into 8 fields for clarity of the illustration). The scores are the average gap of scores
assigned by GPT-4, calculated as avg_score(GLAN) −avg_score(x)."
REFERENCES,0.647887323943662,"Field (Ratio)
Orca2-7B
Mistral-7B-Instruct
WizardLM-13B-V1.2
GPT-4"
REFERENCES,0.6489707475622969,"Academic-Humanities (15.9%)
0.79
0.25
0.02
-0.62
Academic-Social Science (7.9%)
1.22
0.21
0.09
-0.63
Academic-Natural Science (4.0%)
1.73
1.23
0.53
-0.5
Academic-Applied Science (42.1%)
1.58
0.32
0.08
-0.58
Academic-Formal Science (3.2%)
3.87
2.48
2.32
-0.55
Industry-Manufacturing (12.7%)
2.26
0.56
0.33
-0.43
Industry-Services (11.9%)
1.82
0.23
0.09
-0.5
Industry-Agriculture (2.4%)
1.2
0.46
0.13
-0.33"
REFERENCES,0.6500541711809318,"Overall (100.0%)
1.61
0.43
0.19
-0.55"
REFERENCES,0.6511375947995667,"Table 12: Pairwise comparison across 126 disciplines (or domains) on GLAN-Test. The scores are
generated from the average gap between GLAN and other model x in assessment scores assigned by
GPT-4, calculated as avg_score(GLAN) −avg_score(x)."
REFERENCES,0.6522210184182016,"Discipline
Orca-2-7b
Mistral-7B-Instruct-v0.1
WizardLM-13B-V1.2
GPT-4"
REFERENCES,0.6533044420368364,"Avg.
1.61
0.43
0.19
-0.55"
REFERENCES,0.6543878656554712,"Advertising
1.92
0.46
0.21
-0.04
Aerospace industry
3.24
1.24
0.6
-0.42
Agriculture
2.44
0.04
-0.05
-0.48
American history
-0.49
-0.27
-0.76
-0.83
American politics
1.23
-0.3
-0.4
-0.87
Anthropology
0.59
0.17
0.06
-0.27
Applied mathematics
3.75
2.6
2.74
-0.47
Archaeology
2.59
-0.11
0.1
-0.56
Architecture and design
2.63
0.34
0.4
-0.37
Astronomy
1.01
0.83
0.03
-0.44
Automotive industry
1.27
0.71
0.46
-0.06
Biblical studies
-0.05
0.33
-0.47
-0.65
Biology
1.09
0.22
-0.09
-0.17
Business
3.61
1.14
0.88
-0.26
Chemical Engineering
3.15
1.6
1.18
-0.77
Chemistry
3.06
2.09
0.8
-0.87
Civil Engineering
1.94
0.74
0.75
-0.25
Clinical laboratory sciences
1.32
0.94
-0.11
-0.47
Clinical neuropsychology
2.15
0.29
0.25
-0.4
Clinical physiology
2.07
0.41
0.51
-0.08
Communication studies
0.3
0.26
-0.15
-0.3
Computer science
4.29
1.45
1.9
-0.33
Cultural industry
3.15
0.44
0.05
-0.36
Dance
2.11
0.21
0.4
-0.47
Dentistry
1.67
0.66
0.48
0.01
Dermatology
2.12
0.55
-0.05
-0.65
Divinity
-0.34
-0.17
-0.48
-0.89
Earth science
0.39
0.44
-0.08
-0.33
Economics
2.62
0.96
0.62
-0.4
Education
2.67
0.42
0.2
-0.84
Education industry
2.19
0.4
0.56
-1.33
Electric power industry
3.23
1.31
0.39
-0.79
Electrical Engineering
3.81
1.26
1.41
-0.34
Emergency medicine
2.04
0.44
-0.18
-0.86
Energy industry
3.59
0.98
0.54
-0.22
Environmental studies and forestry
0.12
0.41
0.1
-0.45
Epidemiology
3.02
0.52
0.33
-0.46
European history
0.14
0.62
0.15
-0.18
Fashion
2.5
0.66
0.47
-0.53
Film
0.76
0.45
-0.16
-0.78
Film industry
1.58
0.46
0.25
-0.59
Fishing industry
1.67
1
0.57
-0.09
Floral
1.92
0.89
0.58
-0.09
Food industry
3.64
0.12
0.14
-0.42
Foreign policy
2.4
0.49
0.16
-0.46
Geography
0.88
0.6
0.28
-0.66
Geriatrics
2.19
-0.32
-0.56
-0.71
Gynaecology
1.05
-0.27
-0.26
-0.67
Healthcare industry
1.62
-0.25
0.14
-0.5
Hematology
0.35
0.32
-0.05
-0.72
History
0.75
0.54
-0.04
-0.38
Holistic medicine
0.85
0.48
0.26
-0.27
Hospitality industry
2.36
0.48
0.28
-0.07
Housing
4.04
0.15
-0.22
-0.62
Industrial robot industry
3.84
1.22
0.84
-0.71
Infectious disease
1.76
0.14
0.18
-0.56
Insurance industry
2.67
0.42
0.61
-0.4
Intensive care medicine
1.11
0.56
0.08
-0.33
Internal medicine
1.02
0.45
-0.01
-0.42
Journalism
2.77
-0.13
-0.21
-0.69
Languages and literature
0.45
0.05
-0.39
-0.84
Law
0.42
0.39
0.04
-0.49
Leisure industry
1.49
0.12
-0.09
-0.49
Library and museum studies
1.52
0.5
0.33
-0.32"
REFERENCES,0.6554712892741061,"Discipline
Orca-2-7b
Mistral-7B-Instruct-v0.1
WizardLM-13B-V1.2
GPT-4"
REFERENCES,0.656554712892741,"Linguistics
0.39
0.38
-0.12
-0.96
Logic
2.95
1.56
1.62
-0.79
Materials Science and Engineering
1.71
0.97
0.54
-0.91
Mathematics
4.69
3.81
2.73
-0.61
Mechanical Engineering
2.25
1.71
1.15
-0.95
Medical toxicology
0.62
0
0.11
-1.01
Medicine
1.49
0.93
0.36
-0.37
Military sciences
0.42
0.53
0.17
-0.45
Mining
3.17
0.32
0.41
-0.61
Music
2.85
0.38
1.07
-0.05
Music industry
2.05
-0.03
-0.08
-0.8
Nursing
1.49
0.14
-0.12
-0.59
Nutrition
1.15
-0.2
-0.13
-0.65
Obstetrics
1.49
0.08
-0.43
-0.53
Ophthalmology
0.97
0.01
-0.47
-0.97
Otolaryngology
1.51
-0.44
-0.29
-1.11
Pathology
0.23
0.35
0.19
-0.72
Pediatrics
1.62
0.55
-0.34
-0.47
Performing arts
0.38
0.09
-0.36
-1.06
Petroleum industry
3.12
0.44
0.08
-0.54
Pharmaceutical industry
2.75
0.41
0.4
-0.46
Pharmaceutical sciences
0.77
0.19
0.16
-0.8
Philosophy
0.51
0.25
0.49
-0.64
Physics
3.15
2.67
2.05
-0.73
Political science
0.04
-0.05
-0.31
-0.91
Prehistory
0.35
0.19
0.05
-0.41
Preventive medicine
2.69
0.57
0.09
-0.36
Psychiatry
2.93
0.27
-0.07
-0.32
Psychology
0.53
-0.02
-0.3
-0.96
Public administration
0.94
-0.27
0.1
-1.2
Public health
1.21
0.07
0.22
-0.56
Public policy
0.78
-0.06
-0.28
-0.92
Pulp and paper industry
1.13
0.63
0.57
-0.25
Radiology
-0.17
-0.19
-0.82
-0.62
Real estate industry
1.01
0.02
-0.12
-0.5
Religious Studies
0.38
0
-0.32
-0.63
Retail industry
1.1
-0.25
-0.37
-0.6
Semiconductor industry
1.49
0.64
0.71
-0.42
Sexology
1.81
-0.44
-0.37
-0.96
Shipbuilding industry
1.54
0.37
0.42
-0.32
Social work
0.93
-0.42
-0.53
-0.77
Sociology
1.49
0.21
0.76
-0.3
Steel industry
0.88
0.45
0.09
-0.34
Surgery
0.86
-0.02
-0.35
-0.73
Systems science
1.9
0.56
0.41
-0.45
Telecommunications industry
1.81
0.4
0.39
-0.27
Television
0.37
-0.33
-0.69
-1
Textile industry
0.82
-0.26
-0.68
-0.59
Theatre
0.31
-0.27
-0.34
-1.07
Theology
-0.38
0.37
-0.45
-0.54
Tobacco industry
0.59
-0.13
-0.48
-0.67
Transport industry
1.19
-0.33
-0.36
-0.56
Transportation
1.74
0.26
0.17
-0.74
Urology
0.05
-0.29
-0.36
-0.64
Veterinary medicine
-0.14
0.36
-0.31
-0.62
Video game industry
1.67
0.2
-0.24
-0.62
Visual arts
0.98
0.22
0.26
-0.56
Water industry
0.9
-0.11
-0.09
-0.51
Wood industry
1.36
0.5
0.31
-0.25"
REFERENCES,0.6576381365113759,"NeurIPS Paper Checklist
535"
CLAIMS,0.6587215601300108,"1. Claims
536"
CLAIMS,0.6598049837486457,"Question: Do the main claims made in the abstract and introduction accurately reflect the
537"
CLAIMS,0.6608884073672806,"paper’s contributions and scope?
538"
CLAIMS,0.6619718309859155,"Answer: [Yes]
539"
CLAIMS,0.6630552546045504,"Justification: See Abstract and Section 1.
540"
CLAIMS,0.6641386782231853,"Guidelines:
541"
CLAIMS,0.6652221018418202,"• The answer NA means that the abstract and introduction do not include the claims
542"
CLAIMS,0.6663055254604551,"made in the paper.
543"
CLAIMS,0.66738894907909,"• The abstract and/or introduction should clearly state the claims made, including the
544"
CLAIMS,0.6684723726977249,"contributions made in the paper and important assumptions and limitations. A No or
545"
CLAIMS,0.6695557963163596,"NA answer to this question will not be perceived well by the reviewers.
546"
CLAIMS,0.6706392199349945,"• The claims made should match theoretical and experimental results, and reflect how
547"
CLAIMS,0.6717226435536294,"much the results can be expected to generalize to other settings.
548"
CLAIMS,0.6728060671722643,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
549"
CLAIMS,0.6738894907908992,"are not attained by the paper.
550"
LIMITATIONS,0.6749729144095341,"2. Limitations
551"
LIMITATIONS,0.676056338028169,"Question: Does the paper discuss the limitations of the work performed by the authors?
552"
LIMITATIONS,0.6771397616468039,"Answer: [Yes]
553"
LIMITATIONS,0.6782231852654388,"Justification: See Section 5 and Appendix A.1
554"
LIMITATIONS,0.6793066088840737,"Guidelines:
555"
LIMITATIONS,0.6803900325027086,"• The answer NA means that the paper has no limitation while the answer No means that
556"
LIMITATIONS,0.6814734561213435,"the paper has limitations, but those are not discussed in the paper.
557"
LIMITATIONS,0.6825568797399784,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
558"
LIMITATIONS,0.6836403033586133,"• The paper should point out any strong assumptions and how robust the results are to
559"
LIMITATIONS,0.6847237269772481,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
560"
LIMITATIONS,0.685807150595883,"model well-specification, asymptotic approximations only holding locally). The authors
561"
LIMITATIONS,0.6868905742145178,"should reflect on how these assumptions might be violated in practice and what the
562"
LIMITATIONS,0.6879739978331527,"implications would be.
563"
LIMITATIONS,0.6890574214517876,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
564"
LIMITATIONS,0.6901408450704225,"only tested on a few datasets or with a few runs. In general, empirical results often
565"
LIMITATIONS,0.6912242686890574,"depend on implicit assumptions, which should be articulated.
566"
LIMITATIONS,0.6923076923076923,"• The authors should reflect on the factors that influence the performance of the approach.
567"
LIMITATIONS,0.6933911159263272,"For example, a facial recognition algorithm may perform poorly when image resolution
568"
LIMITATIONS,0.6944745395449621,"is low or images are taken in low lighting. Or a speech-to-text system might not be
569"
LIMITATIONS,0.695557963163597,"used reliably to provide closed captions for online lectures because it fails to handle
570"
LIMITATIONS,0.6966413867822319,"technical jargon.
571"
LIMITATIONS,0.6977248104008668,"• The authors should discuss the computational efficiency of the proposed algorithms
572"
LIMITATIONS,0.6988082340195017,"and how they scale with dataset size.
573"
LIMITATIONS,0.6998916576381365,"• If applicable, the authors should discuss possible limitations of their approach to
574"
LIMITATIONS,0.7009750812567714,"address problems of privacy and fairness.
575"
LIMITATIONS,0.7020585048754063,"• While the authors might fear that complete honesty about limitations might be used by
576"
LIMITATIONS,0.7031419284940412,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
577"
LIMITATIONS,0.704225352112676,"limitations that aren’t acknowledged in the paper. The authors should use their best
578"
LIMITATIONS,0.7053087757313109,"judgment and recognize that individual actions in favor of transparency play an impor-
579"
LIMITATIONS,0.7063921993499458,"tant role in developing norms that preserve the integrity of the community. Reviewers
580"
LIMITATIONS,0.7074756229685807,"will be specifically instructed to not penalize honesty concerning limitations.
581"
THEORY ASSUMPTIONS AND PROOFS,0.7085590465872156,"3. Theory Assumptions and Proofs
582"
THEORY ASSUMPTIONS AND PROOFS,0.7096424702058505,"Question: For each theoretical result, does the paper provide the full set of assumptions and
583"
THEORY ASSUMPTIONS AND PROOFS,0.7107258938244854,"a complete (and correct) proof?
584"
THEORY ASSUMPTIONS AND PROOFS,0.7118093174431203,"Answer: [NA]
585"
THEORY ASSUMPTIONS AND PROOFS,0.7128927410617552,"Justification: No theoretical results.
586"
THEORY ASSUMPTIONS AND PROOFS,0.71397616468039,"Guidelines:
587"
THEORY ASSUMPTIONS AND PROOFS,0.7150595882990249,"• The answer NA means that the paper does not include theoretical results.
588"
THEORY ASSUMPTIONS AND PROOFS,0.7161430119176598,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
589"
THEORY ASSUMPTIONS AND PROOFS,0.7172264355362947,"referenced.
590"
THEORY ASSUMPTIONS AND PROOFS,0.7183098591549296,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
591"
THEORY ASSUMPTIONS AND PROOFS,0.7193932827735645,"• The proofs can either appear in the main paper or the supplemental material, but if
592"
THEORY ASSUMPTIONS AND PROOFS,0.7204767063921993,"they appear in the supplemental material, the authors are encouraged to provide a short
593"
THEORY ASSUMPTIONS AND PROOFS,0.7215601300108342,"proof sketch to provide intuition.
594"
THEORY ASSUMPTIONS AND PROOFS,0.7226435536294691,"• Inversely, any informal proof provided in the core of the paper should be complemented
595"
THEORY ASSUMPTIONS AND PROOFS,0.723726977248104,"by formal proofs provided in appendix or supplemental material.
596"
THEORY ASSUMPTIONS AND PROOFS,0.7248104008667389,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
597"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7258938244853738,"4. Experimental Result Reproducibility
598"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7269772481040087,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
599"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7280606717226435,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
600"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7291440953412784,"of the paper (regardless of whether the code and data are provided or not)?
601"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7302275189599133,"Answer: [Yes]
602"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7313109425785482,"Justification: In Section 2 and 3.1, we provide a detailed description of the data generation
603"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7323943661971831,"process. Although we haven’t shared the original prompts yet, they are quite simple and
604"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.733477789815818,"customizable. Besides, we are actively working to gain authorization to release them as
605"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7345612134344529,"soon as possible.
606"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7356446370530878,"Guidelines:
607"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7367280606717227,"• The answer NA means that the paper does not include experiments.
608"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7378114842903575,"• If the paper includes experiments, a No answer to this question will not be perceived
609"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7388949079089924,"well by the reviewers: Making the paper reproducible is important, regardless of
610"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7399783315276273,"whether the code and data are provided or not.
611"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7410617551462622,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
612"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.742145178764897,"to make their results reproducible or verifiable.
613"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7432286023835319,"• Depending on the contribution, reproducibility can be accomplished in various ways.
614"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7443120260021668,"For example, if the contribution is a novel architecture, describing the architecture fully
615"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7453954496208017,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
616"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7464788732394366,"be necessary to either make it possible for others to replicate the model with the same
617"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7475622968580715,"dataset, or provide access to the model. In general. releasing code and data is often
618"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7486457204767064,"one good way to accomplish this, but reproducibility can also be provided via detailed
619"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7497291440953413,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
620"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7508125677139762,"of a large language model), releasing of a model checkpoint, or other means that are
621"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7518959913326111,"appropriate to the research performed.
622"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.752979414951246,"• While NeurIPS does not require releasing code, the conference does require all submis-
623"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7540628385698809,"sions to provide some reasonable avenue for reproducibility, which may depend on the
624"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7551462621885157,"nature of the contribution. For example
625"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7562296858071506,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
626"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7573131094257854,"to reproduce that algorithm.
627"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7583965330444203,"(b) If the contribution is primarily a new model architecture, the paper should describe
628"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7594799566630552,"the architecture clearly and fully.
629"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7605633802816901,"(c) If the contribution is a new model (e.g., a large language model), then there should
630"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.761646803900325,"either be a way to access this model for reproducing the results or a way to reproduce
631"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7627302275189599,"the model (e.g., with an open-source dataset or instructions for how to construct
632"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7638136511375948,"the dataset).
633"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7648970747562297,"(d) We recognize that reproducibility may be tricky in some cases, in which case
634"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7659804983748646,"authors are welcome to describe the particular way they provide for reproducibility.
635"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7670639219934995,"In the case of closed-source models, it may be that access to the model is limited in
636"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7681473456121344,"some way (e.g., to registered users), but it should be possible for other researchers
637"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7692307692307693,"to have some path to reproducing or verifying the results.
638"
OPEN ACCESS TO DATA AND CODE,0.7703141928494042,"5. Open access to data and code
639"
OPEN ACCESS TO DATA AND CODE,0.771397616468039,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
640"
OPEN ACCESS TO DATA AND CODE,0.7724810400866738,"tions to faithfully reproduce the main experimental results, as described in supplemental
641"
OPEN ACCESS TO DATA AND CODE,0.7735644637053087,"material?
642"
OPEN ACCESS TO DATA AND CODE,0.7746478873239436,"Answer: [No]
643"
OPEN ACCESS TO DATA AND CODE,0.7757313109425785,"Justification: While we are temporarily unable to provide open access to the data and code,
644"
OPEN ACCESS TO DATA AND CODE,0.7768147345612134,"we are actively working to gain the necessary authorization to release these resources. Once
645"
OPEN ACCESS TO DATA AND CODE,0.7778981581798483,"obtained, we will ensure that all data and code, along with detailed instructions, are made
646"
OPEN ACCESS TO DATA AND CODE,0.7789815817984832,"available to faithfully reproduce the main experimental results.
647"
OPEN ACCESS TO DATA AND CODE,0.7800650054171181,"Guidelines:
648"
OPEN ACCESS TO DATA AND CODE,0.781148429035753,"• The answer NA means that paper does not include experiments requiring code.
649"
OPEN ACCESS TO DATA AND CODE,0.7822318526543879,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
650"
OPEN ACCESS TO DATA AND CODE,0.7833152762730228,"public/guides/CodeSubmissionPolicy) for more details.
651"
OPEN ACCESS TO DATA AND CODE,0.7843986998916577,"• While we encourage the release of code and data, we understand that this might not be
652"
OPEN ACCESS TO DATA AND CODE,0.7854821235102926,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
653"
OPEN ACCESS TO DATA AND CODE,0.7865655471289275,"including code, unless this is central to the contribution (e.g., for a new open-source
654"
OPEN ACCESS TO DATA AND CODE,0.7876489707475623,"benchmark).
655"
OPEN ACCESS TO DATA AND CODE,0.7887323943661971,"• The instructions should contain the exact command and environment needed to run to
656"
OPEN ACCESS TO DATA AND CODE,0.789815817984832,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
657"
OPEN ACCESS TO DATA AND CODE,0.7908992416034669,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
658"
OPEN ACCESS TO DATA AND CODE,0.7919826652221018,"• The authors should provide instructions on data access and preparation, including how
659"
OPEN ACCESS TO DATA AND CODE,0.7930660888407367,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
660"
OPEN ACCESS TO DATA AND CODE,0.7941495124593716,"• The authors should provide scripts to reproduce all experimental results for the new
661"
OPEN ACCESS TO DATA AND CODE,0.7952329360780065,"proposed method and baselines. If only a subset of experiments are reproducible, they
662"
OPEN ACCESS TO DATA AND CODE,0.7963163596966414,"should state which ones are omitted from the script and why.
663"
OPEN ACCESS TO DATA AND CODE,0.7973997833152763,"• At submission time, to preserve anonymity, the authors should release anonymized
664"
OPEN ACCESS TO DATA AND CODE,0.7984832069339112,"versions (if applicable).
665"
OPEN ACCESS TO DATA AND CODE,0.7995666305525461,"• Providing as much information as possible in supplemental material (appended to the
666"
OPEN ACCESS TO DATA AND CODE,0.800650054171181,"paper) is recommended, but including URLs to data and code is permitted.
667"
OPEN ACCESS TO DATA AND CODE,0.8017334777898159,"6. Experimental Setting/Details
668"
OPEN ACCESS TO DATA AND CODE,0.8028169014084507,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
669"
OPEN ACCESS TO DATA AND CODE,0.8039003250270856,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
670"
OPEN ACCESS TO DATA AND CODE,0.8049837486457205,"results?
671"
OPEN ACCESS TO DATA AND CODE,0.8060671722643553,"Answer: [Yes]
672"
OPEN ACCESS TO DATA AND CODE,0.8071505958829902,"Justification: See Section 3.2, 3.3
673"
OPEN ACCESS TO DATA AND CODE,0.8082340195016251,"Guidelines:
674"
OPEN ACCESS TO DATA AND CODE,0.80931744312026,"• The answer NA means that the paper does not include experiments.
675"
OPEN ACCESS TO DATA AND CODE,0.8104008667388949,"• The experimental setting should be presented in the core of the paper to a level of detail
676"
OPEN ACCESS TO DATA AND CODE,0.8114842903575298,"that is necessary to appreciate the results and make sense of them.
677"
OPEN ACCESS TO DATA AND CODE,0.8125677139761647,"• The full details can be provided either with the code, in appendix, or as supplemental
678"
OPEN ACCESS TO DATA AND CODE,0.8136511375947996,"material.
679"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8147345612134345,"7. Experiment Statistical Significance
680"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8158179848320694,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
681"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8169014084507042,"information about the statistical significance of the experiments?
682"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8179848320693391,"Answer: [No]
683"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.819068255687974,"Justification: We did not include error bars in the experiments due to the high computational
684"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8201516793066089,"demands.
685"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8212351029252438,"Guidelines:
686"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8223185265438786,"• The answer NA means that the paper does not include experiments.
687"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8234019501625135,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
688"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8244853737811484,"dence intervals, or statistical significance tests, at least for the experiments that support
689"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8255687973997833,"the main claims of the paper.
690"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8266522210184182,"• The factors of variability that the error bars are capturing should be clearly stated (for
691"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8277356446370531,"example, train/test split, initialization, random drawing of some parameter, or overall
692"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.828819068255688,"run with given experimental conditions).
693"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8299024918743229,"• The method for calculating the error bars should be explained (closed form formula,
694"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8309859154929577,"call to a library function, bootstrap, etc.)
695"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8320693391115926,"• The assumptions made should be given (e.g., Normally distributed errors).
696"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8331527627302275,"• It should be clear whether the error bar is the standard deviation or the standard error
697"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8342361863488624,"of the mean.
698"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8353196099674973,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
699"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8364030335861322,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
700"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8374864572047671,"of Normality of errors is not verified.
701"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.838569880823402,"• For asymmetric distributions, the authors should be careful not to show in tables or
702"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8396533044420368,"figures symmetric error bars that would yield results that are out of range (e.g. negative
703"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8407367280606717,"error rates).
704"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8418201516793066,"• If error bars are reported in tables or plots, The authors should explain in the text how
705"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8429035752979415,"they were calculated and reference the corresponding figures or tables in the text.
706"
EXPERIMENTS COMPUTE RESOURCES,0.8439869989165764,"8. Experiments Compute Resources
707"
EXPERIMENTS COMPUTE RESOURCES,0.8450704225352113,"Question: For each experiment, does the paper provide sufficient information on the com-
708"
EXPERIMENTS COMPUTE RESOURCES,0.8461538461538461,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
709"
EXPERIMENTS COMPUTE RESOURCES,0.847237269772481,"the experiments?
710"
EXPERIMENTS COMPUTE RESOURCES,0.8483206933911159,"Answer: [Yes]
711"
EXPERIMENTS COMPUTE RESOURCES,0.8494041170097508,"Justification: We included compute resources in Section 3.2.
712"
EXPERIMENTS COMPUTE RESOURCES,0.8504875406283857,"Guidelines:
713"
EXPERIMENTS COMPUTE RESOURCES,0.8515709642470206,"• The answer NA means that the paper does not include experiments.
714"
EXPERIMENTS COMPUTE RESOURCES,0.8526543878656555,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
715"
EXPERIMENTS COMPUTE RESOURCES,0.8537378114842904,"or cloud provider, including relevant memory and storage.
716"
EXPERIMENTS COMPUTE RESOURCES,0.8548212351029253,"• The paper should provide the amount of compute required for each of the individual
717"
EXPERIMENTS COMPUTE RESOURCES,0.8559046587215602,"experimental runs as well as estimate the total compute.
718"
EXPERIMENTS COMPUTE RESOURCES,0.856988082340195,"• The paper should disclose whether the full research project required more compute
719"
EXPERIMENTS COMPUTE RESOURCES,0.8580715059588299,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
720"
EXPERIMENTS COMPUTE RESOURCES,0.8591549295774648,"didn’t make it into the paper).
721"
CODE OF ETHICS,0.8602383531960996,"9. Code Of Ethics
722"
CODE OF ETHICS,0.8613217768147345,"Question: Does the research conducted in the paper conform, in every respect, with the
723"
CODE OF ETHICS,0.8624052004333694,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
724"
CODE OF ETHICS,0.8634886240520043,"Answer: [Yes]
725"
CODE OF ETHICS,0.8645720476706392,"Justification: This study strictly adheres to the NeurIPS Code of Ethics.
726"
CODE OF ETHICS,0.8656554712892741,"Guidelines:
727"
CODE OF ETHICS,0.866738894907909,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
728"
CODE OF ETHICS,0.8678223185265439,"• If the authors answer No, they should explain the special circumstances that require a
729"
CODE OF ETHICS,0.8689057421451788,"deviation from the Code of Ethics.
730"
CODE OF ETHICS,0.8699891657638137,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
731"
CODE OF ETHICS,0.8710725893824486,"eration due to laws or regulations in their jurisdiction).
732"
BROADER IMPACTS,0.8721560130010835,"10. Broader Impacts
733"
BROADER IMPACTS,0.8732394366197183,"Question: Does the paper discuss both potential positive societal impacts and negative
734"
BROADER IMPACTS,0.8743228602383532,"societal impacts of the work performed?
735"
BROADER IMPACTS,0.875406283856988,"Answer: [Yes]
736"
BROADER IMPACTS,0.8764897074756229,"Justification: See Appendix A.2
737"
BROADER IMPACTS,0.8775731310942578,"Guidelines:
738"
BROADER IMPACTS,0.8786565547128927,"• The answer NA means that there is no societal impact of the work performed.
739"
BROADER IMPACTS,0.8797399783315276,"• If the authors answer NA or No, they should explain why their work has no societal
740"
BROADER IMPACTS,0.8808234019501625,"impact or why the paper does not address societal impact.
741"
BROADER IMPACTS,0.8819068255687974,"• Examples of negative societal impacts include potential malicious or unintended uses
742"
BROADER IMPACTS,0.8829902491874323,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
743"
BROADER IMPACTS,0.8840736728060672,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
744"
BROADER IMPACTS,0.8851570964247021,"groups), privacy considerations, and security considerations.
745"
BROADER IMPACTS,0.886240520043337,"• The conference expects that many papers will be foundational research and not tied
746"
BROADER IMPACTS,0.8873239436619719,"to particular applications, let alone deployments. However, if there is a direct path to
747"
BROADER IMPACTS,0.8884073672806068,"any negative applications, the authors should point it out. For example, it is legitimate
748"
BROADER IMPACTS,0.8894907908992417,"to point out that an improvement in the quality of generative models could be used to
749"
BROADER IMPACTS,0.8905742145178764,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
750"
BROADER IMPACTS,0.8916576381365113,"that a generic algorithm for optimizing neural networks could enable people to train
751"
BROADER IMPACTS,0.8927410617551462,"models that generate Deepfakes faster.
752"
BROADER IMPACTS,0.8938244853737811,"• The authors should consider possible harms that could arise when the technology is
753"
BROADER IMPACTS,0.894907908992416,"being used as intended and functioning correctly, harms that could arise when the
754"
BROADER IMPACTS,0.8959913326110509,"technology is being used as intended but gives incorrect results, and harms following
755"
BROADER IMPACTS,0.8970747562296858,"from (intentional or unintentional) misuse of the technology.
756"
BROADER IMPACTS,0.8981581798483207,"• If there are negative societal impacts, the authors could also discuss possible mitigation
757"
BROADER IMPACTS,0.8992416034669556,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
758"
BROADER IMPACTS,0.9003250270855905,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
759"
BROADER IMPACTS,0.9014084507042254,"feedback over time, improving the efficiency and accessibility of ML).
760"
SAFEGUARDS,0.9024918743228603,"11. Safeguards
761"
SAFEGUARDS,0.9035752979414952,"Question: Does the paper describe safeguards that have been put in place for responsible
762"
SAFEGUARDS,0.90465872156013,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
763"
SAFEGUARDS,0.905742145178765,"image generators, or scraped datasets)?
764"
SAFEGUARDS,0.9068255687973997,"Answer: [No]
765"
SAFEGUARDS,0.9079089924160346,"Justification: To ensure future responsible release, we are still in the process of implementing
766"
SAFEGUARDS,0.9089924160346695,"comprehensive safeguards.
767"
SAFEGUARDS,0.9100758396533044,"Guidelines:
768"
SAFEGUARDS,0.9111592632719393,"• The answer NA means that the paper poses no such risks.
769"
SAFEGUARDS,0.9122426868905742,"• Released models that have a high risk for misuse or dual-use should be released with
770"
SAFEGUARDS,0.9133261105092091,"necessary safeguards to allow for controlled use of the model, for example by requiring
771"
SAFEGUARDS,0.914409534127844,"that users adhere to usage guidelines or restrictions to access the model or implementing
772"
SAFEGUARDS,0.9154929577464789,"safety filters.
773"
SAFEGUARDS,0.9165763813651138,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
774"
SAFEGUARDS,0.9176598049837487,"should describe how they avoided releasing unsafe images.
775"
SAFEGUARDS,0.9187432286023836,"• We recognize that providing effective safeguards is challenging, and many papers do
776"
SAFEGUARDS,0.9198266522210184,"not require this, but we encourage authors to take this into account and make a best
777"
SAFEGUARDS,0.9209100758396533,"faith effort.
778"
LICENSES FOR EXISTING ASSETS,0.9219934994582882,"12. Licenses for existing assets
779"
LICENSES FOR EXISTING ASSETS,0.9230769230769231,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
780"
LICENSES FOR EXISTING ASSETS,0.9241603466955579,"the paper, properly credited and are the license and terms of use explicitly mentioned and
781"
LICENSES FOR EXISTING ASSETS,0.9252437703141928,"properly respected?
782"
LICENSES FOR EXISTING ASSETS,0.9263271939328277,"Answer: [Yes]
783"
LICENSES FOR EXISTING ASSETS,0.9274106175514626,"Justification: All existing assets used in this paper are properly credited. The license and
784"
LICENSES FOR EXISTING ASSETS,0.9284940411700975,"terms of use are properly respected.
785"
LICENSES FOR EXISTING ASSETS,0.9295774647887324,"Guidelines:
786"
LICENSES FOR EXISTING ASSETS,0.9306608884073673,"• The answer NA means that the paper does not use existing assets.
787"
LICENSES FOR EXISTING ASSETS,0.9317443120260022,"• The authors should cite the original paper that produced the code package or dataset.
788"
LICENSES FOR EXISTING ASSETS,0.9328277356446371,"• The authors should state which version of the asset is used and, if possible, include a
789"
LICENSES FOR EXISTING ASSETS,0.933911159263272,"URL.
790"
LICENSES FOR EXISTING ASSETS,0.9349945828819068,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
791"
LICENSES FOR EXISTING ASSETS,0.9360780065005417,"• For scraped data from a particular source (e.g., website), the copyright and terms of
792"
LICENSES FOR EXISTING ASSETS,0.9371614301191766,"service of that source should be provided.
793"
LICENSES FOR EXISTING ASSETS,0.9382448537378115,"• If assets are released, the license, copyright information, and terms of use in the
794"
LICENSES FOR EXISTING ASSETS,0.9393282773564464,"package should be provided. For popular datasets, paperswithcode.com/datasets
795"
LICENSES FOR EXISTING ASSETS,0.9404117009750813,"has curated licenses for some datasets. Their licensing guide can help determine the
796"
LICENSES FOR EXISTING ASSETS,0.9414951245937161,"license of a dataset.
797"
LICENSES FOR EXISTING ASSETS,0.942578548212351,"• For existing datasets that are re-packaged, both the original license and the license of
798"
LICENSES FOR EXISTING ASSETS,0.9436619718309859,"the derived asset (if it has changed) should be provided.
799"
LICENSES FOR EXISTING ASSETS,0.9447453954496208,"• If this information is not available online, the authors are encouraged to reach out to
800"
LICENSES FOR EXISTING ASSETS,0.9458288190682557,"the asset’s creators.
801"
NEW ASSETS,0.9469122426868906,"13. New Assets
802"
NEW ASSETS,0.9479956663055255,"Question: Are new assets introduced in the paper well documented and is the documentation
803"
NEW ASSETS,0.9490790899241603,"provided alongside the assets?
804"
NEW ASSETS,0.9501625135427952,"Answer: [Yes]
805"
NEW ASSETS,0.9512459371614301,"Justification: Once authorization is obtained, we will ensure that comprehensive documenta-
806"
NEW ASSETS,0.952329360780065,"tion is provided alongside the assets to facilitate their proper use and understanding.
807"
NEW ASSETS,0.9534127843986999,"Guidelines:
808"
NEW ASSETS,0.9544962080173348,"• The answer NA means that the paper does not release new assets.
809"
NEW ASSETS,0.9555796316359697,"• Researchers should communicate the details of the dataset/code/model as part of their
810"
NEW ASSETS,0.9566630552546046,"submissions via structured templates. This includes details about training, license,
811"
NEW ASSETS,0.9577464788732394,"limitations, etc.
812"
NEW ASSETS,0.9588299024918743,"• The paper should discuss whether and how consent was obtained from people whose
813"
NEW ASSETS,0.9599133261105092,"asset is used.
814"
NEW ASSETS,0.9609967497291441,"• At submission time, remember to anonymize your assets (if applicable). You can either
815"
NEW ASSETS,0.962080173347779,"create an anonymized URL or include an anonymized zip file.
816"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9631635969664138,"14. Crowdsourcing and Research with Human Subjects
817"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9642470205850487,"Question: For crowdsourcing experiments and research with human subjects, does the paper
818"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9653304442036836,"include the full text of instructions given to participants and screenshots, if applicable, as
819"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9664138678223185,"well as details about compensation (if any)?
820"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9674972914409534,"Answer: [NA]
821"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9685807150595883,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
822"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9696641386782232,"Guidelines:
823"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9707475622968581,"• The answer NA means that the paper does not involve crowdsourcing nor research with
824"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.971830985915493,"human subjects.
825"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9729144095341279,"• Including this information in the supplemental material is fine, but if the main contribu-
826"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9739978331527628,"tion of the paper involves human subjects, then as much detail as possible should be
827"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9750812567713976,"included in the main paper.
828"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9761646803900325,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
829"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9772481040086674,"or other labor should be paid at least the minimum wage in the country of the data
830"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9783315276273022,"collector.
831"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9794149512459371,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
832"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.980498374864572,"Subjects
833"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9815817984832069,"Question: Does the paper describe potential risks incurred by study participants, whether
834"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9826652221018418,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
835"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9837486457204767,"approvals (or an equivalent approval/review based on the requirements of your country or
836"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848320693391116,"institution) were obtained?
837"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9859154929577465,"Answer: [NA]
838"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9869989165763814,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
839"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880823401950163,"Guidelines:
840"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9891657638136512,"• The answer NA means that the paper does not involve crowdsourcing nor research with
841"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902491874322861,"human subjects.
842"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991332611050921,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
843"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9924160346695557,"may be required for any human subjects research. If you obtained IRB approval, you
844"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934994582881906,"should clearly state this in the paper.
845"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9945828819068255,"• We recognize that the procedures for this may vary significantly between institutions
846"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956663055254604,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967497291440953,"guidelines for their institution.
848"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9978331527627302,"• For initial submissions, do not include any information that would break anonymity (if
849"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989165763813651,"applicable), such as the institution conducting the review.
850"
