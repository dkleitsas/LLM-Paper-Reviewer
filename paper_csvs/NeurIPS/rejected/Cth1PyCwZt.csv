Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0004901960784313725,"Many recent studies of LLM performance have focused on the ability of LLMs
1"
ABSTRACT,0.000980392156862745,"to achieve outcomes comparable to humans on academic and professional exams.
2"
ABSTRACT,0.0014705882352941176,"However, it is not clear whether such studies shed light on the extent to which
3"
ABSTRACT,0.00196078431372549,"models show reasoning ability, and there is controversy about the significance and
4"
ABSTRACT,0.0024509803921568627,"implications of such results. We seek to look more deeply into the question of
5"
ABSTRACT,0.0029411764705882353,"how and whether the performance of LLMs on exams designed for humans re-
6"
ABSTRACT,0.003431372549019608,"flects true aptitude inherent in LLMs. We do so by making use of the tools of
7"
ABSTRACT,0.00392156862745098,"psychometrics which are designed to perform meaningful measurement in test
8"
ABSTRACT,0.004411764705882353,"taking. We leverage a unique dataset that captures the detailed performance of
9"
ABSTRACT,0.004901960784313725,"over 5M students across 8 college-entrance exams given over a span of two years
10"
ABSTRACT,0.005392156862745098,"in Brazil. With respect to the evaluation of LLM abilities, we show that the tools
11"
ABSTRACT,0.0058823529411764705,"of Item Response Theory (IRT) provide a more informative evaluation of model
12"
ABSTRACT,0.006372549019607843,"performance than the usual accuracy metrics employed in previous studies. Dig-
13"
ABSTRACT,0.006862745098039216,"ging deeper, we show that the modeling framework of IRT, by explicitly modeling
14"
ABSTRACT,0.007352941176470588,"the difficulty levels of questions, allows us to quantitatively distinguish between
15"
ABSTRACT,0.00784313725490196,"LLMs that answer questions in “human-like” patterns versus LLMs that do not.
16"
ABSTRACT,0.008333333333333333,"We also show how to quantitatively identify cases in which exam results are not
17"
ABSTRACT,0.008823529411764706,"reliable measurements of an LLM’s ability. Using the tools of IRT we can also
18"
ABSTRACT,0.009313725490196078,"identify specific questions that appear to be either much easier, or much harder,
19"
ABSTRACT,0.00980392156862745,"for machines than for humans, and we give some reasons for those differences.
20"
ABSTRACT,0.010294117647058823,"Overall, our study shows that the conventional focus on accuracy as the primary
21"
ABSTRACT,0.010784313725490196,"performance metric for LLM studies does not allow us to deeply understand the
22"
ABSTRACT,0.011274509803921568,"true capabilities of LLMs and compare them to that of humans. Thus, we claim
23"
ABSTRACT,0.011764705882352941,"that psychometric modeling should play a larger role in the evaluation of LLM
24"
ABSTRACT,0.012254901960784314,"capabilities on exams designed for humans.
25"
INTRODUCTION,0.012745098039215686,"1
Introduction
26"
INTRODUCTION,0.013235294117647059,"Large Language Models (LLMs) have demonstrated an impressive ability in performing well on ex-
27"
INTRODUCTION,0.013725490196078431,"aminations designed for humans [25, 30], such as the US bar exam [27], the US Medical Licensing
28"
INTRODUCTION,0.014215686274509804,"Exam [21], and many others [45, 53]. This yields controversy in how researchers should interpret
29"
INTRODUCTION,0.014705882352941176,"such results, raising two kinds of criticisms of those apparent successes. The first is the potential for
30"
INTRODUCTION,0.015196078431372549,"publicly-given exams (and answers) to leak into models’ training data. The second, and more fun-
31"
INTRODUCTION,0.01568627450980392,"damental, issue is the notion of construct validity [44]. Most exams given to humans are intended to
32"
INTRODUCTION,0.016176470588235296,"measure a construct, e.g., legal analysis ability, medical analysis ability, etc. However, the reliability
33"
INTRODUCTION,0.016666666666666666,"of these exams in measuring the relevant construct for non-humans is usually ignored, and exams
34"
INTRODUCTION,0.01715686274509804,"that are valid in one context may not generalize across different groups, settings or tasks [24].
35"
INTRODUCTION,0.01764705882352941,"Formalizing the notion of construct validity in general is challenging. Since the 1950s, the field
36"
INTRODUCTION,0.018137254901960786,"of psychometrics has been grappling with how to design examinations that validly measure human
37"
INTRODUCTION,0.018627450980392157,"abilities along specific dimensions. The primary tool developed has been Item Response Theory
38"
INTRODUCTION,0.01911764705882353,"(IRT) [10], which has been employed in psychology, medicine, and especially in educational test-
39"
INTRODUCTION,0.0196078431372549,"ing. IRT formalizes the unobserved construct as a continuous latent variable, and models stochastic
40"
INTRODUCTION,0.020098039215686276,"responses of humans to questions as a logistic regression conditional on that latent variable.
41"
INTRODUCTION,0.020588235294117647,"In this paper, we demonstrate how IRT can help shed light on whether LLMs are in fact show-
42"
INTRODUCTION,0.02107843137254902,"ing human-like performance on exams intended for humans. As a case study, we use one of the
43"
INTRODUCTION,0.021568627450980392,"largest university-entrance exams in the world, a dataset comprising the performance of over 5 mil-
44"
INTRODUCTION,0.022058823529411766,"lion Brazilian students on eight multiple-choice exams administered over two years. Each exam
45"
INTRODUCTION,0.022549019607843137,"was prepared and fitted to an IRT model by educational testing experts, giving us an unparalleled
46"
INTRODUCTION,0.02303921568627451,"opportunity to examine the performance of LLMs in detail.
47"
INTRODUCTION,0.023529411764705882,"Our results show that the LLMs we study reveal performance patterns that are consistent with ex-
48"
INTRODUCTION,0.024019607843137256,"pected human behavior in many cases. Nonetheless, we also frequently observe significant deviation
49"
INTRODUCTION,0.024509803921568627,"from human-like behavior. We demonstrate how to use the tools of IRT to quantitatively distinguish
50"
INTRODUCTION,0.025,"between human-like and non-human-like behavior. We then explore the differences between mod-
51"
INTRODUCTION,0.025490196078431372,"els and exam types that correlate with differences in response patterns. Lastly, we use the tools of
52"
INTRODUCTION,0.025980392156862746,"IRT and psychometrics to identify cases where exams are not producing reliable estimates of LLM
53"
INTRODUCTION,0.026470588235294117,"ability and understand why this happens. This occurs because exams are in some cases too difficult
54"
INTRODUCTION,0.02696078431372549,"for the models, and in other cases too easy for them and as such they cannot properly measure the
55"
INTRODUCTION,0.027450980392156862,"ability of certain LLMs.
56"
INTRODUCTION,0.027941176470588237,"Moving beyond conclusions about current models, the broader contribution of our study is to demon-
57"
INTRODUCTION,0.028431372549019607,"strate the power of IRT as a framework for evaluating LLMs. For example, in Classical Test Theory
58"
INTRODUCTION,0.02892156862745098,"(CTT), no attempt is made to assess the difficulty of individual questions, in-line with majority of in
59"
INTRODUCTION,0.029411764705882353,"standard LLM benchmarks that pursues accuracy [8, 43, 16, 4]. In contrast, as we will show below,
60"
INTRODUCTION,0.029901960784313727,"IRT simultaneously measures both test takers and exam questions (on the same scale). In doing so,
61"
INTRODUCTION,0.030392156862745098,"IRT allows one to distinguish between test takers with similar CTT (accuracy) scores, but differing
62"
INTRODUCTION,0.030882352941176472,"levels of true ability, by inspecting the pattern of correct or incorrect answers given. Moreover, we
63"
INTRODUCTION,0.03137254901960784,"deploy a broader set of tools (e.g., goodness-of-fit, Fisher information, discrimination index) which
64"
INTRODUCTION,0.031862745098039214,"enable us to evaluate which are the cases in which fitting the IRT model to the LLMs response
65"
INTRODUCTION,0.03235294117647059,"patterns gives us reliable estimates of the models’ ability. Thus, we believe that the methods of our
66"
INTRODUCTION,0.03284313725490196,"study represent a valuable step beyond the use of simple accuracy for assessing whether both current
67"
INTRODUCTION,0.03333333333333333,"and future LLMs show human-like response patterns.
68"
RELATED WORK,0.033823529411764704,"2
Related Work
69"
RELATED WORK,0.03431372549019608,"Our study connects a number of research areas, spanning benchmarking LLMs, the applications of
70"
RELATED WORK,0.03480392156862745,"item response theory, and the evaluation of LLMs using exams designed for humans.
71"
RELATED WORK,0.03529411764705882,"Benchmarking LLMs. The most common strategy to evaluate LLMs is through traditional large-
72"
RELATED WORK,0.035784313725490194,"scale NLP benchmarks [46, 40, 11, 18, 16, 19, 4]. Conventionally, benchmark evaluation relies on
73"
RELATED WORK,0.03627450980392157,"some notion of accuracy – the number of correct answers – as a proxy for ability [8, 43]. A key
74"
RELATED WORK,0.03676470588235294,"distinction of our study is to draw attention to the limitations of the use of accuracy alone [34] for
75"
RELATED WORK,0.03725490196078431,"evaluating the performance of LLMs on benchmarks in understanding the similarity between the
76"
RELATED WORK,0.037745098039215684,"performance of models versus humans.
77"
RELATED WORK,0.03823529411764706,"LLMs and Exams Designed for Humans. Many attempts to evaluate LLMs use exams designed
78"
RELATED WORK,0.03872549019607843,"for humans, e.g., at college-entrance [1, 26] or college-level [14, 37, 47, 13, 41, 53]. These exams
79"
RELATED WORK,0.0392156862745098,"also generally use accuracy as a metric of ability; one focus of our work is on how to use IRT
80"
RELATED WORK,0.039705882352941174,"analysis to determine when such exams in fact perform meaningful measurement.
81"
RELATED WORK,0.04019607843137255,"The Brazilian nationwide college-entrance exams we use in this work (ENEM), detailed in Sec-
82"
RELATED WORK,0.04068627450980392,"tion 4.1, were used in previous efforts to evaluate NLP models [38, 39, 26]. However, those studies
83"
RELATED WORK,0.041176470588235294,"only used accuracy and did not make use of the IRT models associated with the exam, which is a
84"
RELATED WORK,0.041666666666666664,"central aspect our work.
85"
RELATED WORK,0.04215686274509804,"IRT in Machine Learning. Work in psychometrics (i.e., the measurement of human cognitive
86"
RELATED WORK,0.04264705882352941,"abilities), detailed in Section 3, has shown that using accuracy as a exam score may not reflect the
87"
RELATED WORK,0.043137254901960784,"true underlying abilities of individuals [15]. As a result, IRT has been advocated for use in machine
88"
RELATED WORK,0.043627450980392155,"learning (ML) as an improved tool for benchmarking. The authors in [33] show that it is possible to
89"
RELATED WORK,0.04411764705882353,"produce rankings of NLP models which are more reliable and stable using IRT than accuracy. Item
90"
RELATED WORK,0.0446078431372549,"response theory has also been shown to help in spotting noisy questions, identifying overfitting,
91"
RELATED WORK,0.045098039215686274,"selecting features, and designing better benchmarks for ML [29, 35, 20, 54, 22]. However, there is
92"
RELATED WORK,0.045588235294117645,"a critical difference between the previous uses of IRT in ML and our work. Previous work uses IRT
93"
RELATED WORK,0.04607843137254902,"by training an IRT model on the results of ML models solving question-answering or classification
94"
RELATED WORK,0.04656862745098039,"questions. Our method is different: we leverage the fact that we have access to an IRT model trained
95"
RELATED WORK,0.047058823529411764,"on human responses, and we do not retrain on model responses. We take this approach because
96"
RELATED WORK,0.047549019607843135,"a central goal of our study is to explore whether LLMs are in fact following response patterns as
97"
RELATED WORK,0.04803921568627451,"exhibited by human test takers.
98"
RELATED WORK,0.04852941176470588,"Finally, we note that [42] shares some goals with our work. The investigation seeks to understand
99"
RELATED WORK,0.049019607843137254,"whether LLMs show human-like response biases in surveys. We also look at the question of whether
100"
RELATED WORK,0.049509803921568625,"LLMs show human-like response patterns, but we study the question along different dimensions:
101"
RELATED WORK,0.05,"(a) patterns of correct and incorrect answers in exams; and (b) the ways in which LLMs choose
102"
RELATED WORK,0.050490196078431374,"incorrect answers. Additionally, Xia et al. [51] recognize that accuracy as a single metric does not
103"
RELATED WORK,0.050980392156862744,"capture errors LLMs can make in intermediate steps when solving mathematical tasks, and they
104"
RELATED WORK,0.051470588235294115,"systematically study those errors.
105"
BACKGROUND,0.05196078431372549,"3
Background
106"
BACKGROUND,0.052450980392156864,"In this section, we give some background of the tools we use from psychometrics.
107"
BACKGROUND,0.052941176470588235,"Classical Test Theory (CTT): CTT [2] evaluates test takers based on the fraction of questions
108"
BACKGROUND,0.053431372549019605,"they answer correctly. We call this score accuracy or CTT score of the test taker and we use these
109"
BACKGROUND,0.05392156862745098,"two terms interchangeably. Inadequately, CTT does not differentiate between difficult and easy
110"
BACKGROUND,0.054411764705882354,"questions, nor does it take into consideration the patterns of correct answers. For example, the CTT
111"
BACKGROUND,0.054901960784313725,"score does not penalize a test taker who answers correctly difficult questions, but answers wrongly
112"
BACKGROUND,0.055392156862745096,"easy ones – despite the fact that such a pattern might be indicative of randomness or cheating.
113"
BACKGROUND,0.05588235294117647,"Item Response Theory (IRT): IRT [12, 5] is a model used extensively in psychometrics to measure
114"
BACKGROUND,0.056372549019607844,"the ability level of the test takers and evaluate the difficulty of the test questions (which are referred
115"
BACKGROUND,0.056862745098039215,"to as items in psychometrics). IRT takes into consideration the difficulty of the questions when eval-
116"
BACKGROUND,0.057352941176470586,"uating test-taker’s performance and also makes use of the pattern of correct and incorrect responses
117"
BACKGROUND,0.05784313725490196,"on the exam. The model associates with every test taker j a parameter θj, which corresponds to the
118"
BACKGROUND,0.058333333333333334,"ability of j. The two-parameter IRT model (2PL) associates every question i with two parameters
119"
BACKGROUND,0.058823529411764705,"ϕi = (αi, βi). The model assumes that a test taker with ability θj answers question i associated with
120"
BACKGROUND,0.059313725490196076,"ϕi correctly with probability given by the logistic function:
121"
BACKGROUND,0.059803921568627454,"pij =
eαi(θj−βi)"
BACKGROUND,0.060294117647058824,"1 + eαi(θj−βi) .
(1)"
BACKGROUND,0.060784313725490195,"Parameter αi is the discrimination parameter and βi is the difficulty of question i. Note that the
122"
BACKGROUND,0.061274509803921566,"ability θj and the difficulty level βi are in the same scale; after all, the difference (θj −βi) directly
123"
BACKGROUND,0.061764705882352944,"affects pij. For fixed αi, the difficulty parameter βi is the value (on the ability scale) for which
124"
BACKGROUND,0.062254901960784315,"pij = 0.5. Parameter αi characterizes how well question i can differentiate among test takers
125"
BACKGROUND,0.06274509803921569,"located at different points of the ability continuum; αi is proportional to the slope of pij = pi(θj)
126"
BACKGROUND,0.06323529411764706,"at the point where pij = 0.5 – the steeper the slope, the higher the discriminatory power of i. All
127"
BACKGROUND,0.06372549019607843,"the parameters of this model take values in (−∞, +∞). Note that any set of questions comprising
128"
BACKGROUND,0.0642156862745098,"an exam spans a certain range of βi values; such a set is not appropriate to assess test takers with
129"
BACKGROUND,0.06470588235294118,"abilities outside this range.
130"
BACKGROUND,0.06519607843137255,"The 3-Parameter IRT model (3PL for short) is an extension of the above model that also incorporates
131"
BACKGROUND,0.06568627450980392,"a pseudo-guessing parameter γi. Thus, in 3PL every question i is associated with three parameters
132"
BACKGROUND,0.0661764705882353,"Φi = (αi, βi, γi); αi and βi are the same as before. Intuitively, γi is the probability of answering
133"
BACKGROUND,0.06666666666666667,"correctly based on random guess with γi ∈[0, 1]. Thus, the probability of a test taker with ability θj
134"
BACKGROUND,0.06715686274509804,"to answer question i correctly is: Pij = γi + (1 −γi)pij.
135"
BACKGROUND,0.06764705882352941,"Given test-taker responses, the parameters of the model can be estimated using Bayesian meth-
136"
BACKGROUND,0.06813725490196078,"ods [5]. In our case, the ENEM dataset came with a set of questions for which the parameters
137"
BACKGROUND,0.06862745098039216,"(αi, βi, γi) had already been fitted by education experts [17]. Therefore, for each one of the LLMs
138"
BACKGROUND,0.06911764705882353,"we considered, we only need to compute their ability parameters – given their response patterns.
139"
BACKGROUND,0.0696078431372549,"Intuitively, large values of θ correspond to test takers with high ability levels and vice versa. High
140"
BACKGROUND,0.07009803921568628,"ability value θ of an LLM implies better performance.
141"
BACKGROUND,0.07058823529411765,"Although the ability levels of test takers can be used as a measure of their performance, one should
142"
BACKGROUND,0.07107843137254902,"also know if the test takers are consistent with the model, e.g., they should answer easy questions
143"
BACKGROUND,0.07156862745098039,"correctly if they answer difficult questions correctly. One index that enables us to evaluate the
144"
BACKGROUND,0.07205882352941176,"consistency of the test takers with the model is the lz index [12]. Intuitively, the lz index is based
145"
BACKGROUND,0.07254901960784314,"on the standardization of a test-taker’s log-likelihood function given their theta values. Assume a
146"
BACKGROUND,0.07303921568627451,"set of I questions and test taker j with ability θj and response vector rj such that rj(i) = 1 (resp.
147"
BACKGROUND,0.07352941176470588,"rj(i) = 0) if j answered question i correctly (resp. wrongly). Then, the log-likelihood of j is
148"
BACKGROUND,0.07401960784313726,simply: Lj = P
BACKGROUND,0.07450980392156863,"i∈I [rj(i) ln Pij + (1 −rj(i)) ln(1 −Pij)] . To standardize Lj we need both its
149"
BACKGROUND,0.075,"mean (E[Lj]) and variance (Var(Lj)). Then, the lz score is computed as:
150"
BACKGROUND,0.07549019607843137,"lz(j) = Lj −E[Lj]
p"
BACKGROUND,0.07598039215686274,"Var(Lj)
.
(2)"
BACKGROUND,0.07647058823529412,"In a well-designed test, the lz scores are expected to have a unit normal distribution – this is the
151"
BACKGROUND,0.0769607843137255,"case for humans taking the ENEM test (see for example Figure 3). In general, lz values close to 0
152"
BACKGROUND,0.07745098039215687,"are considered good: it means the test takers’ response patterns are consistent with what is expected
153"
BACKGROUND,0.07794117647058824,"from them by the model. Negative lz(j) scores reflect an unlikely response vector. A positive lz(j)
154"
BACKGROUND,0.0784313725490196,"score indicates that j has a more likely response vector than indicated by their ability.
155"
BACKGROUND,0.07892156862745098,"We can access the amount of information that an item i provides to estimate θ under the 3PL model
156"
BACKGROUND,0.07941176470588235,"by the Fisher information, which is given by:
157"
BACKGROUND,0.07990196078431372,"Ii(θ) = α2
i"
BACKGROUND,0.0803921568627451,(pi −γi)2
BACKGROUND,0.08088235294117647,(1 −γi)2
BACKGROUND,0.08137254901960785, 1 −pi pi
BACKGROUND,0.08186274509803922,"
.
(3)"
BACKGROUND,0.08235294117647059,"The total information of a test is simply the sum of item information, i.e., I(θ) = P"
BACKGROUND,0.08284313725490196,"i∈I Ii(θ).
158"
BACKGROUND,0.08333333333333333,"The Fisher information is connected with the standard error of the estimation, given by SE(θ) =
159 1/
p"
BACKGROUND,0.0838235294117647,"I(θ). When a test has high Fisher information in a certain θ range, the test has more discrimi-
160"
BACKGROUND,0.08431372549019608,"native power in that range, producing scores with less measurement errors.
161"
METHODS,0.08480392156862746,"4
Methods
162"
THE ENEM EXAM,0.08529411764705883,"4.1
The ENEM Exam
163"
THE ENEM EXAM,0.0857843137254902,"The Exame Nacional do Ensino M´edio (ENEM), world’s second largest university entrance exam
164"
THE ENEM EXAM,0.08627450980392157,"behind Chinese’s Gaokao exam, is taken by millions of Brazilian students each year [39]. ENEM
165"
THE ENEM EXAM,0.08676470588235294,"comprises questions requiring different levels of domain-specific knowledge and reasoning [3].
166"
THE ENEM EXAM,0.08725490196078431,"The exam is in Brazilian Portuguese and consists of four sections, each of which has 45 multiple-
167"
THE ENEM EXAM,0.08774509803921568,"choice questions with five options [17]. Each section is treated as a separate exam for the purposes
168"
THE ENEM EXAM,0.08823529411764706,"of modeling via IRT. The four sections consist of the Humanities, the Languages and Codes, the
169"
THE ENEM EXAM,0.08872549019607844,"Natural Sciences, and the Math exams. The description of these exams is given in Appendix A.11.
170"
THE ENEM EXAM,0.0892156862745098,"Since 2009, the grades assigned to ENEM test-takers have been determined using IRT. Using IRT
171"
THE ENEM EXAM,0.08970588235294118,"helps to penalize guessing, differentiate among students that otherwise would get the same (CTT)
172"
THE ENEM EXAM,0.09019607843137255,"grade, and compare among students that took exams in different years. The ENEM organizers
173"
THE ENEM EXAM,0.09068627450980392,"release not only the exam content and questions, but also the student (anonymized) responses and
174"
THE ENEM EXAM,0.09117647058823529,"their CTT and IRT scores, which enables downstream studies.
175"
THE ENEM EXAM,0.09166666666666666,"From our standpoint, there are a number of relevant aspects of the process used by the ENEM de-
176"
THE ENEM EXAM,0.09215686274509804,"velopers [17]. First, questions are given to a sample of students, whose answers are used to find
177"
THE ENEM EXAM,0.09264705882352942,"inconsistencies and errors. Next, an important test of construct validity is to verify the unidimen-
178"
THE ENEM EXAM,0.09313725490196079,"sionality of the latent trait, for which the ENEM team uses Full Information Factor Analysis [7].
179"
THE ENEM EXAM,0.09362745098039216,"Finally, the IRT model itself is fit using the Marginal Maximum Likelihood Estimator [6]. Using the
180"
THE ENEM EXAM,0.09411764705882353,"results, the developers may exclude questions having poor model fit.
181"
THE ENEM EXAM,0.0946078431372549,"The exams, their solutions, and all the fitted parameters of the 3PL IRT model (θj, αi, βi, γi) are
182"
THE ENEM EXAM,0.09509803921568627,"publicly available at the Brazilian government website [17]. To the best of our knowledge, these data
183"
THE ENEM EXAM,0.09558823529411764,"are the largest and most comprehensive public dataset based on item-response theory available. The
184"
THE ENEM EXAM,0.09607843137254903,"datasets contain questions and complete response patterns of all students taking the exams in 2022
185"
THE ENEM EXAM,0.0965686274509804,"and 2023. Questions for the 2023 exam were released in November 2023, minimizing the chance
186"
THE ENEM EXAM,0.09705882352941177,"they are in training data for most of the LLMs we considered. However, we expect fragments of the
187"
THE ENEM EXAM,0.09754901960784314,"exam being in the training data (e.g. poems, and any other widely available material used as part of
188"
THE ENEM EXAM,0.09803921568627451,"a question) 1. The number of test takers per year ranged from 2.2M to 3.7M.
189"
THE ENEM EXAM,0.09852941176470588,"The ENEM exams are initially made available as PDF files; we used the Python library PyPDF2,
190"
THE ENEM EXAM,0.09901960784313725,"followed by regular expressions and some manual adjustments to extract each question from its
191"
THE ENEM EXAM,0.09950980392156862,"exam file. In order to account for possible effects of Language, as diagnosed in previous work [31],
192"
THE ENEM EXAM,0.1,"we translated all questions to English and run all experiments in Portuguese and English. For those
193"
THE ENEM EXAM,0.10049019607843138,"exam questions that incorporated images, we used the version of the exam designed for blind people
194"
THE ENEM EXAM,0.10098039215686275,"containing textual descriptions of the images. We manually audited all questions in 2022 and 2023
195"
THE ENEM EXAM,0.10147058823529412,"exams to ensure their quality (Appendix A.1).
196"
MODELS,0.10196078431372549,"4.2
Models
197"
MODELS,0.10245098039215686,"We evaluate the following family of models: the open source models Mistral-7B, Gemma-7B,
198"
MODELS,0.10294117647058823,"Llama2-7B, Llama2-13B, Llama3-8B, and GPT 3.5. For the open source models, we evaluate
199"
MODELS,0.1034313725490196,"on both instructed and non-instructed tuned versions. Our choice of models enables the study of
200"
MODELS,0.10392156862745099,"models of similar size (the majority of our models are of size 7B), but also introduces diversity of
201"
MODELS,0.10441176470588236,"architectures (GPT, Gemma, Mistral, Llama), size (7B vs. 13B), training data (Llama2 vs. Llama3),
202"
MODELS,0.10490196078431373,"and training strategies (with and without instruction tuning).
203"
MODELS,0.1053921568627451,"We prompt models with {0, 1, 4}-shots, following conventional question-answer benchmark
204"
MODELS,0.10588235294117647,"prompting strategies [32] (example prompts in Appendix A.4). We measure model’s next token
205"
MODELS,0.10637254901960784,"probability across five option letters, and average predictions across 30 shuffles of the order of the
206"
MODELS,0.10686274509803921,"answer choices to correct for the well-known effect of position bias [28] (Details in Appendix A.4).
207"
RESULTS,0.10735294117647058,"5
Results
208"
RESULTS,0.10784313725490197,"In this section, we present our main findings. All the results we show here are for the 2023 ENEM
209"
RESULTS,0.10833333333333334,"exams, with four-shot prompting. Results for the 2022 ENEM exam and for zero-shot and one-shot
210"
RESULTS,0.10882352941176471,"prompting and for open source instructed tuned models are shown in Appendices A.6 – A.9. The
211"
RESULTS,0.10931372549019608,"results we show in this section are strongly consistent with the results we get for the 2022 ENEM
212"
RESULTS,0.10980392156862745,"exam and for one-shot prompting.
213"
RESULTS,0.11029411764705882,"5.1
Accuracy vs. Ability Level
214"
RESULTS,0.11078431372549019,"We first investigate how humans compare to LLMs when IRT parameter θ is used instead of accuracy
215"
RESULTS,0.11127450980392156,"(the metric that is employed in most LLM benchmarking, e.g., [8, 43]). In Figure 1 we plot the CTT
216"
RESULTS,0.11176470588235295,"score (accuracy) vs IRT score (θ) for 30 shuffles of answer options for each model. The light blue
217"
RESULTS,0.11225490196078432,"background points correspond to the humans who took the exam. Each of the closed curves in the
218"
RESULTS,0.11274509803921569,"figure corresponds to one LLM, and shows the central 90% of the LMM’s distribution.
219"
RESULTS,0.11323529411764706,"First, we observe that there are many cases where identical accuracy scores result in different θ
220"
RESULTS,0.11372549019607843,"scores. This reflects the fact that IRT takes into account not just the number, but also the pattern
221"
RESULTS,0.1142156862745098,"of correct answers. Second, for many LLMs, particularly in the Humanities and Languages exams,
222"
RESULTS,0.11470588235294117,"there is overall greater variability in the accuracy score than in the IRT score. This suggests that IRT
223"
RESULTS,0.11519607843137254,"is less sensitive to the variations in LLM output that are due to the LLM’s inherent randomness.
224"
RESULTS,0.11568627450980393,"To compare the performance between LLMs and humans, we compare their IRT scores (θ). Recall
225"
RESULTS,0.1161764705882353,"that IRT score of 0 corresponds to the average ability of a human test taker. Across all four subjects,
226"
RESULTS,0.11666666666666667,"the majority of models have CTT and IRT scores overlapping with humans. LLMs in general achieve
227"
RESULTS,0.11715686274509804,"θ scores above that of the human average in Humanities, Languages, and Natural Sciences, but below
228"
RESULTS,0.11764705882352941,"human average in Mathematics. Looking at specific models, we find the Llama2 models at the lower
229"
RESULTS,0.11813725490196078,"end of θ scores, Mistral and Llama3 in the middle range, and GPT-3.5 and Gemma-7B at the higher
230"
RESULTS,0.11862745098039215,"end of θ scores.
231"
RESULTS,0.11911764705882352,"The language of the exam affects some models’ performance. In Languages and Natural Sciences,
232"
RESULTS,0.11960784313725491,"GPT-3.5 tends to perform better in Portuguese compared to English, while in Humanities and Natural
233"
RESULTS,0.12009803921568628,1Gemma models are released in 2024 and we suspect contamination issues from analysis in Appendix A.3. -2 -1 0 1 2 3
RESULTS,0.12058823529411765,IRT score
LANGUAGES AND CODES,0.12107843137254902,"2023 Languages and Codes
2023 Humanities"
LANGUAGES AND CODES,0.12156862745098039,"0
15
30
45"
LANGUAGES AND CODES,0.12205882352941176,CTT score -2 -1 0 1 2 3
LANGUAGES AND CODES,0.12254901960784313,IRT score
NATURAL SCIENCES,0.1230392156862745,2023 Natural Sciences
NATURAL SCIENCES,0.12352941176470589,"0
15
30
45"
NATURAL SCIENCES,0.12401960784313726,CTT score
MATHEMATICS,0.12450980392156863,2023 Mathematics
MATHEMATICS,0.125,"ChatGPT-3.5 (EN)
ChatGPT-3.5 (PT-BR)
Gemma-7B (EN)
Gemma-7B (PT-BR)
LLaMA2-13B (EN)
LLaMA2-13B (PT-BR)
LLaMA2-7B (EN)
LLaMA2-7B (PT-BR)
LLaMA3-8B (EN)
LLaMA3-8B (PT-BR)
Mistral-7B (EN)
Mistral-7B (PT-BR)"
MATHEMATICS,0.12549019607843137,"Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM
2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM
datapoints are computed from different shuffles of the order of answer choices."
MATHEMATICS,0.12598039215686274,"Sciences, the Llama models tend to perform worse in Portuguese than in English. This suggests that
234"
MATHEMATICS,0.1264705882352941,"there are differences regarding the reasoning ability and the amount of knowledge accessible to the
235"
MATHEMATICS,0.12696078431372548,"models in each language.
236"
MATHEMATICS,0.12745098039215685,"Importantly, outlier models all tend to have higher accuracy and/or lower IRT scores than humans.
237"
MATHEMATICS,0.12794117647058822,"These models answer more questions correctly than humans do, but show error patterns that are not
238"
MATHEMATICS,0.1284313725490196,"entirely human-like. We dig into this phenomenon next.
239"
RESPONSE PATTERNS,0.128921568627451,"5.2
Response Patterns
240"
RESPONSE PATTERNS,0.12941176470588237,"One of our goals is to assess whether the LLMs we examine show good fit to the ENEM IRT model,
241"
RESPONSE PATTERNS,0.12990196078431374,"as crafted by the educational expert team described in Section 4.1. Intuitively, a test taker showing
242"
RESPONSE PATTERNS,0.1303921568627451,"good fit to an IRT model is an individual j that tends to make less frequent mistakes on “easy”
243"
RESPONSE PATTERNS,0.13088235294117648,"questions (question i with βi < θj) while making more frequent mistakes on “hard” questions
244"
RESPONSE PATTERNS,0.13137254901960785,"(question i with βi > θj). Thus, to assess fit we need to inspect the response patterns of the LLMs.
245"
RESPONSE PATTERNS,0.13186274509803922,"Figure 2 shows the response patterns of LLMs for the 2023 exam. Every cell (i, j) corresponds to
246"
RESPONSE PATTERNS,0.1323529411764706,"the probability that LLM i answered question j correctly, where probabilities are computed over the
247"
RESPONSE PATTERNS,0.13284313725490196,"30 shuffles. We use gray scale with a black (resp. white) cell representing 1 (resp. 0). Questions are
248"
RESPONSE PATTERNS,0.13333333333333333,"ordered in increasing order of their β values. Generally, rows with darker overall patterns (higher
249"
RESPONSE PATTERNS,0.1338235294117647,"correctness) are indicative of higher θ scores.
250"
RESPONSE PATTERNS,0.13431372549019607,"The figure demonstrates a number of points. For example, on the Math exam, the figure exhibits a
251"
RESPONSE PATTERNS,0.13480392156862744,"response pattern that appears to show low θ values for all models, which confirms results in Figure 1.
252"
RESPONSE PATTERNS,0.13529411764705881,"In addition, the figure shows that for some questions, the 30 shuffles of answer choices of a given
253"
RESPONSE PATTERNS,0.13578431372549019,"model are often either all correct or all incorrect. However, there are some grey areas in the figure for
254"
RESPONSE PATTERNS,0.13627450980392156,"all the exams, indicating that shuffling the options can affect the LLM’s answers on certain items.
255"
RESPONSE PATTERNS,0.13676470588235295,"Furthermore, the patterns show that many questions appear to be either “easy” (black) or “hard”
256"
RESPONSE PATTERNS,0.13725490196078433,"(white) for all models at the same time. Likewise, in many cases models show similar performance
257"
RESPONSE PATTERNS,0.1377450980392157,"on the English and Portuguese versions of a given question.
258"
RESPONSE PATTERNS,0.13823529411764707,"Overall, the response patterns we observe suggest that the Math exam is “too difficult,” with mod-
259"
RESPONSE PATTERNS,0.13872549019607844,"els often resorting to guessing. On the other hand, most LLMs consistently answer correctly the
260 1
45"
RESPONSE PATTERNS,0.1392156862745098,"ChatGPT-3.5 (EN)
ChatGPT-3.5 (PT-BR)"
RESPONSE PATTERNS,0.13970588235294118,"Gemma-7B (EN)
Gemma-7B (PT-BR)"
RESPONSE PATTERNS,0.14019607843137255,"LLaMA2-13B (EN)
LLaMA2-13B (PT-BR)"
RESPONSE PATTERNS,0.14068627450980392,"LLaMA2-7B (EN)
LLaMA2-7B (PT-BR)"
RESPONSE PATTERNS,0.1411764705882353,"LLaMA3-8B (EN)
LLaMA3-8B (PT-BR)"
RESPONSE PATTERNS,0.14166666666666666,"Mistral-7B (EN)
Mistral-7B (PT-BR)"
LANGUAGES AND CODES,0.14215686274509803,"2023 Languages and Codes 1
45"
NATURAL SCIENCES,0.1426470588235294,2023 Natural Sciences
NATURAL SCIENCES,0.14313725490196078,"1
45
Question"
NATURAL SCIENCES,0.14362745098039215,"ChatGPT-3.5 (EN)
ChatGPT-3.5 (PT-BR)"
NATURAL SCIENCES,0.14411764705882352,"Gemma-7B (EN)
Gemma-7B (PT-BR)"
NATURAL SCIENCES,0.14460784313725492,"LLaMA2-13B (EN)
LLaMA2-13B (PT-BR)"
NATURAL SCIENCES,0.1450980392156863,"LLaMA2-7B (EN)
LLaMA2-7B (PT-BR)"
NATURAL SCIENCES,0.14558823529411766,"LLaMA3-8B (EN)
LLaMA3-8B (PT-BR)"
NATURAL SCIENCES,0.14607843137254903,"Mistral-7B (EN)
Mistral-7B (PT-BR)"
HUMANITIES,0.1465686274509804,2023 Humanities
HUMANITIES,0.14705882352941177,"1
43
Question"
MATHEMATICS,0.14754901960784314,2023 Mathematics
MATHEMATICS,0.1480392156862745,"Figure 2: Response patterns for each LLM, where darker indicates more often correct (across
random option shuffles). Questions are sorted in increasing difficulty (β value). LLMs are non-
instructed tuned open source models and GPT3.5 with four-shot."
MATHEMATICS,0.14852941176470588,"questions in the Humanities exam, implying that this is an easy exam for them. The performance of
261"
MATHEMATICS,0.14901960784313725,"LLMs in the Natural Science exam is the most interesting as there are blocks of questions that most
262"
MATHEMATICS,0.14950980392156862,"LLMs answer consistently correctly, interleaved with blocks of questions that most LLMs answer
263"
MATHEMATICS,0.15,"incorrectly. This suggests that there are questions that are easy for humans but difficult for LLMs
264"
MATHEMATICS,0.15049019607843137,"and vice versa. In the next subsection we analyze this phenomenon more closely.
265"
RELIABILITY OF IRT SCORES FOR LLMS,0.15098039215686274,"5.3
Reliability of IRT scores for LLMs
266"
RELIABILITY OF IRT SCORES FOR LLMS,0.1514705882352941,"In this section, we investigate whether the ENEM exam is a valid test for LLMs’ ability, in the same
267"
RELIABILITY OF IRT SCORES FOR LLMS,0.15196078431372548,"way it is for humans. Intuitively, we want to define measures that allow us to quantify to what extent
268"
RELIABILITY OF IRT SCORES FOR LLMS,0.15245098039215688,"we trust the IRT scores we obtained for LLMs. We propose three different ways of doing this. The
269"
RELIABILITY OF IRT SCORES FOR LLMS,0.15294117647058825,"first is goodness-of-fit that quantifies whether the response of LLMs fit the IRT model. The second
270"
RELIABILITY OF IRT SCORES FOR LLMS,0.15343137254901962,"is based on Fisher information, measuring how much information the exam provides for estimating
271"
RELIABILITY OF IRT SCORES FOR LLMS,0.153921568627451,"the θs in a certain range. Finally, we use the discrimination index which evaluates the capacity of
272"
RELIABILITY OF IRT SCORES FOR LLMS,0.15441176470588236,"questions to accurately distinguish between high and low performing test takers.
273"
RELIABILITY OF IRT SCORES FOR LLMS,0.15490196078431373,"Goodness-of-fit: We use the lz score (see Section 3) assess whether the test taker is behaving in
274"
RELIABILITY OF IRT SCORES FOR LLMS,0.1553921568627451,"a manner consistent with the model. Alternatively, we ask what is the appropriateness of a test-
275"
RELIABILITY OF IRT SCORES FOR LLMS,0.15588235294117647,"taker’s estimated ˆθ as a measure of the test taker’s true θ? For example, imagine that an LLM has
276"
RELIABILITY OF IRT SCORES FOR LLMS,0.15637254901960784,"a response pattern of missing easy questions and correctly answering more difficult ones. Such a
277"
RELIABILITY OF IRT SCORES FOR LLMS,0.1568627450980392,"pattern may arise because the LLM was lucky on the hard questions, or it may arise because the
278"
RELIABILITY OF IRT SCORES FOR LLMS,0.15735294117647058,"LLM had access to memorized patterns that assisted in answering the hard questions. Generally,
279"
RELIABILITY OF IRT SCORES FOR LLMS,0.15784313725490196,"low lz scores suggest that the θ estimate of the model is less reliable [12].
280"
RELIABILITY OF IRT SCORES FOR LLMS,0.15833333333333333,"In Figure 3 we show lz scores plotted against θ scores of LLMs across the four exams in 2023 (2022
281"
RELIABILITY OF IRT SCORES FOR LLMS,0.1588235294117647,"is shown in Appendix A.9). As in previous plots, the light blue points in the background show the
282"
RELIABILITY OF IRT SCORES FOR LLMS,0.15931372549019607,"distribution of the same two scores for the human test takers. Starting again with the Math exam, we
283"
RELIABILITY OF IRT SCORES FOR LLMS,0.15980392156862744,"note that lz values are low, but now we can see that the response patterns of the LLMs are indeed
284"
RELIABILITY OF IRT SCORES FOR LLMS,0.16029411764705884,"quite human-like; LLMs behave like humans with similarly low lz values. One possible reason for
285"
RELIABILITY OF IRT SCORES FOR LLMS,0.1607843137254902,"this behavior is that the Mathematics exam tends to be the harder exam of ENEM, leading to more
286"
RELIABILITY OF IRT SCORES FOR LLMS,0.16127450980392158,"guessing, which may make the human lz values for Mathematics smaller.
287"
RELIABILITY OF IRT SCORES FOR LLMS,0.16176470588235295,"For the Languages exam, models perform better in general (higher θ values) and the most lz scores
288"
RELIABILITY OF IRT SCORES FOR LLMS,0.16225490196078432,"being close to 0 (and with a similar spread as the human distribution of lz’s) suggest that these θ
289"
RELIABILITY OF IRT SCORES FOR LLMS,0.1627450980392157,"estimates are reliable – the models are showing human-like response patterns.
290"
RELIABILITY OF IRT SCORES FOR LLMS,0.16323529411764706,"6
4
2
0
2
4"
RELIABILITY OF IRT SCORES FOR LLMS,0.16372549019607843,lz score 2 1 0 1 2 3 4
RELIABILITY OF IRT SCORES FOR LLMS,0.1642156862745098,IRT score
LANGUAGES AND CODES,0.16470588235294117,2023 Languages and Codes
LANGUAGES AND CODES,0.16519607843137254,"6
4
2
0
2
4"
LANGUAGES AND CODES,0.16568627450980392,lz score
HUMANITIES,0.1661764705882353,2023 Humanities
HUMANITIES,0.16666666666666666,"6
4
2
0
2
4"
HUMANITIES,0.16715686274509803,lz score
NATURAL SCIENCES,0.1676470588235294,2023 Natural Sciences
NATURAL SCIENCES,0.1681372549019608,"6
4
2
0
2
4"
NATURAL SCIENCES,0.16862745098039217,lz score
MATHEMATICS,0.16911764705882354,2023 Mathematics
MATHEMATICS,0.1696078431372549,"MODEL
ChatGPT-3.5
Gemma-7B
LLaMA2-13B
LLaMA2-7B
LLaMA3-8B
Mistral-7B
LANGUAGE
EN
PT-BR"
MATHEMATICS,0.17009803921568628,"Figure 3: Distribution of lz and IRT scores for humans and LLMs. LLMs are non-instructed tuned
open source models and GPT3.5 with 4-shot. LLM datapoints are computed from different shuffles."
MATHEMATICS,0.17058823529411765,"The results become more nuanced as we look at the Natural Sciences exam. For this exam, most
291"
MATHEMATICS,0.17107843137254902,"models, including the high performing ones (i.e., GPT-3.5 and Gemma-7B), show values well out-
292"
MATHEMATICS,0.1715686274509804,"side the human distribution, with a long tail in the negative values of lz. Comparing the GPT-3.5 and
293"
MATHEMATICS,0.17205882352941176,"Gemma-7B results in Figures 1 and 4, we can infer that the high accuracy (CTT scores) achieved by
294"
MATHEMATICS,0.17254901960784313,"these models on the Natural Sciences exam are quite misleading; although GPT-3.5 and Gemma-7B
295"
MATHEMATICS,0.1730392156862745,"answer many questions correctly, their response pattern is very unlikely, with very low lz values.
296"
MATHEMATICS,0.17352941176470588,"This corroborates with Figure 2, which shows an interchange of blocks of correct and incorrect
297"
MATHEMATICS,0.17401960784313725,"answers from the models, creating an unlikely response pattern.
298"
MATHEMATICS,0.17450980392156862,"In Humanities, almost all LLMs perform reasonably well, achieving θ scores above zero (the average
299"
MATHEMATICS,0.175,"human level). However, Llama2-7B, while obtaining above average accuracy scores (Figure 1) and
300"
MATHEMATICS,0.17549019607843136,"good θ scores, has low average lz scores. This suggests that the IRT scores Llama2-7B may be not
301"
MATHEMATICS,0.17598039215686276,"reliable. Examination of the corresponding rows in Figure 2 shows that this is the only model that
302"
MATHEMATICS,0.17647058823529413,"does not have a consistent response pattern across shuffles, leading to the observed low lz score.
303"
MATHEMATICS,0.1769607843137255,"0
10
20
30
40 ( )"
LANGUAGES AND CODES,0.17745098039215687,"2023 Languages and Codes
2023 Humanities
2023 Natural Sciences
2023 Mathematics"
LANGUAGES AND CODES,0.17794117647058824,"-2 -1
0
1
2
3
4
5"
LANGUAGES AND CODES,0.1784313725490196,IRT score ( )
LANGUAGES AND CODES,0.17892156862745098,ChatGPT-3.5
LANGUAGES AND CODES,0.17941176470588235,Gemma-7B
LANGUAGES AND CODES,0.17990196078431372,LLaMA2-13B
LANGUAGES AND CODES,0.1803921568627451,LLaMA2-7B
LANGUAGES AND CODES,0.18088235294117647,LLaMA3-8B
LANGUAGES AND CODES,0.18137254901960784,Mistral-7B
LANGUAGES AND CODES,0.1818627450980392,"-2 -1
0
1
2
3
4
5"
LANGUAGES AND CODES,0.18235294117647058,IRT score ( )
LANGUAGES AND CODES,0.18284313725490195,"-2 -1
0
1
2
3
4
5"
LANGUAGES AND CODES,0.18333333333333332,IRT score ( )
LANGUAGES AND CODES,0.18382352941176472,"-2 -1
0
1
2
3
4
5"
LANGUAGES AND CODES,0.1843137254901961,IRT score ( )
LANGUAGES AND CODES,0.18480392156862746,"EN
PT-BR"
LANGUAGES AND CODES,0.18529411764705883,"Figure 4: Total Fisher information of the exams and the IRT scores (95% Confidence Interval (CI))
for LLMs. LLM datapoints are computed from different shuffles."
LANGUAGES AND CODES,0.1857843137254902,"Fisher Information: We investigate further whether the ENEM exams are giving us accurate esti-
304"
LANGUAGES AND CODES,0.18627450980392157,"mates of the LLMs ability levels from another standpoint – that of Fisher Information (see Section 3,
305"
LANGUAGES AND CODES,0.18676470588235294,"Equation (3)). Intuitively, Fisher Information quantifies whether there was enough information in
306"
LANGUAGES AND CODES,0.18725490196078431,"the test to infer the ability level of a test taker at a certain ability level. Figure 4 shows, for every
307"
LANGUAGES AND CODES,0.18774509803921569,"ENEM exam, the total Fisher Information I(θ) on the top plot, and the θ scores for the models (95%
308"
LANGUAGES AND CODES,0.18823529411764706,"Confidence Interval (CI) computed using the shuffles) on the bottom plot. This plot reinforces the
309"
LANGUAGES AND CODES,0.18872549019607843,"observation that for some models in Natural Sciences and for all models in Mathematics, the mod-
310"
LANGUAGES AND CODES,0.1892156862745098,"els’ θ are not in the range of the exam with highest information – the models ability levels fall in the
311"
LANGUAGES AND CODES,0.18970588235294117,"tail of the Fisher Information histogram. Hence, the Math exam is not useful for making meaningful
312"
LANGUAGES AND CODES,0.19019607843137254,"measurements of these LLMs, casting doubt on the informativeness of the models’ θ scores on this
313"
LANGUAGES AND CODES,0.1906862745098039,"exam. The lack of discrimination ability of this exam is reflected by the responses for many models
314"
LANGUAGES AND CODES,0.19117647058823528,"showing apparently random response patterns in the corresponding heatmap (see Figure 2).
315"
LANGUAGES AND CODES,0.19166666666666668,"0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0"
LANGUAGES AND CODES,0.19215686274509805,Discrimination Index 0 2 4 6
LANGUAGES AND CODES,0.19264705882352942,Density
LANGUAGES AND CODES,0.1931372549019608,Humans (2023)
LANGUAGES AND CODES,0.19362745098039216,"0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0"
LANGUAGES AND CODES,0.19411764705882353,Discrimination Index
LANGUAGES AND CODES,0.1946078431372549,LLMs (2023)
LANGUAGES AND CODES,0.19509803921568628,"Humanities
Languages and Codes"
LANGUAGES AND CODES,0.19558823529411765,"Mathematics
Natural Sciences"
LANGUAGES AND CODES,0.19607843137254902,Figure 5: Discrimination Indices for questions in the 2023 exam for both Humans and LLMs.
LANGUAGES AND CODES,0.1965686274509804,"Discrimination Index: To further assess the reliability of the IRT scores, we also turn into psycho-
316"
LANGUAGES AND CODES,0.19705882352941176,"metrics and use the notion of the item discrimination index (DI), which measures how well an item
317"
LANGUAGES AND CODES,0.19754901960784313,"on a test distinguishes between high and low scorers on the entire test [9]. Let Ph (resp. Pl) be the
318"
LANGUAGES AND CODES,0.1980392156862745,"proportion of the top 25% (resp. low 25%) LLMs (in terms of θ, including the shuffles) that correctly
319"
LANGUAGES AND CODES,0.19852941176470587,"answer the item; then DI = Ph −Pl, the difference of the two proportions. DI ranges from -1 to 1,
320"
LANGUAGES AND CODES,0.19901960784313724,"and questions with DI higher than 0.2 are considered good, while lower DI indicates flaws [50].
321"
LANGUAGES AND CODES,0.19950980392156864,"Figure 5 shows the distribution of the discrimination indices computed for humans and LLMs for the
322"
LANGUAGES AND CODES,0.2,"2023 exam. Overall, we notice that discrimination indices computed for LLMs are more negative
323"
LANGUAGES AND CODES,0.20049019607843138,"compared to those of humans. We also observe that a significant fraction of Math questions have low
324"
LANGUAGES AND CODES,0.20098039215686275,"discriminative power, reinforcing the hypothesis that this exam is not well designed to measure Math
325"
LANGUAGES AND CODES,0.20147058823529412,"abilities for LLMs. Nonetheless, the Humanities and Languages have several questions with very
326"
LANGUAGES AND CODES,0.2019607843137255,"good discriminative power. Interestingly, the Natural Sciences exam appears to follow a bimodal
327"
LANGUAGES AND CODES,0.20245098039215687,"distribution, containing both informative and poorly-designed questions. This may be a reflection of
328"
LANGUAGES AND CODES,0.20294117647058824,"the fact that the Natural Sciences exam is a hybrid test, containing a mix of knowledge-based items
329"
LANGUAGES AND CODES,0.2034313725490196,"and items that demand more complex reasoning over numbers and images, which can be less useful
330"
LANGUAGES AND CODES,0.20392156862745098,"for evaluating the current state-of-the-art LLMs.
331"
LANGUAGES AND CODES,0.20441176470588235,"Attributes affecting reliability of IRT scores: In a further investigation, shown in Appendix A.2,
332"
LANGUAGES AND CODES,0.20490196078431372,"we explore potential causes of low discrimination. We investigate item attributes such as the ex-
333"
LANGUAGES AND CODES,0.2053921568627451,"istence of images or numbers in the questions as we believe that these attributes impede LLMs
334"
LANGUAGES AND CODES,0.20588235294117646,"from understanding the question properly. Our preliminary results suggest that LLMs’ ability to
335"
LANGUAGES AND CODES,0.20637254901960783,"understand math questions and parse images is sub-par compared to their capacity in answering
336"
LANGUAGES AND CODES,0.2068627450980392,"pure text-based questions. In Appendix A.10 we show examples of non-discriminating and highly
337"
LANGUAGES AND CODES,0.2073529411764706,"discriminating items for the 2023 Natural Sciences exam. In Appendix A.3, we reach a similar
338"
LANGUAGES AND CODES,0.20784313725490197,"conclusion by looking at model accuracy against model perplexity, a model intrinsic metric.
339"
CONCLUSIONS,0.20833333333333334,"6
Conclusions
340"
CONCLUSIONS,0.2088235294117647,"The ongoing debate in LLM evaluation centers around whether exams designed for humans are
341"
CONCLUSIONS,0.20931372549019608,"appropriate tools for measuring the performance of LLMs. In this paper, we provide a case study
342"
CONCLUSIONS,0.20980392156862746,"that illustrates methods that can be used to address this question, as well as specific results for a
343"
CONCLUSIONS,0.21029411764705883,"range of current LLMs. We leverage the largest known human exam for which a public IRT model
344"
CONCLUSIONS,0.2107843137254902,"is available, and show that IRT can be leveraged to distinguish between human-like and non-human-
345"
CONCLUSIONS,0.21127450980392157,"like responses under the model. We show cases where LLMs respond in non-human-like ways and
346"
CONCLUSIONS,0.21176470588235294,"show how to identify those cases using a model-fit metric. Further, we show that using IRT we
347"
CONCLUSIONS,0.2122549019607843,"can determine when an exam is capable of making meaningful measurement of an LLM’s ability
348"
CONCLUSIONS,0.21274509803921568,"in a given subject area. Using our evaluation framework, we find that the ENEM Math exam is not
349"
CONCLUSIONS,0.21323529411764705,"appropriate to make meaningful measurements of the models’ ability, for the LLMs we study. At
350"
CONCLUSIONS,0.21372549019607842,"the same time, Humanities and Language exams are better suited for evaluating the LLMs’ abilities
351"
CONCLUSIONS,0.2142156862745098,"on those subjects. We conclude that IRT modeling, drawing on a long history of psychometric
352"
CONCLUSIONS,0.21470588235294116,"theory, provides a set of crucial tools for assessing whether exams designed for humans are actually
353"
CONCLUSIONS,0.21519607843137256,"meaningful measures of LLM ability. Our results suggest that they should be used in future studies
354"
CONCLUSIONS,0.21568627450980393,"when questions are raised regarding the performance of LLMs on human exams.
355"
REFERENCES,0.2161764705882353,"References
356"
REFERENCES,0.21666666666666667,"[1]
Josh Achiam et al. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774.
357"
REFERENCES,0.21715686274509804,"[2]
M.J. Allen and W. M. Yen. 2002. Introduction to Measurement Theory. Waveland Press.
358"
REFERENCES,0.21764705882352942,"[3]
Thales Sales Almeida, Thiago Laitz, Giovana K Bon´as, and Rodrigo Nogueira. 2023. Bluex: a bench-
359"
REFERENCES,0.2181372549019608,"mark based on brazilian leading universities entrance exams. In Brazilian Conference on Intelligent
360"
REFERENCES,0.21862745098039216,"Systems. Springer, 337–347.
361"
REFERENCES,0.21911764705882353,"[4]
BIG-bench authors. 2023. Beyond the imitation game: quantifying and extrapolating the capabilities of
362"
REFERENCES,0.2196078431372549,"language models. Transactions on Machine Learning Research. https://openreview.net/forum?i
363"
REFERENCES,0.22009803921568627,"d=uyTL5Bvosj.
364"
REFERENCES,0.22058823529411764,"[5]
F. B. Baker and S.-H. Kim. 2004. Item Response Theory: Parameter Estimation Techniques. Marcel
365"
REFERENCES,0.221078431372549,"Dekkerm, Inc.
366"
REFERENCES,0.22156862745098038,"[6]
R Darrell Bock and Murray Aitkin. 1981. Marginal maximum likelihood estimation of item parameters:
367"
REFERENCES,0.22205882352941175,"application of an em algorithm. Psychometrika, 46, 4, 443–459.
368"
REFERENCES,0.22254901960784312,"[7]
R Darrell Bock, Robert Gibbons, and Eiji Muraki. 1988. Full-information item factor analysis. Applied
369"
REFERENCES,0.22303921568627452,"psychological measurement, 12, 3, 261–280.
370"
REFERENCES,0.2235294117647059,"[8]
S´ebastien Bubeck et al. 2023. Sparks of artificial general intelligence: early experiments with gpt-4.
371"
REFERENCES,0.22401960784313726,"arXiv preprint arXiv:2303.12712.
372"
REFERENCES,0.22450980392156863,"[9]
Man Ching Esther Chan. 2015. Young learners: an examination of the psychometric properties of the
373"
REFERENCES,0.225,"early literacy knowledge and skills instrument. Journal of Psychoeducational Assessment, 33, 7, 607–
374"
REFERENCES,0.22549019607843138,"621.
375"
REFERENCES,0.22598039215686275,"[10]
Yunxiao Chen, Xiaoou Li, Jingchen Liu, and Zhiliang Ying. 2021. Item response theory – a statistical
376"
REFERENCES,0.22647058823529412,"framework for educational and psychological measurement. (2021). arXiv: 2108.08604 [stat.ME].
377"
REFERENCES,0.2269607843137255,"[11]
Karl Cobbe et al. 2021. Training verifiers to solve math word problems. arXiv preprint
378"
REFERENCES,0.22745098039215686,"arXiv:2110.14168.
379"
REFERENCES,0.22794117647058823,"[12]
Rafael Jaime De Ayala. 2013. The theory and practice of item response theory. Guilford Publications.
380"
REFERENCES,0.2284313725490196,"[13]
Iddo Drori et al. 2022. A neural network solves, explains, and generates university math problems by pro-
381"
REFERENCES,0.22892156862745097,"gram synthesis and few-shot learning at human level. Proceedings of the National Academy of Sciences,
382"
REFERENCES,0.22941176470588234,"119, 32, e2123433119. eprint: https://www.pnas.org/doi/pdf/10.1073/pnas.2123433119.
383"
REFERENCES,0.2299019607843137,"DOI: 10.1073/pnas.2123433119.
384"
REFERENCES,0.23039215686274508,"[14]
Iddo Drori et al. 2023. From human days to machine seconds: automatically answering and generating
385"
REFERENCES,0.23088235294117648,"machine learning final exams. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
386"
REFERENCES,0.23137254901960785,"Discovery and Data Mining (KDD ’23). Association for Computing Machinery, New York, NY, USA,
387"
REFERENCES,0.23186274509803922,"3947–3955. ISBN: 9798400701030. DOI: 10.1145/3580305.3599827.
388"
REFERENCES,0.2323529411764706,"[15]
Mehtap Erguven. 2013. Two approaches to psychometric process: classical test theory and item response
389"
REFERENCES,0.23284313725490197,"theory. Journal of Education, 2, 2, 23–30.
390"
REFERENCES,0.23333333333333334,"[16]
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob
391"
REFERENCES,0.2338235294117647,"Steinhardt. 2020. Measuring massive multitask language understanding. In International Conference
392"
REFERENCES,0.23431372549019608,"on Learning Representations.
393"
REFERENCES,0.23480392156862745,"[17]
Instituto Nacional de Estudos e Pesquisas Educacionais An´ısio Teixeira. 2024. Microdados do Enem
394"
REFERENCES,0.23529411764705882,"2023. INEP. https://www.gov.br/inep/pt-br/acesso-a-informacao/dados-abertos/micr
395"
REFERENCES,0.2357843137254902,"odados/enem.
396"
REFERENCES,0.23627450980392156,"[18]
Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: a large scale distantly
397"
REFERENCES,0.23676470588235293,"supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of
398"
REFERENCES,0.2372549019607843,"the Association for Computational Linguistics. Association for Computational Linguistics, Vancouver,
399"
REFERENCES,0.23774509803921567,"Canada, (July 2017).
400"
REFERENCES,0.23823529411764705,"[19]
Douwe Kiela et al. 2021. Dynabench: rethinking benchmarking in nlp. In Proceedings of the 2021 Con-
401"
REFERENCES,0.23872549019607844,"ference of the North American Chapter of the Association for Computational Linguistics: Human Lan-
402"
REFERENCES,0.23921568627450981,"guage Technologies, 4110–4124.
403"
REFERENCES,0.23970588235294119,"[20]
Adrienne Kline, Theresa Kline, and Joon Lee. 2021. Item response theory as a feature selection and
404"
REFERENCES,0.24019607843137256,"interpretation tool in the context of machine learning. Medical & Biological Engineering & Computing,
405"
REFERENCES,0.24068627450980393,"59, (Jan. 2021). DOI: 10.1007/s11517-020-02301-x.
406"
REFERENCES,0.2411764705882353,"[21]
TH Kung et al. 2023. Performance of chatgpt on usmle: potential for ai-assisted medical education using
407"
REFERENCES,0.24166666666666667,"large language models. (2023).
408"
REFERENCES,0.24215686274509804,"[22]
John P Lalor, Hao Wu, and Hong Yu. 2016. Building an evaluation scale using item response theory. In
409"
REFERENCES,0.2426470588235294,"Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on
410"
REFERENCES,0.24313725490196078,"Empirical Methods in Natural Language Processing. Vol. 2016. NIH Public Access, 648.
411"
REFERENCES,0.24362745098039215,"[23]
Rob R Meijer. 1996. Person-fit research: an introduction. Applied Measurement in Education, 9, 1, 3–8.
412"
REFERENCES,0.24411764705882352,"[24]
Samuel Messick. 1995. Standards of validity and the validity of standards in performance asessment.
413"
REFERENCES,0.2446078431372549,"Educational measurement: Issues and practice, 14, 4, 5–8.
414"
REFERENCES,0.24509803921568626,"[25]
Arvind Narayanan and Sayash Kapoor. 2023. Gpt-4 and professional benchmarks: the wrong answer to
415"
REFERENCES,0.24558823529411763,"the wrong question. (2023). https://www.aisnakeoil.com/p/gpt-4-and-professional-benc
416"
REFERENCES,0.246078431372549,"hmarks.
417"
REFERENCES,0.2465686274509804,"[26]
Desnes Nunes, Ricardo Primi, Ramon Pires, Roberto Lotufo, and Rodrigo Nogueira. 2023. Evaluat-
418"
REFERENCES,0.24705882352941178,"ing gpt-3.5 and gpt-4 models on brazilian university admission exams. (2023). arXiv: 2303.17003
419"
REFERENCES,0.24754901960784315,"[cs.CL].
420"
REFERENCES,0.24803921568627452,"[27]
OpenAI. 2023. Gpt-4 technical report. (2023). arXiv: 2303.08774 [cs.CL].
421"
REFERENCES,0.2485294117647059,"[28]
Pouya Pezeshkpour and Estevam Hruschka. 2023. Large language models sensitivity to the order of
422"
REFERENCES,0.24901960784313726,"options in multiple-choice questions. (2023). arXiv: 2308.11483 [cs.CL].
423"
REFERENCES,0.24950980392156863,"[29]
Fernando Plumed, Ricardo Prudˆencio, Adolfo Mart´ınez-Us´o, and Jose Hernandez-Orallo. 2016. Making
424"
REFERENCES,0.25,"sense of item response theory in machine learning. In (Sept. 2016). DOI: 10.3233/978-1-61499-672
425"
REFERENCES,0.25049019607843137,"-9-1140.
426"
REFERENCES,0.25098039215686274,"[30]
Deborah Raji, Emily Denton, Emily M. Bender, Alex Hanna, and Amandalynne Paullada. 2021. Ai and
427"
REFERENCES,0.2514705882352941,"the everything in the whole wide world benchmark. In Proceedings of the Neural Information Processing
428"
REFERENCES,0.2519607843137255,"Systems Track on Datasets and Benchmarks. J. Vanschoren and S. Yeung, (Eds.) Vol. 1. Curran. https
429"
REFERENCES,0.25245098039215685,"://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/084b
430"
REFERENCES,0.2529411764705882,"6fbb10729ed4da8c3d3f5a3ae7c9-Paper-round2.pdf.
431"
REFERENCES,0.2534313725490196,"[31]
Leonardo Ranaldi and Giulia Pucci. 2023. Does the english matter? elicit cross-lingual abilities of large
432"
REFERENCES,0.25392156862745097,"language models. In Proceedings of the 3rd Workshop on Multi-lingual Representation Learning (MRL),
433"
REFERENCES,0.25441176470588234,"173–183.
434"
REFERENCES,0.2549019607843137,"[32]
Joshua Robinson and David Wingate. 2023. Leveraging large language models for multiple choice ques-
435"
REFERENCES,0.2553921568627451,"tion answering. In The Eleventh International Conference on Learning Representations. https://ope
436"
REFERENCES,0.25588235294117645,"nreview.net/forum?id=yKbprarjc5B.
437"
REFERENCES,0.2563725490196078,"[33]
Pedro Rodriguez, Joe Barrow, Alexander Miserlis Hoyle, John P. Lalor, Robin Jia, and Jordan Boyd-
438"
REFERENCES,0.2568627450980392,"Graber. 2021. Evaluation examples are not equally informative: how should that change NLP leader-
439"
REFERENCES,0.25735294117647056,"boards? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics
440"
REFERENCES,0.257843137254902,"and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).
441"
REFERENCES,0.25833333333333336,"Chengqing Zong, Fei xia, Wenjie Li, and Roberto Navigli, (Eds.) Association for Computational Lin-
442"
REFERENCES,0.25882352941176473,"guistics, Online, (Aug. 2021), 4486–4503. DOI: 10.18653/v1/2021.acl-long.346.
443"
REFERENCES,0.2593137254901961,"[34]
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. 2023. Are emergent abilities of large language
444"
REFERENCES,0.25980392156862747,"models a mirage? In Advances in Neural Information Processing Systems. A. Oh, T. Naumann, A.
445"
REFERENCES,0.26029411764705884,"Globerson, K. Saenko, M. Hardt, and S. Levine, (Eds.) Vol. 36. Curran Associates, Inc., 55565–55581.
446"
REFERENCES,0.2607843137254902,"https://proceedings.neurips.cc/paper_files/paper/2023/file/adc98a266f45005c403
447"
REFERENCES,0.2612745098039216,"b8311ca7e8bd7-Paper-Conference.pdf.
448"
REFERENCES,0.26176470588235295,"[35]
Jo˜ao Sedoc and Lyle Ungar. 2020. Item response theory for efficient human evaluation of chatbots. In
449"
REFERENCES,0.2622549019607843,"Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems. Steffen Eger, Yang
450"
REFERENCES,0.2627450980392157,"Gao, Maxime Peyrard, Wei Zhao, and Eduard Hovy, (Eds.) Association for Computational Linguistics,
451"
REFERENCES,0.26323529411764707,"Online, (Nov. 2020), 21–33. DOI: 10.18653/v1/2020.eval4nlp-1.3.
452"
REFERENCES,0.26372549019607844,"[36]
Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen,
453"
REFERENCES,0.2642156862745098,"and Luke Zettlemoyer. 2023. Detecting pretraining data from large language models. In The Twelfth
454"
REFERENCES,0.2647058823529412,"International Conference on Learning Representations.
455"
REFERENCES,0.26519607843137255,"[37]
Bruno Silva, Leonardo Nunes, Roberto Estev˜ao, and Ranveer Chandra. 2023. Gpt-4 as an agronomist
456"
REFERENCES,0.2656862745098039,"assistant? answering agriculture exams using large language models. arXiv preprint arXiv:2310.06225.
457"
REFERENCES,0.2661764705882353,"[38]
Igor Cataneo Silveira and Denis Deratani Mau´a. 2017. University entrance exam as a guiding test for
458"
REFERENCES,0.26666666666666666,"artificial intelligence. In 2017 Brazilian Conference on Intelligent Systems (BRACIS), 426–431. DOI:
459"
REFERENCES,0.26715686274509803,"10.1109/BRACIS.2017.44.
460"
REFERENCES,0.2676470588235294,"[39]
Igor Cataneo Silveira and Denis Deratani Mau´a. 2018. Advances in automatically solving the enem. In
461"
REFERENCES,0.2681372549019608,"2018 7th Brazilian Conference on Intelligent Systems (BRACIS). IEEE, 43–48.
462"
REFERENCES,0.26862745098039215,"[40]
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: a ques-
463"
REFERENCES,0.2691176470588235,"tion answering challenge targeting commonsense knowledge. ArXiv, abs/1811.00937. https://api.s
464"
REFERENCES,0.2696078431372549,"emanticscholar.org/CorpusID:53296520.
465"
REFERENCES,0.27009803921568626,"[41]
Christian Terwiesch. 2023. Would Chat GPT3 Get a Wharton MBA? A Prediction Based on Its Perfor-
466"
REFERENCES,0.27058823529411763,"mance in the Operations Management Course. Tech. rep. Mack Institute for Innovation Management at
467"
REFERENCES,0.271078431372549,"the Wharton School, University of Pennsylvania.
468"
REFERENCES,0.27156862745098037,"[42]
Lindia Tjuatja, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar, and Graham Neubig. 2024. Do
469"
REFERENCES,0.27205882352941174,"llms exhibit human-like response biases? a case study in survey design. (2024). arXiv: 2311.04076
470"
REFERENCES,0.2725490196078431,"[cs.CL].
471"
REFERENCES,0.2730392156862745,"[43]
Hugo Touvron et al. 2023. Llama 2: open foundation and fine-tuned chat models. arXiv preprint
472"
REFERENCES,0.2735294117647059,"arXiv:2307.09288.
473"
REFERENCES,0.2740196078431373,"[44]
Sean Trott. 2024. Can large language models help augment english psycholinguistic datasets? Behavior
474"
REFERENCES,0.27450980392156865,"Research Methods, 1–19.
475"
REFERENCES,0.275,"[45]
Lakshmi Varanasi. 2023. Gpt-4 can ace the bar, but it only has a decent chance of passing the cfa exams.
476"
REFERENCES,0.2754901960784314,"here’s a list of difficult exams the chatgpt and gpt-4 have passed. (2023). https://www.businessins
477"
REFERENCES,0.27598039215686276,"ider.com/list-here-are-the-exams-chatgpt-has-passed-so-far-2023-1.
478"
REFERENCES,0.27647058823529413,"[46]
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. Glue:
479"
REFERENCES,0.2769607843137255,"a multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the
480"
REFERENCES,0.2774509803921569,"2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 353–355.
481"
REFERENCES,0.27794117647058825,"[47]
Xiaoxuan Wang et al. 2023. Scibench: evaluating college-level scientific problem-solving abilities of
482"
REFERENCES,0.2784313725490196,"large language models. arXiv preprint arXiv:2307.10635.
483"
REFERENCES,0.278921568627451,"[48]
Xinpeng Wang, Bolei Ma, Chengzhi Hu, Leon Weber-Genzel, Paul R¨ottger, Frauke Kreuter, Dirk Hovy,
484"
REFERENCES,0.27941176470588236,"and Barbara Plank. 2024. ” my answer is c”: first-token probabilities do not match text answers in
485"
REFERENCES,0.27990196078431373,"instruction-tuned language models. arXiv preprint arXiv:2402.14499.
486"
REFERENCES,0.2803921568627451,"[49]
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei xia, Ed Chi, Quoc V Le, Denny Zhou,
487"
REFERENCES,0.28088235294117647,"et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural
488"
REFERENCES,0.28137254901960784,"information processing systems, 35, 24824–24837.
489"
REFERENCES,0.2818627450980392,"[50]
Margaret Wu and Ray Adams. 2007. Applying the Rasch model to psycho-social measurement: A prac-
490"
REFERENCES,0.2823529411764706,"tical approach. Educational Measurement Solutions Melbourne.
491"
REFERENCES,0.28284313725490196,"[51]
Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and Pengfei Liu. 2024. Evaluating mathematical
492"
REFERENCES,0.2833333333333333,"reasoning beyond accuracy. arXiv preprint arXiv:2404.05692.
493"
REFERENCES,0.2838235294117647,"[52]
Longhui Yu et al. 2023. Metamath: bootstrap your own mathematical questions for large language mod-
494"
REFERENCES,0.28431372549019607,"els. arXiv preprint arXiv:2309.12284.
495"
REFERENCES,0.28480392156862744,"[53]
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu
496"
REFERENCES,0.2852941176470588,"Chen, and Nan Duan. 2023. Agieval: a human-centric benchmark for evaluating foundation models.
497"
REFERENCES,0.2857843137254902,"arXiv preprint arXiv:2304.06364.
498"
REFERENCES,0.28627450980392155,"[54]
Yan Zhuang et al. 2023. Efficiently measuring the cognitive ability of llms: an adaptive testing perspec-
499"
REFERENCES,0.2867647058823529,"tive. arXiv preprint arXiv:2306.10512.
500"
REFERENCES,0.2872549019607843,"language
subject
Accuracy (CTT)
θ
lz
en
humanities
29.5 ± 10.7
-0.57 ± 0.56
-1.25 ± 1.18
en
languages
24.7 ± 8.6
-0.99 ± 0.49
-0.39 ± 1.03
en
science
25.5 ± 8.2
-0.34 ± 0.53
-0.74 ± 1.25
en
math
22 ± 6.3
-0.6 ± 0.34
-0.66 ± 0.97"
REFERENCES,0.28774509803921566,"pt-br
humanities
24 ± 7
-0.83 ± 0.38
-0.83 ± 1.08
pt-br
languages
23.1 ± 7.1
-1.06 ± 0.42
-0.32 ± 1.01
pt-br
science
23.5 ± 7.3
-0.48 ± 0.41
-0.5 ± 1.18
pt-br
math
23.2 ± 6.4
-0.55 ± 0.4
-0.86 ± 1.05"
REFERENCES,0.28823529411764703,"Table 1: Random choice selection performance on English and Portuguese versions of 2022 test 4
subjects."
REFERENCES,0.2887254901960784,"A
Supplemental Material
501"
REFERENCES,0.28921568627450983,"A.1
Manual auditing of exam questions
502"
REFERENCES,0.2897058823529412,"Assuming the original questions written by the ENEM authorities are good test instruments for
503"
REFERENCES,0.2901960784313726,"testing student capability, we focus on ensuring the quality of adapted dataset for LLM evaluation.
504"
REFERENCES,0.29068627450980394,"We manually correct the artifacts for each question in 2022 and 2023. In the next sections, we
505"
REFERENCES,0.2911764705882353,"describe the artifacts from those easier to address (sec A.1.2 A.1.3), to deeper-rooted problems (i.e.,
506"
REFERENCES,0.2916666666666667,"harder to correct, sec A.1.4), as well as how we addressed them manually (sec A.1.5).
507"
REFERENCES,0.29215686274509806,"A.1.1
Label accuracy
508"
REFERENCES,0.2926470588235294,"We assume answers are correct as translation and parsing of single characters can be quite reliable,
509"
REFERENCES,0.2931372549019608,"and that the original ENEM test is tested across millions of human test takers and will be discarded
510"
REFERENCES,0.29362745098039217,"if it had a wrong answer. When we look at the label distribution for 2022, options “ABCDE” each
511"
REFERENCES,0.29411764705882354,"occur 39/39/37/36/33 times, making it fairly balanced. We also ran random baselines on the same
512"
REFERENCES,0.2946078431372549,"option shuffles as the model (Table 1).
513"
REFERENCES,0.2950980392156863,"A.1.2
Translation artifacts
514"
REFERENCES,0.29558823529411765,"We found several issues pertaining to initial round of translation in this dataset. Mainly, independent
515"
REFERENCES,0.296078431372549,"translation of question context and answer option leads to incoherence. Details are sometimes mis-
516"
REFERENCES,0.2965686274509804,"translated (“p.d.d” translated to “d.d.p”). There are many non-standardized translations pertaining
517"
REFERENCES,0.29705882352941176,"to chemical formulas, proper nouns, and mathematical formulas. In general, there are significant
518"
REFERENCES,0.29754901960784313,"amount of awkward phrasing, incomplete translation, and linguistic idiosyncrasies lost in transla-
519"
REFERENCES,0.2980392156862745,"tion.
520"
REFERENCES,0.2985294117647059,"Independent translation of context and question
In a few cases, the answer options are expected
521"
REFERENCES,0.29901960784313725,"to complete the last sentence of the question. After translation, options do not all fit as completions of
522"
REFERENCES,0.2995098039215686,"the sentence (Q11). Translation without context also leads to improper translation of polysemantic
523"
REFERENCES,0.3,"terms. “Coagulation” maybe translated correctly in the question, but becomes “coagulating” as
524"
REFERENCES,0.30049019607843136,"a stand-alone word (Q96). “Good” and “fair” (when used as survey options) gets translated to
525"
REFERENCES,0.30098039215686273,"“regular” and “I will” as stand-alone options (Q171)
526"
REFERENCES,0.3014705882352941,"Inconsistent translation details
Within the same questions, there are cases where the same con-
527"
REFERENCES,0.30196078431372547,"cept is translated differently. In one question, the context introduces the concept “potential difference
528"
REFERENCES,0.30245098039215684,"(p.d.d)”, and later referred to it as “d.d.p” and “d.p.d”. Within different options, the same unit can
529"
REFERENCES,0.3029411764705882,"sometimes be plural and sometimes be singular (when it should be consistently plural)
530"
REFERENCES,0.3034313725490196,"Non-standard translation
1) Chemical formula translation is non-standard. “N2O3” becomes “N
531"
REFERENCES,0.30392156862745096,"2O3”, and “NH4+” becomes “NH4 positively charged”. 2) (Proper) nouns are sometimes capital-
532"
REFERENCES,0.3044117647058823,"ized when they shouldn’t. For instance, one question begins with the sentence “On the Gravitational
533"
REFERENCES,0.30490196078431375,"Field of a Mass Point According to Einstein’s Theory A ’Black Hole is a...” 3) Mathematical equa-
534"
REFERENCES,0.3053921568627451,"tions are overly verbatim. This we suspect is partially due to an issue with using audio version of the
535"
REFERENCES,0.3058823529411765,"test. For example, if an option is the formula 9(
8!
(8−2)!2! −1), its Portuguese representation would
536"
REFERENCES,0.30637254901960786,"be “9 vezes ( (8 fatorial dividido por ( (8 menos 2) fatorial vezes 2 fatorial)) menos 1)” and the En-
537"
REFERENCES,0.30686274509803924,"glish translation exacerbates the situation by translating parenthesis literally as well: “9 times open
538"
REFERENCES,0.3073529411764706,"parenthesis, open parenthesis, 8 factorial divided by, open parenthesis, open parenthesis, 8 minus
539"
REFERENCES,0.307843137254902,"2, close parenthesis, factorial times 2 factorial, close parenthesis, close parenthesis, minus 1, close
540"
REFERENCES,0.30833333333333335,"parenthesis.”. Sometimes, delimiters are omitted after translation: “9,300” becomes “9 300”.
541"
REFERENCES,0.3088235294117647,"Awkward phrasings
There exist awkward phrasings throughout translation. They range from
542"
REFERENCES,0.3093137254901961,"causing minor difficulty in understanding (i.e., “Life: the science of biology Bears, because they
543"
REFERENCES,0.30980392156862746,"are not truly hibernating, wake up due to the presence of thermogenin, a mitochondrial protein that
544"
REFERENCES,0.31029411764705883,"prevents protons from reaching ATP synthase, generating heat.”) to sometime completely non-sense
545"
REFERENCES,0.3107843137254902,"(i.e., “articulation of several narrative nuclei”)
546"
REFERENCES,0.3112745098039216,"Incomplete translation
There is no fine line between proper code switching (where proper nouns
547"
REFERENCES,0.31176470588235294,"should remain in Portuguese script) to in-complete translation. The amount of Portuguese left over
548"
REFERENCES,0.3122549019607843,"range from single words, to phrases in options (not consistently across options), to entire sentences
549"
REFERENCES,0.3127450980392157,"within the question.
550"
REFERENCES,0.31323529411764706,"Linguistic idiosyncrasies lost in translation
In one question, the problem arises when English
551"
REFERENCES,0.3137254901960784,"translation does not match with literal tokens of expressions in Portuguese (“Next to the man is
552"
REFERENCES,0.3142156862745098,"the message: “Men don’t cry”, with a large X drawn over the word “no”). The word “no” does
553"
REFERENCES,0.31470588235294117,"not appear in the English phrase “Men don’t cry” but the statement as a whole makes sense in the
554"
REFERENCES,0.31519607843137254,"Portuguese version of the instruction. In a separate question, the topic is on testing for a Portuguese
555"
REFERENCES,0.3156862745098039,"specific pronoun inflection. However, when it was translated into one single word in English, the
556"
REFERENCES,0.3161764705882353,"question no longer makes sense (“They told me... - They told me. - Huh? - The correct word is “they
557"
REFERENCES,0.31666666666666665,"told me”. Not “they told me”. - I speak the way I want to. And I’ll tell you more... Or is it “tell
558"
REFERENCES,0.317156862745098,"you”? - What’s that? - I’m telling you that you... -“You” and “you” don’t go together...”)
559"
REFERENCES,0.3176470588235294,"A.1.3
Document parsing artifacts
560"
REFERENCES,0.31813725490196076,"Each section consistently contains an error of this kind, where the last part of the question got wrong-
561"
REFERENCES,0.31862745098039214,"fully parsed into part of the first option (option (A)). In a separate instance, a figure was wrongfully
562"
REFERENCES,0.3191176470588235,"parsed into one of the options of the previous question. In the Portuguese version of the exam, struc-
563"
REFERENCES,0.3196078431372549,"tural components of the question (e.g., title, subtitle, caption) are consistently concatenated together
564"
REFERENCES,0.32009803921568625,"without proper separation. This often leads to incoherent English translations.
565"
REFERENCES,0.3205882352941177,"A.1.4
Audio-version artifacts
566"
REFERENCES,0.32107843137254904,"Audio description of images, tables, and figures are not always sufficient, or the most intuitive. For
567"
REFERENCES,0.3215686274509804,"instance, a question asks test taker to note why a particular painting stands out, and the answer is
568"
REFERENCES,0.3220588235294118,"due to the painting’s “distortion when representing human figure”, which is difficult to qualitatively
569"
REFERENCES,0.32254901960784316,"describe, no matter how complete the description of an image is. Similarly, textual description
570"
REFERENCES,0.32303921568627453,"of geometric figures can be impossibly complicated (“...Figure of a grid with 7 horizontal and 7
571"
REFERENCES,0.3235294117647059,"vertical lines, on which a polygonal path is drawn by means of a continuous line on the grid lines,
572"
REFERENCES,0.32401960784313727,"joining the starting point P , located on the second vertical line, from left to right, and between the
573"
REFERENCES,0.32450980392156864,"sixth and seventh horizontal lines, from top to bottom, to the end point Q , which is located between
574"
REFERENCES,0.325,"the sixth and seventh vertical lines, from left to right, and on the second horizontal line, from top to
575"
REFERENCES,0.3254901960784314,"bottom...”)
576"
REFERENCES,0.32598039215686275,"A.1.5
Manual Correction
577"
REFERENCES,0.3264705882352941,"The majority of the artifacts begin with incorrect parsing of the PDF documents related to struc-
578"
REFERENCES,0.3269607843137255,"tural components. To address this, we manually audited each question, and added correct spacing
579"
REFERENCES,0.32745098039215687,"and newlines to each question. These improvements result in better translations from DeepL API
580"
REFERENCES,0.32794117647058824,"qualitatively. After translation, we make minimal edits to improve syntactic and semantic issues
581"
REFERENCES,0.3284313725490196,"through Grammarly to obtain a score of at least 95 2 3. For each answer option, we ensure consistent
582"
REFERENCES,0.328921568627451,"2grammarly.com/
3We chose not to use a large model such as GPT3.5 to rephrase the translations because it may artificially
lower the perplexity and change the meaning of the questions."
REFERENCES,0.32941176470588235,"part-of-speech, especially if they are sentence completions of the questions. For math and science
583"
REFERENCES,0.3299019607843137,"sections, we follow consistent markdown-like format the same way as other mathematical reasoning
584"
REFERENCES,0.3303921568627451,"datasets [16, 11, 52]. Here we list the full set of modification rules for 2022 (question numbers are
585"
REFERENCES,0.33088235294117646,"referenced in parenthesis):
586"
REFERENCES,0.33137254901960783,"• Separate description of the image by ’\n’ before and after.
587"
REFERENCES,0.3318627450980392,"• “Por cento” becomes %.
588"
REFERENCES,0.3323529411764706,"• Number in the form 7 000 becomes 7000.
589"
REFERENCES,0.33284313725490194,"• From “abre aspas” “fecha aspas” to “”.
590"
REFERENCES,0.3333333333333333,"• Remove “Descric¸˜ao da estrutura qu´ımica”, “Descric¸˜ao do esquema”, “Descric¸˜ao da
591"
REFERENCES,0.3338235294117647,"associac¸˜ao de baterias”, “Descric¸˜ao da imagem” from the options”.
592"
REFERENCES,0.33431372549019606,"• “De carga positiva” to +, “De carga negativa” to -, “de carga dois menos” to (2-).
593"
REFERENCES,0.3348039215686274,"• For a subset of the questions, we follow the non-blind version of the question (157, 158,
594"
REFERENCES,0.3352941176470588,"163, 166, 168, 171, 174, 177, 178, 179)
595"
REFERENCES,0.33578431372549017,"• Remove period at the end options or questions of math questions (to avoid confusion).
596"
REFERENCES,0.3362745098039216,"Here are the list of rules we use for English version of the exam (2022):
597"
REFERENCES,0.33676470588235297,"• Change number decimal from “3,1415” to “3.1415”.
598"
REFERENCES,0.33725490196078434,"• Manual translation fix (49, 162).
599"
REFERENCES,0.3377450980392157,"A.1.6
Limitations of the dataset
600"
REFERENCES,0.3382352941176471,"There are a few limitations of the dataset:
601"
REFERENCES,0.33872549019607845,"1. Even though the English version of the exam is modified manually, there are still issues
602"
REFERENCES,0.3392156862745098,"with the presentation of the questions. We rely mostly on Grammarly feedback, but it is
603"
REFERENCES,0.3397058823529412,"not perfect. Our judgement of how fluently a question is written is also subjective. The
604"
REFERENCES,0.34019607843137256,"ideal method would be to recruit professional human translators, which is costly and time
605"
REFERENCES,0.34068627450980393,"consuming.
606"
THE CONTENT OF MANY OF THE QUESTIONS ARE FOCUSED ON KNOWLEDGE COMMON TO BRAZILIAN,0.3411764705882353,"2. The content of many of the questions are focused on knowledge common to Brazilian
607"
THE CONTENT OF MANY OF THE QUESTIONS ARE FOCUSED ON KNOWLEDGE COMMON TO BRAZILIAN,0.3416666666666667,"culture, or problems in Brazilian society. The English translations may not cover the full
608"
THE CONTENT OF MANY OF THE QUESTIONS ARE FOCUSED ON KNOWLEDGE COMMON TO BRAZILIAN,0.34215686274509804,"extent of cultural, language specific phenomenons or connotations.
609"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3426470588235294,"3. We assume the transcription of images and tables to be sufficient for the models to under-
610"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3431372549019608,"stand and solve the question.
611"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.34362745098039216,"A.2
Attributes that affect goodness-of-fit
612"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.34411764705882353,"Given that questions have wide range of discrimination indices for LLMs, we investigate a potential
613"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3446078431372549,"cause described in the psychometrics literature for aberrant response patterns: lack of subabili-
614"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.34509803921568627,"ties [23], i.e., specific skills required to answer a question correctly. We hypothesize that some item
615"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.34558823529411764,"attributes, such as whether the question contains images or numbers in its statement or among the
616"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.346078431372549,"options, may be disproportionately harder for LLMs and hence represent subabilities that explain
617"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3465686274509804,"the aberrant response patterns quantified in Figure 3.
618"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.34705882352941175,"We built a contingency table relating non-discriminative/discriminative items (i.e., items with dis-
619"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3475490196078431,"criminative index lower/higher than 0.2) and the aforementioned attributes, and run a χ2 indepen-
620"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3480392156862745,"dence test. The results for the Natural Sciences exam are shown in Table 2. For this exam, we
621"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.34852941176470587,"observe high χ2 values which indicate that the abilities of the LLM models with respect to math
622"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.34901960784313724,"reasoning and interpreting images are sub-par compared to their capacity in solving pure text ques-
623"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3495098039215686,"tions. While Language and Humans exams are most purely text and the Math exam mostly demands
624"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.35,"reasoning with images and numbers, the nature of the Natural Sciences exam is hybrid, containing
625"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.35049019607843135,"both types of questions. This may well explain the bimodal distribution of discrimination indices
626"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3509803921568627,"in Figure 5 and the aberrant response patterns identified by the very low lz scores in Figure 3, and
627"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3514705882352941,"highlights how psychometrics can aid the design of better and more valid benchmarks for LLMs.
628"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3519607843137255,"Table 2: χ2 test for the correlation between poorly-discriminating items and item attributes in the
Natural Sciences exam in 2022 and 2023. Significant values are in bold. High values of χ2 indicate
that images or numbers make the item less useful to evaluate the LLMs we experiment with."
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3524509803921569,"Item Attribute
2022
2023"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.35294117647058826,"Contains images
0.401 (0.052)
3.906 (0.048)
Contains numbers in the answers
7.331 (0.007)
6.264 (0.012)
Contains numbers in the statement
3.961 (0.046)
3.212 (0.073)"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.35343137254901963,"A.3
Model accuracy relation to model perplexity
629"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.353921568627451,"One reason that models may error differently than humans is due to their training corpus. If models
630"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.35441176470588237,"have encountered similar question or topics, if not identical, to those in our dataset during training,
631"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.35490196078431374,"they may perform unexpectedly well, even if the questions are difficult. Recent work in data con-
632"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3553921568627451,"tamination proposed a few model intrinsic metrics that can be used to detect contamination [36].
633"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3558823529411765,"Mainly, the Min-k% Prob score takes the average probability of the top-k percentile tokens with
634"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.35637254901960785,"minimum probabilities 4:
635"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3568627450980392,MIN-K% Prob (x) = −1 E X
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3573529411764706,"xi∈Min-K%(x)
logp(xi|x1, . . . , xi−1)
(4)"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.35784313725490197,"where x = x1, x2, . . . , xN denotes the input sequence of N tokens, Min-K% Prob(x) represents the
636"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.35833333333333334,"set containing tokens with minimum k percentile probabilities, and E represents the size of such set.
637"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3588235294117647,"Note here that Min-k% Prob is intrinsic to each model, and if a model has been exposed to more
638"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3593137254901961,"similar training data as the questions, its Min-k% Prob would be low for that question.
639"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.35980392156862745,"We do not expect any model to have unexpectedly low Min-K% Prob(x) on any of our questions,
640"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3602941176470588,"considering it is highly unlikely that the ENEM questions were parsed and translated to English, and
641"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3607843137254902,"somehow ended up in the training corpus. What we are more interested here, is whether such score
642"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.36127450980392156,"is correlated to model’s accuracy on the answer predictions. If they are negatively correlated (i.e.
643"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.36176470588235293,"high Min-K% Prob corresponds to low accuracy), this is evidence for the hypothesis that training on
644"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3622549019607843,"related data leads to higher accuracy.
645"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3627450980392157,"To investigate this hypothesis, we plot 4-shot model accuracy (averaged across 31 option shuffles)
646"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.36323529411764705,"against Min-20% Prob for four subjects in exam 2022 in English along with the Pearson correlations
647"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3637254901960784,"5 in Figure 6. In all except 1 model-subject pair (Llama2 chat in humanities, we investigate this
648"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3642156862745098,"further) do we see a significant negative correlation (p < 0.05) between accuracy and Min-k 20%
649"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.36470588235294116,"Prob, indicate that model doesn’t necessarily do better if they have encountered similar data during
650"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.36519607843137253,"training. Another way to interpret this, is that it is not likely that these models have seen our data
651"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3656862745098039,"during training.
652"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.36617647058823527,"The few negative correlation cases
As seen before, we observe a significant negative correlation
653"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.36666666666666664,"for Llama-2 7B Chat in humanities. To get a full understanding of whether this is a stand-alone
654"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.367156862745098,"phenomenon, we examine Portuguese version of the exam, as well as exam in 2023, and show our
655"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.36764705882352944,"findings below in Table 3. We do not see the same correlation in the Portuguese version of the
656"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3681372549019608,"exam. However, we additionally see Gemma-it negatively correlated with humanities section in
657"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3686274509803922,"both English and Portuguese version of the exam in 2023, as well as Gemma with languages section
658"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.36911764705882355,"in 2023. The later two correlations are robust across a few other metrics we investigated from [36]
659"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3696078431372549,"as well, we think this may suggest data contamination, but we cannot test such hypothesis because
660"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3700980392156863,"Gemma training data is not public.
661"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.37058823529411766,"Positive correlations in 2022 science
In 2022 Science, both English and Portuguese, we see sig-
662"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.37107843137254903,"nificant positive correlation across all models (Table 3).
663"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3715686274509804,"Through qualitative analysis, we find that the questions with highest perplexities were formatted
664"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3720588235294118,"more in a sentence completion-like structure similar to Question 1. Whereas less perplexity ques-
665"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.37254901960784315,"4We follow the equation in https://github.com/swj0419/detect-pretrain-code/blob/main/src/run.py
5https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3730392156862745,"0
0.2
0.4
0.6
0.8
1.0"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3735294117647059,Accuracy 0 10 20 30 40 50
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.37401960784313726,MIN-20% Prob
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.37450980392156863,"2022 Humanities (EN)
Model: Pearson r (p-value)
Gemma-7B Instruct: -0.06 (0.69)
Gemma-7B: 0.06 (0.70)
LLaMA2-13B Instruct: -0.16 (0.30)
LLaMA2-13B: -0.06 (0.68)
LLaMA2-7B Instruct: -0.36 (0.02)
LLaMA2-7B: -0.14 (0.36)
LLaMA3-8B Instruct: -0.15 (0.32)
LLaMA3-8B: -0.08 (0.62)
Mistral-7B Instruct: 0.04 (0.78)
Mistral-7B: 0.02 (0.87)"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.375,"0
0.2
0.4
0.6
0.8
1.0"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.37549019607843137,Accuracy 0 10 20 30 40 50
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.37598039215686274,MIN-20% Prob
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3764705882352941,"2022 Natural Sciences (EN)
Model: Pearson r (p-value)
Gemma-7B Instruct: 0.44 (0.00)
Gemma-7B: 0.65 (0.00)
LLaMA2-13B Instruct: 0.56 (0.00)
LLaMA2-13B: 0.58 (0.00)
LLaMA2-7B Instruct: 0.52 (0.00)
LLaMA2-7B: 0.45 (0.00)
LLaMA3-8B Instruct: 0.57 (0.00)
LLaMA3-8B: 0.58 (0.00)
Mistral-7B Instruct: 0.59 (0.00)
Mistral-7B: 0.62 (0.00)"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3769607843137255,"0
0.2
0.4
0.6
0.8
1.0"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.37745098039215685,Accuracy 0 10 20 30 40 50
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3779411764705882,MIN-20% Prob
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3784313725490196,"2022 Languages and Codes (EN)
Model: Pearson r (p-value)
Gemma-7B Instruct: 0.24 (0.11)
Gemma-7B: -0.22 (0.15)
LLaMA2-13B Instruct: -0.27 (0.07)
LLaMA2-13B: -0.34 (0.02)
LLaMA2-7B Instruct: 0.02 (0.89)
LLaMA2-7B: 0.14 (0.35)
LLaMA3-8B Instruct: -0.07 (0.65)
LLaMA3-8B: -0.36 (0.02)
Mistral-7B Instruct: -0.00 (1.00)
Mistral-7B: -0.05 (0.76)"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.37892156862745097,"0
0.2
0.4
0.6
0.8
1.0"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.37941176470588234,Accuracy 0 10 20 30 40 50
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3799019607843137,MIN-20% Prob
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3803921568627451,"2022 Mathematics (EN)
Model: Pearson r (p-value)
Gemma-7B Instruct: -0.27 (0.07)
Gemma-7B: -0.03 (0.86)
LLaMA2-13B Instruct: 0.11 (0.48)
LLaMA2-13B: -0.04 (0.81)
LLaMA2-7B Instruct: 0.27 (0.07)
LLaMA2-7B: 0.23 (0.13)
LLaMA3-8B Instruct: 0.14 (0.36)
LLaMA3-8B: -0.05 (0.75)
Mistral-7B Instruct: -0.13 (0.42)
Mistral-7B: -0.04 (0.80)"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.38088235294117645,Figure 6: Model Min-20% Prob vs. 4-shot accuracy across four subjects in 2022 in English
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3813725490196078,"tions involve more image/table description with reasoning needed to obtain the answer (question 2).
666"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.3818627450980392,"This is similar to what we discover with discriminative index in Section ?? in the main text.
667"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.38235294117647056,"1 Question: Technique
modifies
rattlesnake
venom
protein to create a
668"
WE ASSUME THE TRANSCRIPTION OF IMAGES AND TABLES TO BE SUFFICIENT FOR THE MODELS TO UNDER-,0.382843137254902,"drug that
modulates
blood
clotting
669 2
670"
"RATTLESNAKE
VENOM CAN CAUSE LIFE -THREATENING
HEMORRHAGING TO THOSE",0.38333333333333336,"3 Rattlesnake
venom can cause life -threatening
hemorrhaging to those
671"
"RATTLESNAKE
VENOM CAN CAUSE LIFE -THREATENING
HEMORRHAGING TO THOSE",0.38382352941176473,"bitten by the snake. However , researchers
from
Brazil and
Belgium
672"
"RATTLESNAKE
VENOM CAN CAUSE LIFE -THREATENING
HEMORRHAGING TO THOSE",0.3843137254901961,"have
developed a molecule of pharmaceutical
interest , PEG -
673"
"RATTLESNAKE
VENOM CAN CAUSE LIFE -THREATENING
HEMORRHAGING TO THOSE",0.38480392156862747,"collinein -1, from a protein
found in the snake ’s venom. The
674"
"RATTLESNAKE
VENOM CAN CAUSE LIFE -THREATENING
HEMORRHAGING TO THOSE",0.38529411764705884,"molecule is capable of modulating
blood
clotting. Although
the
675"
"RATTLESNAKE
VENOM CAN CAUSE LIFE -THREATENING
HEMORRHAGING TO THOSE",0.3857843137254902,"technique is not new , it was
applied
for the first
time from an
676"
"RATTLESNAKE
VENOM CAN CAUSE LIFE -THREATENING
HEMORRHAGING TO THOSE",0.3862745098039216,"animal
toxin in its
recombinant form , i.e. produced in the
677"
"RATTLESNAKE
VENOM CAN CAUSE LIFE -THREATENING
HEMORRHAGING TO THOSE",0.38676470588235295,"laboratory by a genetically
modified
fungus.
678 4
679"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.3872549019607843,"5 This new drug has
potential
applications
for
680"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.3877450980392157,"6 Options:
681"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.38823529411764707,"7 (A) prevent
the
formation of thrombi , typical in some
cases of stroke.
682"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.38872549019607844,"8 (B) treat the
consequences of profound
anemia , due to the loss of a
683"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.3892156862745098,"large
volume of blood.
684"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.3897058823529412,"9 (C) prevent
the
manifestation of urticaria , commonly
related to
685"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.39019607843137255,"allergic
processes.
686"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.3906862745098039,"10 (D) reduce
swelling of the lymph nodes , part of the immune
response to
687"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.3911764705882353,"different
infections.
688"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.39166666666666666,"11 (E) regulate
the
fluctuations in blood
pressure
characteristic of
689"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.39215686274509803,"hypertension.
690"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.3926470588235294,Listing 1: high perplexity question with high model accuracy.
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.3931372549019608,"1 Question: On a hot day , two
colleagues
are
playing
with the water
from
691"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.39362745098039215,"the hose. One of them
wants to know how high the water jet
692"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.3941176470588235,"reaches
from the outlet
when the hose is positioned
vertically.
693"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.3946078431372549,"The other
colleague
then
proposes
the
following
experiment: they
694"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.39509803921568626,"position
the water
outlet of the hose in a horizontal
direction , 1
695"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.39558823529411763,"meter
above the ground , and then
measure
the
horizontal
distance
696"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.396078431372549,"between
the hose and the place
where the water
hits the ground.
697"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.39656862745098037,"The
measurement of this
distance
was 3 meters , and from this , they
698"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.39705882352941174,"calculated
the
vertical
reach of the water jet. Consider
the
699"
"THIS NEW DRUG HAS
POTENTIAL
APPLICATIONS
FOR",0.3975490196078431,"acceleration of gravity to be 10 meters per second
squared.
700 2
701"
"THE RESULT
THEY
OBTAINED
WAS",0.3980392156862745,"3 The result
they
obtained
was
702"
"THE RESULT
THEY
OBTAINED
WAS",0.3985294117647059,"4 Options:
703"
"THE RESULT
THEY
OBTAINED
WAS",0.3990196078431373,"5 (A) 1.50
meter.
704"
"THE RESULT
THEY
OBTAINED
WAS",0.39950980392156865,"6 (B) 2.25
meters.
705"
"THE RESULT
THEY
OBTAINED
WAS",0.4,"7 (C) 4.00
meters.
706"
"THE RESULT
THEY
OBTAINED
WAS",0.4004901960784314,"8 (D) 4.50
meters.
707"
"THE RESULT
THEY
OBTAINED
WAS",0.40098039215686276,"9 (E) 5.00
meters.
708"
"THE RESULT
THEY
OBTAINED
WAS",0.40147058823529413,Listing 2: low perplexity question with low model accuracy.
"THE RESULT
THEY
OBTAINED
WAS",0.4019607843137255,"We also tried filtering for top N percent most difficult questions per subject and recalculate all the
709"
"THE RESULT
THEY
OBTAINED
WAS",0.4024509803921569,"correlations. We did not find any significant difference to results above.
710"
"THE RESULT
THEY
OBTAINED
WAS",0.40294117647058825,"A.4
Prompting Details
711"
"THE RESULT
THEY
OBTAINED
WAS",0.4034313725490196,"To administering the test to LLMs, we measure the next token logits across the 5 letter options
712"
"THE RESULT
THEY
OBTAINED
WAS",0.403921568627451,"directly (i.e. letter “A”, “B”, “C”, “D”, “E”), and take the argmax as the model’s choice (invariant
713"
"THE RESULT
THEY
OBTAINED
WAS",0.40441176470588236,"to sampling temperature). We shuffle the option orders (30 runs) and take the average to calibrate
714"
"THE RESULT
THEY
OBTAINED
WAS",0.40490196078431373,"model’s prior on generating each letter options. For API-based model (GPT3.5), we query for 1
715"
"THE RESULT
THEY
OBTAINED
WAS",0.4053921568627451,"token generation, and obtain top-20 logits, and use that for our prediction. In the sections below we
716"
"THE RESULT
THEY
OBTAINED
WAS",0.40588235294117647,"include 0-shot (Listing 3), 1-shot (Listing 4, 5, 6, 7), and 4-shot prompts (Listing 8) we use in main
717"
"THE RESULT
THEY
OBTAINED
WAS",0.40637254901960784,"experiments. For 1-shot, we choose the 1-shot example for each of the four subjects by selecting
718"
"THE RESULT
THEY
OBTAINED
WAS",0.4068627450980392,"the easiest question (i.e., with lowest β) from the same subject in the 2021 exam. For 4-shot, we
719"
"THE RESULT
THEY
OBTAINED
WAS",0.4073529411764706,"concatenate the 1-shots from four subjects and shuffle the options to evenly distribute the answer
720"
"THE RESULT
THEY
OBTAINED
WAS",0.40784313725490196,"among five option letters.
721"
"THE RESULT
THEY
OBTAINED
WAS",0.4083333333333333,"year
lang
subj
L2-7b
L2-7b-it
L2-13b
L2-13b-it
L3-8b-it
L3-8b
M-7b
M-7b-it
G-7b-it
G-7b 2022 en"
"THE RESULT
THEY
OBTAINED
WAS",0.4088235294117647,"CH
-0.14/0.36
-0.36/0.02
-0.06/0.68
-0.16/0.30
-0.15/0.32
-0.08/0.62
0.02/0.87
0.04/0.78
-0.06/0.69
0.06/0.70
LC
0.14/0.35
0.02/0.89
-0.34/0.02
-0.27/0.07
-0.07/0.65
-0.36/0.02
-0.05/0.76
-0.00/1.00
0.24/0.11
-0.22/0.15
CN
0.45/0.00
0.52/0.00
0.58/0.00
0.56/0.00
0.57/0.00
0.58/0.00
0.62/0.00
0.59/0.00
0.44/0.00
0.65/0.00
MT
0.23/0.13
0.27/0.07
-0.04/0.81
0.11/0.48
0.14/0.36
-0.05/0.75
-0.04/0.80
-0.13/0.42
-0.27/0.07
-0.03/0.86 pt"
"THE RESULT
THEY
OBTAINED
WAS",0.40931372549019607,"CH
-0.09/0.56
-0.12/0.43
-0.06/0.70
-0.05/0.73
-0.07/0.65
-0.05/0.74
-0.09/0.56
-0.06/0.69
-0.20/0.18
0.18/0.24
LC
0.10/0.53
-0.02/0.88
-0.06/0.67
-0.05/0.73
0.08/0.61
-0.20/0.20
0.14/0.35
-0.09/0.56
0.14/0.37
-0.21/0.16
CN
0.41/0.01
0.42/0.00
0.49/0.00
0.48/0.00
0.57/0.00
0.52/0.00
0.53/0.00
0.52/0.00
0.46/0.00
0.58/0.00
MT
-0.17/0.26
-0.15/0.34
0.12/0.44
-0.02/0.91
0.07/0.66
-0.08/0.59
-0.18/0.23
-0.14/0.35
-0.05/0.76
0.12/0.42 2023 en"
"THE RESULT
THEY
OBTAINED
WAS",0.40980392156862744,"CH
-0.06/0.72
-0.07/0.66
-0.09/0.56
0.06/0.69
-0.20/0.20
-0.18/0.23
-0.20/0.18
-0.07/0.65
-0.32/0.03
-0.16/0.30
LC
-0.06/0.67
-0.22/0.15
-0.31/0.04
-0.24/0.12
-0.21/0.17
-0.30/0.04
-0.18/0.23
-0.08/0.61
-0.05/0.76
-0.32/0.03
CN
0.21/0.17
0.21/0.17
0.31/0.04
0.16/0.31
0.30/0.05
0.28/0.06
0.14/0.35
0.15/0.34
0.20/0.19
0.24/0.11
MT
0.17/0.28
-0.07/0.66
-0.04/0.82
-0.02/0.87
-0.05/0.75
0.15/0.35
0.03/0.85
0.19/0.21
0.06/0.68
0.16/0.32 pt"
"THE RESULT
THEY
OBTAINED
WAS",0.4102941176470588,"CH
-0.00/1.00
-0.02/0.92
0.09/0.58
0.18/0.25
-0.02/0.90
-0.11/0.46
-0.04/0.77
0.01/0.96
-0.30/0.05
-0.09/0.55
LC
-0.21/0.16
-0.23/0.13
-0.27/0.07
-0.17/0.26
-0.20/0.18
-0.24/0.11
-0.18/0.23
-0.10/0.53
-0.13/0.40
-0.36/0.02
CN
0.11/0.49
0.17/0.26
0.25/0.10
0.04/0.82
0.14/0.37
0.36/0.01
0.15/0.32
0.14/0.35
0.08/0.61
0.13/0.41
MT
-0.01/0.96
0.02/0.87
-0.02/0.90
-0.04/0.79
-0.07/0.67
0.06/0.71
-0.08/0.60
0.09/0.56
0.18/0.24
0.28/0.06
Table 3: Correlation between model accuracy and Min-k% Prob across exam, languages, and sub-
jects for all models (L2=llama2, L3=Llama3, M=Mistral, G=gemma, it=instruction-tuned/chat).
The first number indicates the coefficient of the correlation, and the second, the p-value. Entries
with p-value < 0.05 are in bold. CN=Humanities, LC=Languages, CN=Sciences, MT=Math ."
"THE RESULT
THEY
OBTAINED
WAS",0.4107843137254902,"Potential limitations
We ran exploratory experiments with Chain-of-Thought (CoT) like prompt-
722"
"THE RESULT
THEY
OBTAINED
WAS",0.41127450980392155,"ing [49], but and did not see significant changes. We did not include the results because CoT prompt-
723"
"THE RESULT
THEY
OBTAINED
WAS",0.4117647058823529,"ing requires generating reasoning strings and parsing answers, making 30-shuffles extremely slow
724"
"THE RESULT
THEY
OBTAINED
WAS",0.4122549019607843,"to run for all models. Future directions could explore how much effect more complex prompting
725"
"THE RESULT
THEY
OBTAINED
WAS",0.41274509803921566,"techniques have in assimilating model behaviors. Regarding the best prompting strategy, we do
726"
"THE RESULT
THEY
OBTAINED
WAS",0.41323529411764703,"acknowledge recent criticisms on first letter evaluation[48]. At the time of our writing, it is still
727"
"THE RESULT
THEY
OBTAINED
WAS",0.4137254901960784,"the best evaluation strategy for multiple choice question-answering data. We also acknowledge that
728"
"THE RESULT
THEY
OBTAINED
WAS",0.41421568627450983,"there are more capable models than GPT3.5 that is available through API services but as our work is
729"
"THE RESULT
THEY
OBTAINED
WAS",0.4147058823529412,"not trying to identify the SOTA model we did not feel the need to evaluate latest and largest models.
730"
"THE RESULT
THEY
OBTAINED
WAS",0.4151960784313726,"Lastly, we assume Portuguese and Brazilian culture is present in the training data for the language
731"
"THE RESULT
THEY
OBTAINED
WAS",0.41568627450980394,"models we test. Future work could evaluate the amount of multilingual training’s affect on some of
732"
"THE RESULT
THEY
OBTAINED
WAS",0.4161764705882353,"these IRT metric we propose.
733"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4166666666666667,"1 Here are some
questions
from a college
entrance
exam. Choose the
734"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.41715686274509806,"correct
answer to the best of your ability , and output in the
735"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4176470588235294,"following
format:
736"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4181372549019608,"2 Answer: (Option)
737 3
738"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.41862745098039217,"4 Question: {QUESTION}
739"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.41911764705882354,"5 Options:
740"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4196078431372549,"6 (A) {OPTION_A}
741"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4200980392156863,"7 (B) {OPTION_B}
742"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.42058823529411765,"8 (C) {OPTION_C}
743"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.421078431372549,"9 (D) {OPTION_D}
744"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4215686274509804,"10 (E) {OPTION_E}
745"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.42205882352941176,"11 Answer: (
746"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.42254901960784313,Listing 3: 0-shot prompt used across all four subjects.
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4230392156862745,"1 Here are some
questions
from a college
entrance
exam. Choose the
747"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4235294117647059,"correct
answer to the best of your ability , and output in the
748"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.42401960784313725,"following
format:
749"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4245098039215686,"2 Answer: (Option)
750 3
751"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.425,"4 Question:
752"
"BUFFALOS
ARE
ANIMALS
CONSIDERED
RUSTIC BY BREEDERS
AND ARE
THEREFORE",0.42549019607843136,"5 Buffalos
are
animals
considered
rustic by breeders
and are
therefore
753"
"BUFFALOS
ARE
ANIMALS
CONSIDERED
RUSTIC BY BREEDERS
AND ARE
THEREFORE",0.42598039215686273,"left in the field
without
reproductive
control. Because of this
754"
"BUFFALOS
ARE
ANIMALS
CONSIDERED
RUSTIC BY BREEDERS
AND ARE
THEREFORE",0.4264705882352941,"type of breeding , inbreeding is common , leading to the
appearance
755"
"BUFFALOS
ARE
ANIMALS
CONSIDERED
RUSTIC BY BREEDERS
AND ARE
THEREFORE",0.42696078431372547,"of diseases
such as albinism
and heart
defects , among
others.
756"
"BUFFALOS
ARE
ANIMALS
CONSIDERED
RUSTIC BY BREEDERS
AND ARE
THEREFORE",0.42745098039215684,"Separating
the
animals
properly by sex would
minimize
the
757"
"BUFFALOS
ARE
ANIMALS
CONSIDERED
RUSTIC BY BREEDERS
AND ARE
THEREFORE",0.4279411764705882,"occurrence of these
problems.
758 6
759"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.4284313725490196,"7 What
prior
biotechnological
procedure is recommended in this
situation
760 ?
761 8
762"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.42892156862745096,"9 Options:
763"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.4294117647058823,"10 (A) Transgenics.
764"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.42990196078431375,"11 (B) Gene
therapy.
765"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.4303921568627451,"12 (C) DNA
vaccine.
766"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.4308823529411765,"13 (D) Genetic
mapping.
767"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.43137254901960786,"14 (E) Therapeutic
cloning.
768"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.43186274509803924,"15
769"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.4323529411764706,"16 Answer: (D) Genetic
mapping.
770"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.432843137254902,"17
771"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.43333333333333335,"18 Question: {QUESTION}
772"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.4338235294117647,"19 Options:
773"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.4343137254901961,"20 (A) {OPTION_A}
774"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.43480392156862746,"21 (B) {OPTION_B}
775"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.43529411764705883,"22 (C) {OPTION_C}
776"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.4357843137254902,"23 (D) {OPTION_D}
777"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.4362745098039216,"24 (E) {OPTION_E}
778"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.43676470588235294,"25 Answer: (
779"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.4372549019607843,Listing 4: 1-shot prompt used for Natural Science.
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4377450980392157,"1 Here are some
questions
from a college
entrance
exam. Choose the
780"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.43823529411764706,"correct
answer to the best of your ability , and output in the
781"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4387254901960784,"following
format:
782"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4392156862745098,"2 Answer: (Option)
783 3
784"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.43970588235294117,"4 Question:
785"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.44019607843137254,"5 A hamburger
chain has three
franchises in different
cities. To include
786"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.4406862745098039,"a new type of snack on the menu , the chain ’s marketing
manager
787"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.4411764705882353,"suggested
putting
five new types of snacks on sale in special
788"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.44166666666666665,"editions. The snacks
were
offered
for the same
period of time to
789"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.442156862745098,"all the
franchisees. The type with the
highest
average
sold per
790"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.4426470588235294,"franchise
would be permanently
included on the menu. At the end of
791"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.44313725490196076,"the trial period , management
received a report
describing
the
792"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.44362745098039214,"quantities sold , in units , of each of the five
types of snacks in
793"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.4441176470588235,"the three
franchises.
794 6
795"
IMAGE,0.4446078431372549,"7 Image
description: The table
shows the
quantity
sold of each type of
796"
IMAGE,0.44509803921568625,"snack in franchises 1, 2, and 3.
797"
IMAGE,0.4455882352941177,"8 Franchise 1 sold 415 type -1 snacks , 395 type -2 snacks , 425 type -3
798"
IMAGE,0.44607843137254904,"snacks , 430 type -4 snacks , and 435 type -5 snacks.
799"
IMAGE,0.4465686274509804,"9 Franchise 2 sold 415 type -1 snacks; 445 type -2 snacks; 370 type -3
800"
IMAGE,0.4470588235294118,"snacks; 370 type -4 snacks and 425 type -5 snacks.
801"
IMAGE,0.44754901960784316,"10 Franchise 3 sold 415 type -1 snacks; 390 type -2 snacks; 425 type -3
802"
IMAGE,0.44803921568627453,"snacks; 433 type -4 snacks and 420 type -5 snacks.
803"
IMAGE,0.4485294117647059,"11
804"
BASED ON THIS,0.44901960784313727,"12 Based on this
information , the
management
has
decided to include
the
805"
BASED ON THIS,0.44950980392156864,"following
type of snack on the menu
806"
BASED ON THIS,0.45,"13
807"
BASED ON THIS,0.4504901960784314,"14 Options:
808"
BASED ON THIS,0.45098039215686275,"15 (A) 1
809"
BASED ON THIS,0.4514705882352941,"16 (B) 2
810"
BASED ON THIS,0.4519607843137255,"17 (C) 3
811"
BASED ON THIS,0.45245098039215687,"18 (D) 4
812"
BASED ON THIS,0.45294117647058824,"19 (E) 5
813"
BASED ON THIS,0.4534313725490196,"20
814"
BASED ON THIS,0.453921568627451,"21 Answer: (E) 5
815"
BASED ON THIS,0.45441176470588235,"22
816"
BASED ON THIS,0.4549019607843137,"23 Question: {QUESTION}
817"
BASED ON THIS,0.4553921568627451,"24 Options:
818"
BASED ON THIS,0.45588235294117646,"25 (A) {OPTION_A}
819"
BASED ON THIS,0.45637254901960783,"26 (B) {OPTION_B}
820"
BASED ON THIS,0.4568627450980392,"27 (C) {OPTION_C}
821"
BASED ON THIS,0.4573529411764706,"28 (D) {OPTION_D}
822"
BASED ON THIS,0.45784313725490194,"29 (E) {OPTION_E}
823"
BASED ON THIS,0.4583333333333333,"30 Answer: (
824"
BASED ON THIS,0.4588235294117647,Listing 5: 1-shot prompt used for Math.
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.45931372549019606,"1 Here are some
questions
from a college
entrance
exam. Choose the
825"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4598039215686274,"correct
answer to the best of your ability , and output in the
826"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4602941176470588,"following
format:
827"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.46078431372549017,"2 Answer: (Option)
828 3
829"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4612745098039216,"4 Question:
830"
"THE
SITUATION OF THE
WORKING
CLASS IN ENGLAND",0.46176470588235297,"5 The
situation of the
working
class in England
831"
"FRIEDRICH
ENGELS",0.46225490196078434,"6 Friedrich
Engels
832 7
833"
"FRIEDRICH
ENGELS",0.4627450980392157,"8 At the same time , thanks to the ample
opportunities I have had to
834"
"FRIEDRICH
ENGELS",0.4632352941176471,"observe
the middle
classes , your
adversaries , I have
quickly
835"
"FRIEDRICH
ENGELS",0.46372549019607845,"concluded
that you are right , absolutely
right , not to expect any
836"
"FRIEDRICH
ENGELS",0.4642156862745098,"help from them. Its
interests
are
diametrically
opposed to yours ,
837"
"FRIEDRICH
ENGELS",0.4647058823529412,"even if it constantly
tries to claim the
opposite
and wants to
838"
"FRIEDRICH
ENGELS",0.46519607843137256,"persuade
you that it feels the
greatest
sympathy
for your lot. But
839"
"FRIEDRICH
ENGELS",0.46568627450980393,"her
actions
belie her words.
840 9
841"
"FRIEDRICH
ENGELS",0.4661764705882353,"10 In the text , the author
presents
ethical
outlines
that
correspond to
842"
"FRIEDRICH
ENGELS",0.4666666666666667,"11
843"
"FRIEDRICH
ENGELS",0.46715686274509804,"12 Options:
844"
"FRIEDRICH
ENGELS",0.4676470588235294,"13 (A) the
foundation of the idea of surplus
value.
845"
"FRIEDRICH
ENGELS",0.4681372549019608,"14 (B) concept of class
struggle.
846"
"FRIEDRICH
ENGELS",0.46862745098039216,"15 (C) fundamentals of the
scientific
method.
847"
"FRIEDRICH
ENGELS",0.46911764705882353,"16 (D) paradigms of the
inquiry
process.
848"
"FRIEDRICH
ENGELS",0.4696078431372549,"17 (E) domains of commodity
fetishism.
849"
"FRIEDRICH
ENGELS",0.47009803921568627,"18
850"
"FRIEDRICH
ENGELS",0.47058823529411764,"19 Answer: (B) concept of class
struggle.
851"
"FRIEDRICH
ENGELS",0.471078431372549,"20
852"
"FRIEDRICH
ENGELS",0.4715686274509804,"21 Question: {QUESTION}
853"
"FRIEDRICH
ENGELS",0.47205882352941175,"22 Options:
854"
"FRIEDRICH
ENGELS",0.4725490196078431,"23 (A) {OPTION_A}
855"
"FRIEDRICH
ENGELS",0.4730392156862745,"24 (B) {OPTION_B}
856"
"FRIEDRICH
ENGELS",0.47352941176470587,"25 (C) {OPTION_C}
857"
"FRIEDRICH
ENGELS",0.47401960784313724,"26 (D) {OPTION_D}
858"
"FRIEDRICH
ENGELS",0.4745098039215686,"27 (E) {OPTION_E}
859"
"FRIEDRICH
ENGELS",0.475,"28 Answer: (
860"
"FRIEDRICH
ENGELS",0.47549019607843135,Listing 6: 1-shot prompt used for Humanities.
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4759803921568627,"1 Here are some
questions
from a college
entrance
exam. Choose the
861"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4764705882352941,"correct
answer to the best of your ability , and output in the
862"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4769607843137255,"following
format:
863"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.4774509803921569,"2 Answer: (Option)
864 3
865"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.47794117647058826,"4 Question:
866"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.47843137254901963,"5 Sinh\’a
867"
"CHICO
BUARQUE",0.478921568627451,"6 Chico
Buarque
and Jo\~ao Bosco
868 7
869"
"IF THE OWNER
BATHED",0.47941176470588237,"8 If the owner
bathed
870"
"IF THE OWNER
BATHED",0.47990196078431374,"9 I wasn ’t there
871"
BY GOD OUR LORD,0.4803921568627451,"10 By God our Lord
872"
BY GOD OUR LORD,0.4808823529411765,"11 I didn ’t look Sinh\’a
873"
I WAS IN THE FIELDS,0.48137254901960785,"12 I was in the fields
874"
I WAS IN THE FIELDS,0.4818627450980392,"13 I’m not one to look at anyone
875"
I WAS IN THE FIELDS,0.4823529411764706,"14 I’m not greedy
anymore
876"
I WAS IN THE FIELDS,0.48284313725490197,"15 I can ’t see
straight
877"
I WAS IN THE FIELDS,0.48333333333333334,"16
878"
WHY PUT ME IN THE TRUNK,0.4838235294117647,"17 Why put me in the trunk
879"
WHY HURT ME,0.4843137254901961,"18 Why hurt me
880"
I SWEAR TO YOU,0.48480392156862745,"19 I swear to you
881"
I SWEAR TO YOU,0.4852941176470588,"20 I’ve never
seen Sinh\’a
882"
I SWEAR TO YOU,0.4857843137254902,"21 [...]
883"
WHY CARVE UP MY BODY,0.48627450980392156,"22 Why carve up my body
884"
WHY CARVE UP MY BODY,0.48676470588235293,"23 I didn ’t look at Sinh\’a
885"
WHY WOULD YOU,0.4872549019607843,"24 Why would you
886"
WHY WOULD YOU,0.4877450980392157,"25 You ’ll pierce my eyes
887"
I CRY IN YORUBA,0.48823529411764705,"26 I cry in Yoruba
888"
BUT I PRAY FOR JESUS,0.4887254901960784,"27 But I pray for Jesus
889"
SO THAT YOU CAN,0.4892156862745098,"28 So that you can
890"
TAKE AWAY MY LIGHT,0.48970588235294116,"29 Take away my light
891"
TAKE AWAY MY LIGHT,0.49019607843137253,"30
892"
IN THIS,0.4906862745098039,"31 In this
fragment of the song ’s lyrics , the
vocabulary
used and the
893"
IN THIS,0.49117647058823527,"situation
portrayed
are
relevant to the country ’s linguistic
894"
IN THIS,0.49166666666666664,"heritage
and identity , in that
895"
IN THIS,0.492156862745098,"32
896"
IN THIS,0.49264705882352944,"33 Options:
897"
IN THIS,0.4931372549019608,"34 (A) physical
and
symbolic
violence
against
enslaved
people.
898"
IN THIS,0.4936274509803922,"35 (B) value the
influences of African
culture on national
music.
899"
IN THIS,0.49411764705882355,"36 (C) relativize
the
syncretism
that
makes up Brazilian
religious
900"
IN THIS,0.4946078431372549,"practices.
901"
IN THIS,0.4950980392156863,"37 (D) narrate
the
misfortunes of the love
relationship
between
members
902"
IN THIS,0.49558823529411766,"of different
social
classes.
903"
IN THIS,0.49607843137254903,"38 (E) problematize
the
different
worldviews in society
during the
904"
IN THIS,0.4965686274509804,"colonial
period.
905"
IN THIS,0.4970588235294118,"39
906"
IN THIS,0.49754901960784315,"40 Answer: (A) physical
and
symbolic
violence
against
enslaved
people
907"
IN THIS,0.4980392156862745,"41
908"
IN THIS,0.4985294117647059,"42 Question: {QUESTION}
909"
IN THIS,0.49901960784313726,"43 Options:
910"
IN THIS,0.49950980392156863,"44 (A) {OPTION_A}
911"
IN THIS,0.5,"45 (B) {OPTION_B}
912"
IN THIS,0.5004901960784314,"46 (C) {OPTION_C}
913"
IN THIS,0.5009803921568627,"47 (D) {OPTION_D}
914"
IN THIS,0.5014705882352941,"48 (E) {OPTION_E}
915"
IN THIS,0.5019607843137255,"49 Answer: (
916"
IN THIS,0.5024509803921569,Listing 7: 1-shot prompt used for Languages.
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.5029411764705882,"1 Here are some
questions
from a college
entrance
exam. Choose the
917"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.5034313725490196,"correct
answer to the best of your ability , and output in the
918"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.503921568627451,"following
format:
919"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.5044117647058823,"2 Answer: (Option)
920 3
921"
"HERE ARE SOME
QUESTIONS
FROM A COLLEGE
ENTRANCE",0.5049019607843137,"4 Question:
922"
"BUFFALOS
ARE
ANIMALS
CONSIDERED
RUSTIC BY BREEDERS
AND ARE
THEREFORE",0.5053921568627451,"5 Buffalos
are
animals
considered
rustic by breeders
and are
therefore
923"
"BUFFALOS
ARE
ANIMALS
CONSIDERED
RUSTIC BY BREEDERS
AND ARE
THEREFORE",0.5058823529411764,"left in the field
without
reproductive
control. Because of this
924"
"BUFFALOS
ARE
ANIMALS
CONSIDERED
RUSTIC BY BREEDERS
AND ARE
THEREFORE",0.5063725490196078,"type of breeding , inbreeding is common , leading to the
appearance
925"
"BUFFALOS
ARE
ANIMALS
CONSIDERED
RUSTIC BY BREEDERS
AND ARE
THEREFORE",0.5068627450980392,"of diseases
such as albinism
and heart
defects , among
others.
926"
"BUFFALOS
ARE
ANIMALS
CONSIDERED
RUSTIC BY BREEDERS
AND ARE
THEREFORE",0.5073529411764706,"Separating
the
animals
properly by sex would
minimize
the
927"
"BUFFALOS
ARE
ANIMALS
CONSIDERED
RUSTIC BY BREEDERS
AND ARE
THEREFORE",0.5078431372549019,"occurrence of these
problems.
928 6
929"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.5083333333333333,"7 What
prior
biotechnological
procedure is recommended in this
situation
930 ?
931 8
932"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.5088235294117647,"9 Options:
933"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.509313725490196,"10 (A) Transgenics.
934"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.5098039215686274,"11 (B) Gene
therapy.
935"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.5102941176470588,"12 (C) DNA
vaccine.
936"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.5107843137254902,"13 (D) Genetic
mapping.
937"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.5112745098039215,"14 (E) Therapeutic
cloning.
938"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.5117647058823529,"15
939"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.5122549019607843,"16 Answer: (D) Genetic
mapping.
940"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.5127450980392156,"17
941"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.513235294117647,"18 Question:
942"
"WHAT
PRIOR
BIOTECHNOLOGICAL
PROCEDURE IS RECOMMENDED IN THIS
SITUATION",0.5137254901960784,"19 Sinh\’a
943"
"CHICO
BUARQUE",0.5142156862745098,"20 Chico
Buarque
and Jo\~ao Bosco
944"
"CHICO
BUARQUE",0.5147058823529411,"21
945"
"IF THE OWNER
BATHED",0.5151960784313725,"22 If the owner
bathed
946"
"IF THE OWNER
BATHED",0.515686274509804,"23 I wasn ’t there
947"
BY GOD OUR LORD,0.5161764705882353,"24 By God our Lord
948"
BY GOD OUR LORD,0.5166666666666667,"25 I didn ’t look Sinh\’a
949"
I WAS IN THE FIELDS,0.5171568627450981,"26 I was in the fields
950"
I WAS IN THE FIELDS,0.5176470588235295,"27 I’m not one to look at anyone
951"
I WAS IN THE FIELDS,0.5181372549019608,"28 I’m not greedy
anymore
952"
I WAS IN THE FIELDS,0.5186274509803922,"29 I can ’t see
straight
953"
I WAS IN THE FIELDS,0.5191176470588236,"30
954"
WHY PUT ME IN THE TRUNK,0.5196078431372549,"31 Why put me in the trunk
955"
WHY HURT ME,0.5200980392156863,"32 Why hurt me
956"
I SWEAR TO YOU,0.5205882352941177,"33 I swear to you
957"
I SWEAR TO YOU,0.5210784313725491,"34 I’ve never
seen Sinh\’a
958"
I SWEAR TO YOU,0.5215686274509804,"35 [...]
959"
WHY CARVE UP MY BODY,0.5220588235294118,"36 Why carve up my body
960"
WHY CARVE UP MY BODY,0.5225490196078432,"37 I didn ’t look at Sinh\’a
961"
WHY WOULD YOU,0.5230392156862745,"38 Why would you
962"
WHY WOULD YOU,0.5235294117647059,"39 You ’ll pierce my eyes
963"
I CRY IN YORUBA,0.5240196078431373,"40 I cry in Yoruba
964"
BUT I PRAY FOR JESUS,0.5245098039215687,"41 But I pray for Jesus
965"
SO THAT YOU CAN,0.525,"42 So that you can
966"
TAKE AWAY MY LIGHT,0.5254901960784314,"43 Take away my light
967"
TAKE AWAY MY LIGHT,0.5259803921568628,"44
968"
IN THIS,0.5264705882352941,"45 In this
fragment of the song ’s lyrics , the
vocabulary
used and the
969"
IN THIS,0.5269607843137255,"situation
portrayed
are
relevant to the country ’s linguistic
970"
IN THIS,0.5274509803921569,"heritage
and identity , in that
971"
IN THIS,0.5279411764705882,"46
972"
IN THIS,0.5284313725490196,"47 Options:
973"
IN THIS,0.528921568627451,"48 (A) physical
and
symbolic
violence
against
enslaved
people.
974"
IN THIS,0.5294117647058824,"49 (B) value the
influences of African
culture on national
music.
975"
IN THIS,0.5299019607843137,"50 (C) relativize
the
syncretism
that
makes up Brazilian
religious
976"
IN THIS,0.5303921568627451,"practices.
977"
IN THIS,0.5308823529411765,"51 (D) narrate
the
misfortunes of the love
relationship
between
members
978"
IN THIS,0.5313725490196078,"of different
social
classes.
979"
IN THIS,0.5318627450980392,"52 (E) problematize
the
different
worldviews in society
during the
980"
IN THIS,0.5323529411764706,"colonial
period.
981"
IN THIS,0.532843137254902,"53
982"
IN THIS,0.5333333333333333,"54 Answer: (A) physical
and
symbolic
violence
against
enslaved
people
983"
IN THIS,0.5338235294117647,"55
984"
IN THIS,0.5343137254901961,"56 Question:
985"
"THE
SITUATION OF THE
WORKING
CLASS IN ENGLAND",0.5348039215686274,"57 The
situation of the
working
class in England
986"
"FRIEDRICH
ENGELS",0.5352941176470588,"58 Friedrich
Engels
987"
"FRIEDRICH
ENGELS",0.5357843137254902,"59
988"
"FRIEDRICH
ENGELS",0.5362745098039216,"60 At the same time , thanks to the ample
opportunities I have had to
989"
"FRIEDRICH
ENGELS",0.5367647058823529,"observe
the middle
classes , your
adversaries , I have
quickly
990"
"FRIEDRICH
ENGELS",0.5372549019607843,"concluded
that you are right , absolutely
right , not to expect any
991"
"FRIEDRICH
ENGELS",0.5377450980392157,"help from them. Its
interests
are
diametrically
opposed to yours ,
992"
"FRIEDRICH
ENGELS",0.538235294117647,"even if it constantly
tries to claim the
opposite
and wants to
993"
"FRIEDRICH
ENGELS",0.5387254901960784,"persuade
you that it feels the
greatest
sympathy
for your lot. But
994"
"FRIEDRICH
ENGELS",0.5392156862745098,"her
actions
belie her words.
995"
"FRIEDRICH
ENGELS",0.5397058823529411,"61
996"
"FRIEDRICH
ENGELS",0.5401960784313725,"62 In the text , the author
presents
ethical
outlines
that
correspond to
997"
"FRIEDRICH
ENGELS",0.5406862745098039,"63
998"
"FRIEDRICH
ENGELS",0.5411764705882353,"64 Options:
999"
"FRIEDRICH
ENGELS",0.5416666666666666,"65 (A) the
foundation of the idea of surplus
value.
1000"
"FRIEDRICH
ENGELS",0.542156862745098,"66 (B) concept of class
struggle.
1001"
"FRIEDRICH
ENGELS",0.5426470588235294,"67 (C) fundamentals of the
scientific
method.
1002"
"FRIEDRICH
ENGELS",0.5431372549019607,"68 (D) paradigms of the
inquiry
process.
1003"
"FRIEDRICH
ENGELS",0.5436274509803921,"69 (E) domains of commodity
fetishism.
1004"
"FRIEDRICH
ENGELS",0.5441176470588235,"70
1005"
"FRIEDRICH
ENGELS",0.5446078431372549,"71 Answer: (B) concept of class
struggle.
1006"
"FRIEDRICH
ENGELS",0.5450980392156862,"72
1007"
"FRIEDRICH
ENGELS",0.5455882352941176,"73 Question:
1008"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.546078431372549,"74 A hamburger
chain has three
franchises in different
cities. To include
1009"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.5465686274509803,"a new type of snack on the menu , the chain ’s marketing
manager
1010"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.5470588235294118,"suggested
putting
five new types of snacks on sale in special
1011"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.5475490196078432,"editions. The snacks
were
offered
for the same
period of time to
1012"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.5480392156862746,"all the
franchisees. The type with the
highest
average
sold per
1013"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.5485294117647059,"franchise
would be permanently
included on the menu. At the end of
1014"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.5490196078431373,"the trial period , management
received a report
describing
the
1015"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.5495098039215687,"quantities sold , in units , of each of the five
types of snacks in
1016"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.55,"the three
franchises.
1017"
"A HAMBURGER
CHAIN HAS THREE
FRANCHISES IN DIFFERENT",0.5504901960784314,"75
1018"
IMAGE,0.5509803921568628,"76 Image
description: The table
shows the
quantity
sold of each type of
1019"
IMAGE,0.5514705882352942,"snack in franchises 1, 2, and 3.
1020"
IMAGE,0.5519607843137255,"77 Franchise 1 sold 415 type -1 snacks , 395 type -2 snacks , 425 type -3
1021"
IMAGE,0.5524509803921569,"snacks , 430 type -4 snacks , and 435 type -5 snacks.
1022"
IMAGE,0.5529411764705883,"78 Franchise 2 sold 415 type -1 snacks; 445 type -2 snacks; 370 type -3
1023"
IMAGE,0.5534313725490196,"snacks; 370 type -4 snacks and 425 type -5 snacks.
1024"
IMAGE,0.553921568627451,"79 Franchise 3 sold 415 type -1 snacks; 390 type -2 snacks; 425 type -3
1025"
IMAGE,0.5544117647058824,"snacks; 433 type -4 snacks and 420 type -5 snacks.
1026"
IMAGE,0.5549019607843138,"80
1027"
BASED ON THIS,0.5553921568627451,"81 Based on this
information , the
management
has
decided to include
the
1028"
BASED ON THIS,0.5558823529411765,"following
type of snack on the menu
1029"
BASED ON THIS,0.5563725490196079,"82
1030"
BASED ON THIS,0.5568627450980392,"83 Options:
1031"
BASED ON THIS,0.5573529411764706,"84 (A) 1
1032"
BASED ON THIS,0.557843137254902,"85 (B) 2
1033"
BASED ON THIS,0.5583333333333333,"86 (C) 3
1034"
BASED ON THIS,0.5588235294117647,"87 (D) 4
1035"
BASED ON THIS,0.5593137254901961,"88 (E) 5
1036"
BASED ON THIS,0.5598039215686275,"89
1037"
BASED ON THIS,0.5602941176470588,"90 Answer: (E) 5
1038"
BASED ON THIS,0.5607843137254902,"91
1039"
BASED ON THIS,0.5612745098039216,"92 Question: {QUESTION}
1040"
BASED ON THIS,0.5617647058823529,"93 Options:
1041"
BASED ON THIS,0.5622549019607843,"94 (A) {OPTION_A}
1042"
BASED ON THIS,0.5627450980392157,"95 (B) {OPTION_B}
1043"
BASED ON THIS,0.5632352941176471,"96 (C) {OPTION_C}
1044"
BASED ON THIS,0.5637254901960784,"97 (D) {OPTION_D}
1045"
BASED ON THIS,0.5642156862745098,"98 (E) {OPTION_E}
1046"
BASED ON THIS,0.5647058823529412,"99 Answer: (
1047"
BASED ON THIS,0.5651960784313725,Listing 8: 4-shot prompt used across all four subjects.
BASED ON THIS,0.5656862745098039,"A.5
Compute Resources
1048"
BASED ON THIS,0.5661764705882353,"We used GPUs (V100 or A100) provided by a university cluster6. For the main experiments, we
1049"
BASED ON THIS,0.5666666666666667,"used around 200 hours of GPU time (roughly 20 hours per model). Moreover, we used the OpenAI
1050"
BASED ON THIS,0.567156862745098,"API to run the experiments with GPT3.5.
1051"
BASED ON THIS,0.5676470588235294,"A.6
Zero and One Shot prompting Results for 2023
1052"
BASED ON THIS,0.5681372549019608,"A.6.1
CTT and IRT θ
1053 -2 -1 0 1 2 3"
BASED ON THIS,0.5686274509803921,IRT score
LANGUAGES AND CODES,0.5691176470588235,"2023 Languages and Codes
2023 Humanities"
LANGUAGES AND CODES,0.5696078431372549,"0
15
30
45"
LANGUAGES AND CODES,0.5700980392156862,CTT score -2 -1 0 1 2 3
LANGUAGES AND CODES,0.5705882352941176,IRT score
NATURAL SCIENCES,0.571078431372549,2023 Natural Sciences
NATURAL SCIENCES,0.5715686274509804,"0
15
30
45"
NATURAL SCIENCES,0.5720588235294117,CTT score
MATHEMATICS,0.5725490196078431,2023 Mathematics
MATHEMATICS,0.5730392156862745,"ChatGPT-3.5 (EN)
ChatGPT-3.5 (PT-BR)
Gemma-7B (EN)
Gemma-7B (PT-BR)
LLaMA2-13B (EN)
LLaMA2-13B (PT-BR)
LLaMA2-7B (EN)
LLaMA2-7B (PT-BR)
LLaMA3-8B (EN)
LLaMA3-8B (PT-BR)
Mistral-7B (EN)
Mistral-7B (PT-BR)"
MATHEMATICS,0.5735294117647058,"Figure 7: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM
2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with zero-shot. LLM
datapoints are computed from different shuffles."
MATHEMATICS,0.5740196078431372,6We will disclose it after the reviewing phase due to the double-blind process. -2 -1 0 1 2 3
MATHEMATICS,0.5745098039215686,IRT score
LANGUAGES AND CODES,0.575,"2023 Languages and Codes
2023 Humanities"
LANGUAGES AND CODES,0.5754901960784313,"0
15
30
45"
LANGUAGES AND CODES,0.5759803921568627,CTT score -2 -1 0 1 2 3
LANGUAGES AND CODES,0.5764705882352941,IRT score
NATURAL SCIENCES,0.5769607843137254,2023 Natural Sciences
NATURAL SCIENCES,0.5774509803921568,"0
15
30
45"
NATURAL SCIENCES,0.5779411764705882,CTT score
MATHEMATICS,0.5784313725490197,2023 Mathematics
MATHEMATICS,0.578921568627451,"Gemma-7B Instruct (EN)
Gemma-7B Instruct (PT-BR)
LLaMA2-13B Instruct (EN)
LLaMA2-13B Instruct (PT-BR)
LLaMA2-7B Instruct (EN)
LLaMA2-7B Instruct (PT-BR)
LLaMA3-8B Instruct (EN)
LLaMA3-8B Instruct (PT-BR)
Mistral-7B Instruct (EN)
Mistral-7B Instruct (PT-BR)"
MATHEMATICS,0.5794117647058824,"Figure 8: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023
exam. LLMs are instructed tuned open source models with zero-shot. LLM datapoints are computed
from different shuffles. -2 -1 0 1 2 3"
MATHEMATICS,0.5799019607843138,IRT score
LANGUAGES AND CODES,0.5803921568627451,"2023 Languages and Codes
2023 Humanities"
LANGUAGES AND CODES,0.5808823529411765,"0
15
30
45"
LANGUAGES AND CODES,0.5813725490196079,CTT score -2 -1 0 1 2 3
LANGUAGES AND CODES,0.5818627450980393,IRT score
NATURAL SCIENCES,0.5823529411764706,2023 Natural Sciences
NATURAL SCIENCES,0.582843137254902,"0
15
30
45"
NATURAL SCIENCES,0.5833333333333334,CTT score
MATHEMATICS,0.5838235294117647,2023 Mathematics
MATHEMATICS,0.5843137254901961,"ChatGPT-3.5 (EN)
ChatGPT-3.5 (PT-BR)
Gemma-7B (EN)
Gemma-7B (PT-BR)
LLaMA2-13B (EN)
LLaMA2-13B (PT-BR)
LLaMA2-7B (EN)
LLaMA2-7B (PT-BR)
LLaMA3-8B (EN)
LLaMA3-8B (PT-BR)
Mistral-7B (EN)
Mistral-7B (PT-BR)"
MATHEMATICS,0.5848039215686275,"Figure 9: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM
2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with one-shot. LLM
datapoints are computed from different shuffles. -2 -1 0 1 2 3"
MATHEMATICS,0.5852941176470589,IRT score
LANGUAGES AND CODES,0.5857843137254902,"2023 Languages and Codes
2023 Humanities"
LANGUAGES AND CODES,0.5862745098039216,"0
15
30
45"
LANGUAGES AND CODES,0.586764705882353,CTT score -2 -1 0 1 2 3
LANGUAGES AND CODES,0.5872549019607843,IRT score
NATURAL SCIENCES,0.5877450980392157,2023 Natural Sciences
NATURAL SCIENCES,0.5882352941176471,"0
15
30
45"
NATURAL SCIENCES,0.5887254901960784,CTT score
MATHEMATICS,0.5892156862745098,2023 Mathematics
MATHEMATICS,0.5897058823529412,"Gemma-7B Instruct (EN)
Gemma-7B Instruct (PT-BR)
LLaMA2-13B Instruct (EN)
LLaMA2-13B Instruct (PT-BR)
LLaMA2-7B Instruct (EN)
LLaMA2-7B Instruct (PT-BR)
LLaMA3-8B Instruct (EN)
LLaMA3-8B Instruct (PT-BR)
Mistral-7B Instruct (EN)
Mistral-7B Instruct (PT-BR)"
MATHEMATICS,0.5901960784313726,"Figure 10: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM
2023 exam. LLMs are instructed tuned open source models with one-shot. LLM datapoints are
computed from different shuffles."
MATHEMATICS,0.5906862745098039,"A.6.2
Response Patterns
1054"
MATHEMATICS,0.5911764705882353,"We show 43 items for the 2023 Math exam, instead of 45, because 2 items failed to converge and
1055"
MATHEMATICS,0.5916666666666667,"produce item parameters when the ENEM organizers fitted the human model.
1056 1
45"
MATHEMATICS,0.592156862745098,"ChatGPT-3.5 (EN)
ChatGPT-3.5 (PT-BR)"
MATHEMATICS,0.5926470588235294,"Gemma-7B (EN)
Gemma-7B (PT-BR)"
MATHEMATICS,0.5931372549019608,"LLaMA2-13B (EN)
LLaMA2-13B (PT-BR)"
MATHEMATICS,0.5936274509803922,"LLaMA2-7B (EN)
LLaMA2-7B (PT-BR)"
MATHEMATICS,0.5941176470588235,"LLaMA3-8B (EN)
LLaMA3-8B (PT-BR)"
MATHEMATICS,0.5946078431372549,"Mistral-7B (EN)
Mistral-7B (PT-BR)"
LANGUAGES AND CODES,0.5950980392156863,"2023 Languages and Codes 1
45"
NATURAL SCIENCES,0.5955882352941176,2023 Natural Sciences
NATURAL SCIENCES,0.596078431372549,"1
45
Question"
NATURAL SCIENCES,0.5965686274509804,"ChatGPT-3.5 (EN)
ChatGPT-3.5 (PT-BR)"
NATURAL SCIENCES,0.5970588235294118,"Gemma-7B (EN)
Gemma-7B (PT-BR)"
NATURAL SCIENCES,0.5975490196078431,"LLaMA2-13B (EN)
LLaMA2-13B (PT-BR)"
NATURAL SCIENCES,0.5980392156862745,"LLaMA2-7B (EN)
LLaMA2-7B (PT-BR)"
NATURAL SCIENCES,0.5985294117647059,"LLaMA3-8B (EN)
LLaMA3-8B (PT-BR)"
NATURAL SCIENCES,0.5990196078431372,"Mistral-7B (EN)
Mistral-7B (PT-BR)"
HUMANITIES,0.5995098039215686,2023 Humanities
HUMANITIES,0.6,"1
43
Question"
MATHEMATICS,0.6004901960784313,2023 Mathematics
MATHEMATICS,0.6009803921568627,"Figure 11: Response patterns for each LLM, where darker indicates more often correct. Questions
are sorted by difficulty (β value). LLMs are non-instructed tuned open source models and GPT3.5
with zero-shot. 1
45"
MATHEMATICS,0.6014705882352941,"Gemma-7B Instruct (EN)
Gemma-7B Instruct (PT-BR)"
MATHEMATICS,0.6019607843137255,"LLaMA2-13B Instruct (EN)
LLaMA2-13B Instruct (PT-BR)"
MATHEMATICS,0.6024509803921568,"LLaMA2-7B Instruct (EN)
LLaMA2-7B Instruct (PT-BR)"
MATHEMATICS,0.6029411764705882,"LLaMA3-8B Instruct (EN)
LLaMA3-8B Instruct (PT-BR)"
MATHEMATICS,0.6034313725490196,"Mistral-7B Instruct (EN)
Mistral-7B Instruct (PT-BR)"
LANGUAGES AND CODES,0.6039215686274509,"2023 Languages and Codes 1
45"
NATURAL SCIENCES,0.6044117647058823,2023 Natural Sciences
NATURAL SCIENCES,0.6049019607843137,"1
45
Question"
NATURAL SCIENCES,0.6053921568627451,"Gemma-7B Instruct (EN)
Gemma-7B Instruct (PT-BR)"
NATURAL SCIENCES,0.6058823529411764,"LLaMA2-13B Instruct (EN)
LLaMA2-13B Instruct (PT-BR)"
NATURAL SCIENCES,0.6063725490196078,"LLaMA2-7B Instruct (EN)
LLaMA2-7B Instruct (PT-BR)"
NATURAL SCIENCES,0.6068627450980392,"LLaMA3-8B Instruct (EN)
LLaMA3-8B Instruct (PT-BR)"
NATURAL SCIENCES,0.6073529411764705,"Mistral-7B Instruct (EN)
Mistral-7B Instruct (PT-BR)"
HUMANITIES,0.6078431372549019,2023 Humanities
HUMANITIES,0.6083333333333333,"1
43
Question"
MATHEMATICS,0.6088235294117647,2023 Mathematics
MATHEMATICS,0.609313725490196,"Figure 12: Response patterns for each LLM, where darker indicates more often correct. Questions
are sorted by difficulty (β value). LLMs are instructed tuned open source models with zero-shot. 1
45"
MATHEMATICS,0.6098039215686275,"ChatGPT-3.5 (EN)
ChatGPT-3.5 (PT-BR)"
MATHEMATICS,0.6102941176470589,"Gemma-7B (EN)
Gemma-7B (PT-BR)"
MATHEMATICS,0.6107843137254902,"LLaMA2-13B (EN)
LLaMA2-13B (PT-BR)"
MATHEMATICS,0.6112745098039216,"LLaMA2-7B (EN)
LLaMA2-7B (PT-BR)"
MATHEMATICS,0.611764705882353,"LLaMA3-8B (EN)
LLaMA3-8B (PT-BR)"
MATHEMATICS,0.6122549019607844,"Mistral-7B (EN)
Mistral-7B (PT-BR)"
LANGUAGES AND CODES,0.6127450980392157,"2023 Languages and Codes 1
45"
NATURAL SCIENCES,0.6132352941176471,2023 Natural Sciences
NATURAL SCIENCES,0.6137254901960785,"1
45
Question"
NATURAL SCIENCES,0.6142156862745098,"ChatGPT-3.5 (EN)
ChatGPT-3.5 (PT-BR)"
NATURAL SCIENCES,0.6147058823529412,"Gemma-7B (EN)
Gemma-7B (PT-BR)"
NATURAL SCIENCES,0.6151960784313726,"LLaMA2-13B (EN)
LLaMA2-13B (PT-BR)"
NATURAL SCIENCES,0.615686274509804,"LLaMA2-7B (EN)
LLaMA2-7B (PT-BR)"
NATURAL SCIENCES,0.6161764705882353,"LLaMA3-8B (EN)
LLaMA3-8B (PT-BR)"
NATURAL SCIENCES,0.6166666666666667,"Mistral-7B (EN)
Mistral-7B (PT-BR)"
HUMANITIES,0.6171568627450981,2023 Humanities
HUMANITIES,0.6176470588235294,"1
43
Question"
MATHEMATICS,0.6181372549019608,2023 Mathematics
MATHEMATICS,0.6186274509803922,"Figure 13: Response patterns for each LLM, where darker indicates more often correct. Questions
are sorted by difficulty (β value). LLMs are non-instructed tuned open source models and GPT3.5
with one-shot. 1
45"
MATHEMATICS,0.6191176470588236,"Gemma-7B Instruct (EN)
Gemma-7B Instruct (PT-BR)"
MATHEMATICS,0.6196078431372549,"LLaMA2-13B Instruct (EN)
LLaMA2-13B Instruct (PT-BR)"
MATHEMATICS,0.6200980392156863,"LLaMA2-7B Instruct (EN)
LLaMA2-7B Instruct (PT-BR)"
MATHEMATICS,0.6205882352941177,"LLaMA3-8B Instruct (EN)
LLaMA3-8B Instruct (PT-BR)"
MATHEMATICS,0.621078431372549,"Mistral-7B Instruct (EN)
Mistral-7B Instruct (PT-BR)"
LANGUAGES AND CODES,0.6215686274509804,"2023 Languages and Codes 1
45"
NATURAL SCIENCES,0.6220588235294118,2023 Natural Sciences
NATURAL SCIENCES,0.6225490196078431,"1
45
Question"
NATURAL SCIENCES,0.6230392156862745,"Gemma-7B Instruct (EN)
Gemma-7B Instruct (PT-BR)"
NATURAL SCIENCES,0.6235294117647059,"LLaMA2-13B Instruct (EN)
LLaMA2-13B Instruct (PT-BR)"
NATURAL SCIENCES,0.6240196078431373,"LLaMA2-7B Instruct (EN)
LLaMA2-7B Instruct (PT-BR)"
NATURAL SCIENCES,0.6245098039215686,"LLaMA3-8B Instruct (EN)
LLaMA3-8B Instruct (PT-BR)"
NATURAL SCIENCES,0.625,"Mistral-7B Instruct (EN)
Mistral-7B Instruct (PT-BR)"
HUMANITIES,0.6254901960784314,2023 Humanities
HUMANITIES,0.6259803921568627,"1
43
Question"
MATHEMATICS,0.6264705882352941,2023 Mathematics
MATHEMATICS,0.6269607843137255,"Figure 14: Response patterns for each LLM, where darker indicates more often correct. Questions
are sorted by difficulty (β value). LLMs are instructed tuned open source models with one-shot."
MATHEMATICS,0.6274509803921569,"A.6.3
Comparing IRT θ and lz
1057"
MATHEMATICS,0.6279411764705882,"6
4
2
0
2
4"
MATHEMATICS,0.6284313725490196,lz score 2 1 0 1 2 3 4
MATHEMATICS,0.628921568627451,IRT score
LANGUAGES AND CODES,0.6294117647058823,2023 Languages and Codes
LANGUAGES AND CODES,0.6299019607843137,"6
4
2
0
2
4"
LANGUAGES AND CODES,0.6303921568627451,lz score
HUMANITIES,0.6308823529411764,2023 Humanities
HUMANITIES,0.6313725490196078,"6
4
2
0
2
4"
HUMANITIES,0.6318627450980392,lz score
NATURAL SCIENCES,0.6323529411764706,2023 Natural Sciences
NATURAL SCIENCES,0.6328431372549019,"6
4
2
0
2
4"
NATURAL SCIENCES,0.6333333333333333,lz score
MATHEMATICS,0.6338235294117647,2023 Mathematics
MATHEMATICS,0.634313725490196,"MODEL
ChatGPT-3.5
Gemma-7B
LLaMA2-13B
LLaMA2-7B
LLaMA3-8B
Mistral-7B
LANGUAGE
EN
PT-BR"
MATHEMATICS,0.6348039215686274,"Figure 15: Distribution of lz and IRT scores for humans and LLMs in the ENEM 2023 exam.
LLMs are non-instructed tuned open source models and GPT3.5 with zero-shot. LLM datapoints
are computed from different shuffles."
MATHEMATICS,0.6352941176470588,"6
4
2
0
2
4"
MATHEMATICS,0.6357843137254902,lz score 2 1 0 1 2 3 4
MATHEMATICS,0.6362745098039215,IRT score
LANGUAGES AND CODES,0.6367647058823529,2023 Languages and Codes
LANGUAGES AND CODES,0.6372549019607843,"6
4
2
0
2
4"
LANGUAGES AND CODES,0.6377450980392156,lz score
HUMANITIES,0.638235294117647,2023 Humanities
HUMANITIES,0.6387254901960784,"6
4
2
0
2
4"
HUMANITIES,0.6392156862745098,lz score
NATURAL SCIENCES,0.6397058823529411,2023 Natural Sciences
NATURAL SCIENCES,0.6401960784313725,"6
4
2
0
2
4"
NATURAL SCIENCES,0.640686274509804,lz score
MATHEMATICS,0.6411764705882353,2023 Mathematics
MATHEMATICS,0.6416666666666667,"MODEL
Gemma-7B Instruct
LLaMA2-13B Instruct
LLaMA2-7B Instruct
LLaMA3-8B Instruct
Mistral-7B Instruct
LANGUAGE
EN
PT-BR"
MATHEMATICS,0.6421568627450981,"Figure 16: Distribution of lz and IRT scores for humans and LLMs in the ENEM 2023 exam.
LLMs are instructed tuned open source models with zero-shot. LLM datapoints are computed from
different shuffles."
MATHEMATICS,0.6426470588235295,"6
4
2
0
2
4"
MATHEMATICS,0.6431372549019608,lz score 2 1 0 1 2 3 4
MATHEMATICS,0.6436274509803922,IRT score
LANGUAGES AND CODES,0.6441176470588236,2023 Languages and Codes
LANGUAGES AND CODES,0.6446078431372549,"6
4
2
0
2
4"
LANGUAGES AND CODES,0.6450980392156863,lz score
HUMANITIES,0.6455882352941177,2023 Humanities
HUMANITIES,0.6460784313725491,"6
4
2
0
2
4"
HUMANITIES,0.6465686274509804,lz score
NATURAL SCIENCES,0.6470588235294118,2023 Natural Sciences
NATURAL SCIENCES,0.6475490196078432,"6
4
2
0
2
4"
NATURAL SCIENCES,0.6480392156862745,lz score
MATHEMATICS,0.6485294117647059,2023 Mathematics
MATHEMATICS,0.6490196078431373,"MODEL
ChatGPT-3.5
Gemma-7B
LLaMA2-13B
LLaMA2-7B
LLaMA3-8B
Mistral-7B
LANGUAGE
EN
PT-BR"
MATHEMATICS,0.6495098039215687,"Figure 17: Distribution of lz and IRT scores for humans and LLMs in the ENEM 2023 exam.
LLMs are non-instructed tuned open source models and GPT3.5 with one-shot. LLM datapoints are
computed from different shuffles."
MATHEMATICS,0.65,"6
4
2
0
2
4"
MATHEMATICS,0.6504901960784314,lz score 2 1 0 1 2 3 4
MATHEMATICS,0.6509803921568628,IRT score
LANGUAGES AND CODES,0.6514705882352941,2023 Languages and Codes
LANGUAGES AND CODES,0.6519607843137255,"6
4
2
0
2
4"
LANGUAGES AND CODES,0.6524509803921569,lz score
HUMANITIES,0.6529411764705882,2023 Humanities
HUMANITIES,0.6534313725490196,"6
4
2
0
2
4"
HUMANITIES,0.653921568627451,lz score
NATURAL SCIENCES,0.6544117647058824,2023 Natural Sciences
NATURAL SCIENCES,0.6549019607843137,"6
4
2
0
2
4"
NATURAL SCIENCES,0.6553921568627451,lz score
MATHEMATICS,0.6558823529411765,2023 Mathematics
MATHEMATICS,0.6563725490196078,"MODEL
Gemma-7B Instruct
LLaMA2-13B Instruct
LLaMA2-7B Instruct
LLaMA3-8B Instruct
Mistral-7B Instruct
LANGUAGE
EN
PT-BR"
MATHEMATICS,0.6568627450980392,"Figure 18: Distribution of lz and IRT scores for humans and LLMs in the ENEM 2023 exam. LLMs
are instructed tuned open source models with one-shot. LLM datapoints are computed from different
shuffles."
MATHEMATICS,0.6573529411764706,"A.7
CTT and IRT θ for 2022
1058 -2 -1 0 1 2 3"
MATHEMATICS,0.657843137254902,IRT score
LANGUAGES AND CODES,0.6583333333333333,"2022 Languages and Codes
2022 Humanities"
LANGUAGES AND CODES,0.6588235294117647,"0
15
30
45"
LANGUAGES AND CODES,0.6593137254901961,CTT score -2 -1 0 1 2 3
LANGUAGES AND CODES,0.6598039215686274,IRT score
NATURAL SCIENCES,0.6602941176470588,2022 Natural Sciences
NATURAL SCIENCES,0.6607843137254902,"0
15
30
45"
NATURAL SCIENCES,0.6612745098039216,CTT score
MATHEMATICS,0.6617647058823529,2022 Mathematics
MATHEMATICS,0.6622549019607843,"ChatGPT-3.5 (EN)
ChatGPT-3.5 (PT-BR)
Gemma-7B (EN)
Gemma-7B (PT-BR)
LLaMA2-13B (EN)
LLaMA2-13B (PT-BR)
LLaMA2-7B (EN)
LLaMA2-7B (PT-BR)
LLaMA3-8B (EN)
LLaMA3-8B (PT-BR)
Mistral-7B (EN)
Mistral-7B (PT-BR)"
MATHEMATICS,0.6627450980392157,"Figure 19: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM
2022 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM
datapoints are computed from different shuffles. -2 -1 0 1 2 3"
MATHEMATICS,0.663235294117647,IRT score
LANGUAGES AND CODES,0.6637254901960784,"2022 Languages and Codes
2022 Humanities"
LANGUAGES AND CODES,0.6642156862745098,"0
15
30
45"
LANGUAGES AND CODES,0.6647058823529411,CTT score -2 -1 0 1 2 3
LANGUAGES AND CODES,0.6651960784313725,IRT score
NATURAL SCIENCES,0.6656862745098039,2022 Natural Sciences
NATURAL SCIENCES,0.6661764705882353,"0
15
30
45"
NATURAL SCIENCES,0.6666666666666666,CTT score
MATHEMATICS,0.667156862745098,2022 Mathematics
MATHEMATICS,0.6676470588235294,"Gemma-7B Instruct (EN)
Gemma-7B Instruct (PT-BR)
LLaMA2-13B Instruct (EN)
LLaMA2-13B Instruct (PT-BR)
LLaMA2-7B Instruct (EN)
LLaMA2-7B Instruct (PT-BR)
LLaMA3-8B Instruct (EN)
LLaMA3-8B Instruct (PT-BR)
Mistral-7B Instruct (EN)
Mistral-7B Instruct (PT-BR)"
MATHEMATICS,0.6681372549019607,"Figure 20: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM
2022 exam. LLMs are instructed tuned open source models with four-shot. LLM datapoints are
computed from different shuffles. -2 -1 0 1 2 3"
MATHEMATICS,0.6686274509803921,IRT score
LANGUAGES AND CODES,0.6691176470588235,"2022 Languages and Codes
2022 Humanities"
LANGUAGES AND CODES,0.6696078431372549,"0
15
30
45"
LANGUAGES AND CODES,0.6700980392156862,CTT score -2 -1 0 1 2 3
LANGUAGES AND CODES,0.6705882352941176,IRT score
NATURAL SCIENCES,0.671078431372549,2022 Natural Sciences
NATURAL SCIENCES,0.6715686274509803,"0
15
30
45"
NATURAL SCIENCES,0.6720588235294118,CTT score
MATHEMATICS,0.6725490196078432,2022 Mathematics
MATHEMATICS,0.6730392156862746,"ChatGPT-3.5 (EN)
ChatGPT-3.5 (PT-BR)
Gemma-7B (EN)
Gemma-7B (PT-BR)
LLaMA2-13B (EN)
LLaMA2-13B (PT-BR)
LLaMA2-7B (EN)
LLaMA2-7B (PT-BR)
LLaMA3-8B (EN)
LLaMA3-8B (PT-BR)
Mistral-7B (EN)
Mistral-7B (PT-BR)"
MATHEMATICS,0.6735294117647059,"Figure 21: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM
2022 exam. LLMs are non-instructed tuned open source models and GPT3.5 with zero-shot. LLM
datapoints are computed from different shuffles. -2 -1 0 1 2 3"
MATHEMATICS,0.6740196078431373,IRT score
LANGUAGES AND CODES,0.6745098039215687,"2022 Languages and Codes
2022 Humanities"
LANGUAGES AND CODES,0.675,"0
15
30
45"
LANGUAGES AND CODES,0.6754901960784314,CTT score -2 -1 0 1 2 3
LANGUAGES AND CODES,0.6759803921568628,IRT score
NATURAL SCIENCES,0.6764705882352942,2022 Natural Sciences
NATURAL SCIENCES,0.6769607843137255,"0
15
30
45"
NATURAL SCIENCES,0.6774509803921569,CTT score
MATHEMATICS,0.6779411764705883,2022 Mathematics
MATHEMATICS,0.6784313725490196,"Gemma-7B Instruct (EN)
Gemma-7B Instruct (PT-BR)
LLaMA2-13B Instruct (EN)
LLaMA2-13B Instruct (PT-BR)
LLaMA2-7B Instruct (EN)
LLaMA2-7B Instruct (PT-BR)
LLaMA3-8B Instruct (EN)
LLaMA3-8B Instruct (PT-BR)
Mistral-7B Instruct (EN)
Mistral-7B Instruct (PT-BR)"
MATHEMATICS,0.678921568627451,"Figure 22: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM
2022 exam. LLMs are instructed tuned open source models with zero-shot. LLM datapoints are
computed from different shuffles. -2 -1 0 1 2 3"
MATHEMATICS,0.6794117647058824,IRT score
LANGUAGES AND CODES,0.6799019607843138,"2022 Languages and Codes
2022 Humanities"
LANGUAGES AND CODES,0.6803921568627451,"0
15
30
45"
LANGUAGES AND CODES,0.6808823529411765,CTT score -2 -1 0 1 2 3
LANGUAGES AND CODES,0.6813725490196079,IRT score
NATURAL SCIENCES,0.6818627450980392,2022 Natural Sciences
NATURAL SCIENCES,0.6823529411764706,"0
15
30
45"
NATURAL SCIENCES,0.682843137254902,CTT score
MATHEMATICS,0.6833333333333333,2022 Mathematics
MATHEMATICS,0.6838235294117647,"ChatGPT-3.5 (EN)
ChatGPT-3.5 (PT-BR)
Gemma-7B (EN)
Gemma-7B (PT-BR)
LLaMA2-13B (EN)
LLaMA2-13B (PT-BR)
LLaMA2-7B (EN)
LLaMA2-7B (PT-BR)
LLaMA3-8B (EN)
LLaMA3-8B (PT-BR)
Mistral-7B (EN)
Mistral-7B (PT-BR)"
MATHEMATICS,0.6843137254901961,"Figure 23: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM
2022 exam. LLMs are non-instructed tuned open source models and GPT3.5 with one-shot. LLM
datapoints are computed from different shuffles. -2 -1 0 1 2 3"
MATHEMATICS,0.6848039215686275,IRT score
LANGUAGES AND CODES,0.6852941176470588,"2022 Languages and Codes
2022 Humanities"
LANGUAGES AND CODES,0.6857843137254902,"0
15
30
45"
LANGUAGES AND CODES,0.6862745098039216,CTT score -2 -1 0 1 2 3
LANGUAGES AND CODES,0.6867647058823529,IRT score
NATURAL SCIENCES,0.6872549019607843,2022 Natural Sciences
NATURAL SCIENCES,0.6877450980392157,"0
15
30
45"
NATURAL SCIENCES,0.6882352941176471,CTT score
MATHEMATICS,0.6887254901960784,2022 Mathematics
MATHEMATICS,0.6892156862745098,"Gemma-7B Instruct (EN)
Gemma-7B Instruct (PT-BR)
LLaMA2-13B Instruct (EN)
LLaMA2-13B Instruct (PT-BR)
LLaMA2-7B Instruct (EN)
LLaMA2-7B Instruct (PT-BR)
LLaMA3-8B Instruct (EN)
LLaMA3-8B Instruct (PT-BR)
Mistral-7B Instruct (EN)
Mistral-7B Instruct (PT-BR)"
MATHEMATICS,0.6897058823529412,"Figure 24: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM
2022 exam. LLMs are instructed tuned open source models with one-shot. LLM datapoints are
computed from different shuffles."
MATHEMATICS,0.6901960784313725,"A.8
Response Patterns for 2022
1059 1
45"
MATHEMATICS,0.6906862745098039,"ChatGPT-3.5 (EN)
ChatGPT-3.5 (PT-BR)"
MATHEMATICS,0.6911764705882353,"Gemma-7B (EN)
Gemma-7B (PT-BR)"
MATHEMATICS,0.6916666666666667,"LLaMA2-13B (EN)
LLaMA2-13B (PT-BR)"
MATHEMATICS,0.692156862745098,"LLaMA2-7B (EN)
LLaMA2-7B (PT-BR)"
MATHEMATICS,0.6926470588235294,"LLaMA3-8B (EN)
LLaMA3-8B (PT-BR)"
MATHEMATICS,0.6931372549019608,"Mistral-7B (EN)
Mistral-7B (PT-BR)"
LANGUAGES AND CODES,0.6936274509803921,"2022 Languages and Codes 1
41"
NATURAL SCIENCES,0.6941176470588235,2022 Natural Sciences
NATURAL SCIENCES,0.6946078431372549,"1
44
Question"
NATURAL SCIENCES,0.6950980392156862,"ChatGPT-3.5 (EN)
ChatGPT-3.5 (PT-BR)"
NATURAL SCIENCES,0.6955882352941176,"Gemma-7B (EN)
Gemma-7B (PT-BR)"
NATURAL SCIENCES,0.696078431372549,"LLaMA2-13B (EN)
LLaMA2-13B (PT-BR)"
NATURAL SCIENCES,0.6965686274509804,"LLaMA2-7B (EN)
LLaMA2-7B (PT-BR)"
NATURAL SCIENCES,0.6970588235294117,"LLaMA3-8B (EN)
LLaMA3-8B (PT-BR)"
NATURAL SCIENCES,0.6975490196078431,"Mistral-7B (EN)
Mistral-7B (PT-BR)"
HUMANITIES,0.6980392156862745,2022 Humanities
HUMANITIES,0.6985294117647058,"1
42
Question"
MATHEMATICS,0.6990196078431372,2022 Mathematics
MATHEMATICS,0.6995098039215686,"Figure 25: Response patterns for each LLM, where darker indicates more often correct. Questions
are sorted by difficulty (β value). LLMs are non-instructed tuned open source models and GPT3.5
with four-shot. 1
45"
MATHEMATICS,0.7,"Gemma-7B Instruct (EN)
Gemma-7B Instruct (PT-BR)"
MATHEMATICS,0.7004901960784313,"LLaMA2-13B Instruct (EN)
LLaMA2-13B Instruct (PT-BR)"
MATHEMATICS,0.7009803921568627,"LLaMA2-7B Instruct (EN)
LLaMA2-7B Instruct (PT-BR)"
MATHEMATICS,0.7014705882352941,"LLaMA3-8B Instruct (EN)
LLaMA3-8B Instruct (PT-BR)"
MATHEMATICS,0.7019607843137254,"Mistral-7B Instruct (EN)
Mistral-7B Instruct (PT-BR)"
LANGUAGES AND CODES,0.7024509803921568,"2022 Languages and Codes 1
41"
NATURAL SCIENCES,0.7029411764705882,2022 Natural Sciences
NATURAL SCIENCES,0.7034313725490197,"1
44
Question"
NATURAL SCIENCES,0.703921568627451,"Gemma-7B Instruct (EN)
Gemma-7B Instruct (PT-BR)"
NATURAL SCIENCES,0.7044117647058824,"LLaMA2-13B Instruct (EN)
LLaMA2-13B Instruct (PT-BR)"
NATURAL SCIENCES,0.7049019607843138,"LLaMA2-7B Instruct (EN)
LLaMA2-7B Instruct (PT-BR)"
NATURAL SCIENCES,0.7053921568627451,"LLaMA3-8B Instruct (EN)
LLaMA3-8B Instruct (PT-BR)"
NATURAL SCIENCES,0.7058823529411765,"Mistral-7B Instruct (EN)
Mistral-7B Instruct (PT-BR)"
HUMANITIES,0.7063725490196079,2022 Humanities
HUMANITIES,0.7068627450980393,"1
42
Question"
MATHEMATICS,0.7073529411764706,2022 Mathematics
MATHEMATICS,0.707843137254902,"Figure 26: Response patterns for each LLM, where darker indicates more often correct. Questions
are sorted by difficulty (β value). LLMs are instructed tuned open source models with four-shot. 1
45"
MATHEMATICS,0.7083333333333334,"ChatGPT-3.5 (EN)
ChatGPT-3.5 (PT-BR)"
MATHEMATICS,0.7088235294117647,"Gemma-7B (EN)
Gemma-7B (PT-BR)"
MATHEMATICS,0.7093137254901961,"LLaMA2-13B (EN)
LLaMA2-13B (PT-BR)"
MATHEMATICS,0.7098039215686275,"LLaMA2-7B (EN)
LLaMA2-7B (PT-BR)"
MATHEMATICS,0.7102941176470589,"LLaMA3-8B (EN)
LLaMA3-8B (PT-BR)"
MATHEMATICS,0.7107843137254902,"Mistral-7B (EN)
Mistral-7B (PT-BR)"
LANGUAGES AND CODES,0.7112745098039216,"2022 Languages and Codes 1
41"
NATURAL SCIENCES,0.711764705882353,2022 Natural Sciences
NATURAL SCIENCES,0.7122549019607843,"1
44
Question"
NATURAL SCIENCES,0.7127450980392157,"ChatGPT-3.5 (EN)
ChatGPT-3.5 (PT-BR)"
NATURAL SCIENCES,0.7132352941176471,"Gemma-7B (EN)
Gemma-7B (PT-BR)"
NATURAL SCIENCES,0.7137254901960784,"LLaMA2-13B (EN)
LLaMA2-13B (PT-BR)"
NATURAL SCIENCES,0.7142156862745098,"LLaMA2-7B (EN)
LLaMA2-7B (PT-BR)"
NATURAL SCIENCES,0.7147058823529412,"LLaMA3-8B (EN)
LLaMA3-8B (PT-BR)"
NATURAL SCIENCES,0.7151960784313726,"Mistral-7B (EN)
Mistral-7B (PT-BR)"
HUMANITIES,0.7156862745098039,2022 Humanities
HUMANITIES,0.7161764705882353,"1
42
Question"
MATHEMATICS,0.7166666666666667,2022 Mathematics
MATHEMATICS,0.717156862745098,"Figure 27: Response patterns for each LLM, where darker indicates more often correct. Questions
are sorted by difficulty (β value). LLMs are non-instructed tuned open source models and GPT3.5
with zero-shot. 1
45"
MATHEMATICS,0.7176470588235294,"Gemma-7B Instruct (EN)
Gemma-7B Instruct (PT-BR)"
MATHEMATICS,0.7181372549019608,"LLaMA2-13B Instruct (EN)
LLaMA2-13B Instruct (PT-BR)"
MATHEMATICS,0.7186274509803922,"LLaMA2-7B Instruct (EN)
LLaMA2-7B Instruct (PT-BR)"
MATHEMATICS,0.7191176470588235,"LLaMA3-8B Instruct (EN)
LLaMA3-8B Instruct (PT-BR)"
MATHEMATICS,0.7196078431372549,"Mistral-7B Instruct (EN)
Mistral-7B Instruct (PT-BR)"
LANGUAGES AND CODES,0.7200980392156863,"2022 Languages and Codes 1
41"
NATURAL SCIENCES,0.7205882352941176,2022 Natural Sciences
NATURAL SCIENCES,0.721078431372549,"1
44
Question"
NATURAL SCIENCES,0.7215686274509804,"Gemma-7B Instruct (EN)
Gemma-7B Instruct (PT-BR)"
NATURAL SCIENCES,0.7220588235294118,"LLaMA2-13B Instruct (EN)
LLaMA2-13B Instruct (PT-BR)"
NATURAL SCIENCES,0.7225490196078431,"LLaMA2-7B Instruct (EN)
LLaMA2-7B Instruct (PT-BR)"
NATURAL SCIENCES,0.7230392156862745,"LLaMA3-8B Instruct (EN)
LLaMA3-8B Instruct (PT-BR)"
NATURAL SCIENCES,0.7235294117647059,"Mistral-7B Instruct (EN)
Mistral-7B Instruct (PT-BR)"
HUMANITIES,0.7240196078431372,2022 Humanities
HUMANITIES,0.7245098039215686,"1
42
Question"
MATHEMATICS,0.725,2022 Mathematics
MATHEMATICS,0.7254901960784313,"Figure 28: Response patterns for each LLM, where darker indicates more often correct. Questions
are sorted by difficulty (β value). LLMs are instructed tuned open source models with zero-shot. 1
45"
MATHEMATICS,0.7259803921568627,"ChatGPT-3.5 (EN)
ChatGPT-3.5 (PT-BR)"
MATHEMATICS,0.7264705882352941,"Gemma-7B (EN)
Gemma-7B (PT-BR)"
MATHEMATICS,0.7269607843137255,"LLaMA2-13B (EN)
LLaMA2-13B (PT-BR)"
MATHEMATICS,0.7274509803921568,"LLaMA2-7B (EN)
LLaMA2-7B (PT-BR)"
MATHEMATICS,0.7279411764705882,"LLaMA3-8B (EN)
LLaMA3-8B (PT-BR)"
MATHEMATICS,0.7284313725490196,"Mistral-7B (EN)
Mistral-7B (PT-BR)"
LANGUAGES AND CODES,0.7289215686274509,"2022 Languages and Codes 1
41"
NATURAL SCIENCES,0.7294117647058823,2022 Natural Sciences
NATURAL SCIENCES,0.7299019607843137,"1
44
Question"
NATURAL SCIENCES,0.7303921568627451,"ChatGPT-3.5 (EN)
ChatGPT-3.5 (PT-BR)"
NATURAL SCIENCES,0.7308823529411764,"Gemma-7B (EN)
Gemma-7B (PT-BR)"
NATURAL SCIENCES,0.7313725490196078,"LLaMA2-13B (EN)
LLaMA2-13B (PT-BR)"
NATURAL SCIENCES,0.7318627450980392,"LLaMA2-7B (EN)
LLaMA2-7B (PT-BR)"
NATURAL SCIENCES,0.7323529411764705,"LLaMA3-8B (EN)
LLaMA3-8B (PT-BR)"
NATURAL SCIENCES,0.7328431372549019,"Mistral-7B (EN)
Mistral-7B (PT-BR)"
HUMANITIES,0.7333333333333333,2022 Humanities
HUMANITIES,0.7338235294117647,"1
42
Question"
MATHEMATICS,0.734313725490196,2022 Mathematics
MATHEMATICS,0.7348039215686275,"Figure 29: Response patterns for each LLM, where darker indicates more often correct. Questions
are sorted by difficulty (β value). LLMs are non-instructed tuned open source models and GPT3.5
with one-shot. 1
45"
MATHEMATICS,0.7352941176470589,"Gemma-7B Instruct (EN)
Gemma-7B Instruct (PT-BR)"
MATHEMATICS,0.7357843137254902,"LLaMA2-13B Instruct (EN)
LLaMA2-13B Instruct (PT-BR)"
MATHEMATICS,0.7362745098039216,"LLaMA2-7B Instruct (EN)
LLaMA2-7B Instruct (PT-BR)"
MATHEMATICS,0.736764705882353,"LLaMA3-8B Instruct (EN)
LLaMA3-8B Instruct (PT-BR)"
MATHEMATICS,0.7372549019607844,"Mistral-7B Instruct (EN)
Mistral-7B Instruct (PT-BR)"
LANGUAGES AND CODES,0.7377450980392157,"2022 Languages and Codes 1
41"
NATURAL SCIENCES,0.7382352941176471,2022 Natural Sciences
NATURAL SCIENCES,0.7387254901960785,"1
44
Question"
NATURAL SCIENCES,0.7392156862745098,"Gemma-7B Instruct (EN)
Gemma-7B Instruct (PT-BR)"
NATURAL SCIENCES,0.7397058823529412,"LLaMA2-13B Instruct (EN)
LLaMA2-13B Instruct (PT-BR)"
NATURAL SCIENCES,0.7401960784313726,"LLaMA2-7B Instruct (EN)
LLaMA2-7B Instruct (PT-BR)"
NATURAL SCIENCES,0.740686274509804,"LLaMA3-8B Instruct (EN)
LLaMA3-8B Instruct (PT-BR)"
NATURAL SCIENCES,0.7411764705882353,"Mistral-7B Instruct (EN)
Mistral-7B Instruct (PT-BR)"
HUMANITIES,0.7416666666666667,2022 Humanities
HUMANITIES,0.7421568627450981,"1
42
Question"
MATHEMATICS,0.7426470588235294,2022 Mathematics
MATHEMATICS,0.7431372549019608,"Figure 30: Response patterns for each LLM, where darker indicates more often correct. Questions
are sorted by difficulty (β value). LLMs are instructed tuned open source models with one-shot."
MATHEMATICS,0.7436274509803922,"A.9
Comparing IRT θ and lz for 2022
1060"
MATHEMATICS,0.7441176470588236,"6
4
2
0
2
4"
MATHEMATICS,0.7446078431372549,lz score 2 1 0 1 2 3 4
MATHEMATICS,0.7450980392156863,IRT score
LANGUAGES AND CODES,0.7455882352941177,2022 Languages and Codes
LANGUAGES AND CODES,0.746078431372549,"6
4
2
0
2
4"
LANGUAGES AND CODES,0.7465686274509804,lz score
HUMANITIES,0.7470588235294118,2022 Humanities
HUMANITIES,0.7475490196078431,"6
4
2
0
2
4"
HUMANITIES,0.7480392156862745,lz score
NATURAL SCIENCES,0.7485294117647059,2022 Natural Sciences
NATURAL SCIENCES,0.7490196078431373,"6
4
2
0
2
4"
NATURAL SCIENCES,0.7495098039215686,lz score
MATHEMATICS,0.75,2022 Mathematics
MATHEMATICS,0.7504901960784314,"MODEL
ChatGPT-3.5
Gemma-7B
LLaMA2-13B
LLaMA2-7B
LLaMA3-8B
Mistral-7B
LANGUAGE
EN
PT-BR"
MATHEMATICS,0.7509803921568627,"Figure 31: Distribution of lz and IRT scores for humans and LLMs in the ENEM 2022 exam.
LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints
are computed from different shuffles."
MATHEMATICS,0.7514705882352941,"6
4
2
0
2
4"
MATHEMATICS,0.7519607843137255,lz score 2 1 0 1 2 3 4
MATHEMATICS,0.7524509803921569,IRT score
LANGUAGES AND CODES,0.7529411764705882,2022 Languages and Codes
LANGUAGES AND CODES,0.7534313725490196,"6
4
2
0
2
4"
LANGUAGES AND CODES,0.753921568627451,lz score
HUMANITIES,0.7544117647058823,2022 Humanities
HUMANITIES,0.7549019607843137,"6
4
2
0
2
4"
HUMANITIES,0.7553921568627451,lz score
NATURAL SCIENCES,0.7558823529411764,2022 Natural Sciences
NATURAL SCIENCES,0.7563725490196078,"6
4
2
0
2
4"
NATURAL SCIENCES,0.7568627450980392,lz score
MATHEMATICS,0.7573529411764706,2022 Mathematics
MATHEMATICS,0.7578431372549019,"MODEL
Gemma-7B Instruct
LLaMA2-13B Instruct
LLaMA2-7B Instruct
LLaMA3-8B Instruct
Mistral-7B Instruct
LANGUAGE
EN
PT-BR"
MATHEMATICS,0.7583333333333333,"Figure 32: Distribution of lz and IRT scores for humans and LLMs in the ENEM 2022 exam.
LLMs are instructed tuned open source models with four-shot. LLM datapoints are computed from
different shuffles."
MATHEMATICS,0.7588235294117647,"6
4
2
0
2
4"
MATHEMATICS,0.759313725490196,lz score 2 1 0 1 2 3 4
MATHEMATICS,0.7598039215686274,IRT score
LANGUAGES AND CODES,0.7602941176470588,2022 Languages and Codes
LANGUAGES AND CODES,0.7607843137254902,"6
4
2
0
2
4"
LANGUAGES AND CODES,0.7612745098039215,lz score
HUMANITIES,0.7617647058823529,2022 Humanities
HUMANITIES,0.7622549019607843,"6
4
2
0
2
4"
HUMANITIES,0.7627450980392156,lz score
NATURAL SCIENCES,0.763235294117647,2022 Natural Sciences
NATURAL SCIENCES,0.7637254901960784,"6
4
2
0
2
4"
NATURAL SCIENCES,0.7642156862745098,lz score
MATHEMATICS,0.7647058823529411,2022 Mathematics
MATHEMATICS,0.7651960784313725,"MODEL
ChatGPT-3.5
Gemma-7B
LLaMA2-13B
LLaMA2-7B
LLaMA3-8B
Mistral-7B
LANGUAGE
EN
PT-BR"
MATHEMATICS,0.765686274509804,"Figure 33: Distribution of lz and IRT scores for humans and LLMs in the ENEM 2022 exam.
LLMs are non-instructed tuned open source models and GPT3.5 with zero-shot. LLM datapoints
are computed from different shuffles."
MATHEMATICS,0.7661764705882353,"6
4
2
0
2
4"
MATHEMATICS,0.7666666666666667,lz score 2 1 0 1 2 3 4
MATHEMATICS,0.7671568627450981,IRT score
LANGUAGES AND CODES,0.7676470588235295,2022 Languages and Codes
LANGUAGES AND CODES,0.7681372549019608,"6
4
2
0
2
4"
LANGUAGES AND CODES,0.7686274509803922,lz score
HUMANITIES,0.7691176470588236,2022 Humanities
HUMANITIES,0.7696078431372549,"6
4
2
0
2
4"
HUMANITIES,0.7700980392156863,lz score
NATURAL SCIENCES,0.7705882352941177,2022 Natural Sciences
NATURAL SCIENCES,0.7710784313725491,"6
4
2
0
2
4"
NATURAL SCIENCES,0.7715686274509804,lz score
MATHEMATICS,0.7720588235294118,2022 Mathematics
MATHEMATICS,0.7725490196078432,"MODEL
Gemma-7B Instruct
LLaMA2-13B Instruct
LLaMA2-7B Instruct
LLaMA3-8B Instruct
Mistral-7B Instruct
LANGUAGE
EN
PT-BR"
MATHEMATICS,0.7730392156862745,"Figure 34: Distribution of lz and IRT scores for humans and LLMs in the ENEM 2022 exam.
LLMs are instructed tuned open source models with zero-shot. LLM datapoints are computed from
different shuffles."
MATHEMATICS,0.7735294117647059,"6
4
2
0
2
4"
MATHEMATICS,0.7740196078431373,lz score 2 1 0 1 2 3 4
MATHEMATICS,0.7745098039215687,IRT score
LANGUAGES AND CODES,0.775,2022 Languages and Codes
LANGUAGES AND CODES,0.7754901960784314,"6
4
2
0
2
4"
LANGUAGES AND CODES,0.7759803921568628,lz score
HUMANITIES,0.7764705882352941,2022 Humanities
HUMANITIES,0.7769607843137255,"6
4
2
0
2
4"
HUMANITIES,0.7774509803921569,lz score
NATURAL SCIENCES,0.7779411764705882,2022 Natural Sciences
NATURAL SCIENCES,0.7784313725490196,"6
4
2
0
2
4"
NATURAL SCIENCES,0.778921568627451,lz score
MATHEMATICS,0.7794117647058824,2022 Mathematics
MATHEMATICS,0.7799019607843137,"MODEL
ChatGPT-3.5
Gemma-7B
LLaMA2-13B
LLaMA2-7B
LLaMA3-8B
Mistral-7B
LANGUAGE
EN
PT-BR"
MATHEMATICS,0.7803921568627451,"Figure 35: Distribution of lz and IRT scores for humans and LLMs in the ENEM 2022 exam.
LLMs are non-instructed tuned open source models and GPT3.5 with one-shot. LLM datapoints are
computed from different shuffles."
MATHEMATICS,0.7808823529411765,"6
4
2
0
2
4"
MATHEMATICS,0.7813725490196078,lz score 2 1 0 1 2 3 4
MATHEMATICS,0.7818627450980392,IRT score
LANGUAGES AND CODES,0.7823529411764706,2022 Languages and Codes
LANGUAGES AND CODES,0.782843137254902,"6
4
2
0
2
4"
LANGUAGES AND CODES,0.7833333333333333,lz score
HUMANITIES,0.7838235294117647,2022 Humanities
HUMANITIES,0.7843137254901961,"6
4
2
0
2
4"
HUMANITIES,0.7848039215686274,lz score
NATURAL SCIENCES,0.7852941176470588,2022 Natural Sciences
NATURAL SCIENCES,0.7857843137254902,"6
4
2
0
2
4"
NATURAL SCIENCES,0.7862745098039216,lz score
MATHEMATICS,0.7867647058823529,2022 Mathematics
MATHEMATICS,0.7872549019607843,"MODEL
Gemma-7B Instruct
LLaMA2-13B Instruct
LLaMA2-7B Instruct
LLaMA3-8B Instruct
Mistral-7B Instruct
LANGUAGE
EN
PT-BR"
MATHEMATICS,0.7877450980392157,"Figure 36: Distribution of lz and IRT scores for humans and LLMs in the ENEM 2022 exam. LLMs
are instructed tuned open source models with one-shot. LLM datapoints are computed from different
shuffles."
MATHEMATICS,0.788235294117647,"A.10
Examples of non-discriminating and highly discriminating items for the 2023 Natural
1061"
MATHEMATICS,0.7887254901960784,"Sciences exam.
1062"
MATHEMATICS,0.7892156862745098,"A.10.1
Poorly discriminative questions
1063"
MATHEMATICS,0.7897058823529411,"Question 107 (discrimination index -0.013)
1064"
MATHEMATICS,0.7901960784313725,"Municipalities are responsible for managing their urban waste (garbage) cleaning and collection
1065"
MATHEMATICS,0.7906862745098039,"according to the Federal Constitution. However, there are reports that part of this waste winds up in-
1066"
MATHEMATICS,0.7911764705882353,"cinerated, releasing toxic substances into the environment and causing explosions-related accidents
1067"
MATHEMATICS,0.7916666666666666,"when incinerating aerosol bottles (e.g., deodorants, insecticides, and repellents). The high tempera-
1068"
MATHEMATICS,0.792156862745098,"ture causes all the contents inside these bottles to vaporize, increasing the internal pressure until it
1069"
MATHEMATICS,0.7926470588235294,"explodes.
1070"
MATHEMATICS,0.7931372549019607,"Suppose there is a metal aerosol bottle with a capacity of 100 milliliters containing 0.1 mol of
1071"
MATHEMATICS,0.7936274509803921,"gaseous products at a temperature of 650 degrees Celsius at the moment of explosion.
1072"
MATHEMATICS,0.7941176470588235,Consider: R = 0.082×liter×atmosphere
MATHEMATICS,0.7946078431372549,"mol×Kelvin
1073"
MATHEMATICS,0.7950980392156862,"The pressure, in atmospheres, inside the flask at the moment of the explosion is closest to
1074"
MATHEMATICS,0.7955882352941176,"A. 756
1075"
MATHEMATICS,0.796078431372549,"B. 533
1076"
MATHEMATICS,0.7965686274509803,"C. 76
1077"
MATHEMATICS,0.7970588235294118,"D. 53
1078"
MATHEMATICS,0.7975490196078432,"E. 13
1079"
MATHEMATICS,0.7980392156862746,"Question 108 (discrimination index -0.076)
1080"
MATHEMATICS,0.7985294117647059,"The circuit with three identical incandescent light bulbs, shown in the figure, consists of a mixed
1081"
MATHEMATICS,0.7990196078431373,"association of resistors. Each bulb (L1, L2, and L3) is associated in parallel with a resistor of
1082"
MATHEMATICS,0.7995098039215687,"resistance R, forming a set. These sets are connected in series, with all the bulbs having the same
1083"
MATHEMATICS,0.8,"brightness when connected to the power supply. After several days in use, only lamp L2 burns out,
1084"
MATHEMATICS,0.8004901960784314,"while the others remain lit.
1085"
MATHEMATICS,0.8009803921568628,"Figure description: a power supply connected to three sets, arranged in series clockwise, in the
1086"
MATHEMATICS,0.8014705882352942,"following sequence: the parallel set of L1 and R, the parallel set of L2 and R, and the parallel set of
1087"
MATHEMATICS,0.8019607843137255,"L3 and R.
1088"
MATHEMATICS,0.8024509803921569,Figure 37: Question 108 Natural Sciences
MATHEMATICS,0.8029411764705883,"In the case where all the bulbs work, after L2 burns out, the brightness of the bulbs will be
1089"
MATHEMATICS,0.8034313725490196,"A. the same.
1090"
MATHEMATICS,0.803921568627451,"B. more intense.
1091"
MATHEMATICS,0.8044117647058824,"C. less intense.
1092"
MATHEMATICS,0.8049019607843138,"D. less intense for L1 and the same for L3.
1093"
MATHEMATICS,0.8053921568627451,"E. more intense for L1 and less intense for L3.
1094"
MATHEMATICS,0.8058823529411765,"Question 109 (discrimination index 0.013)
1095"
MATHEMATICS,0.8063725490196079,"A company’s transport safety team is evaluating the behavior of the tensions that appear in two
1096"
MATHEMATICS,0.8068627450980392,"horizontal ropes, 1 and 2, used to secure a load of mass M equal to 200 kilograms to the truck,
1097"
MATHEMATICS,0.8073529411764706,"as shown in the illustration. When the truck starts from rest, its acceleration is constant and equal
1098"
MATHEMATICS,0.807843137254902,"to 3 meters per second squared, while when it arbitrarily brakes, its braking is constant and equal
1099"
MATHEMATICS,0.8083333333333333,"to 5 meters per second squared. In both situations, the load is about to move, and the direction of
1100"
MATHEMATICS,0.8088235294117647,"the truck’s movement is shown in the figure. The coefficient of static friction between the box and
1101"
MATHEMATICS,0.8093137254901961,"the bottom surface of the body is 0.2. Consider the acceleration due to gravity to be 10 meters per
1102"
MATHEMATICS,0.8098039215686275,"second squared, the initial tension in the ropes is zero, and the two ropes are ideal.
1103"
MATHEMATICS,0.8102941176470588,"Figure description: a truck traveling horizontally to the right (represented by the vector V). A box M
1104"
MATHEMATICS,0.8107843137254902,"is resting on the central surface of its body. The box is attached to the rear of the body by horizontal
1105"
MATHEMATICS,0.8112745098039216,"rope 1 and to the front by horizontal rope 2.
1106"
MATHEMATICS,0.8117647058823529,Figure 38: Question 109 Natural Sciences
MATHEMATICS,0.8122549019607843,"When the truck is accelerating and braking, the tensions in ropes 1 and 2 in Newton will be
1107"
MATHEMATICS,0.8127450980392157,"A. acceleration: T1=0 and T2=200; braking: T1=600 and T2=0.
1108"
MATHEMATICS,0.8132352941176471,"B. acceleration: T1=0 and T2=200; braking: T1=1400 and T2=0.
1109"
MATHEMATICS,0.8137254901960784,"C. acceleration: T1=0 and T2=600; braking: T1=600 and T2=0.
1110"
MATHEMATICS,0.8142156862745098,"D. acceleration: T1=560 and T2=0; braking: T1=0 and T2=960.
1111"
MATHEMATICS,0.8147058823529412,"E. acceleration: T1=640 and T2=0; braking: T1=0 and T2=1040.
1112"
MATHEMATICS,0.8151960784313725,"A.10.2
Highly discriminative questions
1113"
MATHEMATICS,0.8156862745098039,"Question 124 (discrimination index 0.650)
1114"
MATHEMATICS,0.8161764705882353,"Update of the Portuguese Society of Neonatology’s recommendation
1115"
MATHEMATICS,0.8166666666666667,"Glass containing aluminum is an excellent material for packaging medicines and supplements be-
1116"
MATHEMATICS,0.817156862745098,"cause heating can sterilize it. However, when the drug or supplement contains substances that bind
1117"
MATHEMATICS,0.8176470588235294,"strongly to this metal’s ion, the aluminum’s dissolution is promoted by the displacement of the
1118"
MATHEMATICS,0.8181372549019608,"chemical equilibrium established between the species immobilized in the glass and the species in
1119"
MATHEMATICS,0.8186274509803921,"solution. For this reason, it is recommended that newborn nutrition supplements containing calcium
1120"
MATHEMATICS,0.8191176470588235,"gluconate be packaged in plastic containers rather than in this type of glass.
1121"
MATHEMATICS,0.8196078431372549,"If this supplement is packaged in this type of glass, the risk of contamination by aluminum will be
1122"
MATHEMATICS,0.8200980392156862,"greater if the
1123"
MATHEMATICS,0.8205882352941176,"A. glass of the bottle is translucent.
1124"
MATHEMATICS,0.821078431372549,"B. concentration of calcium gluconate is high.
1125"
MATHEMATICS,0.8215686274509804,"C. glass bottle is thicker.
1126"
MATHEMATICS,0.8220588235294117,"D. glass is previously sterilized at high temperatures.
1127"
MATHEMATICS,0.8225490196078431,"E. reaction of aluminum with calcium gluconate is endothermic.
1128"
MATHEMATICS,0.8230392156862745,"Question 91 (discrimination index 0.624)
1129"
MATHEMATICS,0.8235294117647058,"It is a common requirement to turn off devices, such as cell phones, whose operation involves emit-
1130"
MATHEMATICS,0.8240196078431372,"ting or receiving electromagnetic waves when traveling by plane. The justification for this procedure
1131"
MATHEMATICS,0.8245098039215686,"is, among other things, the need to eliminate sources of electromagnetic signals that could interfere
1132"
MATHEMATICS,0.825,"with the pilots’ radio communications with the control tower.
1133"
MATHEMATICS,0.8254901960784313,"This interference can only occur if the waves emitted by the cell phone and those received by the
1134"
MATHEMATICS,0.8259803921568627,"plane’s radio
1135"
MATHEMATICS,0.8264705882352941,"A. are both audible.
1136"
MATHEMATICS,0.8269607843137254,"B. have the same power.
1137"
MATHEMATICS,0.8274509803921568,"C. have the same frequency.
1138"
MATHEMATICS,0.8279411764705882,"D. have the same intensity.
1139"
MATHEMATICS,0.8284313725490197,"E. propagate at different speeds.
1140"
MATHEMATICS,0.828921568627451,"Question 130 (discrimination index 0.621)
1141"
MATHEMATICS,0.8294117647058824,"The number of bees is in decline in various regions of the world, including Brazil, and multiple
1142"
MATHEMATICS,0.8299019607843138,"factors are contributing to the collapse of their hives. In the United States, seed bombs of native
1143"
MATHEMATICS,0.8303921568627451,"plant species have been used to combat the disappearance of these insects. They are small balls
1144"
MATHEMATICS,0.8308823529411765,"filled with seeds, compost, and clay. When they are thrown and exposed to sun and rain, they
1145"
MATHEMATICS,0.8313725490196079,"germinate even in poorly fertile soil.
1146"
MATHEMATICS,0.8318627450980393,"This method contributes to the preservation of bees because
1147"
MATHEMATICS,0.8323529411764706,"A. it reduces predation.
1148"
MATHEMATICS,0.832843137254902,"B. it reduces the use of pesticides.
1149"
MATHEMATICS,0.8333333333333334,"C. it reduces competition for shelter.
1150"
MATHEMATICS,0.8338235294117647,"D. it increases the food supply.
1151"
MATHEMATICS,0.8343137254901961,"E. it increases breeding sites.
1152"
MATHEMATICS,0.8348039215686275,"A.11
Description of Exams
1153"
MATHEMATICS,0.8352941176470589,"The Humanities exam assesses understanding of geographical, cultural, and socioeconomic trans-
1154"
MATHEMATICS,0.8357843137254902,"formations, as well as comprehension of social and political institutions, technological changes, and
1155"
MATHEMATICS,0.8362745098039216,"the use of historical knowledge to promote conscious engagement in society. It requires recognizing
1156"
MATHEMATICS,0.836764705882353,"the interactions between society and nature in various historical and geographical contexts.
1157"
MATHEMATICS,0.8372549019607843,"The Languages and Codes exam assesses the use of communication in various contexts. This in-
1158"
MATHEMATICS,0.8377450980392157,"cludes some knowledge and use of foreign languages, understanding of body language, analysis and
1159"
MATHEMATICS,0.8382352941176471,"interpretation of expressive resources in different languages, comprehension of opinions in specific
1160"
MATHEMATICS,0.8387254901960784,"languages, and understanding the impact of communication on personal and social life.
1161"
MATHEMATICS,0.8392156862745098,"The Natural Sciences exam assesses understanding of natural sciences and recognizing their roles
1162"
MATHEMATICS,0.8397058823529412,"in production, economic and social development. It involves associating environmental degrada-
1163"
MATHEMATICS,0.8401960784313726,"tion or conservation with productive and social processes, understanding the interactions between
1164"
MATHEMATICS,0.8406862745098039,"organisms and the environment, and applying specific knowledge of physics, chemistry, and biology.
1165"
MATHEMATICS,0.8411764705882353,"The Math exam assesses the usage of geometric knowledge to represent reality, understanding no-
1166"
MATHEMATICS,0.8416666666666667,"tions of magnitudes, measurements, and their variations for solving everyday problems, interpreting
1167"
MATHEMATICS,0.842156862745098,"information of scientific and social nature obtained from reading graphs and tables, and making
1168"
MATHEMATICS,0.8426470588235294,"trend predictions, extrapolations, interpolations, and interpretations.
1169"
MATHEMATICS,0.8431372549019608,"NeurIPS Paper Checklist
1170"
CLAIMS,0.8436274509803922,"1. Claims
1171"
CLAIMS,0.8441176470588235,"Question: Do the main claims made in the abstract and introduction accurately reflect the
1172"
CLAIMS,0.8446078431372549,"paper’s contributions and scope?
1173"
CLAIMS,0.8450980392156863,"Answer: [Yes]
1174"
CLAIMS,0.8455882352941176,"Justification: We show how IRT can be used to study LLM in comparison to humans
1175"
CLAIMS,0.846078431372549,"through multiple-metric propositions (Section 3) and their results and discussions (Sec-
1176"
CLAIMS,0.8465686274509804,"tion 5).
1177"
CLAIMS,0.8470588235294118,"Guidelines:
1178"
CLAIMS,0.8475490196078431,"• The answer NA means that the abstract and introduction do not include the claims
1179"
CLAIMS,0.8480392156862745,"made in the paper.
1180"
CLAIMS,0.8485294117647059,"• The abstract and/or introduction should clearly state the claims made, including the
1181"
CLAIMS,0.8490196078431372,"contributions made in the paper and important assumptions and limitations. A No or
1182"
CLAIMS,0.8495098039215686,"NA answer to this question will not be perceived well by the reviewers.
1183"
CLAIMS,0.85,"• The claims made should match theoretical and experimental results, and reflect how
1184"
CLAIMS,0.8504901960784313,"much the results can be expected to generalize to other settings.
1185"
CLAIMS,0.8509803921568627,"• It is fine to include aspirational goals as motivation as long as it is clear that these
1186"
CLAIMS,0.8514705882352941,"goals are not attained by the paper.
1187"
LIMITATIONS,0.8519607843137255,"2. Limitations
1188"
LIMITATIONS,0.8524509803921568,"Question: Does the paper discuss the limitations of the work performed by the authors?
1189"
LIMITATIONS,0.8529411764705882,"Answer: [Yes]
1190"
LIMITATIONS,0.8534313725490196,"Justification: We discuss limitations of prompts and models in Appendix A.4, limitations
1191"
LIMITATIONS,0.8539215686274509,"of contamination correlation study in Appendix A.3, limitation of the dataset curation in
1192"
LIMITATIONS,0.8544117647058823,"Appendix A.1
1193"
LIMITATIONS,0.8549019607843137,"Guidelines:
1194"
LIMITATIONS,0.8553921568627451,"• The answer NA means that the paper has no limitation while the answer No means
1195"
LIMITATIONS,0.8558823529411764,"that the paper has limitations, but those are not discussed in the paper.
1196"
LIMITATIONS,0.8563725490196078,"• The authors are encouraged to create a separate ”Limitations” section in their paper.
1197"
LIMITATIONS,0.8568627450980392,"• The paper should point out any strong assumptions and how robust the results are to
1198"
LIMITATIONS,0.8573529411764705,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
1199"
LIMITATIONS,0.8578431372549019,"model well-specification, asymptotic approximations only holding locally). The au-
1200"
LIMITATIONS,0.8583333333333333,"thors should reflect on how these assumptions might be violated in practice and what
1201"
LIMITATIONS,0.8588235294117647,"the implications would be.
1202"
LIMITATIONS,0.859313725490196,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
1203"
LIMITATIONS,0.8598039215686275,"only tested on a few datasets or with a few runs. In general, empirical results often
1204"
LIMITATIONS,0.8602941176470589,"depend on implicit assumptions, which should be articulated.
1205"
LIMITATIONS,0.8607843137254902,"• The authors should reflect on the factors that influence the performance of the ap-
1206"
LIMITATIONS,0.8612745098039216,"proach. For example, a facial recognition algorithm may perform poorly when image
1207"
LIMITATIONS,0.861764705882353,"resolution is low or images are taken in low lighting. Or a speech-to-text system might
1208"
LIMITATIONS,0.8622549019607844,"not be used reliably to provide closed captions for online lectures because it fails to
1209"
LIMITATIONS,0.8627450980392157,"handle technical jargon.
1210"
LIMITATIONS,0.8632352941176471,"• The authors should discuss the computational efficiency of the proposed algorithms
1211"
LIMITATIONS,0.8637254901960785,"and how they scale with dataset size.
1212"
LIMITATIONS,0.8642156862745098,"• If applicable, the authors should discuss possible limitations of their approach to ad-
1213"
LIMITATIONS,0.8647058823529412,"dress problems of privacy and fairness.
1214"
LIMITATIONS,0.8651960784313726,"• While the authors might fear that complete honesty about limitations might be used by
1215"
LIMITATIONS,0.865686274509804,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
1216"
LIMITATIONS,0.8661764705882353,"limitations that aren’t acknowledged in the paper. The authors should use their best
1217"
LIMITATIONS,0.8666666666666667,"judgment and recognize that individual actions in favor of transparency play an impor-
1218"
LIMITATIONS,0.8671568627450981,"tant role in developing norms that preserve the integrity of the community. Reviewers
1219"
LIMITATIONS,0.8676470588235294,"will be specifically instructed to not penalize honesty concerning limitations.
1220"
THEORY ASSUMPTIONS AND PROOFS,0.8681372549019608,"3. Theory Assumptions and Proofs
1221"
THEORY ASSUMPTIONS AND PROOFS,0.8686274509803922,"Question: For each theoretical result, does the paper provide the full set of assumptions and
1222"
THEORY ASSUMPTIONS AND PROOFS,0.8691176470588236,"a complete (and correct) proof?
1223"
THEORY ASSUMPTIONS AND PROOFS,0.8696078431372549,"Answer: [NA]
1224"
THEORY ASSUMPTIONS AND PROOFS,0.8700980392156863,"Justification: We do not include theoretical results.
1225"
THEORY ASSUMPTIONS AND PROOFS,0.8705882352941177,"Guidelines:
1226"
THEORY ASSUMPTIONS AND PROOFS,0.871078431372549,"• The answer NA means that the paper does not include theoretical results.
1227"
THEORY ASSUMPTIONS AND PROOFS,0.8715686274509804,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
1228"
THEORY ASSUMPTIONS AND PROOFS,0.8720588235294118,"referenced.
1229"
THEORY ASSUMPTIONS AND PROOFS,0.8725490196078431,"• All assumptions should be clearly stated or referenced in the statement of any theo-
1230"
THEORY ASSUMPTIONS AND PROOFS,0.8730392156862745,"rems.
1231"
THEORY ASSUMPTIONS AND PROOFS,0.8735294117647059,"• The proofs can either appear in the main paper or the supplemental material, but if
1232"
THEORY ASSUMPTIONS AND PROOFS,0.8740196078431373,"they appear in the supplemental material, the authors are encouraged to provide a
1233"
THEORY ASSUMPTIONS AND PROOFS,0.8745098039215686,"short proof sketch to provide intuition.
1234"
THEORY ASSUMPTIONS AND PROOFS,0.875,"• Inversely, any informal proof provided in the core of the paper should be comple-
1235"
THEORY ASSUMPTIONS AND PROOFS,0.8754901960784314,"mented by formal proofs provided in appendix or supplemental material.
1236"
THEORY ASSUMPTIONS AND PROOFS,0.8759803921568627,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
1237"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8764705882352941,"4. Experimental Result Reproducibility
1238"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8769607843137255,"Question: Does the paper fully disclose all the information needed to reproduce the main
1239"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8774509803921569,"experimental results of the paper to the extent that it affects the main claims and/or conclu-
1240"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8779411764705882,"sions of the paper (regardless of whether the code and data are provided or not)?
1241"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8784313725490196,"Answer: [Yes]
1242"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.878921568627451,"Justification: We describe our dataset creation in Section 4.1, data manual auditing process
1243"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8794117647058823,"in Appendix A.1, prompting and evaluation details in Section 4.2 and Appendix A.4. We
1244"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8799019607843137,"will release our code and data.
1245"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8803921568627451,"Guidelines:
1246"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8808823529411764,"• The answer NA means that the paper does not include experiments.
1247"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8813725490196078,"• If the paper includes experiments, a No answer to this question will not be perceived
1248"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8818627450980392,"well by the reviewers: Making the paper reproducible is important, regardless of
1249"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8823529411764706,"whether the code and data are provided or not.
1250"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8828431372549019,"• If the contribution is a dataset and/or model, the authors should describe the steps
1251"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8833333333333333,"taken to make their results reproducible or verifiable.
1252"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8838235294117647,"• Depending on the contribution, reproducibility can be accomplished in various ways.
1253"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.884313725490196,"For example, if the contribution is a novel architecture, describing the architecture
1254"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8848039215686274,"fully might suffice, or if the contribution is a specific model and empirical evaluation,
1255"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8852941176470588,"it may be necessary to either make it possible for others to replicate the model with
1256"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8857843137254902,"the same dataset, or provide access to the model. In general. releasing code and data
1257"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8862745098039215,"is often one good way to accomplish this, but reproducibility can also be provided via
1258"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8867647058823529,"detailed instructions for how to replicate the results, access to a hosted model (e.g., in
1259"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8872549019607843,"the case of a large language model), releasing of a model checkpoint, or other means
1260"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8877450980392156,"that are appropriate to the research performed.
1261"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.888235294117647,"• While NeurIPS does not require releasing code, the conference does require all sub-
1262"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8887254901960784,"missions to provide some reasonable avenue for reproducibility, which may depend
1263"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8892156862745098,"on the nature of the contribution. For example
1264"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8897058823529411,"(a) If the contribution is primarily a new algorithm, the paper should make it clear
1265"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8901960784313725,"how to reproduce that algorithm.
1266"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.890686274509804,"(b) If the contribution is primarily a new model architecture, the paper should describe
1267"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8911764705882353,"the architecture clearly and fully.
1268"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8916666666666667,"(c) If the contribution is a new model (e.g., a large language model), then there should
1269"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8921568627450981,"either be a way to access this model for reproducing the results or a way to re-
1270"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8926470588235295,"produce the model (e.g., with an open-source dataset or instructions for how to
1271"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8931372549019608,"construct the dataset).
1272"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8936274509803922,"(d) We recognize that reproducibility may be tricky in some cases, in which case au-
1273"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8941176470588236,"thors are welcome to describe the particular way they provide for reproducibility.
1274"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8946078431372549,"In the case of closed-source models, it may be that access to the model is limited in
1275"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8950980392156863,"some way (e.g., to registered users), but it should be possible for other researchers
1276"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8955882352941177,"to have some path to reproducing or verifying the results.
1277"
OPEN ACCESS TO DATA AND CODE,0.8960784313725491,"5. Open access to data and code
1278"
OPEN ACCESS TO DATA AND CODE,0.8965686274509804,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
1279"
OPEN ACCESS TO DATA AND CODE,0.8970588235294118,"tions to faithfully reproduce the main experimental results, as described in supplemental
1280"
OPEN ACCESS TO DATA AND CODE,0.8975490196078432,"material?
1281"
OPEN ACCESS TO DATA AND CODE,0.8980392156862745,"Answer: [Yes]
1282"
OPEN ACCESS TO DATA AND CODE,0.8985294117647059,"Justification: We provide the code with reproducibility instructions. We will also provide
1283"
OPEN ACCESS TO DATA AND CODE,0.8990196078431373,"all the data.
1284"
OPEN ACCESS TO DATA AND CODE,0.8995098039215687,"Guidelines:
1285"
OPEN ACCESS TO DATA AND CODE,0.9,"• The answer NA means that paper does not include experiments requiring code.
1286"
OPEN ACCESS TO DATA AND CODE,0.9004901960784314,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/pu
1287"
OPEN ACCESS TO DATA AND CODE,0.9009803921568628,"blic/guides/CodeSubmissionPolicy) for more details.
1288"
OPEN ACCESS TO DATA AND CODE,0.9014705882352941,"• While we encourage the release of code and data, we understand that this might not
1289"
OPEN ACCESS TO DATA AND CODE,0.9019607843137255,"be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
1290"
OPEN ACCESS TO DATA AND CODE,0.9024509803921569,"including code, unless this is central to the contribution (e.g., for a new open-source
1291"
OPEN ACCESS TO DATA AND CODE,0.9029411764705882,"benchmark).
1292"
OPEN ACCESS TO DATA AND CODE,0.9034313725490196,"• The instructions should contain the exact command and environment needed to run to
1293"
OPEN ACCESS TO DATA AND CODE,0.903921568627451,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
1294"
OPEN ACCESS TO DATA AND CODE,0.9044117647058824,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
1295"
OPEN ACCESS TO DATA AND CODE,0.9049019607843137,"• The authors should provide instructions on data access and preparation, including how
1296"
OPEN ACCESS TO DATA AND CODE,0.9053921568627451,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
1297"
OPEN ACCESS TO DATA AND CODE,0.9058823529411765,"• The authors should provide scripts to reproduce all experimental results for the new
1298"
OPEN ACCESS TO DATA AND CODE,0.9063725490196078,"proposed method and baselines. If only a subset of experiments are reproducible, they
1299"
OPEN ACCESS TO DATA AND CODE,0.9068627450980392,"should state which ones are omitted from the script and why.
1300"
OPEN ACCESS TO DATA AND CODE,0.9073529411764706,"• At submission time, to preserve anonymity, the authors should release anonymized
1301"
OPEN ACCESS TO DATA AND CODE,0.907843137254902,"versions (if applicable).
1302"
OPEN ACCESS TO DATA AND CODE,0.9083333333333333,"• Providing as much information as possible in supplemental material (appended to the
1303"
OPEN ACCESS TO DATA AND CODE,0.9088235294117647,"paper) is recommended, but including URLs to data and code is permitted.
1304"
OPEN ACCESS TO DATA AND CODE,0.9093137254901961,"6. Experimental Setting/Details
1305"
OPEN ACCESS TO DATA AND CODE,0.9098039215686274,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
1306"
OPEN ACCESS TO DATA AND CODE,0.9102941176470588,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
1307"
OPEN ACCESS TO DATA AND CODE,0.9107843137254902,"results?
1308"
OPEN ACCESS TO DATA AND CODE,0.9112745098039216,"Answer: [Yes]
1309"
OPEN ACCESS TO DATA AND CODE,0.9117647058823529,"Justification: Prompting and evaluation details given in Section 4.2 and Appendix A.4.
1310"
OPEN ACCESS TO DATA AND CODE,0.9122549019607843,"Evaluation scripts can be seen in our code.
1311"
OPEN ACCESS TO DATA AND CODE,0.9127450980392157,"Guidelines:
1312"
OPEN ACCESS TO DATA AND CODE,0.913235294117647,"• The answer NA means that the paper does not include experiments.
1313"
OPEN ACCESS TO DATA AND CODE,0.9137254901960784,"• The experimental setting should be presented in the core of the paper to a level of
1314"
OPEN ACCESS TO DATA AND CODE,0.9142156862745098,"detail that is necessary to appreciate the results and make sense of them.
1315"
OPEN ACCESS TO DATA AND CODE,0.9147058823529411,"• The full details can be provided either with the code, in appendix, or as supplemental
1316"
OPEN ACCESS TO DATA AND CODE,0.9151960784313725,"material.
1317"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9156862745098039,"7. Experiment Statistical Significance
1318"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9161764705882353,"Question: Does the paper report error bars suitably and correctly defined or other appropri-
1319"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9166666666666666,"ate information about the statistical significance of the experiments?
1320"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.917156862745098,"Answer: [Yes]
1321"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9176470588235294,"Justification: We describe our option shuffling procedure in Section 4.2 and Appendix A.4
1322"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9181372549019607,"to account for model bias for generating option letters. For all plots, we explain what were
1323"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9186274509803921,"the confidence intervals/means are (Figures 1, 2, 4, and 3)
1324"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9191176470588235,"Guidelines:
1325"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9196078431372549,"• The answer NA means that the paper does not include experiments.
1326"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9200980392156862,"• The authors should answer ”Yes” if the results are accompanied by error bars, confi-
1327"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9205882352941176,"dence intervals, or statistical significance tests, at least for the experiments that support
1328"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.921078431372549,"the main claims of the paper.
1329"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9215686274509803,"• The factors of variability that the error bars are capturing should be clearly stated (for
1330"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9220588235294118,"example, train/test split, initialization, random drawing of some parameter, or overall
1331"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9225490196078432,"run with given experimental conditions).
1332"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9230392156862746,"• The method for calculating the error bars should be explained (closed form formula,
1333"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9235294117647059,"call to a library function, bootstrap, etc.)
1334"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9240196078431373,"• The assumptions made should be given (e.g., Normally distributed errors).
1335"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9245098039215687,"• It should be clear whether the error bar is the standard deviation or the standard error
1336"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.925,"of the mean.
1337"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9254901960784314,"• It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
1338"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9259803921568628,"ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
1339"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9264705882352942,"Normality of errors is not verified.
1340"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9269607843137255,"• For asymmetric distributions, the authors should be careful not to show in tables or
1341"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9274509803921569,"figures symmetric error bars that would yield results that are out of range (e.g. negative
1342"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9279411764705883,"error rates).
1343"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9284313725490196,"• If error bars are reported in tables or plots, The authors should explain in the text how
1344"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.928921568627451,"they were calculated and reference the corresponding figures or tables in the text.
1345"
EXPERIMENTS COMPUTE RESOURCES,0.9294117647058824,"8. Experiments Compute Resources
1346"
EXPERIMENTS COMPUTE RESOURCES,0.9299019607843138,"Question: For each experiment, does the paper provide sufficient information on the com-
1347"
EXPERIMENTS COMPUTE RESOURCES,0.9303921568627451,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
1348"
EXPERIMENTS COMPUTE RESOURCES,0.9308823529411765,"the experiments?
1349"
EXPERIMENTS COMPUTE RESOURCES,0.9313725490196079,"Answer: [Yes]
1350"
EXPERIMENTS COMPUTE RESOURCES,0.9318627450980392,"Justification: We disclose the compute related information in Appendix A.5
1351"
EXPERIMENTS COMPUTE RESOURCES,0.9323529411764706,"Guidelines:
1352"
EXPERIMENTS COMPUTE RESOURCES,0.932843137254902,"• The answer NA means that the paper does not include experiments.
1353"
EXPERIMENTS COMPUTE RESOURCES,0.9333333333333333,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
1354"
EXPERIMENTS COMPUTE RESOURCES,0.9338235294117647,"or cloud provider, including relevant memory and storage.
1355"
EXPERIMENTS COMPUTE RESOURCES,0.9343137254901961,"• The paper should provide the amount of compute required for each of the individual
1356"
EXPERIMENTS COMPUTE RESOURCES,0.9348039215686275,"experimental runs as well as estimate the total compute.
1357"
EXPERIMENTS COMPUTE RESOURCES,0.9352941176470588,"• The paper should disclose whether the full research project required more compute
1358"
EXPERIMENTS COMPUTE RESOURCES,0.9357843137254902,"than the experiments reported in the paper (e.g., preliminary or failed experiments
1359"
EXPERIMENTS COMPUTE RESOURCES,0.9362745098039216,"that didn’t make it into the paper).
1360"
CODE OF ETHICS,0.9367647058823529,"9. Code Of Ethics
1361"
CODE OF ETHICS,0.9372549019607843,"Question: Does the research conducted in the paper conform, in every respect, with the
1362"
CODE OF ETHICS,0.9377450980392157,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
1363"
CODE OF ETHICS,0.9382352941176471,"Answer: [Yes]
1364"
CODE OF ETHICS,0.9387254901960784,"Justification: We have reviewed the code of ethics and our work conforms with it.
1365"
CODE OF ETHICS,0.9392156862745098,"Guidelines:
1366"
CODE OF ETHICS,0.9397058823529412,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
1367"
CODE OF ETHICS,0.9401960784313725,"• If the authors answer No, they should explain the special circumstances that require a
1368"
CODE OF ETHICS,0.9406862745098039,"deviation from the Code of Ethics.
1369"
CODE OF ETHICS,0.9411764705882353,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
1370"
CODE OF ETHICS,0.9416666666666667,"eration due to laws or regulations in their jurisdiction).
1371"
BROADER IMPACTS,0.942156862745098,"10. Broader Impacts
1372"
BROADER IMPACTS,0.9426470588235294,"Question: Does the paper discuss both potential positive societal impacts and negative
1373"
BROADER IMPACTS,0.9431372549019608,"societal impacts of the work performed?
1374"
BROADER IMPACTS,0.9436274509803921,"Answer: [Yes]
1375"
BROADER IMPACTS,0.9441176470588235,"Justification: We discuss the impact of our work in Section 5 and 6.
1376"
BROADER IMPACTS,0.9446078431372549,"Guidelines:
1377"
BROADER IMPACTS,0.9450980392156862,"• The answer NA means that there is no societal impact of the work performed.
1378"
BROADER IMPACTS,0.9455882352941176,"• If the authors answer NA or No, they should explain why their work has no societal
1379"
BROADER IMPACTS,0.946078431372549,"impact or why the paper does not address societal impact.
1380"
BROADER IMPACTS,0.9465686274509804,"• Examples of negative societal impacts include potential malicious or unintended uses
1381"
BROADER IMPACTS,0.9470588235294117,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
1382"
BROADER IMPACTS,0.9475490196078431,"(e.g., deployment of technologies that could make decisions that unfairly impact spe-
1383"
BROADER IMPACTS,0.9480392156862745,"cific groups), privacy considerations, and security considerations.
1384"
BROADER IMPACTS,0.9485294117647058,"• The conference expects that many papers will be foundational research and not tied
1385"
BROADER IMPACTS,0.9490196078431372,"to particular applications, let alone deployments. However, if there is a direct path to
1386"
BROADER IMPACTS,0.9495098039215686,"any negative applications, the authors should point it out. For example, it is legitimate
1387"
BROADER IMPACTS,0.95,"to point out that an improvement in the quality of generative models could be used to
1388"
BROADER IMPACTS,0.9504901960784313,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
1389"
BROADER IMPACTS,0.9509803921568627,"that a generic algorithm for optimizing neural networks could enable people to train
1390"
BROADER IMPACTS,0.9514705882352941,"models that generate Deepfakes faster.
1391"
BROADER IMPACTS,0.9519607843137254,"• The authors should consider possible harms that could arise when the technology is
1392"
BROADER IMPACTS,0.9524509803921568,"being used as intended and functioning correctly, harms that could arise when the
1393"
BROADER IMPACTS,0.9529411764705882,"technology is being used as intended but gives incorrect results, and harms following
1394"
BROADER IMPACTS,0.9534313725490197,"from (intentional or unintentional) misuse of the technology.
1395"
BROADER IMPACTS,0.953921568627451,"• If there are negative societal impacts, the authors could also discuss possible mitiga-
1396"
BROADER IMPACTS,0.9544117647058824,"tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
1397"
BROADER IMPACTS,0.9549019607843138,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
1398"
BROADER IMPACTS,0.9553921568627451,"feedback over time, improving the efficiency and accessibility of ML).
1399"
SAFEGUARDS,0.9558823529411765,"11. Safeguards
1400"
SAFEGUARDS,0.9563725490196079,"Question: Does the paper describe safeguards that have been put in place for responsible
1401"
SAFEGUARDS,0.9568627450980393,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
1402"
SAFEGUARDS,0.9573529411764706,"image generators, or scraped datasets)?
1403"
SAFEGUARDS,0.957843137254902,"Answer: [NA]
1404"
SAFEGUARDS,0.9583333333333334,"Justification: We do not forsee any negative impact with our work.
1405"
SAFEGUARDS,0.9588235294117647,"Guidelines:
1406"
SAFEGUARDS,0.9593137254901961,"• The answer NA means that the paper poses no such risks.
1407"
SAFEGUARDS,0.9598039215686275,"• Released models that have a high risk for misuse or dual-use should be released with
1408"
SAFEGUARDS,0.9602941176470589,"necessary safeguards to allow for controlled use of the model, for example by re-
1409"
SAFEGUARDS,0.9607843137254902,"quiring that users adhere to usage guidelines or restrictions to access the model or
1410"
SAFEGUARDS,0.9612745098039216,"implementing safety filters.
1411"
SAFEGUARDS,0.961764705882353,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
1412"
SAFEGUARDS,0.9622549019607843,"should describe how they avoided releasing unsafe images.
1413"
SAFEGUARDS,0.9627450980392157,"• We recognize that providing effective safeguards is challenging, and many papers do
1414"
SAFEGUARDS,0.9632352941176471,"not require this, but we encourage authors to take this into account and make a best
1415"
SAFEGUARDS,0.9637254901960784,"faith effort.
1416"
LICENSES FOR EXISTING ASSETS,0.9642156862745098,"12. Licenses for existing assets
1417"
LICENSES FOR EXISTING ASSETS,0.9647058823529412,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1418"
LICENSES FOR EXISTING ASSETS,0.9651960784313726,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1419"
LICENSES FOR EXISTING ASSETS,0.9656862745098039,"properly respected?
1420"
LICENSES FOR EXISTING ASSETS,0.9661764705882353,"Answer: [Yes]
1421"
LICENSES FOR EXISTING ASSETS,0.9666666666666667,"Justification: We cite and and describe in detail the process through which we obtained
1422"
LICENSES FOR EXISTING ASSETS,0.967156862745098,"ENEM dataset in section 4.1.
1423"
LICENSES FOR EXISTING ASSETS,0.9676470588235294,"Guidelines:
1424"
LICENSES FOR EXISTING ASSETS,0.9681372549019608,"• The answer NA means that the paper does not use existing assets.
1425"
LICENSES FOR EXISTING ASSETS,0.9686274509803922,"• The authors should cite the original paper that produced the code package or dataset.
1426"
LICENSES FOR EXISTING ASSETS,0.9691176470588235,"• The authors should state which version of the asset is used and, if possible, include a
1427"
LICENSES FOR EXISTING ASSETS,0.9696078431372549,"URL.
1428"
LICENSES FOR EXISTING ASSETS,0.9700980392156863,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1429"
LICENSES FOR EXISTING ASSETS,0.9705882352941176,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1430"
LICENSES FOR EXISTING ASSETS,0.971078431372549,"service of that source should be provided.
1431"
LICENSES FOR EXISTING ASSETS,0.9715686274509804,"• If assets are released, the license, copyright information, and terms of use in the pack-
1432"
LICENSES FOR EXISTING ASSETS,0.9720588235294118,"age should be provided. For popular datasets, paperswithcode.com/datasets
1433"
LICENSES FOR EXISTING ASSETS,0.9725490196078431,"has curated licenses for some datasets. Their licensing guide can help determine the
1434"
LICENSES FOR EXISTING ASSETS,0.9730392156862745,"license of a dataset.
1435"
LICENSES FOR EXISTING ASSETS,0.9735294117647059,"• For existing datasets that are re-packaged, both the original license and the license of
1436"
LICENSES FOR EXISTING ASSETS,0.9740196078431372,"the derived asset (if it has changed) should be provided.
1437"
LICENSES FOR EXISTING ASSETS,0.9745098039215686,"• If this information is not available online, the authors are encouraged to reach out to
1438"
LICENSES FOR EXISTING ASSETS,0.975,"the asset’s creators.
1439"
NEW ASSETS,0.9754901960784313,"13. New Assets
1440"
NEW ASSETS,0.9759803921568627,"Question: Are new assets introduced in the paper well documented and is the documenta-
1441"
NEW ASSETS,0.9764705882352941,"tion provided alongside the assets?
1442"
NEW ASSETS,0.9769607843137255,"Answer: [Yes]
1443"
NEW ASSETS,0.9774509803921568,"Justification: We submit our dataset and include details of how we obtained the text in
1444"
NEW ASSETS,0.9779411764705882,"detail in Section 4.1 and Appendix A.1. Human results and related parameters are released
1445"
NEW ASSETS,0.9784313725490196,"by ENEM officials.
1446"
NEW ASSETS,0.9789215686274509,"Guidelines:
1447"
NEW ASSETS,0.9794117647058823,"• The answer NA means that the paper does not release new assets.
1448"
NEW ASSETS,0.9799019607843137,"• Researchers should communicate the details of the dataset/code/model as part of their
1449"
NEW ASSETS,0.9803921568627451,"submissions via structured templates. This includes details about training, license,
1450"
NEW ASSETS,0.9808823529411764,"limitations, etc.
1451"
NEW ASSETS,0.9813725490196078,"• The paper should discuss whether and how consent was obtained from people whose
1452"
NEW ASSETS,0.9818627450980392,"asset is used.
1453"
NEW ASSETS,0.9823529411764705,"• At submission time, remember to anonymize your assets (if applicable). You can
1454"
NEW ASSETS,0.9828431372549019,"either create an anonymized URL or include an anonymized zip file.
1455"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9833333333333333,"14. Crowdsourcing and Research with Human Subjects
1456"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9838235294117647,"Question: For crowdsourcing experiments and research with human subjects, does the pa-
1457"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.984313725490196,"per include the full text of instructions given to participants and screenshots, if applicable,
1458"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848039215686275,"as well as details about compensation (if any)?
1459"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9852941176470589,"Answer: [NA]
1460"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9857843137254902,"Justification: We did not crowdsource. The human results are published by ENEM officials.
1461"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9862745098039216,"Guidelines:
1462"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986764705882353,"• The answer NA means that the paper does not involve crowdsourcing nor research
1463"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9872549019607844,"with human subjects.
1464"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9877450980392157,"• Including this information in the supplemental material is fine, but if the main contri-
1465"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9882352941176471,"bution of the paper involves human subjects, then as much detail as possible should
1466"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9887254901960785,"be included in the main paper.
1467"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9892156862745098,"• According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
1468"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897058823529412,"tion, or other labor should be paid at least the minimum wage in the country of the
1469"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9901960784313726,"data collector.
1470"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.990686274509804,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1471"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9911764705882353,"Subjects
1472"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916666666666667,"Question: Does the paper describe potential risks incurred by study participants, whether
1473"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9921568627450981,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1474"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9926470588235294,"approvals (or an equivalent approval/review based on the requirements of your country or
1475"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9931372549019608,"institution) were obtained?
1476"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9936274509803922,"Answer: [NA]
1477"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941176470588236,"Justification: Our study does not involve human subjects.
1478"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9946078431372549,"Guidelines:
1479"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9950980392156863,"• The answer NA means that the paper does not involve crowdsourcing nor research
1480"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9955882352941177,"with human subjects.
1481"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996078431372549,"• Depending on the country in which research is conducted, IRB approval (or equiva-
1482"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9965686274509804,"lent) may be required for any human subjects research. If you obtained IRB approval,
1483"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970588235294118,"you should clearly state this in the paper.
1484"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9975490196078431,"• We recognize that the procedures for this may vary significantly between institutions
1485"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980392156862745,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1486"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9985294117647059,"guidelines for their institution.
1487"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990196078431373,"• For initial submissions, do not include any information that would break anonymity
1488"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9995098039215686,"(if applicable), such as the institution conducting the review.
1489"
