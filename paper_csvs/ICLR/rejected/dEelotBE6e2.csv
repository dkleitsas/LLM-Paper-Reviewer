Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003048780487804878,"A recent line of work has shown that deep networks are susceptible to backdoor
data poisoning attacks. Speciﬁcally, by injecting a small amount of malicious
data into the training distribution, an adversary gains the ability to control the
behavior of the model during inference. We propose an iterative training procedure
for removing poisoned data from the training set. Our approach consists of two
steps. We ﬁrst train an ensemble of weak learners to automatically discover distinct
subpopulations in the training set. We then leverage a boosting framework to
exclude the poisoned data and recover the clean data. Our algorithm is based on a
novel bootstrapped measure of generalization, which provably separates the clean
from the dirty data under mild assumptions. Empirically, our method successfully
defends against a state-of-the-art dirty label backdoor attack. We ﬁnd that our
approach signiﬁcantly outperforms previous defenses."
OVERVIEW,0.006097560975609756,"1
OVERVIEW"
OVERVIEW,0.009146341463414634,"The past few years has seen the rapid adoption of deep learning in real world applications, from
digital personal assistants to autonomous vehicles. This trend shows no sign of abating, given the
remarkable (super)human performance of deep neural networks on tasks such as computer vision (He
et al., 2016a), speed recognition (Graves et al., 2013), and game playing (Silver et al., 2016)."
OVERVIEW,0.012195121951219513,"However, this widespread integration of deep networks presents a potential security risk, particularly
in performance- and safety-critical applications. In this work, we focus on defending against backdoor
attacks (Chen et al., 2017; Adi et al., 2018). Speciﬁcally, it has been demonstrated that deep networks
can be attacked by injecting small amounts of poisoned (i.e., maliciously perturbed) data during
training to create a backdoor in the model; once installed, an adversary can exploit the backdoor
to change the network’s predictions at inference time. For instance, Gu et al. (2019) demonstrate a
backdoor that causes a model to misclassify stop signs as speed signs by applying a (physical) sticker.
These attacks are particularly pernicious in that the accuracy of the model on unperturbed data is
generally not affected by the backdoor, thus making it difﬁcult to identify compromised models
during standard operation."
OVERVIEW,0.01524390243902439,"Techniques
We ﬁrst introduce the notion of self-expanding sets, which based on a bootstrapped
measure of how well a set generalizes to itself. Under certain compatibility properties, we show
that the process of identifying self-expanding sets naturally separates a dataset into a collection of
homogeneous components (i.e., completely clean or completely poisoned subsets of the training data).
Given such a collection, we then provide a method to identify the clean distribution by boosting an
ensemble of weak learners over the components."
OVERVIEW,0.018292682926829267,"To separate the training set into homogeneous components, we present the Inverse Self-Paced
Learning algorithm. This algorithm uses quantile statistics to repeatedly identify and exclude
samples with high loss. Recursively applying the technique to sets of excluded samples produces the
collection of homogeneous components. We prove sufﬁcient conditions for the convergence of the
algorithm."
OVERVIEW,0.021341463414634148,"Experimental Evaluation
We implement the proposed Inverse Self-Paced Learning algorithm
within our boosting framework and evaluate its performance on two different backdoor attacks on
CIFAR-10 (?). Our method completely defends against the attacks in almost every setting and"
OVERVIEW,0.024390243902439025,Under review as a conference paper at ICLR 2022
OVERVIEW,0.027439024390243903,"substantially reduces the success rate of the attack in the remaining cases, while reducing the accuracy
on clean data by only 2-3%. Our results also show that previous approaches (Tran et al., 2018; Chen
et al., 2018; Shen and Sanghavi, 2019) are substantial weaker at identifying poisoned samples. These
previous approaches also either require an explicit upper bound on expected amount of poison or
suffer from high levels of false positives. Our approach thus presents a novel, empirically veriﬁed
method for defending against backdoor attacks."
RELATED WORK,0.03048780487804878,"2
RELATED WORK"
RELATED WORK,0.03353658536585366,"Our separation results can be viewed as solutions to a clustering problem, where we exploit weak
supervision in the form of class labels, related via our notions of self-expansion and compatibility.
Many previous works analyze similar properties for weak or semi-supervised learning based on
clustering (Seeger, 2000; Rigollet, 2007; Singh et al., 2008) or expansion (Wei et al., 2020). In
general, these works deﬁne expansion as an intrinsic property of the input data, rather than with
respect to a (weak) learner as we do. Balcan et al. (2005) show that under a similar expansion
property, learners ﬁt independently to two different “views” of the data can supervise each other to
improve their joint performance. However, a major departure from these prior works is that we do
not use an expansion property to leverage a small set of trusted or conﬁdent labels for minimizing
a global classiﬁcation error, but rather use self-expansion to identify homogeneous components by
ﬁtting weak learners to certain local minima."
RELATED WORK,0.036585365853658534,"We also introduce the Inverse Self Paced Learning algorithm for efﬁciently ﬁnding self-expanding
sets. Self paced learning (SPL) was introduced by Kumar et al. (2010) as an type of curriculum
learning (Bengio et al., 2009). SPL is a heuristic that dynamically creates a curriculum based on
the losses of the model after each epoch, so as to incorporate the easier samples ﬁrst. SPL and its
variants have been observed to be resilient to noise both in theory (Meng et al., 2016) and practice
(Jiang et al., 2018; Zhang et al., 2020), though prior works focus mostly on clean accuracy under
unrealizeable label noise. In contrast, we measure targeted misclassiﬁcation accuracy under more
challenging noise distributions that are adversarially selected to be realizeable."
RELATED WORK,0.039634146341463415,"Finally, several prior works propose methods for defending against backdoor attacks on neural
networks. In general, it has been observed that standard techniques from robust statistics applied
directly to the data do not successfully identify the poisoned data (Tran et al., 2018). The standard
approach is therefore to ﬁrst ﬁt a deep network to the poisoned distribution, then apply techniques
to the learned representations in the network layers. The Activation Clustering defense (Chen et al.,
2018) uses dimensionality reduction followed by k-means clustering (k=2) on the activation patterns,
and discards the smallest cluster. Tran et al. (2018) propose a Spectral Signature defense that removes
the data with the top ϵ eigenvalues, where ϵ is set to 1.5 times the amount of expected poison. TRIM
(Jagielski et al., 2021) (for linear regression) and Iterative Trimmed Loss Minimization (Shen and
Sanghavi, 2019) (for generalized linear models and deep neural networks) iteratively train on a subset
of the data after removing a constant fraction of the samples with the highest loss. However, the
majority of works do not evaluate their defenses on CIFAR-10, opting instead for simpler datasets,
such as trafﬁc signs or MNIST. Furthermore the triggers skew large and obvious (such as 3x3 patches
(Qiao et al., 2019) or legible text overlays (Gao et al., 2019a)), and are often constrained to lie in the
center of the image; our evaluation shows that existing defenses fail when evaluated on the more
subtle triggers we use."
BACKGROUND AND SETTING,0.042682926829268296,"3
BACKGROUND AND SETTING"
BACKGROUND AND SETTING,0.04573170731707317,"We ﬁrst establish some basic notation and the scope of our classiﬁcation setting. Let X be the input
space, Y be the label space, and L(·, ·) be a loss function over Y ×Y . We assume a bounded loss func-
tion, which includes many commonly used loss functions such as the zero-one or cross entropy loss.
Given a target distribution D supported on X × Y and a parametric family of functions fθ, the goal
is to ﬁnd the parameters θ that minimize the population risk R(θ) :=
R"
BACKGROUND AND SETTING,0.04878048780487805,"X×Y L(fθ(x), y)dPD(x, y)."
BACKGROUND AND SETTING,0.051829268292682924,"The learning problem (fθ, D) is realizable if (1) for every label y, the marginals D(·|y) have disjoint
support; and (2) there exist ground truth parameters θ∗with R(θ∗) = 0. For simplicity, we assume a
(possibly stochastic) learning algorithm A that performs empirical risk minimization, i.e., given a
set of samples T, A tries to return θ minimizing Remp(θ; T) := P"
BACKGROUND AND SETTING,0.054878048780487805,"i∈T L(fθ(xi), yi). Clearly, given"
BACKGROUND AND SETTING,0.057926829268292686,Under review as a conference paper at ICLR 2022
BACKGROUND AND SETTING,0.06097560975609756,"enough training samples S = {(x1, y1)..., (xn, yn)} iid from D, the empirical risk gets arbitrarily
close to the population risk. We will identify the training set S with its indices [n]."
DATA AND THREAT MODEL,0.06402439024390244,"3.1
DATA AND THREAT MODEL"
DATA AND THREAT MODEL,0.06707317073170732,"We consider a mixture of n distributions {(αi, Di)}n
i=1 such that D = ∪i{Di} and P"
DATA AND THREAT MODEL,0.0701219512195122,"i αi = 1. We
observe N inputs according to the following two-step procedure:"
DATA AND THREAT MODEL,0.07317073170731707,"d ∼Cat(α1, ..., αn)
(1)
x, y ∼Dd
(2)"
DATA AND THREAT MODEL,0.07621951219512195,"where Cat(·) is a categorical random variable that returns i with probability αi. If S is a set of
samples produced by this process, for any subset S′ ⊆S, we will denote the samples drawn from the
ith distribution as S′
i, so that S′ = ∪iS′
i."
DATA AND THREAT MODEL,0.07926829268292683,"Our evaluation focuses on the backdoor data poisoning model. It has been observed empirically that
injecting a small amount of malicious data into the training distribution effectively installs a backdoor
in the model, whereby the behavior on clean data is otherwise unaffected, but an attacker can cause
targeted misclassiﬁcation during inference by overlaying a small trigger. In this case sampling from
the training distribution is modeled as follows:"
DATA AND THREAT MODEL,0.08231707317073171,"d ∼Cat(α1, ..., αn)
(3)
x, y ∼Dd
(4)
p ∼Bern(ρ)
(5)
x ←τ(x), y ←π(y), if p
(6)"
DATA AND THREAT MODEL,0.08536585365853659,"where τ(·) is the function which applies a small trigger, π(·) is a permutation on classes, and ρ
controls the probability of observing a poisoned sample. Note that this procedure can easily be
replicated within the original data model. We will also assume the attack is non-trivial in the sense
that the perturbed source distributions τ(Di) and target distributions Dπ(i) are disjoint for all i."
DATA AND THREAT MODEL,0.08841463414634146,"Given a model fθ(·), we measure the success of the attack using the targeted misclassiﬁcation rate:"
DATA AND THREAT MODEL,0.09146341463414634,"Aemp(θ; T) :=
X"
DATA AND THREAT MODEL,0.09451219512195122,"i∈T
[fθ(xi) = yi ∧fθ(τ(xi)) = π(yi)]
(7)"
DATA AND THREAT MODEL,0.0975609756097561,"In other words, the attack succeeds if, during inference, it can ﬂip the label of a correctly-classiﬁed
instance by applying the trigger τ(·)."
LEARNING OBJECTIVE,0.10060975609756098,"3.2
LEARNING OBJECTIVE"
LEARNING OBJECTIVE,0.10365853658536585,"We formulate our learning objective in the general data model of Equations 1-2. Without loss of
generality, we assume the ﬁrst p distributions are primary distributions, and the remaining n −p are
noise distributions. Given a training set S, we write SP = S1 ∪... ∪Sp for the samples from the
primary distributions, and SN = Sp+1 ∪... ∪Sn for the samples from the noise distributions."
LEARNING OBJECTIVE,0.10670731707317073,"Our goal is to learn parameters ˜θ which correspond to training only on the primary distributions:
˜θ := A(SP ). Note this objective differs signiﬁcantly from simply minimizing the risk over the
primary distributions when the mixed distribution D is realizable, i.e., we are also interested in
avoiding effects that occur on portions of the input space that have low density in the primary
distributions. More explicitly, in terms of the data poisoning threat model (Equation 7), we note that
training on SP ∪SN would yield low risk over the unperturbed distributions, but also high targeted
misclassiﬁcation risk. Conversely, for sufﬁciently separated distributions τ(Di) and Dπ(i), we expect
that the hypothesis class fθ enjoys low targeted misclassiﬁcation risk when trained only on clean
data."
SEPARATION OF MIXED DISTRIBUTIONS,0.10975609756097561,"4
SEPARATION OF MIXED DISTRIBUTIONS"
SEPARATION OF MIXED DISTRIBUTIONS,0.11280487804878049,"We next introduce the main theoretical properties that allow us to separate the primary and noise
distributions. Our main tool is a property of “self-expanding” sets; intuitively, given a set, we"
SEPARATION OF MIXED DISTRIBUTIONS,0.11585365853658537,Under review as a conference paper at ICLR 2022
SEPARATION OF MIXED DISTRIBUTIONS,0.11890243902439024,"resample at a given rate and measure how well the learning algorithm generalizes to the rest of the set.
Given primary and noise components satisfying certain compatibility properties, we show that the set
with the optimal expansion must be homogeneous, i.e., drawn entirely from either the primary or
noise components. Finally, we ﬁt weak learners to the recovered (homogeneous) sets in a simpliﬁed
boosting framework to identify the primary component."
SELF-EXPANSION AND COMPATIBILITY,0.12195121951219512,"4.1
SELF-EXPANSION AND COMPATIBILITY"
SELF-EXPANSION AND COMPATIBILITY,0.125,"We begin by stating a formal characterization of the self-expanding property of sets:
Deﬁnition 4.1 (Self-expansion of sets.). Let S and T be sets. We deﬁne the α-expansion error of S
given T for all 0 ≤α ≤1 such that α|S| is integral as"
SELF-EXPANSION AND COMPATIBILITY,0.12804878048780488,"ϵ(S|T; α) := |S|−1E[Remp(A(S′ ∪T); S)]
(8)"
SELF-EXPANSION AND COMPATIBILITY,0.13109756097560976,"where the expectation is over both the randomness in A and S′, a random variable of α|S| samples
drawn from S with replacement."
SELF-EXPANSION AND COMPATIBILITY,0.13414634146341464,"This self-expansion property measures the ability of the learning algorithm A to generalize to the
empirical distribution of a set S with the help of additional training samples T; intuitively, a smaller
expansion error means that the set S is both “easier” and “more homogeneous” with respect to the
learning algorithm and T. When T = ∅we will also write ϵ(S; α) instead of ϵ(S|∅; α). α is also
referred to as the subsampling rate. Finally, we will extend ϵ(S|T; α) to all 0 ≤α ≤1 by linearly
interpolating between the value at integral sample sizes."
SELF-EXPANSION AND COMPATIBILITY,0.13719512195121952,"We now use self-expansion to deﬁne a notion of compatibility between sets:
Deﬁnition 4.2 (Compatibility of sets.). A (nonempty) set T is α-compatible with set S with margin
δ ≥0 if"
SELF-EXPANSION AND COMPATIBILITY,0.1402439024390244,"ϵ(S|T; α) + δ ≤ϵ(S; α)
(9)"
SELF-EXPANSION AND COMPATIBILITY,0.14329268292682926,"where the expectation is over the same random variables as in the deﬁnition of self-expansion.
Furthermore, T is completely α-compatible with S if all (nonempty) subsets T ′ ⊆T are α-compatible
with S. Conversely, T is α-incompatible with S if the opposite holds, i.e.,"
SELF-EXPANSION AND COMPATIBILITY,0.14634146341463414,"ϵ(S; α) + δ ≤ϵ(S|T; α)
(10)"
SELF-EXPANSION AND COMPATIBILITY,0.14939024390243902,"(and similarly for complete incompatibility). We also say that strict compatibility (incompatibility)
holds when δ > 0."
SELF-EXPANSION AND COMPATIBILITY,0.1524390243902439,"In other words, T is compatible with S if the self-expansion error of S given T is not worse than the
self-expansion error of S by itself."
SELF-EXPANSION AND COMPATIBILITY,0.15548780487804878,"In what follows, we make use of the following assumptions about expansion:
Assumption 4.3 (Properties of expansion). The learning procedure satisﬁes the following:"
SELF-EXPANSION AND COMPATIBILITY,0.15853658536585366,"(1) ϵ(S|T; α) is a convex function of α ∈[0, 1] such that ϵ(S|T; 1) = 0 for all S, T"
SELF-EXPANSION AND COMPATIBILITY,0.16158536585365854,"(2) if T is α-incompatible with S, then T is β-incompatible with S for all β ≥α."
SELF-EXPANSION AND COMPATIBILITY,0.16463414634146342,"The ﬁrst assumption rules out the existence of pathological sets where increasing the number of
samples degrades performance and also says that memorization of the training set always occurs.
Convexity holds when the expected marginal information gained from additional samples decreases
as the number of samples increases. The second assumption says that increase the amount of data
from S in the training set (which always improves performance, regardless of the compatibility of T)
should not ﬂip an incompatible T into a compatible set."
SELF-EXPANSION AND COMPATIBILITY,0.1676829268292683,"The following key property enables us to separate the primary and noise distributions. Intuitively, we
want the primary and noise mixture components to be negatively correlated (or at least independent)
in the sense that training on a noise distribution should not improve performance on a primary
distribution, and vice versa:
Property 4.4 (Incompatibility of primary and noise distributions). Let α be given. Then any pair of
(nonempty) sets SP and SN drawn from D1 ∪... ∪Dp and Dp+1 ∪... ∪Dn, respectively, are strictly
and completely α-incompatible."
SELF-EXPANSION AND COMPATIBILITY,0.17073170731707318,Under review as a conference paper at ICLR 2022
SELF-EXPANSION AND COMPATIBILITY,0.17378048780487804,Our technique is designed for separating primary and noise distributions that satisfy this property.
SELF-EXPANSION AND COMPATIBILITY,0.17682926829268292,"We are now ready to state the main result of this section. Given Property 4.4, we show that any subset
of S which achieves the minimum expansion error consists entirely of data drawn from either the
primary or noise distributions:
Theorem 4.5 (Sets minimizing expansion error are homogeneous.). Let S = S1 ∪... ∪Sn be a set
of samples drawn from a mixture of distributions {(αi, Di}n
i=1. Deﬁne"
SELF-EXPANSION AND COMPATIBILITY,0.1798780487804878,"S∗:= arg min
S′⊆S ϵ(S′; α)
(11)"
SELF-EXPANSION AND COMPATIBILITY,0.18292682926829268,"for some expansion factor α. Then if Property 4.4 holds for α, we have either that S∗⊆SP or
S∗⊆SN, where SP = S1 ∪... ∪Sp and SN = Sp+1 ∪... ∪Sn."
SELF-EXPANSION AND COMPATIBILITY,0.18597560975609756,"We defer the proof to Appendix A. Intuitively, if two distributions are incompatible, then adding data
from one distribution to a homogeneous set of the other should only increase the self-expansion error."
SELF-EXPANSION AND COMPATIBILITY,0.18902439024390244,"Remark 1
Theorem 4.5 relies crucially on the incompatibility between the samples from the
primary distribution S1 ∪... ∪Sp and the noise distribution Sp+1 ∪... ∪Sn derive the homogeneity
of S∗, a condition which depends on the interaction between the data S and the learning algorithm A.
We note that the requirement is empirically satisﬁed in many data poisoning settings. For example,
a common adversary for backdoor attacks against deep neural networks inserts a small synthetic
patch in the corner of the image, which, by design, is a location on which the classiﬁcation does not
depend. In this case, the labels for the primary and noise distributions depend on disjoint dimensions
of the input, which gives a very clean example of incompatible distributions; given the number of
shared (spurious) features, empirical results suggest that the two distributions are, in fact, strictly
incompatible for moderately large sets as well."
SELF-EXPANSION AND COMPATIBILITY,0.19207317073170732,"Remark 2
In general, the larger α is in Property 4.4, the easier it is to estimate the value of ϵ(S; α);
the most convenient case would be for incompatibility to hold even when α = 1, in which case
ϵ(S; α) can be evaluated exactly with one call to A. Unfortunately, for overparameterized models
trained using empirical risk minimization, we have that ϵ(S; 1) = 0 for all S (since we assume the
problem is realizable in the limit). One method to circumvent this problem is to prevent A from
converging, e.g., by using early stopping. In fact, it is well known that regularizing deep neural
networks trained with Stochastic Gradient Descent using early stopping is resilient to noise (Li et al.,
2020). In our experiments, we ﬁnd that combining early stopping with our self-expansion property
leads to further improvements in performance."
IDENTIFICATION USING WEAK LEARNERS,0.1951219512195122,"4.2
IDENTIFICATION USING WEAK LEARNERS"
IDENTIFICATION USING WEAK LEARNERS,0.19817073170731708,"The development of the previous section suggests an iterative approach to separating the primary and
noise distributions. In particular, if we ﬁx the expansion factor α, at each step, we can identify the set
S∗which achieves the lowest expansion error and remove it from the training set. Repeating this
procedure partitions the training set into groups of compatible sets. While this sufﬁces to separate
the primary and noise distributions, it remains to identify which components belong to the primary
distribution."
IDENTIFICATION USING WEAK LEARNERS,0.20121951219512196,"We next propose a simpliﬁed boosting framework for identiﬁcation of the primary distribution. We
assume the setting of binary classiﬁcation and use the 0-1 loss, so that the empirical risk simply
counts the number of elements which are misclassiﬁed. Our approach is to ﬁt a weak learner to each
component, then use each learner to vote on the other components."
IDENTIFICATION USING WEAK LEARNERS,0.20426829268292682,"Algorithm 1 presents our approach for boosting from homogeneous sets. The subroutine Loss0,1
takes a set of parameters and a set of samples, and returns the empirical zero-one loss over the entire
set. Note that votes are weighted by size."
IDENTIFICATION USING WEAK LEARNERS,0.2073170731707317,"The correctness of Algorithm 1 follows from an analogous compatibility property (cf. Property 4.4):
Property 4.6 (Compatibility of primary distribution). Let α be given. Then any pair of (nonempty)
sets Si and Sj drawn from Di and Dj, respectively, such that i, j ≤n, are strictly and completely
α-compatible."
IDENTIFICATION USING WEAK LEARNERS,0.21036585365853658,"Finally, we also require unbiased priors for weak learners:"
IDENTIFICATION USING WEAK LEARNERS,0.21341463414634146,Under review as a conference paper at ICLR 2022
IDENTIFICATION USING WEAK LEARNERS,0.21646341463414634,Algorithm 1 Boosting Homogeneous Sets
IDENTIFICATION USING WEAK LEARNERS,0.21951219512195122,"Input: Homogeneous sets S1, ..., SN, total number of samples n, number of estimates B, weak
learner A
Output: Votes V1, ..., VN"
IDENTIFICATION USING WEAK LEARNERS,0.2225609756097561,"1: C1, ..., CN ←0
2: for i = 1 to N do
3:
Vi1, ..., ViN ←0
4:
for j = 1 to B do
5:
θij ←A(Si)
6:
for k = 1 to N do
7:
Vik ←Vik + Loss0,1(θij; Sk)/B
8:
end for
9:
end for
10:
for k = 1 to N do
11:
if Vik > |Sk|/2 then
12:
Ck ←Ck + |Si|
13:
end if
14:
end for
15: end for
16: for i = 1 to N do
17:
Vi ←Ci > n/2
18: end for"
IDENTIFICATION USING WEAK LEARNERS,0.22560975609756098,"Property 4.7 (Weak learners are unbiased.). Let Si be any sets. Then for any untrained weak learner,
we have also that E[Remp(A(∅); Si)] = |Si|/2."
IDENTIFICATION USING WEAK LEARNERS,0.22865853658536586,"This condition is necessary in that the weak learners should not be biased toward learning the noise
distributions. Finally, we state the main result of this section, whose proof is deferred to Appendix A.
Theorem 4.8 (Identiﬁcation of primary samples). Let S be a set of samples drawn from a mixture
of distributions {(αi, Di}n
i=1 such that Properties 4.4, 4.6, and 4.7 hold, and assume that the ratio
of primary samples p = |SP |/|S| > 1/2. Let S1, ..., SN be a partition of S produced by iteratively
applying Theorem 4.5. Then if A is deterministic, Algorithm 1 returns 1 with B = 1 for all
components containing samples from the primary distribution, and 0 otherwise."
IDENTIFICATION USING WEAK LEARNERS,0.23170731707317074,"If A is stochastic, the same result holds with probability"
IDENTIFICATION USING WEAK LEARNERS,0.2347560975609756,"[1 −2 exp(−2δ2B)]MN
(12)"
IDENTIFICATION USING WEAK LEARNERS,0.23780487804878048,"where M is the number of primary components, and B is the number of independent weak learners
used to ﬁt each primary component."
IDENTIFICATION USING WEAK LEARNERS,0.24085365853658536,"Remark 3
At a high level, the approach to identifying the primary distribution presented in
Theorems 4.5 and 4.8 follows a simpliﬁed boosting framework: at each step, we ﬁt a weak learner to
a subset of the distribution, then reweight the remaining training samples by removing the identiﬁed
component; the ensemble of weak learners is then aggregated using a majority vote. However, our
setting is somewhat unique so for clarity we mention several key differences. First, in general the
objective of standard boosting is to achieve low population risk, thus the reweighting is performed via
more sophisticated methods such as using the empirical loss of the ensemble thus far, e.g., AdaBoost
(Freund et al., 1996); in contrast, in our setting there are subpopulations over which we would actually
like to maximize the risk. Another difference is that in standard boosting, the ensemble is used during
inference to vote on new observations to perform classiﬁcation, whereas in our algorithm, we use
each learner to vote over components of the training set to ﬁlter out the noise distributions. Finally,
note that we can succeed with arbitrary probability by taking the number of samples B to inﬁnity."
INVERSE SELF-PACED LEARNING,0.24390243902439024,"5
INVERSE SELF-PACED LEARNING"
INVERSE SELF-PACED LEARNING,0.24695121951219512,"A major question raised by Theorem 4.5 is how to identify the set S∗in its statement. In this section,
we propose an algorithm called Inverse Self-Paced Learning (ISPL) to solve this problem. Rather"
INVERSE SELF-PACED LEARNING,0.25,Under review as a conference paper at ICLR 2022
INVERSE SELF-PACED LEARNING,0.2530487804878049,Algorithm 2 Inverse Self-Paced Learning
INVERSE SELF-PACED LEARNING,0.25609756097560976,"Input: training set S, total iterations N, annealing schedule 1 ≥β0 ≥... ≥βN = βmin > 0,
expansion α ≤1, momentum η, incremental learning procedure A, initial parameters θ0
Output: SN ⊆S such that |SN| = βN|S|"
INVERSE SELF-PACED LEARNING,0.25914634146341464,"1: S0 ←S
2: L ←0
3: for t = 1 to N do
4:
S′ ←Sample(St−1, α)
5:
θt ←A(S′, θt−1)
6:
L ←ηL + (1 −η)Remp(θt; S)
7:
St ←Trim(L, βt)
8: end for"
INVERSE SELF-PACED LEARNING,0.2621951219512195,"than optimizing over all possible subsets of the training data, our objective will instead be to minimize
the expansion error over subsets of ﬁxed size β|S|. The optimization objective is deﬁned as:
S∗
β := arg
min
S′⊆S:|S′|=β|S| ϵ(S′; α)
(13)"
INVERSE SELF-PACED LEARNING,0.2652439024390244,"We attempt to solve for S∗
β by alternating between optimizing parameters θt and the training subset
St. More explicitly, given S′ we update θ using a single subset from S′ of size α−1|S′|. Then we
use θ to compute the loss for each element in S, and set S′ to be the β fraction of the samples with
the lowest losses. To encourage stability of the learning algorithm, the losses are smoothed with an
optional momentum term η. We also anneal the parameter β from an initial value β0 down to the
target value βmin in order encourage more global exploration in the initial stages."
INVERSE SELF-PACED LEARNING,0.2682926829268293,"Algorithm 2 presents the full algorithm. In addition to the incremental learning procedure A (e.g.,
standard SGD), the subroutine Sample takes a training set S and returns α|S| elements uniformly at
random; while Trim takes losses L and returns the β|L| samples with the lowest loss."
INVERSE SELF-PACED LEARNING,0.27134146341463417,"Finally, we show for certain parameters that Algorithm 2 converges on the following objective over
the training set S:"
INVERSE SELF-PACED LEARNING,0.27439024390243905,"F(θt, vt; βt) :=
X"
INVERSE SELF-PACED LEARNING,0.2774390243902439,"i∈S
vt[i]L(fθt(xi), yi) + c max(0, βt|S| −|vt|)
(14)"
INVERSE SELF-PACED LEARNING,0.2804878048780488,"where vt is a 0-1 vector, βt is decreasing, and L(·, ·) ≤c.
Proposition 5.1. Let α = 1 and η = 0 in the setting of Algorithm 2, and assume that A returns
the empirical risk minimizer. Then we have that for each round of the algorithm, F(θt, vt; βt) is
decreasing in t and furthermore, |F(θt, vt; βt) −F(θt+1, vt+1; βt+1)|
t→∞
−−−→0."
INVERSE SELF-PACED LEARNING,0.28353658536585363,We defer the proof of Proposition 5.1 to Appendix A.
INVERSE SELF-PACED LEARNING,0.2865853658536585,"Remark 4
When α = 1 and η = 0, we recover vanilla self-paced learning (SPL) with two major
differences. First, we start on the full set of samples and train on incrementally smaller sets, while
SPL starts with a small set of samples and trains on larger sets. This discrepancy is due to the
differing objectives; whereas SPL is a heuristic for converging faster to a global minimizer of the
population loss by training ﬁrst on easy samples, ISPL attempts to converge to a local minimum over
a subpopulation. Second, our annealing schedule is deﬁned using the quantile statistics, while SPL
uses an absolute loss threshold that generally scales by a multiplicative factor in each iteration. We
chose this to counteract the propensity of deep neural networks to suddenly and rapidly interpolate
the training data; in our experiments, we found this behavior made the performance of ISPL very
sensitive to the speciﬁc annealing schedule when using absolute losses. Conversely, in SPL the ﬁnal
threshold is generally set high enough that most (or all) the samples are incorporated by the end, and
so the speciﬁc schedule may have a smaller impact on the ﬁnal performance."
EXPERIMENTAL EVALUATION,0.2896341463414634,"6
EXPERIMENTAL EVALUATION"
EXPERIMENTAL EVALUATION,0.2926829268292683,"We evaluate our defense against the standard patch-based backdoor attack with dirty labels, where
the adversary inserts a small patch into a training image from the source class, then changes the label"
EXPERIMENTAL EVALUATION,0.29573170731707316,Under review as a conference paper at ICLR 2022
EXPERIMENTAL EVALUATION,0.29878048780487804,"Figure 1: CIFAR-10 images with triggers applied (top), selected to maximize trigger visibility."
EXPERIMENTAL EVALUATION,0.3018292682926829,"Table 1: Results against dirty label backdoor adversary for select pairs of CIFAR-10 classes using
a single pixel trigger. The numbers in column 1 refer to the standard CIFAR-10 labels (e.g., 0 =
airplane, 1 = automobile, etc.). Column 2 gives the (x, y) coordinates of the trigger. S = source class,
T = target class, C = clean accuracy (higher is better), A = targeted misclassiﬁcation rate (lower is
better). Results for our method are in the last two columns under TW (this work)."
EXPERIMENTAL EVALUATION,0.3048780487804878,"S / T
pos
ϵ
No defense
SS
AC
ITLM
TW
C
A
C
A
C
A
C
A
C
A"
EXPERIMENTAL EVALUATION,0.3079268292682927,"2 / 5
(27, 9)
5
94.5
75.6
94.3
74.7
92.4
53.9
94.7
79.4
92.5
0.1
10
94.6
95.2
94.4
0.0
92.9
81.8
94.7
92.4
93.0
0.0
20
94.7
98.1
94.2
0.0
92.6
89.4
94.6
96.3
92.8
0.0"
EXPERIMENTAL EVALUATION,0.31097560975609756,"1 / 3
(15, 4)
5
94.8
99.3
94.6
50.0
92.2
39.2
94.6
57.2
92.0
0.1
10
94.5
99.2
94.2
10.4
92.0
47.9
94.5
75.0
92.9
0.3
20
94.5
98.8
94.3
1.5
91.9
60.5
94.2
92.3
92.3
1.3"
EXPERIMENTAL EVALUATION,0.31402439024390244,"8 / 6
(4, 1)
5
94.7
84.1
94.8
80.5
92.8
73.3
94.4
76.3
93.1
0.0
10
94.6
96.4
94.2
96.0
92.2
97.0
94.5
94.2
93.0
0.0
20
94.1
98.1
94.3
0.0
92.5
96.4
94.1
96.4
92.9
0.0"
EXPERIMENTAL EVALUATION,0.3170731707317073,"9 / 2
(4, 27)
5
94.9
98.0
94.4
65.7
92.8
79.7
94.7
97.6
93.0
0.0
10
94.9
99.1
94.6
0.0
92.6
80.8
94.4
99.3
92.9
0.0
20
94.7
99.1
94.3
0.0
93.1
98.9
94.1
99.1
93.1
0.0"
EXPERIMENTAL EVALUATION,0.3201219512195122,"of the image to the target class. The goal is to induce the learner to misclassify images from the
source class as the target class upon application of the patch. Results in this section use a standard
PreActResNet18 architecture (He et al., 2016b) that achieves 94% accuracy on CIFAR-10 when
trained on a clean dataset. The appendix provides full experimental details and additional results."
RESULTS,0.3231707317073171,"6.1
RESULTS"
RESULTS,0.32621951219512196,"Our implementation of the dirty label backdoor adversary follows the threat model described in
Gu et al. (2017). The perturbation function τ simply overlays a small pattern on the image. For
evaluation, we use the same dataset (CIFAR-10 (?)) and setup for our experiments as Tran et al.
(2018). Example pairs of clean and poisoned data are shown in Figure 1."
RESULTS,0.32926829268292684,"Table 1 presents results for the single-pixel backdoor attack, in which the adversary randomly selects
a position and color for the backdoor and applies the trigger by replacing the pixel at that position
with the selected color. The ﬁrst column, S / T, presents numbers in the form S / T, where S is the
source class and T is the target class in CIFAR-10. The goal of the attacker is to induce the network
to misclassify poisoned images from the S class to the T class. The second column, pos, presents
numbers in the form (X,Y) where X,Y is the position of the single pixel trigger. The third column, ϵ,
presents the percentage of the source class in the training set that is poisoned by the adversary."
RESULTS,0.3323170731707317,Under review as a conference paper at ICLR 2022
RESULTS,0.3353658536585366,"We report results for our defense, This Work (TW), in the last column, in addition to four baseline
defenses: 1) No defense, 2) Spectral Signatures (SS) (Tran et al., 2018), 3) Activation Clustering
(AC) (Chen et al., 2018), and 4) Iterative Trimmed Loss Minimization (ITLM) (Shen and Sanghavi,
2019). For each defense we report the percent accuracy over the clean images in the test set (column
C, higher is better, maximum is 100% when all clean images are classiﬁed correctly) and the targeted
misclassiﬁcation rate (Equation 7) over patched images of the target class in the test set (column A,
lower is better, minimum is 0% when none of the poisoned images are misclassiﬁed). The results show
that the technique we present in this paper 1) almost completely defends against this attack (column
A ranges from 0.0% to 1.3%) at the cost of 2) a small (roughly 2%) decrease in the clean accuracy
(column C, clean accuracies around 92-93%). All other defenses exhibit signiﬁcant vulnerability to
this attack (column A, No defense, SS, AC, and ITLM)."
DISCUSSION,0.3384146341463415,"6.2
DISCUSSION"
DISCUSSION,0.34146341463414637,"Many data poisoning defenses in the literature are evaluated on simpler datasets than those considered
in this paper, such as trafﬁc signs (GTSRB (Houben et al., 2013) or LISA (Mogelmose et al., 2012))
and MNIST (?). Furthermore, these datasets are tested in conjunction with larger or otherwise more
obvious triggers. For instance, the Neural Cleanse (Wang et al., 2019) defense uses a 4x4 white box
as the trigger on the MNIST and GTSRB datasets; MESA (Qiao et al., 2019) uses a 3x3 image as the
trigger on CIFAR-10 and test only at ϵ = 1%; TABOR (Guo et al., 2019) uses a 6x6 square as the
trigger for GTSRB for images that are 32x32 (they additionally test on ImageNet but do not report
good results until the trigger is over 25% of each dimension); and STRIP (Gao et al., 2019b) uses
an 8x8 box on CIFAR-10. Our hypothesis is that the combination of a smaller trigger and more
complex classes breaks defenses that demonstrate good performance in simpler contexts. Table 1
reports results using a single pixel trigger, which is often placed at the border of the image, within the
region cropped by the standard random cropping data augmentation during training."
DISCUSSION,0.3445121951219512,"For comparison, we implemented the AC defense, which is included in the Adversarial Robustness
Toolbox (Nicolae et al., 2019), an open-source collection of tools for security in machine learning.
The authors report that AC achieves nearly perfect performance on two popular settings, namely,
MNIST and trafﬁc signs. We also implemented ITLM, which was tested on CIFAR-10 at ϵ = 5%
using larger L- and X-shaped triggers. Our results in Table 1 indicate that both AC and ITLM fail to
completely defend against the poison in every setting, with the best targeted misclassiﬁcation rate
achieved by AC at 39.2% (compared to nearly 0% in every case with our defense)."
DISCUSSION,0.3475609756097561,"To the best of our knowledge, SS is the only other defense in the literature which is evaluated using
the same dataset (CIFAR-10) and class of triggers. The defense uses the eigenvectors of the feature
matrix to separate clean and poisoned data. However, the authors do not appear to use triggers that
can be cropped out during training by data augmentation. They also limit evaluation to “successfully”
attacked networks, which they deﬁne as over 90% targeted misclassiﬁcation rate of the undefended
network (Column A, No defense). While our experiments suggest that SS is the strongest baseline
after ours, successfully defending against the poison in 6 of the 12 scenarios considered in Table 1,
its performance is poor particularly at lower ϵ. Our results suggest that SS may fail to defend against
harder to learn triggers requiring more complex feature representations."
CONCLUSION,0.35060975609756095,"7
CONCLUSION"
CONCLUSION,0.35365853658536583,"Backdoor data poisoning attacks on deep neural networks are an emerging class of threats in the
growing landscape of deployed machine learning applications. Though defenses exist, our experi-
ments suggest that they only work against narrowly deﬁned adversaries and fail dramatically when
evaluated using more subtle threat models."
CONCLUSION,0.3567073170731707,"We introduce a new approach to defending against backdoor attacks based on an analysis of a novel
self-expansion property in the training data. For a poisoned dataset satisfying mild compatibility
properties, we show that an ensemble of weak learners ﬁt to self-expanding sets successfully removes
the poisoned data. Empirically, our method is resilient to a strong version of the dirty label backdoor
attack introduced by Gu et al. (2017), which successfully evades all the baseline defenses. We believe
our analysis and techniques present a valuable addition to the toolbox for secure deep learning."
CONCLUSION,0.3597560975609756,Under review as a conference paper at ICLR 2022
REFERENCES,0.3628048780487805,REFERENCES
REFERENCES,0.36585365853658536,"Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet. Turning your
weakness into a strength: Watermarking deep neural networks by backdooring. In 27th {USENIX}
Security Symposium ({USENIX} Security 18), pages 1615–1631, 2018."
REFERENCES,0.36890243902439024,"Maria-Florina Balcan, Avrim Blum, and Ke Yang. Co-training and expansion: Towards bridging
theory and practice. Advances in neural information processing systems, 17:89–96, 2005."
REFERENCES,0.3719512195121951,"Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
Proceedings of the 26th annual international conference on machine learning, pages 41–48, 2009."
REFERENCES,0.375,"Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung
Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by
activation clustering. arXiv preprint arXiv:1811.03728, 2018."
REFERENCES,0.3780487804878049,"Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep
learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017."
REFERENCES,0.38109756097560976,"Yoav Freund, Robert E Schapire, et al. Experiments with a new boosting algorithm. In ICML,
volume 96, pages 148–156. Citeseer, 1996."
REFERENCES,0.38414634146341464,"Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. Strip:
A defence against trojan attacks on deep neural networks. In 35th Annual Computer Security
Applications Conference (ACSAC), 2019a."
REFERENCES,0.3871951219512195,"Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C. Ranasinghe, and Surya Nepal.
Strip: A defence against trojan attacks on deep neural networks. In Proceedings of the 35th Annual
Computer Security Applications Conference, ACSAC ’19, page 113–125, New York, NY, USA,
2019b. Association for Computing Machinery. ISBN 9781450376280. doi: 10.1145/3359789.
3359790. URL https://doi.org/10.1145/3359789.3359790."
REFERENCES,0.3902439024390244,"Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent
neural networks. In 2013 IEEE international conference on acoustics, speech and signal processing,
pages 6645–6649. Ieee, 2013."
REFERENCES,0.3932926829268293,"Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the
machine learning model supply chain. arXiv preprint arXiv:1708.06733v1, 2017."
REFERENCES,0.39634146341463417,"Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the
machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2019."
REFERENCES,0.39939024390243905,"Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn Song. Tabor: A highly accurate approach
to inspecting and restoring trojan backdoors in ai systems. arXiv preprint arXiv:1908.01763, 2019."
REFERENCES,0.4024390243902439,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. arXiv preprint arXiv:1502.01852, 2015."
REFERENCES,0.4054878048780488,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770–778, 2016a."
REFERENCES,0.40853658536585363,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. arXiv preprint arXiv:1603.05027, 2016b."
REFERENCES,0.4115853658536585,"Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection of
trafﬁc signs in real-world images: The German Trafﬁc Sign Detection Benchmark. In International
Joint Conference on Neural Networks, number 1288, 2013."
REFERENCES,0.4146341463414634,"Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li.
Manipulating machine learning: Poisoning attacks and countermeasures for regression learning.
arXiv preprint arXiv:1804.00308, 2021."
REFERENCES,0.4176829268292683,Under review as a conference paper at ICLR 2022
REFERENCES,0.42073170731707316,"Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-
driven curriculum for very deep neural networks on corrupted labels. In International Conference
on Machine Learning, pages 2304–2313. PMLR, 2018."
REFERENCES,0.42378048780487804,"M Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models.
Advances in neural information processing systems, 23:1189–1197, 2010."
REFERENCES,0.4268292682926829,"Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is
provably robust to label noise for overparameterized neural networks. In International Conference
on Artiﬁcial Intelligence and Statistics, pages 4313–4324. PMLR, 2020."
REFERENCES,0.4298780487804878,"Deyu Meng, Qian Zhao, and Lu Jiang. What objective does self-paced learning indeed optimize?
arXiv preprint arXiv:1511.06049, 2016."
REFERENCES,0.4329268292682927,"Andreas Mogelmose, Mohan Manubhai Trivedi, and Thomas B. Moeslund. Vision-based trafﬁc sign
detection and analysis for intelligent driver assistance systems: Perspectives and survey. IEEE
Transactions on Intelligent Transportation Systems, 13(4):1484–1497, 2012. doi: 10.1109/TITS.
2012.2209421."
REFERENCES,0.43597560975609756,"Maria-Irina Nicolae, Mathieu Sinn, Minh Ngoc Tran, Beat Buesser, Ambrish Rawat, Martin Wistuba,
Valentina Zantedeschi, Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, Ian M. Molloy, and Ben
Edwards. Adversarial robustness toolbox v1.0.0. arXiv preprint arXiv:1807.01069, 2019."
REFERENCES,0.43902439024390244,"Ximing Qiao, Yukun Yang, and Hai Li. Defending neural backdoors via generative distribution
modeling. arXiv preprint arXiv:1910.04749, 2019."
REFERENCES,0.4420731707317073,"Philippe Rigollet. Generalization error bounds in semi-supervised classiﬁcation under the cluster
assumption. Journal of Machine Learning Research, 8(7), 2007."
REFERENCES,0.4451219512195122,Matthias Seeger. Learning with labeled and unlabeled data. 2000.
REFERENCES,0.4481707317073171,"Yanyao Shen and Sujay Sanghavi. Learning with bad training data via iterative trimmed loss
minimization. arXiv preprint arXiv:1810.11874, 2019."
REFERENCES,0.45121951219512196,"David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016."
REFERENCES,0.45426829268292684,"Aarti Singh, Robert Nowak, and Jerry Zhu. Unlabeled data: Now it helps, now it doesn’t. Advances
in neural information processing systems, 21:1513–1520, 2008."
REFERENCES,0.4573170731707317,"Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. arXiv
preprint arXiv:1811.00636, 2018."
REFERENCES,0.4603658536585366,"Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y
Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019
IEEE Symposium on Security and Privacy (SP), pages 707–723. IEEE, 2019."
REFERENCES,0.4634146341463415,"Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma. Theoretical analysis of self-training with
deep networks on unlabeled data. arXiv preprint arXiv:2010.03622, 2020."
REFERENCES,0.46646341463414637,"Xuchao Zhang, Xian Wu, Fanglan Chen, Liang Zhao, and Chang-Tien Lu. Self-paced robust learning
for leveraging clean labels in noisy data. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 34, pages 6853–6860, 2020."
REFERENCES,0.4695121951219512,Under review as a conference paper at ICLR 2022
REFERENCES,0.4725609756097561,"A
DEFERRED PROOFS"
REFERENCES,0.47560975609756095,"Lemma A.1. Let SN and SP be two sets satisfying Property 4.4, and let S and T be drawn from
SN and SP , respectively. Then for all 0 < γ ≤α, S and T are mutually strictly γ-incompatible."
REFERENCES,0.47865853658536583,"Proof. We will show that T is γ-incompatible with S for all γ ≤α; mutual incompatibility follows
by a symmetric argument. First, T is α-incompatible with S by consequence of Property 4.4. Fix
γ < α. Our main approach will be to subsample S twice: ﬁrst, we sample S at a rate of γ/α to create
S′ such that |S′| ≈γ/α|S|. Then S′ is α-incompatible with T, so ϵ(S′|T, α) ≥ϵ(S′, α). However
we need take some care to ensure γ/α|S| is an integer. Deﬁne"
REFERENCES,0.4817073170731707,"α′ :=
γ|S|
⌊γ|S|/α⌋
(15)"
REFERENCES,0.4847560975609756,"Then α′ ≥α and γ/α′|S| = ⌊γ|S|/α⌋. Thus, we will subsample S at a rate of γ/α′ to create S′; by
Assumption 4.3, T is α′-incompatible with S′. Note that the resulting training sets produced by this
double subsampling procedure have size γ|S| as desired."
REFERENCES,0.4878048780487805,"Next, we will show that there exists some constant c such that"
REFERENCES,0.49085365853658536,"ϵ(S|T, γ) = c(γ/α′|S|)−1E[ϵ(S′|T, α′)]
(16)"
REFERENCES,0.49390243902439024,"ϵ(S, γ) = c(γ/α′|S|)−1E[ϵ(S′, α′)]
(17)"
REFERENCES,0.4969512195121951,"where the expectations are taken over the subset S′ ⊂S, |S′| = γ/α′|S|. Then since T is strictly
α′-incompatible with all such S′ by Property 4.4,"
REFERENCES,0.5,"ϵ(S|T, γ) = c|S′|−1E[ϵ(S′|T, α′)]
(18)"
REFERENCES,0.5030487804878049,"> c|S′|−1E[ϵ(S′, α′)]
(19)
= ϵ(S, γ)
(20)"
REFERENCES,0.5060975609756098,which is what we wanted to prove.
REFERENCES,0.5091463414634146,"Fix a training set S′′, |S′′| = γ|S|, and let T be arbitrary. By Assumption 4.3 we have that
Remp(A(S′′ ∪T); S′′) = 0 for all T ′. Then conditioning on the training set S′′, we have that"
REFERENCES,0.5121951219512195,"E[ϵ(S′|T, α′)|S′′] = E[Remp(A(S′′ ∪T); S′)|S′′]
(21)"
REFERENCES,0.5152439024390244,"= E[Remp(A(S′′ ∪T); S′ −S′′)|S′′]
(22)"
REFERENCES,0.5182926829268293,"where the expectation is over S′ such that S′′ ⊂S′. Now S′ −S′′ is a random variable consisting of
|S′| −|S′′| independent draws from S with replacement; thus Equation 22 is just equal to |S′| −|S′′|
times the empirical risk of a random element in S. On the other hand, Remp(A(S′′ ∪T); S) is
the empirical risk over all elements in S, or equivalently, |S| times the empirical risk of a random
element."
REFERENCES,0.5213414634146342,"Since T was arbitrary, summing over all possible training sets S′′ yields the desired identities with"
REFERENCES,0.524390243902439,"c =
|S|
|S′| −|S′′|.
(23)"
REFERENCES,0.5274390243902439,"Proof of Theorem 4.5. We ﬁrst prove a slightly more general result. Assume by way of contradic-
tion that there exists a partition of S∗into two nonempty sets P and Q that are mutually strictly
incompatible."
REFERENCES,0.5304878048780488,Let ϵ∗be the expansion error of S∗. Recall that this means
REFERENCES,0.5335365853658537,"|S∗|ϵ∗= |S∗|ϵ(S∗; α) = E[Remp(A(S′); S∗)]
(24)"
REFERENCES,0.5365853658536586,"where the expectation is taken over samples S′ of size α−1|S∗| drawn from S∗with replacement.
Since the empirical risk is a linear function of S∗, we can decompose this last term as"
REFERENCES,0.5396341463414634,"E[Remp(A(S′); S∗)] = E[Remp(A(S′); P)] + E[Remp(A(S′); Q)]
(25)"
REFERENCES,0.5426829268292683,Under review as a conference paper at ICLR 2022
REFERENCES,0.5457317073170732,"Given a set of training samples S′, we will denote the elements drawn from P and Q as P ′ = S′ ∩P
and Q′ = S′ ∩Q, respectively. Now consider the term"
REFERENCES,0.5487804878048781,"E[Remp(A(S′); P)] = E[Remp(A(P ′ ∪Q′); P)]
(26)"
REFERENCES,0.551829268292683,"If we ﬁx Q′, then P ′ is drawn uniformly at random from P with replacement, where |P ′| =
α|S∗| −|Q′|. Deﬁne"
REFERENCES,0.5548780487804879,αQ′ := α|S∗| −|Q′|
REFERENCES,0.5579268292682927,"|P|
(27)"
REFERENCES,0.5609756097560976,"which is the subsampling rate of P ′ given Q′. Note that if α′ ≥α, then Q′ is α′-incompatible with
P ′ by Assumption 4.3, and otherwise α′ < α and Q′ is strictly α′-incompatible with P ′ by Lemma
A.1. Thus,"
REFERENCES,0.5640243902439024,"E[Remp(A(P ′ ∪Q′); P)] =
X"
REFERENCES,0.5670731707317073,"Q′
Pr[Q′]E[Remp(A(P ′ ∪Q′); P)|Q′]
(28)"
REFERENCES,0.5701219512195121,"= |P|
X"
REFERENCES,0.573170731707317,"Q′
Pr[Q′]ϵ(P|Q′; αQ′)
(29)"
REFERENCES,0.5762195121951219,"> |P|
X"
REFERENCES,0.5792682926829268,"Q′
Pr[Q′]ϵ(P; αQ′)
(30)"
REFERENCES,0.5823170731707317,"On the other hand, E[αQ′] = α, and since ϵ(P; α) is convex in α by Assumption 4.3, we can apply
Jensen’s inequality to conclude
X"
REFERENCES,0.5853658536585366,"Q′
Pr[Q′]ϵ(P; αQ′) ≥ϵ(P; α),
(31) i.e.,"
REFERENCES,0.5884146341463414,"E[Remp(A(P ′ ∪Q′); P)] > |P|ϵ(P; α).
(32)"
REFERENCES,0.5914634146341463,"Similarly,"
REFERENCES,0.5945121951219512,"E[Remp(A(P ′ ∪Q′); Q)] > |Q|ϵ(Q; α).
(33)"
REFERENCES,0.5975609756097561,Combining these two results yields
REFERENCES,0.600609756097561,"|S∗|ϵ∗> |P|ϵ(P; α) + |Q|ϵ(Q; α).
(34)"
REFERENCES,0.6036585365853658,"Since |P| + |Q| = |S∗|, we have that at least one of ϵ(P; α) or ϵ(Q; α) must be less than ϵ∗, which
contradicts the optimality of S∗. Thus one of P or Q must be empty."
REFERENCES,0.6067073170731707,"Finally, we note that by Property 4.4, the partition P = S∗∩SP and Q = S∗∩SN gives an
incompatible partition, which yields the result."
REFERENCES,0.6097560975609756,"Proof of Theorem 4.8. We begin with the simple observation that if Properties 4.4 and 4.6 hold for S,
they also hold for S \ S′ for any set S′. Thus we are able to apply Theorem 4.5 at each step and so in
fact S1, ..., SN are all homogeneous. Additionally, since p > 1/2 and a component’s vote is weighted
by its size, a sufﬁcient condition for success is when all the primary components vote correctly."
REFERENCES,0.6128048780487805,"We start with the case when A is deterministic. Let Si and Sj be a primary component and
noise component, respectively.
By strict incompatibility, we have that Remp(A(Si); Sj) >
Remp(A(∅); Sj) = |Sj|/2. Thus Si votes 0 on Sj. Conversely, if Sj is a primary component,
then Remp(A(Si); Sj) < Remp(A(∅); Sj) = |Sj|/2, so Si votes 1 on Sj. Putting these together and
using the fact that p > 1/2, we ﬁnd that the noise components have weighted vote strictly less than
|S|/2, while the primary components have weighted vote strictly greater than |S|/2, as required."
REFERENCES,0.6158536585365854,"For the case when A is stochastic, we apply standard concentration bounds to our estimates of the
empirical risk of each component (over the randomness in A). Let Si and Sj be a primary and noise
component, respectively. Again, strict incompatibility gives Remp(A(Si); Sj) ≥Remp(A(∅); Sj) +
δ|Sj| = (1/2 + δ)|Sj|. Deﬁne the sample average empirical risk"
REFERENCES,0.6189024390243902,"VB :=
1
B|Sj| B
X"
REFERENCES,0.6219512195121951,"b=1
Remp(Ab(Si); Sj)
(35)"
REFERENCES,0.625,Under review as a conference paper at ICLR 2022
REFERENCES,0.6280487804878049,"computed from B samples over the randomness in A. Then E[VB] ≥(1/2+δ) and so by Hoeffding’s
inequality"
REFERENCES,0.6310975609756098,"Pr[|VB < 1/2] ≤Pr[|VB −(1/2 + δ)| > δ]
(36)"
REFERENCES,0.6341463414634146,"< 2 exp(−2δ2B)
(37)"
REFERENCES,0.6371951219512195,"Thus with probability at least 1 −2 exp(−2δ2B), primary component Si votes 0 when Sj is a noise
component. By the same argument and using strict compatibility, the bound also holds for Si voting
1 when Si and Sj are both from primary components. Putting this together, we recall that a sufﬁcient
condition for success of the algorithm occurs when all the primary components vote correctly on all
components (both primary and noise), which happens with probability at least"
REFERENCES,0.6402439024390244,"[1 −2 exp(−2δ2B)]MN
(38)"
REFERENCES,0.6432926829268293,as claimed.
REFERENCES,0.6463414634146342,"Proof of Proposition 5.1. The statement is more or less a direct consequence of the alternating convex
minimization strategy. Recall ﬁrst that since α = 1 and η = 0, Lines 4 and 6 in Algorithm 2 have no
effect."
REFERENCES,0.649390243902439,We prove the statement in two steps. First we claim that
REFERENCES,0.6524390243902439,"F(θt+1, vt; βt) ≤F(θt, vt; βt)
(39)"
REFERENCES,0.6554878048780488,"Note that vt in the optimization objective F plays the role of St in Algorithm 2. The inequality follows
from the optimization on Line 5 in Algorithm 2, which sets θt+1 to the empirical risk minimizer of
the set vt."
REFERENCES,0.6585365853658537,"Next, we claim that"
REFERENCES,0.6615853658536586,"F(θt+1, vt+1; βt+1) ≤F(θt+1, vt; βt)
(40)"
REFERENCES,0.6646341463414634,"Since 0 ≤L(·, ·) < c, the optimal size of the set St is |vt| = βt|S|. Since βt is decreasing, we have
that |vt+1| ≤|vt|. Thus the number of elements in the trimmed empirical loss is non-increasing (Line
7)."
REFERENCES,0.6676829268292683,"Combining the two inequalities shows that the objective function is decreasing in t. Since F(θt, vt; βt)
is a decreasing sequence bounded from below by zero, the monotone convergence theorem gives the
second result."
REFERENCES,0.6707317073170732,Under review as a conference paper at ICLR 2022
REFERENCES,0.6737804878048781,"B
EXPERIMENTAL DETAILS"
REFERENCES,0.676829268292683,"B.1
DEFENSE SET UP AND HYPERPARAMETERS"
REFERENCES,0.6798780487804879,"ISPL + Boosting (this work).
For our defense, we use the same set of hyperparameters across all
experiments. We run 8 rounds of ISPL, each of which returns a component consisting of roughly
12% of the total samples. Let p be the target percentage of samples over the remaining samples (i.e.,
p ≈1/(8 −i + 1) in the ith iteration). Then the number of iterations N is set to 2 + min(3, 1/p). β
starts at 3 ∗p in the ﬁrst iteration, then drops linearly to its ﬁnal value of p over the next 2 iterations.
When trimming the training set, we also additionally include the top p/8 samples per class to prevent
the network from collapsing to a trivial solution. For the learning procedure A, we use standard SGD,
trained for 4 epochs per iteration, with a warm-up in the ﬁrst iteration of 8 epochs. The expansion
factor α is set to 1/4, and the momentum factor η is set to 0.9."
REFERENCES,0.6829268292682927,"We run ISPL 3 times to generate 24 weak learners. Each weak learner is trained for 40 epochs on its
respective subset. For the boosting framework, each component votes on a per-sample basis. The
sample is preserved if the modal vote equals the given label, with ties broken randomly."
REFERENCES,0.6859756097560976,"We also include a ﬁnal self-training step by training a fresh model for 100 epochs on the recovered
samples. The main idea is that a model ﬁt to the full “clean” training data can be used to test the
excluded training data, thereby recovering additional consistent data which may have been originally
excluded because the weak learners were ﬁt to a small subset of data for fewer epochs. However,
it may take several repetitions of training a model from scratch before this self-training process no
longer identiﬁes new samples to recover. Therefore, we use a simple self-paced learning algorithm to
dynamically adjust the samples during training to limit the self-training to a single iteration. More
explicitly, we start with the “clean” samples as returned by the boosting framework. Every 5 epochs,
we update the training set to be the samples whose labels agree with the model’s current predictions.
Due to the relative frequency with which we resample the training set, we smooth the predictions by
a momentum factor of 0.8 so that the training process is less noisy. The samples used for training
in the last epoch are returned as the defended dataset. In our experiments, this process decreases
the false positive rate (and thus increases the clean accuracy) but does not materially affect the false
negative rate (nor the targeted misclassiﬁcation rate)."
REFERENCES,0.6890243902439024,"Spectral Signatures.
We use the ofﬁcial implementation of the Spectral Signatures defense (Tran
et al., 2018) by the authors, available on Github, except that we replace the training procedure with
PyTorch (instead of Tensorﬂow 1.x as in the authors’ original implementation). The authors suggest
removing 1.5 times the maximum expected amount of poison from each class for the defense. We
remove 20% of each class for ϵ = 5, 10% and 30% of each class for ϵ = 20%. In selecting the layer
for the activations, for the ResNet32 architecture, we use the input to the third block of the third layer
(which matches the authors’ implementation), and for the PreActResNet18 architecture, we use the
input to the ﬁrst block of the four layer (which was found empirically to remove the most poison on
the ﬁrst set of scenarios). We note that the authors indicate the defense should be fairly successful at
any of the later layers of the network."
REFERENCES,0.6920731707317073,"Iterative Trimmed Loss Minimization.
The Iterative Trimmed Loss Minimization defense (Shen
and Sanghavi, 2019) consists of an iterative procedure. Given a setting 0 < α ≤1, one ﬁrst trains a
model for a number of epochs. Then the α fraction of samples with the lowest loss are retained for
the next iteration. This process is repeated several times, with a fresh model beginning each iteration.
The defended dataset is the α fraction of samples with the lowest loss after the last iteration. For the
backdoor data poisoning experiments on CIFAR-10, the authors use 80 epochs for the ﬁrst round of
training, then 40 epochs thereafter; they also set α = 98% for ϵ = 5%, and do not test at other values
of ϵ. We use the same settings, and scale α linearly with ϵ, i.e., α = 96% for ϵ = 10% and α = 92%
for ϵ = 20%."
REFERENCES,0.6951219512195121,"Activation Clustering.
The Activation Clustering defense (Chen et al., 2018) has an actively
maintained ofﬁcial implementation in the Adversarial Robustness Toolbox (ART) (Nicolae et al.,
2019), an open-source collection of tools for security in machine learning. We use the ofﬁcial
implementation with the default parameters values in ART v1.6.2, the most current version at the"
REFERENCES,0.698170731707317,Under review as a conference paper at ICLR 2022
REFERENCES,0.7012195121951219,"time of writing. In selecting the layer for the activations, we used the same layer as for Spectral
Signatures."
REFERENCES,0.7042682926829268,"Models.
The PreActResNet18 He et al. (2016b) model is optimized using vanilla SGD with learning
rate 0.02, momentum 0.9, and weight decay 5e-4. For the ﬁnal dataset, we train for 200 epochs and
drop the learning rate by 10 at epochs 100, 150, and 180. Using these parameters, we achieve 94.7%
accuracy on CIFAR-10 when trained and tested with clean data."
REFERENCES,0.7073170731707317,"The ResNet32 He et al. (2016a) model is optimized using vanilla SGD with learning rate 0.1,
momentum 0.9, and weight decay 1e-4. For the ﬁnal dataset, we train for 200 epochs and drop the
learning rate by 10 at epochs 100 and 150. Using these parameters, we achieve 91.8% accuracy on
CIFAR-10 when trained and tested with clean data."
REFERENCES,0.7103658536585366,"B.2
BACKDOOR POISON DATASET CONSTRUCTION"
REFERENCES,0.7134146341463414,"Each scenario has a single source and target class. We use the same (source, target) pairs as in Tran
et al. (2018): (airplane, bird), (automobile, cat), (bird, dog), (cat, dog), (cat, horse), (horse, deer),
(ship, frog), (truck, bird)."
REFERENCES,0.7164634146341463,"To generate a perturbation, we choose a shape (L-shape, X-shape, or pixel) uniformly at random.
The (X,Y) coordinates of the perturbation are randomly selected to guarantee that the entire shape is
visible before data augmentation (e.g., the pixel-based perturbation can be placed anywhere within
the 32x32 image, but the X-shape is larger and so must be centered in a 30x30 region, one pixel
away from the border). The color of the perturbation is also selected uniformly at random, with each
of the (R,G,B) coordinates ranging from 0 to 255. Finally, we randomly select an ϵ = 5, 10, 20%
percentage of the source class, apply the perturbation by replacing the pixels in the corresponding
locations with the selected shape and color, then relabel the poisoned images as the target class."
REFERENCES,0.7195121951219512,"Table 2 displays the generated triggers used in our experiments with examples of poisoned images.
Within the row for each (source, target) pair, the ﬁrst subrow gives the parameters for poison 1, the
second subrow gives the parameters for poison 2, and the third subrow gives the parameters for
poison 3. We also provide an example of the corresponding clean image for poison 1 in column clean
1. Note that the results presented in Table 1 of the main paper use the ﬁrst scenario of each (source,
target) pair (poison 1)."
REFERENCES,0.7225609756097561,Under review as a conference paper at ICLR 2022
REFERENCES,0.725609756097561,Table 2: CIFAR-10 dirty label backdoor scenarios.
REFERENCES,0.7286585365853658,"source
target
color
position
method
clean 1
poison 1
poison 2
poison 3"
REFERENCES,0.7317073170731707,"(103, 87, 79)
(24, 3)
pixel"
REFERENCES,0.7347560975609756,"0 / Plane
2 / Bird
(92, 1, 189)
(27, 30)
pixel"
REFERENCES,0.7378048780487805,"(47, 2, 21)
(21, 8)
pixel"
REFERENCES,0.7408536585365854,"(180, 98, 53)
(10, 30)
pixel"
REFERENCES,0.7439024390243902,"1 / Car
3 / Cat
(40, 105, 92)
(25, 13)
pixel"
REFERENCES,0.7469512195121951,"(145, 70, 200)
(9, 29)
pixel"
REFERENCES,0.75,"(93, 86, 130)
(27, 9)
pixel"
REFERENCES,0.7530487804878049,"2 / Bird
5 / Dog
(156, 158, 244)
(18, 30)
pixel"
REFERENCES,0.7560975609756098,"(74, 162, 26)
(11, 9)
pixel"
REFERENCES,0.7591463414634146,"(34, 241, 240)
(2, 14)
L"
REFERENCES,0.7621951219512195,"3 / Cat
5 / Dog
(239, 42, 58)
(28, 13)
pixel"
REFERENCES,0.7652439024390244,"(39, 221, 162)
(7, 23)
X"
REFERENCES,0.7682926829268293,"(61, 14, 183)
(16, 2)
X"
REFERENCES,0.7713414634146342,"3 / Cat
7 / Horse
(180, 50, 21)
(11, 0)
pixel"
REFERENCES,0.774390243902439,"(4, 221, 78)
(24, 22)
L"
REFERENCES,0.7774390243902439,"(107, 60, 58)
(15, 4)
pixel"
REFERENCES,0.7804878048780488,"7 / Horse
4 / Deer
(242, 30, 233)
(4, 21)
X"
REFERENCES,0.7835365853658537,"(76, 14, 15)
(11, 19)
L"
REFERENCES,0.7865853658536586,"(141, 245, 211)
(4, 1)
pixel"
REFERENCES,0.7896341463414634,"8 / Ship
6 / Frog
(213, 221, 138)
(19, 29)
X"
REFERENCES,0.7926829268292683,"(121, 158, 6)
(3, 13)
pixel"
REFERENCES,0.7957317073170732,"(187, 67, 135)
(4, 27)
pixel"
REFERENCES,0.7987804878048781,"9 / Truck
2 / Bird
(69, 204, 11)
(14, 29)
L"
REFERENCES,0.801829268292683,"(239, 186, 219)
(1, 29)
X"
REFERENCES,0.8048780487804879,Under review as a conference paper at ICLR 2022
REFERENCES,0.8079268292682927,"Table 3: Performance on CIFAR-10, dirty label backdoor scenario, using the PreActResNet18
architecture. The S / T column lists the CIFAR-10 source and target classes. ϵ refers to the percentage
of the source class which is poisoned. For the remainder of the columns, the top level column headers
give the defense type: L (clean), ND (no defense), SS (spectral signatures), AC (activation clustering),
ITLM (iterative trimmed loss minimization, and TW (this work); the second level column headers
give the metric type: M (misclassiﬁcation rate), C (clean accuracy, higher is better), A (targeted
misclassiﬁcation rate, lower is better), FP (false positives, lower is better), FN (false negatives, lower
is better). Please refer to the text for a more detailed explanation of the table."
REFERENCES,0.8109756097560976,"S / T
ϵ
L
ND
SS
AC
ITLM
TW
M
C
A
C
A
FP
FN
C
A
FP
FN
C
A
FP
FN
C
A
FP
FN"
REFERENCES,0.8140243902439024,"0 / 2
5
1.3 94.5 91.3 94.5 79.9
7381
130 91.9 58.3 18926 155 94.6 84.9
994
244 92.8
0.0
3640
23
10
1.3 94.1 90.6 94.5 66.0
7383
383 92.3 85.9 19790 320 94.2 92.8 1961 461 92.8
0.0
3479
25
20
1.1 94.6 80.2 94.2 64.2 14335 335 92.4 73.9 19127 601 93.8 93.8 3897 897 91.6 22.9 3711 237"
REFERENCES,0.8170731707317073,"1 / 3
5
0.0 94.4 92.9 94.7
5.5
7319
68
90.9 19.5 19791 155 94.8 96.5
986
236 92.8
0.0
3434
2
10
0.0 94.6 98.4 94.5
0.0
7035
35
92.5 68.7 19033 333 94.6 98.0 1981 481 93.0
0.0
3348
3
20
0.0 94.4 99.6 94.6
0.0
14007
7
92.2 91.8 18317 622 94.4 99.4 3914 914 93.0
0.0
3253
2"
REFERENCES,0.8201219512195121,"2 / 5
5
1.1 94.4 80.4 94.6 76.4
7494
243 92.4 53.9 19937 172 94.7 79.4
985
235 92.6
0.2
3704
5
10
0.9 94.4 97.2 94.4
0.1
7053
53
92.9 81.8 18657 313 94.7 92.4
990
490 92.9
0.2
3200
16
20
1.2 94.4 94.0 94.5 87.7 14263 263 92.6 89.4 20531 406 94.6 95.4
966
966 92.8
0.3
3540
38"
REFERENCES,0.823170731707317,"3 / 5
5
7.6 94.7 91.0 94.7 88.5
7281
30
92.6 90.8 19172 172 94.5 91.4
993
243 92.8 81.1 3693 167
10
5.9 94.8 94.0 94.4
8.6
7014
14
92.3 91.6 20293 296 94.6 92.3
988
488 92.6 90.2 3355 496
20
7.6 94.7 90.8 94.3 90.6 14092
92
92.6 92.6 18236 652 94.7 91.7
974
974 92.6 87.4 3210 995"
REFERENCES,0.8262195121951219,"3 / 7
5
0.6 94.3 33.8 94.6 98.3
7500
249 91.9 97.2 21289 246 94.7 98.4
995
245 92.7
0.0
3692
5
10
0.7 94.4 98.5 94.6 96.6
7135
135 92.2 98.2 20630 484 94.5 98.6 1979 479 92.7
2.4
3427
22
20
0.7 94.4 98.5 93.7 78.5 14675 675 92.3 98.2 20470 987 94.4 98.9
980
980 92.9 10.5 3259
43"
REFERENCES,0.8292682926829268,"7 / 4
5
1.5 94.7 92.0 94.6
0.6
7280
29
92.3 39.2 19353 150 94.8 91.6
992
242 92.8
0.5
3258
30
10
1.5 94.6 94.7 94.4
0.0
7023
23
92.0 47.9 19631 285 94.4 94.5 1979 479 93.0
0.3
3627 293
20
1.5 94.6 96.5 94.3
0.1
14000
0
92.1 78.6 21292
22
94.4 96.5 3910 910 92.8 84.5 3203 867"
REFERENCES,0.8323170731707317,"8 / 6
5
0.2 94.7 97.0 94.8 80.5
7470
219 92.8 73.3 19293 152 94.5 98.3
993
243 92.8
0.0
3494
2
10
0.2 94.4 99.5 94.7
0.0
7008
8
92.6 97.6 19544 288 94.4 99.4 1981 481 92.9
0.0
3571
1
20
0.2 94.7 99.5 94.4
0.0
14000
0
92.5 96.4 18946 597 94.2 99.5 3908 908 92.6
0.2
3492
8"
REFERENCES,0.8353658536585366,"9 / 2
5
0.1 95.0 92.0 94.4 93.3
7501
250 91.7 85.0 23573 151 94.6 97.3
988
238 93.0
0.0
3291
2
10
0.1 94.5 93.9 94.3 93.1
7500
500 92.1 95.1 22896 251 94.4 98.6 1970 471 93.1
0.0
3133
1
20
0.1 94.4 96.1 94.7
0.0
14010
10
93.1 98.9 18651 575 94.1 99.0 3906 906 93.1
0.0
3223
2"
REFERENCES,0.8384146341463414,"C
ADDITIONAL EXPERIMENTAL RESULTS"
REFERENCES,0.8414634146341463,"Tables 3 and 4 summarizes our main results for all the (source, target) pairs using two standard
architectures for image classiﬁcation: a PreActResNet18 He et al. (2016b) network and a ResNet32
He et al. (2016a) network, respectively."
REFERENCES,0.8445121951219512,"For each (source, target) pair, we generated three scenarios. For each (source, target) pair and setting
of epsilon, we report results for the scenario in which the defense’s targeted misclassiﬁcation rate
(column A) was the median of all three scenarios. For the clean and no defense columns, we report
results for the same scenario as TW (This Work)."
REFERENCES,0.8475609756097561,The set of defenses consists of
REFERENCES,0.850609756097561,"1. (L) Clean, training on the entire clean training set. We report only the misclassiﬁcation rate
(M), which is the number of poisoned samples from the test set of the source class that are
misclassiﬁed as the target class.
2. (ND) No Defense, training on entire poisoned training set. We report only the clean accuracy
(C) and targeted misclassiﬁcation rate (A) in this case.
3. (SS) Spectral Signatures (Tran et al., 2018)
4. (AC) Activation Clustering (Chen et al., 2018)"
REFERENCES,0.8536585365853658,Under review as a conference paper at ICLR 2022
REFERENCES,0.8567073170731707,"Table 4: Performance on CIFAR-10, dirty label backdoor scenario, using the ResNet32 architecture.
The S / T column lists the CIFAR-10 source and target classes. ϵ refers to the percentage of the
source class which is poisoned. For the remainder of the columns, the top level column headers give
the defense type: L (clean), ND (no defense), SS (spectral signatures), AC (activation clustering),
ITLM (iterative trimmed loss minimization, and TW (this work); the second level column headers
give the metric type: M (misclassiﬁcation accuracy), C (clean accuracy, higher is better), A (targeted
misclassiﬁcation rate, lower is better), FP (false positives, lower is better), FN (false negatives, lower
is better). Please refer to the text for a more detailed explanation of the table."
REFERENCES,0.8597560975609756,"S / T
ϵ
L
ND
SS
AC
ITLM
TW
M
C
A
C
A
FP
FN
C
A
FP
FN
C
A
FP
FN
C
A
FP
FN"
REFERENCES,0.8628048780487805,"0 / 2
5
1.1 92.6 85.0 91.8 57.3
7499
248 88.8
0.6
24211 133 91.8 83.3
988
238 89.8
0.0
5752
31
10
1.1 92.4 92.6 91.7 91.5
7422
422 88.8 78.6 24031 250 91.7 92.3 1969 469 89.5
0.0
5445
33
20
1.5 92.5 94.9 90.4 64.2 14590 590 88.6 62.7 23686 455 91.9 94.2 3890 890 88.0
0.0
6992
86"
REFERENCES,0.8658536585365854,"1 / 3
5
0.1 92.7 98.5 91.7
5.7
7479
227 88.8
2.4
23903 125 91.9 12.7
988
238 89.8
0.0
5776
1
10
0.1 91.6 35.0 91.1
3.3
7436
436 88.3 24.9 23891 254 92.0 69.2
988
488 89.8
0.0
5736
3
20
0.0 92.1 98.1 91.5
0.0
14004
4
89.1
0.4
21145
70
91.9 97.3 3900 900 89.4
0.0
5833
1"
REFERENCES,0.8689024390243902,"2 / 5
5
1.7 92.2 62.7 91.2 43.2
7493
242 88.0
1.4
24001 128 92.2 75.7
994
244 88.8
0.1
6358
6
10
1.6 92.5 92.4 91.9 89.8
7476
476 89.3 81.5 23827 275 91.6 93.2 1975 475 89.2
0.0
5985
26
20
1.6 91.7 95.8 89.9
3.5
14349 349 88.5 87.2 21086 711 91.9 95.1 3905 905 89.8
0.2
5684
40"
REFERENCES,0.8719512195121951,"3 / 5
5
6.3 91.3 88.9 91.7 87.9
7466
215 88.9 80.0 23817 131 92.2 90.8
996
246 89.2 27.5 5974
59
10
6.2 91.8 90.8 91.6 86.4
7348
348 88.7 73.1 21299 136 92.2 89.5
990
490 89.1 82.1 5681 344
20
6.3 92.3 90.8 90.5 71.9 14090
90
89.3 78.5 20694 156 90.9 88.8 3918 918 89.8 86.7 5093 994"
REFERENCES,0.875,"3 / 7
5
1.1 92.6 98.4 91.0 97.1
7452
201 86.2 72.8 23823 189 92.1 97.1
994
244 89.4
0.2
5498
13
10
1.1 92.4 98.6 91.8 96.7
7315
315 88.6 96.2 23648 482 92.3 98.0 1981 481 89.8
0.0
5126
16
20
1.1 92.9 98.6 90.9 97.0 14167 167 89.1 95.4 20600 551 92.4 98.1 3924 924 88.6
0.2
6642
26"
REFERENCES,0.8780487804878049,"7 / 4
5
1.7 92.1 88.5 91.6 87.5
7486
235 89.3 72.2 23928 149 92.4 92.2
992
242 88.8
0.4
6643
27
10
1.7 92.7 93.9 91.9 94.2
7371
371 88.3 59.2 23753 193 92.1 96.2 1973 473 88.9
0.8
6478
32
20
1.7 92.6 96.9 91.1 94.1 14397 397 88.2 47.6 23737 423 91.7 95.8 3904 904 88.6 46.9 6322 209"
REFERENCES,0.8810975609756098,"8 / 6
5
0.2 92.7 98.0 91.3 97.8
7441
190 89.1 88.7 23658 164 92.5 96.9
988
238 89.6
0.0
5991
0
10
0.2 92.1 97.7 91.8 97.2
7089
89
90.2 95.1 19585 280 92.3 98.7
973
473 89.3
0.0
6007
2
20
0.2 92.8 98.8 91.3 96.0 14297 297 87.2 64.2 23347 646 92.3 98.6
975
975 89.4
0.0
5691
3"
REFERENCES,0.8841463414634146,"9 / 2
5
0.1 92.9 93.2 91.2 93.2
7478
225 88.7
1.2
23518 136 92.1 94.0
991
241 90.3
0.0
5444
2
10
0.1 92.6 98.6 91.4 94.7
7497
497 88.5 91.5 24034 242 92.5 97.8
986
486 88.1
0.0
7220
2
20
0.1 92.6 97.4 90.5
0.0
14011
11
90.6 97.7 18658 578 91.9 99.2 3881 811 89.6
0.0
5742
1"
REFERENCES,0.8871951219512195,Under review as a conference paper at ICLR 2022
REFERENCES,0.8902439024390244,"5. (ITLM) Iterative Trimmed Loss Minimization (Shen and Sanghavi, 2019)"
REFERENCES,0.8932926829268293,6. (TW) This Work
REFERENCES,0.8963414634146342,"For each defense, we report"
REFERENCES,0.899390243902439,"1. (C) clean accuracy, which is the accuracy of the defended network on the entire clean test
set (higher is better)."
REFERENCES,0.9024390243902439,"2. (A) targeted misclassiﬁcation rate as deﬁned in Equation 7, which is measured over the
entire source class of the test set (lower is better)."
REFERENCES,0.9054878048780488,"3. (FP) false positives, which counts the number of clean samples excluded from the defended
training set (lower is better)."
REFERENCES,0.9085365853658537,"4. (FN) false negatives, which counts the number of poisoned samples included in the defended
training set (lower is better)."
REFERENCES,0.9115853658536586,"C.1
DISCUSSION"
REFERENCES,0.9146341463414634,"Our approach consistently outperforms all other defenses by targeted misclassiﬁcation rate (column
A) across both architectures. If we deﬁne a “successful” run as achieving less than 1% targeted
misclassiﬁcation rate, then for the PreActResNet18 architecture, our defense succeeds in 17/24
scenarios, SS succeeds 9/24 scenarios, and both AC and ITLM do not succeed a single time; for the
ResNet32 architecture, our defense succeeds in 20/24 scenarios, SS succeeds in 2/24 scenarios, AC
succeeds in 1/24 scenarios, and ITLM again fails all 24 scenarios."
REFERENCES,0.9176829268292683,"In general, our defense results in a 2-3% drop in clean accuracy for both architectures, when compared
to a model trained and tested using clean data. AC achieves a clean accuracy which is on par with (or
slightly below) ours. Surprisingly, this clean accuracy is despite AC having false positives (FP) of
approximate 6x and 4x ours for the PreActResNet18 and ResNet32 models, respectively. Similarly,
compared to our defense, SS has a slightly higher FP rate (which is roughly constant, as the defense
always removes a ﬁxed amount of data), but suffers a negligible drop in clean accuracy. We attribute
this behavior to the existence of small, difﬁcult to learn subpopulations (that may be removed by
the weak learners as incompatible after training for only 40 epochs) but are responsible for the last
2-3% of performance. However, we note that our defense is designed to remove incompatible data,
rather than poisoned data speciﬁcally, and therefore some such behavior is expected. Conversely,
we hypothesize that SS and AC are removing “easy” data according to statistical properties of the
activation patterns of a trained network, which may constitute redundant data in terms of the training
distribution. ITLM achieves good clean accuracy and the lowest number of false positives (though its
performance is negligible in terms of defending against poison)."
REFERENCES,0.9207317073170732,"The only scenario which consistently evades our defense is the (3 / Cat, 5 / Dog) scenario. This
scenario is also the only one for which the poison misclassiﬁcation rate of a clean network is
noticeable large at around 6-8% (primary column L, secondary column M), which is consistent with
the results in Tran et al. (2018). These results suggest that the scenario violates Property 4.4, i.e., the
poison and clean distributions are not incompatible—training on a clean dataset yields non-negligible
performance on poisoned cats when mislabeled as dogs. Because the poisoned data is compatible
with the clean data, our theoretical analysis suggests that our defense will struggle to separate the
clean and poisoned data, as is reﬂected in our results. Despite this, we note that the performance of
our defense still exceeds that of the SS, AC, and ITLM defenses in several cases for this scenario."
REFERENCES,0.9237804878048781,"Finally, to reconcile our results with the results presented in the Spectral Signatures paper, we
note that the training code in ofﬁcial implementation of the SS defenses uses some non-standard
methodologies, including a random crop with only 2x2 padding (instead of the 4x4 commonly used
for CIFAR-10); no normalization of the input data according to the mean and standard deviation; and
custom initialization of all the layers (such as using a normal distribution to initialize the convolutional
layers, rather than the default Kaiming initialization (He et al., 2015) in PyTorch). The authors also
only report results for cases where the network was “successfully poisoned”, which they deﬁned as
“approximately 90% or higher accuracy on the poisoned set” (corresponding to primary column ND,
secondary column A, in Tables 3 and 4). To verify our results, we ran the ﬁrst scenario of the ﬁrst
(source, target) pair (i.e., the ﬁrst row of Table 2) through the authors’ own implementation and found
that at ϵ = 5%"
REFERENCES,0.926829268292683,Under review as a conference paper at ICLR 2022
REFERENCES,0.9298780487804879,– an undefended network had a 71.9% poison misclassiﬁcation rate;
REFERENCES,0.9329268292682927,– the defense left 205 false negatives (out of 250 poisoned images);
REFERENCES,0.9359756097560976,"– trained on the defended dataset, the network had a 52.9% poison misclassiﬁcation rate,"
REFERENCES,0.9390243902439024,and at ϵ = 10%
REFERENCES,0.9420731707317073,– an undefended network had a 74.1% poison misclassiﬁcation rate;
REFERENCES,0.9451219512195121,– the defense left 193 false negatives (out of 500 poisoned images);
REFERENCES,0.948170731707317,"– trained on the defended dataset, the network had a 23.3% poison misclassiﬁcation rate."
REFERENCES,0.9512195121951219,"These results are not within the scope of the results considered in the original paper (due to not
being over 90% poisoned pre-defense). In contrast, in our experiments, the pre-defense poison
misclassiﬁcation rate is much higher, which we attribute to more modern training methodologies."
REFERENCES,0.9542682926829268,Under review as a conference paper at ICLR 2022
REFERENCES,0.9573170731707317,"Table 5: Ablation studies on CIFAR-10, dirty label backdoor scenario, using the PreActResNet18
architecture, with various settings of α and β. The S / T column lists the CIFAR-10 source and target
classes. ϵ refers to the percentage of the source class which is poisoned. The second level headings
are C (clean accuracy, higher is better), A (targeted misclassiﬁcation rate, lower is better). Please
refer to the text for a more detailed explanation of the table."
REFERENCES,0.9603658536585366,"S / T
ϵ
β = 1/16
β = 1/8
β = 1/4
β = 1
C
A
C
A
C
A
C
A"
REFERENCES,0.9634146341463414,α = 1/4
REFERENCES,0.9664634146341463,"3 / 5
5
92.7
79.8
92.8
81.1
92.9
71.8
93.0
55.7
10
92.6
87.3
92.6
90.2
93.1
89.9
93.5
91.9
20
92.3
76.1
92.6
87.4
92.8
88.4
93.3
92.3"
REFERENCES,0.9695121951219512,"7 / 4
5
92.9
2.4
92.8
0.5
93.3
0.2
93.3
48.2
10
92.6
9.3
92.9
0.3
93.2
27.6
93.2
76.7
20
92.7
83.5
92.8
84.5
92.8
84.7
93.2
43.0"
REFERENCES,0.9725609756097561,"8 / 6
5
92.8
0.0
93.1
0.0
93.3
0.0
93.4
0.0
10
92.7
0.0
93.0
0.0
93.3
0.0
93.5
0.1
20
92.8
0.1
92.6
0.2
93.1
0.0
93.1
95.5"
REFERENCES,0.975609756097561,"9 / 2
5
92.9
0.0
93.0
0.0
93.3
0.0
93.2
0.0
10
92.8
0.0
92.9
0.0
92.9
0.0
92.8
0.0
20
92.7
0.0
93.1
0.0
93.0
0.0
93.3
0.0 α = 1"
REFERENCES,0.9786585365853658,"3 / 5
5
92.8
80.6
92.8
74.7
93.0
73.6
92.7
87.1
10
92.4
86.8
92.3
86.1
92.8
91.4
93.0
90.1
20
91.5
79.8
92.6
85.3
93.0
87.6
93.1
90.4"
REFERENCES,0.9817073170731707,"7 / 4
5
92.6
0.9
93.4
0.9
92.9
5.7
93.5
0.2
10
92.9
2.3
92.8
72.1
93.1
0.5
93.5
72.1
20
92.7
91.7
92.6
83.4
92.9
88.4
93.0
86.6"
REFERENCES,0.9847560975609756,"8 / 6
5
92.8
0.0
92.2
0.0
92.8
0.0
93.3
0.0
10
92.8
0.1
93.1
0.0
93.2
0.0
93.5
0.0
20
92.3
0.0
92.7
0.0
93.5
0.0
93.2
0.0"
REFERENCES,0.9878048780487805,"9 / 2
5
92.6
0.0
92.9
0.0
93.1
0.0
93.1
0.0
10
92.8
0.0
93.0
0.0
93.1
0.0
93.0
0.0
20
92.8
0.5
92.8
0.0
92.6
0.0
93.2
0.0"
REFERENCES,0.9908536585365854,"C.2
ABLATION STUDIES"
REFERENCES,0.9939024390243902,"We conduct some additional ablation studies to better understand the effects of the two main hyper-
parameters in Algorithm 2: the expansion factor α and the subset size β. Computationally, larger
β means fewer components and fewer outer iterations of ISPL (and is thus more efﬁcient); in our
main experiments, we use β = 1/8 and run ISPL 8 times in sequence to generate 8 components.
Additionally, as discussed in Remark 2, smaller α is a more stringer requirement, since mixing
distributions increases the expansion factor. Therefore we would expect that increasing α leads to
worse identiﬁcation of homogeneous components on average."
REFERENCES,0.9969512195121951,"Tables 5 presents the full results of the ablation studies. Our main ﬁnding is that our method is quite
robust to both the expansion factor α and subset size β. There is also a slight trend that smaller α and
β are better at identifying poison, with a small drop in clean accuracy."
