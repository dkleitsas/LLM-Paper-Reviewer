Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0017985611510791368,"Most current few-shot learning methods train a model from abundantly labeled
base category data and then transfer and adapt the model to sparsely labeled novel
category data. These methods mostly generalize well on novel categories from
the same domain as the base categories but perform poorly for distant domain
categories. In this paper, we propose a framework for few-shot learning coined as
ConFeSS (Contrastive Learning and Feature Selection System) that tackles large
domain shift between base and novel categories. The ﬁrst step of our framework
trains a feature extracting backbone with the contrastive loss on the base category
data. Since the contrastive loss does not use supervision, the features can generalize
better to distant target domains. For the second step, we train a masking module to
select relevant features that are more suited to target domain classiﬁcation. Finally,
a classiﬁer is ﬁne-tuned along with the backbone such that the backbone produces
features similar to the relevant ones. To evaluate our framework, we tested it on
a recently introduced cross-domain few-shot learning benchmark. Experimental
results demonstrate that our framework outperforms all meta-learning approaches
and produces competitive results against recent cross-domain methods. Additional
analyses are also performed to better understand our framework."
INTRODUCTION,0.0035971223021582736,"1
INTRODUCTION"
INTRODUCTION,0.00539568345323741,"Recently, there has been an expansion in the quality and quantity of datasets (Zhang et al., 2018; Sun
et al., 2017), computing resources (Jeon et al., 2019), and deep neural architectures (Dhillon & Verma,
2020). When trained with vast amounts of data, these deep neural network models deliver improved
performance on applications like image recognition, action localization, speaker veriﬁcation, text
analysis, and gene sequence prediction (Nguyen et al., 2018; Yun et al., 2019; Yao et al., 2019;
Zhou et al., 2018). However, data collection and annotation at a large scale incurs substantial labor
and cost, which are particularly difﬁcult for specialized domains such as medical imaging and satellite
imagery, where domain expertise is needed. Moreover, most neural networks fail to generalize to
unseen categories when trained with a few labeled samples. To address these limitations, research on
few-shot learning has gained signiﬁcant attention."
INTRODUCTION,0.007194244604316547,"Few-shot learning methods (Wang et al., 2020) aim to uncover the data structure and model the
concept of new categories with only a few labeled samples. A popular strategy to tackle few-shot
learning is meta-learning which consists of two stages: meta-train and meta-test. In the meta-train
stage, a backbone network is trained to classify the base category correctly by leveraging the labeled
source data while mimicking a few-shot regime where only a limited number of samples are available
per class in each learning episode. In the meta-test stage, with the trained backbone, the novel
categories with only a few target class samples can be added (or enrolled) and tested. Here, the
backbone network can be adapted to the target samples."
INTRODUCTION,0.008992805755395683,"Nonetheless, most few-shot learning approaches exhibit insufﬁcient generalization capacity when
there is a big gap between the source and target data. To investigate this problem, there have been
considerable efforts (Triantaﬁllou et al., 2020; Chen et al., 2019; Tseng et al., 2020) in establishing
cross-domain few-shot learning (CDFSL) benchmarks. Still, these datasets limit their focus to natural"
INTRODUCTION,0.01079136690647482,"∗Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc."
INTRODUCTION,0.012589928057553957,Published as a conference paper at ICLR 2022
INTRODUCTION,0.014388489208633094,"images and fail to capture more pragmatic domain shifts where target data may come from more
diverse domains such as satellite and medical imagery. Very recently, Guo et al. (2020) introduced
a challenging benchmark to evaluate generalization capability on distant target domains. Its target
domain datasets consist of images from natural, medical and satellite domains with wide variations
of context, color, and perspective. Thus, it represents practical applications where the generic
model needs to be adapted to a particular use case. On this benchmark, the popular meta-learning
approaches (Vinyals et al., 2016; Finn et al., 2017; Snell et al., 2017; Sung et al., 2018; Lee et al.,
2019) have been found to produce poor recognition performance."
INTRODUCTION,0.01618705035971223,"This paper proposes a novel contrastive learning and feature selection system (ConFeSS) for single-
source cross-domain few-shot learning. Our framework consists of three steps: pre-training a
backbone network on a single-source dataset, learning a feature masking module on the target
dataset, and ﬁne-tuning the backbone network. In the ﬁrst step, a backbone network is trained in an
unsupervised fashion, where a self-supervised learning approach is considered with the contrastive
loss (Chen et al., 2020). This is in contrast to meta-learning approaches, which use supervision during
the pre-training stage. Although the label of the source dataset is given at this step, we consider
the unsupervised learning to alleviate the supervision collapse problem (Doersch et al., 2020) and
also to generalize better to the distant target domains. In the second step, a feature masking module
is learned with target domain data to generate masks for separating task-relevant features from
irrelevant features. This step is required because there is a large discrepancy between the source and
target datasets, and hence all the features useful for the source task might not be helpful or even be
detrimental to the target task. Furthermore, we expect the generalization performance to be improved
with fewer features in the few-shot regime due to the Vapnik-Chervonenkis (VC) dimension reduction.
In the ﬁnal step, both the pre-trained backbone network and the classiﬁer are ﬁne-tuned to adapt to
the target categories by a proper regularization with the relevant features."
INTRODUCTION,0.017985611510791366,"Our main contributions can be summarized as follows: (i) Learning a feature masking module with
appropriate constraints to select relevant features for few-shot target samples; (ii) Fine-tuning the
backbone by regularizing it with the selected relevant features; (iii) Our extensive experimental
evaluation and analyses show that our method produces competitive recognition performance on the
new CDFSL benchmark (Guo et al., 2020)."
RELATED WORK,0.019784172661870502,"2
RELATED WORK"
RELATED WORK,0.02158273381294964,"Meta-learning for Few-shot Learning These methods use episodic pre-training to simulate test
conditions followed by fast adaptation to novel category samples. One of the earliest meta-learning
methods was MatchNet (Vinyals et al., 2016) which learns a mapping function to project labeled and
unlabeled samples to their corresponding labels. ProtoNet (Snell et al., 2017) extended this work
by learning a representation and assigning a class depending on the distance of query samples to
class prototypes while RelationNet (Sung et al., 2018) learns an additional deep metric function.
MetaOpt (Lee et al., 2019) takes a different approach where an SVM-like classiﬁer is learned on top
of the features for better generalization. Finally, MAML (Finn et al., 2017) is an optimization-based
method that learns to adapt to few-shot novel categories in a few iterations. All these meta-learning
methods have performed poorly on the CDFSL benchmark (Guo et al., 2020). There are many
other meta-learning works but they have not been evaluated on the CDFSL benchmark. One can
refer (Hospedales et al., 2021) for a comprehensive survey on this topic."
RELATED WORK,0.023381294964028777,"Domain Adaptation In this problem, we have source and target domains with the same categories,
and the goal is to reduce domain discrepancy between them. Hence, domain adaptation methods
cannot be directly used for CDFSL, where the labels between source and target are disjoint. The
universal domain adaptation (UDA) setting (You et al., 2019) might be more similar to the CDFSL
setting because it has different source and target categories. However, in UDA, there is some overlap
between the source and the unknown target categories with lots of unlabeled target data available
while CDFSL considers completely novel target categories each containing only few labeled data."
RELATED WORK,0.025179856115107913,"Cross-domain Few-shot Learning There have been very few works on cross-domain few-shot
learning. A recent work (Tseng et al., 2020) uses a noisy transformation layer on top of features to
simulate cross-domain distributions and produce better generalization. In (Chen et al., 2019), the
authors compare different meta-learning frameworks and propose a competitive ﬁne-tuning-based
baseline against these methods for the cross-domain setting. However, the datasets used for evaluating"
RELATED WORK,0.02697841726618705,Published as a conference paper at ICLR 2022
RELATED WORK,0.02877697841726619,"these methods contain only natural images. As a result, there is no signiﬁcant domain shift between
the source and target datasets even though the source and target labels are disjoint. Guo et al. (2020)
introduce a novel CDFSL benchmark and show that most meta-learning methods along with the
feature-wise transformation (Tseng et al., 2020) approach perform poorly compared to simple ﬁne-
tuning methods. In our paper, the ﬁne-tuning step is augmented with a feature selection mechanism
to select relevant features. More recent methods that evaluate on the CDFSL benchmark include
CHEF (Adler et al., 2020), ATA (Wang & Deng, 2021) and STARTUP (Phoo & Hariharan, 2021).
CHEF addresses large domain shift by fusion of Hebbian learners applied on different layers. This is
done to increase the importance of low and mid-level features for distant domain recognition. ATA is
a plug-and-play method that improves robustness of models through adversarial task augmentation.
STARTUP assumes access to large unlabelled data from the target domain and proposed combining
knowledge distillation and contrastive learning to learn the target model. In our framework, we only
assume access to few labeled data from the target domain."
RELATED WORK,0.030575539568345324,"Self-supervision for Few-shot Learning Self-supervised learning has been used in the form of
different pretext tasks (He et al., 2020; Noroozi & Favaro, 2016; Gidaris et al., 2018) to pre-train
representations that can be used for down-stream tasks as well. These representations have been able
to generalize well in the few-shot regime. Recent works (Gidaris et al., 2019; Su et al., 2020; Chen
et al., 2021) show that adding self-supervised loss functions for representation learning improves few-
shot recognition performance. In our paper, we solely use self-supervision in the form of contrastive
loss (Chen et al., 2020) during pre-training because it mitigates supervision collapse as observed
in (Doersch et al., 2020). Furthermore, contrastive losses have been theoretically proven (Saunshi
et al., 2019) to produce better representations for few-shot learning but have not been evaluated for
their generalization ability on few-shot novel categories with distant domains."
RELATED WORK,0.03237410071942446,"Feature Selection for Few-shot Learning Feature selection is useful for deriving relevant features
for a particular task or for preventing overﬁtting on few-shot samples. Zhao et al. (2018) separate
the features into orthogonal components where the sparse signal component facilitates the feature
selection. It is similar to our approach where we use a mask to select relevant and irrelevant features,
yet we impose different constraints on these decomposed features. Liu et al. (2017) use a greedy
feature selection mechanism followed by multiple dropouts to reduce gradient variance of few-shot
samples. However, this method is not applicable for transferring to novel tasks with large domain
differences. A more recent work (Dvornik et al., 2020) select features from a universal representation
learned from multiple source domains by optimizing the selection coefﬁcients for different domains.
This is quite different from our method, which can work even with a single source domain by
selecting relevant features instead of relevant source domains. Berriel et al. (2019) use budget-aware
mechanism of optimizing a switch vector to select domain-relevant feature channels from a pre-
trained architecture. Additionally, masking has been used to adapt single network weights to multiple
new tasks (Mancini et al., 2018; Mallya et al., 2018)."
PROPOSED FRAMEWORK,0.0341726618705036,"3
PROPOSED FRAMEWORK"
PROPOSED FRAMEWORK,0.03597122302158273,"𝑳𝒄𝒐𝒏(𝒇, 𝒇𝟏, 𝒇𝟐)"
PROPOSED FRAMEWORK,0.03776978417266187,"Source 
Feature 
Extractor 𝒙𝟏 𝒇𝟐 𝒇 𝒇𝟏 𝒇 𝒎"
PROPOSED FRAMEWORK,0.039568345323741004,"Mask 
Generator"
PROPOSED FRAMEWORK,0.04136690647482014,"Source 
Feature 
Extractor ⊙ 𝒙"
PROPOSED FRAMEWORK,0.04316546762589928,𝒇+ = 𝒎⊙𝒇
PROPOSED FRAMEWORK,0.044964028776978415,𝒇−= (𝟏−𝒎)⊙𝒇
PROPOSED FRAMEWORK,0.046762589928057555,𝑳𝒑𝒐𝒔(𝒇+)
PROPOSED FRAMEWORK,0.048561151079136694,"𝑳𝒅𝒊𝒗(𝒇+, 𝒇−)"
PROPOSED FRAMEWORK,0.050359712230215826,"𝑳𝒏𝒆𝒈(𝒇−) 𝒙
𝒇 𝒎"
PROPOSED FRAMEWORK,0.052158273381294966,"𝑳𝒓𝒆𝒈(𝒇𝒕, 𝒇+)"
PROPOSED FRAMEWORK,0.0539568345323741,"Mask 
Generator
Source 
Feature 
Extractor"
PROPOSED FRAMEWORK,0.05575539568345324,"Target 
Feature 
Extractor
𝒇𝒕"
PROPOSED FRAMEWORK,0.05755395683453238,𝑳𝒕𝒂𝒔𝒌(𝒇𝒕) ⊙
PROPOSED FRAMEWORK,0.05935251798561151,𝒇+ = 𝒎⊙𝒇 𝒙
PROPOSED FRAMEWORK,0.06115107913669065,"(a)
(b)
(c)"
PROPOSED FRAMEWORK,0.06294964028776978,"Figure 1: Our framework consists of three steps: (a) Pre-training the backbone using a self-supervised contrastive
loss; (b) Learning the masking module on the target data to select relevant features, and (c) Fine-tuning the
backbone using a regularized loss with positively relevant features."
PROPOSED FRAMEWORK,0.06474820143884892,Published as a conference paper at ICLR 2022
PROBLEM DESCRIPTON AND NOTATION,0.06654676258992806,"3.1
PROBLEM DESCRIPTON AND NOTATION"
PROBLEM DESCRIPTON AND NOTATION,0.0683453237410072,"For the CDFSL problem, we have a source domain and a target domain. Each domain has an associated
joint distribution P over the input space X and the label space Y. The marginal distribution of the
input space is denoted as PX . Instances (x, y) can be sampled from P, where x is the input and y is
the corresponding label. Accordingly, the source domain can be represented as (Xs, Ys) and the target
domain as (Xt, Yt) with joint distributions Ps and Pt, respectively. Due to the domain difference, the
source marginal distribution PXs will be signiﬁcantly different from the target marginal distribution
PXt. Moreover, the target domain classes are novel; hence there is no overlap between Ys and Yt.
The goal is to ﬁrst learn a model from abundant data sampled from the source distribution Ps. Then
the model is adapted to few data sampled from the target distribution Pt. Finally, the adapted model
is evaluated on held-out test data sampled from the target distribution. In our framework, we learn the
model from the source distribution, using a self-supervised contrastive loss function. The adaptation
step on the target data involves learning a mask generator followed by regularized ﬁne-tuning. Our
framework is depicted in Fig. 1, and the details are described in the following subsections."
UNSUPERVISED TRAINING OF BACKBONE,0.07014388489208633,"3.2
UNSUPERVISED TRAINING OF BACKBONE"
UNSUPERVISED TRAINING OF BACKBONE,0.07194244604316546,"The backbone (the feature extraction module) is trained in an unsupervised manner inspired from
recent works on contrastive learning (Chen et al., 2020) and unsupervised pre-training (Doersch
et al., 2020). Contrastive learning has been found to be effective for transfer learning (?). This
is because contrastively learned features focus more on mid and low-level features, which are
easily adapted. Furthermore, such features produce better reconstruction by learning a holistic
representation of images rather than focusing only on discriminative regions. Thus, contrastive
learning is an effective pre-training strategy for transferring representations to distant target domains.
In our pre-training stage, we augment samples from the existing samples in the training batch using
various transformations and use these augmented samples and original samples to determine a
contrastive loss. Speciﬁcally, let there be Nb training samples in a batch, where the samples are
represented as {xi}Nb
i=1. For each sample xi, we obtain Nt random transformations where the tth
transformed instance is represented as xit and t ∈{1, 2..., Nt}. Following the idea of (Doersch et al.,
2020), we enforce the the transformed instances xit to be close to xi and far from xk, k ̸= i using
the following cross-entropy loss,"
UNSUPERVISED TRAINING OF BACKBONE,0.0737410071942446,"Lcon=−
1
NbNt Nb
X i=1 Nt
X"
UNSUPERVISED TRAINING OF BACKBONE,0.07553956834532374,"t=1
log
exp(−d(φs(xit), φs(xi)))
PNb
k=1exp(−d(φs(xit), φs(xk)))
.
(1)"
UNSUPERVISED TRAINING OF BACKBONE,0.07733812949640288,"Here, φs(·) represents the feature extraction module, and d(·) is a distance metric. Snell et al. (2017)
showed that Euclidean distances model Bregman divergence of mixture densities, which consistently
performs better for the few-shot setting, and so we choose the same metric. The appendix discusses
the theoretical support of constrastive learning for few-shot learning."
LEARNING THE FEATURE MASKING MODULE,0.07913669064748201,"3.3
LEARNING THE FEATURE MASKING MODULE"
LEARNING THE FEATURE MASKING MODULE,0.08093525179856115,"The feature masking module is used to generate masks that can select task-relevant and task-irrelevant
features. For simplicity, we call task-relevant and irrelevant features positive and negative features,
respectively. It is important to note that we cannot afford a large masking sub-network because it
might overﬁt to few-shot target domain samples. So, we just mask on features fed to the classiﬁer
with appropriate regularization during ﬁne-tuning. Let the feature extraction module learned from the
source domain be denoted as φs(·). Given a batch of target domain samples {(xi, yi)}N
i=1, we can
obtain the feature fi = φs(xi) ∈Rd for each sample. We feed the feature into the mask generating
module M(·) to obtain the mask mi = M(fi). This mask is then used to produce positive (f +
i ) and
negative (f −
i ) features, such that"
LEARNING THE FEATURE MASKING MODULE,0.08273381294964029,"f +
i = mi ⊙fi,
f −
i = (1 −mi) ⊙fi
(2)"
LEARNING THE FEATURE MASKING MODULE,0.08453237410071943,"where ⊙is the Hadamard product, and 1 is a vector of ones of the appropriate dimension. mi ∈Rd
is a mask vector consisting of d elements where the jth element is represented as mij. To generate
binary masks mij, we follow the probabilistic procedure introduced in (Maddison et al., 2017; Jang
et al., 2017). Let zij be the unbounded output logit from the mask module corresponding to the"
LEARNING THE FEATURE MASKING MODULE,0.08633093525179857,Published as a conference paper at ICLR 2022
LEARNING THE FEATURE MASKING MODULE,0.08812949640287769,"ith sample and the jth dimension. We generate logistic noise l such that l = log(u) −log(1 −u)
and u ∼uniform(0, 1). The noise is then added to the logits to produce mask mij, such that
mij = σ( zij+l"
LEARNING THE FEATURE MASKING MODULE,0.08992805755395683,"τ
) where σ(·) is the sigmoid operation, and τ is the temperature scale. The noise is
added to the logits to explore different binary masks suitable for the target task. To back-propagate
discrete masks during training, we follow the straight-through estimator (Bengio et al., 2013) where
we use sigmoid during the backward pass and hard-threshold operation during the forward pass. The
hard-threshold operation involves setting mij to 1 if mij > 0.5 or 0 otherwise. During inference
mode, the hard-threshold operation of the mask is carried out but with the logistic noise l = 0"
LEARNING THE FEATURE MASKING MODULE,0.09172661870503597,"To train the feature masking module M(·), we want to make sure that the positive features f +
i are
discriminative while the negative features f −
i are not. To produce discriminative positive features f +
i ,
we use the cross-entropy criterion such that"
LEARNING THE FEATURE MASKING MODULE,0.09352517985611511,"Lpos(f +
i ) = LXEnt(C+(f +
i ), yi),
(3)"
LEARNING THE FEATURE MASKING MODULE,0.09532374100719425,"where LXEnt(·) is the cross-entropy criterion, and C+(·) is a linear classiﬁer used for the positive
features f +
i . To produce negative features f −
i , we use the maximum entropy criterion such that"
LEARNING THE FEATURE MASKING MODULE,0.09712230215827339,"Lneg(f −
i ) = −LEnt(C−(f −
i )),
(4)"
LEARNING THE FEATURE MASKING MODULE,0.09892086330935251,"where LEnt(·) is the entropy of the softmax outputs of C−(f −
i ), and C−(·) is a linear classiﬁer
used for the negative features f −
i . The maximum entropy criterion makes sure that output class
probabilities are uncertain causing the negative features to be less class discriminative."
LEARNING THE FEATURE MASKING MODULE,0.10071942446043165,"The design of the mask only makes positive and negative features apart. However, they can still be
statistically similar in arrangement of clusters and higher-order statistics. Hence, a divergence measure
to maximize the statistical distance between the positive and the negative features is required. If we
let sd(·) be the statistical distance between two sets of features: the positive set F+ = {(f +
i )N
i=1}
and the negative set F−= {(f −
i )N
i=1}, then we would minimize the divergence loss,"
LEARNING THE FEATURE MASKING MODULE,0.10251798561151079,"Ldiv(F+, F−) = e−sd(F+.F−).
(5)"
LEARNING THE FEATURE MASKING MODULE,0.10431654676258993,"The exponent term is used to provide more stable and smaller gradients when close to optimality. The
loss terms in Eq. 3, 4 and 5 are weighted and combined to obtain"
LEARNING THE FEATURE MASKING MODULE,0.10611510791366907,"Lmask = λposLpos + λnegLneg + λdivLdiv.
(6)"
LEARNING THE FEATURE MASKING MODULE,0.1079136690647482,"Here, Lpos and Lneg are averaged over the batch samples, while Ldiv is an aggregated loss function
over all the batch samples. These loss terms are combined to obtain the ﬁnal loss Lmask, which is
back-propagated across M(·), C+(·) and C−(·) to update the respective parameters."
FINE-TUNING,0.10971223021582734,"3.4
FINE-TUNING"
FINE-TUNING,0.11151079136690648,"The ﬁne-tuning stage is the ﬁnal step of adaptation to the target domain. In this step, we train both the
feature extractor and the classiﬁer on the target domain data. Since the target domain contains only
a few labeled data, we regularize the feature extractor to produce positive features using the mask
generator that has been trained in the previous step. Let φt(·) be the target domain feature extractor
that is initialized from the parameters of the source domain feature extractor φs(·). Given a batch of
target domain samples {(xi, yi)}N
i=1, for each sample we can obtain the feature f t
i = φt(xi) ∈Rd.
This feature f t
i is fed into a linear classiﬁer C(·) such that we obtain the cross-entropy loss,"
FINE-TUNING,0.11330935251798561,"Ltask(f t
i ) = LXEnt(C(f t
i ), yi).
(7)"
FINE-TUNING,0.11510791366906475,"To regularize the network, we want to make sure that the target domain feature f t
i is close to the target
relevant (positive) feature f +
i = M(φs(xi)) ⊙φs(xi). This is realized by minimizing the loss,"
FINE-TUNING,0.11690647482014388,"Lreg = ||f t
i −f +
i ||2
2,
(8)"
FINE-TUNING,0.11870503597122302,"where ||·||2 is the 2-norm. The regularization ensures that the network does not catastrophically forget
the positively relevant features and does not allow the negatively relevant features to be transferred.
Additionally, distance-based regularization has been shown to promote tighter generalization (Gouk
et al., 2021) as discussed in the appendix. The loss terms in Eq. 7 and 8 are combined as"
FINE-TUNING,0.12050359712230216,"Lft = Ltask + λregLreg.
(9)"
FINE-TUNING,0.1223021582733813,Published as a conference paper at ICLR 2022
FINE-TUNING,0.12410071942446044,"Lft is then averaged over the training samples in a batch to compute the ﬁnal loss, which is back-
propagated across φt(·) and C(·) to update the respective parameters. We can choose not to ﬁne-tune
φt(·) but ablation studies in Table 2 show that ﬁne-tuning backbone is effective for CDFSL. This
completes the ﬁne-tuning stage. All the stages of our proposed framework, including the pre-training
and the ﬁne-tuning steps, are summarized in Algorithm 1. For a test sample xte, we use C(φt(xte))
followed by the softmax operation to obtain the class probabilities and the most probable class."
FINE-TUNING,0.12589928057553956,"Algorithm 1: ConFeSS framework
Given: Source dataset Ds and Target dataset Dt
Hyper-parameters: λpos, λneg, λdiv, λreg
Step 1: Pre-train backbone φs(·) on Ds
For each sampled batch of source data
For each sampled augmentation
Take gradient descent step of Eq. 1 with respect to φs(·)
Step 2: Obtain mask generator M(·) from Dt
For each sampled batch of target data
Take gradient descent step of Eq. 6 with respect to M(·), C+(·) and C−(·)
Step 3: Fine-tune backbone φt(·) on Dt
Initialize φt(·) from optimized φs(·)
For each sampled batch of target data
Take gradient descent step of Eq. 9 with respect to φt(·) and C(·)
Step 4: Predict test sample class using optimized φt(·) and C(·)"
EXPERIMENTAL RESULTS,0.12769784172661872,"4
EXPERIMENTAL RESULTS"
DATASET DESCRIPTION,0.12949640287769784,"4.1
DATASET DESCRIPTION"
DATASET DESCRIPTION,0.13129496402877697,"To evaluate our proposed framework, we test it on the CDFSL benchmark introduced by Guo et al.
(2020). This benchmark uses mini-ImageNet (Vinyals et al., 2016), which is a subset of the Im-
ageNet (Deng et al., 2009) dataset as the source domain that contains abundantly labeled natural
categories. The model learned on the mini-Imagenet dataset is then tested on target datasets containing
only a few labeled training data. These target datasets have large domain differences from the source
domain, and in order of increasing dissimilarity, they consist of the following: a) CropDiseases (Mo-
hanty et al., 2016), containing images of different plant disease types, b) EuroSAT (Helber et al.,
2019), consisting of different classes of satellite imagery, c) ISIC2018 (Tschandl et al., 2018; Codella
et al., 2018), which contains different dermoscopic images of skin lesions, and d) ChestX (Wang et
al., 2017), a collection of chest X-Ray images of different lung disease types."
DATASET DESCRIPTION,0.13309352517985612,"Table 1: Results of our approach (ConFeSS) as compared with previous methods on the ChestX, ISIC, EuroSAT
and CropDisease datasets. The best results are shown in boldface. NWKS means N-way K-shot test setting."
DATASET DESCRIPTION,0.13489208633093525,"ChestX
ISIC
EuroSAT
CropDisease
Method
5W5S
5W20S
5W50S
5W5S
5W20S
5W50S
5W5S
5W20S
5W50S
5W5S
5W20S
5W50S
MatchNet
22.40
23.61
22.12
36.74
45.72
54.58
64.45
77.10
54.44
66.39
76.38
58.53
MatchNet+FWT
21.26
23.23
23.01
30.40
32.01
33.17
56.04
63.38
62.75
62.74
74.90
75.68
MAML
23.48
27.53
–
40.13
52.36
–
71.70
81.95
–
78.05
89.75
–
ProtoNet
24.05
28.21
29.32
39.57
49.50
51.99
73.29
82.27
80.48
79.72
88.15
90.81
ProtoNet+FWT
23.77
26.87
30.12
38.87
43.78
49.84
67.34
75.74
78.64
72.72
85.82
87.17
RelationNet
22.96
26.63
28.45
39.41
41.77
49.32
61.31
74.43
74.91
68.99
80.45
85.08
RelationNet+FWT
22.74
26.75
27.56
35.54
43.31
46.38
61.16
69.40
73.84
64.91
78.43
81.14
MetaOpt
22.53
25.53
29.35
36.28
49.42
54.80
64.44
79.19
83.62
68.41
82.89
91.76
STARTUP
26.94
33.19
36.91
47.22
58.63
64.16
82.29
89.26
91.99
93.02
97.51
98.45
CHEF
24.72
29.71
31.25
41.26
54.30
60.86
74.15
83.31
86.55
86.87
94.78
96.77
FT-All
25.97
31.32
35.49
48.11
59.31
66.48
79.08
87.64
90.89
89.25
95.51
97.68
ATA
24.43
–
–
45.83
–
–
83.75
–
–
90.59
–
–
ConFeSS
27.09
33.57
39.02
48.85
60.10
65.34
84.65
90.40
92.66
88.88
95.34
97.56"
IMPLEMENTATION DETAILS,0.1366906474820144,"4.2
IMPLEMENTATION DETAILS"
IMPLEMENTATION DETAILS,0.13848920863309352,"For a fair comparison, we use the ResNet-10 backbone introduced by Guo et al. (2020), which
produces a 512 dimension feature space. We use Adam as the optimizer with a learning rate of 0.001.
The statistical distance sd(·) used in Eq. 5 is maximum mean discrepancy (MMD) (Gretton et al.,
2012). MMD between two distributions P and Q over feature space X is deﬁned as ||EX∼P [φ(X)]−"
IMPLEMENTATION DETAILS,0.14028776978417265,Published as a conference paper at ICLR 2022
IMPLEMENTATION DETAILS,0.1420863309352518,"EY ∼Q[φ(Y )]||H where φ : X →H, and H is a reproducing kernel Hilbert space. The MMD can
be easily computed using the kernel trick while we use the Gaussian kernel in our experiments.
Unless explicitly mentioned, we use the pre-training batch size Nb = 50 and the augmentation
size Nt = 3, where the augmentations were chosen as in (Chen et al., 2020) . For larger values
of Nt, we found a dip in performance probably because the extra augmentations do not represent
transformations in the target domain. The results of different values of Nt are reported and analyzed
later in the paper. The masking module M(·) consists of a small two-layer feed-forward network
with a hidden layer dimension of 256. We use a small subnetwork for masking to prevent overﬁtting
issues. We set temperature τ = 1. Also, we set λpos = 10−3, λneg = 10−2, λdiv = 10−2, and
λreg = 10−2. The numbers of training epochs for Step 1, Step 2, and Step 3 in Algorithm 1 are
set as 400, 15, and 50, respectively. The hyper-parameters are kept ﬁxed because it is not possible
to create a small-to-medium validation set from the few-shot target dataset. The epoch number of
400 for pre-training is kept the same as ﬁne-tuning methods described in (Guo et al., 2020). For
all experiments, the average accuracy over 600 episodes of N-way K-shot setting is reported. Each
episode contains randomly sampled K-shot samples per class for adaptation and 15 query samples
per class for evaluation, where N is the number of sampled classes."
COMPARISONS,0.14388489208633093,"4.3
COMPARISONS"
COMPARISONS,0.14568345323741008,"We compare our proposed approach against several meta-learning based few-shot learning methods in-
troduced in the CDFSL benchmark (Guo et al., 2020): MatchNet (Vinyals et al., 2016), MAML (Finn
et al., 2017), ProtoNet (Snell et al., 2017), RelationNet (Sung et al., 2018), and MetaOpt (Lee et al.,
2019). Furthermore, Feature-wise Transformation (FWT) (Tseng et al., 2020) was added to the
backbones to simulate the cross-domain setting of the benchmark. We also include the FT-All (Guo
et al., 2020) baseline for the comparison that ﬁne-tunes the full network with only the cross-entropy
loss. Additionally, we compare with recent methods, STARTUP, CHEF and ATA, which have been
evaluated on the CDFSL benchmark. It is to be noted that SENet (Hu et al., 2017) scales channels
similar to the way we select features. However, it does not decompose features into relevant and
irrelevant ones. Besides, SENet cannot tackle domain-shift because it is only used as a module in a
feature extraction block. Hence, using SENet as the backbone for comparison would not be useful
(or even fair) because all the compared baselines use the ResNet-10 backbone. The results of the
comparison for 5-way 5-shot, 5-way 20-shot, and 5-way 50-shot test settings are shown in Table 1."
COMPARISONS,0.1474820143884892,"From Table 1, we see that our proposed framework ConFeSS outperforms all meta-learning methods
by a large margin. Speciﬁcally, for the 5-shot setting, our method produces improvements of 12.64 %,
21.72 %, 15.50 % and 11.49 % over the best meta-learning method on the ChestX, ISIC, EuroSAT,
and CropDisease datasets, respectively. Veritably, the improvement margin increases further as the
number of shots increases, especially for medical datasets such as ChestX and ISIC. This is because
medical datasets have similar classes and require more annotations to perform reasonably well. Also,
the FWT module fails to generalize to target datasets and sometimes negatively affects these methods.
As expected, the performance of our framework also follows the rank of domain similarity with
miniImagenet: least performance for ChestX and best performance for CropDisease."
COMPARISONS,0.14928057553956833,"The meta-learning methods use supervision for pre-training and cannot mimic distant domain datasets,
which causes them to overﬁt source data with poor generalization to distant target domains. In com-
parison, our contrastively learned backbone only learns the inherent structure of data transformations
and can generalize more effectively to other domains. Secondly, the masking module selects only
relevant features for ﬁne-tuning on the target domain, thus preventing overﬁtting. Our method also
performs better than CHEF on all settings. Compared to FT-All, ATA and STARTUP, our method
achieves much higher scores in most settings except for the CropDisease dataset, which is the easiest
benchmark for the cross-domain task as it is the most similar to miniImageNet. STARTUP uses large
amount of unlabelled target domain data while our proposed approach does not use any. Still, our
method outperforms STARTUP in more difﬁcult 9 out of 12 settings."
ADDITIONAL ANALYSES,0.1510791366906475,"4.4
ADDITIONAL ANALYSES"
ADDITIONAL ANALYSES,0.1528776978417266,"Ablation study: Table 2 shows ablation study results. The masking module uses losses functions
Lneg and Ldiv. Also, the ﬁne-tuning step uses loss Lreg. w/o Feature Mask implies that the pre-
trained network is ﬁne-tuned using only cross-entropy loss, without using a masking module. We
can also choose not to ﬁne-tune the whole backbone, which is denoted as w/o FT BB in Table 2 for"
ADDITIONAL ANALYSES,0.15467625899280577,Published as a conference paper at ICLR 2022
ADDITIONAL ANALYSES,0.1564748201438849,Table 2: Ablation results. ↑shows increase in performance with ablation.
ADDITIONAL ANALYSES,0.15827338129496402,"5-way 5-shot
5-way 20-shot
Setting
CropDisease
EuroSAT
ISIC
ChestX
CropDisease
EuroSAT
ISIC
ChestX
Full Framework
88.88
84.65
48.85
27.09
95.34
90.40
60.10
33.57
w/o Cont. Learn.
87.26
83.15
47.66
26.06
95.47 ↑
88.78
59.96
32.12
w/o FT BB
85.18
83.14
42.25
25.76
93.52
89.70
52.61
31.12
w/o Feature Mask
87.57
83.87
47.10
26.09
95.49 ↑
89.93
61.08 ↑
33.20
w/o Ldiv
87.95
84.23
48.62
26.92
94.74
89.31
59.20
32.77
w/o Lneg
89.03 ↑
84.41
48.04
26.60
94.72
90.37
60.14 ↑
32.81
w/o Lreg
87.83
83.98
48.34
26.73
94.43
90.31
59.83
32.69
Direct Positive
87.15
83.94
47.25
26.58
93.65
89.40
59.66
31.92"
ADDITIONAL ANALYSES,0.16007194244604317,"the 5-way 5-shot and 5-way 20-shot settings. As shown, in most cases, removing these components
result in a drop in performance, suggesting that all these loss functions and components are essential.
An important step in the framework is ﬁne-tuning the whole backbone, where the most signiﬁcant
drop is observed when it is absent. This indicates that for large domain differences between source
and target domain, ﬁne-tuning the backbone is essential. In the 5-shot setting, there is always a
drop in performance when removing the feature masking module. However, for the 20-shot setting,
performance improves slightly on the ISIC and CropDisease datasets. This demonstrates that the
feature masking module is more critical for fewer shot settings. Among Ldiv, Lreg, and Lneg,
there is no clear winner since their order of importance depends on the dataset and the shot. In the
table, w/o Cont. Learn. implies when the contrastive pre-training step is replaced by traditional
supervised pre-training using cross-entropy loss. The results show that using supervised pre-training
produces lower recognition accuracy than contrastive pre-training. However, the standard supervised
pre-training along with masking and ﬁne-tuning still performs better than most of the other compared
methods in Table 1. We also consider the Direct Positive setting for the ablation study, where instead
of using Lreg, we directly use the positive features obtained using the mask generator to ﬁne-tune the
feature extractor and classiﬁer. The results obtained using this technique are competitive compared to
previous work but still worse than the Full framework and w/o Lreg ablation."
ADDITIONAL ANALYSES,0.1618705035971223,"The number of features selected: We also analyzed the number of features selected for different
datasets in Fig. 2 (a). According to VC theory, the number of features selected for 5-shot setting is
less than that of the 20-shot setting to prevent overﬁtting. The number of features selected for the
50-shot setting is less because the 50-shot setting has lots of training samples and does not require a
masking module. Also, CropDisease has the highest number of features selected while ChestX has
the least number of features selected because CropDisease dataset contains natural images and is
more similar to miniImageNet."
ADDITIONAL ANALYSES,0.16366906474820145,"ChestX
ISIC
EuroSAT
CropDisease
250 260 270 280 290 300 310 320 330 340 350"
ADDITIONAL ANALYSES,0.16546762589928057,No. of features selected
SHOT,0.1672661870503597,"1shot
5shot
20shot
50shot (a)"
SHOT,0.16906474820143885,"-15
-10
-5
0
5
10
-15 -10 -5 0 5 10 15 20"
SHOT,0.17086330935251798,"Feat neg
Feat pos (b)"
SHOT,0.17266187050359713,"-15
-10
-5
0
5
10
-15 -10 -5 0 5 10 15 20"
SHOT,0.17446043165467626,"Feat neg
Feat pos"
SHOT,0.17625899280575538,"(c)
Figure 2: (a) Average number of positive features selected for different datasets and shots. T-SNE plot of positive
and negative features of CropDisease dataset for (b) 5-way 5-shot, (c) 5-way 20-shot setting
Comparison between positive and negative features: We also investigate the difference between
the positive and negative features. In practice, the positive features are more relevant to the classiﬁca-
tion task, and therefore expected to be more discriminative than the negative features. We quantify the
discrimination ability of features using the metric DS = Tr(Sb)/Tr(Sw), where Sb is the between-
class scatter matrix, Sw is the within-class scatter matrix and Tr(·) is the trace operation. The scatter
matrices are deﬁned as Sb = PN
i=1 ni(µi −µ)(µi −µ)T and Sw = PM
j=1(xj −µyj)(xj −µyj)T ,
where µi is the sample mean of the ith class, µ is the mean of all the samples, and ni is the number
of samples in the ith class with a total of N classes. (xj, yj) is the jth sample-label pair out of M
total samples. Higher values of DS indicate better discrimination. We compared DS across different
shots and datasets in Table 3 for the 5-way setting. As expected, DS scores for the positive features
are higher than that of the negative features for both the 5-shot and 20-shot settings. The DS scores
get higher for the positive features of the 20-shot setting since more training samples produce better"
SHOT,0.17805755395683454,Published as a conference paper at ICLR 2022
SHOT,0.17985611510791366,Table 3: Discrimination scores (DS) of positive (+) and negative features (-) for different shots (S) and datasets
SHOT,0.18165467625899281,"ChestX
ISIC
EuroSAT
CropDisease
Setting
5S+
5S-
20S+
20S-
5S+
5S-
20S+
20S-
5S+
5S-
20S+
20S-
5S+
5S-
20S+
20S-
DS
0.018
0.016
0.03
0.02
0.13
0.10
0.17
0.09
0.67
0.42
0.62
0.28
0.63
0.41
0.77
0.31"
SHOT,0.18345323741007194,"clusters. The DS difference between positive and negative features for the ChestX dataset is low
mostly because the dataset is very hard to cluster. Visualization using t-SNE (Maaten & Hinton, 2008)
for 5-shot and 20-shot settings are shown in Fig. 2 (b) and (c) respectively. Results show that positive
features produce better clusters compared to negative ones. As expected, the positive features for the
20-shot setting are more discriminative compared to the 5-shot setting. Also, the internal statistics of
positive and negative features are different because of Ldiv, which maximizes the statistical distance
between the two sets of features."
SHOT,0.18525179856115107,"-3
-2
-1
0
1
2
3
20 30 40 50 60 70 80 90"
SHOT,0.18705035971223022,"ISIC
EuroSAT
CropDisease
ChestX (a)"
SHOT,0.18884892086330934,"-3
-2
-1
0
1
2
3
20 30 40 50 60 70 80 90"
SHOT,0.1906474820143885,"ISIC
EuroSAT
CropDisease
ChestX (b)"
SHOT,0.19244604316546762,"-3
-2
-1
0
1
2
3
20 30 40 50 60 70 80 90"
SHOT,0.19424460431654678,"ISIC
EuroSAT
CropDisease
ChestX (c)"
SHOT,0.1960431654676259,"-3
-2
-1
0
1
2
3
20 30 40 50 60 70 80 90"
SHOT,0.19784172661870503,"ISIC
EuroSAT
CropDisease
ChestX"
SHOT,0.19964028776978418,"(d)
Figure 3: Accuracy with 5W5S setting as (a) log10 λneg (b) log10 λdiv , (c) log10 λreg and (d) log10 λpos vary."
SHOT,0.2014388489208633,"Hyper-parameter sensitivity: We study our framework’s performance as the hyper-parameters
λneg, λpos, λdiv, and λreg are varied. The results for the 5-shot setting are shown in Fig. 3, which
shows that the recognition performance is stable with respect to λneg, λpos, and λdiv, while the
performance drops for larger values of λreg. This is because λneg, λpos, and λdiv affect the learning
of a much smaller feature masking network. On the other hand, the value of λreg affects the learning
of a much larger network - the target feature extractor, which eventually plays a direct role in the
inference stage. The plot in Fig. 3 (c) shows that we should choose λreg < 1 for better performance."
SHOT,0.20323741007194246,"The impact of the number of augmentations: We report how the number of augmentations Nt used
in contrastive pre-training affects cross-domain few-shot recognition performance. Results for 5-way
5-shot, 5-way 20-shot, and 5-way 50 shot settings are shown in Table 4. Results show a sharp drop in"
SHOT,0.20503597122302158,Table 4: Recognition performance on the N-way K-shot setting as Nt is varied during pre-training.
SHOT,0.2068345323741007,"5-way 5-shot
5-way 20-shot
5-way 50-shot
Dataset/Nt
3
10
20
30
3
10
20
30
3
10
20
30
CropDisease
88.88
70.11
72.28
70.51
95.34
86.21
86.85
87.08
97.56
92.48
92.10
92.42
EuroSAT
84.65
62.28
60.38
59.93
90.40
72.45
70.40
69.50
92.66
76.87
75.99
75.29
ISIC
48.85
37.59
38.08
36.64
60.10
48.60
48.80
47.99
65.34
53.86
54.78
53.78
ChestX
27.09
23.30
23.29
23.16
33.57
25.68
25.24
25.37
39.02
27.72
27.34
27.25"
SHOT,0.20863309352517986,"recognition performance when the number of augmentations is increased beyond 3. This is because
additional augmentations in the source dataset do not represent the possible augmentations in the
target datasets. With higher Nt, there is a propensity to have augmentations that are not valid for target
classes. The target datasets consist of specialized domains like medical and satellite imagery, which
also exuberate inconsistent categories when the target datasets are transformed using arbitrary source
augmentation policies. As a result, contrastive representations learned using those augmentations
might not generalize well. For example, in ChestX, random cropping or Gaussian blur might affect
discriminative regions in images. This phenomenon has also been recently studied in (Xiao et al.,
2021), where color augmented representations do not transfer well for color discrimination tasks."
CONCLUSION,0.210431654676259,"5
CONCLUSION
We presented a framework called ConFeSS (Contrastive Learning and Feature Selection System) to
learn a generalizable representation followed by a feature selection mechanism while ﬁne-tuning
on the target domain. We introduce novel loss constraints on selecting relevant and irrelevant
features for the target domain. Extensive experiments conducted on the cross-domain few-shot
learning benchmark show our approach’s advantages over the meta-learning and other CDFSL
methods. Additional analyses also provide insights into the feature selection mechanism and justify
the importance of each component of our framework."
CONCLUSION,0.21223021582733814,Published as a conference paper at ICLR 2022
REFERENCES,0.21402877697841727,REFERENCES
REFERENCES,0.2158273381294964,"Thomas Adler, Johannes Brandstetter, Michael Widrich, Andreas Mayr, David Kreil, Michael Kopp,
Günter Klambauer, and Sepp Hochreiter. Cross-domain few-shot learning by representation fusion.
arXiv preprint arXiv:2010.06498, 2020."
REFERENCES,0.21762589928057555,"Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. arXiv:1308.3432, 2013."
REFERENCES,0.21942446043165467,"Rodrigo Berriel, Stephane Lathuillere, Moin Nabi, Tassilo Klein, Thiago Oliveira-Santos, Nicu Sebe,
and Elisa Ricci. Budget-aware adapters for multi-domain learning. In International Conference on
Computer Vision, pp. 382–391, 2019."
REFERENCES,0.22122302158273383,"Zhong Cao, Jiang Lu, Jian Liang, and Changshui Zhang. A theory of self-supervised framework for
few-shot learning, 2021. URL https://openreview.net/forum?id=-aThAo4b1zn."
REFERENCES,0.22302158273381295,"Da Chen, Yuefeng Chen, Yuhong Li, Feng Mao, Yuan He, and Hui Xue. Self-supervised learning
for few-shot image classiﬁcation. In International Conference on Acoustics, Speech, and Signal
Processing, pp. 1745–1749, 2021."
REFERENCES,0.22482014388489208,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International Conference on Machine Learning,
2020."
REFERENCES,0.22661870503597123,"Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look
at few-shot classiﬁcation. In International Conference on Learning Representations, 2019."
REFERENCES,0.22841726618705036,"Noel CF Codella, David Gutman, Celebi, et al. Skin lesion analysis toward melanoma detection. In
International Symposium on Biomedical Imaging, pp. 168–172, 2018."
REFERENCES,0.2302158273381295,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Conference on Computer Vision and Pattern Recognition, pp.
248–255, 2009."
REFERENCES,0.23201438848920863,"Anamika Dhillon and Gyanendra K Verma. Convolutional neural network: a review of models,
methodologies and applications to object detection. Progress in Artiﬁcial Intelligence, 9(2):85–112,
2020."
REFERENCES,0.23381294964028776,"Carl Doersch, Ankush Gupta, and Andrew Zisserman. Crosstransformers: spatially-aware few-shot
transfer. In Conference on Neural Information Processing Systems, 2020."
REFERENCES,0.2356115107913669,"Nikita Dvornik, Cordelia Schmid, and Julien Mairal. Selecting relevant features from a multi-domain
representation for few-shot classiﬁcation. In European Conference on Computer Vision, 2020."
REFERENCES,0.23741007194244604,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In International Conference on Machine Learning, pp. 1126–1135, 2017."
REFERENCES,0.2392086330935252,"Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by
predicting image rotations. In International Conference on Learning Representations, 2018."
REFERENCES,0.24100719424460432,"Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Pérez, et al. Boosting few-shot visual learning
with self-supervision. In International Conference on Computer Vision, pp. 8059–8068, 2019."
REFERENCES,0.24280575539568344,"Henry Gouk, Timothy Hospedales, and massimiliano pontil. Distance-based regularisation of deep
networks for ﬁne-tuning. In International Conference on Learning Representations, 2021."
REFERENCES,0.2446043165467626,"Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. A
kernel two-sample test. Journal of Machine Learning Research, 13(1):723–773, 2012."
REFERENCES,0.24640287769784172,"Yunhui Guo, Noel C Codella, et al. A broader study of cross-domain few-shot learning. In European
Conference on Computer Vision, 2020."
REFERENCES,0.24820143884892087,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Conference on Computer Vision and Pattern
Recognition, pp. 9729–9738, 2020."
REFERENCES,0.25,Published as a conference paper at ICLR 2022
REFERENCES,0.2517985611510791,"Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and
deep learning benchmark for land use and land cover classiﬁcation. Journal of Selected Topics in
Applied Earth Observations and Remote Sensing, 12(7):2217–2226, 2019."
REFERENCES,0.25359712230215825,"Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural
networks: A survey. Transactions on Pattern Analysis and Machine Intelligence, 2021."
REFERENCES,0.25539568345323743,"Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. Squeeze-and-excitation networks.
Transactions on Pattern Analysis and Machine Intelligence, 2017."
REFERENCES,0.25719424460431656,"Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In
International Conference on Learning Representations, 2017."
REFERENCES,0.2589928057553957,"Myeongjae Jeon et al. Analysis of large-scale multi-tenant GPU clusters for DNN training workloads.
In USENIX Annual Technical Conference, pp. 947–960, 2019."
REFERENCES,0.2607913669064748,"Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses
for scene geometry and semantics. In Conference on Computer Vision and Pattern Recognition, pp.
7482–7491, 2018."
REFERENCES,0.26258992805755393,"Siavash Khodadadeh, Ladislau Boloni, and Mubarak Shah. Unsupervised meta-learning for few-shot
image classiﬁcation. In Conference on Neural Information Processing Systems, 2019."
REFERENCES,0.2643884892086331,"Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, et al. Meta-learning with differentiable
convex optimization. In Conference on Computer Vision and Pattern Recognition, pp. 10657–
10665, 2019."
REFERENCES,0.26618705035971224,"Bo Liu, Ying Wei, Yu Zhang, and Qiang Yang. Deep neural networks for high dimension, low sample
size data. In International Joint Conference on Artiﬁcial Intelligence, pp. 2287–2293, 2017."
REFERENCES,0.26798561151079137,"Laurens Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning
Research, 9(Nov):2579–2605, 2008."
REFERENCES,0.2697841726618705,"Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. In International Conference on Learning Representations,
2017."
REFERENCES,0.2715827338129496,"Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple
tasks by learning to mask weights. In European Conference on Computer Vision, pp. 67–82, 2018."
REFERENCES,0.2733812949640288,"Massimiliano Mancini, Elisa Ricci, Barbara Caputo, and Samuel Rota Bulo. Adding new tasks to
a single network with weight transformations using binary masks. In European Conference on
Computer Vision Workshops, pp. 180–189, 2018."
REFERENCES,0.2751798561151079,"Carlos Medina, Arnout Devos, and Matthias Grossglauser. Self-supervised prototypical transfer
learning for few-shot classiﬁcation. arXiv preprint arXiv:2006.11325, 2020."
REFERENCES,0.27697841726618705,"Sharada P Mohanty, David P Hughes, and Marcel Salathé. Using deep learning for image-based
plant disease detection. Frontiers in Plant Science, 7:1419, 2016."
REFERENCES,0.2787769784172662,"Phuc Nguyen, Ting Liu, Gautam Prasad, and Bohyung Han. Weakly supervised action localization
by sparse temporal pooling network. In Conference on Computer Vision and Pattern Recognition,
pp. 6752–6761, 2018."
REFERENCES,0.2805755395683453,"Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw
puzzles. In European Conference on Computer Vision, pp. 69–84, 2016."
REFERENCES,0.2823741007194245,"Cheng Perng Phoo and Bharath Hariharan. Self-training for few-shot transfer across extreme task
differences. In International Conference on Learning Representations, 2021."
REFERENCES,0.2841726618705036,"Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, et al.
A theoretical analysis of contrastive
unsupervised representation learning. In International Conference on Machine Learning, pp.
5628–5637, 2019."
REFERENCES,0.28597122302158273,Published as a conference paper at ICLR 2022
REFERENCES,0.28776978417266186,"John Shawe-Taylor and N Cristianini. An introduction to support vector machines and other kernel-
based learning methods. 2000."
REFERENCES,0.289568345323741,"Jake Snell, Kevin Swersky, et al. Prototypical networks for few-shot learning. In Conference on
Neural Information Processing Systems, 2017."
REFERENCES,0.29136690647482016,"Jong-Chyi Su, Subhransu Maji, and Bharath Hariharan. When does self-supervision improve few-shot
learning? In European Conference on Computer Vision, pp. 645–666, 2020."
REFERENCES,0.2931654676258993,"Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable
effectiveness of data in deep learning era. In International Conference on Computer Vision, pp.
843–852, 2017."
REFERENCES,0.2949640287769784,"Flood Sung, Yongxin Yang, Li Zhang, et al. Learning to compare: Relation network for few-shot
learning. In Conference on Computer Vision and Pattern Recognition, pp. 1199–1208, 2018."
REFERENCES,0.29676258992805754,"Eleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, et al. Meta-dataset: A dataset of datasets for
learning to learn from few examples. In International Conference on Learning Representations,
2020."
REFERENCES,0.29856115107913667,"Philipp Tschandl et al. The ham10000 dataset, a large collection of multi-source dermatoscopic
images of common pigmented skin lesions. Scientiﬁc Data, 2018."
REFERENCES,0.30035971223021585,"Hung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang, and Ming-Hsuan Yang. Cross-domain few-shot
classiﬁcation via learned feature-wise transformation. In International Conference on Learning
Representations, 2020."
REFERENCES,0.302158273381295,"Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. In Conference on Neural Information Processing Systems, pp. 3630–3638, 2016."
REFERENCES,0.3039568345323741,"Haoqing Wang and Zhi-Hong Deng. Cross-domain few-shot classiﬁcation via adversarial task
augmentation. In International Joint Conference on Artiﬁcial Intelligence, pp. 1075–1081, 2021."
REFERENCES,0.3057553956834532,"Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples:
A survey on few-shot learning. Computing Surveys, 53(3):1–34, 2020."
REFERENCES,0.30755395683453235,"Xiaosong Wang et al. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-
supervised classiﬁcation and localization of common thorax diseases. In Conference on Computer
Vision and Pattern Recognition, pp. 2097–2106, 2017."
REFERENCES,0.30935251798561153,"Tete Xiao, Xiaolong Wang, Alexei A Efros, and Trevor Darrell. What should not be contrastive in
contrastive learning. In International Conference on Learning Representations, 2021."
REFERENCES,0.31115107913669066,"Liang Yao, Chengsheng Mao, and Yuan Luo. Graph convolutional networks for text classiﬁcation. In
Conference on Artiﬁcial Intelligence, volume 33, pp. 7370–7377, 2019."
REFERENCES,0.3129496402877698,"Kaichao You, Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Universal
domain adaptation. In Conference on Computer Vision and Pattern Recognition, pp. 2720–2729,
2019."
REFERENCES,0.3147482014388489,"Sungrack Yun, Janghoon Cho, et al. An end-to-end text-independent speaker veriﬁcation framework
with a keyword adversarial network. In Conference of the International Speech Communication
Association, 2019."
REFERENCES,0.31654676258992803,"Qingchen Zhang, Laurence T Yang, Zhikui Chen, and Peng Li. A survey on deep learning for big
data. Information Fusion, 42:146–157, 2018."
REFERENCES,0.3183453237410072,"Bo Zhao, Xinwei Sun, et al. Msplit lbi: Realizing feature selection and dense estimation simultane-
ously in few-shot and zero-shot learning. In International Conference on Machine Learning, pp.
5912–5921, 2018."
REFERENCES,0.32014388489208634,"Jian Zhou et al. Deep learning sequence-based ab initio prediction of variant effects on expression
and disease risk. Nature genetics, 50(8):1171–1179, 2018."
REFERENCES,0.32194244604316546,Published as a conference paper at ICLR 2022
REFERENCES,0.3237410071942446,"A
ADDITIONAL EXPERIMENTAL DETAILS"
REFERENCES,0.3255395683453237,"All our experiments were conducted in a cluster of nodes where the nodes contain NVIDIA Tesla V100
GPUs with a mix of 16GB and 32GB memory. The details of the benchmark used for comparison
have been introduced in the following repository: https://github.com/IBM/cdfsl-benchmark. We
only consider the single source domain setting where the source domain is miniImageNet 1, and the
target domains are ChestX 2, ISIC 3, EuroSAT 4 and CropDisease 5. Additional experimental details
include: (a) Image size: 224 × 224 (b) Batch size during adaptation: 5 (c) MMD kernel numbers: 5
(d) MMD kernel multiplier: 2.0. The masking module architecture is as follows: [Linear(512,256) -
ReLU - Linear(256,512) - Gumbel Sigmoid]"
REFERENCES,0.3273381294964029,"B
IMPLEMENTATION OF MMD"
REFERENCES,0.329136690647482,"We use a multi-kernel approach to implement MMD. Speciﬁcally, for the positive and negative
feature matrix: F+ ∈RN×d and F−∈RN×d, MMD = mean(XX + Y Y −XY −Y X)."
REFERENCES,0.33093525179856115,"Here, [XX]ij = Pnk−1
k=0 exp(−
||F+
i:−F+
j:||2
2
bmk
), [Y Y ]ij = Pnk−1
k=0 exp(−
||F−
i: −F−
j:||2
2
bmk
), [XY ]ij =
Pnk−1
k=0 exp(−
||F+
i:−F−
j:||2
2
bmk
) and [Y X]ij = Pnk−1
k=0 exp(−
||F−
i: −F+
j:||2
2
bmk
). F+
i: is the ith row of F+."
REFERENCES,0.3327338129496403,"m is the kernel multiplier and nk is the number of kernels. b is the bandwidth and is computed as
b =
sum(D)
(4N2−2N)(mﬂoor(0.5nk)) where [D]ij = ||Fi: −Fj:||2
2 and F ∈R2N×d is the concatenation of F+"
REFERENCES,0.3345323741007194,and F−.
REFERENCES,0.3363309352517986,"C
DATA AUGMENTATION FOR CONTRASTIVE PRE-TRAINING"
REFERENCES,0.3381294964028777,"The augmentation policies are the same as those used for pre-training the SimCLR framework (Chen
et al., 2020). They include the following transformations:"
REFERENCES,0.33992805755395683,"• Random Crop and Resize: The random cropping has scale in the range [0.08, 1.0] and
aspect ratio in the range [3/4, 4/3]. The random cropping is always followed by a horizontal
ﬂip with each ﬂip type having a probability of 0.5. This is followed by resizing of image to
the desired size."
REFERENCES,0.34172661870503596,"• Color Distortion: Color jitter is applied with a probability of 0.8. The jitter strength values
of brightness, contrast, saturation and hue are set as 0.8, 0.8, 0.8 and 0.2, respectively. This
is followed by color drop operation (convert to grayscale) with a probability of 0.2."
REFERENCES,0.3435251798561151,"• Gaussian Blur: This is applied with a probability of 0.5, and radius of blur is selected
randomly from the range [0.1, 2.0]."
REFERENCES,0.34532374100719426,"D
EFFECT OF DIFFERENT WAYS"
REFERENCES,0.3471223021582734,"We also test our framework on higher number of ways i.e. higher number of classes per episodic
evaluation. The results of the experiment are shown in Table 5. As expected, higher number of ways
leads to drop in performance because of more difﬁculty in discrimination. Surprisingly, the drop
in performance is less for easier datasets like EuroSAT and CropDisease. This might be probably
because the additional classes for higher ways leads to less confusion compared to those of ChestX
and ISIC."
REFERENCES,0.3489208633093525,"1100 categories. Downloaded from: https://drive.google.com/ﬁle/d/1uxpnJ3Pmmwl-6779qiVJ5JpWwOGl48xt/view
27 categories. License and download information available at: https://www.kaggle.com/nih-chest-xrays/data
37 categories. License and download information available at: https://challenge.isic-archive.com/data#2018
410 categories. Downloaded from: http://madm.dfki.de/ﬁles/sentinel/EuroSAT.zip
538 categories. License and download information available at: https://www.kaggle.com/saroz014/plant-disease"
REFERENCES,0.35071942446043164,Published as a conference paper at ICLR 2022
REFERENCES,0.35251798561151076,"Table 5: Recognition results of our approach with different ways and shots (S) on the ChestX, ISIC, EuroSAT
and CropDisease datasets. The entries with – imply that testing is not possible because of lesser number of total
categories present."
REFERENCES,0.35431654676258995,"ChestX
ISIC
EuroSAT
CropDisease
Setting
5S
20S
50S
5S
20S
50S
5S
20S
50S
5S
20S
50S
5-way
27.09
33.57
39.02
48.85
60.10
65.34
84.65
90.40
92.66
88.88
95.34
97.56
7-way
20.72
27.15
32.33
40.38
52.57
59.29
80.23
87.66
90.90
85.37
94.00
96.51
10-way
–
–
–
–
–
–
76.03
84.59
88.15
82.10
92.63
95.48
19-way
–
–
–
–
–
–
–
–
–
75.31
88.45
93.39"
REFERENCES,0.35611510791366907,"E
COMPARISON IN 1-SHOT SETTING"
REFERENCES,0.3579136690647482,"Of all the compared methods, only STARTUP (Phoo & Hariharan, 2021) and ATA (Wang & Deng,
2021) have been evaluated on the 1-shot setting. Hence, we evaluate our model on the 1-shot setting
and report the results in Table 6. Results show that our method is still competitive with respect to
existing methods."
REFERENCES,0.3597122302158273,Table 6: Comparison on the 1-shot setting.
REFERENCES,0.36151079136690645,"ChestX
ISIC
EuroSAT
CropDisease
Method
1S
1S
1S
1S
STARTUP
23.09
32.66
63.88
75.93
ATA
22.14
34.70
65.94
77.82
ConFeSS
23.67
33.46
65.51
76.49"
REFERENCES,0.36330935251798563,"F
COMPARISON WITH UNSUPERVISED META-TRAINING"
REFERENCES,0.36510791366906475,"We also compare our method with UMTRA (Khodadadeh et al., 2019) - an unsupervised meta-
training framework derived from the popular meta-learning framework MAML (Finn et al., 2017).
UMTRA uses a similar algorithm as MAML but extends it to the case of unlabeled training data.
Class membership for unlabeled data is determined such that a sample and its augmentation belong
to the same class. We compare against two versions: UMTRA-ProtoNet and UMTRA-ProtoTune as
reported in (Medina et al., 2020) on the CDFSL benchmark (Guo et al., 2020). UMTRA-ProtoTune
extends UMTRA-ProtoNet by ﬁne-tuning on target domain data. The results in Table 7 show that our
method outperforms the two variants of UMTRA on all shots and all datasets."
REFERENCES,0.3669064748201439,"Table 7: Comparison against unsupervised meta-training with different shots (S) and 5-way setting on the
ChestX, ISIC, EuroSAT and CropDisease datasets."
REFERENCES,0.368705035971223,"ChestX
ISIC
EuroSAT
CropDisease
Method
5S
20S
50S
5S
20S
50S
5S
20S
50S
5S
20S
50S
UMTRA-ProtoNet
24.94
28.04
29.88
39.21
44.62
46.48
74.91
80.42
82.24
79.81
86.84
88.44
UMTRA-ProtoTune
25.00
30.41
35.63
38.47
51.60
60.12
68.11
81.56
85.05
82.67
92.04
95.46
ConFeSS
27.09
33.57
39.02
48.85
60.10
65.34
84.65
90.40
92.66
88.88
95.34
97.56"
REFERENCES,0.37050359712230213,"G
EFFECT OF DIFFERENT EPOCH NUMBERS"
REFERENCES,0.3723021582733813,"The numbers of epochs for training the masking module and ﬁne-tuning are ﬁxed at 15 and 50
respectively because we do not have validation split from few-shot target domain dataset to set an
optimal value. The epoch numbers (15 and 50) are kept substantially low compared to pre-training
epoch number (400) so that the network is less prone to over-ﬁtting on few-shot data. To show
the effect of different epoch numbers, we perform the following experiments on the 5-way 5-shot
setting. Firstly, the number of epochs for training the masking module is varied and then the number
of epochs for ﬁne-tuning is varied. In Table 8, we show the results for varying epochs for training
masking module while keeping epochs for ﬁne-tuning ﬁxed at 50 as well as the results for varying
epochs for ﬁne-tuning while keeping epochs for training masking module ﬁxed at 15. From the
results, it seems that the recognition performance is not very sensitive to the number of epochs used
for training masking module or for ﬁne-tuning. However, increasing ﬁne-tuning epochs beyond 50
tends to decrease performance slightly for ISIC, EuroSAT and CropDisease datasets."
REFERENCES,0.37410071942446044,Published as a conference paper at ICLR 2022
REFERENCES,0.37589928057553956,"Table 8: Effect of varying epochs for training masking module and for ﬁne-tuning. In the ﬁrst column, number
without parantheses is the epoch number for training masking module while that with parantheses is the epoch
number for ﬁne-tuning. The results are shown in corresponding labelled super-columns."
REFERENCES,0.3776978417266187,"Varying epoch number for training masking module
(Varying epoch number for ﬁne-tuning)
Setting
ChestX
ISIC
EuroSAT
CropDisease
ChestX
ISIC
EuroSAT
CropDisease
5 (25)
26.50
48.82
83.64
87.64
26.82
48.54
84.23
88.27
10 (50)
27.26
48.24
83.90
88.78
27.09
48.85
84.65
88.88
15 (75)
27.09
48.85
84.65
88.88
27.09
48.23
83.10
88.56
20 (100)
26.87
48.48
84.18
88.01
27.33
47.83
82.86
88.23
25 (125)
27.27
48.34
83.69
88.04
27.07
47.48
82.31
88.05"
REFERENCES,0.37949640287769787,"H
ALTERNATIVE MODEL DESIGN CHOICES"
REFERENCES,0.381294964028777,"In this section, we consider the following alternative model designs and report recognition perfor-
mance on the 5-way 5-shot, 5-way 20-shot and 5-way 50-shot setting. The evaluation setup is similar
to that described in Section 4.2:"
REFERENCES,0.3830935251798561,"• L1 norm: In this design of the ConFeSS framework, we just replace the L2 norm ||f t
i −f +
i ||2
2
in Eq. 8 with the L1 norm ||f t
i −f +
i ||2
1.
• Source Mask: In this setup, we train the two layer mask module (deﬁned in appendix A)
on the source dataset rather than the target dataset. The source dataset used is miniImageNet.
Speciﬁcally, the mask module is trained on top of the contrastively learned pre-trained
feature extractor with the miniImageNet dataset. In the ﬁnal step, the target feature extractor
is ﬁne-tuned on the target dataset.
• Neg. Reg.: In this setup, we use negative features for the regularization. Speciﬁcally, we
use ||f t
i −f −
i ||2
2 in Eq. 8 instead of ||f t
i −f +
i ||2
2.
• Dir. Neg.: We also consider the Direct Negative setting for the ablation study, where instead
of using Lreg, we directly use the negative features obtained using the mask generator to
ﬁne-tune the feature extractor and classiﬁer.
• K Layer Mask Mod.: Here, we study the effect of having masking module of different
sizes. Here, K stands for the number of layers used for the masking module. Speciﬁcally, we
study the effect for K = 3, 4, 5. The masking module architecture for K = 3, 4, 5 are [Lin-
ear(512,256) - BatchNorm1D(256) - ReLU - Linear(256,128) - BatchNorm1D(128) - ReLU
- Linear(128, 512) - Gumbel Sigmoid], [Linear(512,256) - BatchNorm1D(256) - ReLU - Lin-
ear(256,128) - BatchNorm1D(128) - ReLU - Linear(128, 256) - BatchNorm1D(256) - ReLU
- Linear(256, 512) - Gumbel Sigmoid], and [Linear(512,256) - BatchNorm1D(256) - ReLU
- Linear(256,128) - BatchNorm1D(128) - ReLU - Linear(128, 64) - BatchNorm1D(64) -
ReLU - Linear(64, 128) - BatchNorm1D(128) - ReLU - Linear(128, 512) - Gumbel Sigmoid],
respectively.
• Joint Training:
We consider the setup where the the masking module and the target
feature extractor are trained together in one stage instead of the proposed two stages, using
combined losses in Eq. 6 and Eq. 9."
REFERENCES,0.38489208633093525,"The results of comparing these alternative model designs with our proposed framework ConFeSS are
shown in Table 9. Results show that among all these alternative model designs, especially L1 norm,
Source Mask, Neg. Reg., and Dir. Neg. perform poorly compared to our original ConFeSS framework.
In most of the cases, having a larger masking module produces similar or slightly better performance
compared to ConFeSS because of better representation capacity of output masks. Joint training of
masking module and target feature extractor produces poorer recognition performance for 5 shot
setting compared to ConFeSS. However, for higher shot setting, the joint training procedure produces
similar or better performance compared to ConFeSS. This is because joint training encompasses
optimization of larger number of parameters, which might cause the network to overﬁt on lower shots
while exploit additional amount of training data for higher shots."
REFERENCES,0.38669064748201437,"I
RESULTS WITH CONFIDENCE INTERVAL"
REFERENCES,0.38848920863309355,"In this section, we re-report comparison studies: Table 10 and Table 11 show the performance with
95 % conﬁdence interval for Table 1 and Table 2, respectively."
REFERENCES,0.3902877697841727,Published as a conference paper at ICLR 2022
REFERENCES,0.3920863309352518,"Table 9: Recognition performance on alternative model designs along with 95 % conﬁdence interval shown in
parentheses for different shots and datasets."
REFERENCES,0.39388489208633093,"ChestX
ISIC
EuroSAT
CropDisease
Method
5W5S
5W20S
5W50S
5W5S
5W20S
5W50S
5W5S
5W20S
5W50S
5W5S
5W20S
5W50S"
REFERENCES,0.39568345323741005,"L1 norm
25.78
(0.44)"
REFERENCES,0.39748201438848924,"31.79
(0.47)"
REFERENCES,0.39928057553956836,"38.41
(0.52)"
REFERENCES,0.4010791366906475,"45.45
(0.35)"
REFERENCES,0.4028776978417266,"56.40
(0.35)"
REFERENCES,0.40467625899280574,"62.32
(0.30)"
REFERENCES,0.4064748201438849,"81.29
(0.42)"
REFERENCES,0.40827338129496404,"89.81
(0.39)"
REFERENCES,0.41007194244604317,"90.34
(0.17)"
REFERENCES,0.4118705035971223,"83.80
(0.28)"
REFERENCES,0.4136690647482014,"92.65
(0.67)"
REFERENCES,0.4154676258992806,"96.19
(0.23)"
REFERENCES,0.4172661870503597,"Source Mask
25.82
(0.34)"
REFERENCES,0.41906474820143885,"30.59
(0.47)"
REFERENCES,0.420863309352518,"36.55
(0.21)"
REFERENCES,0.4226618705035971,"44.04
(0.32)"
REFERENCES,0.4244604316546763,"53.69
(0.24)"
REFERENCES,0.4262589928057554,"61.17
(0.43)"
REFERENCES,0.42805755395683454,"79.57
(0.60)"
REFERENCES,0.42985611510791366,"85.33
(0.48)"
REFERENCES,0.4316546762589928,"90.78
(0.39)"
REFERENCES,0.43345323741007197,"85.12
(0.38)"
REFERENCES,0.4352517985611511,"90.26
(0.25)"
REFERENCES,0.4370503597122302,"94.75
(0.24)"
REFERENCES,0.43884892086330934,"Neg. Reg.
24.05
(0.11)"
REFERENCES,0.44064748201438847,"28.21
(0.25)"
REFERENCES,0.44244604316546765,"29.32
(0.23)"
REFERENCES,0.4442446043165468,"39.57
(0.28)"
REFERENCES,0.4460431654676259,"49.50
(0.51)"
REFERENCES,0.447841726618705,"51.99
(0.50)"
REFERENCES,0.44964028776978415,"73.29
(0.37)"
REFERENCES,0.45143884892086333,"82.27
(0.62)"
REFERENCES,0.45323741007194246,"85.48
(0.35)"
REFERENCES,0.4550359712230216,"79.72
(0.46)"
REFERENCES,0.4568345323741007,"88.15
(0.33)"
REFERENCES,0.45863309352517984,"90.81
(0.22)"
REFERENCES,0.460431654676259,"Dir. Neg.
24.23
(0.41)"
REFERENCES,0.46223021582733814,"27.48
(0.36)"
REFERENCES,0.46402877697841727,"30.32
(0.22)"
REFERENCES,0.4658273381294964,"39.17
(0.44)"
REFERENCES,0.4676258992805755,"48.24
(0.38)"
REFERENCES,0.4694244604316547,"50.72
(0.51)"
REFERENCES,0.4712230215827338,"72.69
(0.36)"
REFERENCES,0.47302158273381295,"81.34
(0.55)"
REFERENCES,0.4748201438848921,"83.98
(0.17)"
REFERENCES,0.4766187050359712,"80.32
(0.41)"
REFERENCES,0.4784172661870504,"86.15
(0.12)"
REFERENCES,0.4802158273381295,"91.24
(0.53)"
REFERENCES,0.48201438848920863,"3 Layer Mask Mod.
26.79
(0.36)"
REFERENCES,0.48381294964028776,"34.12
(0.28)"
REFERENCES,0.4856115107913669,"40.04
(0.12)"
REFERENCES,0.48741007194244607,"48.84
(0.27)"
REFERENCES,0.4892086330935252,"60.26
(0.23)"
REFERENCES,0.4910071942446043,"65.88
(0.46)"
REFERENCES,0.49280575539568344,"84.13
(0.34)"
REFERENCES,0.49460431654676257,"90.59
(0.23)"
REFERENCES,0.49640287769784175,"91.52
(0.27)"
REFERENCES,0.4982014388489209,"88.36
(0.37)"
REFERENCES,0.5,"95.72
(0.41)"
REFERENCES,0.5017985611510791,"97.67
(0.10)"
REFERENCES,0.5035971223021583,"4 Layer Mask Mod.
27.19
(0.38)"
REFERENCES,0.5053956834532374,"34.11
(0.22)"
REFERENCES,0.5071942446043165,"40.34
(0.20)"
REFERENCES,0.5089928057553957,"48.79
(0.18)"
REFERENCES,0.5107913669064749,"60.79
(0.29)"
REFERENCES,0.512589928057554,"65.95
(0.11)"
REFERENCES,0.5143884892086331,"84.16
(0.32)"
REFERENCES,0.5161870503597122,"90.63
(0.26)"
REFERENCES,0.5179856115107914,"91.70
(0.28)"
REFERENCES,0.5197841726618705,"88.38
(0.15)"
REFERENCES,0.5215827338129496,"95.56
(0.44)"
REFERENCES,0.5233812949640287,"97.61
(0.33)"
REFERENCES,0.5251798561151079,"5 Layer Mask Mod.
26.96
(0.42)"
REFERENCES,0.5269784172661871,"34.63
(0.21)"
REFERENCES,0.5287769784172662,"39.79
(0.20)"
REFERENCES,0.5305755395683454,"48.70
(0.37)"
REFERENCES,0.5323741007194245,"60.24
(0.15)"
REFERENCES,0.5341726618705036,"66.39
(0.39)"
REFERENCES,0.5359712230215827,"83.74
(0.23)"
REFERENCES,0.5377697841726619,"90.65
(0.14)"
REFERENCES,0.539568345323741,"91.20
(0.14)"
REFERENCES,0.5413669064748201,"88.10
(0.50)"
REFERENCES,0.5431654676258992,"95.93
(0.56)"
REFERENCES,0.5449640287769785,"97.64
(0.12)"
REFERENCES,0.5467625899280576,"Joint Training
25.93
(0.41)"
REFERENCES,0.5485611510791367,"34.33
(0.32)"
REFERENCES,0.5503597122302158,"39.57
(0.30)"
REFERENCES,0.552158273381295,"47.24
(0.41)"
REFERENCES,0.5539568345323741,"59.62
(0.23)"
REFERENCES,0.5557553956834532,"66.12
(0.34)"
REFERENCES,0.5575539568345323,"83.01
(0.30)"
REFERENCES,0.5593525179856115,"90.23
(0.12)"
REFERENCES,0.5611510791366906,"91.50
(0.38)"
REFERENCES,0.5629496402877698,"88.27
(0.43)"
REFERENCES,0.564748201438849,"95.71
(0.24)"
REFERENCES,0.5665467625899281,"97.60
(0.18)"
REFERENCES,0.5683453237410072,"ConFeSS
27.09
(0.24)"
REFERENCES,0.5701438848920863,"33.57
(0.31)"
REFERENCES,0.5719424460431655,"39.02
(0.12)"
REFERENCES,0.5737410071942446,"48.85
(0.29)"
REFERENCES,0.5755395683453237,"60.10
(0.33)"
REFERENCES,0.5773381294964028,"65.34
(0.45)"
REFERENCES,0.579136690647482,"84.65
(0.38)"
REFERENCES,0.5809352517985612,"90.40
(0.24)"
REFERENCES,0.5827338129496403,"92.66
(0.36)"
REFERENCES,0.5845323741007195,"88.88
(0.51)"
REFERENCES,0.5863309352517986,"95.34
(0.48)"
REFERENCES,0.5881294964028777,"97.56
(0.43)"
REFERENCES,0.5899280575539568,Table 10: Table 1 results along with 95 % conﬁdence interval shown in parentheses.
REFERENCES,0.591726618705036,"ChestX
ISIC
EuroSAT
CropDisease
Method
5W5S
5W20S
5W50S
5W5S
5W20S
5W50S
5W5S
5W20S
5W50S
5W5S
5W20S
5W50S"
REFERENCES,0.5935251798561151,"MatchNet
22.40
(0.7)"
REFERENCES,0.5953237410071942,"23.61
(0.86)"
REFERENCES,0.5971223021582733,"22.12
(0.88)"
REFERENCES,0.5989208633093526,"36.74
(0.53)"
REFERENCES,0.6007194244604317,"45.72
(0.53)"
REFERENCES,0.6025179856115108,"54.58
(0.65)"
REFERENCES,0.60431654676259,"64.45
(0.63)"
REFERENCES,0.6061151079136691,"77.10
(0.57)"
REFERENCES,0.6079136690647482,"54.44
(0.67)"
REFERENCES,0.6097122302158273,"66.39
(0.78)"
REFERENCES,0.6115107913669064,"76.38
(0.67)"
REFERENCES,0.6133093525179856,"58.53
(0.73)"
REFERENCES,0.6151079136690647,"MatchNet+FWT
21.26
(0.31)"
REFERENCES,0.6169064748201439,"23.23
(0.37)"
REFERENCES,0.6187050359712231,"23.01
(0.34)"
REFERENCES,0.6205035971223022,"30.40
(0.48)"
REFERENCES,0.6223021582733813,"32.01
(0.48)"
REFERENCES,0.6241007194244604,"33.17
(0.43)"
REFERENCES,0.6258992805755396,"56.04
(0.65)"
REFERENCES,0.6276978417266187,"63.38
(0.69)"
REFERENCES,0.6294964028776978,"62.75
(0.76)"
REFERENCES,0.6312949640287769,"62.74
(0.90)"
REFERENCES,0.6330935251798561,"74.90
(0.71)"
REFERENCES,0.6348920863309353,"75.68
(0.78)"
REFERENCES,0.6366906474820144,"MAML
23.48
(0.96)"
REFERENCES,0.6384892086330936,"27.53
(0.43)
–
40.13
(0.58)"
REFERENCES,0.6402877697841727,"52.36
(0.57)
–
71.70
(0.72)"
REFERENCES,0.6420863309352518,"81.95
(0.55)
–
78.05
(0.68)"
REFERENCES,0.6438848920863309,"89.75
(0.42)
–"
REFERENCES,0.64568345323741,"ProtoNet
24.05
(1.01)"
REFERENCES,0.6474820143884892,"28.21
(1.15)"
REFERENCES,0.6492805755395683,"29.32
(1.12)"
REFERENCES,0.6510791366906474,"39.57
(0.57)"
REFERENCES,0.6528776978417267,"49.50
(0.55)"
REFERENCES,0.6546762589928058,"51.99
(0.52)"
REFERENCES,0.6564748201438849,"73.29
(0.71)"
REFERENCES,0.658273381294964,"82.27
(0.57)"
REFERENCES,0.6600719424460432,"80.48
(0.57)"
REFERENCES,0.6618705035971223,"79.72
(0.67)"
REFERENCES,0.6636690647482014,"88.15
(0.51)"
REFERENCES,0.6654676258992805,"90.81
(0.43)"
REFERENCES,0.6672661870503597,"ProtoNet+FWT
23.77
(0.42)"
REFERENCES,0.6690647482014388,"26.87
(0.43)"
REFERENCES,0.670863309352518,"30.12
(0.46)"
REFERENCES,0.6726618705035972,"38.87
(0.52)"
REFERENCES,0.6744604316546763,"43.78
(0.47)"
REFERENCES,0.6762589928057554,"49.84
(0.51)"
REFERENCES,0.6780575539568345,"67.34
(0.76)"
REFERENCES,0.6798561151079137,"75.74
(0.70)"
REFERENCES,0.6816546762589928,"78.64
(0.57)"
REFERENCES,0.6834532374100719,"72.72
(0.70)"
REFERENCES,0.685251798561151,"85.82
(0.51)"
REFERENCES,0.6870503597122302,"87.17
(0.50)"
REFERENCES,0.6888489208633094,"RelationNet
22.96
(0.88)"
REFERENCES,0.6906474820143885,"26.63
(0.92)"
REFERENCES,0.6924460431654677,"28.45
(1.20)"
REFERENCES,0.6942446043165468,"39.41
(0.58)"
REFERENCES,0.6960431654676259,"41.77
(0.49)"
REFERENCES,0.697841726618705,"49.32
(0.51)"
REFERENCES,0.6996402877697842,"61.31
(0.72)"
REFERENCES,0.7014388489208633,"74.43
(0.66)"
REFERENCES,0.7032374100719424,"74.91
(0.58)"
REFERENCES,0.7050359712230215,"68.99
(0.75)"
REFERENCES,0.7068345323741008,"80.45
(0.64)"
REFERENCES,0.7086330935251799,"85.08
(0.53)"
REFERENCES,0.710431654676259,"RelationNet+FWT
22.74
(0.40)"
REFERENCES,0.7122302158273381,"26.75
(0.41)"
REFERENCES,0.7140287769784173,"27.56
(0.40)"
REFERENCES,0.7158273381294964,"35.54
(0.55)"
REFERENCES,0.7176258992805755,"43.31
(0.51)"
REFERENCES,0.7194244604316546,"46.38
(0.53)"
REFERENCES,0.7212230215827338,"61.16
(0.70)"
REFERENCES,0.7230215827338129,"69.40
(0.64)"
REFERENCES,0.7248201438848921,"73.84
(0.60)"
REFERENCES,0.7266187050359713,"64.91
(0.79)"
REFERENCES,0.7284172661870504,"78.43
(0.59)"
REFERENCES,0.7302158273381295,"81.14
(0.56)"
REFERENCES,0.7320143884892086,"MetaOpt
22.53
(0.91)"
REFERENCES,0.7338129496402878,"25.53
(1.02)"
REFERENCES,0.7356115107913669,"29.35
(0.99)"
REFERENCES,0.737410071942446,"36.28
(0.50)"
REFERENCES,0.7392086330935251,"49.42
(0.60)"
REFERENCES,0.7410071942446043,"54.80
(0.54)"
REFERENCES,0.7428057553956835,"64.44
(0.73)"
REFERENCES,0.7446043165467626,"79.19
(0.62)"
REFERENCES,0.7464028776978417,"83.62
(0.58)"
REFERENCES,0.7482014388489209,"68.41
(0.73)"
REFERENCES,0.75,"82.89
(0.54)"
REFERENCES,0.7517985611510791,"91.76
(0.38)"
REFERENCES,0.7535971223021583,"STARTUP
26.94
(0.94)"
REFERENCES,0.7553956834532374,"33.19
(0.46)"
REFERENCES,0.7571942446043165,"36.91
(0.50)"
REFERENCES,0.7589928057553957,"47.22
(0.61)"
REFERENCES,0.7607913669064749,"58.63
(0.58)"
REFERENCES,0.762589928057554,"64.16
(0.58)"
REFERENCES,0.7643884892086331,"82.29
(0.60)"
REFERENCES,0.7661870503597122,"89.26
(0.43)"
REFERENCES,0.7679856115107914,"91.99
(0.36)"
REFERENCES,0.7697841726618705,"93.02
(0.45)"
REFERENCES,0.7715827338129496,"97.51
(0.21)"
REFERENCES,0.7733812949640287,"98.45
(0.17)"
REFERENCES,0.7751798561151079,"CHEF
24.72
(0.14)"
REFERENCES,0.7769784172661871,"29.71
(0.27)"
REFERENCES,0.7787769784172662,"31.25
(0.20)"
REFERENCES,0.7805755395683454,"41.26
(0.34)"
REFERENCES,0.7823741007194245,"54.30
(0.34)"
REFERENCES,0.7841726618705036,"60.86
(0.18)"
REFERENCES,0.7859712230215827,"74.15
(0.27)"
REFERENCES,0.7877697841726619,"83.31
(0.14)"
REFERENCES,0.789568345323741,"86.55
(0.15)"
REFERENCES,0.7913669064748201,"86.87
(0.27)"
REFERENCES,0.7931654676258992,"94.78
(0.12)"
REFERENCES,0.7949640287769785,"96.77
(0.88)"
REFERENCES,0.7967625899280576,"FT-All
25.97
(0.41)"
REFERENCES,0.7985611510791367,"31.32
(0.45)"
REFERENCES,0.8003597122302158,"35.49
(0.45)"
REFERENCES,0.802158273381295,"48.11
(0.64)"
REFERENCES,0.8039568345323741,"59.31
(0.48)"
REFERENCES,0.8057553956834532,"66.48
(0.56)"
REFERENCES,0.8075539568345323,"79.08
(0.61)"
REFERENCES,0.8093525179856115,"87.64
(0.47)"
REFERENCES,0.8111510791366906,"90.89
(0.36)"
REFERENCES,0.8129496402877698,"89.25
(0.51)"
REFERENCES,0.814748201438849,"95.51
(0.31)"
REFERENCES,0.8165467625899281,"97.68
(0.21)"
REFERENCES,0.8183453237410072,"ATA
24.43
(0.2) –
(–) –
(–)"
REFERENCES,0.8201438848920863,"45.83
(0.3) –
(–) –
(–)"
REFERENCES,0.8219424460431655,"83.75
(0.4) –
(–) –
(–)"
REFERENCES,0.8237410071942446,"90.59
(0.3) –
(–) –
(–)"
REFERENCES,0.8255395683453237,"ConFeSS
27.09
(0.24)"
REFERENCES,0.8273381294964028,"33.57
(0.31)"
REFERENCES,0.829136690647482,"39.02
(0.12)"
REFERENCES,0.8309352517985612,"48.85
(0.29)"
REFERENCES,0.8327338129496403,"60.10
(0.33)"
REFERENCES,0.8345323741007195,"65.34
(0.45)"
REFERENCES,0.8363309352517986,"84.65
(0.38)"
REFERENCES,0.8381294964028777,"90.40
(0.24)"
REFERENCES,0.8399280575539568,"92.66
(0.36)"
REFERENCES,0.841726618705036,"88.88
(0.51)"
REFERENCES,0.8435251798561151,"95.34
(0.48)"
REFERENCES,0.8453237410071942,"97.56
(0.43)"
REFERENCES,0.8471223021582733,"J
FEATURE MASKING AND VC THEORY"
REFERENCES,0.8489208633093526,"The generalization ability of a machine learning model is related to the Vapnik-Chervonenkis (VC)
theory. The VC dimension (Shawe-Taylor & Cristianini, 2000) measures the capacity or complexity
of a machine learning model. For a model family, the VC dimension is the maximum number
of training points that can be shattered by that family. The VC dimension of a set of separating
hyperplanes is d + 1 where d is the feature space dimensionality. Vapnik proved that with probability
1 −η, the test loss (Lte) is upper bounded as"
REFERENCES,0.8507194244604317,Lte ≤Ltr + r
REFERENCES,0.8525179856115108,γ + log(2N) −log( η
REFERENCES,0.85431654676259,"4)
N
,
(10)"
REFERENCES,0.8561151079136691,"where Ltr is the training loss, N is the number of training samples, and γ is the VC dimension. For
better generalization, the goal is to reduce the upper bound, which can be decreased by having more
training samples N. However, when N is small in the few-shot setting, the upper bound increases,
triggering generalization performance to drop. If we reduce γ, we can decrease the upper bound. For
a linear classiﬁer, γ is upper bounded by the number of features. Hence, if we reduce the number of
features, we also reduce the upper bound of γ and subsequently the generalization upper bound. This
is realized with the masking module M(·), which selects a fraction of features before forwarding them"
REFERENCES,0.8579136690647482,Published as a conference paper at ICLR 2022
REFERENCES,0.8597122302158273,Table 11: Table 2 results along with 95 % conﬁdence interval shown in parentheses.
REFERENCES,0.8615107913669064,"5-way 5-shot
5-way 20-shot
Setting
CropDisease
EuroSAT
ISIC
ChestX
CropDisease
EuroSAT
ISIC
ChestX"
REFERENCES,0.8633093525179856,"Full Framework
88.88
(0.51)"
REFERENCES,0.8651079136690647,"84.65
(0.38)"
REFERENCES,0.8669064748201439,"48.85
(0.29)"
REFERENCES,0.8687050359712231,"27.09
(0.24)"
REFERENCES,0.8705035971223022,"95.34
(0.48)"
REFERENCES,0.8723021582733813,"90.40
(0.24)"
REFERENCES,0.8741007194244604,"60.10
(0.33)"
REFERENCES,0.8758992805755396,"33.57
(0.31)"
REFERENCES,0.8776978417266187,"w/o Cont. Learn.
87.26
(0.32)"
REFERENCES,0.8794964028776978,"83.15
(0.21)"
REFERENCES,0.8812949640287769,"47.66
(0.18)"
REFERENCES,0.8830935251798561,"26.06
(0.42)"
REFERENCES,0.8848920863309353,"95.47
(0.53)
↑
88.78
(0.17)"
REFERENCES,0.8866906474820144,"59.96
(0.28)"
REFERENCES,0.8884892086330936,"32.12
(0.28)"
REFERENCES,0.8902877697841727,"w/o FT BB
85.18
(0.42)"
REFERENCES,0.8920863309352518,"83.14
(0.25)"
REFERENCES,0.8938848920863309,"42.25
(0.73)"
REFERENCES,0.89568345323741,"25.76
(0.52)"
REFERENCES,0.8974820143884892,"93.52
(0.56)"
REFERENCES,0.8992805755395683,"89.70
(0.25)"
REFERENCES,0.9010791366906474,"52.61
(0.34)"
REFERENCES,0.9028776978417267,"31.12
(0.24)"
REFERENCES,0.9046762589928058,"w/o Feature Mask
87.57
(0.44)"
REFERENCES,0.9064748201438849,"83.87
(0.26)"
REFERENCES,0.908273381294964,"47.10
(0.13)"
REFERENCES,0.9100719424460432,"26.09
(0.15)"
REFERENCES,0.9118705035971223,"95.49
(0.26)
↑
89.93
(0.20)"
REFERENCES,0.9136690647482014,"61.08
(0.24) ↑
33.20
(0.18)"
REFERENCES,0.9154676258992805,"w/o Ldiv
87.95
(0.24)"
REFERENCES,0.9172661870503597,"84.23
(0.21)"
REFERENCES,0.9190647482014388,"48.62
(0.22)"
REFERENCES,0.920863309352518,"26.92
(0.50)"
REFERENCES,0.9226618705035972,"94.74
(0.34)"
REFERENCES,0.9244604316546763,"89.31
(0.25)"
REFERENCES,0.9262589928057554,"59.20
(0.25)"
REFERENCES,0.9280575539568345,"32.77
(0.09)"
REFERENCES,0.9298561151079137,"w/o Lneg
89.03
(0.12)
↑
84.41
(0.36)"
REFERENCES,0.9316546762589928,"48.04
(0.18)"
REFERENCES,0.9334532374100719,"26.60
(0.30)"
REFERENCES,0.935251798561151,"94.72
(0.17)"
REFERENCES,0.9370503597122302,"90.37
(0.38)"
REFERENCES,0.9388489208633094,"60.14
(0.18) ↑
32.81
(0.34)"
REFERENCES,0.9406474820143885,"w/o Lreg
87.83
(0.30)"
REFERENCES,0.9424460431654677,"83.98
(0.26)"
REFERENCES,0.9442446043165468,"48.34
(0.32)"
REFERENCES,0.9460431654676259,"26.73
(0.24)"
REFERENCES,0.947841726618705,"94.43
(0.22)"
REFERENCES,0.9496402877697842,"90.31
(0.22)"
REFERENCES,0.9514388489208633,"59.83
(0.08)"
REFERENCES,0.9532374100719424,"32.69
(0.12)"
REFERENCES,0.9550359712230215,"Direct Positive
87.15
(0.16)"
REFERENCES,0.9568345323741008,"83.94
(0.15)"
REFERENCES,0.9586330935251799,"47.25
(0.24)"
REFERENCES,0.960431654676259,"26.58
(0.14)"
REFERENCES,0.9622302158273381,"93.65
(0.21)"
REFERENCES,0.9640287769784173,"89.40
(0.11)"
REFERENCES,0.9658273381294964,"59.66
(0.16)"
REFERENCES,0.9676258992805755,"31.92
(0.20)"
REFERENCES,0.9694244604316546,"to the linear classiﬁer. Thus, feature selection has theoretical support for improving generalization
performance in the few-shot setting. Also, empirical results in Fig. 2 (a) show different datasets and
shots selecting different number of features and hence realizing different upper bounds of γ."
REFERENCES,0.9712230215827338,"K
THEORY OF CONTRASTIVE LEARNING AND FEW-SHOT LEARNING"
REFERENCES,0.9730215827338129,"In (Cao et al., 2021), the authors proved the following bound:"
REFERENCES,0.9748201438848921,"Lsup ≤γ0L−
U + γ1s(fk).
(11)"
REFERENCES,0.9766187050359713,"Here, Lsup is the supervised evaluation metric for learned representations. L−
U is the unsupervised
contrastive evaluation metric for true negative samples. s(fk) is the intra-class deviation using the key
encoder fk. γ0 and γ1 are coefﬁcients depending on class distributions. Lsup can be the training loss
of any supervised few-shot meta-learning method which can generalize to novel categories. Since
L−
U upper bounds Lsup, decreasing L−
U amounts to decreasing Lsup. Also, L−
U can be decreased
arbitrarily because it is evaluated only on true negative samples. Hence, contrastive losses can be
useful for learning representations that are effective for few-shot learning."
REFERENCES,0.9784172661870504,"L
THEORY OF DISTANCE-BASED REGULARIZATION FOR FINE-TUNING"
REFERENCES,0.9802158273381295,"In (Gouk et al., 2021), the authors proved that with probability 1 −η, the test loss (Lte) is upper
bounded as"
REFERENCES,0.9820143884892086,"Lte ≤Ltr + κ L
X j=1"
REFERENCES,0.9838129496402878,"DF
j
2BF
j
Qj
i=1
√ni L
Y"
REFERENCES,0.9856115107913669,"j=1
2BF
j
√nj + 3 r"
REFERENCES,0.987410071942446,log(2/η)
M,0.9892086330935251,"2m
.
(12)"
M,0.9910071942446043,"Here, Ltr is the training loss. κ is a coefﬁcient depending on the properties of dataset. m is the
number of training samples. BF
j is the upper bound of the Frobenius norm of weight parameter of
layer j of both pre-trained and ﬁne-tuned model. DF
j is the upper bound of the Frobenius norm of
the difference between weight parameter of layer j of pre-trained and ﬁne-tuned model. nj is the
number of columns in weight parameter of layer j. According to the bound, the generalization gap
between Ltr and Lte decreases if DF
j ’s of all the layers can be decreased. However, minimizing the
weights between pre-trained and ﬁne-tuned model for all layers might be cumbersome. Hence, we
choose to minimize the Frobenius norm of difference in features for our regularization term Lreg."
M,0.9928057553956835,"M
LIMITATIONS OF OUR FRAMEWORK"
M,0.9946043165467626,"Although our framework produces competitive performance on the CDFSL benchmark, it has the
following limitations: (a) In online setting, when target domain samples arrive in a streaming"
M,0.9964028776978417,Published as a conference paper at ICLR 2022
M,0.9982014388489209,"fashion, our method might not be applicable. This is mainly because of the presence of the mask
generator. Even though the mask generator is a small network, it still requires a small batch of
samples for learning the parameters. In the online setting, samples arrive one at a time, and the small
masking network might overﬁt. A workaround to prevent overﬁtting can be selectively updating
only certain parameters during online learning. (b) Another limitation of our framework is the
use of large number of hyperparameters in the adaptation step for weighing the loss functions i.e.
λpos, λneg, λdiv and λreg. For practical few-shot adaptation, it is difﬁcult to set aside sufﬁcient
number of validation samples to tune the optimal hyperparameter conﬁguration. Hence, we just
ﬁxed the hyperparameter values in our experiments. Another possible workaround involves learning
hyperparameters themselves within the framework of multi-task uncertainty (Kendall et al., 2018)."
