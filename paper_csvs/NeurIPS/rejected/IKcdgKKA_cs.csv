Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002036659877800407,"We formulate a stochastic process, FILEX, as a mathematical model of lexicon
1"
ABSTRACT,0.004073319755600814,"entropy in deep learning-based emergent language systems. Defining a model
2"
ABSTRACT,0.006109979633401222,"mathematically allows it to generate clear predictions which can be directly and
3"
ABSTRACT,0.008146639511201629,"decisively tested. We empirically verify across four different environments that
4"
ABSTRACT,0.010183299389002037,"FILEX predicts the correct correlation between hyperparameters (training steps,
5"
ABSTRACT,0.012219959266802444,"lexicon size, learning rate, rollout buffer size, and Gumbel-Softmax temperature)
6"
ABSTRACT,0.014256619144602852,"and the emergent language’s entropy in 20 out of 20 environment-hyperparameter
7"
ABSTRACT,0.016293279022403257,"combinations. Furthermore, our experiments reveal that different environments
8"
ABSTRACT,0.018329938900203666,"show diverse relationships between their hyperparameters and entropy which
9"
ABSTRACT,0.020366598778004074,"demonstrates the need for a model which can make well-defined predictions at a
10"
ABSTRACT,0.02240325865580448,"precise level of granularity.
11"
INTRODUCTION,0.024439918533604887,"1
Introduction
12"
INTRODUCTION,0.026476578411405296,"The methods of deep learning-based emergent language provide a uniquely powerful way to study
13"
INTRODUCTION,0.028513238289205704,"the nature of language and language change. In addressing these topics, some papers hypothesize
14"
INTRODUCTION,0.03054989816700611,"general principles describing emergent language. For example, Resnick et al. [2020] hypothesize a
15"
INTRODUCTION,0.032586558044806514,"predictable relationship exists between compositionality and neural network capacity, and Kharitonov
16"
INTRODUCTION,0.034623217922606926,"et al. [2020] hypothesize a general entropy minimization pressure in deep learning-based emergent
17"
INTRODUCTION,0.03665987780040733,"language. In many cases, these hypotheses are derived from intuitions and stated in natural language;
18"
INTRODUCTION,0.038696537678207736,"this can lead to ambiguous interpretation, inadequate experiments, and ad hoc explanations. To this
19"
INTRODUCTION,0.04073319755600815,"end, we study a general principle of emergent language by proposing a mathematical model which
20"
INTRODUCTION,0.04276985743380855,"generates a testable hypothesis which can be directly evaluated through the empirical studies, akin to
21"
INTRODUCTION,0.04480651731160896,"what we find prototypically in natural science.
22"
INTRODUCTION,0.04684317718940937,"We formulate a stochastic process, FILEX, as a mathematical model of lexicon entropy in deep
23"
INTRODUCTION,0.048879837067209775,"learning-based emergent language systems1 (ELS). We empirically verify across four different
24"
INTRODUCTION,0.05091649694501019,"environments that FILEX predicts the correct correlation between hyperparameters (training steps,
25"
INTRODUCTION,0.05295315682281059,"lexicon size, learning rate, rollout buffer size, and Gumbel-Softmax temperature) and the emergent
26"
INTRODUCTION,0.054989816700611,"language’s entropy in 20 out of 20 environment-hyperparameter combinations.
27"
INTRODUCTION,0.05702647657841141,"There are three primary reasons for using an explicitly defined model for studying a topic like
28"
INTRODUCTION,0.059063136456211814,"emergent language: clarity, testability, and extensibility. A mathematical model yields a clear,
29"
INTRODUCTION,0.06109979633401222,"unambiguous interpretation since its components have precise meanings; this is especially important
30"
INTRODUCTION,0.06313645621181263,"when conveying such concepts in writing. It is easier to test a model than a hypothesis articulated
31"
INTRODUCTION,0.06517311608961303,"in natural language because the model yields clear predictions which can be shown to be accurate
32"
INTRODUCTION,0.06720977596741344,"or inaccurate; as a result, models can also be directly compared to one another. Our experiments
33"
INTRODUCTION,0.06924643584521385,"1Emergent language system or ELS refers to the combination of agents (neural networks), the environment,
and the training procedure used as part of an emergent language experiment."
INTRODUCTION,0.07128309572301425,"reveal that different environments show diverse relationships between their hyperparameters and
34"
INTRODUCTION,0.07331975560081466,"entropy which demonstrates the need for such clarity in making well-defined predictions at a precise
35"
INTRODUCTION,0.07535641547861507,"level of granularity. Finally, mathematical models hypothesize a mechanism for an observed effect
36"
INTRODUCTION,0.07739307535641547,"and not simply the effect itself (with a possibly ad hoc explanation). This is what facilitates their
37"
INTRODUCTION,0.07942973523421588,"extensibility since a multitude of hypotheses can be derived from these mechanisms; furthermore,
38"
INTRODUCTION,0.0814663951120163,"this “mechanical” nature allows future work to build directly on top of the model.
39"
INTRODUCTION,0.0835030549898167,"As mathematical models are seldom used to their full potential in studying emergent language, this
40"
INTRODUCTION,0.0855397148676171,"paper is meant to serve as a reference and starting point for entire methodology of developing and
41"
INTRODUCTION,0.08757637474541752,"testing such models. We articulate our contributions as follows:
42"
INTRODUCTION,0.08961303462321792,"• Defining a mathematical model of lexicon entropy in emergent language systems which we
43"
INTRODUCTION,0.09164969450101833,"demonstrate to be accurate in predicting hyperparameter-entropy correlations.
44"
INTRODUCTION,0.09368635437881874,"• Presenting a case study of defining and empirically evaluating a mathematical model in
45"
INTRODUCTION,0.09572301425661914,"emergent language.
46"
INTRODUCTION,0.09775967413441955,"• Provide a direct, intuitive comparison of the effects of hyperparameters on lexicon entropy
47"
INTRODUCTION,0.09979633401221996,"across different environments.
48"
INTRODUCTION,0.10183299389002037,"We briefly discuss related work in Section 2. In Section 3, we introduce the mathematical model,
49"
INTRODUCTION,0.10386965376782077,"FILEX, as well as the ELSs. Empirical evaluation is presented in Section 4 and discussed in
50"
INTRODUCTION,0.10590631364562118,"Section 5, concluding with Section 6. Code is available at https://example.com/reponame (in
51"
INTRODUCTION,0.1079429735234216,"supplemental material while under review).
52"
RELATED WORK,0.109979633401222,"2
Related work
53"
RELATED WORK,0.1120162932790224,"For a survey of deep learning-based emergent language work, please see Lazaridou and Baroni
54"
RELATED WORK,0.11405295315682282,"[2020]. Contemporary deep learning-based emergent language research often aims at establishing and
55"
RELATED WORK,0.11608961303462322,"refining general principles about emergent language. In large part, these principles can be expressed
56"
RELATED WORK,0.11812627291242363,"as relationships between certain characteristics of the environment or agents (e.g., model capacity
57"
RELATED WORK,0.12016293279022404,"[Resnick et al., 2020], population size [Rita et al., 2022]) and properties of the emergent language
58"
RELATED WORK,0.12219959266802444,"(e.g., compositionality [Resnick et al., 2020, Rodríguez Luna et al., 2020], entropy [Kharitonov et al.,
59"
RELATED WORK,0.12423625254582485,"2020, Chaabouni et al., 2021, Rita et al., 2022], and generalizability [Chaabouni et al., 2020, Guo
60"
RELATED WORK,0.12627291242362526,"et al., 2021, Słowik et al., 2020]). Some of these works [Kharitonov et al., 2020, Khomtchouk and
61"
RELATED WORK,0.12830957230142567,"Sudhakaran, 2018, Resnick et al., 2020] make use of mathematical models to describe parts of the
62"
RELATED WORK,0.13034623217922606,"hypotheses and/or experiments, but these fall short of establishing a clear model which generates a
63"
RELATED WORK,0.13238289205702647,"testable hypothesis which is then evaluated through the empirical studies.
64"
RELATED WORK,0.13441955193482688,"Pre-deep learning emergent language research frequently relied on mathematical models [Skyrms,
65"
RELATED WORK,0.1364562118126273,"2010, Kirby et al., 2015, Brighton et al., 2005], but such models played a different role. Whereas
66"
RELATED WORK,0.1384928716904277,"these models were meant to account for some property of language observed in human language, the
67"
RELATED WORK,0.14052953156822812,"model presented in this paper is accounting for emergent language directly (and human language
68"
RELATED WORK,0.1425661914460285,"only indirectly). Thus, this paper presents a (mathematical) model of a (computational) model which,
69"
RELATED WORK,0.1446028513238289,"in the future, will be used to more directly study human language.
70"
METHODS,0.14663951120162932,"3
Methods
71"
MODEL,0.14867617107942974,"3.1
Model
72"
MODEL,0.15071283095723015,"FILEX (“fixed lexicon stochastic process”) is a mathematical model developed from the Chinese
73"
MODEL,0.15274949083503056,"restaurant process [Blei, 2007, Aldous, 1985], a stochastic process where each element in the sequence
74"
MODEL,0.15478615071283094,"is a stochastic distribution over the positive integers (i.e., a distribution over distributions). The
75"
MODEL,0.15682281059063136,"analogy for the Chinese restaurant process is a restaurant with tables indexed by the natural numbers;
76"
MODEL,0.15885947046843177,"as each customer walks in, they sit at a random table with a probability proportional to the number of
77"
MODEL,0.16089613034623218,"people already at that table. The key property here is that the process is self-reinforcing; tables with
78"
MODEL,0.1629327902240326,"many people are likely to get even more. By analogy to language, the more a word is used the more
79"
MODEL,0.164969450101833,"likely it is to continue to be used. For example, speakers may develop a cognitive preference for it, or
80"
MODEL,0.1670061099796334,"it gets passed along to subsequent generations as a higher rate [Francis et al., 2021].
81"
MODEL,0.1690427698574338,Algorithm 1 FILEX pseudocode
MODEL,0.1710794297352342,"1
alpha: float > 0
2
beta: int > 0
3
N: int > 0
4
S: int > 0
5
6
weights = array(size=S)
7
weights.fill (1 / S)
8
for _ in range(N):
9
W +=
sample_multinomial (W / sum(W), beta) / beta
10
w_copy = weights.copy ()
11
for _ in range(beta ):
# equivalent to normalized
multinomial
12
i = sample_categorical (w_copy / sum(w_copy ))
13
weights[i] += alpha / beta
14
return
weights / sum(weights)"
MODEL,0.17311608961303462,"Formulation
FILEX is defined as a sequence of stochastic vectors indexed by N ∈N+ given by:
82"
MODEL,0.17515274949083504,"FILEX(α, β, S, N) =
w(N)"
MODEL,0.17718940936863545,"∥w(N)∥1
(1)"
MODEL,0.17922606924643583,"w(n+1) = w(n) + αx(n) β
(2)"
MODEL,0.18126272912423624,"x(n) ∼Multi

β,
w(n)"
MODEL,0.18329938900203666,"∥w(n)∥1 
(3)"
MODEL,0.18533604887983707,w(1) = 1
MODEL,0.18737270875763748,"S · (1, 1, . . . , 1) ∈RS
(4)"
MODEL,0.1894093686354379,"where w(n) is a vector of weights, α ∈R>0 controls the weight update magnitude, β ∈N+ controls
83"
MODEL,0.19144602851323828,"the variance of the updates, S ∈N+ is the size of the weight vector (i.e., lexicon), and Multi(k, p) is
84"
MODEL,0.1934826883910387,"a k-trial multinomial distribution with probabilities p ∈RS. The pseudocode describing FILEX is
85"
MODEL,0.1955193482688391,"given in Algorithm 1. Conceptually, the process starts with an S-element array of weights initialized
86"
MODEL,0.1975560081466395,"to 1/S. At each iteration we draw from a β-trial multinomial distribution parameterized by the
87"
MODEL,0.19959266802443992,"normalized weights.2 This multinomial sample is multiplied by α/β and added to the weights so that
88"
MODEL,0.20162932790224034,"the update magnitude is α. This proceeds N times. Since the sequence elements are the normalized
89"
MODEL,0.20366598778004075,"weights, the elements are themselves probability distributions; thus, FILEX is technically a sequence
90"
MODEL,0.20570264765784113,"of distributions over distributions.
91"
MODEL,0.20773930753564154,"The two key differences between FILEX and the Chinese restaurant process are the hyperparameters
92"
MODEL,0.20977596741344195,"S and β.3 FILEX has a fixed number of parameters so as to match the fact that the agents in the ELS
93"
MODEL,0.21181262729124237,"have a fixed-size bottleneck layer, that is, a fixed lexicon. Secondly, β is introduced to modulate
94"
MODEL,0.21384928716904278,"the smoothness of parameter updates. It is closely connected to the fact that certain RL algorithms
95"
MODEL,0.2158859470468432,"like PPO accumulate a buffer of data points from the environment with the same parameters before
96"
MODEL,0.21792260692464357,"performing gradient descent.
97"
ENVIRONMENTS,0.219959266802444,"3.2
Environments
98"
ENVIRONMENTS,0.2219959266802444,"To evaluate , we use four different reinforcement learning environments in our experiments. These
99"
ENVIRONMENTS,0.2240325865580448,"are inhabited by two deep learning-based agents: (1) a sender agent which receives an observation
100"
ENVIRONMENTS,0.22606924643584522,"and produces a message and (2) a receiver agent which receives a message (and possibly additional
101"
ENVIRONMENTS,0.22810590631364563,"observation) and takes an action. The agent architecture and optimization are detailed Section 3.3.
102"
ENVIRONMENTS,0.23014256619144602,"NODYN
The “no dynamics” environment is a proof-of-concept environment which is not intended
103"
ENVIRONMENTS,0.23217922606924643,"to be realistic but rather to match as closely as possible the simplifying assumptions which FILEX
104"
ENVIRONMENTS,0.23421588594704684,"2The β-trial multinomial sample is written as β i.i.d. samples from a categorical distribution to draw parallels
to PPO in Algorithm 2.
3Note that α in FILEX is actually equivalent to the inverse of α in the Chinese restaurant process."
ENVIRONMENTS,0.23625254582484725,"makes while keeping the same neural architecture in the environments below. As the name suggests,
105"
ENVIRONMENTS,0.23828920570264767,"the primary simplification in this environment is that there are trivial dynamics, that is, every episode
106"
ENVIRONMENTS,0.24032586558044808,"immediately ends with reward of 1 no matter what the sender or receiver do. The sender input and
107"
ENVIRONMENTS,0.24236252545824846,"receiver output are identical to those of NAV, defined below. Just as FILEX assumes that every
108"
ENVIRONMENTS,0.24439918533604887,"instance of word use is reinforced, this process reinforces every message which the sender produces.
109"
ENVIRONMENTS,0.24643584521384929,"RECON
The reconstruction game [Chaabouni et al., 2020], in the general case, mimics a discrete
110"
ENVIRONMENTS,0.2484725050916497,"autoencoder: the input value is translated into a discrete message by the sender, and the receiver
111"
ENVIRONMENTS,0.2505091649694501,"tries to output the original input based on the message. For a given episode, the sender observes
112"
ENVIRONMENTS,0.2525458248472505,"x ∼U(−1, 1) and produces a message; the receiver’s action is a real number ˆx, yielding a reward
113"
ENVIRONMENTS,0.2545824847250509,"(x −ˆx)2.
114"
ENVIRONMENTS,0.25661914460285135,"SIG
The signaling game environment comes from Lewis [1970] and has been frequently used
115"
ENVIRONMENTS,0.25865580448065173,"in the literature [Lazaridou et al., 2017, Bouchacourt and Baroni, 2018]. In this setup, the data is
116"
ENVIRONMENTS,0.2606924643584521,"partitioned into a fixed number of discrete classes. The sender observes a datum from one of the
117"
ENVIRONMENTS,0.26272912423625255,"classes and produces a message; the receivers observes this message, the sender’s datum, and data
118"
ENVIRONMENTS,0.26476578411405294,"points from other classes (i.e., “distractors”). The reward for the environment is 1 if the receiver
119"
ENVIRONMENTS,0.2668024439918534,"correctly identifies the sender’s datum among the distractors and 0 otherwise.
120"
ENVIRONMENTS,0.26883910386965376,"To eliminate the potential confounding factors from using natural inputs (e.g., image embeddings
121"
ENVIRONMENTS,0.2708757637474542,"[Lazaridou et al., 2017]), we use a synthetic dataset. For an n-dimensional signaling game, we have
122"
ENVIRONMENTS,0.2729124236252546,"2n classes. Each class is represented by an isotropic multivariate normal distribution with mean
123"
ENVIRONMENTS,0.27494908350305497,"(µ1, µ2, . . . , µn) where µi ∈{−3, 3}. Observations of a given are samples of its corresponding
124"
ENVIRONMENTS,0.2769857433808554,"distribution. For example, in the 2-dimensional game, the 4 classes would be represented by the
125"
ENVIRONMENTS,0.2790224032586558,"distributions: N((−3, −3), I2), N((3, −3), I2), N((−3, 3), I2), and N((3, 3), I2) (we use a 5-
126"
ENVIRONMENTS,0.28105906313645623,"dimensional signaling game for our experiments with 32 classes). The motivation for this setup is
127"
ENVIRONMENTS,0.2830957230142566,"minimal need for feature extraction while still using real-valued, stochastic inputs.
128"
ENVIRONMENTS,0.285132382892057,"NAV
For a multi-step environment, we use a 2-dimensional, obstacle-free navigation task. The
129"
ENVIRONMENTS,0.28716904276985744,"sender agent observes the (x, y) position of a receiver and produces a message; the receiver moves
130"
ENVIRONMENTS,0.2892057026476578,"by producing an (x, y) vector. For a given episode, the receiver is initialized uniformly at random
131"
ENVIRONMENTS,0.29124236252545826,"within a circle and must navigate towards a smaller circular goal region at the center. The agents are
132"
ENVIRONMENTS,0.29327902240325865,"rewarded for both reaching the goal and for moving towards the center. An illustration is provided in
133"
ENVIRONMENTS,0.2953156822810591,"Appendix A. The receiver’s location and action are continuous variables.
134"
AGENTS,0.2973523421588595,"3.3
Agents
135"
AGENTS,0.29938900203665986,"Architecture
Our architecture comprises two agents, conceptually speaking, but in practice, they
136"
AGENTS,0.3014256619144603,"are a single neural network. The sender and receiver are randomly initialized at the start of training,
137"
AGENTS,0.3034623217922607,"are trained together, and are tested together. The sender itself is a 2-layer perceptron with tanh
138"
AGENTS,0.3054989816700611,"activations. The sender’s input is environment-dependent. The output of the second layer is passed to
139"
AGENTS,0.3075356415478615,"a Gumbel-Softmax bottleneck layer [Maddison et al., 2017, Jang et al., 2017] which enables learning a
140"
AGENTS,0.3095723014256619,"discrete, one-hot representation.4 The activations of this layer can be thought of as the words forming
141"
AGENTS,0.31160896130346233,"the lexicon of the emergent language. Messages consist only of a single one-hot vector (word) passed
142"
AGENTS,0.3136456211812627,"from sender to receiver. At evaluation time, the bottleneck layer functions deterministically as an
143"
AGENTS,0.31568228105906315,"argmax layer, emitting one-hot vectors. The receiver is a 1-layer perceptron which takes the output of
144"
AGENTS,0.31771894093686354,"the Gumbel-Softmax layer as input. The receiver’s output is environment-dependent. An illustration
145"
AGENTS,0.319755600814664,"and precise specification are provided in Appendices A and B.
146"
AGENTS,0.32179226069246436,"Optimization
Although only our NAV environment involves multi-step episodes, using a full
147"
AGENTS,0.32382892057026474,"reinforcement learning algorithm across all environments benefits comparability and extensibility
148"
AGENTS,0.3258655804480652,"in future work. Specifically, we use proximal policy optimization (PPO) [Schulman et al., 2017]
149"
AGENTS,0.32790224032586557,"paired with Adam [Kingma and Ba, 2015] to optimize the neural networks. PPO is widely used RL
150"
AGENTS,0.329938900203666,"algorithm which selected primarily for its stability (e.g., training almost always converges, minimal
151"
AGENTS,0.3319755600814664,"hyperparameter tuning); attempts to train with “vanilla” advantage actor critic did not consistently
152"
AGENTS,0.3340122199592668,"4Using a Gumbel-Softmax bottleneck layer allows for end-to-end backpropagation, making optimization
faster and more consistent than using a backpropagation-free method like REINFORCE [Kharitonov et al., 2020,
Williams, 1992]. Nevertheless, future work may want to use REINFORCE for its more realistic assumptions
about communication."
AGENTS,0.3360488798370672,Algorithm 2 PPO pseudocode
AGENTS,0.3380855397148676,"1
n_updates: int
>= 0
2
buffer_size: int > 0
3
4
for _ in range(n_updates ):
# outer
loop
5
rollout_buffer = []
6
for _ in range(buffer_size ):
# inner
loop
7
episode = run_episode(model , environment)
8
rollout_buffer .append(episode)
9
update_parameters (model , rollout_buffer )"
AGENTS,0.34012219959266804,"converge. We use the PPO implementation of Stable Baselines 3 (MIT license) built on PyTorch
153"
AGENTS,0.3421588594704684,"(BSD license) [Raffin et al., 2019, Paszke et al., 2019].
154"
AGENTS,0.34419551934826886,"One relevant characteristic of PPO and similar algorithms is that in their training they contain an
155"
AGENTS,0.34623217922606925,"inner and outer loop analogous to FILEX (Algorithm 1); this is illustrated in Algorithm 2. The (main)
156"
AGENTS,0.34826883910386963,"outer loop consists of two steps: the inner loop which populates a rollout buffer with “experience”
157"
AGENTS,0.35030549898167007,"from the environment and the updating of parameters based on that buffer. What is important to note
158"
AGENTS,0.35234215885947046,"is that the buffer is populated with data from the same model parameters, and it is not until after this
159"
AGENTS,0.3543788187372709,"that model parameters change.
160"
HYPOTHESIS,0.3564154786150713,"3.4
Hypothesis
161"
HYPOTHESIS,0.35845213849287166,"Here we state the hypothesis used to evaluate FILEX. The sign of hyperparameter-entropy correlation
162"
HYPOTHESIS,0.3604887983706721,"observed in FILEX will be the same as what we observe for a corresponding hyperparameter in the
163"
HYPOTHESIS,0.3625254582484725,"ELSs. We can state this more formally as: for each pair of corresponding hyperparameters (h, h′) in
164"
HYPOTHESIS,0.3645621181262729,"FILEX and an ELS respectively,
165"
HYPOTHESIS,0.3665987780040733,"sgn(corr(D)) = sgn(corr(D′))
(5)
D = {(x, H(y)) | x ∈Xh, y ∼FILEXh=x}
(6)"
HYPOTHESIS,0.36863543788187375,"D′ = {(x, H(y)) | x ∈Xh′, y ∼ELSh′=x}
(7)"
HYPOTHESIS,0.37067209775967414,"H(y) = − S
X"
HYPOTHESIS,0.3727087576374745,"i=1
yi log2 yi
(8)"
HYPOTHESIS,0.37474541751527496,"where corr(·) is the Kendall rank correlation coefficient (τ) [Kendall, 1938], FILEXh=x is the distri-
166"
HYPOTHESIS,0.37678207739307534,"bution over frequency vectors yielded by the model for hyperparameter h set to x (assume likewise
167"
HYPOTHESIS,0.3788187372708758,"for ELSh′=x), H is Shannon entropy, and Xh is the set of experimental values for hyperparameter
168"
HYPOTHESIS,0.38085539714867617,"h. A “sample” from an ELS consists of training the agents in the environment, and estimating word
169"
HYPOTHESIS,0.38289205702647655,"frequencies by collecting the sender’s messages over a random sample of inputs. Accordingly, our
170"
HYPOTHESIS,0.384928716904277,"null hypothesis is that FILEX does not meaningfully correspond to the ELSs, and thus the signs of
171"
HYPOTHESIS,0.3869653767820774,"correlation would be expected to match with a probability 0.5.
172"
HYPOTHESIS,0.3890020366598778,"We intentionally formulate our hypothesis at this level of granularity: equality of direction (sign)
173"
HYPOTHESIS,0.3910386965376782,"of correlation rather stronger claims such as raw correlation: |corr(D) −corr(D′)| < ϵ or mean
174"
HYPOTHESIS,0.39307535641547864,squared error: 1/|X| · P
HYPOTHESIS,0.395112016293279,"x∈X(D(x) −D′(x))2. We select this level of direction of correlation for a
175"
HYPOTHESIS,0.3971486761710794,"few reasons. The level of simplicity of FILEX compared to the ELSs means that the unaccounted for
176"
HYPOTHESIS,0.39918533604887985,"factors would make supporting stronger hypotheses too difficult; furthermore, even if the hypothesis
177"
HYPOTHESIS,0.40122199592668023,"were defended, it would be less widely applicable for the same reasons. Additionally, the current
178"
HYPOTHESIS,0.40325865580448067,"literature tends to speak of the general principles of emergent language at the level of “relationships”
179"
HYPOTHESIS,0.40529531568228105,"and “effects” rather than exact numeric approximations [Kharitonov et al., 2020, Resnick et al., 2020].
180"
HYPOTHESIS,0.4073319755600815,"Corresponding Hyperparameters
A key component of the hypothesis is the correspondence of
181"
HYPOTHESIS,0.4093686354378819,"hyperparameters of the ELSs with those of FILEX. These correspondences are the foundation for
182"
HYPOTHESIS,0.41140529531568226,"applying reasoning about FILEX to the ELSs; accordingly, they also determine how the model will be
183"
HYPOTHESIS,0.4134419551934827,"empirically tested. We present five pairs of corresponding environment-agnostic hyperparameters in
184"
HYPOTHESIS,0.4154786150712831,"Table 1. Although environment-specific hyperparameters can easily correspond with those of FILEX
185"
HYPOTHESIS,0.4175152749490835,"we chose the agnostic for ease of experimentation and comparison.
186"
HYPOTHESIS,0.4195519348268839,Table 1: Corresponding hyperparameters in the ELSs and FILEX.
HYPOTHESIS,0.4215885947046843,"ELS
FILEX"
HYPOTHESIS,0.42362525458248473,"Time steps
N
Lexicon size
S
Learning rate
α
Buffer size
β
Temperature
β"
HYPOTHESIS,0.4256619144602851,Table 2: Kendall’s τ’s for various configurations. All values have a significance of p ≤0.01.
HYPOTHESIS,0.42769857433808556,"Environment
Time Steps
Lexicon Size
Learning Rate
Buffer Size
Temperature"
HYPOTHESIS,0.42973523421588594,"FILEX
−0.53
+0.67
−0.87
+0.93
+0.93
NODYN
−0.81
+0.12
−0.74
+0.07
+0.58
RECON
−0.17
+0.93
−0.35
+0.84
+0.68
SIG
−0.49
+0.15
−0.16
+0.30
+0.49
NAV
−0.81
+0.36
−0.84
+0.20
+0.68"
HYPOTHESIS,0.4317718940936864,"To identify these correspondences, it is important to understand the intuitive similarities between the
187"
HYPOTHESIS,0.43380855397148677,"ELSs and FILEX. Firstly, the weights of FILEX correspond the learned likelihood with which a given
188"
HYPOTHESIS,0.43584521384928715,"bottleneck unit is used in the ELS; in turn, both of these correspond to the frequency with which a
189"
HYPOTHESIS,0.4378818737270876,"word is used in a language. Each iteration of FILEX’s outer loop is analogous to a whole cycle in
190"
HYPOTHESIS,0.439918533604888,"the ELS of simulating episodes in the environment, receiving the rewards, and performing gradient
191"
HYPOTHESIS,0.4419551934826884,"descent with respect to the rewards (compare Algorithms 1 and 2).
192"
HYPOTHESIS,0.4439918533604888,"Based on this analogy, we can explain the corresponding hyperparameters as follows. N corresponds
193"
HYPOTHESIS,0.4460285132382892,"the number of parameter updates taken throughout the course of training the ELS (i.e., the outer loop
194"
HYPOTHESIS,0.4480651731160896,"of PPO). S corresponds the size of the bottleneck layer in the ELS. α corresponds to the learning
195"
HYPOTHESIS,0.45010183299389,"rate (i.e., magnitude of parameter updates) in the ELS. The ELS has two analogs of β. First, β
196"
HYPOTHESIS,0.45213849287169044,"corresponds to the rollout buffer size of PPO because both control the number of iterations of the
197"
HYPOTHESIS,0.45417515274949083,"inner loop of training where episodes are collected before updating the weights. Second, β, more
198"
HYPOTHESIS,0.45621181262729127,"generally, control how smooth the updates to FILEX’s weights are which makes it analogous to the
199"
HYPOTHESIS,0.45824847250509165,"temperature of the Gumbel-Softmax distribution in the ELS since a higher temperature results in
200"
HYPOTHESIS,0.46028513238289204,"smoother updates to the bottleneck’s parameters.
201"
EXPERIMENTS,0.4623217922606925,"4
Experiments
202"
EXPERIMENTS,0.46435845213849286,"Our experiments consist of comparing the correlation between the hyperparameters of FILEX and
203"
EXPERIMENTS,0.4663951120162933,"the ELSs and the Shannon entropy of lexicon at the end of training. The entropy for the ELSs is
204"
EXPERIMENTS,0.4684317718940937,"calculated based on the bottleneck unit (word) frequencies gathered by sampling from the sender’s
205"
EXPERIMENTS,0.47046843177189407,"input distribution. To gather data for FILEX, we run a Rust implementation of a sampling algorithm.
206"
EXPERIMENTS,0.4725050916496945,"Each experiment consists of a logarithmic sweep of a hyperparameter plotted against the entropy
207"
EXPERIMENTS,0.4745417515274949,"yielded by those hyperparameters (see Appendix B for details).
208"
EXPERIMENTS,0.47657841140529533,"Each point in the resulting scatter plots corresponds to an independent run of the model or ELS
209"
EXPERIMENTS,0.4786150712830957,"with the hyperparameter on the x-axis and entropy on the y-axis. The plots also include a Gaussian
210"
EXPERIMENTS,0.48065173116089616,"convolution of the data points (the solid line) to better illustrate the general trend of the data. The
211"
EXPERIMENTS,0.48268839103869654,"plots are presented in Figure 1 with the rank correlation coefficients in Table 2.
212"
DISCUSSION,0.4847250509164969,"5
Discussion
213"
MODEL EVALUATION,0.48676171079429736,"5.1
Model evaluation
214"
MODEL EVALUATION,0.48879837067209775,"Looking at the signs of correlations shows that FILEX makes the correct prediction 20 out of 20 times.
215"
MODEL EVALUATION,0.4908350305498982,"Given a simple one-sided binomial test, the empirical data rejects the null hypothesis at p < 0.001.
216"
MODEL EVALUATION,0.49287169042769857,"Time Steps
Lexicon Size
Learning Rate
Buffer Size
Temperature FILEX"
MODEL EVALUATION,0.49490835030549896,": 0.53
: +0.67
: 0.87
: +0.93
: +0.93 NODYN"
MODEL EVALUATION,0.4969450101832994,": 0.81
: +0.12
: 0.74
: +0.07
: +0.58 RECON"
MODEL EVALUATION,0.4989816700610998,": 0.17
: +0.93
: 0.35
: +0.84
: +0.68 SIG"
MODEL EVALUATION,0.5010183299389002,": 0.49
: +0.15
: 0.16
: +0.30
: +0.49 NAV"
MODEL EVALUATION,0.5030549898167006,": 0.81
: +0.36
: 0.84
: +0.20
: +0.68"
MODEL EVALUATION,0.505091649694501,"Figure 1: Plots of hyperparameters (x-axis, log scale) vs. entropy (y-axis) . Each row corresponds to
a particular environment. Each column corresponds to a particular hyperparameter. All y-axes are on
the same scale with the dashed lines representing min/max entropy. The points are individual runs
and the lines are a Gaussian convolution of the points."
MODEL EVALUATION,0.5071283095723014,"Although this number drops to 15 out of 20 if we require |τ| ≥0.2, the binomial test rejects the null
217"
MODEL EVALUATION,0.5091649694501018,"hypothesis with p = 0.02 for this stronger hypothesis.
218"
MODEL EVALUATION,0.5112016293279023,"Though the directions of correlations predicted by FILEX are correct, looking at the plots show that
219"
MODEL EVALUATION,0.5132382892057027,"ELSs do not always demonstrate the monotonicity predicted by the model. This is especially evident
220"
MODEL EVALUATION,0.515274949083503,"in Time Steps for RECON: moving left-to-right, the plot follows a similar path to the other environment
221"
MODEL EVALUATION,0.5173116089613035,"and FILEX at first but then diverges halfway through with increasing entropy. A possible explanation
222"
MODEL EVALUATION,0.5193482688391039,"of this is that RECON allows learning new, useful words more easily than SIG or NAV, meaning that
223"
MODEL EVALUATION,0.5213849287169042,"additional training can lead to further improvement. The conclusion we draw from these plots is that
224"
MODEL EVALUATION,0.5234215885947047,"FILEX correctly predicts a sort of baseline correlation between the hyperparameters and entropy.
225"
MODEL EVALUATION,0.5254582484725051,"Other works, Kharitonov et al. [2020], Chaabouni et al. [2021] for example, find similar correlations
226"
MODEL EVALUATION,0.5274949083503055,"between entropy and bottleneck temperature. Nevertheless, this correlation can be overridden by the
227"
MODEL EVALUATION,0.5295315682281059,"specifics of the environment.
228"
ENVIRONMENT VARIABILITY,0.5315682281059063,"5.2
Environment variability
229"
ENVIRONMENT VARIABILITY,0.5336048879837068,"When looking beyond just the direction of correlation at the slopes and shapes of the curves, the four
230"
ENVIRONMENT VARIABILITY,0.5356415478615071,"ELSs all present unique set of relationships between entropy their hyperparameters. This implies
231"
ENVIRONMENT VARIABILITY,0.5376782077393075,"that none of these environments are reducible to each other, that is, we cannot make observations
232"
ENVIRONMENT VARIABILITY,0.539714867617108,"about one environment and automatically assume they apply to other environments. Certainly this
233"
ENVIRONMENT VARIABILITY,0.5417515274949084,"makes an researcher’s task harder as learning general principles would not be possible from a single
234"
ENVIRONMENT VARIABILITY,0.5437881873727087,"environment. Furthermore, there is a sensitivity to hyperparameters within a given environment,
235"
ENVIRONMENT VARIABILITY,0.5458248472505092,"which would imply that discovering general principles within single environment could not be done
236"
ENVIRONMENT VARIABILITY,0.5478615071283096,"with just a single set of hyperparameters.
237"
ENVIRONMENT VARIABILITY,0.5498981670061099,"Although this diversity in behavior makes modeling it more difficult, it also shows the importance
238"
ENVIRONMENT VARIABILITY,0.5519348268839104,"of precision we get from a mathematical model. For example, say RECON has not been empirically
239"
ENVIRONMENT VARIABILITY,0.5539714867617108,"tested and we wanted to predict the lexicon size-entropy relationship in RECON. It is the case that
240"
ENVIRONMENT VARIABILITY,0.5560081466395111,"we could simply observe the positive correlations in the other environments and predict the same
241"
ENVIRONMENT VARIABILITY,0.5580448065173116,"RECON, but we could easily over-extrapolate and predict a relatively shallow slope when RECON’s
242"
ENVIRONMENT VARIABILITY,0.560081466395112,"slope is relatively steep. What this paper’s model, hypothesis, and evaluation offer in this situation is
243"
ENVIRONMENT VARIABILITY,0.5621181262729125,"not a more detailed prediction but a “prepackaged” prediction which is precisely stated and supported
244"
ENVIRONMENT VARIABILITY,0.5641547861507128,"by data.
245"
APPLICATIONS TO FUTURE WORK,0.5661914460285132,"5.3
Applications to future work
246"
APPLICATIONS TO FUTURE WORK,0.5682281059063137,"There are two primary ways in which FILEX can be applied in future research. First, the model
247"
APPLICATIONS TO FUTURE WORK,0.570264765784114,"can be applied to and tested against further phenomena in emergent language (i.e., it is extensible).
248"
APPLICATIONS TO FUTURE WORK,0.5723014256619144,"The fact that it is formulated mathematically means that it does not just predict correlations but
249"
APPLICATIONS TO FUTURE WORK,0.5743380855397149,"mechanisms which account for the correlations. For example, FILEX’s β hyperparameter was
250"
APPLICATIONS TO FUTURE WORK,0.5763747454175153,"designed to account for Buffer Size and the Temperature experiment was conducted after the fact. The
251"
APPLICATIONS TO FUTURE WORK,0.5784114052953157,"fact that FILEX describes both Buffer Size and Temperature with the same hyperparameter suggests
252"
APPLICATIONS TO FUTURE WORK,0.5804480651731161,"that similar mechanisms account for their positive correlations with entropy. This statement about
253"
APPLICATIONS TO FUTURE WORK,0.5824847250509165,"similar mechanisms, on the other hand, is not present set of one-off hypotheses about hyperparameter-
254"
APPLICATIONS TO FUTURE WORK,0.5845213849287169,"entropy correlations derived from intuition. Second, FILEX and accompanying experiments provide
255"
APPLICATIONS TO FUTURE WORK,0.5865580448065173,"an easy way for future research to discover confounding factors in their experiments. For example,
256"
APPLICATIONS TO FUTURE WORK,0.5885947046843177,"an experiment might show that entropy decreases as rewards are scaled up, yet FILEX would suggest
257"
APPLICATIONS TO FUTURE WORK,0.5906313645621182,"that this might be equivalent to simply increasing the learning rate rather than being its own unique
258"
APPLICATIONS TO FUTURE WORK,0.5926680244399185,"cause of the effect on entropy.
259"
METHODOLOGICAL DIFFICULTIES,0.594704684317719,"5.4
Methodological difficulties
260"
METHODOLOGICAL DIFFICULTIES,0.5967413441955194,"The greatest challenge in the methodology of this work is not the formulation of the model but rather
261"
METHODOLOGICAL DIFFICULTIES,0.5987780040733197,"evaluating the quality of the model. In part, this is on account of a lack of established baseline
262"
METHODOLOGICAL DIFFICULTIES,0.6008146639511202,"model—comparative analysis (“which is better?”) is significantly easier than absolute analysis (“how
263"
METHODOLOGICAL DIFFICULTIES,0.6028513238289206,"good is this?”) yet requires an adequate baseline to compare against. But more significantly, the
264"
METHODOLOGICAL DIFFICULTIES,0.604887983706721,"granularity of experimentation is a design decision with no obvious answer.
265"
METHODOLOGICAL DIFFICULTIES,0.6069246435845214,"For example, merely comparing the signs of rank correlations is very coarse-grained as it makes
266"
METHODOLOGICAL DIFFICULTIES,0.6089613034623218,"minimal assumptions about the data (e.g., linearity, absence of outliers) and captures very little
267"
METHODOLOGICAL DIFFICULTIES,0.6109979633401222,"information about the data. Naturally, it is easier to apply such an analysis, and as mentioned before,
268"
METHODOLOGICAL DIFFICULTIES,0.6130346232179226,"researcher typically phrase hypotheses in terms of such correlations, but it can only offer minimal
269"
METHODOLOGICAL DIFFICULTIES,0.615071283095723,"support for applicability of the model to the actual system. On the other hand, evaluating the model’s
270"
METHODOLOGICAL DIFFICULTIES,0.6171079429735234,"ability to predict exact behavior of the system (e.g., measuring mean squared error of the model’s
271"
METHODOLOGICAL DIFFICULTIES,0.6191446028513238,"predictions) can establish a more precise link between model and system but might miss more general
272"
METHODOLOGICAL DIFFICULTIES,0.6211812627291242,"but important similarities. For example, Lexicon Size for FILEX and NAV might show similar trends,
273"
METHODOLOGICAL DIFFICULTIES,0.6232179226069247,"but be different by a constant, yielding a high mean squared error.
274"
METHODOLOGICAL DIFFICULTIES,0.6252545824847251,"A subtle but significant methodological difficulty is the selection of hyperparameters. In RECON’s
275"
METHODOLOGICAL DIFFICULTIES,0.6272912423625254,"Time Steps plot, it is easy to see that changing the range of hyperparameters could easily yield either
276"
METHODOLOGICAL DIFFICULTIES,0.6293279022403259,"a positive or a negative correlation when in reality there are both. To a certain extent, this can be
277"
METHODOLOGICAL DIFFICULTIES,0.6313645621181263,"resolved be choosing a “reasonable” range of hyperparameters based on values are typically, but this
278"
METHODOLOGICAL DIFFICULTIES,0.6334012219959266,"is of little help to selection of FILEX’s hyperparameters as there is no “typical usage.” For example,
279"
METHODOLOGICAL DIFFICULTIES,0.6354378818737271,"FILEX for β = 1 and β = 100 yield significantly different distributions, but there is no obvious
280"
METHODOLOGICAL DIFFICULTIES,0.6374745417515275,"a priori reason to say that one value of β should be preferred over the other for comparing to the
281"
METHODOLOGICAL DIFFICULTIES,0.639511201629328,"ELSs. Although additional hyperparameters increase the range of phenomena which the model can
282"
METHODOLOGICAL DIFFICULTIES,0.6415478615071283,"account for, the additional degrees of freedom can weaken the model’s predictions by introducing
283"
METHODOLOGICAL DIFFICULTIES,0.6435845213849287,"confounding variables (cf. overparameterization).
284"
METHODOLOGICAL DIFFICULTIES,0.6456211812627292,"One of the primary contributions of this work is to serve as a case study and example of working
285"
METHODOLOGICAL DIFFICULTIES,0.6476578411405295,"with explicitly defined models in studying deep learning-based emergent language. Thus, this paper
286"
METHODOLOGICAL DIFFICULTIES,0.6496945010183299,"is starting point for future work to improve upon. One of the most important improvements would be
287"
METHODOLOGICAL DIFFICULTIES,0.6517311608961304,"finding a more rigorous way to select “reasonable” experimental hyperparameters. Additionally, it
288"
METHODOLOGICAL DIFFICULTIES,0.6537678207739308,"would be better to develop the hypothesis and experimental in full before performing any evaluation;
289"
METHODOLOGICAL DIFFICULTIES,0.6558044806517311,"the process was somewhat iterative in this paper.
290"
CONCLUSION,0.6578411405295316,"6
Conclusion
291"
CONCLUSION,0.659877800407332,"We have presented FILEX as a mathematical model of lexicon entropy in deep learning-based
292"
CONCLUSION,0.6619144602851323,"emergent language systems and demonstrated that, at the level of correlations, it accurately predicts
293"
CONCLUSION,0.6639511201629328,"the behavior of our emergent language environments. Opting for a mathematical model possesses
294"
CONCLUSION,0.6659877800407332,"the benefits of having a clear interpretation, making testable predictions, and being reused for new
295"
CONCLUSION,0.6680244399185336,"predictions in future studies. Although the model’s hypothesis was testable, the process is not free
296"
CONCLUSION,0.670061099796334,"from non-trivial design decisions which affect the quality of evaluation. Nevertheless, this paper
297"
CONCLUSION,0.6720977596741344,"serves as starting point and example of how more rigorous models can be applied to the study of
298"
CONCLUSION,0.6741344195519349,"emergent language.
299"
REFERENCES,0.6761710794297352,"References
300"
REFERENCES,0.6782077393075356,"David J. Aldous. Exchangeability and related topics. In P. L. Hennequin, editor, École d’Été de
301"
REFERENCES,0.6802443991853361,"Probabilités de Saint-Flour XIII — 1983, pages 1–198, Berlin, Heidelberg, 1985. Springer Berlin
302"
REFERENCES,0.6822810590631364,"Heidelberg. ISBN 978-3-540-39316-0.
303"
REFERENCES,0.6843177189409368,"David Blei. The chinese restaurant process, 2007. URL https://www.cs.princeton.edu/
304"
REFERENCES,0.6863543788187373,"courses/archive/fall07/cos597C/scribe/20070921.pdf.
305"
REFERENCES,0.6883910386965377,"Diane Bouchacourt and Marco Baroni. How agents see things: On visual representations in an
306"
REFERENCES,0.6904276985743381,"emergent language game. In Proceedings of the 2018 Conference on Empirical Methods in Natural
307"
REFERENCES,0.6924643584521385,"Language Processing, page 981–985, Brussels, Belgium, Oct 2018. Association for Computational
308"
REFERENCES,0.6945010183299389,"Linguistics. doi: 10.18653/v1/D18-1119. URL https://aclanthology.org/D18-1119.
309"
REFERENCES,0.6965376782077393,"Henry Brighton, Kenny Smith, and Simon Kirby. Language as an evolutionary system. Physics of
310"
REFERENCES,0.6985743380855397,"Life Reviews, 2:177–226, 2005.
311"
REFERENCES,0.7006109979633401,"Rahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux, and Marco Baroni.
312"
REFERENCES,0.7026476578411406,"Compositionality and generalization in emergent languages. Proceedings of the 58th Annual
313"
REFERENCES,0.7046843177189409,"Meeting of the Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.
314"
REFERENCES,0.7067209775967414,"407. URL http://dx.doi.org/10.18653/v1/2020.acl-main.407.
315"
REFERENCES,0.7087576374745418,"Rahma Chaabouni, Eugene Kharitonov, Emmanuel Dupoux, and Marco Baroni. Communicating
316"
REFERENCES,0.7107942973523421,"artificial neural networks develop efficient color-naming systems. Proceedings of the National
317"
REFERENCES,0.7128309572301426,"Academy of Sciences, 118(12), Mar 2021. ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.
318"
REFERENCES,0.714867617107943,"2016569118. URL https://www.pnas.org/content/118/12/e2016569118.
319"
REFERENCES,0.7169042769857433,"David Francis, Ella Rabinovich, Farhan Samir, David Mortensen, and Suzanne Stevenson. Quan-
320"
REFERENCES,0.7189409368635438,"tifying Cognitive Factors in Lexical Decline. Transactions of the Association for Computa-
321"
REFERENCES,0.7209775967413442,"tional Linguistics, 9:1529–1545, 12 2021. ISSN 2307-387X. doi: 10.1162/tacl_a_00441. URL
322"
REFERENCES,0.7230142566191446,"https://doi.org/10.1162/tacl_a_00441.
323"
REFERENCES,0.725050916496945,"Shangmin Guo, Yi Ren, Simon Kirby, Kenny Smith, Kory Wallace Mathewson, and Stefano V.
324"
REFERENCES,0.7270875763747454,"Albrecht. Expressivity of Emergent Languages is a Trade-off between Contextual Complexity and
325"
REFERENCES,0.7291242362525459,"Unpredictability. September 2021. URL https://openreview.net/forum?id=WxuE_JWxjkW.
326"
REFERENCES,0.7311608961303462,"Eric Jang, Shixian Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In
327"
REFERENCES,0.7331975560081466,"Proceedings of the 2017 International Conference on Learning Representations (ICLR), 2017.
328"
REFERENCES,0.7352342158859471,"URL https://openreview.net/forum?id=rkE3y85ee.
329"
REFERENCES,0.7372708757637475,"M. G. Kendall. A new measure of rank correlation. Biometrika, 30(1-2):81–93, 06 1938. ISSN 0006-
330"
REFERENCES,0.7393075356415478,"3444. doi: 10.1093/biomet/30.1-2.81. URL https://doi.org/10.1093/biomet/30.1-2.81.
331"
REFERENCES,0.7413441955193483,"Eugene Kharitonov, Rahma Chaabouni, Diane Bouchacourt, and Marco Baroni. Entropy minimization
332"
REFERENCES,0.7433808553971487,"in emergent languages. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th
333"
REFERENCES,0.745417515274949,"International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
334"
REFERENCES,0.7474541751527495,"Research, pages 5220–5230. PMLR, 13–18 Jul 2020. URL http://proceedings.mlr.press/
335"
REFERENCES,0.7494908350305499,"v119/kharitonov20a.html.
336"
REFERENCES,0.7515274949083504,"Bohdan Khomtchouk and Shyam Sudhakaran. Modeling natural language emergence with integral
337"
REFERENCES,0.7535641547861507,"transform theory and reinforcement learning. arXiv:1812.01431 [cs], Nov 2018. URL http:
338"
REFERENCES,0.7556008146639511,"//arxiv.org/abs/1812.01431. arXiv: 1812.01431.
339"
REFERENCES,0.7576374745417516,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster),
340"
REFERENCES,0.7596741344195519,"2015. URL http://arxiv.org/abs/1412.6980.
341"
REFERENCES,0.7617107942973523,"Simon Kirby, Monica Tamariz, Hannah Cornish, and Kenny Smith. Compression and communication
342"
REFERENCES,0.7637474541751528,"in the cultural evolution of linguistic structure. Cognition, 141:87–102, 2015. ISSN 0010-0277.
343"
REFERENCES,0.7657841140529531,"doi: https://doi.org/10.1016/j.cognition.2015.03.016.
344"
REFERENCES,0.7678207739307535,"Angeliki Lazaridou and Marco Baroni. Emergent multi-agent communication in the deep learning
345"
REFERENCES,0.769857433808554,"era. arXiv:2006.02419 [cs], Jul 2020. URL http://arxiv.org/abs/2006.02419. arXiv:
346"
REFERENCES,0.7718940936863544,"2006.02419.
347"
REFERENCES,0.7739307535641547,"Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and
348"
REFERENCES,0.7759674134419552,"the emergence of (natural) language, 2017.
URL https://openreview.net/forum?id=
349"
REFERENCES,0.7780040733197556,"Hk8N3Sclg.
350"
REFERENCES,0.780040733197556,"David Lewis. Convention: A philosophical study. 1970.
351"
REFERENCES,0.7820773930753564,"Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relax-
352"
REFERENCES,0.7841140529531568,"ation of discrete random variables. In Proceedings of the 2017 International Conference on Learn-
353"
REFERENCES,0.7861507128309573,"ing Representations (ICLR), 2017. URL https://openreview.net/forum?id=S1jE5L5gl.
354"
REFERENCES,0.7881873727087576,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
355"
REFERENCES,0.790224032586558,"Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
356"
REFERENCES,0.7922606924643585,"Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
357"
REFERENCES,0.7942973523421588,"Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.
Pytorch: An imperative style,
358"
REFERENCES,0.7963340122199593,"high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-
359"
REFERENCES,0.7983706720977597,"Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32,
360"
REFERENCES,0.8004073319755601,"pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
361"
REFERENCES,0.8024439918533605,"9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
362"
REFERENCES,0.8044806517311609,"pdf.
363"
REFERENCES,0.8065173116089613,"Antonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah
364"
REFERENCES,0.8085539714867617,"Dormann. Stable baselines3. https://github.com/DLR-RM/stable-baselines3, 2019.
365"
REFERENCES,0.8105906313645621,"Cinjon Resnick, Abhinav Gupta, Jakob Foerster, Andrew M. Dai, and Kyunghyun Cho. Capacity,
366"
REFERENCES,0.8126272912423625,"Bandwidth, and Compositionality in Emergent Language Learning. International Conference on
367"
REFERENCES,0.814663951120163,"Autonomous Agents and Multi-Agent Systems, April 2020. URL http://arxiv.org/abs/1910.
368"
REFERENCES,0.8167006109979633,"11424.
369"
REFERENCES,0.8187372708757638,"Mathieu Rita, Florian Strub, Jean-Bastien Grill, Olivier Pietquin, and Emmanuel Dupoux. On the role
370"
REFERENCES,0.8207739307535642,"of population heterogeneity in emergent communication. In International Conference on Learning
371"
REFERENCES,0.8228105906313645,"Representations, 2022. URL https://openreview.net/forum?id=5Qkd7-bZfI.
372"
REFERENCES,0.824847250509165,"Diana Rodríguez Luna, Edoardo Maria Ponti, Dieuwke Hupkes, and Elia Bruni. Internal and external
373"
REFERENCES,0.8268839103869654,"pressures on language emergence: least effort, object constancy and frequency. In Findings of
374"
REFERENCES,0.8289205702647657,"the Association for Computational Linguistics: EMNLP 2020, page 4428–4437, Online, 2020.
375"
REFERENCES,0.8309572301425662,"Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.397. URL
376"
REFERENCES,0.8329938900203666,"https://www.aclweb.org/anthology/2020.findings-emnlp.397.
377"
REFERENCES,0.835030549898167,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
378"
REFERENCES,0.8370672097759674,"optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
379"
REFERENCES,0.8391038696537678,"Brian Skyrms. Signals: Evolution, learning, and information. OUP Oxford, 2010.
380"
REFERENCES,0.8411405295315683,"Agnieszka
Słowik,
Abhinav
Gupta,
William
L.
Hamilton,
M.
Jamnik,
S.
Holden,
381"
REFERENCES,0.8431771894093686,"and
C.
Pal.
Exploring
Structural
Inductive
Biases
in
Emergent
Communica-
382"
REFERENCES,0.845213849287169,"tion.
undefined,
2020.
URL
https://www.semanticscholar.org/paper/
383"
REFERENCES,0.8472505091649695,"Exploring-Structural-Inductive-Biases-in-Emergent-S%C5%82owik-Gupta/
384"
REFERENCES,0.8492871690427699,"29d1adb458d5b5a0fc837d37af01a6673efd531c.
385"
REFERENCES,0.8513238289205702,"Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
386"
REFERENCES,0.8533604887983707,"learning. Machine learning, 8(3):229–256, 1992.
387"
REFERENCES,0.8553971486761711,"Checklist
388"
REFERENCES,0.8574338085539714,"1. For all authors...
389"
REFERENCES,0.8594704684317719,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
390"
REFERENCES,0.8615071283095723,"contributions and scope? [Yes] Claims in the abstract are specifically discussed in
391"
REFERENCES,0.8635437881873728,"Section 5.1.
392"
REFERENCES,0.8655804480651731,"(b) Did you describe the limitations of your work? [Yes] The primary limitations of the
393"
REFERENCES,0.8676171079429735,"work are those discussed in Section 5.4.
394"
REFERENCES,0.869653767820774,"(c) Did you discuss any potential negative societal impacts of your work? [No] This work
395"
REFERENCES,0.8716904276985743,"is basic research with few to no immediate applications. The nearest applications would
396"
REFERENCES,0.8737270875763747,"be in evolutionary linguistics which we see as having minimal foreseeable negative
397"
REFERENCES,0.8757637474541752,"societal impacts.
398"
REFERENCES,0.8778004073319755,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
399"
REFERENCES,0.879837067209776,"them? [Yes]
400"
REFERENCES,0.8818737270875764,"2. If you are including theoretical results...
401"
REFERENCES,0.8839103869653768,"(a) Did you state the full set of assumptions of all theoretical results? [N/A]
402"
REFERENCES,0.8859470468431772,"(b) Did you include complete proofs of all theoretical results? [N/A]
403"
REFERENCES,0.8879837067209776,"3. If you ran experiments...
404"
REFERENCES,0.890020366598778,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
405"
REFERENCES,0.8920570264765784,"mental results (either in the supplemental material or as a URL)? [Yes] Supplemental
406"
REFERENCES,0.8940936863543788,"material for submission; repo URL will be given if accepted.
407"
REFERENCES,0.8961303462321792,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
408"
REFERENCES,0.8981670061099797,"were chosen)? [Yes] Choosing hyperparameters was not entirely straightforward, and
409"
REFERENCES,0.90020366598778,"this is discussed in Section 5.4.
410"
REFERENCES,0.9022403258655805,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
411"
REFERENCES,0.9042769857433809,"ments multiple times)? [No] We did not run the entire set of experiments multiple times
412"
REFERENCES,0.9063136456211812,"so as to report error bars. Nevertheless, the individual experiments themselves account
413"
REFERENCES,0.9083503054989817,"for stochasticity by displaying scatter plots; additionally we mention the p-values for
414"
REFERENCES,0.9103869653767821,"the correlation values.
415"
REFERENCES,0.9124236252545825,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
416"
REFERENCES,0.9144602851323829,"of GPUs, internal cluster, or cloud provider)? [Yes] The ELS experiments took 36
417"
REFERENCES,0.9164969450101833,"hours on an in-house server with a 20-core i9-9900X CPU; no experiments used a GPU.
418"
REFERENCES,0.9185336048879837,"Although not tracked, we expect that less than 300 hours of total server computer time
419"
REFERENCES,0.9205702647657841,"was used over the course of the whole project. The Rust implementation of FILEX
420"
REFERENCES,0.9226069246435845,"takes on the order of seconds to run.
421"
REFERENCES,0.924643584521385,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
422"
REFERENCES,0.9266802443991853,"(a) If your work uses existing assets, did you cite the creators? [Yes]
423"
REFERENCES,0.9287169042769857,"(b) Did you mention the license of the assets? [Yes] All assets have free licenses; supple-
424"
REFERENCES,0.9307535641547862,"mental code is under the GPLv3 license.
425"
REFERENCES,0.9327902240325866,"(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
426"
REFERENCES,0.9348268839103869,"Code in supplemental material; repo URL will be given if accepted.
427"
REFERENCES,0.9368635437881874,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
428"
REFERENCES,0.9389002036659878,"using/curating? [N/A]
429"
REFERENCES,0.9409368635437881,"(e) Did you discuss whether the data you are using/curating contains personally identifiable
430"
REFERENCES,0.9429735234215886,"information or offensive content? [N/A]
431"
REFERENCES,0.945010183299389,"5. If you used crowdsourcing or conducted research with human subjects...
432"
REFERENCES,0.9470468431771895,"(a) Did you include the full text of instructions given to participants and screenshots, if
433"
REFERENCES,0.9490835030549898,"applicable? [N/A]
434"
REFERENCES,0.9511201629327902,"(b) Did you describe any potential participant risks, with links to Institutional Review
435"
REFERENCES,0.9531568228105907,"Board (IRB) approvals, if applicable? [N/A]
436"
REFERENCES,0.955193482688391,"(c) Did you include the estimated hourly wage paid to participants and the total amount
437"
REFERENCES,0.9572301425661914,"spent on participant compensation? [N/A]
438"
REFERENCES,0.9592668024439919,"A
Emergent language system illustration
439 Goal"
REFERENCES,0.9613034623217923,"(a) The receiver (pictured) is rewarded for
moving towards the goal region at the center
in the NAV environment."
REFERENCES,0.9633401221995926,"S-agent
(perceptron)"
REFERENCES,0.9653767820773931,"R-agent
(perceptron)"
REFERENCES,0.9674134419551935,"(x, y)
action
(x, y)
location"
REFERENCES,0.9694501018329938,Gumbel-Softmax
REFERENCES,0.9714867617107943,(b) The agent architecture for NAV.
REFERENCES,0.9735234215885947,Figure 2
REFERENCES,0.9755600814663951,"B
Experiment parameters
440"
REFERENCES,0.9775967413441955,"Each experiment uses a logarithmic sweep across hyperparameters; the sweep is defined by Equation 9,
441"
REFERENCES,0.9796334012219959,"where x and y are the inclusive upper and lower bounds respectively and n is the number steps to
442"
REFERENCES,0.9816700610997964,"divide the interval into. The floor function is applied if the elements must be integers.
443"
REFERENCES,0.9837067209775967,"LS(x, y, n) =

x ·
y x"
REFERENCES,0.9857433808553971,"
i
n−1  i ∈{0, 1, . . . , n −1}

(9)"
REFERENCES,0.9877800407331976,"Hyperparameter
Default
Low
High
Steps"
REFERENCES,0.9898167006109979,"N
103
100
103
1000
S
26
23
28
1000
α
1
10−3
103
1000
β
8
100
103
1000"
REFERENCES,0.9918533604887984,"Table 3: Hyperparameters for the empirical evaluation of FILEX. “Low” and “High” refer to the
logarithmic sweep used for that experiment; default values used for all other experiments."
REFERENCES,0.9938900203665988,"Hyperparameter
Default
Low
High
Steps"
REFERENCES,0.9959266802443992,"Time steps
2 · 105
102
106
600
Bottleneck size
26
23
28
600
Learning rate
3 · 10−3
10−4
10−1
600
Buffer size
28
23
210
600
Temperature
1.5
10−1
101
600"
REFERENCES,0.9979633401221996,"Table 4: Hyperparameters for the empirical evaluation of FILEX. “Low” and “High” refer to the
logarithmic sweep used for that experiment; default values used for all other experiments. Please see
code for further details and default values."
