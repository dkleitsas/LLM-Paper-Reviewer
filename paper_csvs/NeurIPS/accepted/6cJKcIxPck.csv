Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0011299435028248588,"We study the fundamental mistake bound and sample complexity in the strategic
classification, where agents can strategically manipulate their feature vector up
to an extent in order to be predicted as positive. For example, given a classifier
determining college admission, student candidates may try to take easier classes to
improve their GPA, retake SAT and change schools in an effort to fool the classifier.
Ball manipulations are a widely studied class of manipulations in the literature,
where agents can modify their feature vector within a bounded radius ball. Unlike
most prior work, our work considers manipulations to be personalized, meaning
that agents can have different levels of manipulation abilities (e.g., varying radii
for ball manipulations), and unknown to the learner.
We formalize the learning problem in an interaction model where the learner
first deploys a classifier and the agent manipulates the feature vector within their
manipulation set to game the deployed classifier. We investigate various scenarios
in terms of the information available to the learner during the interaction, such
as observing the original feature vector before or after deployment, observing the
manipulated feature vector, or not seeing either the original or the manipulated
feature vector. We begin by providing online mistake bounds and PAC sample
complexity in these scenarios for ball manipulations. We also explore non-ball
manipulations and show that, even in the simplest scenario where both the original
and the manipulated feature vectors are revealed, the mistake bounds and sample
complexity are lower bounded by Ω(|H|) when the target function belongs to a
known class H."
INTRODUCTION,0.0022598870056497176,"1
Introduction"
INTRODUCTION,0.003389830508474576,"Strategic classification addresses the problem of learning a classifier robust to manipulation and
gaming by self-interested agents (Hardt et al., 2016). For example, given a classifier determining loan
approval based on credit scores, applicants could open or close credit cards and bank accounts to
increase their credit scores. In the case of a college admission classifier, students may try to take easier
classes to improve their GPA, retake the SAT or change schools in an effort to be admitted. In both
cases, such manipulations do not change their true qualifications. Recently, a collection of papers has
studied strategic classification in both the online setting where examples are chosen by an adversary
in a sequential manner (Dong et al., 2018; Chen et al., 2020; Ahmadi et al., 2021, 2023), and the"
INTRODUCTION,0.004519774011299435,"distributional setting where the examples are drawn from an underlying data distribution (Hardt
et al., 2016; Zhang and Conitzer, 2021; Sundaram et al., 2021; Lechner and Urner, 2022). Most
existing works assume that manipulation ability is uniform across all agents or is known to the learner.
However, in reality, this may not always be the case. For instance, low-income students may have a
lower ability to manipulate the system compared to their wealthier peers due to factors such as the
high costs of retaking the SAT or enrolling in additional classes, as well as facing more barriers to
accessing information about college (Milli et al., 2019) and it is impossible for the learner to know
the highest achievable GPA or the maximum number of times a student may retake the SAT due to
external factors such as socio-economic background and personal circumstances."
INTRODUCTION,0.005649717514124294,"We characterize the manipulation of an agent by a set of alternative feature vectors that she can modify
her original feature vector to, which we refer to as the manipulation set. Ball manipulations are a
widely studied class of manipulations in the literature, where agents can modify their feature vector
within a bounded radius ball. For example, Dong et al. (2018); Chen et al. (2020); Sundaram et al.
(2021) studied ball manipulations with distance function being some norm and Zhang and Conitzer
(2021); Lechner and Urner (2022); Ahmadi et al. (2023) studied a manipulation graph setting, which
can be viewed as ball manipulation w.r.t. the graph distance on a predefined known graph."
INTRODUCTION,0.006779661016949152,"In the online learning setting, the strategic agents come sequentially and try to game the current
classifier. Following previous work, we model the learning process as a repeated Stackelberg
game over T time steps. In round t, the learner proposes a classifier ft and then the agent, with a
manipulation set (unknown to the learner), manipulates her feature in an effort to receive positive
prediction from ft. There are several settings based on what and when the information is revealed
about the original feature vector and the manipulated feature vector in the game. The simplest setting
for the learner is observing the original feature vector before choosing ft and the manipulated vector
after. In a slightly harder setting, the learner observes both the original and manipulated vectors after
selecting ft. An even harder setting involves observing only the manipulated feature vector after
selecting ft. The hardest and least informative scenario occurs when neither the original nor the
manipulated feature vectors are observed."
INTRODUCTION,0.007909604519774011,"In the distributional setting, the agents are sampled from an underlying data distribution. Previous
work assumes that the learner has full knowledge of the original feature vector and the manipulation
set, and then views learning as a one-shot game and solves it by computing the Stackelberg equilibria
of it. However, when manipulations are personalized and unknown, we cannot compute an equilibrium
and study learning as a one-shot game. In this work, we extend the iterative online interaction model
from the online setting to the distributional setting, where the sequence of agents is sampled i.i.d.
from the data distribution. After repeated learning for T (which is equal to the sample size) rounds,
the learner has to output a strategy-robust predictor for future use."
INTRODUCTION,0.00903954802259887,"In both online and distributional settings, examples are viewed through the lens of the current predictor
and the learner does not have the ability to inquire about the strategies the previous examples would
have adopted under a different predictor."
INTRODUCTION,0.010169491525423728,"Related work
Our work is primarily related to strategic classification in online and distributional
settings. Strategic classification was first studied in a distributional model by Hardt et al. (2016)
and subsequently by Dong et al. (2018) in an online model. Hardt et al. (2016) assumed that agents
manipulate by best response with respect to a uniform cost function known to the learner. Building
on the framework of (Hardt et al., 2016), Lechner and Urner (2022); Sundaram et al. (2021); Zhang
and Conitzer (2021); Hu et al. (2019); Milli et al. (2019) studied the distributional learning problem,
and all of them assumed that the manipulations are predefined and known to the learner, either by a
cost function or a predefined manipulation graph. For online learning, Dong et al. (2018) considered
a similar manipulation setting as in this work, where manipulations are personalized and unknown.
However, they studied linear classification with ball manipulations in the online setting and focused
on finding appropriate conditions of the cost function to achieve sub-linear Stackelberg regret. Chen
et al. (2020) also studied Stackelberg regret in linear classification with uniform ball manipulations.
Ahmadi et al. (2021) studied the mistake bound under uniform (possbily unknown) ball manipulations,
and Ahmadi et al. (2023) studied regret under a pre-defined and known manipulation. The most
relevant work is a recent concurrent study by Lechner et al. (2023), which also explores strategic
classification involving unknown personalized manipulations but with a different loss function. In
their work, a predictor incurs a loss of 0 if and only if the agent refrains from manipulation and the
predictor correctly predicts at the unmanipulated feature vector. In our work, the predictor’s loss is 0"
INTRODUCTION,0.011299435028248588,"if it correctly predicts at the manipulated feature, even when the agent manipulates. As a result, their
loss function serves as an upper bound of our loss function."
INTRODUCTION,0.012429378531073447,"There has been a lot of research on various other issues and models in strategic classification. Beyond
sample complexity, Hu et al. (2019); Milli et al. (2019) focused on other social objectives, such as
social burden and fairness. Recent works also explored different models of agent behavior, including
proactive agents Zrnic et al. (2021), non-myopic agents (Haghtalab et al., 2022) and noisy agents (Ja-
gadeesan et al., 2021). Ahmadi et al. (2023) considers two agent models of randomized learners: a
randomized algorithm model where the agents respond to the realization, and a fractional classifier
model where agents respond to the expectation, and our model corresponds to the randomized al-
gorithm model. Additionally, there is also a line of research on agents interested in improving their
qualifications instead of gaming (Kleinberg and Raghavan, 2020; Haghtalab et al., 2020; Ahmadi
et al., 2022). Strategic interactions in the regression setting have also been studied (e.g., Bechavod
et al. (2021))."
INTRODUCTION,0.013559322033898305,"Beyond strategic classification, there is a more general research area of learning using data from strate-
gic sources, such as a single data generation player who manipulates the data distribution (Brückner
and Scheffer, 2011; Dalvi et al., 2004). Adversarial perturbations can be viewed as another type of
strategic source (Montasser et al., 2019)."
MODEL,0.014689265536723164,"2
Model"
MODEL,0.015819209039548022,"Strategic classification
Throughout this work, we consider the binary classification task. Let X
denote the feature vector space, Y = {+1, −1} denote the label space, and H ⊆YX denote the
hypothesis class. In the strategic setting, instead of an example being a pair (x, y), an example, or
agent, is a triple (x, u, y) where x ∈X is the original feature vector, y ∈Y is the label, and u ⊆X is
the manipulation set, which is a set of feature vectors that the agent can modify their original feature
vector x to. In particular, given a hypothesis h ∈YX , the agent will try to manipulate her feature
vector x to another feature vector x′ within u in order to receive a positive prediction from h. The
manipulation set u is unknown to the learner. In this work, we will be considering several settings
based on what the information is revealed to the learner, including both the original/manipulated
feature vectors, the manipulated feature vector only, or neither, and when the information is revealed."
MODEL,0.01694915254237288,"More formally, for agent (x, u, y), given a predictor h, if h(x) = −1 and her manipulation set
overlaps the positive region by h, i.e., u ∩Xh,+ ̸= ∅with Xh,+ := {x ∈X|h(x) = +1}, the agent
will manipulate x to ∆(x, h, u) ∈u ∩Xh,+1 to receive positive prediction by h. Otherwise, the agent
will do nothing and maintain her feature vector at x, i.e., ∆(x, h, u) = x. We call ∆(x, h, u) the
manipulated feature vector of agent (x, u, y) under predictor h."
MODEL,0.01807909604519774,"A general and fundamental type of manipulations is ball manipulations, where agents can manipulate
their feature within a ball of personalized radius. More specifically, given a metric d over X, the
manipulation set is a ball B(x; r) = {x′|d(x, x′) ≤r} centered at x with radius r for some r ∈R≥0.
Note that we allow different agents to have different manipulation power and the radius can vary over
agents. Let Q denote the set of allowed pairs (x, u), which we refer to as the feature-manipulation
set space. For ball manipulations, we have Q = {(x, B(x; r))|x ∈X, r ∈R≥0} for some known
metric d over X. In the context of ball manipulations, we use (x, r, y) to represent (x, B(x; r), y)
and ∆(x, h, r) to represent ∆(x, h, B(x; r)) for notation simplicity."
MODEL,0.0192090395480226,"For any hypothesis h, let the strategic loss ℓstr(h, (x, u, y)) of h be defined as the loss at the manip-
ulated feature, i.e., ℓstr(h, (x, u, y)) := 1(h(∆(x, h, u)) ̸= y). According to our definition of ∆(·),
we can write down the strategic loss explicitly as"
MODEL,0.020338983050847456,"ℓstr(h, (x, u, y)) ="
MODEL,0.021468926553672316,"


 

"
MODEL,0.022598870056497175,"1
if y = −1, h(x) = +1
1
if y = −1, h(x) = −1 and u ∩Xh,+ ̸= ∅,
1
if y = +1, h(x) = −1 and u ∩Xh,+ = ∅,
0
otherwise. (1)"
MODEL,0.023728813559322035,"For any randomized predictor p (a distribution over hypotheses), the strategic behavior depends on the
realization of the predictor and the strategic loss of p is ℓstr(p, (x, u, y)) := Eh∼p [ℓstr(h, (x, u, y))]."
MODEL,0.024858757062146894,"1For ball manipulations, agents break ties by selecting the closest vector. When there are multiple closest
vectors, agents break ties arbitrarily. For non-ball manipulations, agents break ties in any fixed way."
MODEL,0.02598870056497175,"Online learning
We consider the task of sequential classification where the learner aims to classify
a sequence of agents (x1, u1, y1), (x2, u2, y2), . . . , (xT , uT , yT ) ∈Q × Y that arrives in an online
manner. At each round, the learner feeds a predictor to the environment and then observes his
prediction byt, the true label yt and possibly along with some additional information about the
original/manipulated feature vectors. We say the learner makes a mistake at round t if byt ̸= yt and
the learner’s goal is to minimize the number of mistakes on the sequence. The interaction protocol
(which repeats for t = 1, . . . , T) is described in the following."
MODEL,0.02711864406779661,Protocol 1 Learner-Agent Interaction at round t
MODEL,0.02824858757062147,"1: The environment picks an agent (xt, ut, yt) and reveals some context C(xt). In the online setting,
the agent is chosen adversarially, while in the distributional setting, the agent is sampled i.i.d.
2: The learner A observes C(xt) and picks a hypothesis ft ∈YX .
3: The learner A observes the true label yt, the prediction byt = ft(∆t), and some feedback
F(xt, ∆t), where ∆t = ∆(xt, ft, ut) is the manipulated feature vector."
MODEL,0.02937853107344633,"The context function C(·) and feedback function F(·) reveals information about the original feature
vector xt and the manipulated feature vector ∆t. C(·) reveals the information before the learner picks
ft while F(·) does after. We study several different settings based on what and when information is
revealed."
MODEL,0.030508474576271188,"• The simplest setting for the learner is observing the original feature vector xt before choosing
ft and the manipulated vector ∆t after. Consider a teacher giving students a writing assignment
or take-home exam. The teacher might have a good knowledge of the students’ abilities (which
correspond to the original feature vector xt) based on their performance in class, but the grade
has to be based on how well they do the assignment. The students might manipulate by using the
help of ChatGPT / Google / WolframAlpha / their parents, etc. The teacher wants to create an
assignment that will work well even in the presence of these manipulation tools. In addition, If we
think of each example as representing a subpopulation (e.g., an organization is thinking of offering
loans to a certain group), then there might be known statistics about that population, even though
the individual classification (loan) decisions have to be made based on responses to the classifier.
This setting corresponds to C(xt) = xt and F(xt, ∆t) = ∆t. We denote a setting by their values
of C, F and thus, we denote this setting by (x, ∆)."
MODEL,0.031638418079096044,"• In a slightly harder setting, the learner observes both the original and manipulated vectors after
selecting ft and thus, ft cannot depend on the original feature vector in this case. For example, if
a high-school student takes the SAT test multiple times, most colleges promise to only consider
the highest one (or even to ""superscore"" the test by considering the highest score separately
in each section) but they do require the student to submit all of them. Then C(xt) =⊥and
F(xt, ∆t) = (xt, ∆t), where ⊥is a token for “no information”, and this setting is denoted by
(⊥, (x, ∆))."
MODEL,0.0327683615819209,"• An even harder setting involves observing only the manipulated feature vector after selecting ft
(which can only be revealed after ft since ∆t depends on ft). Then C(xt) =⊥and F(xt, ∆t) = ∆t
and this setting is denoted by (⊥, ∆)."
MODEL,0.03389830508474576,"• The hardest and least informative scenario occurs when neither the original nor the manipulated
feature vectors are observed. Then C(xt) =⊥and F(xt, ∆t) =⊥and it is denoted by (⊥, ⊥)."
MODEL,0.03502824858757062,"Throughout this work, we focus on the realizable setting, where there exists a perfect classifier in H
that never makes any mistake at the sequence of strategic agents. More specifically, there exists a
hypothesis h∗∈H such that for any t ∈[T], we have yt = h∗(∆(xt, h∗, ut))2. Then we define the
mistake bound as follows."
MODEL,0.03615819209039548,"Definition 1.
For any choice of (C, F),
let A be an online learning algorithm un-
der
Protocol
1
in
the
setting
of
(C, F).
Given
any
realizable
sequence
S
=
((x1, u1, h∗(∆(x1, h∗, u1))), . . . , (xT , uT , h∗(∆(xT , h∗, uT ))) ∈(Q × Y)T , where T is any in-
teger and h∗∈H, let MA(S) be the number of mistakes A makes on the sequence S. The mistake
bound of (H, Q), denoted MBC,F , is the smallest number B ∈N such that there exists an algorithm
A such that MA(S) ≤B over all realizable sequences S of the above form."
MODEL,0.03728813559322034,2It is possible that there is no hypothesis h ∈YX s.t. yt = h(xt) for all t ∈[T].
MODEL,0.0384180790960452,"According the rank of difficulty of the four settings with different choices of (C, F), the mistake
bounds are ranked in the order of MBx,∆≤MB⊥,(x,∆) ≤MB⊥,∆≤MB⊥,⊥."
MODEL,0.03954802259887006,"PAC learning
In the distributional setting, the agents are sampled from an underlying dis-
tribution D over Q × Y.
The learner’s goal is to find a hypothesis h with low popula-
tion loss Lstr
D(h) := E(x,u,y)∼D [ℓstr(h, (x, u, y))].
One may think of running empirical risk
minimizer (ERM) over samples drawn from the underlying data distribution, i.e., returning
arg minh∈H
1
m
Pm
i=1 ℓstr(h, (xi, ui, yi)), where (x1, u1, y1), . . . , (xm, um, ym) are i.i.d. sampled
from D. However, ERM is unimplementable because the manipulation sets ui’s are never revealed to
the algorithm, and only the partial feedback in response to the implemented classifier is provided. In
particular, in this work we consider using the same interaction protocol as in the online setting, i.e.,
Protocol 1, with agents (xt, ut, yt) i.i.d. sampled from the data distribution D. After T rounds of
interaction (i.e., T i.i.d. agents), the learner has to output a predictor fout for future use."
MODEL,0.04067796610169491,"Again, we focus on the realizable setting, where the sequence of sampled agents (with manipulation)
can be perfectly classified by a target function in H. Alternatively, there exists a classifier with zero
population loss, i.e., there exists a hypothesis h∗∈H such that Lstr
D(h∗) = 0. Then we formalize the
notion of PAC sample complexity under strategic behavior as follows.
Definition 2. For any choice of (C, F), let A be a learning algorithm that interacts with agents using
Protocol 1 in the setting of (C, F) and outputs a predictor fout in the end. For any ε, δ ∈(0, 1), the
sample complexity of realizable (ε, δ)-PAC learning of (H, Q), denoted SCC,F (ε, δ), is defined as
the smallest m ∈N for which there exists a learning algorithm A in the above form such that for any
distribution D over Q × Y where there exists a predictor h∗∈H with zero loss, Lstr
D (h) = 0, with"
MODEL,0.04180790960451977,"probability at least 1 −δ over (x1, u1, y1), . . . , (xm, um, ym)
i.i.d.
∼D, Lstr
D (fout) ≤ε."
MODEL,0.04293785310734463,"Similar to mistake bounds, the sample complexities are ranked in the same order SCx,∆≤
SC⊥,(x,∆) ≤SC⊥,∆≤SC⊥,⊥according to the rank of difficulty of the four settings."
OVERVIEW OF RESULTS,0.04406779661016949,"3
Overview of Results"
OVERVIEW OF RESULTS,0.04519774011299435,"In classic (non-strategic) online learning, the Halving algorithm achieves a mistake bound of log(|H|)
by employing the majority vote and eliminating inconsistent hypotheses at each round. In classic
PAC learning, the sample complexity of O( log(|H|)"
OVERVIEW OF RESULTS,0.04632768361581921,"ε
) is achievable via ERM. Both mistake bound
and sample complexity exhibit logarithmic dependency on |H|. This logarithmic dependency on |H|
(when there is no further structural assumptions) is tight in both settings, i.e., there exist examples
of H with mistake bound of Ω(log(|H|)) and with sample complexity of Ω( log(|H|)"
OVERVIEW OF RESULTS,0.04745762711864407,"ε
). In the setting
where manipulation is known beforehand and only ∆t is observed, Ahmadi et al. (2023) proved a
lower bound of Ω(|H|) for the mistake bound. Since in the strategic setting we can achieve a linear
dependency on |H| by trying each hypothesis in H one by one and discarding it once it makes a
mistake, the question arises:"
OVERVIEW OF RESULTS,0.04858757062146893,Can we achieve a logarithmic dependency on |H| in strategic classification?
OVERVIEW OF RESULTS,0.04971751412429379,"In this work, we show that the dependency on |H| varies across different settings and that in some
settings mistake bound and PAC sample complexity can exhibit different dependencies on |H|. We
start by presenting our results for ball manipulations in the four settings."
OVERVIEW OF RESULTS,0.05084745762711865,"• Setting of (x, ∆) (observing xt before choosing ft and observing ∆t after) : For online learning,
we propose an variant of the Halving algorithm, called Strategic Halving (Algorithm 1), which can
eliminate half of the remaining hypotheses when making a mistake. The algorithm depends on ob-
serving xt before choosing the predictor ft. Then by applying the standard technique of converting
mistake bound to PAC bound, we are able to achieve sample complexity of O( log(|H|) loglog(|H|)"
OVERVIEW OF RESULTS,0.0519774011299435,"ε
).
• Setting of (⊥, (x, ∆)) (observing both xt and ∆t after selecting ft) : We prove that, there exists
an example of (H, Q) s.t. the mistake bound is lower bounded by Ω(|H|). This implies that no
algorithm can perform significantly better than sequentially trying each hypothesis, which would
make at most |H| mistakes before finding the correct hypothesis. However, unlike the construction
of mistake lower bounds in classic online learning, where all mistakes can be forced to occur in the
initial rounds, we demonstrate that we require Θ(|H|2) rounds to ensure that all mistakes occur. In"
OVERVIEW OF RESULTS,0.05310734463276836,"the PAC setting, we first show that, any learning algorithm with proper output fout, i.e., fout ∈H,
needs a sample size of Ω( |H|"
OVERVIEW OF RESULTS,0.05423728813559322,ε ). We can achieve a sample complexity of O( log2(|H|)
OVERVIEW OF RESULTS,0.05536723163841808,"ε
) by executing
Algorithm 2, which is a randomized algorithm with improper output."
OVERVIEW OF RESULTS,0.05649717514124294,"• Setting of (⊥, ∆) (observing only ∆t after selecting ft) : The mistake bound of Ω(|H|) also holds
in this setting, as it is known to be harder than the previous setting. For the PAC learning, we show
that any conservative algorithm, which only depends on the information from the mistake rounds,
requires Ω( |H|"
OVERVIEW OF RESULTS,0.0576271186440678,ε ) samples. The optimal sample complexity is left as an open problem.
OVERVIEW OF RESULTS,0.05875706214689266,"• Setting of (⊥, ⊥) (observing neither xt nor ∆t) : Similarly, the mistake bound of Ω(|H|) still holds.
For the PAC learning, we show that the sample complexity is Ω( |H|"
OVERVIEW OF RESULTS,0.059887005649717516,"ε ) by reducing the problem to a
stochastic linear bandit problem."
OVERVIEW OF RESULTS,0.061016949152542375,"Then we move on to non-ball manipulations. However, we show that even in the simplest setting of
observing xt before choosing ft and observing ∆t after, there is an example of (H, Q) such that the
sample complexity is eΩ( |H|"
OVERVIEW OF RESULTS,0.062146892655367235,"ε ). This implies that in all four settings of different revealed information,
we will have sample complexity of eΩ( |H|"
OVERVIEW OF RESULTS,0.06327683615819209,"ε ) and mistake bound of eΩ(|H|). We summarize our results
in Table 1."
OVERVIEW OF RESULTS,0.06440677966101695,"setting
mistake bound
sample complexity ball"
OVERVIEW OF RESULTS,0.0655367231638418,"(x, ∆)
Θ(log(|H|)) (Thm 1)
e
O( log(|H|)"
OVERVIEW OF RESULTS,0.06666666666666667,"ε
)a (Thm 2), Ω( log(|H|) ε
)"
OVERVIEW OF RESULTS,0.06779661016949153,"(⊥, (x, ∆))
O(min(
p"
OVERVIEW OF RESULTS,0.06892655367231638,"log(|H|)T, |H|)) (Thm 4)
O( log2(|H|)"
OVERVIEW OF RESULTS,0.07005649717514124,"ε
) (Thm 6), Ω( log(|H|) ε
)"
OVERVIEW OF RESULTS,0.0711864406779661,"Ω(min(
T
|H| log(|H|), |H|))(Thm 3)
SCprop = Ω( |H|"
OVERVIEW OF RESULTS,0.07231638418079096,ε ) (Thm 5)
OVERVIEW OF RESULTS,0.07344632768361582,"(⊥, ∆)
Θ(|H|) (implied by Thm 3)
SCcsv = eΩ( |H|"
OVERVIEW OF RESULTS,0.07457627118644068,ε ) (Thm 7)
OVERVIEW OF RESULTS,0.07570621468926554,"(⊥, ⊥)
Θ(|H|) (implied by Thm 3)
e
O( |H|"
OVERVIEW OF RESULTS,0.0768361581920904,"ε ) , eΩ( |H|"
OVERVIEW OF RESULTS,0.07796610169491526,ε ) (Thm 8)
OVERVIEW OF RESULTS,0.07909604519774012,"nonball
all
eΩ(|H|)(Cor 1) , O(|H|)
e
O( |H|"
OVERVIEW OF RESULTS,0.08022598870056497,"ε ) , eΩ( |H|"
OVERVIEW OF RESULTS,0.08135593220338982,ε ) (Cor 1)
OVERVIEW OF RESULTS,0.08248587570621468,"a A factor of loglog(|H|) is neglected.
Table 1: The summary of results. e
O and eΩignore logarithmic factors on |H| and 1"
OVERVIEW OF RESULTS,0.08361581920903954,"ε. The superscripts
prop stands for proper learning algorithms and csv stands for conservative learning algorithms.
All lower bounds in the non-strategic setting also apply to the strategic setting, implying that
MBC,F ≥Ω(log(|H|)) and SCC,F ≥Ω( log(|H|)"
OVERVIEW OF RESULTS,0.0847457627118644,"ε
) for all settings of (C, F). In all four settings, a
mistake bound of O(|H|) can be achieved by simply trying each hypothesis in H while the sample
complexity can be achieved as e
O( |H|"
OVERVIEW OF RESULTS,0.08587570621468926,"ε ) by converting the mistake bound of O(|H|) to a PAC bound
using standard techniques."
BALL MANIPULATIONS,0.08700564971751412,"4
Ball manipulations"
BALL MANIPULATIONS,0.08813559322033898,"In ball manipulations, when B(x; r) ∩Xh,+ has multiple elements, the agent will always break ties
by selecting the one closest to x, i.e., ∆(x, h, r) = arg minx′∈B(x;r)∩Xh,+ d(x, x′). In round t, the
learner deploys predictor ft, and once he knows xt and byt, he can calculate ∆t himself without
needing knowledge of rt by ∆t ="
BALL MANIPULATIONS,0.08926553672316384,"(
arg minx′∈Xft,+ d(xt, x′)
if byt = +1 ,
xt
if byt = −1 ."
BALL MANIPULATIONS,0.0903954802259887,"Thus, for ball manipulations, knowing xt is equivalent to knowing both xt and ∆t."
BALL MANIPULATIONS,0.09152542372881356,"4.1
Setting (x, ∆): Observing xt Before Choosing ft"
BALL MANIPULATIONS,0.09265536723163842,"Online learning
We propose a new algorithm with mistake bound of log(|H|) in setting (x, ∆). To
achieve a logarithmic mistake bound, we must construct a predictor ft such that if it makes a mistake,
we can reduce a constant fraction of the remaining hypotheses. The primary challenge is that we do
not have access to the full information, and predictions of other hypotheses are hidden. To extract
the information of predictions of other hypotheses, we take advantage of ball manipulations, which"
BALL MANIPULATIONS,0.09378531073446328,"induces an ordering over all hypotheses. Specifically, for any hypothesis h and feature vector x, we
define the distance between x and h by the distance between x and the positive region by h, Xh,+,
i.e.,
d(x, h) := min{d(x, x′)|x′ ∈Xh,+} .
(2)"
BALL MANIPULATIONS,0.09491525423728814,"At each round t, given xt, the learner calculates the distance d(xt, h) for all h in the version space
(meaning hypotheses consistent with history) and selects a hypothesis ft such that d(xt, ft) is the
median among all distances d(xt, h) for h in the version space. We can show that by selecting ft in
this way, the learner can eliminate half of the version space if ft makes a mistake. We refer to this
algorithm as Strategic Halving, and provide a detailed description of it in Algorithm 1."
BALL MANIPULATIONS,0.096045197740113,"Theorem 1. For any feature-ball manipulation set space Q and hypothesis class H, Strategic Halving
achieves mistake bound MBx,∆≤log(|H|)."
BALL MANIPULATIONS,0.09717514124293786,Algorithm 1 Strategic Halving
BALL MANIPULATIONS,0.09830508474576272,"1: Initialize the version space VS = H.
2: for t = 1, . . . , T do
3:
pick an ft ∈VS such that d(xt, ft) is the median of {d(xt, h)|h ∈VS}.
4:
if byt ̸= yt and yt = + then VS ←VS \ {h ∈VS|d(xt, h) ≥d(xt, ft)};
5:
else if byt ̸= yt and yt = −then VS ←VS \ {h ∈VS|d(xt, h) ≤d(xt, ft)}.
6: end for"
BALL MANIPULATIONS,0.09943502824858758,"To prove Theorem 1, we only need to show that each mistake reduces the version space by half.
Supposing that ft misclassifies a true positive example (xt, rt, +1) by negative, then we know
that d(xt, ft) > rt while the target hypothesis h∗must satisfy that d(xt, h∗) ≤rt. Hence any h
with d(xt, h) ≥d(xt, ft) cannot be h∗and should be eliminated. Since d(xt, ft) is the median of
{d(xt, h)|h ∈VS}, we can elimate half of the version space. It is similar when ft misclassifies a
true negative. The detailed proof is deferred to Appendix B."
BALL MANIPULATIONS,0.10056497175141244,"PAC learning
We can convert Strategic Halving to a PAC learner by the standard technique of
converting a mistake bound to a PAC bound (Gallant, 1986). Specifically, the learner runs Strategic
Halving until it produces a hypothesis ft that survives for 1"
BALL MANIPULATIONS,0.1016949152542373,ε log( log(|H|)
BALL MANIPULATIONS,0.10282485875706214,"δ
) rounds and outputs this ft.
Then we have Theorem 2, and the proof is included in Appendix C."
BALL MANIPULATIONS,0.103954802259887,"Theorem 2. For any feature-ball manipulation set space Q and hypothesis class H, we can achieve
SCx,∆(ε, δ) = O( log(|H|)"
BALL MANIPULATIONS,0.10508474576271186,"ε
log( log(|H|)"
BALL MANIPULATIONS,0.10621468926553672,"δ
)) by combining Strategic Halving and the standard technique
of converting a mistake bound to a PAC bound."
BALL MANIPULATIONS,0.10734463276836158,"4.2
Setting (⊥, (x, ∆)): Observing xt After Choosing ft"
BALL MANIPULATIONS,0.10847457627118644,"When xt is not revealed before the learner choosing ft, the algorithm of Strategic Halving does not
work anymore. We demonstrate that it is impossible to reduce constant fraction of version space when
making a mistake, and prove that the mistake bound is lower bounded by Ω(|H|) by constructing a
negative example of (H, Q). However, we can still achieve sample complexity with poly-logarithmic
dependency on |H| in the distributional setting."
RESULTS IN THE ONLINE LEARNING MODEL,0.1096045197740113,"4.2.1
Results in the Online Learning Model"
RESULTS IN THE ONLINE LEARNING MODEL,0.11073446327683616,"To offer readers an intuitive understanding of the distinctions between the strategic setting and
standard online learning, we commence by presenting an example in which no deterministic learners,
including the Halving algorithm, can make fewer than |H| −1 mistakes.
Example 1. Consider a star shape metric space (X, d), where X = {0, 1, . . . , n}, d(i, j) = 2 and
d(0, i) = 1 for all i, j ∈[n] with i ̸= j. The hypothesis class is composed of singletons over [n],
i.e., H = {21{i} −1|i ∈[n]}. When the learner is deterministic, the environment can pick an agent
(xt, rt, yt) dependent on ft. If ft is all-negative, then the environment picks (xt, rt, yt) = (0, 1, +1),
and then the learner makes a mistake but no hypothesis can be eliminated. If ft predicts 0 by positive,
the environment will pick (xt, rt, yt) = (0, 0, −1), and then the learner makes a mistake but no
hypothesis can be eliminated. If ft predicts some i ∈[n] by positive, the environment will pick
(xt, rt, yt) = (i, 0, −1), and then the learner makes a mistake with only one hypothesis 21{i} −1
eliminated. Therefore, the learner will make n −1 mistakes."
RESULTS IN THE ONLINE LEARNING MODEL,0.11186440677966102,"In this work, we allow the learner to be randomized. When an (xt, rt, yt) is generated by the
environment, the learner can randomly pick an ft, and the environment does not know the realization
of ft but knows the distribution where ft comes from. It turns out that randomization does not help
much. We prove that there exists an example in which any (possibly randomized) learner will incur
Ω(|H|) mistakes.
Theorem 3. There exists a feature-ball manipulation set space Q and hypothesis class H s.t. the
mistake bound MB⊥,(x,∆) ≥|H|−1. For any (randomized) algorithm A and any T ∈N, there exists
a realizable sequence of (xt, rt, yt)1:T such that with probability at least 1 −δ (over randomness of
A), A makes at least min(
T
5|H| log(|H|/δ), |H| −1) mistakes."
RESULTS IN THE ONLINE LEARNING MODEL,0.11299435028248588,"Essentially, we design an adversarial environment such that the learner has a probability of
1
|H| of
making a mistake at each round before identifying the target function h∗. The learner only gains
information about the target function when a mistake is made. The detailed proof is deferred to
Appendix D. Theorem 3 establishes a lower bound on the mistake bound, which is |H| −1. However,
achieving this bound requires a sufficiently large number of rounds, specifically T = eΩ(|H|2). This
raises the question of whether there exists a learning algorithm that can make o(T) mistakes for any
T ≤|H|2. In Example 1, we observed that the adversary can force any deterministic learner to make
|H| −1 mistakes in |H| −1 rounds. Consequently, no deterministic algorithm can achieve o(T)
mistakes."
RESULTS IN THE ONLINE LEARNING MODEL,0.11412429378531073,"To address this, we propose a randomized algorithm that closely resembles Algorithm 1, with a
modification in the selection of ft. Instead of using line 3, we choose ft randomly from VS since
we lack prior knowledge of xt. This algorithm can be viewed as a variation of the well-known
multiplicative weights method, applied exclusively during mistake rounds. For improved clarity, we
present this algorithm as Algorithm 3 in Appendix E due to space limitations."
RESULTS IN THE ONLINE LEARNING MODEL,0.1152542372881356,"Theorem 4. For any T ∈N, Algorithm 3 will make at most min(
p"
RESULTS IN THE ONLINE LEARNING MODEL,0.11638418079096045,"4 log(|H|)T, |H| −1) mistakes
in expectation in T rounds."
RESULTS IN THE ONLINE LEARNING MODEL,0.11751412429378531,"Note that the T-dependent upper bound in Theorem 4 matches the lower bound in Theorem 3 up
to a logarithmic factor when T = |H|2. This implies that approximately |H|2 rounds are needed to
achieve |H| −1 mistakes, which is a tight bound up to a logarithmic factor. Proof of Theorem 4 is
included in Appendix E."
RESULTS IN THE PAC LEARNING MODEL,0.11864406779661017,"4.2.2
Results in the PAC Learning Model"
RESULTS IN THE PAC LEARNING MODEL,0.11977401129943503,"In the PAC setting, the goal of the learner is to output a predictor fout after the repeated interactions.
A common class of learning algorithms, which outputs a hypothesis fout ∈H, is called proper.
Proper learning algorithms are a common starting point when designing algorithms for new learning
problems due to their natural appeal and ability to achieve good performance, such as ERM in classic
PAC learning. However, in the current setting, we show that proper learning algorithms do not work
well and require a sample size linear in |H|. The formal theorem is stated as follows and the proof is
deferred to Appendix F.
Theorem 5. There exists a feature-ball manipulation set space Q and hypothesis class H s.t.
SCprop
⊥,∆(ε, 7"
RESULTS IN THE PAC LEARNING MODEL,0.12090395480225989,8) = Ω( |H|
RESULTS IN THE PAC LEARNING MODEL,0.12203389830508475,"ε ), where SCprop
⊥,∆(ε, δ) is the (ε, δ)-PAC sample complexity achievable by
proper algorithms."
RESULTS IN THE PAC LEARNING MODEL,0.12316384180790961,"Theorem 5 implies that any algorithm capable of achieving sample complexity sub-linear in |H| must
be improper. As a result, we are inspired to devise an improper learning algorithm. Before presenting
the algorithm, we introduce some notations. For two hypotheses h1, h2, let h1 ∨h2 denote the union
of them, i.e., (h1 ∨h2)(x) = +1 iff. h1(x) = +1 or h2(x) = +1. Similarly, we can define the union
of more than two hypotheses. Then for any union of k hypotheses, f = ∨k
i=1hi, the positive region of
f is the union of positive regions of the k hypotheses and thus, we have d(x, f) = mini∈[k] d(x, hi).
Therefore, we can decrease the distance between f and any feature vector x by increasing k. Based
on this, we devise a new randomized algorithm with improper output, described in Algorithm 2.
Theorem 6. For any feature-ball manipulation set space Q and hypothesis class H, we can achieve
SC⊥,(x,∆)(ε, δ) = O( log2(|H|)+log(1/δ)"
RESULTS IN THE PAC LEARNING MODEL,0.12429378531073447,"ε
log( 1"
RESULTS IN THE PAC LEARNING MODEL,0.12542372881355932,"δ )) by combining Algorithm 2 with a standard confi-
dence boosting technique. Note that the algorithm is improper."
RESULTS IN THE PAC LEARNING MODEL,0.12655367231638417,Algorithm 2
RESULTS IN THE PAC LEARNING MODEL,0.12768361581920903,"1: Initialize the version space VS0 = H.
2: for t = 1, . . . , T do
3:
randomly pick kt ∼Unif({1, 2, 22, . . . , 2⌊log2(nt)−1⌋}) where nt = |VSt−1|;
4:
sample kt hypotheses h1, . . . , hkt independently and uniformly at random from VSt−1;
5:
let ft = ∨kt
i=1hi.
6:
if byt ̸= yt and yt = + then VSt = VSt−1 \ {h ∈VSt−1|d(xt, h) ≥d(xt, ft)};
7:
else if byt ̸= yt and yt = −then VSt = VSt−1 \ {h ∈VSt−1|d(xt, h) ≤d(xt, ft)};
8:
else VSt = VSt−1.
9: end for
10: randomly pick τ from [T] and randomly sample h1, h2 from VSτ−1 with replacement.
11: output h1 ∨h2"
RESULTS IN THE PAC LEARNING MODEL,0.1288135593220339,"Now we outline the high-level ideas behind Algorithm 2. In correct rounds where ft makes no
mistake, the predictions of all hypotheses are either correct or unknown, and thus, it is hard to
determine how to make updates. In mistake rounds, we can always update the version space similar
to what was done in Strategic Halving. To achieve a poly-logarithmic dependency on |H|, we aim to
reduce a significant number of misclassifying hypotheses in mistake rounds. The maximum number
we can hope to reduce is a constant fraction of the misclassifying hypotheses. We achieve this by
randomly sampling a ft (lines 3-5) s.t. ft makes a mistake, and d(xt, ft) is greater (smaller) than the
median of d(xt, h) for all misclassifying hypotheses h for true negative (positive) examples. However,
due to the asymmetric nature of manipulation, which aims to be predicted as positive, the rate of
decreasing misclassifications over true positives is slower than over true negatives. To compensate
for this asymmetry, we output a fout = h1 ∨h2 with two selected hypotheses h1, h2 (lines 10-11)
instead of a single one to increase the chance of positive prediction."
RESULTS IN THE PAC LEARNING MODEL,0.12994350282485875,"We prove that Algorithm 2 can achieve small strategic loss in expectation as described in Lemma 1.
Then we can achieve the sample complexity in Theorem 6 by boosting Algorithm 2 to a strong learner.
This is accomplished by running Algorithm 2 multiple times until we obtain a good predictor. The
proofs of Lemma 1 and Theorem 6 are deferred to Appendix G."
RESULTS IN THE PAC LEARNING MODEL,0.1310734463276836,"Lemma 1. Let S = (xt, rt, yt)T
t=1 ∼DT denote the i.i.d. sampled agents in T rounds and let A(S)
denote the output of Algorithm 2 interacting with S. For any feature-ball manipulation set space Q
and hypothesis class H, when T ≥320 log2(|H|)"
RESULTS IN THE PAC LEARNING MODEL,0.13220338983050847,"ε
, we have EA,S [Lstr(A(S))] ≤ε."
RESULTS IN THE PAC LEARNING MODEL,0.13333333333333333,"4.3
Settings (⊥, ∆) and (⊥, ⊥)"
RESULTS IN THE PAC LEARNING MODEL,0.1344632768361582,"Online learning
As mentioned in Section 2, both the settings of (⊥, ∆) and (⊥, ⊥) are harder than
the setting of (⊥, (x, ∆)), all lower bounds in the setting of (⊥, (x, ∆)) also hold in the former two
settings. Therefore, by Theorem 3, we have MB⊥,⊥≥MB⊥,∆≥MB⊥,(x,∆) = |H| −1."
RESULTS IN THE PAC LEARNING MODEL,0.13559322033898305,"PAC learning
In the setting of (⊥, ∆), Algorithm 2 is not applicable anymore since the learner
lacks observation of xt, making it impossible to replicate the version space update steps in lines
6-7. It is worth noting that both PAC learning algorithms we have discussed so far fall under a
general category called conservative algorithms, depend only on information from the mistake rounds.
Specifically, an algorithm is said to be conservative if for any t, the predictor ft only depends on
the history of mistake rounds up to t, i.e., τ < t with byτ ̸= yτ, and the output fout only depends
on the history of mistake rounds, i.e., (ft, byt, yt, ∆t)t:byt̸=yt. Any algorithm that goes beyond this
category would need to utilize the information in correct rounds. As mentioned earlier, in correct
rounds, the predictions of all hypotheses are either correct or unknown, which makes it challenging to
determine how to make updates. For conservative algorithms, we present a lower bound on the sample
complexity in the following theorem, which is eΩ( |H|"
RESULTS IN THE PAC LEARNING MODEL,0.1367231638418079,"ε ), and its proof is included in Appendix H. The
optimal sample complexity in the setting (⊥, ∆) is left as an open problem."
RESULTS IN THE PAC LEARNING MODEL,0.13785310734463277,"Theorem 7. There exists a feature-ball manipulation set space Q and hypothesis class H s.t.
SCcsv
⊥,∆(ε, 7"
RESULTS IN THE PAC LEARNING MODEL,0.13898305084745763,8) = eΩ( |H|
RESULTS IN THE PAC LEARNING MODEL,0.1401129943502825,"ε ), where SCcsv
⊥,∆(ε, δ) is (ε, δ)-PAC the sample complexity achievable by
conservative algorithms."
RESULTS IN THE PAC LEARNING MODEL,0.14124293785310735,"In the setting of (⊥, ⊥), our problem reduces to a best arm identification problem in stochastic bandits.
We prove a lower bound on the sample complexity of eΩ( |H|"
RESULTS IN THE PAC LEARNING MODEL,0.1423728813559322,"ε ) in Theorem 8 by reduction to stochastic
linear bandits and applying the tools from information theory. The proof is deferred to Appendix I.
Theorem 8. There exists a feature-ball manipulation set space Q and hypothesis class H s.t.
SC⊥,⊥(ε, 7"
RESULTS IN THE PAC LEARNING MODEL,0.14350282485875707,8) = eΩ( |H| ε ).
NON-BALL MANIPULATIONS,0.14463276836158193,"5
Non-ball Manipulations"
NON-BALL MANIPULATIONS,0.14576271186440679,"In this section, we move on to non-ball manipulations. In ball manipulations, for any feature vector
x, we have an ordering of hypotheses according to their distances to x, which helps to infer the
predictions of some hypotheses without implementing them. However, in non-ball manipulations, we
don’t have such structure anymore. Therefore, even in the simplest setting of observing xt before ft
and ∆t, we have the PAC sample complexity lower bounded by eΩ( |H|"
NON-BALL MANIPULATIONS,0.14689265536723164,"ε ).
Theorem 9. There exists a feature-manipulation set space Q and hypothesis class H s.t.
SCx,∆(ε, 7"
NON-BALL MANIPULATIONS,0.1480225988700565,8) = eΩ( |H| ε ).
NON-BALL MANIPULATIONS,0.14915254237288136,"The proof is deferred to Appendix J. It is worth noting that in the construction of the proof, we let
all agents to have their original feature vector xt = 0 such that xt does not provide any information.
Since (x, ∆) is the simplest setting and any mistake bound can be converted to a PAC bound via
standard techniques (see Section A.2 for more details), we have the following corollary.
Corollary 1. There exists a feature-manipulation set space Q and hypothesis class H s.t. for all
choices of (C, F), SCC,F (ε, 7"
NON-BALL MANIPULATIONS,0.15028248587570622,8) = eΩ( |H|
NON-BALL MANIPULATIONS,0.15141242937853108,"ε ) and MBC,F = eΩ(|H|)."
DISCUSSION AND OPEN PROBLEMS,0.15254237288135594,"6
Discussion and Open Problems"
DISCUSSION AND OPEN PROBLEMS,0.1536723163841808,"In this work, we investigate the mistake bound and sample complexity of strategic classification across
multiple settings. Unlike prior work, we assume that the manipulation is personalized and unknown
to the learner, which makes the strategic classification problem more challenging. In the case of
ball manipulations, when the original feature vector xt is revealed prior to choosing ft, the problem
exhibits a similar level of difficulty as the non-strategic setting (see Table 1 for details). However,
when the original feature vector xt is not revealed beforehand, the problem becomes significantly
more challenging. Specifically, any learner will experience a mistake bound that scales linearly with
|H|, and any proper learner will face sample complexity that also scales linearly with |H|. In the case
of non-ball manipulations, the situation worsens. Even in the simplest setting, where the original
feature is observed before choosing ft and the manipulated feature is observed afterward, any learner
will encounter a linear mistake bound and sample complexity."
DISCUSSION AND OPEN PROBLEMS,0.15480225988700566,"Besides the question of optimal sample complexity in the setting of (⊥, ∆) as mentioned in Sec 4.3,
there are some other fundamental open questions."
DISCUSSION AND OPEN PROBLEMS,0.15593220338983052,"Combinatorial measure
Throughout this work, our main focus is on analyzing the dependency
on the size of the hypothesis class |H| without assuming any specific structure of H. Just as VC
dimension provides tight characterization for PAC learnability and Littlestone dimension characterizes
online learnability, we are curious if there exists a combinatorial measure that captures the essence
of strategic classification in this context. In the proofs of the most lower bounds in this work, we
consider hypothesis class to be singletons, in which both the VC dimension and Littlestone dimension
are 1. Therefore, they cannot be candidates to characterize learnability in the strategic setting."
DISCUSSION AND OPEN PROBLEMS,0.15706214689265538,"Agnostic setting
We primarily concentrate on the realizable setting in this work. However, inves-
tigating the sample complexity and regret bounds in the agnostic setting would be an interesting
avenue for future research."
DISCUSSION AND OPEN PROBLEMS,0.15819209039548024,Acknowledgements
DISCUSSION AND OPEN PROBLEMS,0.15932203389830507,"This work was supported in part by the National Science Foundation under grant CCF-2212968, by
the Simons Foundation under the Simons Collaboration on the Theory of Algorithmic Fairness, by the"
DISCUSSION AND OPEN PROBLEMS,0.16045197740112993,"Defense Advanced Research Projects Agency under cooperative agreement HR00112020003. The
views expressed in this work do not necessarily reflect the position or the policy of the Government and
no official endorsement should be inferred. Approved for public release; distribution is unlimited."
REFERENCES,0.1615819209039548,References
REFERENCES,0.16271186440677965,"Ahmadi, S., Beyhaghi, H., Blum, A., and Naggita, K. (2021). The strategic perceptron. In Proceedings
of the 22nd ACM Conference on Economics and Computation, pages 6–25."
REFERENCES,0.1638418079096045,"Ahmadi, S., Beyhaghi, H., Blum, A., and Naggita, K. (2022). On classification of strategic agents
who can both game and improve. arXiv preprint arXiv:2203.00124."
REFERENCES,0.16497175141242937,"Ahmadi, S., Blum, A., and Yang, K. (2023). Fundamental bounds on online strategic classification.
arXiv preprint arXiv:2302.12355."
REFERENCES,0.16610169491525423,"Bechavod, Y., Ligett, K., Wu, S., and Ziani, J. (2021). Gaming helps! learning from strategic
interactions in natural dynamics. In International Conference on Artificial Intelligence and
Statistics, pages 1234–1242. PMLR."
REFERENCES,0.1672316384180791,"Brückner, M. and Scheffer, T. (2011). Stackelberg games for adversarial prediction problems. In
Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data
mining, pages 547–555."
REFERENCES,0.16836158192090395,"Chen, Y., Liu, Y., and Podimata, C. (2020). Learning strategy-aware linear classifiers. Advances in
Neural Information Processing Systems, 33:15265–15276."
REFERENCES,0.1694915254237288,"Dalvi, N., Domingos, P., Sanghai, S., and Verma, D. (2004). Adversarial classification. In Proceedings
of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,
pages 99–108."
REFERENCES,0.17062146892655367,"Dong, J., Roth, A., Schutzman, Z., Waggoner, B., and Wu, Z. S. (2018). Strategic classification from
revealed preferences. In Proceedings of the 2018 ACM Conference on Economics and Computation,
pages 55–70."
REFERENCES,0.17175141242937852,"Gallant, S. I. (1986). Optimal linear discriminants. Eighth International Conference on Pattern
Recognition, pages 849–852."
REFERENCES,0.17288135593220338,"Haghtalab, N., Immorlica, N., Lucier, B., and Wang, J. Z. (2020). Maximizing welfare with incentive-
aware evaluation mechanisms. arXiv preprint arXiv:2011.01956."
REFERENCES,0.17401129943502824,"Haghtalab, N., Lykouris, T., Nietert, S., and Wei, A. (2022). Learning in stackelberg games with
non-myopic agents. In Proceedings of the 23rd ACM Conference on Economics and Computation,
pages 917–918."
REFERENCES,0.1751412429378531,"Hardt, M., Megiddo, N., Papadimitriou, C., and Wootters, M. (2016). Strategic classification. In
Proceedings of the 2016 ACM conference on innovations in theoretical computer science, pages
111–122."
REFERENCES,0.17627118644067796,"Hu, L., Immorlica, N., and Vaughan, J. W. (2019). The disparate effects of strategic manipulation. In
Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 259–268."
REFERENCES,0.17740112994350282,"Jagadeesan, M., Mendler-Dünner, C., and Hardt, M. (2021). Alternative microfoundations for
strategic classification. In International Conference on Machine Learning, pages 4687–4697.
PMLR."
REFERENCES,0.17853107344632768,"Kleinberg, J. and Raghavan, M. (2020). How do classifiers induce agents to invest effort strategically?
ACM Transactions on Economics and Computation (TEAC), 8(4):1–23."
REFERENCES,0.17966101694915254,"Lechner, T. and Urner, R. (2022). Learning losses for strategic classification. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 36, pages 7337–7344."
REFERENCES,0.1807909604519774,"Lechner, T., Urner, R., and Ben-David, S. (2023). Strategic classification with unknown user
manipulations."
REFERENCES,0.18192090395480226,"Milli, S., Miller, J., Dragan, A. D., and Hardt, M. (2019). The social cost of strategic classification.
In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 230–239."
REFERENCES,0.18305084745762712,"Montasser, O., Hanneke, S., and Srebro, N. (2019). Vc classes are adversarially robustly learnable,
but only improperly. In Conference on Learning Theory, pages 2512–2530. PMLR."
REFERENCES,0.18418079096045198,"Rajaraman, N., Han, Y., Jiao, J., and Ramchandran, K. (2023). Beyond ucb: Statistical complexity
and optimal algorithms for non-linear ridge bandits. arXiv preprint arXiv:2302.06025."
REFERENCES,0.18531073446327684,"Sundaram, R., Vullikanti, A., Xu, H., and Yao, F. (2021). Pac-learning for strategic classification. In
International Conference on Machine Learning, pages 9978–9988. PMLR."
REFERENCES,0.1864406779661017,"Zhang, H. and Conitzer, V. (2021). Incentive-aware pac learning. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 35, pages 5797–5804."
REFERENCES,0.18757062146892656,"Zrnic, T., Mazumdar, E., Sastry, S., and Jordan, M. (2021). Who leads and who follows in strategic
classification? Advances in Neural Information Processing Systems, 34:15257–15269."
REFERENCES,0.18870056497175142,"A
Technical Lemmas"
REFERENCES,0.18983050847457628,"A.1
Boosting expected guarantee to high probability guarantee"
REFERENCES,0.19096045197740114,"Consider any (possibly randomized) PAC learning algorithm A in strategic setting, which can output
a predictor A(S) after T steps of interaction with i.i.d. agents S ∼DT s.t. E [Lstr(A(S))] ≤ε,
where the expectation is taken over both the randomness of S and the randomness of algorithm. One
standard way in classic PAC learning of boosting the expected loss guarantee to high probability loss
guarantee is: running A on new data S and verifying the loss of A(S) on a validation data set; if the
validation loss is low, outputting the current A(S), and repeating this process otherwise."
REFERENCES,0.192090395480226,"We will adopt this method to boost the confidence as well. The only difference in our strategic setting
is that we can not re-use validation data set as we are only allowed to interact with the data through
the interaction protocol. Our boosting scheme is described in the following."
REFERENCES,0.19322033898305085,"• For round r = 1, . . . , R,"
REFERENCES,0.1943502824858757,"– Run A for T steps of interactions to obtain a predictor hr.
– Apply hr for the following m0 rounds to obtain the empirical strategic loss on m0,
denoted as blr =
1
m0
Ptr+m0
t=tr+1 ℓstr(hr, (xt, rt, yt)), where tr + 1 is the starting time of
these m0 rounds.
– Break and output hr if blr ≤4ε."
REFERENCES,0.19548022598870057,"• If for all r ∈[R], blr > 4ε, output an arbitrary hypothesis."
REFERENCES,0.19661016949152543,"Lemma 2. Given an algorithm A, which can output a predictor A(S) after T steps of interaction with
i.i.d. agents S ∼DT s.t. the expected loss satisfies E [Lstr(A(S))] ≤ε. Let hA denote the output of
the above boosting scheme given algorithm A as input. By setting R = log 2"
REFERENCES,0.1977401129943503,δ and m0 = 3 log(4R/δ)
REFERENCES,0.19887005649717515,"2ε
,
we have Lstr(hA) ≤8ε with probability 1 −δ. The total sample size is R(T + m0) = O(log( 1"
REFERENCES,0.2,"δ )(T +
log(1/δ) ε
))."
REFERENCES,0.20112994350282487,"Proof. For all r = 1, . . . , R, we have E [Lstr(hr)] ≤ε. By Markov’s inequality, we have"
REFERENCES,0.20225988700564973,Pr(Lstr(hr) > 2ε) ≤1 2 .
REFERENCES,0.2033898305084746,"For any fixed hr, if Lstr(hr) ≥8ε, we will have blr ≤4ε with probability ≤e−m0ε; if Lstr(hr) ≤2ε,
we will have blr ≤4ε with probability ≥1 −e−2m0ε/3 by Chernoff bound."
REFERENCES,0.20451977401129945,"Let E denote the event of {∃r ∈[R], Lstr(hr) ≤2ε} and F denote the event of {blr > 4ε for all
r ∈[R]}. When F does not hold, our boosting will output hr for some r ∈[R]."
REFERENCES,0.20564971751412428,Pr(Lstr(hA) > 8ε)
REFERENCES,0.20677966101694914,"≤Pr(E, ¬F) Pr(Lstr(hA) > 8ε|E, ¬F) + Pr(E, F) + Pr(¬E) ≤ R
X"
REFERENCES,0.207909604519774,"r=1
Pr(hA = hr, Lstr(hr) > 8ε|E, ¬F) + Pr(E, F) + Pr(¬E)"
REFERENCES,0.20903954802259886,≤Re−m0ε + e−2m0ε/3 + 1
R,0.21016949152542372,"2R
≤δ ,"
R,0.21129943502824858,by setting R = log 2
R,0.21242937853107344,"δ and m0 = 3 log(4R/δ) 2ε
."
R,0.2135593220338983,"A.2
Converting mistake bound to PAC bound"
R,0.21468926553672316,"In any setting of (C, F), if there is an algorithm A that can achieve the mistake bound of B, then we
can convert A to a conservative algorithm by not updating at correct rounds. The new algorithm can
still achieve mistake bound of B as A still sees a legal sequence of examples. Given any conservative
online algorithm, we can convert it to a PAC learning algorithm using the standard longest survivor
technique (Gallant, 1986)."
R,0.21581920903954802,"Lemma 3. In any setting of (C, F), given any conservative algorithm A with mistake bound B, let
algorithm A′ run A and output the first ft which survives over 1"
R,0.21694915254237288,ε log( B
R,0.21807909604519773,"δ ) examples. A′ can achieve
sample complexity of O( B"
R,0.2192090395480226,ε log( B δ )).
R,0.22033898305084745,Proof of Lemma 3. When the sample size m ≥B
R,0.2214689265536723,ε log( B
R,0.22259887005649717,"δ ), the algorithm A will produce at most B
different hypotheses and there must exist one surviving for 1"
R,0.22372881355932203,ε log( B
R,0.2248587570621469,"δ ) rounds since A is a conservative
algorithm with at most B mistakes. Let h1, . . . , hB denote these hypotheses and let t1, . . . , tB denote
the time step they are produced. Then we have"
R,0.22598870056497175,"Pr(fout = hi and Lstr(hi) > ε) = E

Pr(fout = hi and Lstr(hi) > ε|ti, z1:ti−1)
"
R,0.2271186440677966,"<E
h
(1 −ε)
1
ε log( B"
R,0.22824858757062147,"δ )i
= δ B ."
R,0.22937853107344633,"By union bound, we have"
R,0.2305084745762712,"Pr(Lstr(fout) > ε) ≤ B
X"
R,0.23163841807909605,"i=1
Pr
z1:T(fout = hi and Lstr(hi) > ε) < δ."
R,0.2327683615819209,We are done.
R,0.23389830508474577,"A.3
Smooth the distribution"
R,0.23502824858757063,"Lemma 4. For any two data distribution D1 and D2, let D3 = (1 −p)D1 + pD2 be the
mixture of them.
For any setting of (C, F) and any algorithm, let PD be the dynamics of
(C(x1), f1, y1, by1, F(x1, ∆1), . . . , C(xT ), fT , yT , byT , F(xT , ∆T )) under the data distribution D.
Then for any event A, we have |PD3(A) −PD1(A)| ≤2pT."
R,0.23615819209039549,"Proof. Let B denote the event of all (xt, ut, yt)T
t=1 being sampled from D1. Then PD3(¬B) ≤pT.
Then"
R,0.23728813559322035,"PD3(A) = PD3(A|B)PD3(B) + PD3(A|¬B)PD3(¬B)
= PD1(A)PD3(B) + PD3(A|¬B)PD3(¬B)
= PD1(A)(1 −PD3(¬B)) + PD3(A|¬B)PD3(¬B) ."
R,0.2384180790960452,"By re-arranging terms, we have"
R,0.23954802259887006,|PD1(A) −PD3(A)| = |PD1(A)PD3(¬B) −PD3(A|¬B)PD3(¬B)| ≤2pT .
R,0.24067796610169492,"B
Proof of Theorem 1"
R,0.24180790960451978,"Proof. When a mistake occurs, there are two cases."
R,0.24293785310734464,"• If ft misclassifies a true positive example (xt, rt, +1) by negative, we know that d(xt, ft) >
rt while the target hypothesis h∗must satisfy that d(xt, h∗) ≤rt. Then any h ∈VS with
d(xt, h) ≥d(xt, ft) cannot be h∗and are eliminated. Since d(xt, ft) is the median of
{d(xt, h)|h ∈VS}, we can eliminate half of the version space."
R,0.2440677966101695,"• If ft misclassifies a true negative example (xt, rt, −1) by positive, we know that d(xt, ft) ≤
rt while the target hypothesis h∗must satisfy that d(xt, h∗) > rt. Then any h ∈VS with
d(xt, h) ≤d(xt, ft) cannot be h∗and are eliminated. Since d(xt, ft) is the median of
{d(xt, h)|h ∈VS}, we can eliminate half of the version space."
R,0.24519774011299436,"Each mistake reduces the version space by half and thus, the algorithm of Strategic Halving suffers at
most log2(|H|) mistakes."
R,0.24632768361581922,"C
Proof of Theorem 2"
R,0.24745762711864408,"Proof. In online learning setting, an algorithm is conservative if it updates it’s current predictor
only when making a mistake. It is straightforward to check that Strategic Halving is conservative.
Combined with the technique of converting mistake bound to PAC bound in Lemma 3, we prove
Theorem 2."
R,0.24858757062146894,"D
Proof of Theorem 3"
R,0.2497175141242938,"Proof. Consider the feature space X = {0, e1, . . . , en, 0.9e1, . . . , 0.9en}, where ei’s are standard
basis vectors in Rn and metric d(x, x′) = ∥x −x′∥2 for all x, x′ ∈X. Let the hypothesis class be a
set of singletons over {ei|i ∈[n]}, i.e., H = {21{ei} −1|i ∈[n]}. We divide all possible hypotheses
(not necessarily in H) into three categories:"
R,0.25084745762711863,"• The hypothesis 21∅−1, which predicts all negative."
R,0.2519774011299435,"• For each x ∈{0, 0.9e1, . . . , 0.9en}, let Fx,+ denote the class of hypotheses h predicting x
as positive."
R,0.25310734463276835,"• For each i ∈[n], let Fi denote the class of hypotheses h satisfying h(x) = −1 for all
x ∈{0, 0.9e1, . . . , 0.9en} and h(ei) = +1. And let F∗= ∪i∈[n]Fi denote the union of
them."
R,0.2542372881355932,Note that all hypotheses over X fall into one of the three categories.
R,0.25536723163841807,"Now we consider a set of adversaries E1, . . . , En, such that the target function in the adversarial
environment Ei is 21{ei} −1. We allow the learners to be randomized and thus, at round t, the
learner draws an ft from a distribution D(ft) over hypotheses. The adversary, who only knows the
distribution D(ft) but not the realization ft, picks an agent (xt, rt, yt) in the following way."
R,0.25649717514124293,"• Case 1: If there exists x ∈{0, 0.9e1, . . . , 0.9en} such that Prft∼D(ft)(ft ∈Fx,+) ≥c for
some c > 0, then for all j ∈[n], the adversary Ej picks (xt, rt, yt) = (x, 0, −1). Let Bt
1,x
denote the event of ft ∈Fx,+."
R,0.2576271186440678,"– In this case, the learner will make a mistake with probability c. Since for all h ∈H,
h(∆(x, h, 0)) = h(x) = −1, they are all consistent with (x, 0, −1)."
R,0.25875706214689265,"• Case 2: If Prft∼D(ft)(ft = 21∅−1) ≥c, then for all j ∈[n], the adversary Ej picks
(xt, rt, yt) = (0, 1, +1). Let Bt
2 denote the event of ft = 21∅−1."
R,0.2598870056497175,"– In this case, with probability c, the learner will sample a ft = 21∅−1 and misclassify
(0, 1, +1). Since for all h ∈H, h(∆(0, h, 1)) = +1, they are all consistent with
(0, 1, +1)."
R,0.26101694915254237,"• Case 3: If the above two cases do not hold, let it = arg maxi∈[n] Pr(ft(ei) = 1|ft ∈F∗),
xt = 0.9eit. For radius and label, different adversaries set them differently. Adversary Eit
will set (rt, yt) = (0, −1) while other Ej for j ̸= it will set (rt, yt) = (0.1, −1). Since
Cases 1 and 2 do not hold, we have Prft∼D(ft)(ft ∈F∗) ≥1 −(n + 2)c. Let Bt
3 denote
the event of ft ∈F∗and Bt
3,i denote the event of ft ∈Fi."
R,0.2621468926553672,"(a) With probability Pr(Bt
3,it) ≥
1
n Pr(Bt
3) ≥
1−(n+2)c"
R,0.2632768361581921,"n
, the learner samples a ft ∈
Fit, and thus misclassifies (0.9eit, 0.1, −1) in Ej for j ̸= it but correctly classifies
(0.9eit, 0, −1). In this case, the learner observes the same feedback in all Ej for j ̸= it
and identifies the target function 21{eit} −1 in Eit.
(b) If the learner samples a ft with ft(eit) = ft(0.9eit) = −1, then the learner observes
xt = 0.9eit, yt = −1 and byt = −1 in all Ej for j ∈[n]. Therefore the learner cannot
distinguish between adversaries in this case.
(c) If the learner samples a ft with ft(0.9eit) = +1, then the learner observes xt = 0.9eit,
yt = −1 and byt = +1 in all Ej for j ∈[n]. Again, since the feedback are identical in
all Ej and the learner cannot distinguish between adversaries in this case."
R,0.26440677966101694,"For any learning algorithm A, his predictions are identical in all of adversarial environments {Ej|j ∈
[n]} before he makes a mistake in Case 3(a) in one environment Eit. His predictions in the following
rounds are identical in all of adversarial environments {Ej|j ∈[n]} \ {Eit} before he makes
another mistake in Case 3(a). Suppose that we run A in all adversarial environment of {Ej|j ∈[n]}
simultaneously. Note that once we make a mistake, the mistake must occur simultaneously in at
least n −1 environments. Specifically, if we make a mistake in Case 1, 2 or 3(c), such a mistake
simultaneously occur in all n environments. If we make a mistake in Case 3(a), such a mistake
simultaneously occur in all n environments except Eit. Since we will make a mistake with probability
at least min(c, 1−(n+2)c"
R,0.2655367231638418,"n
) at each round, there exists one environment in {Ej|j ∈[n]} in which A
will make n −1 mistakes."
R,0.26666666666666666,"Now we lower bound the number of mistakes dependent on T. Let t1, t2, . . . denote the time steps in
which we makes a mistake. Let t0 = 0 for convenience. Now we prove that"
R,0.2677966101694915,Pr(ti > ti−1 + k|ti−1) =
R,0.2689265536723164,"ti−1+k
Y"
R,0.27005649717514124,"τ=ti−1+1
Pr(we don’t make a mistake in round τ) ≤"
R,0.2711864406779661,"ti−1+k
Y"
R,0.27231638418079096,"τ=ti−1+1
(1(Case 3 at round τ)(1 −1 −(n + 2)c"
R,0.2734463276836158,"n
) + 1(Case 1 or 2 at round τ)(1 −c))"
R,0.2745762711864407,≤(1 −min(1 −(n + 2)c
R,0.27570621468926554,"n
, c))k ≤(1 −
1
2(n + 2))k ,"
R,0.2768361581920904,"by setting c =
1
2(n+2). Then by letting k = 2(n + 2) ln(n/δ), we have"
R,0.27796610169491526,Pr(ti > ti−1 + k|ti−1) ≤δ/n .
R,0.2790960451977401,"For any T,"
R,0.280225988700565,"Pr(# of mistakes < min(
T
k + 1, n −1))"
R,0.28135593220338984,"=≤Pr(∃i ∈[n −1], ti −ti−1 > k) ≤ n−1
X"
R,0.2824858757062147,"i=1
Pr(ti −ti−1 > k) ≤δ ."
R,0.28361581920903955,"Therefore, we have proved that for any T, with probability at least 1 −δ, we will make at least
min(
T
2(n+2) ln(n/δ)+1, n −1) mistakes."
R,0.2847457627118644,"E
Proof of Theorem 4"
R,0.2858757062146893,Algorithm 3 MWMR (Multiplicative Weights on Mistake Rounds)
R,0.28700564971751413,"1: Initialize the version space VS = H.
2: for t=1,...,T do
3:
Pick one hypotheses ft from VS uniformly at random.
4:
if byt ̸= yt and yt = + then
5:
VS ←VS \ {h ∈VS|d(xt, h) ≥d(xt, ft)}.
6:
else if byt ̸= yt and yt = −then
7:
VS ←VS \ {h ∈VS|d(xt, h) ≤d(xt, ft)}.
8:
end if
9: end for"
R,0.288135593220339,"Proof. First, when the algorithm makes a mistake at round t, he can at least eliminate ft. Therefore,
the total number of mistakes will be upper bounded by |H| −1."
R,0.28926553672316385,"Let pt denote the fraction of hypotheses misclassifying xt. We say a hypothesis h is inconsistent
with (xt, ft, yt, byt) iff (d(xt, h) ≥d(xt, ft) ∧byt = −∧yt = +) or (d(xt, h) ≤d(xt, ft) ∧byt =
+ ∧yt = −). Then we define the following events."
R,0.2903954802259887,• Et denotes the event that MWMR makes a mistake at round t. We have Pr(Et) = pt.
R,0.29152542372881357,"• Bt denotes the event that at least
pt"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.29265536723163843,"2
fraction of hypotheses are inconsistent with
(xt, ft, yt, byt). We have Pr(Bt|Et) ≥1 2."
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.2937853107344633,"Let n = |H| denote the cardinality of hypothesis class and nt denote the number of hypotheses in
VS after round t. Then we have"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.29491525423728815,"1 ≤nT = n · T
Y"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.296045197740113,"t=1
(1 −1(Et)1(Bt)pt 2 ) ."
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.29717514124293787,"By taking logarithm of both sides, we have"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.2983050847457627,"0 ≤ln(nT ) = ln(n) + T
X"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.2994350282485876,"t=1
ln(1 −1(Et)1(Bt)pt"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.30056497175141245,"2 ) ≤ln(n) − T
X"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3016949152542373,"t=1
1(Et)1(Bt)pt 2 ,"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.30282485875706217,"where the last inequality adopts ln(1 −x) ≤−x for x ∈[0, 1). Then by taking expectation of both
sides, we have"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.303954802259887,"0 ≤ln(n) − T
X"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3050847457627119,"t=1
Pr(Et ∧Bt)pt 2 ."
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.30621468926553674,Since Pr(Et) = pt and Pr(Bt|Et) ≥1
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3073446327683616,"2, then we have 1
4 T
X"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.30847457627118646,"t=1
p2
t ≤ln(n) ."
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3096045197740113,Then we have the expected number of mistakes E [MMWMR(T)] as
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3107344632768362,"E [MMWMR(T)] = T
X"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.31186440677966104,"t=1
pt ≤"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3129943502824859,"v
u
u
t T
X"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.31412429378531076,"t=1
p2
t ·
√ T ≤
p"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3152542372881356,"4 ln(n)T ,"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3163841807909605,where the first inequality applies Cauchy-Schwarz inequality.
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.31751412429378534,"F
Proof of Theorem 5"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.31864406779661014,"Proof. Construction of Q, H and a set of realizable distributions"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.319774011299435,"• Let feature space X = {0, e1, . . . , en} ∪X0, where X0 = { σ(0,1,...,n−1)"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.32090395480225986,"z
|σ ∈Sn} with z =
√"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3220338983050847,12+...+(n−1)2
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3231638418079096,"α
for some small α = 0.1. Here Sn is the set of all permutations
over n elements. So X0 is the set of points whose coordinates are a permutation of
{0, 1/z, . . . , (n −1)/z} and all points in X0 have the ℓ2 norm equal to α. Define a metric
d by letting d(x1, x2) = ∥x1 −x2∥2 for all x1, x2 ∈X. Then for any x ∈X0 and"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.32429378531073444,"i ∈[n], d(x, ei) = ∥x −ei∥2 =
q"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3254237288135593,(xi −1)2 + P
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.32655367231638416,"j̸=i x2
j =
q"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.327683615819209,"1 + Pn
j=1 x2
j −2xi =
√"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3288135593220339,"1 + α2 −2xi. Note that we consider space (X, d) rather than (Rn, ∥·∥2)."
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.32994350282485874,"• Let the hypothesis class be a set of singletons over {ei|i ∈[n]}, i.e., H = {21{ei} −1|i ∈
[n]}."
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3310734463276836,"• We now define a collection of distributions {Di|i ∈[n]} in which Di is realized by 21{ei}−1.
For any i ∈[n], Di puts probability mass 1 −3nε on (0, 0, −1). For the remaining 3nε
probability mass, Di picks x uniformly at random from X0 and label it as positive. If xi = 0,"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.33220338983050846,"set radius r(x) = ru :=
√"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3333333333333333,"1 + α2; otherwise, set radius r(x) = rl :=
q"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3344632768361582,1 + α2 −2 · 1
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.33559322033898303,"z).
Hence, X0 are all labeled as positive. For j ̸= i, hj = 21{ej} −1 labels {x ∈X0|xj = 0}
negative since r(x) = rl and d(x, hj) = ru > r(x). Therefore, Lstr(hj) = 1"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3367231638418079,"n · 3nε = 3ε.
To output fout ∈H, we must identify the true target function."
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.33785310734463275,"Information gain from different choices of ft Let h∗= 21{ei∗} −1 denote the target function.
Since (0, 0, −1) is realized by all hypotheses, we can only gain information about the target function
when xt ∈X0. For any xt ∈X0, if d(xt, ft) ≤rl or d(xt, ft) > ru, we cannot learn anything about
the target function. In particular, if d(xt, ft) ≤rl, the learner will observe xt ∼Unif(X0), yt = +1,
byt = +1 in all {Di|i ∈[n]}. If d(xt, ft) > ru, the learner will observe xt ∼Unif(X0), yt = +1,
byt = −1 in all {Di|i ∈[n]}. Therefore, we cannot obtain any information about the target function."
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3389830508474576,"Now for any xt ∈X0, with the it-th coordinate being 0, we enumerate the distance between x and x′
for all x′ ∈X."
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.34011299435028247,"• For all x′ ∈X0, d(x, x′) ≤∥x∥+ ∥x′∥≤2α < rl;"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.34124293785310733,"• For all j ̸= it, d(x, ej) =
p"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3423728813559322,1 + α2 −2xj ≤rl;
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.34350282485875705,"• d(x, eit) = ru;"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3446327683615819,"• d(x, 0) = α < rl."
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.34576271186440677,"Only ft = 21{eit} −1 satisfies that rl < d(xt, ft) ≤ru and thus, we can only obtain information
when ft = 21{eit} −1. And the only information we learn is whether it = i∗because if it ̸= i∗, no
matter which i∗is, our observation is identical. If it ̸= i∗, we can eliminate 21{eit} −1."
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.34689265536723163,"Sample size analysis For any algorithm A, his predictions are identical in all environments {Di|i ∈
[n]} before a round t in which ft = 21{eit} −1. Then either he learns it in Dit or he eliminates
21{eit} −1 and continues to perform the same in the other environments {Di|i ̸= it}. Suppose
that we run A in all stochastic environments {Di|i ∈[n]} simultaneously. When we identify it in
environment Dit, we terminate A in Dit. Consider a good algorithm A which can identify i in Di
with probability 7"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3480225988700565,"8 after T rounds of interaction for each i ∈[n], that is,"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.34915254237288135,"Pr
Di,A(iout ̸= i) ≤1"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3502824858757062,"8, ∀i ∈[n] .
(3)"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.35141242937853107,"Therefore, we have
X"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3525423728813559,"i∈[n]
Pr
Di,A(iout ̸= i) ≤n"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3536723163841808,"8 .
(4)"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.35480225988700564,"Let nT denote the number of environments that have been terminated by the end of round T. Let
Bt denote the event of xt being in X0 and Ct denote the event of ft = 21{eit} −1. Then we have
Pr(Bt) = 3nε and Pr(Ct|Bt) = 1"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3559322033898305,"n, and thus Pr(Bt ∧Ct) = 3nε · 1"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.35706214689265536,"n. Since at each round, we can
eliminate one environment only when Bt ∧Ct is true, then we have"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3581920903954802,"E [nT ] ≤E "" T
X"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3593220338983051,"t=1
1(Bt ∧Ct) #"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.36045197740112994,= T · 3nε · 1
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3615819209039548,n = 3εT .
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.36271186440677966,"Therefore, by setting T = ⌊n 2 ⌋−1"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3638418079096045,"6ε
and Markov’s inequality, we have"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3649717514124294,"Pr(nT ≥
jn 2"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.36610169491525424,"k
−1) ≤
3εT
 n"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3672316384180791,"2

−1 = 1 2 ."
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.36836158192090396,"When there are
 n"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3694915254237288,"2

+ 1 environments remaining, the algorithm has to pick one iout, which fails in at
least
 n"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3706214689265537,"2

of the environments. Then we have
X"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.37175141242937854,"i∈[n]
Pr
Di,A(iout ̸= i) ≥
ln 2"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3728813559322034,"m
Pr(nT ≤
jn 2"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.37401129943502825,"k
−1) ≥n 4 ,"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3751412429378531,"which conflicts with Eq (4). Therefore, for any algorithm A, to achieve Eq (3), it requires T ≥
⌊n 2 ⌋−1 6ε
."
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.376271186440678,"G
Proof of Theorem 6"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.37740112994350283,"Given Lemma 1, we can upper bound the expected strategic loss, then we can boost the confidence of
the algorithm through the scheme in Section A.1. Theorem 6 follows by combining Lemma 1 and
Lemma 2. Now we only need to prove Lemma 1."
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3785310734463277,"Proof of Lemma 1. For any set of hypotheses H, for every z = (x, r, y), we define"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.37966101694915255,"κp(H, z) :=
|{h ∈H|h(∆(x, h, r)) = −}|
if y = + ,
0
otherwise."
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.3807909604519774,"So κp(H, z) is the number of hypotheses mislabeling z for positive z’s and 0 for negative z’s.
Similarly, we define κn as follows,"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.38192090395480227,"κn(H, z) :=
|{h ∈H|h(∆(x, h, r)) = +}|
if y = −,
0
otherwise."
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.38305084745762713,"So κn(H, z) is the number of hypotheses mislabeling z for negative z’s and 0 for positive z’s."
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.384180790960452,"In the following, we divide the proof into two parts. First, recall that in Algorithm 2, the output
is constructed by randomly sampling two hypotheses with replacement and taking the union of
them. We represent the loss of such a random predictor using κp(H, z) and κn(H, z) defined above.
Then we show that whenever the algorithm makes a mistake, with some probability, we can reduce
κp(VSt−1,zt)"
FRACTION OF HYPOTHESES ARE INCONSISTENT WITH,0.38531073446327685,"2
or κn(VSt−1,zt)"
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.3864406779661017,"2
hypotheses and utilize this to provide a guarantee on the loss of the final
output."
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.38757062146892657,"Upper bounds on the strategic loss
For any hypothesis h, let fpr(h) and fnr(h) denote the false
positive rate and false negative rate of h respectively. Let p+ denote the probability of drawing
a positive sample from D, i.e., Pr(x,r,y)∼D(y = +) and p−denote the probability of drawing a
negative sample from D. Let D+ and D−denote the data distribution conditional on that the label
is positive and that the label is negative respectively. Given any set of hypotheses H, we define a
random predictor R2(H) = h1 ∨h2 with h1, h2 randomly picked from H with replacement. For a
true positive z, R2(H) will misclassify it with probability κp(H,z)2"
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.3887005649717514,"|H|2
. Then we can find that the false
negative rate of R2(H) is"
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.3898305084745763,"fnr(R2(H)) = Ez=(x,r,+)∼D+ [Pr(R2(H)(x) = −)] = Ez=(x,r,+)∼D+"
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.39096045197740115,"""
κp(H, z)2 |H|2 # ."
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.392090395480226,"Similarly, for a true negative z, R2(H) will misclassify it with probability 1 −(1 −κn(H,z)"
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.39322033898305087,"|H|
)2 ≤"
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.3943502824858757,"2κn(H,z)"
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.3954802259887006,"|H|
. Then the false positive rate of R2(H) is"
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.39661016949152544,"fpr(R2(H)) = Ez=(x,r,−)∼D−[Pr(R2(H)(x) = +)] ≤Ez=(x,r,−)∼D+"
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.3977401129943503,"2κn(H, z) |H| 
."
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.39887005649717516,Hence the loss of R2(H) is
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.4,Lstr(R2(H)) ≤p+Ez∼D+
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.4011299435028249,"""
κp(H, z)2 |H|2 #"
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.40225988700564974,+ p−Ez∼D+
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.4033898305084746,"2κn(H, z) |H| "
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.40451977401129946,= Ez∼D
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.4056497175141243,"""
κp(H, z)2"
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.4067796610169492,"|H|2
+ 2κn(H, z) |H| # ,
(5)"
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.40790960451977404,"where the last equality holds since κp(H, z) = 0 for true negatives and κn(H, z) = 0 for true
positives."
HYPOTHESES AND UTILIZE THIS TO PROVIDE A GUARANTEE ON THE LOSS OF THE FINAL,0.4090395480225989,"Loss analysis
In each round, the data zt = (xt, rt, yt) is sampled from D. When the label yt is posi-
tive, if the drawn ft satisfying that 1) ft(∆(xt, ft, rt)) = −and 2) d(xt, ft) ≤median({d(xt, h)|h ∈
VSt−1, h(∆(xt, h, rt)) = −}), then we are able to remove κp(VSt−1,zt)"
HYPOTHESES FROM THE VERSION,0.4101694915254237,"2
hypotheses from the version"
HYPOTHESES FROM THE VERSION,0.41129943502824856,"space. Let Ep,t denote the event of ft satisfying the conditions 1) and 2). With probability
1
⌊log2(nt)⌋,"
HYPOTHESES FROM THE VERSION,0.4124293785310734,"we sample kt = 1. Then we sample an ft ∼Unif(VSt−1). With probability κp(VSt−1,zt)"
NT,0.4135593220338983,"2nt
, the
sampled ft satisfies the two conditions. So we have"
NT,0.41468926553672314,"Pr(Ep,t|zt, VSt−1) ≥
1
log2(nt)
κp(VSt−1, zt)"
NT,0.415819209039548,"2nt
.
(6)"
NT,0.41694915254237286,"The case of yt being negative is similar to the positive case. Let En,t denote the event of ft satisfying
that 1) ft(∆(xt, ft, rt)) = + and 2) d(xt, ft) ≥median({d(xt, h)|h ∈VSt−1, h(∆(xt, h, rt)) =
+}). If κn(VSt−1, zt) ≥
nt"
NT,0.4180790960451977,"2 , then with probability
1
⌊log2(nt)⌋, we sample kt = 1. Then with
probability greater than 1"
NT,0.4192090395480226,"4 we will sample an ft satisfying that 1) ft(∆(xt, ft, rt)) = + and 2)
d(xt, ft) ≥median({d(xt, h)|h ∈VSt−1, h(∆(xt, h, rt)) = +}). If κn(VSt−1, zt) < nt"
NT,0.42033898305084744,"2 , then
with probability
1
⌊log2(nt)⌋, we sampled a kt satisfying"
NT,0.4214689265536723,"nt
4κn(VSt−1, zt) < kt ≤
nt
2κn(VSt−1, zt) ."
NT,0.42259887005649716,"Then we randomly sample kt hypotheses and the expected number of sampled hypotheses which
mislabel zt is kt · κn(VSt−1,zt)"
NT,0.423728813559322,"nt
∈( 1 4, 1"
NT,0.4248587570621469,"2]. Let gt (given the above fixed kt) denote the number of
sampled hypotheses which mislabel xt and we have E [gt] ∈( 1 4, 1"
NT,0.42598870056497173,"2]. When gt > 0, ft will misclassify
zt by positive. We have"
NT,0.4271186440677966,"Pr(gt = 0) = (1 −κn(VSt−1, zt)"
NT,0.42824858757062145,"nt
)kt < (1 −κn(VSt−1, zt) nt
)"
NT,0.4293785310734463,"nt
4κn(VSt−1,zt) ≤e−1/4 ≤0.78"
NT,0.43050847457627117,"and by Markov’s inequality, we have"
NT,0.43163841807909603,"Pr(gt ≥3) ≤E [gt] 3
≤1"
NT,0.4327683615819209,6 ≤0.17 .
NT,0.43389830508474575,"Thus Pr(gt ∈{1, 2}) ≥0.05. Conditional on gt is either 1 or 2, with probability ≥1"
NT,0.4350282485875706,"4, all of these
gt hypotheses h′ satisfies d(xt, h′) ≥median({d(xt, h)|h ∈VSt−1, h(∆(xt, h, rt)) = +}), which
implies that d(xt, ft) ≥median({d(xt, h)|h ∈VSt−1, h(∆(xt, h, rt)) = +}). Therefore, we have"
NT,0.43615819209039547,"Pr(En,t|zt, , VSt−1) ≥
1
80 log2(nt) .
(7)"
NT,0.43728813559322033,"Let vt denote the fraction of hypotheses we eliminated at round t, i.e., vt = 1 −nt+1"
NT,0.4384180790960452,"nt
. Then we have"
NT,0.43954802259887005,"vt ≥1(Ep,t)κp(VSt−1, zt)"
NT,0.4406779661016949,"2nt
+ 1(En,t)κn(VSt−1, zt)"
NT,0.44180790960451977,"2nt
.
(8)"
NT,0.4429378531073446,"Since nt+1 = nt(1 −vt), we have"
NT,0.4440677966101695,"1 ≤nT +1 = n T
Y"
NT,0.44519774011299434,"t=1
(1 −vt) ."
NT,0.4463276836158192,"By taking logarithm of both sides, we have"
NT,0.44745762711864406,"0 ≤ln nT +1 = ln n + T
X"
NT,0.4485875706214689,"t=1
ln(1 −vt) ≤ln n − T
X"
NT,0.4497175141242938,"t=1
vt ,"
NT,0.45084745762711864,"where we use ln(1 −x) ≤−x for x ∈[0, 1) in the last inequality. By re-arranging terms, we have T
X"
NT,0.4519774011299435,"t=1
vt ≤ln n ."
NT,0.45310734463276836,"Combined with Eq (8), we have T
X"
NT,0.4542372881355932,"t=1
1(Ep,t)κp(VSt−1, zt)"
NT,0.4553672316384181,"2nt
+ 1(En,t)κn(VSt−1, zt)"
NT,0.45649717514124294,"2nt
≤ln n ."
NT,0.4576271186440678,"By taking expectation w.r.t. the randomness of f1:T and dataset S = z1:T on both sides, we have T
X"
NT,0.45875706214689266,"t=1
Ef1:T ,z1:T"
NT,0.4598870056497175,"
1(Ep,t)κp(VSt−1, zt)"
NT,0.4610169491525424,"2nt
+ 1(En,t)κn(VSt−1, zt)"
NT,0.46214689265536724,2nt
NT,0.4632768361581921,"
≤ln n ."
NT,0.46440677966101696,"Since the t-th term does not depend on ft+1:T , zt+1:T and VSt−1 is determined by z1:t−1 and f1:t−1,
the t-th term becomes"
NT,0.4655367231638418,"Ef1:t,z1:t"
NT,0.4666666666666667,"
1(Ep,t)κp(VSt−1, zt)"
NT,0.46779661016949153,"2nt
+ 1(En,t)κn(VSt−1, zt)"
NT,0.4689265536723164,2nt 
NT,0.47005649717514125,"=Ef1:t−1,z1:t 
Eft"
NT,0.4711864406779661,"
1(Ep,t)κp(VSt−1, zt)"
NT,0.47231638418079097,"2nt
+ 1(En,t)κn(VSt−1, zt)"
NT,0.47344632768361583,"2nt
|f1:t−1, z1:t "
NT,0.4745762711864407,"=Ef1:t−1,z1:t"
NT,0.47570621468926555,"
Eft [1(Ep,t)|f1:t−1, z1:t] κp(VSt−1, zt)"
NT,0.4768361581920904,"2nt
+ Eft [1(En,t)|f1:t−1, z1:t] κn(VSt−1, zt)"
NT,0.47796610169491527,2nt  (9)
NT,0.47909604519774013,"≥Ef1:t−1,z1:t"
NT,0.480225988700565,"""
1
log2(nt)
κ2
p(VSt−1, zt)"
NT,0.48135593220338985,"4n2
t
+
1
80 log2(nt)
κn(VSt−1, zt)"
NT,0.4824858757062147,2nt #
NT,0.48361581920903957,",
(10)"
NT,0.4847457627118644,"where Eq (9) holds due to that VSt−1 is determined by f1:t−1, z1:t−1 and does not depend on ft
and Eq (10) holds since Prft(Ep,t|f1:t−1, z1:t) = Prft(Ep,t|VSt−1, zt) ≥
1
log2(nt)
κp(VSt−1,zt)"
"NT
BY",0.4858757062146893,"2nt
by
Eq (6) and Prft(En,t|f1:t−1, z1:t) = Prft(En,t|VSt−1, zt) ≥
1
80 log2(nt) by Eq (7). Thus, we have T
X"
"NT
BY",0.48700564971751414,"t=1
Ef1:t−1,z1:t"
"NT
BY",0.488135593220339,"""
1
log2(nt)
κ2
p(VSt−1, zt)"
"NT
BY",0.48926553672316386,"4n2
t
+
1
80 log2(nt)
κn(VSt−1, zt)"
NT,0.4903954802259887,2nt #
NT,0.4915254237288136,≤ln n .
NT,0.49265536723163844,"Since zt ∼D and zt is independent of z1:t−1 and f1:t−1, thus, we have the t-th term on the LHS
being"
NT,0.4937853107344633,"Ef1:t−1,z1:t"
NT,0.49491525423728816,"""
1
log2(nt)
κ2
p(VSt−1, zt)"
NT,0.496045197740113,"4n2
t
+
1
80 log2(nt)
κn(VSt−1, zt)"
NT,0.4971751412429379,2nt #
NT,0.49830508474576274,"=Ef1:t−1,z1:t−1 "" Ezt∼D"
NT,0.4994350282485876,"""
1
log2(nt)
κ2
p(VSt−1, zt)"
NT,0.5005649717514125,"4n2
t
+
1
80 log2(nt)
κn(VSt−1, zt)"
NT,0.5016949152542373,2nt ##
NT,0.5028248587570622,"≥
1
320 log2(n)Ef1:t−1,z1:t−1 "" Ez∼D"
NT,0.503954802259887,"""
κ2
p(VSt−1, z)"
NT,0.5050847457627119,"n2
t
+ 2κn(VSt−1, z) nt ##"
NT,0.5062146892655367,"≥
1
320 log2(n)Ef1:t−1,z1:t−1

Lstr(R2(VSt−1))

,"
NT,0.5073446327683616,"where the last inequality adopts Eq (5). By summing them up and re-arranging terms, we have"
NT,0.5084745762711864,"Ef1:T ,z1:T ""
1
T T
X"
NT,0.5096045197740113,"t=1
Lstr(R2(VSt−1)) # = 1 T T
X"
NT,0.5107344632768361,"t=1
Ef1:t−1,z1:t−1

Lstr(R2(VSt−1))

≤320 log2(n) ln(n) T
."
NT,0.511864406779661,"For the output of Algorithm 2, which randomly picks τ from [T], randomly samples h1, h2 from
VSτ−1 with replacement and outputs h1 ∨h2, the expected loss is"
NT,0.5129943502824859,"E

Lstr(A(S))

=ES,f1:T ""
1
T T
X"
NT,0.5141242937853108,"t=1
Eh1,h2∼Unif(VSt−1)

Lstr(h1 ∨h2)

#"
NT,0.5152542372881356,"=ES,f1:T ""
1
T T
X"
NT,0.5163841807909605,"t=1
Lstr(R2(VSt−1)) #"
NT,0.5175141242937853,≤320 log2(n) ln(n)
NT,0.5186440677966102,"T
≤ε ,"
NT,0.519774011299435,"when T ≥320 log2(n) ln(n) ε
."
NT,0.5209039548022599,Post proof discussion of Lemma 1
NT,0.5220338983050847,"• Upon first inspection, readers might perceive a resemblance between the proof of the loss
analysis section and the standard proof of converting regret bound to error bound.This
standard proof converts a regret guarantee on f1:T to an error guarantee of 1"
NT,0.5231638418079096,"T
PT
t=1 ft.
However, in this proof, the predictor employed in each round is ft, while the output is an
average over R2(VSt−1) for all t ∈[T]. Our algorithm does not provide a regret guarantee
on f1:T ."
NT,0.5242937853107345,"• Please note that our analysis exhibits asymmetry regarding losses on true positives and true
negatives. Specifically, the probability of identifying and reducing half of the misclassifying
hypotheses on true positives, denoted as Pr(Ep,t|zt, VSt−1) (Eq (6)), is lower than the
corresponding probability for true negatives, Pr(En,t|zt, VSt−1) (Eq (7)). This discrepancy
arises due to the different levels of difficulty in detecting misclassifying hypotheses. For
example, if there is exactly one hypothesis h misclassifying a true positive zt = (xt, rt, yt),
it is very hard to detect this h. We must select an ft satisfying that d(xt, ft) > d(xt, h′) for
all h′ ∈H \ {h} (hence ft will make a mistake), and that d(xt, ft) ≤d(xt, h) (so that we
will know h misclassifies zt). Algorithm 2 controls the distance d(xt, ft) through kt, which
is the number of hypotheses in the union. In this case, we can only detect h when kt = 1
and ft = h, which occurs with probability
1
nt log(nt)."
NT,0.5254237288135594,"However, if there is exactly one hypothesis h misclassifying a true negative zt = (xt, rt, yt),
we have that d(xt, h) = minh′∈H d(xt, h′). Then by setting ft = ∨h∈Hh, which will
makes a mistake and tells us h is a misclassifying hypothesis. Our algorithm will pick such
an ft with probability
1
log(nt)."
NT,0.5265536723163842,"H
Proof of Theorem 7"
NT,0.5276836158192091,"Proof. We will prove Theorem 7 by constructing an instance of Q and H and showing that for any
conservative learning algorithm, there exists a realizable data distribution s.t. achieving ε loss requires
at least eΩ( |H|"
NT,0.5288135593220339,ε ) samples.
NT,0.5299435028248588,"Construction of Q, H and a set of realizable distributions"
NT,0.5310734463276836,"• Let the input metric space (X, d) be constructed in the following way. Consider the
feature space X = {e1, . . . , en} ∪X0, where X0 = { σ(0,1,...,n−1)"
NT,0.5322033898305085,"z
|σ ∈Sn} with z =
√"
NT,0.5333333333333333,12+...+(n−1)2
NT,0.5344632768361582,"α
for some small α = 0.1. Here Sn is the set of all permutations over n
elements. So X0 is the set of points whose coordinates are a permutation of {0, 1/z, . . . , (n−
1)/z} and all points in X0 have the ℓ2 norm equal to α. We define the metric d by restricting
ℓ2 distance to X, i.e., d(x1, x2) = ∥x1 −x2∥2 for all x1, x2 ∈X. Then we have that for
any x ∈X0 and i ∈[n], the distance between x and ei is"
NT,0.535593220338983,"d(x, ei) = ∥x −ei∥2 =
s"
NT,0.536723163841808,"(xi −1)2 +
X"
NT,0.5378531073446328,"j̸=i
x2
j ="
NT,0.5389830508474577,"v
u
u
t1 + n
X"
NT,0.5401129943502825,"j=1
x2
j −2xi =
p"
NT,0.5412429378531074,"1 + α2 −2xi ,"
NT,0.5423728813559322,"which is greater than
√"
NT,0.5435028248587571,"1 + α2 −2α > 0.8 > 2α. For any two points x, x′ ∈X0,
d(x, x′) ≤2α by triangle inequality."
NT,0.5446327683615819,"• Let the hypothesis class be a set of singletons over {ei|i ∈[n]}, i.e., H = {21{ei} −1|i ∈
[n]}."
NT,0.5457627118644067,"• We now define a collection of distributions {Di|i ∈[n]} in which Di is realized by 21{ei}−1.
For any i ∈[n], we define Di in the following way. Let the marginal distribution DX
over X be uniform over X0. For any x, the label y is + with probability 1 −6ε and
−with probability 6ε, i.e., D(y|x) = Rad(1 −6ε). Note that the marginal distribution
DX×Y = Unif(X0) × Rad(1 −6ε) is identical for any distribution in {Di|i ∈[n]} and
does not depend on i."
NT,0.5468926553672316,"If the label is positive y = +, then let the radius r = 2. If the label is negative y = −, then let r =
q"
NT,0.5480225988700564,1 + α2 −2(xi + 1
NT,0.5491525423728814,"z), which guarantees that x can be manipulated to ej iff d(x, ej) <"
NT,0.5502824858757062,"d(x, ei) for all j ∈[n]. Since xi ≤α and 1"
NT,0.5514124293785311,"z ≤α, we have
q"
NT,0.5525423728813559,1 + α2 −2(xi + 1
NT,0.5536723163841808,"z) ≥
√1 −4α > 2α. Therefore, for both positive and negative examples, we have radius r
strictly greater than 2α in both cases."
NT,0.5548022598870056,"Randomization and improperness of the output fout do not help
Note that algorithms are
allowed to output a randomized fout and to output fout /∈H. We will show that randomization and
improperness of fout don’t make the problem easier. That is, supposing that the data distribution
is Di∗for some i∗∈[n], finding a (possibly randomized and improper) fout is not easier than
identifying i∗. Since our feature space X is finite, we can enumerate all hypotheses not equal to
21{ei∗} −1 and calculate their strategic population loss as follows."
NT,0.5559322033898305,• 21∅−1 predicts all negative and thus Lstr(21∅−1) = 1 −6ε;
NT,0.5570621468926553,"• For any a ⊂X s.t. a ∩X0 ̸= ∅, 21a −1 will predict any point drawn from Di∗as positive
(since all points have radius greater than 2α and the distance between any two points in X0
is smaller than 2α) and thus Lstr(21a −1) = 6ε;"
NT,0.5581920903954802,"• For any a ⊂{e1, . . . , en} satisfying that ∃i ̸= i∗, ei ∈a, we have Lstr(21a −1) ≥3ε. This
is due to that when y = −, x is chosen from Unif(X0) and the probability of d(x, ei) <
d(x, ei∗) is 1"
NT,0.559322033898305,"2. When d(x, ei) < d(x, ei∗), 21a −1 will predict x as positive."
NT,0.56045197740113,"Under distribution Di∗, if we are able to find a (possibly randomized) fout with strategic loss of
Lstr(fout) ≤ε, then we have Lstr(fout) = Eh∼fout [Lstr(h)] ≥Prh∼fout(h ̸= 21{ei∗} −1) · 3ε.
Thus, Prh∼fout(h = 21{ei∗} −1) ≥2"
NT,0.5615819209039548,"3. Hence, if we are able to find a (possibly randomized) fout
with ε error, then we are able to identify i∗by checking which realization of fout has probability
greater than 2"
NT,0.5627118644067797,"3. In the following, we will focus on the sample complexity to identify i∗. Let iout
denote the algorithm’s answer to question “what is i∗?”."
NT,0.5638418079096045,"Conservative algorithms
When running a conservative algorithm, the rule of choosing ft at round
t and choosing the final output fout does not depend on the correct rounds, i.e. {τ ∈[T]|byτ = yτ}.
Let’s define"
NT,0.5649717514124294,"∆′
t =
∆t
if byt ̸= yt
⊥
if byt = yt ,
(11)"
NT,0.5661016949152542,"where ⊥is just a symbol representing “no information”. Then for any conservative algorithm,
the selected predictor ft is determined by (fτ, byτ, yτ, ∆′
τ) for τ < t and the final output fout is
determined by (ft, byt, yt, ∆′
t)T
t=1. From now on, we consider ∆′
t as the feedback in the learning
process of a conservative algorithm since it make no difference from running the same algorithm with
feedback ∆t."
NT,0.5672316384180791,"Smooth the data distribution
For technical reasons (appearing later in the analysis), we don’t want
to analyze distribution {Di|i ∈[n]} directly as the probability of ∆t = ei is 0 when ft(ei) = +1
under distribution Di. Instead, we consider the mixture of Di and another distribution D′′
i , which
is identical to Di except that r(x) = d(x, ei) when y = −. More specifically, let D′
i = (1 −
p)Di + pD′′
i with some extremely small p, where D′′
i ’s marginal distribution over X × Y is still
Unif(X0) × Rad(1 −6ε); the radius is r = 2 when y = +, ; and the radius is r = d(x, ei) when
y = −. For any data distribution D, let PD be the dynamics of (f1, y1, by1, ∆′
1, . . . , fT , yT , byT , ∆′
T )
under D. According to Lemma 4, by setting p =
ε
16n2 , when T ≤n"
NT,0.5683615819209039,"ε , with high probability we never
sample from D′′
i and have that for any i, j ∈[n]
PDi(iout = j) −PD′
i(iout = j)
 ≤1"
NT,0.5694915254237288,"8 .
(12)"
NT,0.5706214689265536,"From now on, we only consider distribution D′
i instead of Di. The readers might have the question
that why not using D′
i for construction directly. This is because D′
i does not satisfy realizability and
no hypothesis has zero loss under D′
i."
NT,0.5717514124293785,"Information gain from different choices of ft
In each round of interaction, the learner picks a
predictor ft, which can be out of H. Here we enumerate all choices of ft."
NT,0.5728813559322034,"• ft(·) = 21∅−1 predicts all points in X by negative. No matter what i∗is, we will observe
(∆t = xt, yt) ∼Unif(X0) × Rad(1 −6ε) and byt = −. They are identically distributed for
all i∗∈[n], and thus, ∆′
t is also identically distributed. We cannot tell any information of i∗
from this round."
NT,0.5740112994350283,"• ft = 21at −1 for some at ⊂X s.t. a ∩X0 ̸= ∅. Then ∆t = ∆(xt, ft, rt) = ∆(xt, ft, 2α)
since rt > 2α and d(xt, ft) ≤2α, byt = +, yt ∼Rad(1 −6ε). None of these depends on
i∗and again, the distribution of (byt, yt, ∆′
t) is identical for all i∗and we cannot tell any
information of i∗from this round."
NT,0.5751412429378531,"• ft = 21at −1 for some non-empty at ⊂{e1, . . . , en}. For rounds with yt = +, we have
byt = + and ∆t = ∆(xt, ft, 2), which still not depend on i∗. Thus we cannot learn any
information about i∗. But we can learn when yt = −. For rounds with yt = −, if ∆t ∈at,
then we could observe byt = + and ∆′
t = ∆t, which at least tells that 21{∆t} −1 is not the
target function (with high probability); if ∆t /∈at, then byt = −and we observe ∆′
t =⊥."
NT,0.576271186440678,"Therefore, we only need to focus on the rounds with ft = 21at −1 for some non-empty at ⊂
{e1, . . . , en} and yt = −. It is worth noting that drawing an example x from X0 uniformly, it
is equivalent to uniformly drawing a permutation of H such that the distances between x and h
over all h ∈H are permuted according to it. Then ∆t = ej iff ej ∈at, d(x, ej) ≤d(x, ei∗) and
d(x, ej) ≤d(x, el) for all el ∈at. Let kt = |at| denote the cardinality of at. In such rounds, under
distribution D′
i∗, the distribution of ∆′
t are described as follows."
NT,0.5774011299435028,"1. The case of ei∗
∈at:
For all j ∈at \ {i∗}, with probability
1
kt , d(xt, ej) =
minel∈at d(xt, el) and thus, ∆′
t = ∆t = ej and byt = + (mistake round). With prob-
ability 1"
NT,0.5785310734463277,"kt , we have d(xt, ei∗) = minel∈at d(xt, el). If the example is drawn from Di∗, we
have ∆t = xt and yt = −(correct round), thus ∆′
t =⊥. If the example is drawn from D′′
i∗,
we have we have ∆′
t = ∆t = ei∗and yt = + (mistake round). Therefore, according to the
definition of ∆′
t (Eq (11)), we have"
NT,0.5796610169491525,"∆′
t = 

 
"
NT,0.5807909604519774,"ej
w.p.
1
kt for ej ∈at, j ̸= i∗"
NT,0.5819209039548022,"ei∗
w.p.
1
kt p
⊥
w.p.
1
kt (1 −p) ."
NT,0.5830508474576271,"We denote this distribution by P∈(at, i∗)."
NT,0.584180790960452,"2. The case of ei∗
/∈at: For all j ∈at, with probability
1
kt+1, then d(xt, ej) =
minel∈at∪{ei∗} d(xt, el) and thus, ∆t = ej and byt = + (mistake round). With proba-
bility
1
kt+1, we have d(x, ei∗) < minel∈at d(xt, el) and thus, ∆t = xt, byt = −(correct
round), and ∆′
t =⊥. Therefore, the distribution of ∆′
t is"
NT,0.5853107344632769,"∆′
t ="
NT,0.5864406779661017,"(
ej
w.p.
1
kt+1 for ej ∈at
⊥
w.p.
1
kt+1 ."
NT,0.5875706214689266,We denote this distribution by P/∈(at).
NT,0.5887005649717514,"To measure the information obtained from ∆′
t, we will utilize the KL divergence of the distribution
of ∆′
t under the data distribution Di∗from that under a benchmark distribution. Let D = 1 n
P"
NT,0.5898305084745763,"i∈n D′
i
denote the average distribution. The process of sampling from D is equivalent to sampling i∗"
NT,0.5909604519774011,"uniformly at random from [n] first and drawing a sample from Di∗. Then under D, for any ej ∈at,
we have"
NT,0.592090395480226,"Pr(∆′
t = ej) = Pr(i∗= j) Pr(∆′
t = ej|i∗= j) + Pr(i∗∈at \ {j}) Pr(∆′
t = ej|i∗∈at \ {j})"
NT,0.5932203389830508,"+ Pr(i∗/∈at) Pr(∆′
t = ej|i∗/∈at) = 1 n · p"
NT,0.5943502824858757,"kt
+ kt −1 n
· 1"
NT,0.5954802259887005,"kt
+ n −kt"
NT,0.5966101694915255,"n
·
1
kt + 1 = nkt −1 + p(kt + 1)"
NT,0.5977401129943503,"nkt(kt + 1)
, and"
NT,0.5988700564971752,"Pr(∆′
t =⊥) = Pr(i∗∈at) Pr(∆′
t =⊥|i∗∈at) + Pr(i∗/∈at) Pr(∆′
t =⊥|i∗/∈at) = kt"
NT,0.6,n · 1 −p
NT,0.6011299435028249,"kt
+ n −kt"
NT,0.6022598870056497,"n
·
1
kt + 1 = n + 1 −p(kt + 1)"
NT,0.6033898305084746,"n(kt + 1)
."
NT,0.6045197740112994,"Thus, the distribution of ∆′
t under D is"
NT,0.6056497175141243,"∆′
t ="
NT,0.6067796610169491,"(
ej
w.p. nkt−1+p(kt+1)"
NT,0.607909604519774,"nkt(kt+1)
for ej ∈at
⊥
w.p. n+1−p(kt+1)"
NT,0.6090395480225989,"n(kt+1)
."
NT,0.6101694915254238,"We denote this distribution by P(at). Next we will compute the KL divergences of P∈(at, i∗) and
P/∈(at) from P(at). We will use the inequality log(1+x) ≤x for x ≥0 in the following calculation.
For any i∗s.t. ei∗∈at, we have"
NT,0.6112994350282486,"DKL(P(at)∥P∈(at, i∗))"
NT,0.6124293785310735,=(kt −1)nkt −1 + p(kt + 1)
NT,0.6135593220338983,"nkt(kt + 1)
log(nkt −1 + p(kt + 1)"
NT,0.6146892655367232,"nkt(kt + 1)
kt)"
NT,0.615819209039548,+ nkt −1 + p(kt + 1)
NT,0.6169491525423729,"nkt(kt + 1)
log(nkt −1 + p(kt + 1)"
NT,0.6180790960451977,"nkt(kt + 1)
· kt p )"
NT,0.6192090395480226,+ n + 1 −p(kt + 1)
NT,0.6203389830508474,"n(kt + 1)
log(n + 1 −p(kt + 1)"
NT,0.6214689265536724,"n(kt + 1)
·
kt
1 −p)"
NT,0.6225988700564972,"≤0 +
1
kt + 1 log(1"
NT,0.6237288135593221,"p) +
2p
kt + 1 =
1
kt + 1 log(1"
NT,0.6248587570621469,"p) +
2p
kt + 1 ,
(13) and"
NT,0.6259887005649718,DKL(P(at)∥P/∈(at))
NT,0.6271186440677966,"=kt
nkt −1 + p(kt + 1)"
NT,0.6282485875706215,"nkt(kt + 1)
log(nkt −1 + p(kt + 1)"
NT,0.6293785310734463,"nkt(kt + 1)
(kt + 1))"
NT,0.6305084745762712,+ n + 1 −p(kt + 1)
NT,0.631638418079096,"n(kt + 1)
log(n + 1 −p(kt + 1)"
NT,0.632768361581921,"n(kt + 1)
(kt + 1))"
NT,0.6338983050847458,"≤0 +
n + 1
n2(kt + 1) =
n + 1
n2(kt + 1) .
(14)"
NT,0.6350282485875707,"Lower bound of the information
We utilize the information theoretical framework of proving
lower bounds for linear bandits (Theorem 11 by Rajaraman et al. (2023)) here. For notation simplicity,
for all i ∈[n], let Pi denote the dynamics of (f1, ∆′
1, y1, by1, . . . , fT , ∆′
T , yT , byT ) under D′
i and P
denote the dynamics under D. Let Bt denote the event of {ft = 21at −1 for some non-empty at ⊂
{e1, . . . , en}}. As discussed before, for any at, conditional on ¬Bt or yt = +1, (∆′
t, yt, byt) are
identical in all {D′
i|i ∈[n]}, and therefore, also identical in D. We can only obtain information at
rounds when Bt ∧(yt = −1) occurs. In such rounds, we know that ft is fully determined by history
(possibly with external randomness , which does not depend on data distribution), yt = −1 and byt is
fully determined by ∆′
t (byt = +1 iff. ∆′
t ∈at)."
NT,0.6361581920903955,"Therefore, conditional the history Ht−1 = (f1, ∆′
1, y1, by1, . . . , ft−1, ∆′
t−1, yt−1, byt−1) before time
t, we have"
NT,0.6372881355932203,"DKL(P(ft, ∆′
t, yt, byt|Ht−1)∥Pi(ft, ∆′
t, yt, byt|Ht−1))"
NT,0.6384180790960452,"=P(Bt ∧(yt = −1))DKL(P(∆′
t|Ht−1, Bt ∧(yt = −1))∥Pi(∆′
t|Ht−1, Bt ∧(yt = −1)))"
NT,0.63954802259887,"=6εP(Bt)DKL(P(∆′
t|Ht−1, Bt ∧(yt = −1))∥Pi(∆′
t|Ht−1, Bt ∧(yt = −1))) ,
(15)"
NT,0.6406779661016949,where the last equality holds due to that yt ∼Rad(1 −6ε) and does not depend on Bt.
NT,0.6418079096045197,For any algorithm that can successfully identify i under the data distribution Di with probability 3
NT,0.6429378531073446,"4
for all i ∈[n], then PDi(iout = i) ≥3"
NT,0.6440677966101694,4 and PDj(iout = i) ≤1
NT,0.6451977401129944,"4 for all j ̸= i. Recall that Di and D′
i
are very close when the mixture parameter p is small. Combining with Eq (12), we have"
NT,0.6463276836158192,|Pi(iout = i) −Pj(iout = i)|
NT,0.6474576271186441,"≥
PDi(iout = i) −PDj(iout = i)
 −|PDi(iout = i) −Pi(iout = i)| −
PDj(iout = i) −Pj(iout = i) ≥1 2 −1 4 = 1 4 ."
NT,0.6485875706214689,Then we have the total variation distance between Pi and Pj
NT,0.6497175141242938,"TV(Pi, Pj) ≥|Pi(iout = i) −Pj(iout = i)| ≥1"
NT,0.6508474576271186,"4 .
(16)"
NT,0.6519774011299435,Then we have
NT,0.6531073446327683,"Ei∼Unif([n])

TV2(Pi, P(i+1) mod n)

≤4Ei∼Unif([n])

TV2(Pi, P)
"
NT,0.6542372881355932,"≤2Ei

DKL(P∥Pi)

(Pinsker’s ineq) =2Ei "" T
X"
NT,0.655367231638418,"t=1
DKL(P(ft, ∆′
t, yt, byt|Ht−1)∥Pi(ft, ∆′
t, yt, byt|Ht−1)) #"
NT,0.656497175141243,(Chain rule)
NT,0.6576271186440678,"=12εEi "" T
X t=1"
NT,0.6587570621468927,"P(Bt)DKL(P(∆′
t|Ht−1, Bt ∧(yt = −1))∥Pi(∆′
t|Ht−1, Bt ∧(yt = −1))) #"
NT,0.6598870056497175,"(Apply Eq (15)) =12ε n T
X t=1 P(Bt) n
X"
NT,0.6610169491525424,"i=1
DKL(P(∆′
t|Ht−1, Bt ∧(yt = −1))∥Pi(∆′
t|Ht−1, Bt ∧(yt = −1))) =12ε"
NT,0.6621468926553672,"n Ef1:T ∼P  
T
X"
NT,0.6632768361581921,"t=1
1(Bt)  X"
NT,0.6644067796610169,"i:i∈at
DKL(P(at)∥P∈(at, i)) +
X"
NT,0.6655367231638418,"i:i/∈at
DKL(P(at)∥P/∈(at))     ≤12ε n T
X"
NT,0.6666666666666666,"t=1
Ef1:T ∼P  X"
NT,0.6677966101694915,i:i∈at
NT,0.6689265536723163,"
1
kt + 1 log(1"
NT,0.6700564971751413,"p) +
2p
kt + 1 
+
X"
NT,0.6711864406779661,i:i/∈at
NT,0.672316384180791,"n + 1
n2(kt + 1)  "
NT,0.6734463276836158,"(Apply Eq (13),(14)) ≤12ε n T
X"
NT,0.6745762711864407,"t=1
(log(1"
NT,0.6757062146892655,p) + 2p + 1)
NT,0.6768361581920904,"≤12Tε(log(16n2/ε) + 2) n
."
NT,0.6779661016949152,"Combining with Eq (16), we have that there exists a universal constant c such that T ≥
cn
ε(log(n/ε)+1)."
NT,0.6790960451977401,"I
Proof of Theorem 8"
NT,0.6802259887005649,"Proof. We will prove Theorem 8 by constructing an instance of Q and H and then reduce it to a
linear stochastic bandit problem."
NT,0.6813559322033899,"Construction of Q, H and a set of realizable distributions"
NT,0.6824858757062147,"• Consider the input metric space in the shape of a star, where X = {0, 1, . . . , n} and the
distance function of d(0, i) = 1 and d(i, j) = 2 for all i ̸= j ∈[n]."
NT,0.6836158192090396,"• Let the hypothesis class be a set of singletons over [n], i.e., H = {21{i} −1|i ∈[n]}."
NT,0.6847457627118644,"• We define a collection of distributions {Di|i ∈[n]} in which Di is realized by 21{i} −1.
The data distribution Di put 1 −3(n −1)ε on (0, 1, +) and 3ε on (i, 1, −) for all i ̸= i∗.
Hence, note that all distributions in {Di|i ∈[n]} share the same distribution support
{(0, 1, +)} ∪{(i, 1, −)|i ∈[n]}, but have different weights."
NT,0.6858757062146893,"Randomization and improperness of the output fout do not help.
Note that algorithms are
allowed to output a randomized fout and to output fout /∈H. We will show that randomization and
improperness of fout don’t make the problem easier. Supposing that the data distribution is Di∗
for some i∗∈[n], finding a (possibly randomized and improper) fout is not easier than identifying
i∗. Since our feature space X is finite, we can enumerate all hypotheses not equal to 21{i∗} −1
and calculate their strategic population loss as follows. The hypothesis 21∅−1 will predict all by
negative and thus Lstr(21∅−1) = 1 −3(n −1)ε. For any hypothesis predicting 0 by positive, it will
predict all points in the distribution support by positive and thus incurs strategic loss 3(n −1)ε. For
any hypothesis predicting 0 by negative and some i ̸= i∗by positive, then it will misclassify (i, 1, −)
and incur strategic loss 3ε. Therefore, for any hypothesis h ̸= 21{i∗} −1, we have Lstr
Di∗(h) ≥3ε."
NT,0.6870056497175141,"Similar to the proof of Theorem 7, under distribution Di∗, if we are able to find a (possibly random-
ized) fout with strategic loss Lstr(fout) ≤ε. Then Prh∼fout(h = 21{i∗} −1) ≥2"
WE CAN IDENTIFY,0.688135593220339,"3. We can identify
i∗by checking which realization of fout has probability greater than 2"
WE CAN IDENTIFY,0.6892655367231638,"3. In the following, we will
focus on the sample complexity to identify the target function 21{i∗} −1 or simply i∗. Let iout
denote the algorithm’s answer to question of “what is i∗?”."
WE CAN IDENTIFY,0.6903954802259887,"Smooth the data distribution For technical reasons (appearing later in the analysis), we don’t want
to analyze distribution {Di|i ∈[n]} directly as the probability of (i, 1, −) is 0 under distribution Di.
Instead, for each i ∈[n], let D′
i = (1 −p)Di + pD′′
i be the mixture of Di and D′′
i for some small p,
where D′′
i = (1 −3(n −1)ε)1{(0,1,+)} + 3(n −1)ε1{(i,1,−)}. Specifically,"
WE CAN IDENTIFY,0.6915254237288135,"D′
i(z) = 
 "
WE CAN IDENTIFY,0.6926553672316385,"1 −3(n −1)ε
for z = (0, 1, +)
3(1 −p)ε
for z = (j, 1, −), ∀j ̸= i
3(n −1)pε
for z = (i, 1, −)"
WE CAN IDENTIFY,0.6937853107344633,"For any data distribution D, let PD be the dynamics of (f1, y1, by1, . . . , fT , yT , byT ) under D. Accord-
ing to Lemma 4, by setting p =
ε
16n2 , when T ≤n"
WE CAN IDENTIFY,0.6949152542372882,"ε , we have that for any i, j ∈[n]"
WE CAN IDENTIFY,0.696045197740113,"PDi(iout = j) −PD′
i(iout = j)
 ≤1"
WE CAN IDENTIFY,0.6971751412429379,"8 .
(17)"
WE CAN IDENTIFY,0.6983050847457627,"From now on, we only consider distribution D′
i instead of Di. The readers might have the question
that why not using D′
i for construction directly. This is because no hypothesis has zero loss under D′
i,
and thus D′
i does not satisfy realizability requirement."
WE CAN IDENTIFY,0.6994350282485876,"Information gain from different choices of ft
Note that in each round, the learner picks a ft and
then only observes byt and yt. Here we enumerate choices of ft as follows."
WE CAN IDENTIFY,0.7005649717514124,"1. ft = 21∅−1 predicts all points in X by negative. No matter what i∗is, we observe byt = −
and yt = 21(xt = 0) −1. Hence (byt, yt) are identically distributed for all i∗∈[n], and
thus, we cannot learn anything about i∗from this round."
WE CAN IDENTIFY,0.7016949152542373,"2. ft predicts 0 by positive. Then no matter what i∗is, we have byt = + and yt = 1(xt = 0).
Thus again, we cannot learn anything about i∗."
WE CAN IDENTIFY,0.7028248587570621,"3. ft = 21at −1 for some non-empty at ⊂[n]. For rounds with xt = 0, we have byt = yt = +
no matter what i∗is and thus, we cannot learn anything about i∗. For rounds with yt = −,
i.e., xt ̸= 0, we will observe byt = ft(∆(xt, ft, 1)) = 1(xt ∈at)."
WE CAN IDENTIFY,0.703954802259887,"Hence, we can only extract information with the third type of ft at rounds with xt ̸= 0."
WE CAN IDENTIFY,0.7050847457627119,"Reduction to stochastic linear bandits
In rounds with ft = 21at −1 for some non-empty at ⊂[n]
and xt ̸= 0, our problem is identical to a stochastic linear bandit problem. Let us state our problem
as Problem 1 and a linear bandit problem as Problem 2. Let A = {0, 1}n \ {0}."
WE CAN IDENTIFY,0.7062146892655368,"Problem 1. The environment picks an i∗∈[n]. At each round t, the environment picks xt ∈{ei|i ∈
[n]} with P(i) = 1−p"
WE CAN IDENTIFY,0.7073446327683616,"n−1 for i ̸= i∗and P(i∗) = p and the learner picks an at ∈A (where we use
a n-bit string to represent at and at,i = 1 means that at predicts i by positive). Then the learner
observes byt = 1(a⊺
t xt > 0) (where we use 0 to represent nagative label)."
WE CAN IDENTIFY,0.7084745762711865,Problem 2. The environment picks a linear parameter w∗∈{wi|i ∈[n]} with wi = 1−p
WE CAN IDENTIFY,0.7096045197740113,n−11−( 1−p
WE CAN IDENTIFY,0.7107344632768362,"n−1 −
p)ei. The arm set is A. For each arm a ∈A, the reward is i.i.d. from the following distribution:"
WE CAN IDENTIFY,0.711864406779661,"rw(a) =
−1, w.p. w⊺a ,
0 .
(18)"
WE CAN IDENTIFY,0.7129943502824859,"If the linear parameter w∗= wi∗, the optimal arm is ei∗."
WE CAN IDENTIFY,0.7141242937853107,"Claim 1. For any δ > 0, for any algorithm A that identify i∗correctly with probability 1 −δ within
T rounds for any i∗∈[n] in Problem 1, we can construct another algorithm A′ can also identify the
optimal arm in any environment with probability 1 −δ within T rounds in Problem 2."
WE CAN IDENTIFY,0.7152542372881356,"This claim follows directly from the problem descriptions. Given any algorithm A for Problem 1,
we can construct another algorithm A′ which simulates A. At round t, if A selects predictor at,
then A′ picks arm the same as at. Then A′ observes a reward rwi∗(at), which is −1 w.p. wi∗⊺at
and feed −rwi∗(at) to A. Since byt in Problem 1 is 1 w.p. Pn
i=1 at,iP(i) = wi∗⊺at, it is distributed
identically as −rwi∗(at). Since A will be able to identify i∗w.p. 1 −δ in T rounds, A′ just need to
output ei∗as the optimal arm."
WE CAN IDENTIFY,0.7163841807909604,"Then any lower bound on T for Problem 2 also lower bounds Problem 1. Hence, we adopt the
information theoretical framework of proving lower bounds for linear bandits (Theorem 11 by
Rajaraman et al. (2023)) to prove a lower bound for our problem. In fact, we also apply this
framework to prove the lower bounds in other settings of this work, including Theorem 7 and
Theorem 9."
WE CAN IDENTIFY,0.7175141242937854,"Lower bound of the information
For notation simplicity, for all i ∈[n], let Pi denote the dynamics
of (f1, y1, by1, . . . , fT , yT , byT ) under D′
i and and P denote the dynamics under D = 1"
WE CAN IDENTIFY,0.7186440677966102,"nD′
i. Let Bt
denote the event of {ft = 21at −1 for some non-empty at ⊂[n]}. As discussed before, for any
at, conditional on ¬Bt or yt = +1, (xt, yt, byt) are identical in all {D′
i|i ∈[n]}, and therefore, also
identical in D. We can only obtain information at rounds when Bt ∧yt = −1 occurs. In such rounds,
ft is fully determined by history (possibly with external randomness , which does not depend on
data distribution), yt = −1 and byt = −rw(at) with rw(at) sampled from the distribution defined in
Eq (18)."
WE CAN IDENTIFY,0.7197740112994351,For any algorithm that can successfully identify i under the data distribution Di with probability 3
WE CAN IDENTIFY,0.7209039548022599,"4
for all i ∈[n], then PDi(iout = i) ≥3"
WE CAN IDENTIFY,0.7220338983050848,4 and PDj(iout = i) ≤1
WE CAN IDENTIFY,0.7231638418079096,"4 for all j ̸= i. Recall that Di and D′
i
are very close when the mixture parameter p is small. Combining with Eq (17), we have"
WE CAN IDENTIFY,0.7242937853107345,|Pi(iout = i) −Pj(iout = i)|
WE CAN IDENTIFY,0.7254237288135593,"≥
PDi(iout = i) −PDj(iout = i)
 −|PDi(iout = i) −Pi(iout = i)| −
PDj(iout = i) −Pj(iout = i) ≥1 2 −1 4 = 1"
WE CAN IDENTIFY,0.7265536723163842,"4 .
(19)"
WE CAN IDENTIFY,0.727683615819209,"Let w =
1
n1. Let kl(q, q′) denote the KL divergence from Ber(q) to Ber(q′). Let Ht−1 =
(f1, y1, by1, . . . , ft−1, yt−1, byt−1) denote the history up to time t −1. Then we have"
WE CAN IDENTIFY,0.7288135593220338,"Ei∼Unif([n])

TV2(Pi, Pi+1 mod n)

≤4Ei∼Unif([n])

TV2(Pi, P)
"
WE CAN IDENTIFY,0.7299435028248588,"≤2Ei

DKL(P∥Pi)

(Pinsker’s ineq) =2Ei "" T
X"
WE CAN IDENTIFY,0.7310734463276836,"t=1
DKL(P(ft, yt, byt|Ht−1)∥Pi(ft, yt, byt|Ht−1)) #"
WE CAN IDENTIFY,0.7322033898305085,"(Chain rule) =2Ei "" T
X t=1"
WE CAN IDENTIFY,0.7333333333333333,"P(Bt ∧yt = −1)Ea1:T ∼P

DKL(Ber(⟨w, at⟩)∥Ber(

wi, at

))

#"
WE CAN IDENTIFY,0.7344632768361582,"=6(n −1)εEi "" T
X t=1"
WE CAN IDENTIFY,0.735593220338983,"P(Bt)Ea1:T ∼P

DKL(Ber(⟨w, at⟩)∥Ber(

wi, at

))

#"
WE CAN IDENTIFY,0.7367231638418079,"=6(n −1)ε n T
X"
WE CAN IDENTIFY,0.7378531073446327,"t=1
Ea1:T ∼P "" n
X"
WE CAN IDENTIFY,0.7389830508474576,"i=1
DKL(Ber(⟨w, at⟩)∥Ber(

wi, at

)) #"
WE CAN IDENTIFY,0.7401129943502824,"=6(n −1)ε n T
X"
WE CAN IDENTIFY,0.7412429378531074,"t=1
Ea1:T ∼P  X"
WE CAN IDENTIFY,0.7423728813559322,"i:i∈at
kl(kt"
WE CAN IDENTIFY,0.7435028248587571,"n , (kt −1)(1 −p)"
WE CAN IDENTIFY,0.7446327683615819,"n −1
+ p) +
X"
WE CAN IDENTIFY,0.7457627118644068,"i:i/∈at
kl(kt"
WE CAN IDENTIFY,0.7468926553672316,"n , kt(1 −p)"
WE CAN IDENTIFY,0.7480225988700565,"n −1
)  "
WE CAN IDENTIFY,0.7491525423728813,"=6(n −1)ε n T
X"
WE CAN IDENTIFY,0.7502824858757062,"t=1
Ea1:T ∼P"
WE CAN IDENTIFY,0.751412429378531,"
ktkl(kt"
WE CAN IDENTIFY,0.752542372881356,"n , (kt −1)(1 −p)"
WE CAN IDENTIFY,0.7536723163841808,"n −1
+ p) + (n −kt)kl(kt"
WE CAN IDENTIFY,0.7548022598870057,"n , kt(1 −p)"
WE CAN IDENTIFY,0.7559322033898305,"n −1
)

(20)"
WE CAN IDENTIFY,0.7570621468926554,"If kt = 1, then"
WE CAN IDENTIFY,0.7581920903954802,kt · kl(kt
WE CAN IDENTIFY,0.7593220338983051,"n , (kt −1)(1 −p)"
WE CAN IDENTIFY,0.7604519774011299,"n −1
+ p) = kl( 1"
WE CAN IDENTIFY,0.7615819209039548,"n, p) ≤1"
WE CAN IDENTIFY,0.7627118644067796,"n log(1 p) , and"
WE CAN IDENTIFY,0.7638418079096045,(n −kt) · kl(kt
WE CAN IDENTIFY,0.7649717514124293,"n , kt(1 −p)"
WE CAN IDENTIFY,0.7661016949152543,"n −1
) = (n −1) · kl( 1"
WE CAN IDENTIFY,0.7672316384180791,"n, 1 −p"
WE CAN IDENTIFY,0.768361581920904,"n −1) ≤
1
(1 −p)n(n −2) ,"
WE CAN IDENTIFY,0.7694915254237288,"where the ineq holds due to kl(q, q′) ≤(q−q′)2"
WE CAN IDENTIFY,0.7706214689265537,"q′(1−q′). If kt = n −1, it is symmetric to the case of kt = 1.
We have"
WE CAN IDENTIFY,0.7717514124293785,kt · kl(kt
WE CAN IDENTIFY,0.7728813559322034,"n , (kt −1)(1 −p)"
WE CAN IDENTIFY,0.7740112994350282,"n −1
+ p) = (n −1)kl(n −1"
WE CAN IDENTIFY,0.7751412429378531,"n
, n −2"
WE CAN IDENTIFY,0.7762711864406779,"n −1 +
1
n −1p) = (n −1)kl( 1"
WE CAN IDENTIFY,0.7774011299435029,"n, 1 −p n −1)"
WE CAN IDENTIFY,0.7785310734463277,"≤
1
(1 −p)n(n −2) , and"
WE CAN IDENTIFY,0.7796610169491526,(n −kt) · kl(kt
WE CAN IDENTIFY,0.7807909604519774,"n , kt(1 −p)"
WE CAN IDENTIFY,0.7819209039548023,"n −1
) = kl(n −1"
WE CAN IDENTIFY,0.7830508474576271,"n
, 1 −p) = kl( 1"
WE CAN IDENTIFY,0.784180790960452,"n, p) ≤1"
WE CAN IDENTIFY,0.7853107344632768,n log(1 p) .
WE CAN IDENTIFY,0.7864406779661017,"If 1 < kt < n −1, then"
WE CAN IDENTIFY,0.7875706214689265,kt · kl(kt
WE CAN IDENTIFY,0.7887005649717514,"n , (kt −1)(1 −p)"
WE CAN IDENTIFY,0.7898305084745763,"n −1
+ p) =kt · kl(kt"
WE CAN IDENTIFY,0.7909604519774012,"n , kt −1"
WE CAN IDENTIFY,0.792090395480226,n −1 + n −kt
WE CAN IDENTIFY,0.7932203389830509,"n −1 p)
(a)
≤kt · kl(kt"
WE CAN IDENTIFY,0.7943502824858757,"n , kt −1"
WE CAN IDENTIFY,0.7954802259887006,n −1 )
WE CAN IDENTIFY,0.7966101694915254,"(b)
≤kt ·
( kt"
WE CAN IDENTIFY,0.7977401129943503,n −kt−1
WE CAN IDENTIFY,0.7988700564971751,n−1 )2 kt−1
WE CAN IDENTIFY,0.8,n−1 (1 −kt−1
WE CAN IDENTIFY,0.8011299435028248,"n−1 ) = kt ·
n −kt
n2(kt −1) ≤
kt·
n(kt −1) ≤2 n ,"
WE CAN IDENTIFY,0.8022598870056498,where inequality (a) holds due to that kt−1
WE CAN IDENTIFY,0.8033898305084746,n−1 + n−kt
WE CAN IDENTIFY,0.8045197740112995,n−1 p ≤kt
WE CAN IDENTIFY,0.8056497175141243,"n and kl(q, q′) is monotonically decreasing"
WE CAN IDENTIFY,0.8067796610169492,"in q′ when q′ ≤q and inequality (b) adopts kl(q, q′) ≤(q−q′)2"
WE CAN IDENTIFY,0.807909604519774,"q′(1−q′), and"
WE CAN IDENTIFY,0.8090395480225989,(n −kt) · kl(kt
WE CAN IDENTIFY,0.8101694915254237,"n , kt(1 −p)"
WE CAN IDENTIFY,0.8112994350282486,"n −1
) ≤(n −kt) · kl(kt"
WE CAN IDENTIFY,0.8124293785310734,"n ,
kt
n −1) ≤
kt(n −kt)
n2(n −1 −kt) ≤2kt n2 ,"
WE CAN IDENTIFY,0.8135593220338984,where the first inequality hold due to that kt(1−p)
WE CAN IDENTIFY,0.8146892655367232,"n−1
≥kt"
WE CAN IDENTIFY,0.8158192090395481,"n , and kl(q, q′) is monotonically increasing in"
WE CAN IDENTIFY,0.8169491525423729,"q′ when q′ ≥q and the second inequality adopts kl(q, q′) ≤(q−q′)2"
WE CAN IDENTIFY,0.8180790960451978,"q′(1−q′). Therefore, we have"
WE CAN IDENTIFY,0.8192090395480226,"Eq (20) ≤6(n −1)ε n T
X"
WE CAN IDENTIFY,0.8203389830508474,"t=1
Ea1:T ∼P  2"
WE CAN IDENTIFY,0.8214689265536723,n log(1
WE CAN IDENTIFY,0.8225988700564971,"p)

≤12εT log(1/p) n
."
WE CAN IDENTIFY,0.823728813559322,"Combining with Eq (19), we have that there exists a universal constant c such that T ≥
cn
ε(log(n/ε)+1)."
WE CAN IDENTIFY,0.8248587570621468,"J
Proof of Theorem 9"
WE CAN IDENTIFY,0.8259887005649718,"Proof. We will prove Theorem 9 by constructing an instance of Q and H and showing that for any
learning algorithm, there exists a realizable data distribution s.t. achieving ε loss requires at least
eΩ( |H|"
WE CAN IDENTIFY,0.8271186440677966,ε ) samples.
WE CAN IDENTIFY,0.8282485875706215,"Construction of Q, H and a set of realizable distributions"
WE CAN IDENTIFY,0.8293785310734463,"• Let feature vector space X = {0, 1, . . . , n} and let the space of feature-manipulation set
pairs Q = {(0, {0} ∪s)|s ⊂[n]}. That is to say, every agent has the same original feature
vector x = 0 but has different manipulation ability according to s."
WE CAN IDENTIFY,0.8305084745762712,"• Let the hypothesis class be a set of singletons over [n], i.e., H = {21{i} −1|i ∈[n]}."
WE CAN IDENTIFY,0.831638418079096,"• We now define a collection of distributions {Di|i ∈[n]} in which Di is realized by 21{i}−1.
For any i ∈[n], let Di put probability mass 1 −6ε on (0, X, +1) and 6ε uniformly over
{(0, {0} ∪sσ,i, −1)|σ ∈Sn}, where Sn is the set of all permutations over n elements and
sσ,i := {j|σ−1(j) < σ−1(i)} is the set of elements appearing before i in the permutation
(σ(1), . . . , σ(n)). In other words, with probability 1 −6ε, we will sample (0, X, +1) and
with ε, we will randomly draw a permutation σ ∼Unif(Sn) and return (0, {0} ∪sσ,i, −1).
The data distribution Di is realized by 21{i} −1 since for negative examples (0, {0} ∪
sσ,i, −1), we have i /∈s and for positive examples (0, X, +1), we have i ∈X."
WE CAN IDENTIFY,0.8327683615819209,"Randomization and improperness of the output fout do not help
Note that algorithms are
allowed to output a randomized fout and to output fout /∈H. We will show that randomization and
improperness of fout don’t make the problem easier. That is, supposing that the data distribution
is Di∗for some i∗∈[n], finding a (possibly randomized and improper) fout is not easier than
identifying i∗. Since our feature space X is finite, we can enumerate all hypotheses not equal to
21{i∗} −1 and calculate their strategic population loss as follows."
WE CAN IDENTIFY,0.8338983050847457,• 21∅−1 predicts all points in X by negative and thus Lstr(21∅−1) = 1 −6ε;
WE CAN IDENTIFY,0.8350282485875706,"• For any a ⊂X s.t. 0 ∈a, 21a −1 will predict 0 as positive and thus will predict any point
drawn from Di∗as positive. Hence Lstr(21a −1) = 6ε;"
WE CAN IDENTIFY,0.8361581920903954,"• For any a ⊂[n] s.t. ∃i ̸= i∗, i ∈a, we have Lstr(21a −1) ≥3ε. This is due to that when
y = −1, the probability of drawing a permutation σ with σ−1(i) < σ−1(i∗) is 1"
IN THIS,0.8372881355932204,"2. In this
case, we have i ∈sσ,i∗and the prediction of 21a −1 is +1."
IN THIS,0.8384180790960452,"Under distribution Di∗, if we are able to find a (possibly randomized) fout with strategic loss
Lstr(fout) ≤ε, then we have Lstr(fout) = Eh∼fout [Lstr(h)] ≥Prh∼fout(h ̸= 21{i∗} −1) · 3ε. Thus,
Prh∼fout(h = 21{i∗} −1) ≥2"
IN THIS,0.8395480225988701,"3 and then, we can identify i∗by checking which realization of fout
has probability greater than 2"
IN THIS,0.8406779661016949,"3. In the following, we will focus on the sample complexity to identify
the target function 21{i∗} −1 or simply i∗. Let iout denote the algorithm’s answer to question of
“what is i∗?”."
IN THIS,0.8418079096045198,"Smoothing the data distribution
For technical reasons (appearing later in the analysis), we
don’t want to analyze distribution {Di|i ∈[n]} directly as the probability of ∆t = i∗is 0 when
ft(i∗) = +1. Instead, we consider the mixture of Di and another distribution D′′
i to make the
probability of ∆t = i∗be a small positive number. More specifically, let D′
i = (1 −p)Di + pD′′
i ,
where D′′
i is defined by drawing (0, X, +1) with probability 1−6ε and (0, {0, i}, −1) with probability
6ε. When p is extremely small, we will never sample from D′′
i when time horizon T is not too large
and therefore, the algorithm behaves the same under D′
i and Di. For any data distribution D, let PD
be the dynamics of (x1, f1, ∆1, y1, by1, . . . , xT , fT , ∆T , yT , byT ) under D. According to Lemma 4,
by setting p =
ε
16n2 , when T ≤n"
IN THIS,0.8429378531073446,"ε , we have that for any i, j ∈[n]"
IN THIS,0.8440677966101695,"PDi(iout = j) −PD′
i(iout = j)
 ≤1"
IN THIS,0.8451977401129943,"8 .
(21)"
IN THIS,0.8463276836158192,"From now on, we only consider distribution D′
i instead of Di. The readers might have the question
that why not using D′
i for construction directly. This is because no hypothesis has zero loss under D′
i,
and thus D′
i does not satisfy realizability requirement."
IN THIS,0.847457627118644,"Information gain from different choices of ft
In each round of interaction, the learner picks
a predictor ft, which can be out of H. Suppose that the target function is 21{i∗} −1 . Here we
enumerate all choices of ft and discuss how much we can learn from each choice."
IN THIS,0.848587570621469,"• ft = 21∅−1 predicts all points in X by negative. No matter what i∗is, we will observe
∆t = xt = 0, yt ∼Rad(1 −6ε), byt = −1. They are identically distributed for any i∗∈[n]
and thus we cannot tell any information of i∗from this round."
IN THIS,0.8497175141242937,"• ft = 21at −1 for some at ⊂X s.t. 0 ∈at. Then no matter what i∗is, we will observe
∆t = xt = 0, yt ∼Rad(1 −6ε), byt = +1. Again, we cannot tell any information of i∗
from this round."
IN THIS,0.8508474576271187,"• ft = 21at −1 for some some non-empty at ⊂[n]. For rounds with yt = +1, we have
xt = 0, byt = +1 and ∆t = ∆(0, ft, X) ∼Unif(at), which still do not depend on i∗. For
rounds with yt = −1, if the drawn example (0, {0} ∪s, −1) satisfies that s ∩at ̸= ∅, the
we would observe ∆t ∈at and byt = +1. At least we could tell that 1{∆t} is not the target
function. Otherwise, we would observe ∆t = xt = 0 and byt = −1."
IN THIS,0.8519774011299435,"Therefore, we can only gain some information about i∗at rounds in which ft = 21at −1 for some
non-empty at ⊂[n] and yt = −1. In such rounds, under distribution D′
i∗, the distribution of ∆t is
described as follows. Let kt = |at| denote the cardinality of at. Recall that agent (0, {0} ∪s, −1)
breaks ties randomly when choosing ∆t if there are multiple elements in at ∩s. Here are two cases:
i∗∈at and i∗/∈at."
IN THIS,0.8531073446327684,"1. The case of i∗∈at: With probability p, we are sampling from D′′
i∗and then ∆t = i∗.
With probability 1 −p, we are sampling from Di∗. Conditional on this, with probability 1"
IN THIS,0.8542372881355932,"kt ,
we sample an agent (0, {0} ∪sσ,i∗, −1) with the permutation σ satisfying that σ−1(i∗) <
σ−1(j) for all j ∈at \ {i∗} and thus, ∆t = 0. With probability 1 −
1
kt , there exists
j ∈at \ {i∗} s.t. σ−1(j) < σ−1(i∗) and ∆t ̸= 0. Since all j ∈at \ {i∗} are symmetric,
we have Pr(∆t = j) = (1 −p)(1 −1"
IN THIS,0.8553672316384181,"kt ) ·
1
kt−1 = 1−p"
IN THIS,0.8564971751412429,"kt . Hence, the distribution of ∆t is ∆t = 

 
"
IN THIS,0.8576271186440678,"j
w.p. 1−p"
IN THIS,0.8587570621468926,"kt for j ∈at, j ̸= i∗"
IN THIS,0.8598870056497175,"i∗
w.p. p
0
w.p. 1−p kt ."
IN THIS,0.8610169491525423,"We denote this distribution by P∈(at, i∗)."
IN THIS,0.8621468926553673,"2. The case of i∗/∈at: With probability p, we are sampling from D′′
i∗, we have ∆t = xt = 0.
With probability 1 −p, we are sampling from Di∗. Conditional on this, with probability of
1
kt+1, σ−1(i∗) < σ−1(j) for all j ∈at and thus, ∆t = xt = 0. With probability 1 −
1
kt+1
there exists j ∈at s.t. σ−1(j) < σ−1(i∗) and ∆t ∈at. Since all j ∈at are symmetric, we
have Pr(∆t = j) = (1 −p)(1 −
1
kt+1) · 1"
IN THIS,0.8632768361581921,"kt =
1−p
kt+1. Hence the distribution of ∆t is ∆t ="
IN THIS,0.864406779661017,"(
j
w.p.
1−p
kt+1 for j ∈at
0
w.p. p + 1−p"
IN THIS,0.8655367231638418,kt+1 .
IN THIS,0.8666666666666667,We denote this distribution by P/∈(at).
IN THIS,0.8677966101694915,"To measure the information obtained from ∆t, we will use the KL divergence of the distribution
of ∆t under the data distribution D′
i∗from that under a benchmark data distribution. We use the
average distribution over {D′
i|i ∈[n]}, which is denoted by D = 1 n
P"
IN THIS,0.8689265536723164,"i∈n D′
i. The sampling process
is equivalent to drawing i∗∼Unif([n]) first and then sampling from D′
i∗. Under D, for any j ∈at,
we have"
IN THIS,0.8700564971751412,"Pr(∆t = j) = Pr(i∗∈at \ {j}) Pr(∆t = j|i∗∈at \ {j}) + Pr(i∗= j) Pr(∆t = j|i∗= j)
+ Pr(i∗/∈at) Pr(∆t = ej|i∗/∈at)"
IN THIS,0.8711864406779661,= kt −1
IN THIS,0.8723163841807909,"n
· 1 −p"
IN THIS,0.8734463276836159,"kt
+ 1"
IN THIS,0.8745762711864407,n · p + n −kt
IN THIS,0.8757062146892656,"n
· 1 −p"
IN THIS,0.8768361581920904,kt + 1 = (nkt −1)(1 −p)
IN THIS,0.8779661016949153,"nkt(kt + 1)
+ p n , and"
IN THIS,0.8790960451977401,Pr(∆t = 0) = Pr(i∗∈at) Pr(∆t = 0|i∗∈at) + Pr(i∗/∈at) Pr(∆t = 0|i∗/∈at) = kt
IN THIS,0.880225988700565,n · 1 −p
IN THIS,0.8813559322033898,"kt
+ n −kt"
IN THIS,0.8824858757062147,"n
· (p + 1 −p"
IN THIS,0.8836158192090395,kt + 1) = (n + 1)(1 −p)
IN THIS,0.8847457627118644,"n(kt + 1)
+ (n −kt)p n
."
IN THIS,0.8858757062146893,"Thus, the distribution of ∆t under D is ∆t ="
IN THIS,0.8870056497175142,"(
j
w.p. (nkt−1)(1−p)"
IN THIS,0.888135593220339,"nkt(kt+1)
+ p"
IN THIS,0.8892655367231639,"n for j ∈at
0
w.p. (n+1)(1−p)"
IN THIS,0.8903954802259887,"n(kt+1)
+ (n−kt)p n
."
IN THIS,0.8915254237288136,"We denote this distribution by P(at). Next we will compute the KL divergence of P/∈(at) and P∈(at)
from P(at). Since p =
ε
16n2 ≤
1
16n2 , we have (nkt−1)(1−p)"
IN THIS,0.8926553672316384,"nkt(kt+1)
+ p"
IN THIS,0.8937853107344633,"n ≤
1−p
kt+1 and (n+1)(1−p)"
IN THIS,0.8949152542372881,"n(kt+1)
+ (n−kt)p n
≤"
IN THIS,0.896045197740113,"1
kt + p. We will also use log(1 + x) ≤x for x ≥0 in the following calculation. For any i∗∈at, we
have"
IN THIS,0.8971751412429378,"DKL(P(at)∥P∈(at, i∗))"
IN THIS,0.8983050847457628,"=(kt −1)
(nkt −1)(1 −p)"
IN THIS,0.8994350282485876,"nkt(kt + 1)
+ p n"
IN THIS,0.9005649717514125,"
log

((nkt −1)(1 −p)"
IN THIS,0.9016949152542373,"nkt(kt + 1)
+ p"
IN THIS,0.9028248587570622,"n) ·
kt
1 −p "
IN THIS,0.903954802259887,"+
(nkt −1)(1 −p)"
IN THIS,0.9050847457627119,"nkt(kt + 1)
+ p n"
IN THIS,0.9062146892655367,"
log

((nkt −1)(1 −p)"
IN THIS,0.9073446327683616,"nkt(kt + 1)
+ p"
IN THIS,0.9084745762711864,n) · 1 p 
IN THIS,0.9096045197740112,"+
(n + 1)(1 −p)"
IN THIS,0.9107344632768362,"n(kt + 1)
+ (n −kt)p n"
IN THIS,0.911864406779661,"
log
(n + 1)(1 −p)"
IN THIS,0.9129943502824859,"n(kt + 1)
+ (n −kt)p n"
IN THIS,0.9141242937853107,"
·
kt
1 −p "
IN THIS,0.9152542372881356,"≤(kt −1)
(nkt −1)(1 −p)"
IN THIS,0.9163841807909604,"nkt(kt + 1)
+ p n"
IN THIS,0.9175141242937853,"
log( 1 −p"
IN THIS,0.9186440677966101,"kt + 1 ·
kt
1 −p) + 1 −p"
IN THIS,0.919774011299435,kt + 1 log(1 · 1 p) + ( 1
IN THIS,0.9209039548022598,"kt
+ p) · log (1 + pkt)"
IN THIS,0.9220338983050848,"≤0 +
1
kt + 1 log(1"
IN THIS,0.9231638418079096,p) + 2
IN THIS,0.9242937853107345,"kt
· pkt =
1
kt + 1 log(1"
IN THIS,0.9254237288135593,"p) + 2p .
(22)"
IN THIS,0.9265536723163842,"For P/∈(at), we have"
IN THIS,0.927683615819209,DKL(P(at)∥P/∈(at)) =kt
IN THIS,0.9288135593220339,(nkt −1)(1 −p)
IN THIS,0.9299435028248587,"nkt(kt + 1)
+ p n"
IN THIS,0.9310734463276836,"
log

((nkt −1)(1 −p)"
IN THIS,0.9322033898305084,"nkt(kt + 1)
+ p"
IN THIS,0.9333333333333333,n) · kt + 1 1 −p 
IN THIS,0.9344632768361582,"+
(n + 1)(1 −p)"
IN THIS,0.9355932203389831,"n(kt + 1)
+ (n −kt)p n 
log"
IN THIS,0.9367231638418079,(n + 1)(1 −p)
IN THIS,0.9378531073446328,"n(kt + 1)
+ (n −kt)p n"
IN THIS,0.9389830508474576,"
·
1
p + 1−p kt+1 ! ≤kt"
IN THIS,0.9401129943502825,(nkt −1)(1 −p)
IN THIS,0.9412429378531073,"nkt(kt + 1)
+ p n"
IN THIS,0.9423728813559322,"
log( 1 −p"
IN THIS,0.943502824858757,kt + 1 · kt + 1
IN THIS,0.9446327683615819,1 −p ) + ( 1
IN THIS,0.9457627118644067,"kt
+ p) log"
IN THIS,0.9468926553672317,(n + 1)(1 −p)
IN THIS,0.9480225988700565,"n(kt + 1)
+ (n −kt)p n"
IN THIS,0.9491525423728814,"
·
1
p + 1−p kt+1 !"
IN THIS,0.9502824858757062,=0 + ( 1
IN THIS,0.9514124293785311,"kt
+ p) log(1 + 1 −p(k2
t + kt + 1)
n(1 + ktp)
) ≤( 1"
IN THIS,0.9525423728813559,"kt
+ p)
1
n(1 + ktp) =
1
nkt
.
(23)"
IN THIS,0.9536723163841808,"Lower bound of the information
Now we adopt the similar framework used in the proofs
of Theorem 7 and 8. For notation simplicity, for all i ∈[n], let Pi denote the dynamics of
(x1, f1, ∆1, y1, by1, . . . , xT , fT , ∆T , yT , byT ) under D′
i and and P denote the dynamics under D.
Let Bt denote the event of {ft = 21at −1 for some non-empty at ⊂[n]}. As discussed before,
for any at, conditional on ¬Bt or yt = +1, (xt, ∆t, yt, byt) are identical in all {D′
i|i ∈[n]}, and
therefore, also identical in D. We can only obtain information at rounds when Bt ∧(yt = −1) occurs."
IN THIS,0.9548022598870056,"In such rounds, we know that xt is always 0, ft is fully determined by history (possibly with external
randomness , which does not depend on data distribution), yt = −1 and byt is fully determined by ∆t
(byt = +1 iff. ∆t ̸= 0)."
IN THIS,0.9559322033898305,"Therefore, conditional the history Ht−1 = (x1, f1, ∆1, y1, by1, . . . , xt−1, ft−1, ∆t−1, yt−1, byt−1)
before time t, we have"
IN THIS,0.9570621468926553,"DKL(P(xt, ft, ∆t, yt, byt|Ht−1)∥Pi(xt, ft, ∆t, yt, byt|Ht−1))"
IN THIS,0.9581920903954803,"=P(Bt ∧yt = −1)DKL(P(∆t|Ht−1, Bt ∧yt = −1)∥Pi(∆t|Ht−1, Bt ∧yt = −1))"
IN THIS,0.9593220338983051,"=6εP(Bt)DKL(P(∆t|Ht−1, Bt ∧yt = −1)∥Pi(∆t|Ht−1, Bt ∧yt = −1)) ,
(24)"
IN THIS,0.96045197740113,where the last equality holds due to that yt ∼Rad(1 −6ε) and does not depend on Bt.
IN THIS,0.9615819209039548,For any algorithm that can successfully identify i under the data distribution Di with probability 3
IN THIS,0.9627118644067797,"4
for all i ∈[n], then PDi(iout = i) ≥3"
IN THIS,0.9638418079096045,4 and PDj(iout = i) ≤1
IN THIS,0.9649717514124294,"4 for all j ̸= i. Recall that Di and D′
i
are very close when the mixture parameter p is small. Combining with Eq (21), we have"
IN THIS,0.9661016949152542,|Pi(iout = i) −Pj(iout = i)|
IN THIS,0.9672316384180791,"≥
PDi(iout = i) −PDj(iout = i)
 −|PDi(iout = i) −Pi(iout = i)| −
PDj(iout = i) −Pj(iout = i) ≥1 2 −1 4 = 1 4 ."
IN THIS,0.9683615819209039,Then we have the total variation distance between Pi and Pj
IN THIS,0.9694915254237289,"TV(Pi, Pj) ≥|Pi(iout = i) −Pj(iout = i)| ≥1"
IN THIS,0.9706214689265537,"4 .
(25)"
IN THIS,0.9717514124293786,Then we have
IN THIS,0.9728813559322034,"Ei∼Unif([n])

TV2(Pi, P(i+1) mod n)

≤4Ei∼Unif([n])

TV2(Pi, P)
"
IN THIS,0.9740112994350283,"≤2Ei

DKL(P∥Pi)

(Pinsker’s ineq) =2Ei "" T
X"
IN THIS,0.9751412429378531,"t=1
DKL(P(xt, ft, ∆t, yt, byt|Ht−1)∥Pi(xt, ft, ∆t, yt, byt|Ht−1)) #"
IN THIS,0.976271186440678,(Chain rule)
IN THIS,0.9774011299435028,"≤12εEi "" T
X t=1"
IN THIS,0.9785310734463277,"P(Bt)DKL(P(∆t|Ht−1, Bt ∧yt = −1)∥Pi(∆t|Ht−1, Bt ∧yt = −1)) #"
IN THIS,0.9796610169491525,"(Apply Eq (24)) ≤12ε n T
X t=1 P(Bt) n
X"
IN THIS,0.9807909604519774,"i=1
DKL(P(∆t|Ht−1, Bt ∧yt = −1)∥Pi(∆t|Ht−1, Bt ∧yt = −1)) =12ε"
IN THIS,0.9819209039548022,"n Ea1:T ∼P  
T
X"
IN THIS,0.9830508474576272,"t=1
1(Bt)  X"
IN THIS,0.984180790960452,"i:i∈at
DKL(P(at)∥P∈(at)) +
X"
IN THIS,0.9853107344632769,"i:i/∈at
DKL(P(at)∥P/∈(at))     ≤12ε"
IN THIS,0.9864406779661017,"n Ea1:T ∼P  
X"
IN THIS,0.9875706214689266,t:1(Bt)=1  X
IN THIS,0.9887005649717514,i:i∈at
IN THIS,0.9898305084745763,"
1
kt + 1 log(1"
IN THIS,0.9909604519774011,"p) + 2p

+
X"
IN THIS,0.992090395480226,i:i/∈at
NKT,0.9932203389830508,"1
nkt   "
NKT,0.9943502824858758,"(Apply Eq (22),(23)) ≤12ε n T
X"
NKT,0.9954802259887006,"t=1
(log(1"
NKT,0.9966101694915255,p) + 2np + 1)
NKT,0.9977401129943503,"≤12Tε(log(16n2/ε) + 2) n
."
NKT,0.9988700564971752,"Combining with Eq (25), we have that there exists a universal constant c such that T ≥
cn
ε(log(n/ε)+1)."
