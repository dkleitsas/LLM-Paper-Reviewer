Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009970089730807576,"Recent research on Self-Supervised Learning (SSL) has demonstrated its ability to
1"
ABSTRACT,0.0019940179461615153,"extract high-quality representations from unlabeled samples. However, in continual
2"
ABSTRACT,0.0029910269192422734,"learning scenarios where training data arrives sequentially, SSL’s performance
3"
ABSTRACT,0.003988035892323031,"tends to deteriorate. This study focuses on Continual Contrastive Self-Supervised
4"
ABSTRACT,0.004985044865403789,"Learning (CCSSL) and highlights that the absence of contrastive learning on inter-
5"
ABSTRACT,0.005982053838484547,"task data, due to the unavailability of historical samples, leads to a significant drop
6"
ABSTRACT,0.006979062811565304,"in performance. To tackle this issue, we introduce a simple and effective method
7"
ABSTRACT,0.007976071784646061,"called BGE, which Bridges the inter-task Gap of CCSSL using External data from
8"
ABSTRACT,0.00897308075772682,"publicly available datasets. BGE enables the contrastive learning of each task data
9"
ABSTRACT,0.009970089730807577,"with external data, allowing relationships between them to be passed along the tasks,
10"
ABSTRACT,0.010967098703888335,"thereby facilitating implicit inter-task data comparisons. To overcome the limitation
11"
ABSTRACT,0.011964107676969093,"of the external data selection and maintain its effectiveness, we further propose
12"
ABSTRACT,0.01296111665004985,"the One-Propose-One algorithm to collect more relevant and diverse high-quality
13"
ABSTRACT,0.013958125623130608,"samples from the chosen external data while filtering out distractions from the out-
14"
ABSTRACT,0.014955134596211365,"of-distribution data. Experiments show that BGE can generate better discriminative
15"
ABSTRACT,0.015952143569292122,"representation in CCSSL, especially for inter-task data, and improve classification
16"
ABSTRACT,0.01694915254237288,"results with various external data compositions. Additionally, the proposed method
17"
ABSTRACT,0.01794616151545364,"can be seamlessly integrated into existing continual learning methods yielding
18"
ABSTRACT,0.018943170488534396,"significant performance improvement.
19"
INTRODUCTION,0.019940179461615155,"1
Introduction
20"
INTRODUCTION,0.020937188434695914,"In recent years, deep neural networks [13, 22, 35] have achieved great success, but plenty of works
21"
INTRODUCTION,0.02193419740777667,"are under the assumption that all data are available simultaneously for training. In practical scenarios,
22"
INTRODUCTION,0.022931206380857428,"acquiring the entire dataset at once is often challenging due to data being constantly updated. In this
23"
INTRODUCTION,0.023928215353938187,"case, training the network continually suffers from catastrophic forgetting [38], meaning that the
24"
INTRODUCTION,0.024925224327018942,"network severely forgets old task knowledge after learning the new one. Hence, continual learning
25"
INTRODUCTION,0.0259222333000997,"investigates methods to train networks incrementally while mitigating catastrophic forgetting.
26"
INTRODUCTION,0.026919242273180457,"Although continual learning has been widely studied and numerous effective methods [32, 36, 40]
27"
INTRODUCTION,0.027916251246261216,"have been proposed, most existing research remains focused on supervised learning, with Continual
28"
INTRODUCTION,0.028913260219341975,"Contrastive Self-Supervised Learning (CCSSL) receiving relatively little attention. However, studying
29"
INTRODUCTION,0.02991026919242273,"CCSSL is equally significant.
30"
INTRODUCTION,0.03090727816550349,"To prevent catastrophic forgetting, prior CCSSL works CaSSLe [16], PFR [18], and POCON [19]
31"
INTRODUCTION,0.031904287138584245,"use knowledge distillation, while CPPF [11] incorporates prototype clustering. In this paper, we
32"
INTRODUCTION,0.03290129611166501,"highlight an important but generally overlooked issue in these works: Comparisons of inter-task
33"
INTRODUCTION,0.03389830508474576,"data are absent. Specifically, a widely accepted opinion in continual learning is that if the sum of
34"
INTRODUCTION,0.03489531405782652,"each task’s loss is minimized, then continual learning’s performance reaches its upper bound: joint
35"
INTRODUCTION,0.03589232303090728,"learning. However, in CCSSL, even if each task’s loss is minimized, there is still a gap between joint
36"
INTRODUCTION,0.036889332003988036,"Compare
Compare"
INTRODUCTION,0.03788634097706879,"Task t-1
Task t"
INTRODUCTION,0.038883349950149554,Incomparable
INTRODUCTION,0.03988035892323031,External Dataset …
INTRODUCTION,0.040877367896311065,"Compare
Compare"
INTRODUCTION,0.04187437686939183,"Bridge the 
Inter-task 
Gap
Compare"
INTRODUCTION,0.04287138584247258,"a) Fine-tune
b) CaSSLe"
INTRODUCTION,0.04386839481555334,"d) Joint
c) Ours"
INTRODUCTION,0.0448654037886341,"bottle
fox
cloud
maple"
INTRODUCTION,0.045862412761714856,"Figure 1:
Left: Overview of our method BGE. In typical CCSSL methods, the inter-task data
pairs are incomparable. We employ an external dataset to complement these missing comparisons,
effectively bridging the inter-task gap. Right: t-SNE [47] visualization of four classes belonging to
different tasks in continual learning. Compared to prior methods Fine-tune and CaSSLe [16], we
make the inter-task data more separable."
INTRODUCTION,0.04685942173479561,"learning. Because joint learning requires any sample pair in the entire dataset to participate in the
37"
INTRODUCTION,0.047856430707876374,"contrastive loss computation. In contrast, in continual learning, inter-task data are unavailable to each
38"
INTRODUCTION,0.04885343968095713,"other, meaning this aspect of the contrastive loss is never computed and optimized. This omission
39"
INTRODUCTION,0.049850448654037885,"increases the likelihood of inter-task class confusion, as illustrated in Figure 1 Right, despite classes
40"
INTRODUCTION,0.05084745762711865,"from four different tasks having distinctly different semantics, they still show confusion in prior
41"
INTRODUCTION,0.0518444666001994,"methods Fine-tune and CaSSLe [16]. In contrast, our method and joint training consider inter-task
42"
INTRODUCTION,0.05284147557328016,"comparisons and can better distinguish them.
43"
INTRODUCTION,0.053838484546360914,"Since we could not directly use data from other tasks for inter-task comparisons, we would like to
44"
INTRODUCTION,0.054835493519441676,"compensate for these comparisons with the help of external data. Some prior works [31, 52, 56]
45"
INTRODUCTION,0.05583250249252243,"have explored using external data for continual learning. GD [31] and ZSCL [56] use external
46"
INTRODUCTION,0.05682951146560319,"data for distillation to stabilize the feature space, while requiring extensive external data and high
47"
INTRODUCTION,0.05782652043868395,"computational costs. ST [52] employs external data as additional training data, but as a supervised
48"
INTRODUCTION,0.058823529411764705,"method, it requires pseudo-labels, making it less robust to out-of-distribution (OOD) data. Tang et
49"
INTRODUCTION,0.05982053838484546,"al. [45] enhance exemplar diversity with external data. Existing methods focus on using external
50"
INTRODUCTION,0.06081754735792622,"data in supervised learning, but given that CCSSL does not require labels for training, we propose
51"
INTRODUCTION,0.06181455633100698,"using external data in CCSSL, which avoids the need for pseudo-labels and is more generalizable and
52"
INTRODUCTION,0.06281156530408774,"robust to OOD data. Besides, our motivation is to improve feature space by compensating for absent
53"
INTRODUCTION,0.06380857427716849,"comparisons rather than merely stabilizing it, and it does not require extensive external data.
54"
INTRODUCTION,0.06480558325024925,"In summary, we propose incorporating publicly available external data into training to compensate for
55"
INTRODUCTION,0.06580259222333001,"the absent inter-task comparisons, as shown in Figure 1 Left. When the external dataset is sufficiently
56"
INTRODUCTION,0.06679960119641076,"large, it is reasonable to assume a high probability that some external data share similar features with
57"
INTRODUCTION,0.06779661016949153,"the task data, even if they are in different classes. By incorporating these high-quality external data
58"
INTRODUCTION,0.06879361914257229,"into CCSSL, the data from each task can be compared with them. enables the inter-data relationship to
59"
INTRODUCTION,0.06979062811565304,"be passed along the tasks, thereby constructing implicit inter-task comparisons. Further, considering
60"
INTRODUCTION,0.0707876370887338,"that external data in open-world scenarios may contain extensive OOD data that is not beneficial for
61"
INTRODUCTION,0.07178464606181456,"task training, we propose the One-Propose-One (OPO) sampling algorithm, to sample high-quality
62"
INTRODUCTION,0.07278165503489531,"external data that are relevant to tasks and sufficiently diverse without any hyperparameters.
63"
INTRODUCTION,0.07377866400797607,"Experiments demonstrate that BGE can be seamlessly integrated into existing methods, resulting
64"
INTRODUCTION,0.07477567298105683,"in significant performance improvement. We also point out that although it may seem unsurprising
65"
INTRODUCTION,0.07577268195413758,"that network performance improves with more training data, this improvement is not due to richer
66"
INTRODUCTION,0.07676969092721835,"input features, because when we add equal external data into joint training, the performance doesn’t
67"
INTRODUCTION,0.07776669990029911,"improve even sometimes decreases. Instead, BGE compensates for the absent comparisons caused by
68"
INTRODUCTION,0.07876370887337986,"inter-task data unavailability, which is much more meaningful in continual learning. Our contributions
69"
INTRODUCTION,0.07976071784646062,"can be summarized as follows:
70"
INTRODUCTION,0.08075772681954138,"• We point out that existing methods overlook the issue of inter-task data comparisons, and
71"
INTRODUCTION,0.08175473579262213,"propose BGE to incorporate external data into training to address this gap.
72"
INTRODUCTION,0.08275174476570289,"• We propose the One-Propose-One (OPO) sampling algorithm to sample external data that
73"
INTRODUCTION,0.08374875373878365,"are relevant to tasks and sufficiently diverse, while also filtering out OOD data that are not
74"
INTRODUCTION,0.0847457627118644,"beneficial for learning.
75"
INTRODUCTION,0.08574277168494517,"• Experiments show that BGE can be seamlessly integrated into existing CCSSL methods and
76"
INTRODUCTION,0.08673978065802593,"consistently yields significant improvement.
77"
RELATED WORK,0.08773678963110668,"2
Related work
78"
RELATED WORK,0.08873379860418744,"Self-Supervised Learning (SSL)
SSL trains the network without the need for supervised signals.
79"
RELATED WORK,0.0897308075772682,"One of the prominent branches is contrastive learning [5, 8–10, 21, 23, 53]. The objective of
80"
RELATED WORK,0.09072781655034895,"contrastive learning can be roughly explained as reducing the distance between positive pairs while
81"
RELATED WORK,0.09172482552342971,"enlarging it between negative pairs. SimCLR [8] simply follows this objective but requires a large
82"
RELATED WORK,0.09272183449651047,"batch size. MoCo [10, 23] introduces a momentum encoder and a negative sample dictionary to
83"
RELATED WORK,0.09371884346959122,"solve this problem. SwAV [5] and Barlow Twins [53] introduces prototype comparisons and cross-
84"
RELATED WORK,0.09471585244267199,"decorrelation loss, respectively. Then BYOL [21] and SimSiam [9] can conduct contrastive learning
85"
RELATED WORK,0.09571286141575275,"without negative samples. However, all these methods assume that a large dataset is available for
86"
RELATED WORK,0.0967098703888335,"pre-training, which is often impractical in real-world scenarios where data acquisition is incremental.
87"
RELATED WORK,0.09770687936191426,"Therefore, we research a continual method, which is more practical.
88"
RELATED WORK,0.09870388833499502,"Since no labeling requirement, incorporating external data into SSL is straightforward. Prior long-
89"
RELATED WORK,0.09970089730807577,"tailed SSL works [3, 28] leverage external data to balance head and tail classes. Instead, we extend the
90"
RELATED WORK,0.10069790628115653,"exploration to continual learning, aiming to use external data to compensate for the absent inter-task
91"
RELATED WORK,0.1016949152542373,"comparisons while further preventing catastrophic forgetting.
92"
RELATED WORK,0.10269192422731804,"Continual learning
Continual learning allows the network to learn from sequentially arriving data
93"
RELATED WORK,0.1036889332003988,"and prevent catastrophic forgetting. Existing continual learning methods can be categorized into
94"
RELATED WORK,0.10468594217347957,"three groups, which are 1) Regularization-based methods [1, 14, 29, 32, 34, 50, 54] add additional
95"
RELATED WORK,0.10568295114656032,"regularization constraints such as knowledge distillation [14, 32, 50] or limiting important parameters
96"
RELATED WORK,0.10667996011964108,"update [1, 29, 34, 54] to network training. 2) Replay-based methods [4, 26, 40, 43, 55] save few
97"
RELATED WORK,0.10767696909272183,"representative data from old tasks called exemplars to recover the distribution of old data when the
98"
RELATED WORK,0.10867397806580259,"new task is trained. 3) Architecture-based methods [15, 36, 37, 41, 51], which adjust the architecture
99"
RELATED WORK,0.10967098703888335,"or parameters of the network during each task training. Currently, most continual learning methods
100"
RELATED WORK,0.1106679960119641,"still focus on supervised learning. While some of them [6, 33, 44] draw on the idea of contrastive
101"
RELATED WORK,0.11166500498504486,"learning, there are still few works consider continual learning without any supervision. Among them,
102"
RELATED WORK,0.11266201395812563,"CaSSLe [16], PFR[18], and POCON[19] use distillation, and CPPF[11] adds clustering to form
103"
RELATED WORK,0.11365902293120637,"a more complete framework. Sy-CON [7] also reveals the distinction between CCSSL and joint
104"
RELATED WORK,0.11465603190428714,"training, but it only additionally passes current task data into the old network to get more diverse
105"
RELATED WORK,0.1156530408773679,"intra-task negative features, which still fails to provide effective inter-task comparisons. Thus it
106"
RELATED WORK,0.11665004985044865,"underperforms in most contrastive learning frameworks. Compared to them, we introduce external
107"
RELATED WORK,0.11764705882352941,"data to facilitate implicit inter-task comparisons to solve the problem of absent inter-task comparisons.
108"
PROPOSED METHOD,0.11864406779661017,"3
Proposed method
109"
PRELIMINARY,0.11964107676969092,"3.1
Preliminary
110"
PRELIMINARY,0.12063808574277168,"Contrastive Self-Supervised Learning (CSSL)
In Self-Supervised Learning (SSL), the dataset D
111"
PRELIMINARY,0.12163509471585245,"contains only n image inputs {x1, x2, ..., xn} without labels. SSL trains a network fθ parameterized
112"
PRELIMINARY,0.1226321036889332,"by θ to map these inputs to embeddings {z1, z2, ..., zn}. Many well-known SSL works [5, 8, 21, 23,
113"
PRELIMINARY,0.12362911266201396,"53] use contrastive learning framework. In contrastive learning, a random augmentation function
114"
PRELIMINARY,0.12462612163509472,"A is pre-designed. Given an input x, two augmented views (xa, xb) are obtained by applying A
115"
PRELIMINARY,0.12562313060817548,"twice. Subsequently, embeddings za = fθ(xa) and zb = fθ(xb) are passed through a projector hθ′
116"
PRELIMINARY,0.12662013958125623,"parameterized by θ′ to get z′
a = hθ′(za), z′
b = hθ′(zb), which are involved in LSSL. In essence,
117"
PRELIMINARY,0.12761714855433698,"LSSL expects the network to output similar embeddings for two views of the same input (i.e. positive
118"
PRELIMINARY,0.12861415752741776,"pair), while ensuring that embeddings from views of different inputs (i.e. negative pair) are dissimilar.
119"
PRELIMINARY,0.1296111665004985,"Continual CSSL (CCSSL)
In CCSSL setting, The overall dataset D is divided into multiple tasks.
120"
PRELIMINARY,0.13060817547357925,"Assuming that T tasks {T1, T2, ..., TT } are to be learned, D can be divided into {D1, D2, ..., DT },
121"
PRELIMINARY,0.13160518444666003,"where Di ∩Dj = ∅, ∀i, j ∈{1 : T}. Also as SSL, for each task Tt , Dt is only composed of nt
122"
PRELIMINARY,0.13260219341974078,"images {x1, x2, ..., xnt} without labels. Continual learning requires the network to learn knowledge
123"
PRELIMINARY,0.13359920239282153,"as each task’s data arrives sequentially, with dataset Di only available at Ti. The optimization
124"
PRELIMINARY,0.1345962113659023,"objective is to continually train the network parameter θ to satisfy every task, which is defined as:
125"
PRELIMINARY,0.13559322033898305,"argmin
θ T
X"
PRELIMINARY,0.1365902293120638,"t=1
E(xa,xb)∼A(Dt)LSSL(hθ′(fθ(xa)), hθ′(fθ(xb)))
(1)"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.13758723828514458,"3.2
Revising and improving CCSSL via external data
126"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.13858424725822532,"Typical contrastive learning paradigms [8, 23, 53] can be generalized as reducing distances between
127"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.13958125623130607,"positive pairs and enlarging them between negative pairs on feature hyperspheres. Adjusting the
128"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.14057826520438685,"interrelationships of sample pairs in this way enables the network to effectively represent features
129"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.1415752741774676,"[27, 49]. However, in CCSSL, the data is divided by tasks. During the learning process of task Tt, data
130"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.14257228315054835,"from other tasks are unavailable. This prevents adequate tuning of inter-sample relationships, resulting
131"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.14356929212362912,"in suboptimal network training. We identify two reasons for this suboptimality: 1) The network
132"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.14456630109670987,"rapidly forgets knowledge about old data due to catastrophic forgetting, so their features cannot
133"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.14556331006979062,"be well extracted in subsequent tasks. 2) Insufficient learning about each task occurs because data
134"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.1465603190428714,"from one task cannot act as negative samples for another task. While prior works address problem 1
135"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.14755732801595214,"through techniques like distillation [16, 18, 19] and clustering [11], problem 2 remains underexplored.
136"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.1485543369890329,"However, we argue that this is unreasonable, and solving problem 2 is equally important.
137"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.14955134596211367,"Prior works [20, 32] widely agree that in the ideal case, continual learning can perform up to joint
138"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.15054835493519442,"learning, wherein no forgetting occurs and each task reaches optimality. However, in CSSL, even if
139"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.15154536390827517,"no forgetting occurs, there is still an optimization gap between continual and joint learning due to the
140"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.15254237288135594,"absence of inter-task data comparisons in the training objective. Unlike supervised learning which
141"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.1535393818544367,"guides the network through labels, CSSL relies on data interactions for network learning. When data
142"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.15453639082751744,"is incomplete, the training objective also becomes incomplete. For better comprehension, we can
143"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.15553339980059822,"decompose the joint training contrastive loss into two terms as in Eq. 2, representing the comparisons
144"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.15653040877367896,"of intra-task and inter-task data, denoted as Lintra and Linter, respectively. Lintra is the training
145"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.1575274177467597,"objective of the conventional CCSSL, also referred to as Lcontinual. However, for input x ∈Dt
146"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.1585244267198405,"in task Tt, negative samples come exclusively from Dt rather than the overall dataset D, making
147"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.15952143569292124,"direct comparisons between inter-task data infeasible. Consequently, Linter can not be computed and
148"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.16051844466600199,"optimized in continual learning forever, resulting in a Linter gap between Lcontinual and Ljoint.
149"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.16151545363908276,"Ljoint = 1 T T
X t=1"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.1625124626121635,"
Lintra = Lcontinual
z
}|
{
E(xa,xb)∼A(Dt)LSSL (hθ′ (fθ (xa)) , hθ′ (fθ (xb)))"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.16350947158524426,"+ E
xa∼A(Dt),
xb∼A(D−Dt) LSSL (hθ′ (fθ (xa)) , hθ′ (fθ (xb)))
|
{z
}
Linter 
(2)"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.16450648055832504,"We argue that the lack of optimization for Linter leads to confusion between inter-task data. Figure 1
150"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.16550348953140578,"Right compares the t-SNE visualizations of features from 4 CIFAR100 classes under joint and 10
151"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.16650049850448653,"tasks continual training (4 classes belong to different tasks during continual training). Compared to
152"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.1674975074775673,"the joint-trained network, the continually trained network shows poor clustering and severe class
153"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.16849451645064806,"boundary confusion. More experiments about inter-task confusion can be found at Appendix A.2.1.
154"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.1694915254237288,"Despite CaSSLe [16] employing distillation to consolidate old knowledge, the issue of inter-task class
155"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.17048853439680958,"boundary confusion remains. To address the overlooked problem of Linter, a straightforward idea
156"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.17148554336989033,"is to save exemplars for each task. However, this may raise serious privacy concerns. We therefore
157"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.17248255234297108,"explore an alternative method to optimize Linter without exemplars and protect the discriminative
158"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.17347956131605186,"class boundaries. Figure 1c shows the feature distribution of our method, with all 4 inter-task classes
159"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.1744765702891326,"better distinguished, and the overall distribution closer to joint training.
160"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.17547357926221335,"To compensate for Linter, bridging the gap of inter-task comparisons is essential. This requires
161"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.17647058823529413,"introducing additional comparisons into each task, implying extra data incorporation. Under the
162"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.17746759720837488,"constraints of continual learning, simultaneous access to data from multiple tasks is infeasible.
163"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.17846460618145563,"Therefore, the idea emerges to incorporate publicly available external data into CCSSL to address the
164"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.1794616151545364,"lack of inter-task comparisons. Each task’s data can be directly compared with external data, enabling
165"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.18045862412761715,"relationships between data to be passed along the task sequence. Moreover, using external data better
166"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.1814556331006979,"protects privacy, and the costs of obtaining unlabeled data from public data sources are extremely low.
167"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.18245264207377868,"We thus propose our method BGE, meaning Bridging the inter-task comparison Gap with External
168"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.18344965104685942,"data, as shown in Figure 1 Left. BGE incorporates external data into each task’s training except
169"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.18444666001994017,"the first one, and resamples part of them after each task using our sampling algorithm ( detailed in
170"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.18544366899302095,"Section 3.3). This external data acts as a bridge for inter-task comparisons, constructing implicit
171"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.1864406779661017,"comparisons for inter-task data. For task Tt, with Dt−1
e
as the external data sampled after task Tt−1,
172"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.18743768693918245,"the training objective is defined as:
173"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.18843469591226322,"Lt = E(xa,xb)∼A(Dt∪Dt−1
e
)LSSL (hθ′ (fθ (xa)) , hθ′ (fθ (xb)))
(3)"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.18943170488534397,"Incorporating external data aligns the optimization objective of continual learning more closely with
174"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.19042871385842472,"Eq. 2, enhancing the mutual understanding of inter-task classes.
175"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.1914257228315055,"3.3
One-Propose-One (OPO) sampling
176"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.19242273180458624,"While abundant external data features generally cover in-task data comprehensively, incorporating all
177"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.193419740777667,"external data into continual learning is impractical due to computational constraints. Additionally,
178"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.19441674975074777,"open-world external data may include substantial task-irrelevant out-of-distribution (OOD) data,
179"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.19541375872382852,"which is unhelpful for training. Therefore, a sampling algorithm is needed to select high-quality
180"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.19641076769690927,"external data. We observe that Linter includes comparisons of current task data Dt with both old task
181"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.19740777666999004,"data D1:t−1 and future task data Dt+1:T . So sampled external data should ideally proxy for both old
182"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.1984047856430708,"and future task data. To represent old data, sampled data should have similar features to them, while
183"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.19940179461615154,"representing future data requires imaginative sampling. Therefore, our sampling algorithm is based
184"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.20039880358923232,"on both proximity and diversity considerations, and integrates these two aspects into a single objective
185"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.20139581256231306,"without any hyperparameters. We noted that prior sampling algorithms [3, 28] for long-tailed learning
186"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.2023928215353938,"also consider proximity and diversity, but they require hyperparameters selection.
187"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.2033898305084746,"We measure proximity using the cosine distance between sample features. On the other hand, prior
188"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.20438683948155534,"work [49] indicates that to avoid collapse, contrastive learning methods tend to map all inputs to
189"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.2053838484546361,"a uniform distribution within the feature hypersphere (i.e. uniformity). Thus we assume that the
190"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.20638085742771686,"entire distribution of the current task data approximately covers the hypersphere, ensuring diversity.
191"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.2073778664007976,"Based on the above, we propose a sampling algorithm called One-Propose-One (OPO) as depicted
192"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.20837487537387836,"in Algorithm 1. After training each task Tt, OPO constructs the external dataset Dt
e, which is then
193"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.20937188434695914,"incorporated in training task Tt+1. Specifically, OPO considers that each in-task sample can equally
194"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.21036889332003988,"propose an external sample with the closest feature distance to itself and has not been proposed.
195"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.21136590229312063,"Given the current task budget Kt, we collect all proposed samples as a candidate set Dc, and select
196"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.2123629112662014,"the Kt minimum distance samples to be added to the external dataset Dt
e. We follow iCaRL [40]’s
197"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.21335992023928216,"exemplar update algorithm, maintaining an equal budget for each task within the total budget K.
198"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.2143569292123629,"OPO ensures proximity and diversity without hyperparameters, maintaining similarity to old data and
199"
REVISING AND IMPROVING CCSSL VIA EXTERNAL DATA,0.21535393818544366,"adequate coverage of future data features.
200"
EXPERIMENTS,0.21635094715852443,"4
Experiments
201"
EXPERIMENTAL SETUP,0.21734795613160518,"4.1
Experimental setup
202"
EXPERIMENTAL SETUP,0.21834496510468593,"Dataset setup
We conduct experiments with the following datasets: 1) CIFAR100 [30], which
203"
EXPERIMENTAL SETUP,0.2193419740777667,"contains 100 classes, each with 500 train images and 100 test images. Each image is 32×32 pixels.
204"
EXPERIMENTAL SETUP,0.22033898305084745,"We follow the class incremental learning setting to split the classes equally by the number of tasks.
205"
EXPERIMENTAL SETUP,0.2213359920239282,"Experiments are conducted under 4 tasks and 10 tasks settings, wherein each task contains 25 classes
206"
EXPERIMENTAL SETUP,0.22233300099700898,"Algorithm 1 One-Propose-One(OPO) Sampling Algorithm
Input: current task ID t, current task dataset Dt, entire external dataset Dout, last task sampled
external dataset Dt−1
e
, model f, total budget K, cosine distance metric cos(·, ·)
Output: sampled external dataset Dt
e
1: Calculate current task budget Kt = K"
EXPERIMENTAL SETUP,0.22333000997008973,"t , Adjust Dt−1
e
= REDUCEDATA(Dt−1
e
, Kt) [40]
2: Create candidate set Dc = {}
3: while | Dc |< Kt do
4:
for each x ∈Dt do
5:
u = argminx′∈(Dout−Dt−1
e
)cos(f(x), f(x′)), du = minxi∈Dtcos(f(xi), f(u))
6:
Dc = Dc ∪{u}, Dout = Dout −{u}
7:
end for
8: end while
9: D′
c = SORT(Dc, key = du) [: Kt], Dt
e = Dt−1
e
∪D′
c
10: return Dt
e"
EXPERIMENTAL SETUP,0.22432701894317048,"and 10 classes. 2) ImageNet100 [46], which consists of 100 classes selected from ImageNet [12],
207"
EXPERIMENTAL SETUP,0.22532402791625125,"with a total of 130K images of 224×224 pixels. It is equally split under 5 tasks and 10 tasks settings.
208"
EXPERIMENTAL SETUP,0.226321036889332,"External dataset setup
For CIFAR100, the selected external datasets include CIFAR10,
209"
EXPERIMENTAL SETUP,0.22731804586241275,"Places365test (the test set of Places365 [57]) and ImageNet-R [24], among them, Places365test and
210"
EXPERIMENTAL SETUP,0.22831505483549352,"ImageNet-R are OOD for CIFAR100. CIFAR10 contains 50,000 images with 32×32 pixels in 10
211"
EXPERIMENTAL SETUP,0.22931206380857427,"classes. Places365 is a scene recognition dataset with its test set containing 328,500 images of various
212"
EXPERIMENTAL SETUP,0.23030907278165502,"scenes. ImageNet-R contains 24,000 images featuring art, cartoons, and other styles. We resize both
213"
EXPERIMENTAL SETUP,0.2313060817547358,"Places365test and ImageNet-R to 32×32 pixels. We consider three compositions of external datasets,
214"
EXPERIMENTAL SETUP,0.23230309072781655,"CIFAR (CIFAR10), CP (CIFAR10+Places365test) and CPI (CIFAR10+Places365test+ImageNet-R)
215"
EXPERIMENTAL SETUP,0.2333000997008973,"For ImageNet100, the external datasets include ImageNet900, Places365 and DomainNet [39].
216"
EXPERIMENTAL SETUP,0.23429710867397807,"ImageNet900 is all data in ImageNet excluding ImageNet100, totaling 1.1 million images. Places365
217"
EXPERIMENTAL SETUP,0.23529411764705882,"contains 1.8 million images, and DomainNet contains 0.6 million images of 6 domains. They are also
218"
EXPERIMENTAL SETUP,0.23629112662013957,"used here as OOD data. All data are 224×224 pixels. We consider three compositions of external
219"
EXPERIMENTAL SETUP,0.23728813559322035,"datasets, IN (ImageNet-900), INP (ImageNet900+Places365) and IND (ImageNet900+DomainNet).
220"
EXPERIMENTAL SETUP,0.2382851445663011,"Baselines
We compare the original performance of existing exemplar-free CCSSL methods to their
221"
EXPERIMENTAL SETUP,0.23928215353938184,"performance when with BGE. The methods we compare include 1) Fine-Tune (FT): Sequentially
222"
EXPERIMENTAL SETUP,0.24027916251246262,"training the network with data from each task without additional prevention of catastrophic forgetting.
223"
EXPERIMENTAL SETUP,0.24127617148554337,"2) CaSSLe [16]: Introducing a distillation loss between the current model and the old model in
224"
EXPERIMENTAL SETUP,0.24227318045862412,"the form of contrastive loss. 3) PFR [18]: Addressing catastrophic forgetting based on functional
225"
EXPERIMENTAL SETUP,0.2432701894317049,"regularization [17]. We slightly optimized its network structure and training procedure.
226"
EXPERIMENTAL SETUP,0.24426719840478564,"Training and evaluation setup
Unless specified otherwise, all experiments employ Barlow Twins
227"
EXPERIMENTAL SETUP,0.2452642073778664,"[53] as the contrastive learning framework and Resnet18 [22] as the backbone. The sampling budget
228"
EXPERIMENTAL SETUP,0.24626121635094717,"is uniformly set at 10K. For evaluation, we follow [16, 18, 19] to report the linear evaluation accuracy
229"
EXPERIMENTAL SETUP,0.2472582253240279,"of the final network across all classes as the evaluation metric. For other setups see Appendix A.1.
230"
RESULTS,0.24825523429710866,"4.2
Results
231"
RESULTS,0.24925224327018944,"Performance improvement on prior methods
We compare the performance improvement BGE
232"
RESULTS,0.2502492522432702,"yields to the base methods when using different external data compositions. Table 1 shows that
233"
RESULTS,0.25124626121635096,"on CIFAR100, BGE can consistently and significantly improve base methods. It is worth noting
234"
RESULTS,0.2522432701894317,"that as the number of tasks increases, BGE yields even greater improvement, with improvement of
235"
RESULTS,0.25324027916251246,"1.5%-3.5% for 4 tasks and 2.5%-7% for 10 tasks. This is also in line with our motivation, as an
236"
RESULTS,0.2542372881355932,"increasing number of tasks results in more missing inter-task data comparisons.
237"
RESULTS,0.25523429710867396,"Moreover, across different external dataset compositions, we observe that CIFAR yields the most
238"
RESULTS,0.25623130608175476,"significant improvement. This is attributed to the CIFAR10 dataset best matches the distribution of
239"
RESULTS,0.2572283150548355,"CIFAR100, thereby offering highly relevant features, even if their classes do not intersect. When in-
240"
RESULTS,0.25822532402791626,"corporating datasets like Places365 or ImageNet-R, which are OOD for CIFAR100, the improvement
241"
RESULTS,0.259222333000997,"decreases. Thanks to our OPO sampling algorithm can well resist the harm of OOD data (detailed in
242"
RESULTS,0.26021934197407776,"Table 1: Comparison of BGE’s performance improvement on CIFAR100. CIFAR, CP, and CPI are
different external dataset compositions. Performance was evaluated by linear evaluation accuracy of
the final network. We equally divided classes into 4 tasks and 10 tasks. BGE consistently improves
base methods across different external dataset compositions. As for Joint training, ED represents
adding equivalent external data, which does not improve the performance."
RESULTS,0.2612163509471585,"Methods
CIFAR
CP
CPI"
TASKS,0.2622133599202393,"4tasks
10tasks
4tasks
10tasks
4tasks
10tasks"
TASKS,0.26321036889332006,"FT
56.19
49.36
56.19
49.36
56.19
49.36
FT+BGE
59.49(+3.30) 56.62(+7.26) 58.69(+2.50) 55.14(+5.78) 58.71(+2.52) 55.74(+6.38)"
TASKS,0.2642073778664008,"CaSSLe [16]
60.04
53.89
60.04
53.89
60.04
53.89
CaSSLe+BGE
62.38(+2.34) 58.14(+4.25) 61.72(+1.68) 56.92(+3.03) 61.51(+1.47) 56.36(+2.47)"
TASKS,0.26520438683948155,"PFR [18]
60.92
55.57
60.92
55.57
60.92
55.57
PFR+BGE
64.37(+3.45) 61.02(+5.45) 63.15(+2.23) 60.31(+4.74) 62.88(+1.96) 59.99(+4.42)"
TASKS,0.2662013958125623,Joint Acc
TASKS,0.26719840478564305,"Joint
68.09
68.09
68.09
Joint+ED
68.15(+0.06)
67.11(-0.98)
68.19(+0.10)"
TASKS,0.26819541375872386,"Table 2: Performance improvement yielded by BGE on ImageNet100. IN, INP, and IND are different
external dataset compositions. ED represents adding equivalent external data in joint training."
TASKS,0.2691924227318046,"Methods
IN
INP
IND"
TASKS,0.27018943170488535,"5tasks
10tasks
5tasks
10tasks
5tasks
10tasks"
TASKS,0.2711864406779661,"FT
64.02
56.72
64.02
56.72
64.02
56.72
FT+BGE
68.20(+4.18) 64.16(+7.44) 67.84(+3.82) 64.08(+7.36) 69.06(+5.04) 65.00(+8.28)"
TASKS,0.27218344965104685,"CaSSLe [16]
70.02
60.68
70.02
60.68
70.02
60.68
CaSSLe+BGE
72.46(+2.44) 66.80(+6.12) 71.44(+1.42) 65.94(+5.26) 72.68(+2.66) 67.10(+6.42)"
TASKS,0.2731804586241276,"PFR [18]
70.14
63.12
70.14
63.12
70.14
63.12
PFR+BGE
72.52(+2.38) 69.28(+6.16) 72.94(+2.80) 68.40(+5.28) 72.60(+2.46) 68.94(+5.82)"
TASKS,0.2741774675972084,Joint Acc
TASKS,0.27517447657028915,"Joint
80.44
80.44
80.44
Joint+ED
80.24(-0.20)
79.70(-0.74)
78.88(-1.56)"
TASKS,0.2761714855433699,"Section 4.3). On ImageNet100, the performance improvement is shown in Table 2, showcasing a
243"
TASKS,0.27716849451645065,"similar improvement regularity to that observed on CIFAR100. BGE achieves 1.5%-4% improvement
244"
TASKS,0.2781655034895314,"for 5 tasks and 5%-7.5% improvement for 10 tasks. More experiments see Appendix A.2.7.
245"
TASKS,0.27916251246261214,"We also emphasize that although it might seem intuitive that network performance would improve
246"
TASKS,0.28015952143569295,"with richer data because of richer features, BGE yielded improvement does not simply stem from
247"
TASKS,0.2811565304087737,"using more data. In Table 1 and Table 2, we incorporate an equal amount of external data into
248"
TASKS,0.28215353938185445,"joint training. However, the results do not improve, and may even decrease when the external data
249"
TASKS,0.2831505483549352,"contains OOD samples. We believe this is because incorporating irrelevant external data into the
250"
TASKS,0.28414755732801594,"training process causes the model to allocate some capacity to learning these unrelated data, thereby
251"
TASKS,0.2851445663010967,"weakening its focus on the in-task data. Hence, the learning of external data can not directly contribute
252"
TASKS,0.28614157527417744,"to the learning of in-task data.
253"
TASKS,0.28713858424725824,"Long task sequence experiments
We conduct experiments with 100 tasks on CIFAR100, which
254"
TASKS,0.288135593220339,"means one task only contains one class, to verify the effectiveness of BGE on long task sequences.
255"
TASKS,0.28913260219341974,"We set the sampling budget to 1000. Figure 2 shows the performance of different base methods
256"
TASKS,0.2901296111665005,"with or without BGE as the learned tasks increase. On one hand, BGE improves the final network
257"
TASKS,0.29112662013958124,"performance, especially evident in FT and PFR. On the other hand, the network’s performance
258"
TASKS,0.292123629112662,"increases even more rapidly with BGE, indicating that the network’s generalization ability to unseen
259"
TASKS,0.2931206380857428,"FT
CaSSLe
PFR"
TASKS,0.29411764705882354,"20
40
60
80
100
5 10 15 20 25 30 35 40 45 50 tasks"
TASKS,0.2951146560319043,"w/o BGE
  w/ BGE"
TASKS,0.29611166500498504,"20
40
60
80
100
5 10 15 20 25 30 35 40 45 50 tasks"
TASKS,0.2971086739780658,"w/o BGE
  w/ BGE"
TASKS,0.29810568295114653,"20
40
60
80
100
5 10 15 20 25 30 35 40 45 50 tasks"
TASKS,0.29910269192422734,"w/o BGE
  w/ BGE"
TASKS,0.3000997008973081,Accuracy(%)
TASKS,0.30109670987038883,Figure 2: Performance improvement of BGE at CIFAR100 100 tasks setting.
TASKS,0.3020937188434696,"Table 3: Accuracy on CIFAR100 and ImageNet100 with different sampling algorithms. Bold
indicates better performance."
TASKS,0.30309072781655033,"CIFAR100 FT
CIFAR100 PFR"
TASKS,0.3040877367896311,"External dataset
CP
CPI
CP
CPI"
TASKS,0.3050847457627119,"Sampling algorithm
4tasks
10tasks
4tasks
10tasks
4tasks
10tasks
4tasks
10tasks"
TASKS,0.30608175473579263,"random
57.41
52.78
57.22
52.56
62.57
59.33
62.58
58.45
OPO
58.69
55.14
58.71
55.74
63.15
60.31
62.88
59.99"
TASKS,0.3070787637088734,"ImageNet100 FT
ImageNet100 PFR"
TASKS,0.30807577268195413,"External dataset
INP
IND
INP
IND"
TASKS,0.3090727816550349,"Sampling algorithm
4tasks
10tasks
4tasks
10tasks
4tasks
10tasks
4tasks
10tasks"
TASKS,0.3100697906281156,"random
66.50
61.90
66.90
61.90
71.36
67.26
72.56
67.98
OPO
67.84
64.08
69.06
65.00
72.94
68.40
72.60
68.94"
TASKS,0.31106679960119643,"tasks is higher. This stems from BGE can both overcome catastrophic forgetting and compare with
260"
TASKS,0.3120638085742772,"future tasks it guessed, thus accumulating more knowledge in the early training stages.
261"
ABLATION STUDY,0.31306081754735793,"4.3
Ablation study
262"
ABLATION STUDY,0.3140578265204387,"CIFAR
CPI
0 5 10 15 20 25 30 35 40"
ABLATION STUDY,0.3150548354935194,FID Score ↓
ABLATION STUDY,0.3160518444666002,External datasets compositions
ABLATION STUDY,0.317048853439681,"Random       
 OPO"
ABLATION STUDY,0.3180458624127617,"Figure 3: FID score of different sam-
pling algorithms when CIFAR and
CPI as external data."
ABLATION STUDY,0.3190428713858425,"Sampling algorithm
Table 3 shows the effect of OPO sam-
263"
ABLATION STUDY,0.3200398803589232,"pling compared to random sampling for FT and PFR improve-
264"
ABLATION STUDY,0.32103688933200397,"ment when external datasets contain OOD data. OPO algo-
265"
ABLATION STUDY,0.3220338983050847,"rithm consistently provides more improvement than random
266"
ABLATION STUDY,0.3230309072781655,"sampling. However, we also observed that when all external
267"
ABLATION STUDY,0.3240279162512463,"data are in-distribution (ID), the improvement from OPO algo-
268"
ABLATION STUDY,0.325024925224327,"rithm is not stable. This suggests that external data quality is
269"
ABLATION STUDY,0.32602193419740777,"sufficiently high, making random sampling sufficient for our
270"
ABLATION STUDY,0.3270189431704885,"needs. To validate this, we calculated the Fréchet Inception
271"
ABLATION STUDY,0.32801595214356927,"Distance (FID) scores [25] between the in-task dataset and
272"
ABLATION STUDY,0.32901296111665007,"external datasets obtained by different sampling algorithms
273"
ABLATION STUDY,0.3300099700897308,"under CIFAR and CPI compositions, as shown in Figure 3.
274"
ABLATION STUDY,0.33100697906281157,"A lower FID score indicates greater similarity between two
275"
ABLATION STUDY,0.3320039880358923,"datasets, and vice versa. Figure 3 shows that with the CIFAR
276"
ABLATION STUDY,0.33300099700897307,"composition, the FID score is lower, and the effect of the OPO
277"
ABLATION STUDY,0.3339980059820538,"algorithm is little, indicating that this dataset is already of
278"
ABLATION STUDY,0.3349950149551346,"high quality. In contrast, under CPI, the FID score is higher when random sampling, while shows a
279"
ABLATION STUDY,0.33599202392821537,"significant decrease when OPO sampling. It indicates that the OPO algorithm adjusts the distribution
280"
ABLATION STUDY,0.3369890329012961,"of the external dataset considerably to make it more compatible with the in-task dataset. Therefore
281"
ABLATION STUDY,0.33798604187437686,"OPO algorithm will have more advantages when the external dataset contains OOD data.
282"
ABLATION STUDY,0.3389830508474576,"Besides, we observed that the advantage of OPO sampling algorithm is more significant on the
283"
ABLATION STUDY,0.33998005982053836,"ImageNet100 dataset. We believe this can be attributed to two factors: 1) Higher image pixels contain
284"
ABLATION STUDY,0.34097706879361916,"more information, and fewer images will satisfy the proximity. 2) With a larger quantity of external
285"
ABLATION STUDY,0.3419740777666999,"data, there are more potentially high-quality data, facilitating better sampling.
286"
ABLATION STUDY,0.34297108673978066,"Table 4: Comparison of additional
positive and negative pairs’ effects."
ABLATION STUDY,0.3439680957128614,"Negative
Positive
Acc"
ABLATION STUDY,0.34496510468594216,"52.79
✓
53.40
✓
55.61
✓
✓
56.21"
ABLATION STUDY,0.3459621136590229,"Effect of additional positive and negative pairs
We fur-
287"
ABLATION STUDY,0.3469591226321037,"ther investigate whether additional positive or negative pairs
288"
ABLATION STUDY,0.34795613160518446,"provided by BGE contribute more to performance improve-
289"
ABLATION STUDY,0.3489531405782652,"ment. We conduct experiments based on CaSSLe [16] on the
290"
ABLATION STUDY,0.34995014955134596,"CIFAR100 4 tasks setting. Because this experiment requires
291"
ABLATION STUDY,0.3509471585244267,"explicitly calculating the loss incurred by each positive and
292"
ABLATION STUDY,0.35194416749750745,"negative pair, we convert the framework to SimCLR [8]. We
293"
ABLATION STUDY,0.35294117647058826,"masked the additional positive or negative pairs in Table 4.
294"
ABLATION STUDY,0.353938185443669,"The results show that both types of pairs improve performance
295"
ABLATION STUDY,0.35493519441674976,"individually, and negative pairs yield more significant improve-
296"
ABLATION STUDY,0.3559322033898305,"ment, supporting our emphasis that the impact of absent inter-task comparisons is severe but neglected.
297"
ABLATION STUDY,0.35692921236291125,"But positive pairs also yield performance improvement, which is because high-quality external data
298"
ABLATION STUDY,0.357926221335992,"have feature intersections with in-task data, proving that external data can prevent catastrophic
299"
ABLATION STUDY,0.3589232303090728,"forgetting as well. With the synergistic effect of both, the improvement reaches the highest.
300"
ABLATION STUDY,0.35992023928215355,"Experiments with only OOD external data
In the experiments presented in Table 1 and Table 2,
301"
ABLATION STUDY,0.3609172482552343,"all external data contain some amount of ID data. To assess BGE’s performance without any ID data
302"
ABLATION STUDY,0.36191425722831505,"in the external dataset, we conduct experiments on CIFAR100 4 tasks based on PFR, as shown in
303"
ABLATION STUDY,0.3629112662013958,"Table 5. The external dataset is only composed of ImageNet-R or Places365test. In joint training,
304"
ABLATION STUDY,0.36390827517447655,"these data are detrimental. While in continual training, BGE consistently improves the base method
305"
ABLATION STUDY,0.36490528414755735,"by nearly 2%, regardless of the composition of OOD data used. It indicates that the performance
306"
ABLATION STUDY,0.3659022931206381,"improvement from BGE does not only come from imitating in-task data features, but also from
307"
ABLATION STUDY,0.36689930209371885,"introducing similar additional comparisons into each task itself, which is beneficial for constructing
308"
ABLATION STUDY,0.3678963110667996,"implicit inter-task comparisons. Even if the external data has few recognizable similar features to
309"
ABLATION STUDY,0.36889332003988035,"the in-task data, the network can still try its best to mine valuable knowledge from external data to
310"
ABLATION STUDY,0.3698903290129611,"compensate for inter-task comparisons.
311"
ABLATION STUDY,0.3708873379860419,Table 5: Effectiveness of BGE when external data are totally OOD.
ABLATION STUDY,0.37188434695912265,"External dataset compositions
PFR
+BGE
Joint
Joint+ED
ImageNet-R
Places365test
✓
60.92
62.85(+1.93)
68.09
68.03(-0.06)
✓
60.92
62.81(+1.89)
68.09
67.75(-0.34)
✓
✓
60.92
62.88(+1.96)
68.09
67.15(-0.94)"
ABLATION STUDY,0.3728813559322034,"Table 6: Performance of BGE
when choosing more types of
datasets."
ABLATION STUDY,0.37387836490528414,"External datasets
Acc"
ABLATION STUDY,0.3748753738783649,"N/A
60.92
GenImage [58]
64.37
CC3M [42]
63.53
CUB200 [48]
62.42"
ABLATION STUDY,0.37587238285144564,"BGE with more types of datasets
We validate the effective-
312"
ABLATION STUDY,0.37686939182452645,"ness of BGE across more aspects of external datasets. Table 6
313"
ABLATION STUDY,0.3778664007976072,"presents the results when using GenImage [58], a dataset of gen-
314"
ABLATION STUDY,0.37886340977068794,"erated images; CC3M [42], a dataset sourced from the Internet;
315"
ABLATION STUDY,0.3798604187437687,"and CUB200 [48], a fine-grained bird dataset as external dataset.
316"
ABLATION STUDY,0.38085742771684944,"Experiments with GenImage and CC3M demonstrate BGE’s effec-
317"
ABLATION STUDY,0.3818544366899302,"tiveness with both model-generated and real-world Internet data,
318"
ABLATION STUDY,0.382851445663011,"demonstrating its practical value. Since CUB200 is fine-grained
319"
ABLATION STUDY,0.38384845463609174,"and lacking in diversity, it is extremely unfriendly to BGE, yet
320"
ABLATION STUDY,0.3848454636091725,"BGE can still improve the base method.
321"
CONCLUSION,0.38584247258225324,"5
Conclusion
322"
CONCLUSION,0.386839481555334,"In this paper, we address a commonly overlooked but severe issue in Continual Contrastive Self-
323"
CONCLUSION,0.38783649052841473,"Supervised Learning (CCSSL): the lack of inter-task comparisons. To tackle this, we propose our
324"
CONCLUSION,0.38883349950149554,"method BGE to incorporate external data into training, bridging the inter-task gap and facilitating
325"
CONCLUSION,0.3898305084745763,"implicit inter-task data comparisons. We also design the One-Propose-One sampling algorithm to
326"
CONCLUSION,0.39082751744765704,"select high-quality external data and filter out irrelevant OOD data. BGE can be seamlessly integrated
327"
CONCLUSION,0.3918245264207378,"into existing methods and yield significant improvement.
328"
REFERENCES,0.39282153539381853,"References
329"
REFERENCES,0.3938185443668993,"[1] R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and T. Tuytelaars. Memory aware synapses:
330"
REFERENCES,0.3948155533399801,"Learning what (not) to forget. In Proceedings of the European conference on computer vision
331"
REFERENCES,0.39581256231306083,"(ECCV), pages 139–154, 2018.
332"
REFERENCES,0.3968095712861416,"[2] S. Amir, Y. Gandelsman, S. Bagon, and T. Dekel. Deep vit features as dense visual descriptors.
333"
REFERENCES,0.39780658025922233,"arXiv preprint arXiv:2112.05814, 2(3):4, 2021.
334"
REFERENCES,0.3988035892323031,"[3] J. Bai, Z. Liu, H. Wang, J. Hao, Y. Feng, H. Chu, and H. Hu. On the effectiveness of out-
335"
REFERENCES,0.39980059820538383,"of-distribution data in self-supervised long-tail learning. arXiv preprint arXiv:2306.04934,
336"
REFERENCES,0.40079760717846463,"2023.
337"
REFERENCES,0.4017946161515454,"[4] J. Bang, H. Kim, Y. Yoo, J.-W. Ha, and J. Choi. Rainbow memory: Continual learning with a
338"
REFERENCES,0.40279162512462613,"memory of diverse samples. In Proceedings of the IEEE/CVF conference on computer vision
339"
REFERENCES,0.4037886340977069,"and pattern recognition, pages 8218–8227, 2021.
340"
REFERENCES,0.4047856430707876,"[5] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning of
341"
REFERENCES,0.4057826520438684,"visual features by contrasting cluster assignments. Advances in neural information processing
342"
REFERENCES,0.4067796610169492,"systems, 33:9912–9924, 2020.
343"
REFERENCES,0.4077766699900299,"[6] H. Cha, J. Lee, and J. Shin. Co2l: Contrastive continual learning. In Proceedings of the
344"
REFERENCES,0.4087736789631107,"IEEE/CVF International conference on computer vision, pages 9516–9525, 2021.
345"
REFERENCES,0.4097706879361914,"[7] S. Cha and T. Moon. Sy-con: Symmetric contrastive loss for continual self-supervised represen-
346"
REFERENCES,0.4107676969092722,"tation learning. arXiv preprint arXiv:2306.05101, 2023.
347"
REFERENCES,0.4117647058823529,"[8] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning
348"
REFERENCES,0.4127617148554337,"of visual representations. In International conference on machine learning, pages 1597–1607.
349"
REFERENCES,0.4137587238285145,"PMLR, 2020.
350"
REFERENCES,0.4147557328015952,"[9] X. Chen and K. He. Exploring simple siamese representation learning. In Proceedings of the
351"
REFERENCES,0.41575274177467597,"IEEE/CVF conference on computer vision and pattern recognition, pages 15750–15758, 2021.
352"
REFERENCES,0.4167497507477567,"[10] X. Chen, H. Fan, R. Girshick, and K. He. Improved baselines with momentum contrastive
353"
REFERENCES,0.41774675972083747,"learning. arXiv preprint arXiv:2003.04297, 2020.
354"
REFERENCES,0.4187437686939183,"[11] X. Chen, Z. Sun, K. Yan, S. Ding, and H. Lu. Combining past, present and future: A self-
355"
REFERENCES,0.419740777666999,"supervised approach for class incremental learning. arXiv preprint arXiv:2311.08764, 2023.
356"
REFERENCES,0.42073778664007977,"[12] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical
357"
REFERENCES,0.4217347956131605,"image database. In 2009 IEEE conference on computer vision and pattern recognition, pages
358"
REFERENCES,0.42273180458624127,"248–255. Ieee, 2009.
359"
REFERENCES,0.423728813559322,"[13] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,
360"
REFERENCES,0.4247258225324028,"M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for
361"
REFERENCES,0.42572283150548357,"image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
362"
REFERENCES,0.4267198404785643,"[14] A. Douillard, M. Cord, C. Ollion, T. Robert, and E. Valle. Podnet: Pooled outputs distillation for
363"
REFERENCES,0.42771684945164506,"small-tasks incremental learning. In Computer vision–ECCV 2020: 16th European conference,
364"
REFERENCES,0.4287138584247258,"Glasgow, UK, August 23–28, 2020, proceedings, part XX 16, pages 86–102. Springer, 2020.
365"
REFERENCES,0.42971086739780656,"[15] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A. Rusu, A. Pritzel, and D. Wier-
366"
REFERENCES,0.4307078763708873,"stra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint
367"
REFERENCES,0.4317048853439681,"arXiv:1701.08734, 2017.
368"
REFERENCES,0.43270189431704886,"[16] E. Fini, V. G. T. Da Costa, X. Alameda-Pineda, E. Ricci, K. Alahari, and J. Mairal. Self-
369"
REFERENCES,0.4336989032901296,"supervised models are continual learners. In Proceedings of the IEEE/CVF Conference on
370"
REFERENCES,0.43469591226321036,"Computer Vision and Pattern Recognition, pages 9621–9630, 2022.
371"
REFERENCES,0.4356929212362911,"[17] S. Garg and Y. Liang. Functional regularization for representation learning: A unified theoretical
372"
REFERENCES,0.43668993020937186,"perspective. Advances in Neural Information Processing Systems, 33:17187–17199, 2020.
373"
REFERENCES,0.43768693918245266,"[18] A. Gomez-Villa, B. Twardowski, L. Yu, A. D. Bagdanov, and J. Van de Weijer. Continually
374"
REFERENCES,0.4386839481555334,"learning self-supervised representations with projected functional regularization. In Proceedings
375"
REFERENCES,0.43968095712861416,"of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3867–3877,
376"
REFERENCES,0.4406779661016949,"2022.
377"
REFERENCES,0.44167497507477566,"[19] A. Gomez-Villa, B. Twardowski, K. Wang, and J. van de Weijer. Plasticity-optimized comple-
378"
REFERENCES,0.4426719840478564,"mentary networks for unsupervised continual learning. In Proceedings of the IEEE/CVF Winter
379"
REFERENCES,0.4436689930209372,"Conference on Applications of Computer Vision, pages 1690–1700, 2024.
380"
REFERENCES,0.44466600199401796,"[20] D. Goswami, Y. Liu, B. Twardowski, and J. van de Weijer. Fecam: Exploiting the heterogeneity
381"
REFERENCES,0.4456630109670987,"of class distributions in exemplar-free continual learning. Advances in Neural Information
382"
REFERENCES,0.44666001994017945,"Processing Systems, 36, 2024.
383"
REFERENCES,0.4476570289132602,"[21] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch,
384"
REFERENCES,0.44865403788634095,"B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent-a new ap-
385"
REFERENCES,0.44965104685942175,"proach to self-supervised learning. Advances in neural information processing systems, 33:
386"
REFERENCES,0.4506480558325025,"21271–21284, 2020.
387"
REFERENCES,0.45164506480558325,"[22] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In
388"
REFERENCES,0.452642073778664,"Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–
389"
REFERENCES,0.45363908275174475,"778, 2016.
390"
REFERENCES,0.4546360917248255,"[23] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual
391"
REFERENCES,0.4556331006979063,"representation learning. In Proceedings of the IEEE/CVF conference on computer vision and
392"
REFERENCES,0.45663010967098705,"pattern recognition, pages 9729–9738, 2020.
393"
REFERENCES,0.4576271186440678,"[24] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Para-
394"
REFERENCES,0.45862412761714855,"juli, M. Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution
395"
REFERENCES,0.4596211365902293,"generalization. In Proceedings of the IEEE/CVF international conference on computer vision,
396"
REFERENCES,0.46061814556331004,"pages 8340–8349, 2021.
397"
REFERENCES,0.46161515453639085,"[25] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two
398"
REFERENCES,0.4626121635094716,"time-scale update rule converge to a local nash equilibrium. Advances in neural information
399"
REFERENCES,0.46360917248255235,"processing systems, 30, 2017.
400"
REFERENCES,0.4646061814556331,"[26] S. Hou, X. Pan, C. C. Loy, Z. Wang, and D. Lin. Learning a unified classifier incrementally
401"
REFERENCES,0.46560319042871384,"via rebalancing. In Proceedings of the IEEE/CVF conference on computer vision and pattern
402"
REFERENCES,0.4666001994017946,"recognition, pages 831–839, 2019.
403"
REFERENCES,0.4675972083748754,"[27] W. Huang, M. Yi, X. Zhao, and Z. Jiang. Towards the generalization of contrastive self-
404"
REFERENCES,0.46859421734795614,"supervised learning. arXiv preprint arXiv:2111.00743, 2021.
405"
REFERENCES,0.4695912263210369,"[28] Z. Jiang, T. Chen, T. Chen, and Z. Wang. Improving contrastive learning on imbalanced data
406"
REFERENCES,0.47058823529411764,"via open-world sampling. Advances in Neural Information Processing Systems, 34:5997–6009,
407"
REFERENCES,0.4715852442671984,"2021.
408"
REFERENCES,0.47258225324027914,"[29] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan,
409"
REFERENCES,0.47357926221335994,"J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in
410"
REFERENCES,0.4745762711864407,"neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017.
411"
REFERENCES,0.47557328015952144,"[30] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.
412"
REFERENCES,0.4765702891326022,"[31] K. Lee, K. Lee, J. Shin, and H. Lee. Overcoming catastrophic forgetting with unlabeled data in
413"
REFERENCES,0.47756729810568294,"the wild. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
414"
REFERENCES,0.4785643070787637,"312–321, 2019.
415"
REFERENCES,0.4795613160518445,"[32] Z. Li and D. Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and
416"
REFERENCES,0.48055832502492524,"machine intelligence, 40(12):2935–2947, 2017.
417"
REFERENCES,0.481555333998006,"[33] H. Lin, B. Zhang, S. Feng, X. Li, and Y. Ye. Pcr: Proxy-based contrastive replay for online
418"
REFERENCES,0.48255234297108673,"class-incremental continual learning. In Proceedings of the IEEE/CVF Conference on Computer
419"
REFERENCES,0.4835493519441675,"Vision and Pattern Recognition, pages 24246–24255, 2023.
420"
REFERENCES,0.48454636091724823,"[34] X. Liu, M. Masana, L. Herranz, J. Van de Weijer, A. M. Lopez, and A. D. Bagdanov. Rotate
421"
REFERENCES,0.48554336989032904,"your networks: Better weight consolidation and less catastrophic forgetting. In 2018 24th
422"
REFERENCES,0.4865403788634098,"International Conference on Pattern Recognition (ICPR), pages 2262–2268. IEEE, 2018.
423"
REFERENCES,0.48753738783649053,"[35] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s. In
424"
REFERENCES,0.4885343968095713,"Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages
425"
REFERENCES,0.48953140578265203,"11976–11986, 2022.
426"
REFERENCES,0.4905284147557328,"[36] A. Mallya and S. Lazebnik. Packnet: Adding multiple tasks to a single network by iterative
427"
REFERENCES,0.4915254237288136,"pruning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,
428"
REFERENCES,0.49252243270189433,"pages 7765–7773, 2018.
429"
REFERENCES,0.4935194416749751,"[37] A. Mallya, D. Davis, and S. Lazebnik. Piggyback: Adapting a single network to multiple tasks
430"
REFERENCES,0.4945164506480558,"by learning to mask weights. In Proceedings of the European conference on computer vision
431"
REFERENCES,0.4955134596211366,"(ECCV), pages 67–82, 2018.
432"
REFERENCES,0.4965104685942173,"[38] M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The
433"
REFERENCES,0.49750747756729813,"sequential learning problem. In Psychology of learning and motivation, volume 24, pages
434"
REFERENCES,0.4985044865403789,"109–165. Elsevier, 1989.
435"
REFERENCES,0.4995014955134596,"[39] X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang. Moment matching for multi-source
436"
REFERENCES,0.5004985044865404,"domain adaptation. In Proceedings of the IEEE/CVF international conference on computer
437"
REFERENCES,0.5014955134596212,"vision, pages 1406–1415, 2019.
438"
REFERENCES,0.5024925224327019,"[40] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert. icarl: Incremental classifier and
439"
REFERENCES,0.5034895314057827,"representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern
440"
REFERENCES,0.5044865403788634,"Recognition, pages 2001–2010, 2017.
441"
REFERENCES,0.5054835493519442,"[41] J. Serra, D. Suris, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with
442"
REFERENCES,0.5064805583250249,"hard attention to the task. In International conference on machine learning, pages 4548–4557.
443"
REFERENCES,0.5074775672981057,"PMLR, 2018.
444"
REFERENCES,0.5084745762711864,"[42] P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed,
445"
REFERENCES,0.5094715852442672,"image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual
446"
REFERENCES,0.5104685942173479,"Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
447"
REFERENCES,0.5114656031904287,"2556–2565, 2018.
448"
REFERENCES,0.5124626121635095,"[43] H. Shin, J. K. Lee, J. Kim, and J. Kim. Continual learning with deep generative replay. Advances
449"
REFERENCES,0.5134596211365903,"in neural information processing systems, 30, 2017.
450"
REFERENCES,0.514456630109671,"[44] Z. Song, Y. Zhao, Y. Shi, P. Peng, L. Yuan, and Y. Tian. Learning with fantasy: Semantic-aware
451"
REFERENCES,0.5154536390827518,"virtual contrastive constraint for few-shot class-incremental learning. In Proceedings of the
452"
REFERENCES,0.5164506480558325,"IEEE/CVF conference on computer vision and pattern recognition, pages 24183–24192, 2023.
453"
REFERENCES,0.5174476570289133,"[45] Y.-M. Tang, Y.-X. Peng, and W.-S. Zheng.
Learning to imagine: Diversify memory for
454"
REFERENCES,0.518444666001994,"incremental learning using unlabeled data. In Proceedings of the IEEE/CVF Conference on
455"
REFERENCES,0.5194416749750748,"Computer Vision and Pattern Recognition, pages 9549–9558, 2022.
456"
REFERENCES,0.5204386839481555,"[46] Y. Tian, D. Krishnan, and P. Isola. Contrastive multiview coding. In Computer Vision–ECCV
457"
REFERENCES,0.5214356929212363,"2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16,
458"
REFERENCES,0.522432701894317,"pages 776–794. Springer, 2020.
459"
REFERENCES,0.5234297108673978,"[47] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine learning
460"
REFERENCES,0.5244267198404786,"research, 9(11), 2008.
461"
REFERENCES,0.5254237288135594,"[48] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011
462"
REFERENCES,0.5264207377866401,"dataset. 2011.
463"
REFERENCES,0.5274177467597209,"[49] T. Wang and P. Isola. Understanding contrastive representation learning through alignment
464"
REFERENCES,0.5284147557328016,"and uniformity on the hypersphere. In International conference on machine learning, pages
465"
REFERENCES,0.5294117647058824,"9929–9939. PMLR, 2020.
466"
REFERENCES,0.5304087736789631,"[50] Y. Wu, Y. Chen, L. Wang, Y. Ye, Z. Liu, Y. Guo, and Y. Fu. Large scale incremental learning.
467"
REFERENCES,0.5314057826520439,"In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages
468"
REFERENCES,0.5324027916251246,"374–382, 2019.
469"
REFERENCES,0.5333998005982054,"[51] S. Yan, J. Xie, and X. He. Der: Dynamically expandable representation for class incremen-
470"
REFERENCES,0.5343968095712861,"tal learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern
471"
REFERENCES,0.5353938185443669,"recognition, pages 3014–3023, 2021.
472"
REFERENCES,0.5363908275174477,"[52] L. Yu, X. Liu, and J. Van de Weijer. Self-training for class-incremental semantic segmentation.
473"
REFERENCES,0.5373878364905285,"IEEE Transactions on Neural Networks and Learning Systems, 2022.
474"
REFERENCES,0.5383848454636092,"[53] J. Zbontar, L. Jing, I. Misra, Y. LeCun, and S. Deny. Barlow twins: Self-supervised learning via
475"
REFERENCES,0.53938185443669,"redundancy reduction. In International conference on machine learning, pages 12310–12320.
476"
REFERENCES,0.5403788634097707,"PMLR, 2021.
477"
REFERENCES,0.5413758723828515,"[54] F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence. In
478"
REFERENCES,0.5423728813559322,"International conference on machine learning, pages 3987–3995. PMLR, 2017.
479"
REFERENCES,0.543369890329013,"[55] M. Zhai, L. Chen, and G. Mori. Hyper-lifelonggan: Scalable lifelong learning for image
480"
REFERENCES,0.5443668993020937,"conditioned generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and
481"
REFERENCES,0.5453639082751744,"Pattern Recognition, pages 2246–2255, 2021.
482"
REFERENCES,0.5463609172482552,"[56] Z. Zheng, M. Ma, K. Wang, Z. Qin, X. Yue, and Y. You.
Preventing zero-shot transfer
483"
REFERENCES,0.5473579262213359,"degradation in continual learning of vision-language models. In Proceedings of the IEEE/CVF
484"
REFERENCES,0.5483549351944168,"International Conference on Computer Vision, pages 19125–19136, 2023.
485"
REFERENCES,0.5493519441674976,"[57] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba. Places: A 10 million image
486"
REFERENCES,0.5503489531405783,"database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence,
487"
REFERENCES,0.551345962113659,"2017.
488"
REFERENCES,0.5523429710867398,"[58] M. Zhu, H. Chen, Q. Yan, X. Huang, G. Lin, W. Li, Z. Tu, H. Hu, J. Hu, and Y. Wang. Genimage:
489"
REFERENCES,0.5533399800598205,"A million-scale benchmark for detecting ai-generated image. Advances in Neural Information
490"
REFERENCES,0.5543369890329013,"Processing Systems, 36, 2024.
491"
REFERENCES,0.555333998005982,"A
Appendix / supplemental material
492"
REFERENCES,0.5563310069790628,"A.1
Experimental details
493"
REFERENCES,0.5573280159521435,"We use SGD optimizer with warmup cosine scheduler to train the network with batchsize of 256. For
494"
REFERENCES,0.5583250249252243,"CIFAR100, we train 500 epochs per task with a learning rate of 0.3 and weight decay of 1e-4 for
495"
REFERENCES,0.559322033898305,"FT and CaSSLe[16]. For PFR[18], we use the learning rate as 0.4. For ImageNet100, we train 400
496"
REFERENCES,0.5603190428713859,"epochs per task with a learning rate of 0.4 and weight decay of 1e-4.
497"
REFERENCES,0.5613160518444666,"We use one RTX 3090 for CIFAR100 experiments and one A40 for ImageNet100 experiments. For
498"
REFERENCES,0.5623130608175474,"CIFAR100 experiments, it takes about 5 hours in 4 tasks setting and 8 hours in 10 tasks setting. For
499"
REFERENCES,0.5633100697906281,"ImageNet100 experiments, it takes about 17 hours in 5 tasks setting and 27 hours in 10 tasks setting.
500"
REFERENCES,0.5643070787637089,"A.2
More experiments
501"
REFERENCES,0.5653040877367896,"A.2.1
BGE’s improvement to inter-task confusion
502"
REFERENCES,0.5663010967098704,"We categorize the results of classification errors into two types, inter-task confusion (the wrong
503"
REFERENCES,0.5672981056829511,"prediction belongs to a different task than the target) and intra-task confusion (the wrong prediction
504"
REFERENCES,0.5682951146560319,"belongs to the same task as the target). Under the CIFAR100 4 tasks setting, we compare the
505"
REFERENCES,0.5692921236291126,"probability of each of the two types of confusion occurring for the class contained in the last task for
506"
REFERENCES,0.5702891326021934,"the three baseline methods, as shown in Table 7. Ideally, the ratio of intra-task confusion to inter-task
507"
REFERENCES,0.5712861415752741,"confusion should be 1:3, since the ratio of the number of current task classes to the total number
508"
REFERENCES,0.5722831505483549,"of previous task classes is 1:3. However, the inter-task confusion in Table 7 is 5 to 7 times higher
509"
REFERENCES,0.5732801595214357,"than the intra-task confusion, suggesting that the lack of Linter optimization has a severe impact on
510"
REFERENCES,0.5742771684945165,"performance, while BGE improves this and decreases inter-task confusion."
REFERENCES,0.5752741774675972,"Table 7: Comparison of intra-task confusion and inter-task confusion. ↓means the value is the lower
the better."
REFERENCES,0.576271186440678,"Method
Intra-task confusion↓
Inter-task confusion↓"
REFERENCES,0.5772681954137587,"FT
4.56%
33.48%
FT+BGE
4.60%(+0.04%)
30.12%(-3.36%)
CaSSLe
6.84%
32.08%
CaSSLe+BGE
6.08%(-0.76%)
28.52%(-3.56%)
PFR
6.32%
29.64%
PFR+BGE
6.44%(+0.12%)
27.36%(-2.28%) 511"
REFERENCES,0.5782652043868395,"A.2.2
Experiments on the method without negative samples
512"
REFERENCES,0.5792622133599202,"While the results in Table 4 indicate that the effectiveness of BGE mainly stems from additional
513"
REFERENCES,0.580259222333001,"negative samples, we conducted experiments using the contrastive learning framework BYOL, which
514"
REFERENCES,0.5812562313060817,"calculates contrastive loss without the need of negative samples, as shown in Table 8. The results
515"
REFERENCES,0.5822532402791625,"indicate that our method still achieves improvement, demonstrating its applicability even in methods
516"
REFERENCES,0.5832502492522432,without negative samples.
REFERENCES,0.584247258225324,Table 8: Performance improvement yielded by BGE in BYOL.
REFERENCES,0.5852442671984048,"Methods
CIFAR
CP"
TASKS,0.5862412761714856,"4tasks
10tasks
4tasks
10tasks"
TASKS,0.5872382851445663,"FT
52.36
47.97
52.36
47.97
FT+BGE
56.88(+4.52)
49.42(+1.45)
56.37(+4.01)
49.22(+1.25)"
TASKS,0.5882352941176471,"CaSSLe
57.46
52.61
57.46
52.61
CaSSLe+BGE
59.20(+1.78)
56.16(+3.55)
58.92(+1.46)
55.22(+2.61) 517"
TASKS,0.5892323030907278,"A.2.3
Visualization of sample algorithm
518"
TASKS,0.5902293120638086,"We visualize the relationship between external and in-task samples obtained by different sampling
519"
TASKS,0.5912263210368893,"algorithms under CIFAR and CPI compositions, as shown in Figure 4. When CIFAR10 as external
520"
TASKS,0.5922233300099701,"data, the distributions of random and OPO samples are similar, both covering the entire area effectively.
521"
TASKS,0.5932203389830508,"While in the CPI setting, random sampling fails to cover the entire area, in contrast, the OPO algorithm
522"
TASKS,0.5942173479561316,"achieves superior proximity and diversity, consequently leading to greater performance improvement.
523"
TASKS,0.5952143569292123,This observation corroborates our discussion about the sampling algorithm in Section 4.3.
TASKS,0.5962113659022931,"CPI
CIFAR10"
TASKS,0.5972083748753739,"Random
OPO
Random
OPO
In-task data
Sampled external data"
TASKS,0.5982053838484547,"Figure 4: Comparison of external data sampled by different algorithms. When the entire external data
quality is high (CIFAR), there is little difference between random and OPO sampling. When the data
contains many OOD data (CPI), OPO outperforms random in sampling relevant and diverse samples. 524"
TASKS,0.5992023928215354,"A.2.4
Self-supervised learning feature characteristics
525"
TASKS,0.6001994017946162,"Previous work [2] points out that self-supervised trained networks map inputs together according
526"
TASKS,0.6011964107676969,"to feature characteristics rather than according to labels as supervised trained networks tend to do.
527"
TASKS,0.6021934197407777,"Inspired by them, we validate that we adopted network also has such characteristics. Table 9 shows
528"
TASKS,0.6031904287138584,"the average number of one sample’s k-nearest neighbors belonging to the class of this sample for
529"
TASKS,0.6041874376869392,"networks trained in the supervised or self-supervised manner. It is evident that supervised networks
530"
TASKS,0.6051844466600199,"consistently have more same-class neighbors, indicating that they cluster images based on labels. In
531"
TASKS,0.6061814556331007,"contrast, self-supervised networks are less influenced by image classes, which is advantageous for
532"
TASKS,0.6071784646061814,incorporating external data.
TASKS,0.6081754735792622,"Table 9: Statistics on how many of the k-nearest neighbors of a sample belong to the same class as
this sample in self-supervised and supervised networks."
TASKS,0.609172482552343,"k
3
5
10
20
30
50
100
Acc"
TASKS,0.6101694915254238,"Supervised
1.76
2.93
5.58
10.87
15.63
24.38
40.86
71.64
Self-supervised
1.36
2.25
4.14
7.24
9.96
14.53
22.00
68.09 533"
TASKS,0.6111665004985045,"Table 10 presents the class statistics of the top 100 nearest neighbors of the ""willow tree"" class on the
534"
TASKS,0.6121635094715853,"CIFAR100 dataset, as learned by self-supervised and supervised networks. Self-supervised learning
535"
TASKS,0.613160518444666,"results in a lower proportion of same-class neighbors, indicating less influence from class labels.
536"
TASKS,0.6141575274177468,"Additionally, the neighbors of other classes in the self-supervised network exhibit features more
537"
TASKS,0.6151545363908275,"similar to the ""willow tree"" class.
538"
TASKS,0.6161515453639083,"This insight suggests that external data, despite having different actual classes with in-task data,
539"
TASKS,0.617148554336989,"can proxy for the in-task data in self-supervised learning due to shared features. Thus giving us
540"
TASKS,0.6181455633100698,"confidence that using external data in self-supervised learning as in BGE can yield good results and
541"
TASKS,0.6191425722831505,"justify our cosine distance based sampling algorithm.
542"
TASKS,0.6201395812562313,"A.2.5
Fairness alignment
543"
TASKS,0.6211365902293121,"Introducing external data incurs additional iterations and new knowledge. To ensure fairness, we
544"
TASKS,0.6221335992023929,"train the base method PFR for more epochs and use pre-training with external data to initialize the
545"
TASKS,0.6231306081754736,"weights for in-task data training. Experimental results, as shown in Table 11, reveal that training
546"
TASKS,0.6241276171485544,"Table 10: The class name and average number of the top 5 classes with the highest number of the top
100 neighbors of the ""willow tree"" class."
TASKS,0.6251246261216351,"Supervised learning
Self-supervised learning"
TASKS,0.6261216350947159,"Neighbor class
Avg number
Neighbor class
Avg number"
TASKS,0.6271186440677966,"willow tree
48.59
willow tree
18.68
mushroom
7.85
oak tree
18.47
girl
4.19
maple tree
16.45
butterfly
3.05
pine tree
8.48
bus
2.94
forest
8.10"
TASKS,0.6281156530408774,"for more epochs and pre-training with external data do not lead to performance improvement. This
547"
TASKS,0.6291126620139581,highlights the effectiveness of BGE under fairer conditions.
TASKS,0.6301096709870389,Table 11: Comparison of the performance improvement of BGE and other factors to ensure fairness.
TASKS,0.6311066799601196,"Methods
Acc"
TASKS,0.6321036889332003,"Base
60.92
Train more epochs
61.21
Use external data to pre-train
61.28
Ours
64.37 548"
TASKS,0.6331006979062812,"A.2.6
Experiment statistical significance
549"
TASKS,0.634097706879362,"Due to limited computational resources, we report the mean and standard deviation of three random
550"
TASKS,0.6350947158524427,"trials for only the primary experiments in Tables 12 and 13. The performance of the BGE on the three
551"
TASKS,0.6360917248255235,"base methods when using CIFAR and CPI as external dataset compositions under the CIFAR100
552"
TASKS,0.6370887337986042,"4 tasks and 10 tasks setting is shown in Table 12. Table 13 shows the performance of BGE using
553"
TASKS,0.638085742771685,"different sampling algorithms with CPI as the external dataset, also in the CIFAR100 4 tasks and 10
554"
TASKS,0.6390827517447657,"tasks setting, across the same three baseline methods."
TASKS,0.6400797607178464,Table 12: Results with multiple runs.
TASKS,0.6410767696909272,"Methods
CIFAR
CPI"
TASKS,0.6420737786640079,"4tasks
10tasks
4tasks
10tasks"
TASKS,0.6430707876370887,"FT
59.80±0.27
56.92±0.29
59.06±0.39
55.18±0.51
CaSSLe
62.39±0.41
57.99±0.28
61.86±0.36
56.52±0.21
PFR
64.13±0.24
60.01±0.02
63.12±0.33
59.94±0.05"
TASKS,0.6440677966101694,Table 13: Results with multiple runs.
TASKS,0.6450648055832503,"Methods
4tasks
10tasks"
TASKS,0.646061814556331,"random
OPO
random
OPO"
TASKS,0.6470588235294118,"FT
57.61±0.42
59.06±0.39
52.81±0.23
55.18±0.51
CaSSLe
61.59±0.25
61.86±0.36
55.50±0.23
56.52±0.21
PFR
62.50±0.11
63.12±0.33
58.66±0.27
59.94±0.05 555"
TASKS,0.6480558325024925,"A.2.7
Full experiments
556"
TASKS,0.6490528414755733,"We present here the full set of experiments, encompassing various base methods, sampling bud-
557"
TASKS,0.650049850448654,"gets, sampling methods, and compositions of external datasets, demonstrating the performance
558"
TASKS,0.6510468594217348,"improvement of BGE on CIFAR100 (Table 14) and ImageNet100 (Table 15).
559"
TASKS,0.6520438683948155,Table 14: Full experiment results on CIFAR100 dataset.
TASKS,0.6530408773678963,"Methods
External Dataset
CIFAR10
CP
CPI"
TASKS,0.654037886340977,"Budget
Sample
method
4tasks
10tasks
4tasks
10tasks
4tasks
10tasks FT"
-,0.6550348953140578,"0
-
56.19
49.36
56.19
49.36
56.19
49.36"
K,0.6560319042871385,"5K
random 58.65(+2.46) 54.78(+5.42) 57.54(+1.35) 52.09(+2.73) 56.95(+0.76) 52.3(+2.94)"
K,0.6570289132602194,"OPO
58.51(+2.32) 54.39(+5.03) 57.56(+1.37) 54.59(+5.23) 58.3(+2.11)
53.15(+3.79)"
K,0.6580259222333001,"10K
random 60.01(+3.82) 56.56(+7.20) 57.41(+1.22) 52.78(+3.42) 57.22(+1.03) 52.56(+3.20)"
K,0.6590229312063809,"OPO
59.49(+3.30) 56.62(+7.26) 58.69(+2.50) 55.14(+5.78) 58.71(+2.52) 55.74(+6.38)"
K,0.6600199401794616,CaSSLe
-,0.6610169491525424,"0
-
60.04
53.89
60.04
53.89
60.04
53.89"
K,0.6620139581256231,"5K
random 61.26(+1.22) 56.72(+2.83) 60.86(+0.82) 54.47(+0.58) 61.06(+1.02) 54.52(+0.63)"
K,0.6630109670987039,"OPO
61.35(+1.31) 56.63(+2.74) 61.39(+1.35) 55.24(+1.35) 61.30(+1.26) 55.77(+1.88)"
K,0.6640079760717846,"10K
random 62.49(+2.45) 57.49(+3.60) 60.98(+0.94) 55.48(+1.59) 61.44(+1.40) 55.40(+1.51)"
K,0.6650049850448654,"OPO
62.38(+2.34) 58.14(+4.25) 61.72(+1.68) 56.92(+3.03) 61.51(+1.47) 56.36(+2.47) PFR"
-,0.6660019940179461,"0
-
60.92
55.57
60.92
55.57
60.92
55.57"
K,0.6669990029910269,"5K
random 62.84(+1.92) 60.01(+4.44) 62.39+(1.47) 58.49(+2.92) 62.16(+1.24) 57.78(+2.21)"
K,0.6679960119641076,"OPO
62.79(+1.87) 59.66(+4.09) 62.16(+1.24) 59.29(+3.72) 62.87(+1.95) 58.41(+2.84)"
K,0.6689930209371885,"10K
random 63.51(+2.59) 61.58(+6.01) 62.57(+1.65) 59.33(+3.76) 62.58(+1.66) 58.45(+2.88)"
K,0.6699900299102692,"OPO
64.37(+3.45) 61.02(+5.45) 63.15(+2.23) 60.31(+4.74) 62.88(+1.96) 59.99(+4.42)"
K,0.67098703888335,Table 15: Full experiment results on ImageNet100 dataset.
K,0.6719840478564307,"Methods
External Dataset
IN
INP
IND"
K,0.6729810568295115,"Budget
Sample
method
5tasks
10tasks
5tasks
10tasks
5tasks
10tasks FT"
-,0.6739780658025922,"0
-
64.02
56.72
64.02
56.72
64.02
56.72"
K,0.674975074775673,"10K
random 67.66(+3.64) 63.02(+6.30) 66.50(+2.48) 61.90(+5.18) 66.90(+2.88) 61.90(+5.18)"
K,0.6759720837487537,"OPO
68.20(+4.18) 64.16(+7.44) 67.84(+3.82) 64.08(+7.36) 69.06(+5.04) 65.00(+8.28)"
K,0.6769690927218345,CaSSLe
-,0.6779661016949152,"0
-
70.02
60.68
70.02
60.68
70.02
60.68"
K,0.678963110667996,"10K
random 71.52(+1.50) 65.02(+4.34) 71.04(+1.02) 64.34(+3.66) 70.98(+0.96) 65.44(+4.76)"
K,0.6799601196410767,"OPO
72.46(+2.44) 66.80(+6.12) 71.44(+1.42) 65.94(+5.26) 72.68(+2.66) 67.10(+6.42) PFR"
-,0.6809571286141576,"0
-
70.14
63.12
70.14
63.12
70.14
63.12"
K,0.6819541375872383,"10K
random 72.82(+2.68) 68.20(+5.08) 71.36(+1.22) 67.26(+4.14) 72.56(+2.42) 67.98(+4.86)"
K,0.6829511465603191,"OPO
72.52(+2.38) 69.28(+6.16) 72.94(+2.80) 68.40(+5.28) 72.60(+2.46) 68.94(+5.82)"
K,0.6839481555333998,"A.3
Limitations and future directions
560"
K,0.6849451645064806,"There are still limitations to BGE, such as increased data volume for training, leading to additional
561"
K,0.6859421734795613,"computational costs. For future directions, we believe BGE can inspire further research into continual
562"
K,0.6869391824526421,"learning from the perspective of inter-task data relationships. Additionally, BGE’s use of external
563"
K,0.6879361914257228,"data instead of exemplars to compensate for inter-task comparisons enhances privacy preservation,
564"
K,0.6889332003988036,"offering a pathway for future work to address privacy concerns associated with using exemplars. We
565"
K,0.6899302093718843,"research methods to allow the network to learn continually, which have no negative impact on society,
566"
K,0.6909272183449651,"and at the same time, we proposed method facilitates privacy protection and has a positive impact on
567"
K,0.6919242273180458,"society.
568"
K,0.6929212362911267,"NeurIPS Paper Checklist
569"
CLAIMS,0.6939182452642074,"1. Claims
570"
CLAIMS,0.6949152542372882,"Question: Do the main claims made in the abstract and introduction accurately reflect the
571"
CLAIMS,0.6959122632103689,"paper’s contributions and scope?
572"
CLAIMS,0.6969092721834497,"Answer: [Yes]
573"
CLAIMS,0.6979062811565304,"Justification: The abstract and introduction in Section 1 accurately reflect our contributions
574"
CLAIMS,0.6989032901296112,"in continual contrastive self-supervised learning.
575"
CLAIMS,0.6999002991026919,"Guidelines:
576"
CLAIMS,0.7008973080757727,"• The answer NA means that the abstract and introduction do not include the claims
577"
CLAIMS,0.7018943170488534,"made in the paper.
578"
CLAIMS,0.7028913260219342,"• The abstract and/or introduction should clearly state the claims made, including the
579"
CLAIMS,0.7038883349950149,"contributions made in the paper and important assumptions and limitations. A No or
580"
CLAIMS,0.7048853439680958,"NA answer to this question will not be perceived well by the reviewers.
581"
CLAIMS,0.7058823529411765,"• The claims made should match theoretical and experimental results, and reflect how
582"
CLAIMS,0.7068793619142573,"much the results can be expected to generalize to other settings.
583"
CLAIMS,0.707876370887338,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
584"
CLAIMS,0.7088733798604188,"are not attained by the paper.
585"
LIMITATIONS,0.7098703888334995,"2. Limitations
586"
LIMITATIONS,0.7108673978065803,"Question: Does the paper discuss the limitations of the work performed by the authors?
587"
LIMITATIONS,0.711864406779661,"Answer: [Yes]
588"
LIMITATIONS,0.7128614157527418,"Justification: We discuss the limitations of our work in Appendix A.3.
589"
LIMITATIONS,0.7138584247258225,"Guidelines:
590"
LIMITATIONS,0.7148554336989033,"• The answer NA means that the paper has no limitation while the answer No means that
591"
LIMITATIONS,0.715852442671984,"the paper has limitations, but those are not discussed in the paper.
592"
LIMITATIONS,0.7168494516450648,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
593"
LIMITATIONS,0.7178464606181456,"• The paper should point out any strong assumptions and how robust the results are to
594"
LIMITATIONS,0.7188434695912264,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
595"
LIMITATIONS,0.7198404785643071,"model well-specification, asymptotic approximations only holding locally). The authors
596"
LIMITATIONS,0.7208374875373879,"should reflect on how these assumptions might be violated in practice and what the
597"
LIMITATIONS,0.7218344965104686,"implications would be.
598"
LIMITATIONS,0.7228315054835494,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
599"
LIMITATIONS,0.7238285144566301,"only tested on a few datasets or with a few runs. In general, empirical results often
600"
LIMITATIONS,0.7248255234297108,"depend on implicit assumptions, which should be articulated.
601"
LIMITATIONS,0.7258225324027916,"• The authors should reflect on the factors that influence the performance of the approach.
602"
LIMITATIONS,0.7268195413758723,"For example, a facial recognition algorithm may perform poorly when image resolution
603"
LIMITATIONS,0.7278165503489531,"is low or images are taken in low lighting. Or a speech-to-text system might not be
604"
LIMITATIONS,0.7288135593220338,"used reliably to provide closed captions for online lectures because it fails to handle
605"
LIMITATIONS,0.7298105682951147,"technical jargon.
606"
LIMITATIONS,0.7308075772681955,"• The authors should discuss the computational efficiency of the proposed algorithms
607"
LIMITATIONS,0.7318045862412762,"and how they scale with dataset size.
608"
LIMITATIONS,0.732801595214357,"• If applicable, the authors should discuss possible limitations of their approach to
609"
LIMITATIONS,0.7337986041874377,"address problems of privacy and fairness.
610"
LIMITATIONS,0.7347956131605184,"• While the authors might fear that complete honesty about limitations might be used by
611"
LIMITATIONS,0.7357926221335992,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
612"
LIMITATIONS,0.7367896311066799,"limitations that aren’t acknowledged in the paper. The authors should use their best
613"
LIMITATIONS,0.7377866400797607,"judgment and recognize that individual actions in favor of transparency play an impor-
614"
LIMITATIONS,0.7387836490528414,"tant role in developing norms that preserve the integrity of the community. Reviewers
615"
LIMITATIONS,0.7397806580259222,"will be specifically instructed to not penalize honesty concerning limitations.
616"
THEORY ASSUMPTIONS AND PROOFS,0.7407776669990029,"3. Theory Assumptions and Proofs
617"
THEORY ASSUMPTIONS AND PROOFS,0.7417746759720838,"Question: For each theoretical result, does the paper provide the full set of assumptions and
618"
THEORY ASSUMPTIONS AND PROOFS,0.7427716849451645,"a complete (and correct) proof?
619"
THEORY ASSUMPTIONS AND PROOFS,0.7437686939182453,"Answer: [NA]
620"
THEORY ASSUMPTIONS AND PROOFS,0.744765702891326,"Justification: We do not include theoretical results.
621"
THEORY ASSUMPTIONS AND PROOFS,0.7457627118644068,"Guidelines:
622"
THEORY ASSUMPTIONS AND PROOFS,0.7467597208374875,"• The answer NA means that the paper does not include theoretical results.
623"
THEORY ASSUMPTIONS AND PROOFS,0.7477567298105683,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
624"
THEORY ASSUMPTIONS AND PROOFS,0.748753738783649,"referenced.
625"
THEORY ASSUMPTIONS AND PROOFS,0.7497507477567298,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
626"
THEORY ASSUMPTIONS AND PROOFS,0.7507477567298105,"• The proofs can either appear in the main paper or the supplemental material, but if
627"
THEORY ASSUMPTIONS AND PROOFS,0.7517447657028913,"they appear in the supplemental material, the authors are encouraged to provide a short
628"
THEORY ASSUMPTIONS AND PROOFS,0.752741774675972,"proof sketch to provide intuition.
629"
THEORY ASSUMPTIONS AND PROOFS,0.7537387836490529,"• Inversely, any informal proof provided in the core of the paper should be complemented
630"
THEORY ASSUMPTIONS AND PROOFS,0.7547357926221336,"by formal proofs provided in appendix or supplemental material.
631"
THEORY ASSUMPTIONS AND PROOFS,0.7557328015952144,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
632"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7567298105682951,"4. Experimental Result Reproducibility
633"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7577268195413759,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
634"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7587238285144566,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
635"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7597208374875374,"of the paper (regardless of whether the code and data are provided or not)?
636"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7607178464606181,"Answer: [Yes]
637"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7617148554336989,"Justification: We realease our code to prove reproducibility.
638"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7627118644067796,"Guidelines:
639"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7637088733798604,"• The answer NA means that the paper does not include experiments.
640"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7647058823529411,"• If the paper includes experiments, a No answer to this question will not be perceived
641"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.765702891326022,"well by the reviewers: Making the paper reproducible is important, regardless of
642"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7666999002991027,"whether the code and data are provided or not.
643"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7676969092721835,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
644"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7686939182452642,"to make their results reproducible or verifiable.
645"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.769690927218345,"• Depending on the contribution, reproducibility can be accomplished in various ways.
646"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7706879361914257,"For example, if the contribution is a novel architecture, describing the architecture fully
647"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7716849451645065,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7726819541375872,"be necessary to either make it possible for others to replicate the model with the same
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.773678963110668,"dataset, or provide access to the model. In general. releasing code and data is often
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7746759720837487,"one good way to accomplish this, but reproducibility can also be provided via detailed
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7756729810568295,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7766699900299102,"of a large language model), releasing of a model checkpoint, or other means that are
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7776669990029911,"appropriate to the research performed.
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7786640079760718,"• While NeurIPS does not require releasing code, the conference does require all submis-
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7796610169491526,"sions to provide some reasonable avenue for reproducibility, which may depend on the
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7806580259222333,"nature of the contribution. For example
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7816550348953141,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7826520438683948,"to reproduce that algorithm.
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7836490528414756,"(b) If the contribution is primarily a new model architecture, the paper should describe
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7846460618145563,"the architecture clearly and fully.
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7856430707876371,"(c) If the contribution is a new model (e.g., a large language model), then there should
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7866400797607178,"either be a way to access this model for reproducing the results or a way to reproduce
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7876370887337986,"the model (e.g., with an open-source dataset or instructions for how to construct
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7886340977068793,"the dataset).
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7896311066799602,"(d) We recognize that reproducibility may be tricky in some cases, in which case
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7906281156530409,"authors are welcome to describe the particular way they provide for reproducibility.
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7916251246261217,"In the case of closed-source models, it may be that access to the model is limited in
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7926221335992024,"some way (e.g., to registered users), but it should be possible for other researchers
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7936191425722832,"to have some path to reproducing or verifying the results.
670"
OPEN ACCESS TO DATA AND CODE,0.7946161515453639,"5. Open access to data and code
671"
OPEN ACCESS TO DATA AND CODE,0.7956131605184447,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
672"
OPEN ACCESS TO DATA AND CODE,0.7966101694915254,"tions to faithfully reproduce the main experimental results, as described in supplemental
673"
OPEN ACCESS TO DATA AND CODE,0.7976071784646062,"material?
674"
OPEN ACCESS TO DATA AND CODE,0.7986041874376869,"Answer: [Yes]
675"
OPEN ACCESS TO DATA AND CODE,0.7996011964107677,"Justification: We release our code, and related information can be found at README.md in
676"
OPEN ACCESS TO DATA AND CODE,0.8005982053838484,"our code supplemental material.
677"
OPEN ACCESS TO DATA AND CODE,0.8015952143569293,"Guidelines:
678"
OPEN ACCESS TO DATA AND CODE,0.80259222333001,"• The answer NA means that paper does not include experiments requiring code.
679"
OPEN ACCESS TO DATA AND CODE,0.8035892323030908,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
680"
OPEN ACCESS TO DATA AND CODE,0.8045862412761715,"public/guides/CodeSubmissionPolicy) for more details.
681"
OPEN ACCESS TO DATA AND CODE,0.8055832502492523,"• While we encourage the release of code and data, we understand that this might not be
682"
OPEN ACCESS TO DATA AND CODE,0.806580259222333,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
683"
OPEN ACCESS TO DATA AND CODE,0.8075772681954138,"including code, unless this is central to the contribution (e.g., for a new open-source
684"
OPEN ACCESS TO DATA AND CODE,0.8085742771684945,"benchmark).
685"
OPEN ACCESS TO DATA AND CODE,0.8095712861415753,"• The instructions should contain the exact command and environment needed to run to
686"
OPEN ACCESS TO DATA AND CODE,0.810568295114656,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
687"
OPEN ACCESS TO DATA AND CODE,0.8115653040877367,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
688"
OPEN ACCESS TO DATA AND CODE,0.8125623130608175,"• The authors should provide instructions on data access and preparation, including how
689"
OPEN ACCESS TO DATA AND CODE,0.8135593220338984,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
690"
OPEN ACCESS TO DATA AND CODE,0.8145563310069791,"• The authors should provide scripts to reproduce all experimental results for the new
691"
OPEN ACCESS TO DATA AND CODE,0.8155533399800599,"proposed method and baselines. If only a subset of experiments are reproducible, they
692"
OPEN ACCESS TO DATA AND CODE,0.8165503489531406,"should state which ones are omitted from the script and why.
693"
OPEN ACCESS TO DATA AND CODE,0.8175473579262214,"• At submission time, to preserve anonymity, the authors should release anonymized
694"
OPEN ACCESS TO DATA AND CODE,0.8185443668993021,"versions (if applicable).
695"
OPEN ACCESS TO DATA AND CODE,0.8195413758723828,"• Providing as much information as possible in supplemental material (appended to the
696"
OPEN ACCESS TO DATA AND CODE,0.8205383848454636,"paper) is recommended, but including URLs to data and code is permitted.
697"
OPEN ACCESS TO DATA AND CODE,0.8215353938185443,"6. Experimental Setting/Details
698"
OPEN ACCESS TO DATA AND CODE,0.8225324027916251,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
699"
OPEN ACCESS TO DATA AND CODE,0.8235294117647058,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
700"
OPEN ACCESS TO DATA AND CODE,0.8245264207377866,"results?
701"
OPEN ACCESS TO DATA AND CODE,0.8255234297108675,"Answer: [Yes]
702"
OPEN ACCESS TO DATA AND CODE,0.8265204386839482,"Justification: We specify all the training and test details in section 4.1 and Appendix A.1.
703"
OPEN ACCESS TO DATA AND CODE,0.827517447657029,"Guidelines:
704"
OPEN ACCESS TO DATA AND CODE,0.8285144566301097,"• The answer NA means that the paper does not include experiments.
705"
OPEN ACCESS TO DATA AND CODE,0.8295114656031904,"• The experimental setting should be presented in the core of the paper to a level of detail
706"
OPEN ACCESS TO DATA AND CODE,0.8305084745762712,"that is necessary to appreciate the results and make sense of them.
707"
OPEN ACCESS TO DATA AND CODE,0.8315054835493519,"• The full details can be provided either with the code, in appendix, or as supplemental
708"
OPEN ACCESS TO DATA AND CODE,0.8325024925224327,"material.
709"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8334995014955134,"7. Experiment Statistical Significance
710"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8344965104685942,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
711"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8354935194416749,"information about the statistical significance of the experiments?
712"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8364905284147557,"Answer: [Yes]
713"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8374875373878365,"Justification: we report error bars in Appendix A.2.6.
714"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8384845463609173,"Guidelines:
715"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.839481555333998,"• The answer NA means that the paper does not include experiments.
716"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8404785643070788,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
717"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8414755732801595,"dence intervals, or statistical significance tests, at least for the experiments that support
718"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8424725822532403,"the main claims of the paper.
719"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.843469591226321,"• The factors of variability that the error bars are capturing should be clearly stated (for
720"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8444666001994018,"example, train/test split, initialization, random drawing of some parameter, or overall
721"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8454636091724825,"run with given experimental conditions).
722"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8464606181455633,"• The method for calculating the error bars should be explained (closed form formula,
723"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.847457627118644,"call to a library function, bootstrap, etc.)
724"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8484546360917248,"• The assumptions made should be given (e.g., Normally distributed errors).
725"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8494516450648056,"• It should be clear whether the error bar is the standard deviation or the standard error
726"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8504486540378864,"of the mean.
727"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8514456630109671,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
728"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8524426719840479,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
729"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8534396809571286,"of Normality of errors is not verified.
730"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8544366899302094,"• For asymmetric distributions, the authors should be careful not to show in tables or
731"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8554336989032901,"figures symmetric error bars that would yield results that are out of range (e.g. negative
732"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8564307078763709,"error rates).
733"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8574277168494516,"• If error bars are reported in tables or plots, The authors should explain in the text how
734"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8584247258225324,"they were calculated and reference the corresponding figures or tables in the text.
735"
EXPERIMENTS COMPUTE RESOURCES,0.8594217347956131,"8. Experiments Compute Resources
736"
EXPERIMENTS COMPUTE RESOURCES,0.8604187437686939,"Question: For each experiment, does the paper provide sufficient information on the com-
737"
EXPERIMENTS COMPUTE RESOURCES,0.8614157527417746,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
738"
EXPERIMENTS COMPUTE RESOURCES,0.8624127617148555,"the experiments?
739"
EXPERIMENTS COMPUTE RESOURCES,0.8634097706879362,"Answer: [Yes]
740"
EXPERIMENTS COMPUTE RESOURCES,0.864406779661017,"Justification: We provide sufficient information on the computer resources in Appendix A.1.
741"
EXPERIMENTS COMPUTE RESOURCES,0.8654037886340977,"Guidelines:
742"
EXPERIMENTS COMPUTE RESOURCES,0.8664007976071785,"• The answer NA means that the paper does not include experiments.
743"
EXPERIMENTS COMPUTE RESOURCES,0.8673978065802592,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
744"
EXPERIMENTS COMPUTE RESOURCES,0.86839481555334,"or cloud provider, including relevant memory and storage.
745"
EXPERIMENTS COMPUTE RESOURCES,0.8693918245264207,"• The paper should provide the amount of compute required for each of the individual
746"
EXPERIMENTS COMPUTE RESOURCES,0.8703888334995015,"experimental runs as well as estimate the total compute.
747"
EXPERIMENTS COMPUTE RESOURCES,0.8713858424725822,"• The paper should disclose whether the full research project required more compute
748"
EXPERIMENTS COMPUTE RESOURCES,0.872382851445663,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
749"
EXPERIMENTS COMPUTE RESOURCES,0.8733798604187437,"didn’t make it into the paper).
750"
CODE OF ETHICS,0.8743768693918246,"9. Code Of Ethics
751"
CODE OF ETHICS,0.8753738783649053,"Question: Does the research conducted in the paper conform, in every respect, with the
752"
CODE OF ETHICS,0.8763708873379861,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
753"
CODE OF ETHICS,0.8773678963110668,"Answer: [Yes]
754"
CODE OF ETHICS,0.8783649052841476,"Justification: We have read the NeurIPS Code of Ethics, and conduct research with it.
755"
CODE OF ETHICS,0.8793619142572283,"Guidelines:
756"
CODE OF ETHICS,0.8803589232303091,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
757"
CODE OF ETHICS,0.8813559322033898,"• If the authors answer No, they should explain the special circumstances that require a
758"
CODE OF ETHICS,0.8823529411764706,"deviation from the Code of Ethics.
759"
CODE OF ETHICS,0.8833499501495513,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
760"
CODE OF ETHICS,0.8843469591226321,"eration due to laws or regulations in their jurisdiction).
761"
BROADER IMPACTS,0.8853439680957128,"10. Broader Impacts
762"
BROADER IMPACTS,0.8863409770687937,"Question: Does the paper discuss both potential positive societal impacts and negative
763"
BROADER IMPACTS,0.8873379860418744,"societal impacts of the work performed?
764"
BROADER IMPACTS,0.8883349950149552,"Answer: [Yes]
765"
BROADER IMPACTS,0.8893320039880359,"Justification: We discuss the societal impacts in Appendix A.3.
766"
BROADER IMPACTS,0.8903290129611167,"Guidelines:
767"
BROADER IMPACTS,0.8913260219341974,"• The answer NA means that there is no societal impact of the work performed.
768"
BROADER IMPACTS,0.8923230309072782,"• If the authors answer NA or No, they should explain why their work has no societal
769"
BROADER IMPACTS,0.8933200398803589,"impact or why the paper does not address societal impact.
770"
BROADER IMPACTS,0.8943170488534397,"• Examples of negative societal impacts include potential malicious or unintended uses
771"
BROADER IMPACTS,0.8953140578265204,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
772"
BROADER IMPACTS,0.8963110667996012,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
773"
BROADER IMPACTS,0.8973080757726819,"groups), privacy considerations, and security considerations.
774"
BROADER IMPACTS,0.8983050847457628,"• The conference expects that many papers will be foundational research and not tied
775"
BROADER IMPACTS,0.8993020937188435,"to particular applications, let alone deployments. However, if there is a direct path to
776"
BROADER IMPACTS,0.9002991026919243,"any negative applications, the authors should point it out. For example, it is legitimate
777"
BROADER IMPACTS,0.901296111665005,"to point out that an improvement in the quality of generative models could be used to
778"
BROADER IMPACTS,0.9022931206380858,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
779"
BROADER IMPACTS,0.9032901296111665,"that a generic algorithm for optimizing neural networks could enable people to train
780"
BROADER IMPACTS,0.9042871385842473,"models that generate Deepfakes faster.
781"
BROADER IMPACTS,0.905284147557328,"• The authors should consider possible harms that could arise when the technology is
782"
BROADER IMPACTS,0.9062811565304087,"being used as intended and functioning correctly, harms that could arise when the
783"
BROADER IMPACTS,0.9072781655034895,"technology is being used as intended but gives incorrect results, and harms following
784"
BROADER IMPACTS,0.9082751744765702,"from (intentional or unintentional) misuse of the technology.
785"
BROADER IMPACTS,0.909272183449651,"• If there are negative societal impacts, the authors could also discuss possible mitigation
786"
BROADER IMPACTS,0.9102691924227319,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
787"
BROADER IMPACTS,0.9112662013958126,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
788"
BROADER IMPACTS,0.9122632103688934,"feedback over time, improving the efficiency and accessibility of ML).
789"
SAFEGUARDS,0.9132602193419741,"11. Safeguards
790"
SAFEGUARDS,0.9142572283150548,"Question: Does the paper describe safeguards that have been put in place for responsible
791"
SAFEGUARDS,0.9152542372881356,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
792"
SAFEGUARDS,0.9162512462612163,"image generators, or scraped datasets)?
793"
SAFEGUARDS,0.9172482552342971,"Answer: [NA]
794"
SAFEGUARDS,0.9182452642073778,"Justification: The paper poses no such risks.
795"
SAFEGUARDS,0.9192422731804586,"Guidelines:
796"
SAFEGUARDS,0.9202392821535393,"• The answer NA means that the paper poses no such risks.
797"
SAFEGUARDS,0.9212362911266201,"• Released models that have a high risk for misuse or dual-use should be released with
798"
SAFEGUARDS,0.922233300099701,"necessary safeguards to allow for controlled use of the model, for example by requiring
799"
SAFEGUARDS,0.9232303090727817,"that users adhere to usage guidelines or restrictions to access the model or implementing
800"
SAFEGUARDS,0.9242273180458624,"safety filters.
801"
SAFEGUARDS,0.9252243270189432,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
802"
SAFEGUARDS,0.9262213359920239,"should describe how they avoided releasing unsafe images.
803"
SAFEGUARDS,0.9272183449651047,"• We recognize that providing effective safeguards is challenging, and many papers do
804"
SAFEGUARDS,0.9282153539381854,"not require this, but we encourage authors to take this into account and make a best
805"
SAFEGUARDS,0.9292123629112662,"faith effort.
806"
LICENSES FOR EXISTING ASSETS,0.9302093718843469,"12. Licenses for existing assets
807"
LICENSES FOR EXISTING ASSETS,0.9312063808574277,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
808"
LICENSES FOR EXISTING ASSETS,0.9322033898305084,"the paper, properly credited and are the license and terms of use explicitly mentioned and
809"
LICENSES FOR EXISTING ASSETS,0.9332003988035892,"properly respected?
810"
LICENSES FOR EXISTING ASSETS,0.93419740777667,"Answer: [Yes]
811"
LICENSES FOR EXISTING ASSETS,0.9351944167497508,"Justification: All existing assets we use are cited in section 4
812"
LICENSES FOR EXISTING ASSETS,0.9361914257228315,"Guidelines:
813"
LICENSES FOR EXISTING ASSETS,0.9371884346959123,"• The answer NA means that the paper does not use existing assets.
814"
LICENSES FOR EXISTING ASSETS,0.938185443668993,"• The authors should cite the original paper that produced the code package or dataset.
815"
LICENSES FOR EXISTING ASSETS,0.9391824526420738,"• The authors should state which version of the asset is used and, if possible, include a
816"
LICENSES FOR EXISTING ASSETS,0.9401794616151545,"URL.
817"
LICENSES FOR EXISTING ASSETS,0.9411764705882353,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
818"
LICENSES FOR EXISTING ASSETS,0.942173479561316,"• For scraped data from a particular source (e.g., website), the copyright and terms of
819"
LICENSES FOR EXISTING ASSETS,0.9431704885343968,"service of that source should be provided.
820"
LICENSES FOR EXISTING ASSETS,0.9441674975074775,"• If assets are released, the license, copyright information, and terms of use in the
821"
LICENSES FOR EXISTING ASSETS,0.9451645064805583,"package should be provided. For popular datasets, paperswithcode.com/datasets
822"
LICENSES FOR EXISTING ASSETS,0.9461615154536391,"has curated licenses for some datasets. Their licensing guide can help determine the
823"
LICENSES FOR EXISTING ASSETS,0.9471585244267199,"license of a dataset.
824"
LICENSES FOR EXISTING ASSETS,0.9481555333998006,"• For existing datasets that are re-packaged, both the original license and the license of
825"
LICENSES FOR EXISTING ASSETS,0.9491525423728814,"the derived asset (if it has changed) should be provided.
826"
LICENSES FOR EXISTING ASSETS,0.9501495513459621,"• If this information is not available online, the authors are encouraged to reach out to
827"
LICENSES FOR EXISTING ASSETS,0.9511465603190429,"the asset’s creators.
828"
NEW ASSETS,0.9521435692921236,"13. New Assets
829"
NEW ASSETS,0.9531405782652044,"Question: Are new assets introduced in the paper well documented and is the documentation
830"
NEW ASSETS,0.9541375872382851,"provided alongside the assets?
831"
NEW ASSETS,0.9551345962113659,"Answer: [NA]
832"
NEW ASSETS,0.9561316051844466,"Justification: The paper does not release new assets.
833"
NEW ASSETS,0.9571286141575274,"Guidelines:
834"
NEW ASSETS,0.9581256231306082,"• The answer NA means that the paper does not release new assets.
835"
NEW ASSETS,0.959122632103689,"• Researchers should communicate the details of the dataset/code/model as part of their
836"
NEW ASSETS,0.9601196410767697,"submissions via structured templates. This includes details about training, license,
837"
NEW ASSETS,0.9611166500498505,"limitations, etc.
838"
NEW ASSETS,0.9621136590229312,"• The paper should discuss whether and how consent was obtained from people whose
839"
NEW ASSETS,0.963110667996012,"asset is used.
840"
NEW ASSETS,0.9641076769690927,"• At submission time, remember to anonymize your assets (if applicable). You can either
841"
NEW ASSETS,0.9651046859421735,"create an anonymized URL or include an anonymized zip file.
842"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9661016949152542,"14. Crowdsourcing and Research with Human Subjects
843"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.967098703888335,"Question: For crowdsourcing experiments and research with human subjects, does the paper
844"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9680957128614157,"include the full text of instructions given to participants and screenshots, if applicable, as
845"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9690927218344965,"well as details about compensation (if any)?
846"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9700897308075773,"Answer: [NA]
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9710867397806581,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
848"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9720837487537388,"Guidelines:
849"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9730807577268196,"• The answer NA means that the paper does not involve crowdsourcing nor research with
850"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9740777666999003,"human subjects.
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9750747756729811,"• Including this information in the supplemental material is fine, but if the main contribu-
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760717846460618,"tion of the paper involves human subjects, then as much detail as possible should be
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9770687936191426,"included in the main paper.
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9780658025922233,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
855"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9790628115653041,"or other labor should be paid at least the minimum wage in the country of the data
856"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9800598205383848,"collector.
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9810568295114656,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
858"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9820538384845464,"Subjects
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9830508474576272,"Question: Does the paper describe potential risks incurred by study participants, whether
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9840478564307079,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9850448654037887,"approvals (or an equivalent approval/review based on the requirements of your country or
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9860418743768694,"institution) were obtained?
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9870388833499502,"Answer: [NA]
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880358923230309,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9890329012961117,"Guidelines:
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9900299102691924,"• The answer NA means that the paper does not involve crowdsourcing nor research with
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910269192422732,"human subjects.
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9920239282153539,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9930209371884346,"may be required for any human subjects research. If you obtained IRB approval, you
870"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9940179461615155,"should clearly state this in the paper.
871"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9950149551345963,"• We recognize that the procedures for this may vary significantly between institutions
872"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996011964107677,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
873"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970089730807578,"guidelines for their institution.
874"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980059820538385,"• For initial submissions, do not include any information that would break anonymity (if
875"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990029910269193,"applicable), such as the institution conducting the review.
876"
