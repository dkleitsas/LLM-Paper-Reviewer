Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0012610340479192938,"Large neural models (such as Transformers) achieve state-of-the-art performance
1"
ABSTRACT,0.0025220680958385876,"for information retrieval (IR). In this paper, we aim to improve distillation methods
2"
ABSTRACT,0.0037831021437578815,"that pave the way for the resource-efﬁcient deployment of such models in practice.
3"
ABSTRACT,0.005044136191677175,"Inspired by our theoretical analysis of the teacher-student generalization gap for
4"
ABSTRACT,0.006305170239596469,"IR models, we propose a novel distillation approach that leverages the relative
5"
ABSTRACT,0.007566204287515763,"geometry among queries and documents learned by the large teacher model. Unlike
6"
ABSTRACT,0.008827238335435058,"existing teacher score-based distillation methods, our proposed approach employs
7"
ABSTRACT,0.01008827238335435,"embedding matching tasks to provide a stronger signal to align the representations
8"
ABSTRACT,0.011349306431273645,"of the teacher and student models. In addition, it utilizes query generation to
9"
ABSTRACT,0.012610340479192938,"explore the data manifold to reduce the discrepancies between the student and the
10"
ABSTRACT,0.013871374527112233,"teacher where training data is sparse. Furthermore, our analysis also motivates
11"
ABSTRACT,0.015132408575031526,"novel asymmetric architectures for student models which realizes better embedding
12"
ABSTRACT,0.01639344262295082,"alignment without increasing online inference cost. On standard benchmarks like
13"
ABSTRACT,0.017654476670870115,"MSMARCO, we show that our approach successfully distills from both dual-
14"
ABSTRACT,0.018915510718789406,"encoder (DE) and cross-encoder (CE) teacher models to 1/10th size asymmetric
15"
ABSTRACT,0.0201765447667087,"students that can retain 95-97% of the teacher performance.
16"
INTRODUCTION,0.021437578814627996,"1
Introduction
17"
INTRODUCTION,0.02269861286254729,"Neural models for information retrieval (IR) are increasingly used to model the true ranking function
18"
INTRODUCTION,0.02395964691046658,"in various applications, including web search [38], recommendation [65], and question-answering
19"
INTRODUCTION,0.025220680958385876,"(QA) [6]. Notably, the recent success of Transformers [59]-based pre-trained language models [11,
20"
INTRODUCTION,0.02648171500630517,"30, 49] on a wide range of natural language understanding tasks has also prompted their utilization in
21"
INTRODUCTION,0.027742749054224466,"IR to capture query-document relevance [see, e.g., 10, 34, 43, 26, 20].
22"
INTRODUCTION,0.029003783102143757,"A typical IR system comprises two stages: (1) A retriever ﬁrst selects a small subset of potentially
23"
INTRODUCTION,0.03026481715006305,"relevant candidate documents (out of a large collection) for a given query; and (2) A re-ranker then
24"
INTRODUCTION,0.031525851197982346,"identiﬁes a precise ranking among the candidates provided by the retriever. Dual-encoder (DE)
25"
INTRODUCTION,0.03278688524590164,"models are the de-facto architecture for retrievers [26, 20]. Such models independently embed queries
26"
INTRODUCTION,0.034047919293820936,"and documents into a common space, and capture their relevance by simple operations on these
27"
INTRODUCTION,0.03530895334174023,"embeddings such as the inner product. This enables ofﬂine creation of a document index and supports
28"
INTRODUCTION,0.03656998738965952,"fast retrieval during inference via efﬁcient maximum inner product search implementations [12, 19],
29"
INTRODUCTION,0.03783102143757881,"with online query embedding generation primarily dictating the inference latency. Cross-encoder (CE)
30"
INTRODUCTION,0.03909205548549811,"models, on the other hand, are preferred as re-rankers, owing to their excellent performance [43, 9, 62].
31"
INTRODUCTION,0.0403530895334174,"A CE model jointly encodes a query-document pair while enabling early interaction among query
32"
INTRODUCTION,0.0416141235813367,"and document features. Employing a CE model for retrieval is often infeasible, as it would require
33"
INTRODUCTION,0.04287515762925599,"processing a given query with every document in the collection at inference time. In fact, even in
34"
INTRODUCTION,0.044136191677175286,"the re-ranking stage, the inference cost of CE models is high enough [22] to warrant exploration of
35"
INTRODUCTION,0.04539722572509458,"efﬁcient alternatives [14, 22, 37]. Across both architectures, scaling to larger models brings improved
36"
INTRODUCTION,0.04665825977301387,"performance at increased computational cost [41, 39].
37"
INTRODUCTION,0.04791929382093316,"Knowledge distillation [5, 13] provides a general strategy to address the prohibitive inference cost
38"
INTRODUCTION,0.04918032786885246,"associated with high-quality large neural models. In the IR literature, most existing distillation
39"
INTRODUCTION,0.05044136191677175,"methods only rely on the teacher’s query-document relevance scores [see, e.g., 31, 14, 8, 51, 56] or
40"
INTRODUCTION,0.05170239596469105,"their proxies [16]. However, given that neural IR models are inherently embedding-based, it is natural
41"
INTRODUCTION,0.05296343001261034,"to ask: Is it useful to go beyond matching of the teacher and student models’ scores, and directly aim
42"
INTRODUCTION,0.05422446406052964,"to align their embedding spaces?
43"
INTRODUCTION,0.05548549810844893,"With this in mind, we propose a novel distillation method for IR models that utilizes an embedding
44"
INTRODUCTION,0.05674653215636822,"matching task to train student models. The proposed method is inspired by our rigorous treatment
45"
INTRODUCTION,0.058007566204287514,"of the generalization gap between the teacher and student models in IR settings. Our theoretical
46"
INTRODUCTION,0.05926860025220681,"analysis of the teacher-student generalization gap further suggests novel design choices involving
47"
INTRODUCTION,0.0605296343001261,"asymmetric conﬁgurations for student DE models, intending to further reduce the gap by better
48"
INTRODUCTION,0.0617906683480454,"aligning teacher and student embedding spaces. Notably, our proposed distillation method supports
49"
INTRODUCTION,0.06305170239596469,"cross-architecture distillation and improves upon existing (score-based) distillation methods for both
50"
INTRODUCTION,0.06431273644388398,"retriever and re-ranker models. When distilling a large teacher DE model into a smaller student DE
51"
INTRODUCTION,0.06557377049180328,"model, for a given query (document), one can minimize the distance between the query (document)
52"
INTRODUCTION,0.06683480453972257,"embeddings of the teacher and student (after compatible projection layers to account for dimension
53"
INTRODUCTION,0.06809583858764187,"mismatch, if any). In contrast, a teacher CE model doesn’t directly provide document and query
54"
INTRODUCTION,0.06935687263556116,"embeddings, and so to effectively employ embedding matching-based distillation requires modifying
55"
INTRODUCTION,0.07061790668348046,"the scoring layer with dual-pooling [61] and adding various regularizers. Both of these changes
56"
INTRODUCTION,0.07187894073139975,"improve geometry of teacher embeddings and facilitate effective knowledge transfer to the student
57"
INTRODUCTION,0.07313997477931904,"DE model via embedding matching-based distillation.
58"
INTRODUCTION,0.07440100882723834,"Our key contributions toward improving IR models via distillation are:
59"
INTRODUCTION,0.07566204287515763,"• We provide the ﬁrst rigorous analysis of the teacher-student generalization gap for IR settings
60"
INTRODUCTION,0.07692307692307693,"which captures the role of alignment of embedding spaces of the teacher and student towards
61"
INTRODUCTION,0.07818411097099622,"reducing the gap (Sec. 3).
62"
INTRODUCTION,0.07944514501891552,"• Inspired by our analysis, we propose a novel distillation approach for neural IR models, namely
63"
INTRODUCTION,0.0807061790668348,"EmbedDistill, that goes beyond score matching and aligns the embedding spaces of the teacher and
64"
INTRODUCTION,0.08196721311475409,"student models (Sec. 4). We also show that EmbedDistill can leverage synthetic data to improve a
65"
INTRODUCTION,0.0832282471626734,"student by further aligning the embedding spaces of the teacher and student (Sec. 4.3).
66"
INTRODUCTION,0.08448928121059268,"• Our analysis motivates novel distillation setups. Speciﬁcally, we consider a student DE model with
67"
INTRODUCTION,0.08575031525851198,"an asymmetric conﬁguration, consisting of a small query encoder and a frozen document encoder
68"
INTRODUCTION,0.08701134930643127,"inherited from the teacher. This signiﬁcantly reduces inference latency of query embedding
69"
INTRODUCTION,0.08827238335435057,"generation, while leveraging the teachers’ high-quality document index (Sec. 4.1).
70"
INTRODUCTION,0.08953341740226986,"• We provide a comprehensive empirical evaluation of EmbedDistill (Sec. 5) on two standard IR
71"
INTRODUCTION,0.09079445145018916,"benchmarks – Natural Questions [23] and MSMARCO [40]. We also evaluate EmbedDistill on
72"
INTRODUCTION,0.09205548549810845,"BEIR benchmark [57] which is used to measure the zero-shot performance of an IR model.
73"
INTRODUCTION,0.09331651954602774,"Note that prior works have utilized embedding alignment during distillation for non-IR setting [see,
74"
INTRODUCTION,0.09457755359394704,"e.g., 52, 55, 18, 1, 64, 7]. However, to the best of our knowledge, our work is the ﬁrst to study
75"
INTRODUCTION,0.09583858764186633,"embedding matching-based distillation method for IR settings which requires addressing multiple
76"
INTRODUCTION,0.09709962168978563,"IR-speciﬁc challenges such as cross-architecture distillation, partial representation alignment, and en-
77"
INTRODUCTION,0.09836065573770492,"abling novel asymmetric student conﬁgurations. Furthermore, unlike these prior works, our proposed
78"
INTRODUCTION,0.09962168978562422,"method is theoretically justiﬁed to reduce the teacher-student performance gap.
79"
BACKGROUND,0.1008827238335435,"2
Background
80"
BACKGROUND,0.1021437578814628,"Let Q and D denote the query and document spaces, respectively. An IR model is equivalent to
81"
BACKGROUND,0.1034047919293821,"a scorer s : Q ⇥D ! R, i.e., it assigns a (relevance) score s(q, d) for a query-document pair
82"
BACKGROUND,0.10466582597730138,"(q, d) 2 Q ⇥D. Ideally, we want to learn a scorer such that s(q, d) > s(q, d0) iff the document d is
83"
BACKGROUND,0.10592686002522068,"more relevant to the query q than document d0. We assume access to n labeled training examples
84"
BACKGROUND,0.10718789407313997,"Sn = {(qi, di, yi)}i2[n]. Here, di = (di,1, . . . , di,L) 2 DL, 8i 2 [n], denotes a list of L documents
85"
BACKGROUND,0.10844892812105927,"and yi = (yi,1, . . . , yi,L) 2 {0, 1}L denotes the corresponding labels such that yi,j = 1 iff the
86"
BACKGROUND,0.10970996216897856,"document di,j is relevant to the query qi. Given Sn, we learn an IR model by minimizing
87"
BACKGROUND,0.11097099621689786,R(s; Sn) := 1 n X
BACKGROUND,0.11223203026481715,"i2[n] ` """
BACKGROUND,0.11349306431273644,"sqi,di, yi # ,
(1)"
BACKGROUND,0.11475409836065574,"where sqi,di := (s(qi, d1,i), . . . , s(qi, d1,L)) and ` """
BACKGROUND,0.11601513240857503,"sqi,di, yi #"
BACKGROUND,0.11727616645649433,"denotes the loss s incurs on (qi, di, yi).
88"
BACKGROUND,0.11853720050441362,"Due to space constraint, we defer concrete choices for the loss function ` to Appendix A.
89"
BACKGROUND,0.11979823455233292,"While this learning framework is general enough to work with any IR models, next, we formally
90"
BACKGROUND,0.1210592686002522,"introduce two families of Transformer-based IR models that are prevalent in the recent literature.
91"
BACKGROUND,0.1223203026481715,"2.1
Transformer-based IR models: Cross-encoders and Dual-encoders
92"
BACKGROUND,0.1235813366960908,"Let query q = (q1, . . . , qm1) and document d = (d1, . . . , dm2) consist of m1 and m2 tokens, respec-
93"
BACKGROUND,0.12484237074401008,"tively. We now discuss how Transformers-based CE and DE models process the (q, d) pair.
94"
BACKGROUND,0.12610340479192939,"Cross-encoder model. Let p = [q; d] be the sequence obtained by concatenating q and d. Further,
95"
BACKGROUND,0.1273644388398487,"let ˜p be the sequence obtained by adding special tokens such [CLS] and [SEP] to p. Given an
96"
BACKGROUND,0.12862547288776796,"encoder-only Transformer model Enc, the relevance score for the (q, d) pair is
97"
BACKGROUND,0.12988650693568726,"s(q, d) = hw, pool """
BACKGROUND,0.13114754098360656,Enc(˜p) #
BACKGROUND,0.13240857503152584,"i = hw, embq,di,
(2)"
BACKGROUND,0.13366960907944514,"where w is a d-dimensional classiﬁcation vector, and pool(·) denotes a pooling operation that
98"
BACKGROUND,0.13493064312736444,"transforms the contextualized token embeddings Enc(˜p) to a joint embedding vector embq,d. [CLS]-
99"
BACKGROUND,0.13619167717528374,"pooling is a common operation that simply outputs the embedding of the [CLS] token as embq,d.
100"
BACKGROUND,0.13745271122320302,"Dual-encoder model. Let ˜q and ˜d be the sequences obtained by adding appropriate special tokens
101"
BACKGROUND,0.13871374527112232,"to q and d, respectively. A DE model comprises two (encoder-only) Transformers EncQ and EncD,
102"
BACKGROUND,0.13997477931904162,"which we call query and document encoders, respectively.1 Let embq = pool """
BACKGROUND,0.14123581336696092,EncQ(˜q) #
BACKGROUND,0.1424968474148802,"and embd
103"
BACKGROUND,0.1437578814627995,"= pool """
BACKGROUND,0.1450189155107188,EncD( ˜d) #
BACKGROUND,0.14627994955863807,"denote the query and document embeddings, respectively. Now, one can deﬁne
104"
BACKGROUND,0.14754098360655737,"s(q, d) = hembq, embdi to be the relevance score assigned to the (q, d) pair by the DE model.
105"
SCORE-BASED DISTILLATION FOR IR MODELS,0.14880201765447668,"2.2
Score-based distillation for IR models
106"
SCORE-BASED DISTILLATION FOR IR MODELS,0.15006305170239598,"Most distillation schemes for IR [e.g., 31, 14, 8] rely on teacher relevance scores. Given a training set
107"
SCORE-BASED DISTILLATION FOR IR MODELS,0.15132408575031525,"Sn and a teacher with scorer st, one learns a student with scorer ss by minimizing
108"
SCORE-BASED DISTILLATION FOR IR MODELS,0.15258511979823455,"R(ss, st; Sn) = 1 n X"
SCORE-BASED DISTILLATION FOR IR MODELS,0.15384615384615385,"i2[n] `d "" ss"
SCORE-BASED DISTILLATION FOR IR MODELS,0.15510718789407313,"q,di, st q,di # ,
(3)"
SCORE-BASED DISTILLATION FOR IR MODELS,0.15636822194199243,"where `d captures the discrepancy between ss and st. See Appendix A for common choices for `d.
109"
SCORE-BASED DISTILLATION FOR IR MODELS,0.15762925598991173,"3
Teacher-student generalization gap: Inspiration for embedding alignment
110"
SCORE-BASED DISTILLATION FOR IR MODELS,0.15889029003783103,"Our main objective is to devise novel distillation methods to realize high-performing student DE
111"
SCORE-BASED DISTILLATION FOR IR MODELS,0.1601513240857503,"models. As a ﬁrst step in this direction, we rigorously study the teacher-student generalization
112"
SCORE-BASED DISTILLATION FOR IR MODELS,0.1614123581336696,"gap as realized by standard (score-based) distillation in IR settings. Informed by our analysis, we
113"
SCORE-BASED DISTILLATION FOR IR MODELS,0.1626733921815889,"subsequently identify novel ways to improve the student model’s performance. In particular, our
114"
SCORE-BASED DISTILLATION FOR IR MODELS,0.16393442622950818,"analysis suggests two natural directions to reduce the teacher-student generalization gap: 1) enforcing
115"
SCORE-BASED DISTILLATION FOR IR MODELS,0.16519546027742749,"tighter alignment between embedding spaces of teacher and student models; and 2) exploring novel
116"
SCORE-BASED DISTILLATION FOR IR MODELS,0.1664564943253468,"asymmetric conﬁguration for student DE model.
117"
SCORE-BASED DISTILLATION FOR IR MODELS,0.1677175283732661,"Let R(s) = E ⇥ ` """
SCORE-BASED DISTILLATION FOR IR MODELS,0.16897856242118536,"sq,d, y #⇤"
SCORE-BASED DISTILLATION FOR IR MODELS,0.17023959646910466,"be the population version of the empirical risk in Eq. 1, which measures
118"
SCORE-BASED DISTILLATION FOR IR MODELS,0.17150063051702397,"the test time performance of the IR model deﬁned by the scorer s. Thus, R(ss) −R(st) denotes the
119"
SCORE-BASED DISTILLATION FOR IR MODELS,0.17276166456494324,"teacher-student generalization gap. In the following result, we bound this quantity (see Appendix C.1
120"
SCORE-BASED DISTILLATION FOR IR MODELS,0.17402269861286254,"for a formal statement and proof). We focus on distilling a teacher DE model to a student DE model
121"
SCORE-BASED DISTILLATION FOR IR MODELS,0.17528373266078184,"and L = 1 (cf. Sec. 2) as it leads to easier exposition without changing the main takeaways. Our
122"
SCORE-BASED DISTILLATION FOR IR MODELS,0.17654476670870115,"analysis can be extended to L > 1 or CE to DE distillation with more complex notation.
123"
SCORE-BASED DISTILLATION FOR IR MODELS,0.17780580075662042,"Theorem 3.1 (Teacher-student generalization gap (informal)). Let F and G denote the function
124"
SCORE-BASED DISTILLATION FOR IR MODELS,0.17906683480453972,"classes for the query and document encoders for the student model, respectively. Suppose that the
125"
SCORE-BASED DISTILLATION FOR IR MODELS,0.18032786885245902,"score-based distillation loss `d in Eq. 3 is based on binary cross entropy loss (Eq. 12 in Appendix A).
126"
SCORE-BASED DISTILLATION FOR IR MODELS,0.18158890290037832,"Let one-hot (label-dependent) loss ` in Eq. 1 be the binary cross entropy loss (Eq. 10 in Appendix A).
127"
SCORE-BASED DISTILLATION FOR IR MODELS,0.1828499369482976,"Further, assume that all encoders have the same output dimension and embeddings have their `2-norm
128"
SCORE-BASED DISTILLATION FOR IR MODELS,0.1841109709962169,"bounded by K. Then, we have
129"
SCORE-BASED DISTILLATION FOR IR MODELS,0.1853720050441362,"R(ss) −R(st) En(F, G) + 2KREmb,Q(t, s; Sn) + 2KREmb,D(t, s; Sn)"
SCORE-BASED DISTILLATION FOR IR MODELS,0.18663303909205547,"+ ∆(st; Sn) + K2"" E"
SCORE-BASED DISTILLATION FOR IR MODELS,0.18789407313997478,⇥&&σ(st
SCORE-BASED DISTILLATION FOR IR MODELS,0.18915510718789408,"q,d) −y &&⇤ + 1 n X i2[n]"
SCORE-BASED DISTILLATION FOR IR MODELS,0.19041614123581338,&&σ(st
SCORE-BASED DISTILLATION FOR IR MODELS,0.19167717528373265,"qi,di) −yi && # ,
(4)"
SCORE-BASED DISTILLATION FOR IR MODELS,0.19293820933165196,1It is common to employ dual-encoder models where query and document encoders are shared.
SCORE-BASED DISTILLATION FOR IR MODELS,0.19419924337957126,"Special tokens
Query & doc tokens"
SCORE-BASED DISTILLATION FOR IR MODELS,0.19546027742749053,Student Query
SCORE-BASED DISTILLATION FOR IR MODELS,0.19672131147540983,Encoder
SCORE-BASED DISTILLATION FOR IR MODELS,0.19798234552332913,Teacher Doc
SCORE-BASED DISTILLATION FOR IR MODELS,0.19924337957124844,"Encoder …
…"
SCORE-BASED DISTILLATION FOR IR MODELS,0.2005044136191677,"Doc tokens
Query tokens Score"
SCORE-BASED DISTILLATION FOR IR MODELS,0.201765447667087,Teacher Doc
SCORE-BASED DISTILLATION FOR IR MODELS,0.2030264817150063,"Encoder …
…"
SCORE-BASED DISTILLATION FOR IR MODELS,0.2042875157629256,"Doc tokens
Query tokens Score"
SCORE-BASED DISTILLATION FOR IR MODELS,0.2055485498108449,Teacher Query
SCORE-BASED DISTILLATION FOR IR MODELS,0.2068095838587642,Encoder
SCORE-BASED DISTILLATION FOR IR MODELS,0.2080706179066835,"≈
Embedding matching
Pooling
Pooling"
SCORE-BASED DISTILLATION FOR IR MODELS,0.20933165195460277,Pooling
SCORE-BASED DISTILLATION FOR IR MODELS,0.21059268600252207,Pooling
SCORE-BASED DISTILLATION FOR IR MODELS,0.21185372005044137,Score-based distillation
SCORE-BASED DISTILLATION FOR IR MODELS,0.21311475409836064,(a) DE to DE distillation
SCORE-BASED DISTILLATION FOR IR MODELS,0.21437578814627994,"Teacher
Cross Encoder"
SCORE-BASED DISTILLATION FOR IR MODELS,0.21563682219419925,Student Query
SCORE-BASED DISTILLATION FOR IR MODELS,0.21689785624211855,Encoder
SCORE-BASED DISTILLATION FOR IR MODELS,0.21815889029003782,Student Doc
SCORE-BASED DISTILLATION FOR IR MODELS,0.21941992433795712,"Encoder …
…"
SCORE-BASED DISTILLATION FOR IR MODELS,0.22068095838587642,"Doc tokens
Query tokens"
SCORE-BASED DISTILLATION FOR IR MODELS,0.22194199243379573,"Score
Score"
SCORE-BASED DISTILLATION FOR IR MODELS,0.223203026481715,"Special tokens
Query & doc tokens …
…"
SCORE-BASED DISTILLATION FOR IR MODELS,0.2244640605296343,"Doc tokens
Query tokens"
SCORE-BASED DISTILLATION FOR IR MODELS,0.2257250945775536,"Pooling
Pooling"
SCORE-BASED DISTILLATION FOR IR MODELS,0.22698612862547288,Score-based
SCORE-BASED DISTILLATION FOR IR MODELS,0.22824716267339218,"distillation
≈
Embedding matching"
SCORE-BASED DISTILLATION FOR IR MODELS,0.22950819672131148,(b) CE to DE distillation
SCORE-BASED DISTILLATION FOR IR MODELS,0.23076923076923078,"Figure 1: Proposed distillation method with query embedding matching. Left: The setting where student employs
an asymmetric DE conﬁguration with a small query encoder and a large (non-trainable) document encoder
inherited from the teacher DE model. The smaller query encoder ensures small latency for encoding query
during inference, and large document encoder leads to a good quality document index. Right: Similarly the
setting of CE to DE distillation using EmbedDistill, with teacher CE model employing dual pooling."
SCORE-BASED DISTILLATION FOR IR MODELS,0.23203026481715006,"where En(F, G) := supss2F⇥G"
SCORE-BASED DISTILLATION FOR IR MODELS,0.23329129886506936,"&&R(ss, st; Sn) −E`d "" ss"
SCORE-BASED DISTILLATION FOR IR MODELS,0.23455233291298866,"q,d, st q,d"
SCORE-BASED DISTILLATION FOR IR MODELS,0.23581336696090793,"#&&; σ denotes the sigmoid function;
130"
SCORE-BASED DISTILLATION FOR IR MODELS,0.23707440100882723,"and ∆(st; Sn) denotes the deviation between the empirical risk (on Sn) and population risk of the
131"
SCORE-BASED DISTILLATION FOR IR MODELS,0.23833543505674654,"teacher st. Here, REmb,Q(t, s; Sn) and REmb,D(t, s; Sn) measure misalignment between teacher and
132"
SCORE-BASED DISTILLATION FOR IR MODELS,0.23959646910466584,"student embeddings by focusing on queries and documents, respectively (cf. Eq. 7 & 8 in Sec. 4.1).
133"
SCORE-BASED DISTILLATION FOR IR MODELS,0.2408575031525851,"The last three quantities in the bound in Thm. 3.1, namely ∆(st; Sn), E[|σ(st"
SCORE-BASED DISTILLATION FOR IR MODELS,0.2421185372005044,"q,d) −y|], and
134"
N,0.24337957124842372,"1
n P"
N,0.244640605296343,i2[n] |σ(st
N,0.2459016393442623,"qi,di) −yi|, are independent of the underlying student model. These terms solely
135"
N,0.2471626733921816,"depend on the quality of the underlying teacher model st. That said, the teacher-student gap can be
136"
N,0.2484237074401009,"made small by reducing the following three terms: 1) uniform deviation of the student’s empirical
137"
N,0.24968474148802017,"distillation risk from its population version En(F, G); 2) misalignment between teacher student query
138"
N,0.2509457755359395,"embeddings REmb,Q(t, s; Sn); and 3) misalignment between teacher student document embeddings
139"
N,0.25220680958385877,"REmb,D(t, s; Sn).
140"
N,0.25346784363177804,"The last two terms motivate us to propose an embedding matching-based distillation that explicitly
141"
N,0.2547288776796974,"aims to minimize these terms during student training. Even more interestingly, these terms also
142"
N,0.25598991172761665,"inspire an asymmetric DE conﬁguration for the student which strikes a balance between the goals of
143"
N,0.2572509457755359,"reducing the misalignment between the embeddings of teacher and student (by inheriting teacher’s
144"
N,0.25851197982345525,"document encoder) and ensuring serving efﬁciency (small inference latency) by employing a small
145"
N,0.2597730138713745,"query encoder. Before discussing these proposals in detail in Sec. 4 and Fig. 1, we explore the ﬁrst
146"
N,0.2610340479192938,"term En(F, G) and highlight how our proposals also have implications for reducing this term. Towards
147"
N,0.26229508196721313,"this, the following result bounds En(F, G). Due to space constraints, we present an informal statement
148"
N,0.2635561160151324,"of the result (see Appendix C.2 for a more precise statement and proof).
149"
N,0.2648171500630517,"Proposition 3.2. Let `d be a distillation loss which is L`d-Lipschitz in its ﬁrst argument. Let F and G
150"
N,0.266078184110971,"denote the function classes for the query and document encoders, respectively. Further assume that,
151"
N,0.2673392181588903,"for each query and document encoder in our function class, the query and document embeddings
152"
N,0.2686002522068096,"have their `2-norm bounded by K. Then,
153"
N,0.2698612862547289,"En(F, G) ESn"
N,0.27112232030264816,"48KL`d
pn Z 1 0 q log """
N,0.2723833543505675,"N(u, F)N(u, G) #"
N,0.27364438839848676,"du.
(5)"
N,0.27490542244640603,"Furthermore, with a ﬁxed document encoder, i.e., G = {g⇤},
154"
N,0.27616645649432536,"En(F, {g⇤}) ESn"
N,0.27742749054224464,"48KL`d
pn Z 1 0 p"
N,0.2786885245901639,"log N(u, F) du.
(6)"
N,0.27994955863808324,"Here, N(u, ·) is the u-covering number of a function class.
155"
N,0.2812105926860025,"Note that Eq. 5 and Eq. 6 correspond to uniform deviation when we train without and with a frozen
156"
N,0.28247162673392184,"document encoder, respectively. It is clear that the bound in Eq. 6 is less than or equal to that in
157"
N,0.2837326607818411,"Eq. 5 (because N(u, G) ≥1 for any u), which alludes to desirable impact of employing a frozen
158"
N,0.2849936948297604,"document encoder as one of our proposal seeks to do via inheriting teacher’s document encoder (for
159"
N,0.2862547288776797,"instance in an asymmetric DE conﬁguration). Furthermore, our proposal of employing an embedding-
160"
N,0.287515762925599,"matching task will regularize the function class of query encoders; effectively reducing it to F0 with
161"
N,0.28877679697351827,"|F0| |F|. The same holds true for document encoder function class when document encoder is
162"
N,0.2900378310214376,"trainable (as in Eq. 5), leading to an effective function class G0 with |G0| |G|. Since we would have
163"
N,0.29129886506935687,"N(u, F0) N(u, F) and N(u, G0) N(u, G), this suggests desirable implications of embedding
164"
N,0.29255989911727615,"matching for reducing the uniform deviation bound.
165"
EMBEDDING-MATCHING BASED DISTILLATION,0.2938209331651955,"4
Embedding-matching based distillation
166"
EMBEDDING-MATCHING BASED DISTILLATION,0.29508196721311475,"Informed by our analysis of teacher-student generalization gap in Sec. 3, we propose EmbedDistill – a
167"
EMBEDDING-MATCHING BASED DISTILLATION,0.296343001261034,"novel distillation method that explicitly focuses on aligning the embedding spaces of the teacher and
168"
EMBEDDING-MATCHING BASED DISTILLATION,0.29760403530895335,"student. Our proposal goes beyond existing distillation methods in the IR literature that only use the
169"
EMBEDDING-MATCHING BASED DISTILLATION,0.2988650693568726,"teacher scores. Next, we introduce EmbedDistill for two prevalent settings: (1) distilling a large DE
170"
EMBEDDING-MATCHING BASED DISTILLATION,0.30012610340479196,"model to a smaller DE model; 2 and (2) distilling a CE model to a DE model.
171"
DE TO DE DISTILLATION,0.30138713745271123,"4.1
DE to DE distillation
172"
DE TO DE DISTILLATION,0.3026481715006305,"Given a (q, d) pair, let embt"
DE TO DE DISTILLATION,0.30390920554854983,q and embt
DE TO DE DISTILLATION,0.3051702395964691,"d be the query and document embeddings produced by the
173"
DE TO DE DISTILLATION,0.3064312736443884,query encoder Enct
DE TO DE DISTILLATION,0.3076923076923077,Q and document encoder Enct
DE TO DE DISTILLATION,0.308953341740227,"D of the teacher DE model, respectively. Similarly,
174"
DE TO DE DISTILLATION,0.31021437578814626,let embs
DE TO DE DISTILLATION,0.3114754098360656,q and embs
DE TO DE DISTILLATION,0.31273644388398486,"d denote the query and document embeddings produced by a student DE model
175"
DE TO DE DISTILLATION,0.31399747793190413,with (Encs
DE TO DE DISTILLATION,0.31525851197982346,"Q, Encs"
DE TO DE DISTILLATION,0.31651954602774274,"D) as its query and document encoders. Now, EmbedDistill optimizes the following
176"
DE TO DE DISTILLATION,0.31778058007566207,"embedding alignment losses in addition to the score-matching loss from Sec. 2.2 to align query and
177"
DE TO DE DISTILLATION,0.31904161412358134,"document embeddings of the teacher and student:
178"
DE TO DE DISTILLATION,0.3203026481715006,"REmb,Q(t, s; Sn) = 1 n X"
DE TO DE DISTILLATION,0.32156368221941994,q2Sn kembt
DE TO DE DISTILLATION,0.3228247162673392,"q −proj "" embs q #"
DE TO DE DISTILLATION,0.3240857503152585,"k;
(7)"
DE TO DE DISTILLATION,0.3253467843631778,"REmb,D(t, s; Sn) = 1 n X"
DE TO DE DISTILLATION,0.3266078184110971,d2Sn kembt
DE TO DE DISTILLATION,0.32786885245901637,"d −proj "" embs d #"
DE TO DE DISTILLATION,0.3291298865069357,"k.
(8)"
DE TO DE DISTILLATION,0.33039092055485497,"Asymmetric DE. We also propose a novel student DE conﬁguration where the student employs the
179"
DE TO DE DISTILLATION,0.3316519546027743,"teacher’s document encoder (i.e., Encs"
DE TO DE DISTILLATION,0.3329129886506936,D = Enct
DE TO DE DISTILLATION,0.33417402269861285,"D) and only train its query encoder, which is much
180"
DE TO DE DISTILLATION,0.3354350567465322,"smaller compared to the teacher’s query encoder. For such a setting, it is natural to only employ the
181"
DE TO DE DISTILLATION,0.33669609079445145,"embedding matching loss in Eq. 7 as the document embeddings are aligned by design (cf. Fig. 1a).
182"
DE TO DE DISTILLATION,0.3379571248423707,"Note that this asymmetric student DE does not incur an increase in latency despite the use of a
183"
DE TO DE DISTILLATION,0.33921815889029006,"large teacher document encoder. This is because the large document encoder is only needed to
184"
DE TO DE DISTILLATION,0.34047919293820933,"create a good quality document index ofﬂine, and only the query encoder is evaluated at inference
185"
DE TO DE DISTILLATION,0.3417402269861286,"time. Also, the similarity search cost is not increased as the projection layer ensures the same small
186"
DE TO DE DISTILLATION,0.34300126103404793,"embedding dimension as in the symmetric DE student. Thus, for DE to DE distillation, we prescribe
187"
DE TO DE DISTILLATION,0.3442622950819672,"the asymmetric DE conﬁguration universally. Our theoretical analysis (cf. Sec. 3) and experimental
188"
DE TO DE DISTILLATION,0.3455233291298865,"results (cf. Sec. 5) suggest that the ability to inherit the document tower from the teacher DE model
189"
DE TO DE DISTILLATION,0.3467843631778058,"can drastically improve the ﬁnal performance, especially when combined with query embedding
190"
DE TO DE DISTILLATION,0.3480453972257251,"matching task (cf. Eq. 7).
191"
CE TO DE DISTILLATION,0.3493064312736444,"4.2
CE to DE distillation
192"
CE TO DE DISTILLATION,0.3505674653215637,"Given that CE models jointly encode query-document pairs, individual query and document embed-
193"
CE TO DE DISTILLATION,0.35182849936948296,"dings are not readily available to implement embedding matching losses as per Eq. 7 and 8. This
194"
CE TO DE DISTILLATION,0.3530895334174023,"makes it challenging to employ EmbedDistill for CE to DE distillation.
195"
CE TO DE DISTILLATION,0.35435056746532156,"As a naïve solution, for a (q, d) pair, one can simply match a joint transformation of the student’s query
196"
CE TO DE DISTILLATION,0.35561160151324084,embedding embs
CE TO DE DISTILLATION,0.35687263556116017,q and document embedding embs
CE TO DE DISTILLATION,0.35813366960907944,d to the teacher’s joint embedding embt
CE TO DE DISTILLATION,0.3593947036569987,"q,d , produced
197"
CE TO DE DISTILLATION,0.36065573770491804,"by (single) teacher encoder Enct. However, we observed that including such an embedding matching
198"
CE TO DE DISTILLATION,0.3619167717528373,"task often leads to severe over-ﬁtting, and results in a poor student. Since st(q, d) = hw, embt"
CE TO DE DISTILLATION,0.36317780580075665,"q,di,
199"
CE TO DE DISTILLATION,0.3644388398486759,"during CE model training, the joint embeddings embt"
CE TO DE DISTILLATION,0.3656998738965952,"q,d for relevant and irrelevant (q, d) pairs are
200"
CE TO DE DISTILLATION,0.3669609079445145,"encouraged to be aligned with w and −w, respectively. This produces degenerate embeddings that
201"
CE TO DE DISTILLATION,0.3682219419924338,"do not capture semantic query-to-document relationships. We notice that even the ﬁnal query and
202"
CE TO DE DISTILLATION,0.3694829760403531,"document token embeddings lose such semantic structure (cf. Appendix G.2). Thus, a teacher CE
203"
CE TO DE DISTILLATION,0.3707440100882724,"model with st(q, d) = hw, embt"
CE TO DE DISTILLATION,0.3720050441361917,"q,di does not add value for distillation beyond score-matching; in
204"
CE TO DE DISTILLATION,0.37326607818411095,"fact, it hurts to include naïve embedding matching. Next, we propose a modiﬁed CE model training
205"
CE TO DE DISTILLATION,0.3745271122320303,"strategy that facilitates EmbedDistill.
206"
CE TO DE DISTILLATION,0.37578814627994955,"CE models with dual pooling. A dual pooling scheme is employed in the scoring layer to produce
207"
CE TO DE DISTILLATION,0.3770491803278688,two embeddings embt
CE TO DE DISTILLATION,0.37831021437578816,"q (q,d) and embt"
CE TO DE DISTILLATION,0.37957124842370743,"d (q,d) from a CE model that serve as the proxy query and
208"
CE TO DE DISTILLATION,0.38083228247162676,"document embeddings, respectively. Accordingly, we deﬁne the relevance score as st(q, d) =
209 hembt"
CE TO DE DISTILLATION,0.38209331651954603,"q (q,d), embt"
CE TO DE DISTILLATION,0.3833543505674653,"d (q,d)i. We explore two variants of dual pooling: (1) special token-based pooling
210"
CE TO DE DISTILLATION,0.38461538461538464,"that pools from [CLS] and [SEP]; and (2) segment-based weighted mean pooling that separately
211"
CE TO DE DISTILLATION,0.3858764186633039,2CE to CE distillation is a special case of this with classiﬁcation vector w (cf. Eq. 2) as trivial second encoder.
CE TO DE DISTILLATION,0.3871374527112232,"Table 1: Full recall performance of various student DE models
on NQ dev set, including symmetric DE student model (67.5M
or 11.3M transformer for both encoders), and asymmetric DE
student model (67.5M or 11.3M transformer as query encoder and
document embeddings inherited from the teacher). All distilled
students used the same teacher (110.1M parameter BERT-base
models as both encoders), with the full Recall@5 = 72.3, Re-
call@20 = 86.1, and Recall@100 = 93.6."
CE TO DE DISTILLATION,0.3883984867591425,"Method
6-Layer (67.5M)
4-Layer (11.3M)"
CE TO DE DISTILLATION,0.3896595208070618,"R@5 R@20 R@100
R@5 R@20 R@100"
CE TO DE DISTILLATION,0.39092055485498106,"Train student directly
36.2
59.7
80.0
24.8
44.7
67.5
+ Distill from teacher
65.3
81.6
91.2
44.3
64.9
81.0
+ Inherit doc embeddings
69.9
83.9
92.3
56.3
70.9
82.5
+ Query embedding matching
72.7
86.5
93.9
61.2
75.2
85.1
+ Query generation
73.4
86.3
93.8
64.3
77.8
87.9"
CE TO DE DISTILLATION,0.3921815889029004,"Train student using only
embedding matching and
inherit doc embeddings
71.4
84.9
92.6
64.6
50.2
76.8
+ Query generation
71.8
85.0
93.0
54.2
68.9
80.8"
CE TO DE DISTILLATION,0.39344262295081966,"Table 2: Performance of EmbedDistill for
DE to DE distillation on NQ test set. While
prior works listed in the table rely on tech-
niques such as negative mining and multi-
stage training, we explore the orthogonal
direction of embedding-matching that im-
proves single-stage distillation, which can
be combined with them."
CE TO DE DISTILLATION,0.39470365699873894,"Method
#Layers
R@20
R@100"
CE TO DE DISTILLATION,0.39596469104665827,"DPR [20]
12
78.4
85.4
DPR + PAQ [47]
12
84.0
89.2
DPR + PAQ [47]
24
84.7
89.2
ACNE [60]
12
81.9
87.5
RocketQA [48]
12
82.7
88.5
MSS-DPR [53]
12
84.0
89.2
MSS-DPR [53]
24
84.8
89.8"
CE TO DE DISTILLATION,0.39722572509457754,"Our teacher [63]
12 (220.2M)
85.4
90.0
EmbedDistill
6 (67.5M)
85.1
89.8
EmbedDistill
4 (11.3M)
81.2
87.4"
CE TO DE DISTILLATION,0.39848675914249687,"performs weighted averaging on the query and document segments of the ﬁnal token embeddings.
212"
CE TO DE DISTILLATION,0.39974779319041615,"See Appendix B for details.
213"
CE TO DE DISTILLATION,0.4010088272383354,"In addition to dual pooling, we also utilize a reconstruction loss during the CE training, which
214"
CE TO DE DISTILLATION,0.40226986128625475,"measures the likelihood of predicting each token of the original input from the ﬁnal token embed-
215"
CE TO DE DISTILLATION,0.403530895334174,"dings. This loss encourages reconstruction of query and document tokens based on the ﬁnal token
216"
CE TO DE DISTILLATION,0.4047919293820933,"embeddings and prevents the degeneration of the token embeddings during training. Given proxy
217"
CE TO DE DISTILLATION,0.4060529634300126,"embeddings from the teacher CE, we can perform EmbedDistill with the embedding matching loss
218"
CE TO DE DISTILLATION,0.4073139974779319,"deﬁned in Eq. 7 and Eq. 8 (cf. Fig. 1b).
219"
CE TO DE DISTILLATION,0.4085750315258512,"4.3
Task-speciﬁc online data generation
220"
CE TO DE DISTILLATION,0.4098360655737705,"Data augmentation as a general technique has been previously considered in the IR literature [see, e.g.,
221"
CE TO DE DISTILLATION,0.4110970996216898,"45, 47, 17], especially in data-limited, out-of-domain, or zero-shot settings. As EmbedDistill aims
222"
CE TO DE DISTILLATION,0.4123581336696091,"to align the embeddings spaces of the teacher and student, the ability to generate similar queries or
223"
CE TO DE DISTILLATION,0.4136191677175284,"documents can naturally help enforce such an alignment globally on the task-speciﬁc manifold. Given
224"
CE TO DE DISTILLATION,0.41488020176544765,"a set of unlabeled task-speciﬁc query and document pairs Um, we can further add the embedding
225"
CE TO DE DISTILLATION,0.416141235813367,"matching losses REmb,Q(t, s; Um) or REmb,D(t, s; Um) to our training objective. Interestingly, for
226"
CE TO DE DISTILLATION,0.41740226986128626,"DE to DE distillation setting, our approach can even beneﬁt from a large collection of task-speciﬁc
227"
CE TO DE DISTILLATION,0.41866330390920553,"queries Q0 or documents D0. Here, we can independently employ embedding matching losses
228"
CE TO DE DISTILLATION,0.41992433795712486,"REmb,Q(t, s; Q0) or REmb,D(t, s; D0) that focus on queries and documents, respectively. Please refer
229"
CE TO DE DISTILLATION,0.42118537200504413,"to Appendix E describing how the task-speciﬁc data were generated.
230"
EXPERIMENTS,0.4224464060529634,"5
Experiments
231"
EXPERIMENTS,0.42370744010088274,"We now conduct a comprehensive evaluation of the proposed distillation approach. Speciﬁcally, we
232"
EXPERIMENTS,0.424968474148802,"highlight the utility of the approach for both DE to DE and CE to DE distillation. We also showcase
233"
EXPERIMENTS,0.4262295081967213,"the beneﬁts of combining our distillation approach with query generation methods.
234"
SETUP,0.4274905422446406,"5.1
Setup
235"
SETUP,0.4287515762925599,"Benchmarks and evaluation metrics. We consider two popular IR benchmarks — Natural Questions
236"
SETUP,0.4300126103404792,"(NQ) [24] and MSMARCO [40], which focus on ﬁnding the most relevant passage/document given
237"
SETUP,0.4312736443883985,"a question and a search query, respectively. NQ provides both standard test and dev sets, whereas
238"
SETUP,0.43253467843631777,"MSMARCO provides only the dev set that are widely used for common benchmarks. In what
239"
SETUP,0.4337957124842371,"follows, we use the terms query (document) and question (passages) interchangeably. For NQ, we
240"
SETUP,0.43505674653215637,"use the standard full recall (strict) as well as the relaxed recall metric [20] to evaluate the retrieval
241"
SETUP,0.43631778058007564,"performance. For MSMARCO, we focus on the standard metrics Mean Reciprocal Rank (MRR)@10,
242"
SETUP,0.43757881462799497,"and normalized Discounted Cumulative Gain (nDCG)@10 to evaluate both re-ranking and retrieval
243"
SETUP,0.43883984867591425,"performance. For the re-ranking, we restrict to re-ranking only the top 1000 candidate document
244"
SETUP,0.4401008827238335,"provided as part of the dataset to be fair, while some works use stronger methods to ﬁnd better
245"
SETUP,0.44136191677175285,"top 1000 candidates for re-ranking (resulting in higher evaluation numbers) See Appendix D for a
246"
SETUP,0.4426229508196721,"detailed discussion on these evaluation metrics. Finally, we also evaluate EmbedDistill on the BEIR
247"
SETUP,0.44388398486759145,"benchmark [57] in terms of nDCG@10 and recall@100 metrics.
248"
SETUP,0.4451450189155107,"Model architectures. We follow the standard Transformers-based IR model architectures similar
249"
SETUP,0.44640605296343,"to Karpukhin et al. [20], Qu et al. [48], O˘guz et al. [47]. We utilized various sizes of DE models based
250"
SETUP,0.44766708701134933,"on BERT-base [11] (12-layer, 768 dim, 110M parameters), DistilBERT [55] (6-layer, 768 dim, 67.5M
251"
SETUP,0.4489281210592686,"parameters – ⇠2/3 of base), or BERT-mini [58] (4-layer, 256 dim, 11.3M parameters – ⇠1/10 of
252"
SETUP,0.4501891551071879,"base). For query generation (cf. Sec. 4.3), we employ BART-base [27], an encoder-decoder model, to
253"
SETUP,0.4514501891551072,"generate similar questions from each training example’s input question (query). We randomly mask
254"
SETUP,0.4527112232030265,"10% of tokens and inject zero mean Gaussian noise with σ = {0.1, 0.2} between the encoder and
255"
SETUP,0.45397225725094575,"decoder. See Appendix E for more details on query generation and Appendix F.1 for hyperparameters.
256"
DE TO DE DISTILLATION,0.4552332912988651,"5.2
DE to DE distillation
257"
DE TO DE DISTILLATION,0.45649432534678436,"Table 3: Performance of various DE models on MSMARCO
dev set for both re-ranking and retrieval tasks (full corpus).
The teacher model (110.1M parameter BERT-base models
as both encoders) for re-ranking achieves MRR@10 of 36.8
and that for retrieval get MRR@10 of 37.2. The table shows
performance (in MRR@10) of the symmetric DE student
model (67.5M or 11.3M transformer as both encoders), and
asymmetric DE student model (67.5M or 11.3M transformer
as query encoder and document embeddings inherited from
the teacher)."
DE TO DE DISTILLATION,0.45775535939470363,"Method
Re-ranking
Retrieval"
M,0.45901639344262296,"67.5M
11.3M
67.5M
11.3M"
M,0.46027742749054223,"Train student directly
27.0
23.0
22.6
18.6
+ Distill from teacher
34.6
30.4
35.0
28.6
+ Inherit doc embeddings
35.2
32.1
35.7
30.3
+ Query embedding matching
36.2
35.0
35.4
40.8
+ Query generation
36.2
34.4
37.2
34.8"
M,0.46153846153846156,"Train student using only
embedding matching and
inherit doc embeddings
36.5
33.5
36.6
31.4
+ Query generation
36.4
34.1
36.7
32.8"
M,0.46279949558638084,"We employ AR2 [63]3 and SentenceBERT-
258"
M,0.4640605296343001,"v5 [50]4 as teacher DE models for NQ
259"
M,0.46532156368221944,"and MSMARCO. Note that both models
260"
M,0.4665825977301387,"are based on BERT-base. For DE to DE
261"
M,0.467843631778058,"distillation, we consider two kinds of con-
262"
M,0.4691046658259773,"ﬁgurations for the student DE model: (1)
263"
M,0.4703656998738966,"Symmetric: We use identical question and
264"
M,0.47162673392181587,"document encoders. We evaluate Distil-
265"
M,0.4728877679697352,"BERT and BERT-mini on both datasets. (2)
266"
M,0.47414880201765447,"Asymmetric: The student inherits document
267"
M,0.47540983606557374,"embeddings from the teacher DE model
268"
M,0.4766708701134931,"and are not trained during the distillation.
269"
M,0.47793190416141235,"For query encoder, we use DistilBERT or
270"
M,0.4791929382093317,"BERT-mini which are smaller than docu-
271"
M,0.48045397225725095,"ment encoder.
272"
M,0.4817150063051702,"Student DE model training. We train stu-
273"
M,0.48297604035308955,"dent DE models using a combination of
274"
M,0.4842370744010088,"(i) one-hot loss (cf. Eq. 9 in Appendix A)
275"
M,0.4854981084489281,"on training data; (ii) distillation loss in
276"
M,0.48675914249684743,"(cf. Eq. 11 in Appendix A); and (iii) em-
277"
M,0.4880201765447667,"bedding matching loss in Eq. 7. We used [CLS]-pooling for all student encoders. Unlike DPR [20]
278"
M,0.489281210592686,"or AR2, we do not use hard negatives from BM25 or other models, which greatly simpliﬁes our
279"
M,0.4905422446406053,"distillation procedure.
280"
M,0.4918032786885246,"Results and discussion. To understand the impact of various proposed conﬁgurations and losses, we
281"
M,0.4930643127364439,"train models by sequentially adding components and evaluate their retrieval performance on NQ and
282"
M,0.4943253467843632,"MSMARCO dev set as shown in Table 1 and Table 3 respectively. (See Table 6 in Appendix F.2 for
283"
M,0.49558638083228246,"performance on NQ in terms of the relaxed recall and Table 7 in Appendix F.3 for MSMARCO in
284"
M,0.4968474148802018,"terms of nDCG@10.)
285"
M,0.49810844892812106,"We begin by training a symmetric DE without distillation. As expected, moving to distillation brings
286"
M,0.49936948297604034,"in considerable gains. Next, we swap the student document encoder with document embeddings
287"
M,0.5006305170239597,"from the teacher (non-trainable), which leads to a good jump in the performance. Now we can
288"
M,0.501891551071879,"introduce EmbedDistill with Eq. 7 for aligning query representations between student and teacher.
289"
M,0.5031525851197982,"The two losses are combined with weight of 1.0 (except for BERT-mini models in the presence of
290"
M,0.5044136191677175,"query generation with 5.0). This improves performance signiﬁcantly, e.g.,it provides ⇠3 and ⇠5
291"
M,0.5056746532156369,"points increase in recall@5 on NQ with students based on DistilBERT and BERT-mini, respectively
292"
M,0.5069356872635561,"(Table 1). We further explore the utility of EmbedDistill in aligning the teacher and student embedding
293"
M,0.5081967213114754,"spaces in Appendix G.1.
294"
M,0.5094577553593947,"On top of the two losses (standard distillation and embedding matching), we also use REmb,Q(t, s; Q0)
295"
M,0.510718789407314,"from Sec. 4.3 on 2 additional questions (per input question) generated from BART. We also try a
296"
M,0.5119798234552333,"variant where we eliminate the standard distillation loss and only employ the embedding matching
297"
M,0.5132408575031526,"loss in Eq. 7 along with inheriting teacher’s document embeddings. This conﬁguration without the
298"
M,0.5145018915510718,"standard distillation loss leads to excellent performance (with query generation again providing
299"
M,0.5157629255989912,"additional gains in most cases.)
300"
M,0.5170239596469105,"3https://github.com/microsoft/AR￿/tree/main/AR￿
4https://huggingface.co/sentence-transformers/msmarco-bert-base-dot-v￿"
M,0.5182849936948297,"Table 4: Average BEIR performance of our DE
teacher and EmbedDistill student models and their
numbers of trainable parameters. Both models
are trained on MSMARCO and evaluated on 14
other datasets (the average does not include MS-
MARCO). The full table is at Appendix F.4. With
EmbedDistill, student materializes most of the per-
formance of the teacher on the unforeseen datasets."
M,0.519546027742749,"Method
#Layers
nDCG@10
R@100"
M,0.5208070617906684,"DPR [21]
12
22.5
47.7
ANCE [60]
12
40.5
60.0
TAS-B [15]
6
42.8
64.8
GenQ [57]
6
42.5
64.2"
M,0.5220680958385876,"Our teacher [50]
12 (220.2M)
45.7
65.1
EmbedDistill
6 (67.5M)
44.0
63.5"
M,0.5233291298865069,"It is worth highlighting that DE models trained with
301"
M,0.5245901639344263,"the proposed methods (e.g., asymmetric DE with em-
302"
M,0.5258511979823455,"bedding matching and generation) achieve 99% of
303"
M,0.5271122320302648,"the performance in both NQ/MSMARCO tasks with
304"
M,0.5283732660781841,"a query encoder that is 2/3rd the size of that of the
305"
M,0.5296343001261034,"teacher. Furthermore, even with 1/10th size of the
306"
M,0.5308953341740227,"query encoder, our proposal can achieve 95-97% of
307"
M,0.532156368221942,"the performance. This is particularly useful for la-
308"
M,0.5334174022698613,"tency critical applications with minimal impact on
309"
M,0.5346784363177806,"the ﬁnal performance.
310"
M,0.5359394703656999,"Finally, we take our best student models, i.e., one
311"
M,0.5372005044136192,"trained using with additional embedding matching
312"
M,0.5384615384615384,"loss and using data augmentation from query gen-
313"
M,0.5397225725094578,"eration, and evaluate on test sets. We compare with
314"
M,0.5409836065573771,"various prior work and note that most prior work used
315"
M,0.5422446406052963,"considerably bigger models in terms of parameters,
316"
M,0.5435056746532156,"depth (12 or 24 layers), or width (upto 1024 dims). For NQ test set results are reported in Table 2, but
317"
M,0.544766708701135,"as MSMARCO does not have any public test set, we instead present results for the BEIR benchmark
318"
M,0.5460277427490542,"in Table 4. Note we also provide evaluation of our SentenceBERT teacher achieving very high
319"
M,0.5472887767969735,"performance on the benchmark which can be of independent interest (please refer to Appendix F.4
320"
M,0.5485498108448928,"for details). For both NQ and BEIR, our approach obtains competitive student model with fewer than
321"
M,0.5498108448928121,"50% of the parameters: even with 6 layers, our student model is very close (98-99%) to its teacher.
322"
CE TO DE DISTILLATION,0.5510718789407314,"5.3
CE to DE distillation
323"
CE TO DE DISTILLATION,0.5523329129886507,"Table 5: Performance of DE models distilled from
[CLS]-pooled and Dual-pooled CE models on MS-
MARCO re-ranking task (original top1000 dev).
While both teacher models perform similarly, em-
bedding matching-based distillation only works
with the Dual-pooled teacher. See Appendix F for
nDCG@10 metric."
CE TO DE DISTILLATION,0.5535939470365699,"Method
MRR@10"
CE TO DE DISTILLATION,0.5548549810844893,"[CLS]-pooled teacher
37.1
Dual-pooled teacher
37.0"
CE TO DE DISTILLATION,0.5561160151324086,"Standard distillation from [CLS]-pooled teacher
33.0
+Joint matching
32.4
Standard distillation from Dual-pooled teacher
33.3
+Query matching
33.7"
CE TO DE DISTILLATION,0.5573770491803278,"We consider two CE teachers for MSMARCO re-
324"
CE TO DE DISTILLATION,0.5586380832282472,"ranking task5: a standard [CLS]-pooled CE teacher,
325"
CE TO DE DISTILLATION,0.5598991172761665,"and the Dual-pooled CE teacher (cf. Sec. 4.2). Both
326"
CE TO DE DISTILLATION,0.5611601513240857,"teachers are based on RoBERTa-base and trained on
327"
CE TO DE DISTILLATION,0.562421185372005,"triples in the training set for 300K steps with cross-
328"
CE TO DE DISTILLATION,0.5636822194199244,"entropy loss.
329"
CE TO DE DISTILLATION,0.5649432534678437,"Student DE model training. We considered the fol-
330"
CE TO DE DISTILLATION,0.5662042875157629,"lowing distillation variants: standard score-based dis-
331"
CE TO DE DISTILLATION,0.5674653215636822,"tillation from the [CLS]-pooled teacher, and our novel
332"
CE TO DE DISTILLATION,0.5687263556116016,"Dual-pooled CE teacher (with and without embed-
333"
CE TO DE DISTILLATION,0.5699873896595208,"ding matching loss). For each variant, we initialize en-
334"
CE TO DE DISTILLATION,0.5712484237074401,"coders of the student DE model with two RoBERTa-
335"
CE TO DE DISTILLATION,0.5725094577553594,"base models and train for 500K steps on the train-
336"
CE TO DE DISTILLATION,0.5737704918032787,"ing triples. We performed the naïve joint embedding
337"
CE TO DE DISTILLATION,0.575031525851198,"matching for the [CLS]-pooled teacher (cf. Sec. 4.2) and employed the query embedding matching
338"
CE TO DE DISTILLATION,0.5762925598991173,"(cf. Eq.7) for the Dual-pooled CE teacher. In either case, embedding-matching loss is added on top of
339"
CE TO DE DISTILLATION,0.5775535939470365,"the standard cross entropy loss with the weight of 1.0 (when used).
340"
CE TO DE DISTILLATION,0.5788146279949559,"Results and discussion. Table 5 evaluates the effectiveness of the dual pooling and the embedding
341"
CE TO DE DISTILLATION,0.5800756620428752,"matching for CE to DE distillation. As described in Sec. 4.2, the traditional [CLS]-pooled teacher did
342"
CE TO DE DISTILLATION,0.5813366960907944,"not provide any useful embedding for the embedding matching (see Appendix G.2 for the further
343"
CE TO DE DISTILLATION,0.5825977301387137,"analysis of the resulting embedding space). However, with the Dual-pooled teacher, embedding
344"
CE TO DE DISTILLATION,0.5838587641866331,"matching does boost student’s performance.
345"
RELATED WORK,0.5851197982345523,"6
Related work
346"
RELATED WORK,0.5863808322824716,"Here, we position our EmbedDistill work with respect to prior work on distillation and data augmenta-
347"
RELATED WORK,0.587641866330391,"tion for Transformers-based IR models. We also cover prior efforts on aligning representations during
348"
RELATED WORK,0.5889029003783102,"distillation for non-IR settings. Unlike our problem setting where the DE student is factorized, these
349"
RELATED WORK,0.5901639344262295,"works mainly consider distilling a single large Transformer into a smaller one.
350"
RELATED WORK,0.5914249684741488,"Distillation for IR. Traditional distillation techniques have been widely applied in the IR literature,
351"
RELATED WORK,0.592686002522068,"often to distill a teacher CE model to a student DE model [28, 8]. Recently, distillation from a DE
352"
RELATED WORK,0.5939470365699874,5Note: Full retrieval is prohibitively expensive with CE models.
RELATED WORK,0.5952080706179067,"model (with complex late interaction) to another DE model (with inner-product scoring) has also been
353"
RELATED WORK,0.5964691046658259,"considered [29, 15]. As for distilling across different model architectures, Lu et al. [31], Izacard and
354"
RELATED WORK,0.5977301387137453,"Grave [16] consider distillation from a teacher CE model to a student DE model. Hofstätter et al. [14]
355"
RELATED WORK,0.5989911727616646,"conduct an extensive study of knowledge distillation across a wide-range of model architectures. Most
356"
RELATED WORK,0.6002522068095839,"existing distillation schemes for IR rely on only teacher scores; by contrast, we propose a geometric
357"
RELATED WORK,0.6015132408575031,"approach that also utilizes the teacher embeddings. Many recent efforts [48, 51, 56] show that iterative
358"
RELATED WORK,0.6027742749054225,"multi-stage (self-)distillation improves upon single-stage distillation [48, 51, 56]. These approaches
359"
RELATED WORK,0.6040353089533418,"use a model from the previous stage to obtain labels [56] as well as mine harder-negatives [60]. We
360"
RELATED WORK,0.605296343001261,"only focus on the single-stage distillation in this paper. Multi-stage procedures are complementary to
361"
RELATED WORK,0.6065573770491803,"our work, as one can employ our proposed embedding-matching approach in various stages of such a
362"
RELATED WORK,0.6078184110970997,"procedure. Interestingly, we demonstrate in Sec. 5 that our proposed EmbedDistill can successfully
363"
RELATED WORK,0.6090794451450189,"beneﬁt from high quality models trained with such complex procedures [50, 63]. In particular, our
364"
RELATED WORK,0.6103404791929382,"single-stage distillation method can transfer almost all of their performance gains to even smaller
365"
RELATED WORK,0.6116015132408575,"models. Also to showcase that our method brings gain orthogonal to how teacher was trained, we
366"
RELATED WORK,0.6128625472887768,"conduct experiments with single-stage trained teacher in Appendix F.5.
367"
RELATED WORK,0.6141235813366961,"Distillation with representation alignments. Outside of the IR context, a few prior works proposed
368"
RELATED WORK,0.6153846153846154,"to utilize alignment between hidden layers during distillation [52, 55, 18, 1, 64]. Chen et al. [7] utilize
369"
RELATED WORK,0.6166456494325346,"the representation alignment to re-use teacher’s classiﬁcation layer for image classiﬁcation. Unlike
370"
RELATED WORK,0.617906683480454,"these works, our work is grounded in a rigorous theoretical understanding of the teacher-student
371"
RELATED WORK,0.6191677175283733,"(generalization) gap for IR models. Further, our work differs from these as it needs to address multiple
372"
RELATED WORK,0.6204287515762925,"challenges presented by an IR setting: 1) cross-architecture distillation such as CE to DE distillation;
373"
RELATED WORK,0.6216897856242118,"2) partial representation alignment of query or document representations as opposed to aligning for
374"
RELATED WORK,0.6229508196721312,"the entire input, i.e., a query-documents pair; and 3) catering representation alignment approach to
375"
RELATED WORK,0.6242118537200504,"novel IR setups such as asymmetric DE conﬁguration. To the best of our knowledge, our work is ﬁrst
376"
RELATED WORK,0.6254728877679697,"in the IR literature that goes beyond simply matching scores (or its proxies) for distillation.
377"
RELATED WORK,0.626733921815889,"Semi-supervised learning for IR. Data augmentation or semi-supervised learning has been previ-
378"
RELATED WORK,0.6279949558638083,"ously used to ensure data efﬁciency in IR [see, e.g., 35, 66]. More interestingly, data augmentation
379"
RELATED WORK,0.6292559899117276,"have enabled performance improvements as well. Doc2query [45, 44] performs document expan-
380"
RELATED WORK,0.6305170239596469,"sion by generating queries that are relevant to the document and appending those queries to the
381"
RELATED WORK,0.6317780580075663,"document. Query expansion has also been considered, e.g., for document re-ranking [67]. Notably,
382"
RELATED WORK,0.6330390920554855,"generating synthetic (query, passage, answer) triples from a text corpus to augment existing training
383"
RELATED WORK,0.6343001261034048,"data for QA systems also leads to signiﬁcant gains [2, 47]. Furthermore, even zero-shot approaches,
384"
RELATED WORK,0.6355611601513241,"where no labeled query-document pairs are used, can also perform competitively to supervised
385"
RELATED WORK,0.6368221941992434,"methods [26, 17, 33, 54]. Unlike these works, we utilize query-generation capability to ensure tighter
386"
RELATED WORK,0.6380832282471627,"alignment between the embedding spaces of the teacher and student.
387"
RELATED WORK,0.639344262295082,"Richer transformers-based architectures for IR. Besides DE and CE models (cf. Sec. 2), interme-
388"
RELATED WORK,0.6406052963430012,"diate conﬁgurations [36, 22, 42, 32] have been proposed. Such models independently encode query
389"
RELATED WORK,0.6418663303909206,"and document before applying a more complex late interaction between the two. Nogueira et al.
390"
RELATED WORK,0.6431273644388399,"[46] explore generative encoder-decoder style model for re-ranking. In this paper, we focus on basic
391"
RELATED WORK,0.6443883984867591,"DE/CE models to showcase the beneﬁts of our proposed geometric distillation approach. Exploring
392"
RELATED WORK,0.6456494325346784,"embedding matching for aforementioned architectures is an interesting avenue for future work.
393"
CONCLUSION,0.6469104665825978,"7
Conclusion
394"
CONCLUSION,0.648171500630517,"We propose EmbedDistill — a novel distillation method for IR that goes beyond simple score matching.
395"
CONCLUSION,0.6494325346784363,"En route, we provide a theoretical understanding of the teacher-student generalization gap in an IR
396"
CONCLUSION,0.6506935687263556,"setting which not only motivated EmbedDistill but also inspired new design choices for the student DE
397"
CONCLUSION,0.6519546027742749,"models: (a) reusing the teacher’s document encoder in the student and (b) aligning query embeddings
398"
CONCLUSION,0.6532156368221942,"of the teacher and student. This simple approach delivers consistent quality and computational gains
399"
CONCLUSION,0.6544766708701135,"in practical deployments and we demonstrate them on MSMARCO, NQ, and BEIR benchmarks.
400"
CONCLUSION,0.6557377049180327,"Finally, we found EmbedDistill retains 95-97% of the teacher performance to with 1/10th size students.
401"
CONCLUSION,0.6569987389659521,"Limitations. As discussed in Sec. 4.2 and 5.3, EmbedDistill requires modiﬁcations in the CE scoring
402"
CONCLUSION,0.6582597730138714,"function to be effective. In terms of underlying IR model architectures, we only explore Transformer-
403"
CONCLUSION,0.6595208070617906,"based models in our experiments; primarily due to their widespread utilization. That said, we expect
404"
CONCLUSION,0.6607818411097099,"our results to extend to non-Transformer architectures such as MLPs. Finally, we note that our
405"
CONCLUSION,0.6620428751576293,"experiments only consider NLP domains, and exploring other modalities (e.g., vision) or multi-modal
406"
CONCLUSION,0.6633039092055486,"settings (e.g., image-to-text search) is left as an interesting avenue for future work.
407"
REFERENCES,0.6645649432534678,"References
408"
REFERENCES,0.6658259773013872,"[1] Gustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao, Xing Fan, and Chenlei Guo. Knowledge
409"
REFERENCES,0.6670870113493065,"distillation from internal representations. In Proceedings of the AAAI Conference on Artiﬁcial
410"
REFERENCES,0.6683480453972257,"Intelligence, volume 34, pages 7350–7357, 2020.
411"
REFERENCES,0.669609079445145,"[2] Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, and Michael Collins. Synthetic
412"
REFERENCES,0.6708701134930644,"QA corpora generation with roundtrip consistency.
In Proceedings of the 57th Annual
413"
REFERENCES,0.6721311475409836,"Meeting of the Association for Computational Linguistics, pages 6168–6173, Florence, Italy,
414"
REFERENCES,0.6733921815889029,"July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1620. URL
415"
REFERENCES,0.6746532156368222,"https://aclanthology.org/P￿￿-￿￿￿￿.
416"
REFERENCES,0.6759142496847415,"[3] Yoshua Bengio and Jean-SÉbastien Senecal. Adaptive importance sampling to accelerate
417"
REFERENCES,0.6771752837326608,"training of a neural probabilistic language model. IEEE Transactions on Neural Networks, 19
418"
REFERENCES,0.6784363177805801,"(4):713–722, 2008. doi: 10.1109/TNN.2007.912312.
419"
REFERENCES,0.6796973518284993,"[4] Olivier Bousquet, Stéphane Boucheron, and Gábor Lugosi. Introduction to Statistical Learn-
420"
REFERENCES,0.6809583858764187,"ing Theory, pages 169–207. Springer Berlin Heidelberg, Berlin, Heidelberg, 2004. ISBN
421"
REFERENCES,0.682219419924338,"978-3-540-28650-9. doi: 10.1007/978-3-540-28650-9_8. URL https://doi.org/￿￿.￿￿￿￿/
422"
REFERENCES,0.6834804539722572,"￿￿￿-￿-￿￿￿-￿￿￿￿￿-￿_￿.
423"
REFERENCES,0.6847414880201765,"[5] Cristian Bucilˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In
424"
REFERENCES,0.6860025220680959,"Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and
425"
REFERENCES,0.6872635561160151,"Data Mining, KDD ’06, pages 535–541, New York, NY, USA, 2006. ACM.
426"
REFERENCES,0.6885245901639344,"[6] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to answer
427"
REFERENCES,0.6897856242118537,"open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for
428"
REFERENCES,0.691046658259773,"Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada,
429"
REFERENCES,0.6923076923076923,"July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL
430"
REFERENCES,0.6935687263556116,"https://aclanthology.org/P￿￿-￿￿￿￿.
431"
REFERENCES,0.694829760403531,"[7] Defang Chen, Jian-Ping Mei, Hailin Zhang, Can Wang, Yan Feng, and Chun Chen. Knowledge
432"
REFERENCES,0.6960907944514502,"distillation with the reused teacher classiﬁer. In Proceedings of the IEEE/CVF Conference on
433"
REFERENCES,0.6973518284993695,"Computer Vision and Pattern Recognition, pages 11933–11942, 2022.
434"
REFERENCES,0.6986128625472888,"[8] Xuanang Chen, Ben He, Kai Hui, Le Sun, and Yingfei Sun. Simpliﬁed tinybert: Knowledge
435"
REFERENCES,0.699873896595208,"distillation for document retrieval. In Djoerd Hiemstra, Marie-Francine Moens, Josiane Mothe,
436"
REFERENCES,0.7011349306431274,"Raffaele Perego, Martin Potthast, and Fabrizio Sebastiani, editors, Advances in Information
437"
REFERENCES,0.7023959646910467,"Retrieval, pages 241–248, Cham, 2021. Springer International Publishing. ISBN 978-3-030-
438"
REFERENCES,0.7036569987389659,"72240-1.
439"
REFERENCES,0.7049180327868853,"[9] Zhuyun Dai and Jamie Callan. Deeper text understanding for IR with contextual neural language
440"
REFERENCES,0.7061790668348046,"modeling. In Benjamin Piwowarski, Max Chevalier, Éric Gaussier, Yoelle Maarek, Jian-Yun
441"
REFERENCES,0.7074401008827238,"Nie, and Falk Scholer, editors, Proceedings of the 42nd International ACM SIGIR Conference
442"
REFERENCES,0.7087011349306431,"on Research and Development in Information Retrieval, SIGIR 2019, Paris, France, July 21-25,
443"
REFERENCES,0.7099621689785625,"2019, pages 985–988. ACM, 2019.
444"
REFERENCES,0.7112232030264817,"[10] Zhuyun Dai and Jamie Callan. Context-aware sentence/passage term importance estimation for
445"
REFERENCES,0.712484237074401,"ﬁrst stage retrieval. arXiv preprint arXiv:1910.10687, 2019.
446"
REFERENCES,0.7137452711223203,"[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of
447"
REFERENCES,0.7150063051702396,"deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and
448"
REFERENCES,0.7162673392181589,"Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter
449"
REFERENCES,0.7175283732660782,"of the Association for Computational Linguistics: Human Language Technologies, NAACL-
450"
REFERENCES,0.7187894073139974,"HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages
451"
REFERENCES,0.7200504413619168,"4171–4186. Association for Computational Linguistics, 2019.
452"
REFERENCES,0.7213114754098361,"[12] Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv
453"
REFERENCES,0.7225725094577553,"Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International
454"
REFERENCES,0.7238335435056746,"Conference on Machine Learning, 2020. URL https://arxiv.org/abs/￿￿￿￿.￿￿￿￿￿.
455"
REFERENCES,0.725094577553594,"[13] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network,
456"
REFERENCES,0.7263556116015133,"2015.
457"
REFERENCES,0.7276166456494325,"[14] Sebastian Hofstätter, Sophia Althammer, Michael Schröder, Mete Sertkan, and Allan Hanbury.
458"
REFERENCES,0.7288776796973518,"Improving efﬁcient neural ranking models with cross-architecture knowledge distillation. CoRR,
459"
REFERENCES,0.7301387137452712,"abs/2010.02666, 2020. URL https://arxiv.org/abs/￿￿￿￿.￿￿￿￿￿.
460"
REFERENCES,0.7313997477931904,"[15] Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury.
461"
REFERENCES,0.7326607818411097,"Efﬁciently teaching an effective dense retriever with balanced topic aware sampling. In Pro-
462"
REFERENCES,0.733921815889029,"ceedings of the 44th International ACM SIGIR Conference on Research and Development in
463"
REFERENCES,0.7351828499369483,"Information Retrieval, SIGIR ’21, page 113–122, New York, NY, USA, 2021. Association
464"
REFERENCES,0.7364438839848676,"for Computing Machinery. ISBN 9781450380379. doi: 10.1145/3404835.3462891. URL
465"
REFERENCES,0.7377049180327869,"https://doi.org/￿￿.￿￿￿￿/￿￿￿￿￿￿￿.￿￿￿￿￿￿￿.
466"
REFERENCES,0.7389659520807061,"[16] Gautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question
467"
REFERENCES,0.7402269861286255,"answering. In International Conference on Learning Representations, 2021. URL https:
468"
REFERENCES,0.7414880201765448,"//openreview.net/forum?id￿NTEz-￿wysdb.
469"
REFERENCES,0.742749054224464,"[17] Gautier Izacard, Mathild Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand
470"
REFERENCES,0.7440100882723834,"Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning.
471"
REFERENCES,0.7452711223203027,"arXiv preprint arXiv:2112.09118, 2021.
472"
REFERENCES,0.7465321563682219,"[18] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and
473"
REFERENCES,0.7477931904161412,"Qun Liu. TinyBERT: Distilling BERT for natural language understanding. In Findings of the
474"
REFERENCES,0.7490542244640606,"Association for Computational Linguistics: EMNLP 2020, pages 4163–4174, Online, November
475"
REFERENCES,0.7503152585119798,"2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.372.
476"
REFERENCES,0.7515762925598991,"URL https://aclanthology.org/￿￿￿￿.findings-emnlp.￿￿￿.
477"
REFERENCES,0.7528373266078184,"[19] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE
478"
REFERENCES,0.7540983606557377,"Transactions on Big Data, 7(3):535–547, 2021. doi: 10.1109/TBDATA.2019.2921572.
479"
REFERENCES,0.755359394703657,"[20] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov,
480"
REFERENCES,0.7566204287515763,"Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering.
481"
REFERENCES,0.7578814627994955,"In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-
482"
REFERENCES,0.7591424968474149,"ing (EMNLP), pages 6769–6781, Online, November 2020. Association for Computational
483"
REFERENCES,0.7604035308953342,"Linguistics.
484"
REFERENCES,0.7616645649432535,"[21] Vladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov,
485"
REFERENCES,0.7629255989911727,"Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering.
486"
REFERENCES,0.7641866330390921,"arXiv preprint arXiv:2004.04906, 2020.
487"
REFERENCES,0.7654476670870114,"[22] Omar Khattab and Matei Zaharia.
ColBERT: Efﬁcient and Effective Passage Search via
488"
REFERENCES,0.7667087011349306,"Contextualized Late Interaction over BERT, page 39–48. Association for Computing Machinery,
489"
REFERENCES,0.7679697351828499,"New York, NY, USA, 2020. ISBN 9781450380164.
490"
REFERENCES,0.7692307692307693,"[23] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris
491"
REFERENCES,0.7704918032786885,"Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion
492"
REFERENCES,0.7717528373266078,"Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav
493"
REFERENCES,0.7730138713745272,"Petrov. Natural questions: A benchmark for question answering research. Transactions of the
494"
REFERENCES,0.7742749054224464,"Association for Computational Linguistics, 7:452–466, 2019. doi: 10.1162/tacl_a_00276. URL
495"
REFERENCES,0.7755359394703657,"https://aclanthology.org/Q￿￿-￿￿￿￿.
496"
REFERENCES,0.776796973518285,"[24] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris
497"
REFERENCES,0.7780580075662042,"Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a
498"
REFERENCES,0.7793190416141236,"benchmark for question answering research. Transactions of the Association for Computational
499"
REFERENCES,0.7805800756620429,"Linguistics, 7:453–466, 2019.
500"
REFERENCES,0.7818411097099621,"[25] Michel Ledoux and Michel Talagrand. Probability in Banach spaces. Springer-Verlag, 1991.
501"
REFERENCES,0.7831021437578815,"[26] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised
502"
REFERENCES,0.7843631778058008,"open domain question answering. In Anna Korhonen, David R. Traum, and Lluís Màrquez,
503"
REFERENCES,0.78562421185372,"editors, Proceedings of the 57th Conference of the Association for Computational Linguistics,
504"
REFERENCES,0.7868852459016393,"ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 6086–6096.
505"
REFERENCES,0.7881462799495587,"Association for Computational Linguistics, 2019.
506"
REFERENCES,0.7894073139974779,"[27] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
507"
REFERENCES,0.7906683480453972,"Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-
508"
REFERENCES,0.7919293820933165,"training for natural language generation, translation, and comprehension. In Proceedings of
509"
REFERENCES,0.7931904161412359,"the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880,
510"
REFERENCES,0.7944514501891551,"Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.
511"
REFERENCES,0.7957124842370744,"703. URL https://aclanthology.org/￿￿￿￿.acl-main.￿￿￿.
512"
REFERENCES,0.7969735182849937,"[28] Canjia Li, Andrew Yates, Sean MacAvaney, Ben He, and Yingfei Sun. Parade: Passage repre-
513"
REFERENCES,0.798234552332913,"sentation aggregation for document reranking. arXiv preprint arXiv:2008.09093, 2020.
514"
REFERENCES,0.7994955863808323,"[29] Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. In-batch negatives for knowledge dis-
515"
REFERENCES,0.8007566204287516,"tillation with tightly-coupled teachers for dense retrieval. In Proceedings of the 6th Work-
516"
REFERENCES,0.8020176544766708,"shop on Representation Learning for NLP (RepL4NLP-2021), pages 163–173, Online, August
517"
REFERENCES,0.8032786885245902,"2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.repl4nlp-1.17. URL
518"
REFERENCES,0.8045397225725095,"https://aclanthology.org/￿￿￿￿.repl￿nlp-￿.￿￿.
519"
REFERENCES,0.8058007566204287,"[30] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
520"
REFERENCES,0.807061790668348,"Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
521"
REFERENCES,0.8083228247162674,"approach. arXiv preprint arXiv:1907.11692, 2019.
522"
REFERENCES,0.8095838587641866,"[31] Wenhao Lu, Jian Jiao, and Ruofei Zhang. Twinbert: Distilling knowledge to twin-structured
523"
REFERENCES,0.8108448928121059,"compressed bert models for large-scale retrieval. In Proceedings of the 29th ACM International
524"
REFERENCES,0.8121059268600253,"Conference on Information & Knowledge Management, CIKM ’20, page 2645–2652, New
525"
REFERENCES,0.8133669609079445,"York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450368599. doi:
526"
REFERENCES,0.8146279949558638,"10.1145/3340531.3412747. URL https://doi.org/￿￿.￿￿￿￿/￿￿￿￿￿￿￿.￿￿￿￿￿￿￿.
527"
REFERENCES,0.8158890290037831,"[32] Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. Sparse, dense, and
528"
REFERENCES,0.8171500630517023,"attentional representations for text retrieval. Transactions of the Association for Computational
529"
REFERENCES,0.8184110970996217,"Linguistics, 9:329–345, 2021. doi: 10.1162/tacl_a_00369. URL https://aclanthology.org/
530"
REFERENCES,0.819672131147541,"￿￿￿￿.tacl-￿.￿￿.
531"
REFERENCES,0.8209331651954602,"[33] Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. Zero-shot neural pas-
532"
REFERENCES,0.8221941992433796,"sage retrieval via domain-targeted synthetic question generation. In Proceedings of the 16th
533"
REFERENCES,0.8234552332912989,"Conference of the European Chapter of the Association for Computational Linguistics: Main
534"
REFERENCES,0.8247162673392182,"Volume, pages 1075–1088, Online, April 2021. Association for Computational Linguistics. doi:
535"
REFERENCES,0.8259773013871374,"10.18653/v1/2021.eacl-main.92. URL https://aclanthology.org/￿￿￿￿.eacl-main.￿￿.
536"
REFERENCES,0.8272383354350568,"[34] Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. CEDR: Contextualized
537"
REFERENCES,0.8284993694829761,"embeddings for document ranking. In Proceedings of the 42nd International ACM SIGIR
538"
REFERENCES,0.8297604035308953,"Conference on Research and Development in Information Retrieval, SIGIR’19, page 1101–1104,
539"
REFERENCES,0.8310214375788146,"New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450361729. doi:
540"
REFERENCES,0.832282471626734,"10.1145/3331184.3331317. URL https://doi.org/￿￿.￿￿￿￿/￿￿￿￿￿￿￿.￿￿￿￿￿￿￿.
541"
REFERENCES,0.8335435056746532,"[35] Sean MacAvaney, Andrew Yates, Kai Hui, and Ophir Frieder. Content-based weak supervision
542"
REFERENCES,0.8348045397225725,"for ad-hoc re-ranking. In Proceedings of the 42nd International ACM SIGIR Conference on
543"
REFERENCES,0.8360655737704918,"Research and Development in Information Retrieval, SIGIR’19, page 993–996, New York, NY,
544"
REFERENCES,0.8373266078184111,"USA, 2019. Association for Computing Machinery. ISBN 9781450361729. doi: 10.1145/
545"
REFERENCES,0.8385876418663304,"3331184.3331316. URL https://doi.org/￿￿.￿￿￿￿/￿￿￿￿￿￿￿.￿￿￿￿￿￿￿.
546"
REFERENCES,0.8398486759142497,"[36] Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli Goharian,
547"
REFERENCES,0.8411097099621689,"and Ophir Frieder. Efﬁcient Document Re-Ranking for Transformers by Precomputing Term
548"
REFERENCES,0.8423707440100883,"Representations, page 49–58. Association for Computing Machinery, New York, NY, USA,
549"
REFERENCES,0.8436317780580076,"2020. ISBN 9781450380164.
550"
REFERENCES,0.8448928121059268,"[37] Aditya Menon, Sadeep Jayasumana, Ankit Singh Rawat, Seungyeon Kim, Sashank Reddi,
551"
REFERENCES,0.8461538461538461,"and Sanjiv Kumar. In defense of dual-encoders for neural ranking. In Kamalika Chaudhuri,
552"
REFERENCES,0.8474148802017655,"Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings
553"
REFERENCES,0.8486759142496847,"of the 39th International Conference on Machine Learning, volume 162 of Proceedings of
554"
REFERENCES,0.849936948297604,"Machine Learning Research, pages 15376–15400. PMLR, 17–23 Jul 2022.
URL https:
555"
REFERENCES,0.8511979823455234,"//proceedings.mlr.press/v￿￿￿/menon￿￿a.html.
556"
REFERENCES,0.8524590163934426,"[38] Bhaskar Mitra and Nick Craswell. An introduction to neural information retrieval. Foundations
557"
REFERENCES,0.8537200504413619,"and Trends® in Information Retrieval, 13(1):1–126, 2018. ISSN 1554-0669. doi: 10.1561/
558"
REFERENCES,0.8549810844892812,"1500000061. URL http://dx.doi.org/￿￿.￿￿￿￿/￿￿￿￿￿￿￿￿￿￿.
559"
REFERENCES,0.8562421185372006,"[39] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek,
560"
REFERENCES,0.8575031525851198,"Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. Text and code embeddings
561"
REFERENCES,0.8587641866330391,"by contrastive pre-training. arXiv preprint arXiv:2201.10005, 2022.
562"
REFERENCES,0.8600252206809584,"[40] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,
563"
REFERENCES,0.8612862547288777,"and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In
564"
REFERENCES,0.862547288776797,"Tarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors,
565"
REFERENCES,0.8638083228247163,"Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic
566"
REFERENCES,0.8650693568726355,"approaches 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016.
567"
REFERENCES,0.8663303909205549,"[41] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao,
568"
REFERENCES,0.8675914249684742,"Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable
569"
REFERENCES,0.8688524590163934,"retrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language
570"
REFERENCES,0.8701134930643127,"Processing, pages 9844–9855, Abu Dhabi, United Arab Emirates, December 2022. Association
571"
REFERENCES,0.8713745271122321,"for Computational Linguistics. URL https://aclanthology.org/￿￿￿￿.emnlp-main.￿￿￿.
572"
REFERENCES,0.8726355611601513,"[42] Ping Nie, Yuyu Zhang, Xiubo Geng, Arun Ramamurthy, Le Song, and Daxin Jiang. DC-BERT:
573"
REFERENCES,0.8738965952080706,"decoupling question and document for efﬁcient contextual encoding. In Jimmy Huang, Yi Chang,
574"
REFERENCES,0.8751576292559899,"Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu, editors, Proceedings
575"
REFERENCES,0.8764186633039092,"of the 43rd International ACM SIGIR conference on research and development in Information
576"
REFERENCES,0.8776796973518285,"Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, pages 1829–1832. ACM, 2020.
577"
REFERENCES,0.8789407313997478,"doi: 10.1145/3397271.3401271. URL https://doi.org/￿￿.￿￿￿￿/￿￿￿￿￿￿￿.￿￿￿￿￿￿￿.
578"
REFERENCES,0.880201765447667,"[43] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. CoRR, abs/1901.04085,
579"
REFERENCES,0.8814627994955864,"2019. URL http://arxiv.org/abs/￿￿￿￿.￿￿￿￿￿.
580"
REFERENCES,0.8827238335435057,"[44] Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. From doc2query to doctttttquery. Online
581"
REFERENCES,0.8839848675914249,"preprint, 6, 2019.
582"
REFERENCES,0.8852459016393442,"[45] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. Document expansion by query
583"
REFERENCES,0.8865069356872636,"prediction. arXiv preprint arXiv:1904.08375, 2019.
584"
REFERENCES,0.8877679697351829,"[46] Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin.
Document ranking
585"
REFERENCES,0.8890290037831021,"with a pretrained sequence-to-sequence model.
In Findings of the Association for Com-
586"
REFERENCES,0.8902900378310215,"putational Linguistics: EMNLP 2020, pages 708–718, Online, November 2020. Associa-
587"
REFERENCES,0.8915510718789408,"tion for Computational Linguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.63. URL https:
588"
REFERENCES,0.89281210592686,"//aclanthology.org/￿￿￿￿.findings-emnlp.￿￿.
589"
REFERENCES,0.8940731399747793,"[47] Barlas O˘guz, Kushal Lakhotia, Anchit Gupta, Patrick Lewis, Vladimir Karpukhin, Aleksandra
590"
REFERENCES,0.8953341740226987,"Piktus, Xilun Chen, Sebastian Riedel, Wen-tau Yih, Sonal Gupta, et al. Domain-matched
591"
REFERENCES,0.8965952080706179,"pre-training tasks for dense retrieval. arXiv preprint arXiv:2107.13602, 2021.
592"
REFERENCES,0.8978562421185372,"[48] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua
593"
REFERENCES,0.8991172761664565,"Wu, and Haifeng Wang. RocketQA: An optimized training approach to dense passage retrieval
594"
REFERENCES,0.9003783102143758,"for open-domain question answering. In Kristina Toutanova, Anna Rumshisky, Luke Zettle-
595"
REFERENCES,0.9016393442622951,"moyer, Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty,
596"
REFERENCES,0.9029003783102144,"and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter
597"
REFERENCES,0.9041614123581336,"of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT
598"
REFERENCES,0.905422446406053,"2021, Online, June 6-11, 2021, pages 5835–5847. Association for Computational Linguistics,
599"
REFERENCES,0.9066834804539723,"2021.
600"
REFERENCES,0.9079445145018915,"[49] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
601"
REFERENCES,0.9092055485498108,"Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed
602"
REFERENCES,0.9104665825977302,"text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL
603"
REFERENCES,0.9117276166456494,"http://jmlr.org/papers/v￿￿/￿￿-￿￿￿.html.
604"
REFERENCES,0.9129886506935687,"[50] Nils Reimers, Iryna Gurevych, and Iryna Gurevych. Sentence-BERT: Sentence embeddings
605"
REFERENCES,0.914249684741488,"using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods
606"
REFERENCES,0.9155107187894073,"in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL
607"
REFERENCES,0.9167717528373266,"http://arxiv.org/abs/￿￿￿￿.￿￿￿￿￿.
608"
REFERENCES,0.9180327868852459,"[51] Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu, Haifeng Wang,
609"
REFERENCES,0.9192938209331651,"and Ji-Rong Wen. Rocketqav2: A joint training method for dense passage retrieval and passage
610"
REFERENCES,0.9205548549810845,"re-ranking. In Proceedings of EMNLP, 2021.
611"
REFERENCES,0.9218158890290038,"[52] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
612"
REFERENCES,0.9230769230769231,"Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.
613"
REFERENCES,0.9243379571248423,"[53] Devendra Sachan, Mostofa Patwary, Mohammad Shoeybi, Neel Kant, Wei Ping, William L.
614"
REFERENCES,0.9255989911727617,"Hamilton, and Bryan Catanzaro. End-to-end training of neural retrievers for open-domain
615"
REFERENCES,0.926860025220681,"question answering.
In Proceedings of the 59th Annual Meeting of the Association for
616"
REFERENCES,0.9281210592686002,"Computational Linguistics and the 11th International Joint Conference on Natural Lan-
617"
REFERENCES,0.9293820933165196,"guage Processing (Volume 1: Long Papers), pages 6648–6662, Online, August 2021. As-
618"
REFERENCES,0.9306431273644389,"sociation for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.519. URL https:
619"
REFERENCES,0.9319041614123581,"//aclanthology.org/￿￿￿￿.acl-long.￿￿￿.
620"
REFERENCES,0.9331651954602774,"[54] Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle
621"
REFERENCES,0.9344262295081968,"Pineau, and Luke Zettlemoyer. Improving passage retrieval with zero-shot question generation.
622"
REFERENCES,0.935687263556116,"arXiv preprint arXiv:2204.07496, 2022.
623"
REFERENCES,0.9369482976040353,"[55] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version
624"
REFERENCES,0.9382093316519546,"of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.
625"
REFERENCES,0.9394703656998739,"[56] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. Col-
626"
REFERENCES,0.9407313997477932,"bertv2: Effective and efﬁcient retrieval via lightweight late interaction. CoRR, abs/2112.01488,
627"
REFERENCES,0.9419924337957125,"2021.
628"
REFERENCES,0.9432534678436317,"[57] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych.
629"
REFERENCES,0.9445145018915511,"BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In
630"
REFERENCES,0.9457755359394704,"Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks
631"
REFERENCES,0.9470365699873896,"Track (Round 2), 2021. URL https://openreview.net/forum?id￿wCu￿T￿xFjeJ.
632"
REFERENCES,0.9482976040353089,"[58] Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn
633"
REFERENCES,0.9495586380832283,"better: On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962,
634"
REFERENCES,0.9508196721311475,"2019.
635"
REFERENCES,0.9520807061790668,"[59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
636"
REFERENCES,0.9533417402269861,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st
637"
REFERENCES,0.9546027742749055,"International Conference on Neural Information Processing Systems, NIPS’17, page 6000–6010,
638"
REFERENCES,0.9558638083228247,"Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.
639"
REFERENCES,0.957124842370744,"[60] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid
640"
REFERENCES,0.9583858764186634,"Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning
641"
REFERENCES,0.9596469104665826,"for dense text retrieval. In International Conference on Learning Representations, 2021. URL
642"
REFERENCES,0.9609079445145019,"https://openreview.net/forum?id￿zeFrfgyZln.
643"
REFERENCES,0.9621689785624212,"[61] Nishant Yadav, Nicholas Monath, Rico Angell, Manzil Zaheer, and Andrew McCallum. Efﬁcient
644"
REFERENCES,0.9634300126103404,"nearest neighbor search for cross-encoder models using matrix factorization. In Proceedings of
645"
REFERENCES,0.9646910466582598,"the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2171–2194,
646"
REFERENCES,0.9659520807061791,"Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
647"
REFERENCES,0.9672131147540983,"URL https://aclanthology.org/￿￿￿￿.emnlp-main.￿￿￿.
648"
REFERENCES,0.9684741488020177,"[62] Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, and Jimmy Lin. Cross-domain modeling
649"
REFERENCES,0.969735182849937,"of sentence-level evidence for document retrieval. In Proceedings of the 2019 Conference on
650"
REFERENCES,0.9709962168978562,"Empirical Methods in Natural Language Processing and the 9th International Joint Conference
651"
REFERENCES,0.9722572509457755,"on Natural Language Processing (EMNLP-IJCNLP), pages 3490–3496, Hong Kong, China,
652"
REFERENCES,0.9735182849936949,"November 2019. Association for Computational Linguistics.
653"
REFERENCES,0.9747793190416141,"[63] Hang Zhang, Yeyun Gong, Yelong Shen, Jiancheng Lv, Nan Duan, and Weizhu Chen. Ad-
654"
REFERENCES,0.9760403530895334,"versarial retriever-ranker for dense text retrieval. In International Conference on Learning
655"
REFERENCES,0.9773013871374527,"Representations, 2022. URL https://openreview.net/forum?id￿MR￿XubKUFB.
656"
REFERENCES,0.978562421185372,"[64] Linfeng Zhang and Kaisheng Ma. Improve object detection with feature-based knowledge
657"
REFERENCES,0.9798234552332913,"distillation: Towards accurate and efﬁcient detectors. In International Conference on Learning
658"
REFERENCES,0.9810844892812106,"Representations, 2020.
659"
REFERENCES,0.9823455233291298,"[65] Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. Deep learning based recommender system:
660"
REFERENCES,0.9836065573770492,"A survey and new perspectives. ACM Comput. Surv., 52(1), feb 2019. ISSN 0360-0300. doi:
661"
REFERENCES,0.9848675914249685,"10.1145/3285029. URL https://doi.org/￿￿.￿￿￿￿/￿￿￿￿￿￿￿.
662"
REFERENCES,0.9861286254728878,"[66] Chen Zhao, Chenyan Xiong, Jordan Boyd-Graber, and Hal Daumé III. Distantly-supervised
663"
REFERENCES,0.987389659520807,"dense retrieval enables open-domain question answering without evidence annotation. In
664"
REFERENCES,0.9886506935687264,"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-
665"
REFERENCES,0.9899117276166457,"ing, pages 9612–9622, Online and Punta Cana, Dominican Republic, November 2021. As-
666"
REFERENCES,0.9911727616645649,"sociation for Computational Linguistics.
doi: 10.18653/v1/2021.emnlp-main.756.
URL
667"
REFERENCES,0.9924337957124842,"https://aclanthology.org/￿￿￿￿.emnlp-main.￿￿￿.
668"
REFERENCES,0.9936948297604036,"[67] Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, and Andrew Yates. BERT-QE: Con-
669"
REFERENCES,0.9949558638083228,"textualized Query Expansion for Document Re-ranking. In Findings of the Association for
670"
REFERENCES,0.9962168978562421,"Computational Linguistics: EMNLP 2020, pages 4718–4728, Online, November 2020. As-
671"
REFERENCES,0.9974779319041615,"sociation for Computational Linguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.424. URL
672"
REFERENCES,0.9987389659520807,"https://aclanthology.org/￿￿￿￿.findings-emnlp.￿￿￿.
673"
