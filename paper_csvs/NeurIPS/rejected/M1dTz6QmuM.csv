Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001607717041800643,"Reinforcement learning (RL) algorithms have proven transformative in a range of
1"
ABSTRACT,0.003215434083601286,"domains. To tackle real-world domains, these systems often use neural networks
2"
ABSTRACT,0.00482315112540193,"to learn policies directly from pixels or other high-dimensional sensory input. By
3"
ABSTRACT,0.006430868167202572,"contrast, much theory of RL has focused on discrete state spaces or worst-case
4"
ABSTRACT,0.008038585209003215,"analysis, and fundamental questions remain about the dynamics of policy learning
5"
ABSTRACT,0.00964630225080386,"in high-dimensional settings. Here, we propose a solvable high-dimensional model
6"
ABSTRACT,0.011254019292604502,"of RL that can capture a variety of learning protocols, and derive its typical
7"
ABSTRACT,0.012861736334405145,"dynamics as a set of closed-form ordinary differential equations (ODEs). We derive
8"
ABSTRACT,0.014469453376205787,"optimal schedules for the learning rates and task difficulty—analogous to annealing
9"
ABSTRACT,0.01607717041800643,"schemes and curricula during training in RL—and show that the model exhibits rich
10"
ABSTRACT,0.017684887459807074,"behaviour, including delayed learning under sparse rewards; a variety of learning
11"
ABSTRACT,0.01929260450160772,"regimes depending on reward baselines; and a speed-accuracy trade-off driven by
12"
ABSTRACT,0.02090032154340836,"reward stringency. Experiments on a variant of the Procgen game “Bossfight” also
13"
ABSTRACT,0.022508038585209004,"show such a speed-accuracy trade-off in practice. Together, these results take a
14"
ABSTRACT,0.024115755627009645,"step towards closing the gap between theory and practice in high-dimensional RL.
15"
ABSTRACT,0.02572347266881029,"Recent years have seen rapid progress in Reinforcement Learning (RL): algorithmic and engineering
16"
ABSTRACT,0.027331189710610933,"breakthroughs led to super-human performance in a variety of domains, for example complex games
17"
ABSTRACT,0.028938906752411574,"like Go [Silver et al., 2016, Mnih et al., 2015]. Despite these practical successes, our theoretical
18"
ABSTRACT,0.03054662379421222,"understanding of RL for high-dimensional problems requiring non-linear function approximation
19"
ABSTRACT,0.03215434083601286,"is still limited. While comprehensive theoretical results exist for tabular RL, where the state and
20"
ABSTRACT,0.03376205787781351,"action spaces are discrete and small enough for value functions to be represented directly, the curse
21"
ABSTRACT,0.03536977491961415,"of dimensionality limits these methods to low-dimensional problems. The lack of a clear notion of
22"
ABSTRACT,0.03697749196141479,"similarity between discrete states further means that tabular methods do not address the core question
23"
ABSTRACT,0.03858520900321544,"of generalisation: how are values and policies extended to unseen states and across seen states [Kirk
24"
ABSTRACT,0.04019292604501608,"et al., 2023]? As a consequence, much of this theoretical work is far from the current practice of RL,
25"
ABSTRACT,0.04180064308681672,"which increasingly relies on deep neural networks to approximate and generalise value functions,
26"
ABSTRACT,0.04340836012861737,"policies and other building blocks of RL. Moreover, while RL theory has often addressed “worst-case”
27"
ABSTRACT,0.04501607717041801,"performance and convergence behaviour, the typical behaviour has received comparatively little
28"
ABSTRACT,0.04662379421221865,"attention (cf. further related work below). Meanwhile, a growing sub-field of deep learning theory
29"
ABSTRACT,0.04823151125401929,"has employed tools from statistical mechanics to analyse various supervised learning paradigms
30"
ABSTRACT,0.04983922829581994,"in the average-case, see Seung et al. [1992], Engel and Van den Broeck [2001], Carleo et al. [2019],
31"
ABSTRACT,0.05144694533762058,"Bahri et al. [2020], Gabrié et al. [2023] for classical and recent reviews. While this approach has
32"
ABSTRACT,0.05305466237942122,"recently been extended to curriculum learning [Saglietti et al., 2022], continual learning [Asanuma
33"
ABSTRACT,0.05466237942122187,"et al., 2021, Lee et al., 2021, 2022], few-shot learning [Sorscher et al., 2022] and transfer learning
34"
ABSTRACT,0.05627009646302251,"[Lampinen and Ganguli, 2018, Dhifallah and Lu, 2021, Gerace et al., 2022], RL has not been
35"
ABSTRACT,0.05787781350482315,"analysed yet using statistical mechanics—a gap we address here by studying the high-dimensional
36"
ABSTRACT,0.0594855305466238,"generalisation dynamics of a simple neural network trained on a reinforcement learning task.
37 (a)"
ABSTRACT,0.06109324758842444,"Input
x"
ABSTRACT,0.06270096463022508,"Teacher
w∗"
ABSTRACT,0.06430868167202572,"Student
w y∗ y"
ABSTRACT,0.06591639871382636,"L(y, y∗)"
ABSTRACT,0.06752411575562701,"y∗
1
y∗
2
y∗
3
y∗
T"
ABSTRACT,0.06913183279742766,"y1
y2
y3
yT"
ABSTRACT,0.0707395498392283,"Φ({y}, {y∗})
x1
x2
x3
xT"
ABSTRACT,0.07234726688102894,"w∗
w∗
w∗
w∗"
ABSTRACT,0.07395498392282958,"w
w
w
w"
ABSTRACT,0.07556270096463022,"s1
s2
s3
sT"
ABSTRACT,0.07717041800643087,"(b)
(c)"
ABSTRACT,0.07877813504823152,"0
2
4
6 0 0.2 0.4 0.6 0.8"
ABSTRACT,0.08038585209003216,"Time, t"
ABSTRACT,0.0819935691318328,Expected Reward ·106
ABSTRACT,0.08360128617363344,"Sim
ODE 0 1 2 Q ·107"
ABSTRACT,0.08520900321543408,"0 2 4 6
0 2 4 t R ·103 ·106"
ABSTRACT,0.08681672025723473,"Figure 1: The RL-Perceptron is a model for policy learning in high dimensions. (a) In the classic
teacher-student model for supervised learning, a neural network called the student is trained on
inputs x whose label y∗is given by another neural network, called the teacher. (b) In the RL setting
the student moves through states st making a series of T choices given in response to inputs xt. The
RL-perceptron is an extension of the teacher-student model as we assume there is a ‘right’ choice
yt on each timestep given by a teacher network. The student receives a reward after T decisions
according to a criterion Φ that depends on the choices made and the corresponding correct choices.
(c) Example learning dynamics in the RL-perceptron for a problem with T = 12 choices where the
reward is given only if all the decisions are correct. The plot shows the expected reward of a student
trained in the RL perceptron setting in simulations (solid) and for our theoretical results (dashed)
obtained from solving the dynamical equations eqs. (5) and (6). Finite size simulations and theory
show good agreement. We reduce the stochastic evolution of the high dimensional student to the
study of deterministic evolution of two scalar quantities R and Q (more details in Sec. 2.1), their
evolution are shown in the inset. Parameters: D = 900, η1 = 1, η2 = 0, T = 12."
ABSTRACT,0.08842443729903537,"The RL perceptron:
In the classic teacher-student model of supervised learning [Gardner and
38"
ABSTRACT,0.09003215434083602,"Derrida, 1989, Seung et al., 1992], a neural network called the student is trained on inputs x whose
39"
ABSTRACT,0.09163987138263666,"labels y∗are given by another neural network called the teacher (see fig. 1a). The goal of the
40"
ABSTRACT,0.0932475884244373,"student is to learn the function represented by the teacher from samples (x, y∗). In RL, agents face
41"
ABSTRACT,0.09485530546623794,"a sequential decision-making task in which a sequence of correct intermediate choices is required
42"
ABSTRACT,0.09646302250803858,"to successfully complete an episode. We translate this process into the RL perceptron, a solvable
43"
ABSTRACT,0.09807073954983923,"model for a high-dimensional, sequential policy learning task shown in fig. 1b. The student with
44"
ABSTRACT,0.09967845659163987,"weights w takes a sequence of T choices over an episode. The correct choices are governed by
45"
ABSTRACT,0.10128617363344052,"the same teacher network w∗, i.e. the same underlying rule throughout every time-step of every
46"
ABSTRACT,0.10289389067524116,"episode. Crucially, unlike in the supervised learning setting, the student does not observe the correct
47"
ABSTRACT,0.1045016077170418,"choice for each input; instead, it receives a reward which depends on whether earlier decisions are
48"
ABSTRACT,0.10610932475884244,"correct. For instance, the student could receive a reward only if all T choices are correct, and no
49"
ABSTRACT,0.10771704180064309,"reward otherwise—a learning signal that is considerably less informative than in supervised learning.
50"
ABSTRACT,0.10932475884244373,"In addition to introducing the RL perceptron, our main contributions are as follows:
51"
ABSTRACT,0.11093247588424437,"• We derive an asymptotically exact set of Ordinary Differential Equations (ODEs) that
52"
ABSTRACT,0.11254019292604502,"describe the typical learning dynamics of policy gradient RL agents by building on classic
53"
ABSTRACT,0.11414790996784566,"work by Saad and Solla [1995], Biehl and Schwarze [1995], see section 2.1.
54"
ABSTRACT,0.1157556270096463,"• We use these ODEs to characterize learning behaviour in a diverse range of scenarios:
55"
ABSTRACT,0.11736334405144695,"– We explore several sparse delayed reward schemes and investigate the impact of
56"
ABSTRACT,0.1189710610932476,"negative rewards (section 2.2)
57"
ABSTRACT,0.12057877813504823,"– We derive optimal learning rate schedules and episode length curricula, and recover
58"
ABSTRACT,0.12218649517684887,"annealing strategies typically used in practice (section 2.3)
59"
ABSTRACT,0.12379421221864952,"– At fixed learning rates, we identify ranges of learning rates for which learning is
60"
ABSTRACT,0.12540192926045016,"‘easy,’ and ‘hybrid-hard’—possibly causing a critical slowing down in the dynamics
61"
ABSTRACT,0.1270096463022508,"(section 2.4)
62"
ABSTRACT,0.12861736334405144,"– We identify a speed-accuracy trade-off driven by reward stringency (section 2.5)
63"
ABSTRACT,0.1302250803858521,"• Finally we demonstrate that a similar speed-accuracy trade-off exists in simulations of high-
64"
ABSTRACT,0.13183279742765272,"dimensional policy learning from pixels using the procgen environment “Bossfight” [Cobbe
65"
ABSTRACT,0.13344051446945338,"et al., 2019], see section 3.
66"
ABSTRACT,0.13504823151125403,"Further related work
67"
ABSTRACT,0.13665594855305466,"Sample complexity in RL.
An important line of work in the theory of RL focuses on the sample
68"
ABSTRACT,0.1382636655948553,"complexity and other learnability measures for specific classes of models such as tabular RL [Azar
69"
ABSTRACT,0.13987138263665594,"et al., 2017, Zhang et al., 2020b], state aggregation [Dong et al., 2019], various forms of MDPs [Jin
70"
ABSTRACT,0.1414790996784566,"et al., 2020, Yang and Wang, 2019, Modi et al., 2020, Ayoub et al., 2020, Du et al., 2019a, Zhang
71"
ABSTRACT,0.14308681672025725,"et al., 2022], reactive POMDPs [Krishnamurthy et al., 2016], and FLAMBE [Agarwal et al., 2020].
72"
ABSTRACT,0.14469453376205788,"Here, we are instead concerned with the learning dynamics: how do reward rates, episode length, etc.
73"
ABSTRACT,0.14630225080385853,"influence the speed of learning and the final performance of the model.
74"
ABSTRACT,0.14790996784565916,"Statistical learning theory for RL aims at finding complexity measures analogous to the Rademacher
75"
ABSTRACT,0.1495176848874598,"complexity or VC dimension from statistical learning theory for supervised learning Bartlett and
76"
ABSTRACT,0.15112540192926044,"Mendelson [2002], Vapnik and Chervonenkis [2015]. Proposals include the Bellman Rank Jiang et al.
77"
ABSTRACT,0.1527331189710611,"[2017], or the Eluder dimension [Russo and Van Roy, 2013] and its generalisations [Jin et al., 2021].
78"
ABSTRACT,0.15434083601286175,"This approach focuses on worst-case analysis, which typically differs significantly from practice (at
79"
ABSTRACT,0.15594855305466238,"least in supervised learning [Zhang et al., 2021]). Furthermore, complexity measures for RL are
80"
ABSTRACT,0.15755627009646303,"generally more suitable for value-based methods; policy gradient methods have received less attention
81"
ABSTRACT,0.15916398713826366,"despite their prevalence in practice Bhandari and Russo [2019], Agarwal et al. [2021]. We focus
82"
ABSTRACT,0.1607717041800643,"instead on average-case dynamics of policy-gradient methods.
83"
ABSTRACT,0.16237942122186494,"Dynamics of learning.
A series of recent papers considered the dynamics of temporal-difference
84"
ABSTRACT,0.1639871382636656,"learning and policy gradient in the limit of wide two-layer neural networks Cai et al. [2019], Zhang
85"
ABSTRACT,0.16559485530546625,"et al. [2020a], Agazzi and Lu [2021, 2022]. These works focus on one of two “wide” limits: either
86"
ABSTRACT,0.16720257234726688,"the neural tangent kernel [Jacot et al., 2018, Du et al., 2019b] or “lazy” regime [Chizat et al., 2019],
87"
ABSTRACT,0.16881028938906753,"where the network behaves like an effective kernel machine and does not learn data-dependent
88"
ABSTRACT,0.17041800643086816,"features, which is key for efficient generalisation in high-dimensions. In our setting, the success
89"
ABSTRACT,0.1720257234726688,"of the student crucially relies on learning the weight vector of the teacher, which is hard for lazy
90"
ABSTRACT,0.17363344051446947,"methods [Ghorbani et al., 2019, 2020, Chizat and Bach, 2020, Refinetti et al., 2021]. The other
91"
ABSTRACT,0.1752411575562701,"“wide” regime is the mean-field limit of interacting particles, akin to Mei et al. [2018], Chizat and
92"
ABSTRACT,0.17684887459807075,"Bach [2018], Rotskoff and Vanden-Eijnden [2018], where learning dynamics are captured by a
93"
ABSTRACT,0.17845659163987138,"non-linear partial differential equation. While this elegant description allows them to establish global
94"
ABSTRACT,0.18006430868167203,"convergence properties, it is hard to solve in practice. The ODE description we derive here instead
95"
ABSTRACT,0.18167202572347266,"will allow us to describe a series of effects in the following sections.
96"
ABSTRACT,0.1832797427652733,"1
The RL Perceptron: setup and learning algorithm
97"
ABSTRACT,0.18488745980707397,"We study the simplest possible student network, a perceptron with weight vector w that takes in
98"
ABSTRACT,0.1864951768488746,"high-dimensional inputs x ∈RD and outputs y(x) = sgn(w⊺x). We interpret the outputs y(x) as
99"
ABSTRACT,0.18810289389067525,"decisions, for example whether to go left or right in an environment. Because the student makes
100"
ABSTRACT,0.18971061093247588,"choices in response to high-dimensional inputs, it is analogous to a policy network. To train the
101"
ABSTRACT,0.19131832797427653,"network, we therefore consider a policy gradient learning update analogous to the REINFORCE
102"
ABSTRACT,0.19292604501607716,"algorithm [Sutton et al., 2000] that is adapted to the perceptron. At every timestep t during the µth
103"
ABSTRACT,0.1945337620578778,"episode of length T, the agent occupies some state st in the environment, receives an observation xµ
t
104"
ABSTRACT,0.19614147909967847,"conditioned on st, and takes an action yµ
t = sgn(wµ⊺xµ
t ), with t = 1, . . . , T. The correct choice for
105"
ABSTRACT,0.1977491961414791,"each input is given by a fixed perceptron teacher with weights w∗. The crucial point is that the student
106"
ABSTRACT,0.19935691318327975,"does not have access to all the correct choices; it only receives a reward at the end of the episode if it
107"
ABSTRACT,0.20096463022508038,"completes the episode successfully, for example by making the correct decision at all times. If it does
108"
ABSTRACT,0.20257234726688103,"not succeed, it may receive a penalty; we will see in section 2.4 that receiving penalties is not always
109"
ABSTRACT,0.20418006430868169,"beneficial. In our setup, this translates into a weight update at the end of the µth episode that is given
110"
ABSTRACT,0.2057877813504823,"by
111"
ABSTRACT,0.20739549839228297,"wµ+1 = wµ + η1
√ D"
T,0.2090032154340836,"1
T T
X"
T,0.21061093247588425,"t=1
ytxtI(Φ) !µ −η2
√ D"
T,0.21221864951768488,"1
T T
X"
T,0.21382636655948553,"t=1
ytxt(1 −I(Φ)) !µ ,
(1)"
T,0.21543408360128619,"where I is an indicator function and Φ is the criterion that determines whether the episode was
112"
T,0.2170418006430868,"completed successfully—for instance, I(Φ) = QT
t θ(yty∗
t ) (where θ is the step function) if the student
113"
T,0.21864951768488747,"has to get every decision right in order to receive a reward. The update is general in the sense that the
114"
T,0.2202572347266881,"term proportional to the learning rate η1 > 0 prescribes the reward update for the fulfillment of the
115"
T,0.22186495176848875,"condition, while the term proportional to η2 ≥0 gives us the possibility to add a a penalty or negative
116 (a)"
T,0.22347266881028938,"100
102
104
106
108 0 0.5 1"
T,0.22508038585209003,"Time, t"
T,0.2266881028938907,"Normalised Overlap, R/√Q η2 .0 .005 .01 (b)"
T,0.2282958199356913,"100
102
104
106
108 0 0.5 1"
T,0.22990353697749197,"Time, t"
T,0.2315112540192926,"Normalised Overlap, R/√Q"
T,0.23311897106109325,"T0 = 8
T0 = 7
T0 = 6"
T,0.2347266881028939,"T0 = 5
T0 = 4
T0 = 3
T0 = 2 (c)"
T,0.23633440514469453,"100
102
104
106
108 0 0.5 1"
T,0.2379421221864952,"Time, t"
T,0.2395498392282958,"Normalised Overlap, R/√Q rb .0 .005 .01"
T,0.24115755627009647,"Figure 2: ODEs accurately describe diverse learning protocols. Evolution of the normalised
student-teacher overlap ρ for the numerical solution of the ODEs (dashed) and simulation (coloured)
in three reward protocols. All students receive a reward of η1 for getting all decisions in an episode
correct, and additionally: (a) A penalty η2 (i.e. negative reward) is received if the agent does not
survive until the end of an episode. (b) An additional reward of 0.2 is received if the agent survives
beyond T0 timesteps. (c) An additional reward rb is received for every correct decision made in an
episode. Parameters: D = 900, T = 12, η1 = 1."
T,0.2427652733118971,"reward should the student not succeed. Note that in the case of T = 1, η2 = 0, and I(Φ) = θ(yy∗),
117"
T,0.24437299035369775,"the learning rule updates the weight only if the student is correct on a given sample. It can thus be seen
118"
T,0.2459807073954984,"as the “opposite” of the famous perceptron learning rule of supervised learning [Rosenblatt, 1962],
119"
T,0.24758842443729903,"where weights are only updated if the student is wrong. For a more in-detail discussion of the relation
120"
T,0.2491961414790997,"between the weight update in eq. (1) and the REINFORCE algorithm, see appendix A.
121"
THEORETICAL RESULTS,0.2508038585209003,"2
Theoretical Results
122"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.25241157556270094,"2.1
A set of dynamical equations captures the learning dynamics of an RL perceptron exactly
123"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.2540192926045016,"The goal of the student during training is to emulate the teacher as closely as possible; or in other
124"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.25562700964630225,"words, have a small number of disagreements with the teacher y(x) ̸= y∗(x). The generalisation
125"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.2572347266881029,"error is given by the average number of disagreements
126"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.25884244372990356,"ϵg ≡⟨y(x)y∗(x)⟩=
D
sgn

w∗· x/
√"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.2604501607717042,"D

sgn

w · x/
√"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.2620578778135048,"D
E
= ⟨sgn(ν)sgn(λ)⟩
(2)"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.26366559485530544,"where the average ⟨·⟩is taken over the inputs x, and we have introduced the scalar pre-activations
127"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.2652733118971061,"for the student and the teacher, λ ≡w · x/
√"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.26688102893890675,"D and ν ≡w∗· x/
√"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.2684887459807074,"D, respectively. We can therefore
128"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.27009646302250806,"transform the high-dimensional average over the inputs x into a low-dimensional average over the
129"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.2717041800643087,"pre-activations (λ, ν). The average in eq. (2) can be carried out by noting that the tuple (λ, ν) follow
130"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.2733118971061093,"a jointly Gaussian distribution with means ⟨λ⟩= ⟨ν⟩= 0 and covariances
131"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.27491961414790994,Q ≡⟨λ2⟩= w · w
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.2765273311897106,"D
,
R ≡⟨λν⟩= w · w∗"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.27813504823151125,"D
and
S ≡⟨ν2⟩= w∗· w∗"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.2797427652733119,"D
.
(3)"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.28135048231511256,"These covariances, or overlaps as they are sometimes called in the literature, have a simple interpreta-
132"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.2829581993569132,"tion. The overlap S is simply the length of the weight vector of the teacher; in the high-dimensional
133"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.2845659163987138,"limit D →∞, S →1. Likewise, the overlap Q gives the length of the student weight vector; however,
134"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.2861736334405145,"this is a quantity that will vary during training. For example, when starting from small initial weights,
135"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.2877813504823151,"Q will be small, and grow throughout training. Lastly, the “alignment” R quantifies the correlation
136"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.28938906752411575,"between the student and the teacher weight vector. At the beginning of training, R ≈0, as both the
137"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.2909967845659164,"teacher and the initial condition of the student are drawn at random. As the student starts learning,
138"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.29260450160771706,"the overlap R increases. Evaluating the Gaussian average in eq. (2) shows that the generalisation
139"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.2942122186495177,"error is then a function of the normalised overlap ρ = R/√Q, and given by
140"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.2958199356913183,ϵg = 1
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.297427652733119,"π arccos
 R
√Q 
(4)"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.2990353697749196,"The crucial point here is that we have reduced the description of the high-dimensional learning
141"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.30064308681672025,"problem from the D parameters of the student weight w to two time-evolving quantities, Q and R.
142"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.3022508038585209,"We now discuss how to analyse their dynamics.
143"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.30385852090032156,"The dynamics of order parameters.
At any given point during training, the value of the order
144"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.3054662379421222,"parameters determines the test error via eq. (4). But how do the order parameters evolve during
145"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.3070739549839228,"training with the update rule eq. (1)? We followed the approach of Kinzel and Ruján [1990], Saad
146"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.3086816720257235,"and Solla [1995], Biehl and Schwarze [1995] to derive a set of dynamical equations that describe
147"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.3102893890675241,"the dynamics of the student in the high-dimensional limit where the input dimension goes to infinity.
148"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.31189710610932475,"We give explicit dynamics for different reward conditions Φ, namely requiring all decisions correct
149"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.3135048231511254,"in an episode of length T; requiring n or more decisions correct in an episode of length T; and
150"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.31511254019292606,"receiving reward for each correct response. Due to the length of these expressions, we report the
151"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.3167202572347267,"generic expression of the updates in the supplementary material in appendix B. Below, we state a
152"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.3183279742765273,"version of the equations for the specific reward condition where the agent must survive until the end
153"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.319935691318328,"of an episode to receive a reward, I(Φ) = QT
t θ(yty∗
t ). The ODEs for the order parameters then read
154 dR"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.3215434083601286,"dα = η1 + η2
√ 2π"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.32315112540192925,"
1 + R
√Q"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.3247588424437299,"
P T −1 −η2R
r 2"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.32636655948553056,"πQ
(5) dQ"
A SET OF DYNAMICAL EQUATIONS CAPTURES THE LEARNING DYNAMICS OF AN RL PERCEPTRON EXACTLY,0.3279742765273312,dα = (η1 + η2) r
Q,0.3295819935691318,2Q π
Q,0.3311897106109325,"
1 + R
√Q"
Q,0.3327974276527331,"
P T −1 −2η2 r"
Q,0.33440514469453375,2Q
Q,0.3360128617363344,"π + (η2
1 −η2
2)
T
P T + η2
2
T ,
(6)"
Q,0.33762057877813506,"where α ≡µ/D serves as a continuous time variable in the limit D →∞(not to be confused
155"
Q,0.3392282958199357,"with t which counts episode steps), and P =
 
1 −cos−1(R/√Q)/π

is the probability of a single
156"
Q,0.3408360128617363,"correct decision. While our derivation of the equations follow heuristics from statistical physics, we
157"
Q,0.342443729903537,"anticipate that their asymptotic correctness in the limit D →∞can be established rigorously using
158"
Q,0.3440514469453376,"the techniques of Goldt et al. [2019], Veiga et al. [2022], Arnaboldi et al. [2023]. We illustrate the
159"
Q,0.34565916398713825,"accuracy of these equations already in finite dimensions (D = 900) in fig. 1c, where we show the
160"
Q,0.34726688102893893,"expected reward, as well as the overlaps R and Q, of a student as measured during a simulation and
161"
Q,0.34887459807073956,"from integration of the dynamical equations (solid and dotted lines, respectively).
162"
Q,0.3504823151125402,"The derivation of the dynamical equations that govern the learning dynamics of the RL perceptron
163"
Q,0.3520900321543408,"are our first main result. Equipped with this tool, we now analyse several phenomena exhibited by
164"
Q,0.3536977491961415,"the RL perceptron through a detailed study of these equations.
165"
LEARNING PROTOCOLS,0.3553054662379421,"2.2
Learning protocols
166"
LEARNING PROTOCOLS,0.35691318327974275,"The RL perceptron allows for the characterization of different RL protocols by adapting the reward
167"
LEARNING PROTOCOLS,0.35852090032154343,"condition Φ. We considered the following three settings:
168"
LEARNING PROTOCOLS,0.36012861736334406,"Vanilla: The dynamics in the ‘standard’ case without penalty, η2 = 0, is shown in fig. 5a and fig. 5b.
169"
LEARNING PROTOCOLS,0.3617363344051447,"Rewards are sparsest in this protocol, and as a result we observe a characteristic initial plateau in
170"
LEARNING PROTOCOLS,0.3633440514469453,"expected reward followed by a rapid jump. The length of this plateau increases with T, consistent
171"
LEARNING PROTOCOLS,0.364951768488746,"with the notion that sparse rewards make exploration hard and slow learning [Bellemare et al., 2016].
172"
LEARNING PROTOCOLS,0.3665594855305466,"Plateaus during learning, which arise from saddle points in the loss landscape, have also been studied
173"
LEARNING PROTOCOLS,0.36816720257234725,"for (deep) neural networks in the supervised setting [Saad and Solla, 1995, Dauphin et al., 2014],
174"
LEARNING PROTOCOLS,0.36977491961414793,"but do not arise in the supervised perceptron. Hence the RL setting can qualitatively change the
175"
LEARNING PROTOCOLS,0.37138263665594856,"learning trajectory. The benefit of withholding penalties is that while slower, the perceptron reaches
176"
LEARNING PROTOCOLS,0.3729903536977492,"the highest level of expected reward in this case. This is a first example of a speed-accuracy trade-off
177"
LEARNING PROTOCOLS,0.3745980707395498,"that we will explore in more detail in section 2.5 and that we also found in our experiments with
178"
LEARNING PROTOCOLS,0.3762057877813505,"Bossfight in section 3.
179"
LEARNING PROTOCOLS,0.3778135048231511,"Penalty: The initial plateau can be reduced by providing a penalty or negative reward (η2 > 0) when
180"
LEARNING PROTOCOLS,0.37942122186495175,"the student fails in the task. This change provides weight updates much earlier in training and thus
181"
LEARNING PROTOCOLS,0.38102893890675243,"accelerates the escape from the plateau. The dynamics under this protocol are shown in fig. 2a. It is
182"
LEARNING PROTOCOLS,0.38263665594855306,"clear the penalty provides an initial speed-up in learning, as expected if the agent were to be unaligned
183"
LEARNING PROTOCOLS,0.3842443729903537,"and more likely to commit an error. However, a high penalty can create additional sub-optimal fixed
184"
LEARNING PROTOCOLS,0.3858520900321543,"points in the dynamics leading to a low asymptotic performance (more on this in section 2.4). In the
185"
LEARNING PROTOCOLS,0.387459807073955,"simulations, finite size effects occasionally permit escape from the sub-optimal fixed point and jumps
186"
LEARNING PROTOCOLS,0.3890675241157556,"to the optimal one, leading to a high variance in the results.
187"
LEARNING PROTOCOLS,0.39067524115755625,"Subtask and breadcrumbs: The model is also able to capture the dynamics of more complicated
188"
LEARNING PROTOCOLS,0.39228295819935693,"protocols: fig. 3b shows learning under the protocol where a smaller sub-reward is received if the
189"
LEARNING PROTOCOLS,0.39389067524115756,"agent survives beyond a shorter duration T0 < T, i.e. some reward is still received even if the
190"
LEARNING PROTOCOLS,0.3954983922829582,"agent does not survive for the entire episode. Another learning protocol we can capture is that of
191"
LEARNING PROTOCOLS,0.3971061093247588,"‘graded-breadcrumbs’, where the agent receives a small reward rb for every correct decision made
192 (a)"
LEARNING PROTOCOLS,0.3987138263665595,"0
0.5
1
1.5
2 0 0.5 1"
LEARNING PROTOCOLS,0.4003215434083601,"Time, t ρ"
LEARNING PROTOCOLS,0.40192926045016075,"Optimal 10
1 ·105 T (b)"
LEARNING PROTOCOLS,0.40353697749196143,"0
0.5
1
1.5
2 0 0.5 1"
LEARNING PROTOCOLS,0.40514469453376206,"Time, t ρ"
LEARNING PROTOCOLS,0.4067524115755627,"Optimal 10
0 ·105 η1 (c)"
LEARNING PROTOCOLS,0.40836012861736337,"100
102
104
106
10−4 10−2 100 102"
LEARNING PROTOCOLS,0.409967845659164,"Time, t η 100 101 102 103 104 T"
LEARNING PROTOCOLS,0.4115755627009646,"η = 0.1
η = 1
η = 10"
LEARNING PROTOCOLS,0.41318327974276525,"T = 3
T = 8
T = 13"
LEARNING PROTOCOLS,0.41479099678456594,"Figure 3: Optimal schedules for episode length T and learning rate η. (a) Evolution of the
normalised overlap under optimal episode length scheduling (dashed) and various constant episode
lengths (green). (b) Evolution of the normalised overlap under optimal learning rate scheduling
(dashed) and various constant learning rates (blue). (c) Evolution of optimal T (green) and η (blue)
over learning. Parameters: D = 900, Q = 1, η2 = 0, (a) η = 1, (b) T = 8."
LEARNING PROTOCOLS,0.41639871382636656,"in an episode, i.e. like the previous method some reward is still received even if the agent does not
193"
LEARNING PROTOCOLS,0.4180064308681672,"survive for the entire episode, these dynamics are captured in fig. 3c.
194"
LEARNING PROTOCOLS,0.41961414790996787,"2.3
Optimal hyper-parameter schedules: make episodes longer and anneal your learning rate
195"
LEARNING PROTOCOLS,0.4212218649517685,"Hyper-parameter schedules are crucial for successful training of RL agents. In our setup, the two
196"
LEARNING PROTOCOLS,0.4228295819935691,"most important hyper-parameters are the learning rates and the episode length. In the RL perceptron,
197"
LEARNING PROTOCOLS,0.42443729903536975,"we can derive optimal schedules for both hyper-parameters. For simplicity, here we report the
198"
LEARNING PROTOCOLS,0.42604501607717044,"results in the spherical case, where the length of the student vector is fixed at
√"
LEARNING PROTOCOLS,0.42765273311897106,"D (we discuss the
199"
LEARNING PROTOCOLS,0.4292604501607717,"unconstrained case in the appendix C), then Q(α) = 1 at all times and we only need to track the
200"
LEARNING PROTOCOLS,0.43086816720257237,"teacher-student overlap ρ = R/√Q, which quantifies the generalisation performance of the agent.
201"
LEARNING PROTOCOLS,0.432475884244373,"Keeping the choice I(Φ) = QT
t=1 θ(yty∗
t ) and turning off the penalty term (η2 = 0), we find that the
202"
LEARNING PROTOCOLS,0.4340836012861736,"teacher-student overlap is governed by the equation
203"
LEARNING PROTOCOLS,0.43569131832797425,"dρ
dα =
η
√2πQ(1 −ρ2)

1 −1"
LEARNING PROTOCOLS,0.43729903536977494,"π cos−1 (ρ)
T −1
−
η2"
LEARNING PROTOCOLS,0.43890675241157556,"2TQρ

1 −1"
LEARNING PROTOCOLS,0.4405144694533762,"π cos−1 (ρ)
T
(7)"
LEARNING PROTOCOLS,0.44212218649517687,"The optimal schedules over episodes for T and η can then be found by maximising the change
204"
LEARNING PROTOCOLS,0.4437299035369775,"in overlap at each update, i.e. setting
∂
∂T

dρ
dα

and
∂
∂η

dρ
dα

to zero respectively. After some
205"
LEARNING PROTOCOLS,0.4453376205787781,"calculations, we find the optimal schedules to be
206"
LEARNING PROTOCOLS,0.44694533762057875,Topt =
LEARNING PROTOCOLS,0.44855305466237944,"
√π"
LEARNING PROTOCOLS,0.45016077170418006,"2
ηρP
(1 −ρ2)√2Q  1 + s"
LEARNING PROTOCOLS,0.4517684887459807,"1 −
√2Q"
LEARNING PROTOCOLS,0.4533762057877814,"ηρ
4(1 −ρ2)
√πP ln(P)  "
LEARNING PROTOCOLS,0.454983922829582,"
and
ηopt = r"
LEARNING PROTOCOLS,0.4565916398713826,"Q
2π
T(1 −ρ2) ρP"
LEARNING PROTOCOLS,0.45819935691318325,"(8)
where ⌊·⌋indicates the floor function.
207"
LEARNING PROTOCOLS,0.45980707395498394,"Figure 3a shows the evolution of ρ under the optimal episode length schedule (dashed) compared to
208"
LEARNING PROTOCOLS,0.46141479099678456,"other constant episode lengths (green). Similarly, fig. 3b shows the evolution of ρ under the optimal
209"
LEARNING PROTOCOLS,0.4630225080385852,"learning rate schedule (dashed) compared to other constant learning rates (blue). The functional
210"
LEARNING PROTOCOLS,0.4646302250803859,"forms of Topt and ηopt over time are shown in fig. 3c.
211"
LEARNING PROTOCOLS,0.4662379421221865,"During learning the student seeks increasingly refined information to improve its expected reward.
212"
LEARNING PROTOCOLS,0.4678456591639871,"This simple observation explains the monotonic increase of the optimal episode length and the
213"
LEARNING PROTOCOLS,0.4694533762057878,"decrease in learning rates. Starting from the episode duration, we can observe that given the discrete
214"
LEARNING PROTOCOLS,0.47106109324758844,"nature of the decisions, information obtained from the rewards simply pushes the decision boundary
215"
LEARNING PROTOCOLS,0.47266881028938906,"towards a partition of the input space. This partition is determined by the episode length T and
216"
LEARNING PROTOCOLS,0.4742765273311897,"correspond to a fraction 1/2T of the entire input space. Therefore a positive reward conveys T bits of
217"
LEARNING PROTOCOLS,0.4758842443729904,"information. At a fixed learning rate, when the student becomes proficient in the task it will not be
218"
LEARNING PROTOCOLS,0.477491961414791,"able to improve further the decision boundary, and will fluctuate around the optimal solution unless
219"
LEARNING PROTOCOLS,0.4790996784565916,"longer episodes are provided.
220"
LEARNING PROTOCOLS,0.4807073954983923,"(a)
(b)
(c)"
LEARNING PROTOCOLS,0.48231511254019294,"0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1 η2 ρfix"
LEARNING PROTOCOLS,0.48392282958199356,"0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1 η1 η2"
LEARNING PROTOCOLS,0.4855305466237942,"0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1 η1 η2"
LEARNING PROTOCOLS,0.4871382636655949,"Hybrid-hard
Easy"
LEARNING PROTOCOLS,0.4887459807073955,"Figure 4: Phase plots characterising learnability. In the case where all decisions in an episode
of length T must be correct in order to receive a reward. (a) the fixed points of ρ for T = 13 and
η1 = 1, the dashed portion of the line denotes where the fixed points are unstable. (b) Phase plot
showing regions of hardness for T = 13. (c) Phase plot showing regions or hardness for T = 8. The
green regions represent the Easy phase where with probability 1 the algorithm naturally converges
to the optimal ρfix from a random initialisation. The orange region indicates the Hybrid-hard phase,
where with high probability the algorithm converges to the sub-optimal ρfix from random initilisation.
Parameters: D = 900, Q = 1."
LEARNING PROTOCOLS,0.4903536977491961,"Our analysis shows that a polynomial increase in the episode length gives the optimal performance
221"
LEARNING PROTOCOLS,0.4919614147909968,"in the RL perceptron, see fig. 3c (top); increasing T in the RL perceptron is akin to increasing task
222"
LEARNING PROTOCOLS,0.49356913183279744,"difficulty, and the polynomial scheduling of Topt specifies a curriculum. Curricula of increasing task
223"
LEARNING PROTOCOLS,0.49517684887459806,"difficulty are commonly used in RL to give convergence speed-ups and learn problems that otherwise
224"
LEARNING PROTOCOLS,0.4967845659163987,"would be too difficult to learn ab initio Narvekar et al. [2020]. Analogously, the fluctuations can be
225"
LEARNING PROTOCOLS,0.4983922829581994,"reduced by annealing the learning rate and averaging over a larger number of samples. Akin to work in
226"
LEARNING PROTOCOLS,0.5,"RL literature studying adaptive step-sizes [Dabney, 2014, Pirotta et al., 2013], we find that annealing
227"
LEARNING PROTOCOLS,0.5016077170418006,"the learning rate during training is beneficial for greater speed and generalisation performance. For the
228"
LEARNING PROTOCOLS,0.5032154340836013,"RL perceptron, a polynomial decay in the learning rate gives optimal performance as shown in fig. 3c
229"
LEARNING PROTOCOLS,0.5048231511254019,"(bottom), consistent with work in the parallel area of high-dimensional non-convex optimization
230"
LEARNING PROTOCOLS,0.5064308681672026,"problems [d’Ascoli et al., 2022], and stochastic approximation algorithms in RL [Dalal et al., 2017].
231"
PHASE SPACE,0.5080385852090032,"2.4
Phase Space
232"
PHASE SPACE,0.5096463022508039,"With a non-zero penalty (η2), the generalisation performance of the agent can enter different regimes
233"
PHASE SPACE,0.5112540192926045,"of learning. This is most clearly exemplified in the spherical case, where the number of fixed points of
234"
PHASE SPACE,0.5128617363344051,"the ODE governing the dynamics of the overlap exist in distinct phases determined by the combination
235"
PHASE SPACE,0.5144694533762058,"of reward and penalty. For the simplest case

I(Φ) = QT
t (yty∗
t )

these phases are shown in fig. 4.
236"
PHASE SPACE,0.5160771704180064,"Figure 4a shows the fixed points achievable over a range of penalties for a fixed η1 = 1 (obtained from
237"
PHASE SPACE,0.5176848874598071,"a numerical solution of the ODE in ρ). There are two distinct regions: 1) Easy, where there is a unique
238"
PHASE SPACE,0.5192926045016077,"fixed point and the algorithm naturally converges to this optimal ρfix from a random initialisation,
239"
PHASE SPACE,0.5209003215434084,"2) a Hybrid-hard region (given the analogy with results from inference problems Ricci-Tersenghi
240"
PHASE SPACE,0.522508038585209,"et al. [2019]), where there are two stable (1 good and 1 bad) fixed points, and 1 unstable fixed point,
241"
PHASE SPACE,0.5241157556270096,"and either stable point is achievable depending on the initialisation of the student (orange). The
242"
PHASE SPACE,0.5257234726688103,"‘hybrid-hard’ region separates two easy regions with very distinct performance levels. In this region
243"
PHASE SPACE,0.5273311897106109,"the algorithm with high probability converges to ρfix with the worse performance level. These two
244"
PHASE SPACE,0.5289389067524116,"regions are visualised in (η1, η2) space in fig. 4b for an episode length of T = 13. The topology
245"
PHASE SPACE,0.5305466237942122,"of these regions are also governed by episode length, with a sufficiently small T reducing the the
246"
PHASE SPACE,0.5321543408360129,"area of the ‘hybrid-hard’ phase to zero, meaning there is always 1 stable fixed point which may not
247"
PHASE SPACE,0.5337620578778135,"necessarily give ‘good’ generalisation. Figure 4c shows the phase plot for T = 8, where the orange
248"
PHASE SPACE,0.5353697749196141,"(hybrid-hard) has shrunk, this corresponds to the s-shaped curve in fig. 4a becoming flatter (closer
249"
PHASE SPACE,0.5369774919614148,"to monotonic). Learning with η2 This is not a peculiarity specific to the spherical case, indeed, we
250"
PHASE SPACE,0.5385852090032154,"observe different regimes in the learning dynamics in the setting with unrestricted Q which we report
251"
PHASE SPACE,0.5401929260450161,"in appendix C.
252"
PHASE SPACE,0.5418006430868167,"These phases show that at a fixed η1 increasing η2 will eventually lead to a first order phase transition,
253"
PHASE SPACE,0.5434083601286174,"and the speed benefits gained from a non-zero η2 will be nullified due to the transition into the
254"
PHASE SPACE,0.545016077170418,"hybrid-hard phase. In fact, when taking η2 close to the transition point, instead of speeding up
255"
PHASE SPACE,0.5466237942122186,"learning there is the presence of a critical slowing down, which we report in appendix C.
256 (a)"
PHASE SPACE,0.5482315112540193,"100
102
104
106
108 0 0.5 1"
PHASE SPACE,0.5498392282958199,"Time, t"
PHASE SPACE,0.5514469453376206,Expected Reward (b)
PHASE SPACE,0.5530546623794212,"100
102
104
106
108 0 0.5 1"
PHASE SPACE,0.5546623794212219,"Time, t"
PHASE SPACE,0.5562700964630225,"Normalised Overlap, R/√Q T 12 9 6 3 (c)"
PHASE SPACE,0.5578778135048231,"100
102
104
106
108 0 0.5 1"
PHASE SPACE,0.5594855305466238,"Time, t"
PHASE SPACE,0.5610932475884244,"Normalised Overlap, R/√Q"
PHASE SPACE,0.5627009646302251,"n = 7
n = 8
n = 9
n = 10
n = 11
n = 12
n = 13"
PHASE SPACE,0.5643086816720257,"Figure 5: Speed-accuracy tradeoff. Evolution of (a) the expected reward and (b) corresponding
normalised overlap for simulation (solid) and ODE solution (dashed) over a range of T when all
decisions in an episode of length T are required correct, and η2 = 0. (c) Evolution of the normalised
overlap between student and teacher weights for simulation (solid) and ODE solution (dashed) for
the case where n or more decisions in an episode of length 13 are required correct for an update with
η2 = 0. More stringent reward conditions slow learning but can improve performance. Parameters:
D = 900, η1 = 1, η2 = 0."
PHASE SPACE,0.5659163987138264,"A common problem with REINFORCE is high variance gradient estimates leading to bad
257"
PHASE SPACE,0.567524115755627,"performance [Marbach and Tsitsiklis, 2003, Schulman et al., 2015]. The reward (η1) and punishment
258"
PHASE SPACE,0.5691318327974276,"(η2) magnitude alters the variance of the updates, and we show that the interplay between reward,
259"
PHASE SPACE,0.5707395498392283,"penalty and reward-condition and their effect on performance can be probed within our model. This
260"
PHASE SPACE,0.572347266881029,"framework opens the possibility for studying phase transitions between learning regimes [Gamarnik
261"
PHASE SPACE,0.5739549839228296,"et al., 2022].
262"
SPEED-ACCURACY TRADE-OFF,0.5755627009646302,"2.5
Speed-accuracy trade-off
263"
SPEED-ACCURACY TRADE-OFF,0.5771704180064309,"Figure 5c shows the evolution of normalised overlap ρ = R/√Q between the student and teacher
264"
SPEED-ACCURACY TRADE-OFF,0.5787781350482315,"obtained from simulations and from solving the ODEs in the case where n or more decisions must
265"
SPEED-ACCURACY TRADE-OFF,0.5803858520900321,"be correctly made in an episode of length T = 13 in order to receive a reward (with η2 = 0). We
266"
SPEED-ACCURACY TRADE-OFF,0.5819935691318328,"observe a speed-accuracy trade-off, where decreasing n increases the initial speed of learning but
267"
SPEED-ACCURACY TRADE-OFF,0.5836012861736335,"leads to worse asymptotic performance; this alleviates the initial plateau in learning seen previously
268"
SPEED-ACCURACY TRADE-OFF,0.5852090032154341,"in fig. 5b at the cost of good generalisation. In essence, a lax reward function is probabilistically
269"
SPEED-ACCURACY TRADE-OFF,0.5868167202572347,"more achievable early in learning; but it rewards some fraction of incorrect decisions, leading to
270"
SPEED-ACCURACY TRADE-OFF,0.5884244372990354,"lower asymptotic accuracy. By contrast a stringent reward function slows learning but eventually
271"
SPEED-ACCURACY TRADE-OFF,0.590032154340836,"produces a highly aligned student. For a given MDP, it is known that arbitrary shaping applied
272"
SPEED-ACCURACY TRADE-OFF,0.5916398713826366,"to the reward function will change the optimal policy (reduce asymptotic performance) [Ng et al.,
273"
SPEED-ACCURACY TRADE-OFF,0.5932475884244373,"1999]. Empirically, reward shaping has been shown to speed up learning and help overcome difficult
274"
SPEED-ACCURACY TRADE-OFF,0.594855305466238,"exploration problems [Gullapalli and Barto, 1992]. Reconciling these results with the phenomena
275"
SPEED-ACCURACY TRADE-OFF,0.5964630225080386,"observed in our setting is an interesting avenue for future work.
276"
EXPERIMENTS,0.5980707395498392,"3
Experiments
277"
EXPERIMENTS,0.5996784565916399,"To verify that our theoretical framework captures qualitative features of more general settings, we
278"
EXPERIMENTS,0.6012861736334405,"train agents from pixels on the Procgen [Cobbe et al., 2019] game ‘Bossfight’ (example frame, fig. 6a
279"
EXPERIMENTS,0.6028938906752411,"(top)). To remain close to our theoretical setting, we consider a modified version of the game where
280"
EXPERIMENTS,0.6045016077170418,"the agent cannot defeat the enemy and wins only if it survives for a given duration T. On each
281"
EXPERIMENTS,0.6061093247588425,"timestep the agent has the binary choice of moving left/right and aims to dodge incoming projectiles.
282"
EXPERIMENTS,0.6077170418006431,"We give the agent h lives, where the agent loses a life if struck by a projectile and continues an
283"
EXPERIMENTS,0.6093247588424437,"episode if it has lives remaining. This reward structure reflects the sparse reward setup from our
284"
EXPERIMENTS,0.6109324758842444,"theory and is analogous to requiring n out of T decisions to be correct within an episode. We further
285"
EXPERIMENTS,0.612540192926045,"add asteroids at the left and right boundaries of the playing field which destroy the agent on contact,
286"
EXPERIMENTS,0.6141479099678456,"such that the agent cannot hide in the corners. Observations, shown in fig. 6a (bottom), are centred on
287"
EXPERIMENTS,0.6157556270096463,"the agent and downsampled to size 35 × 64 with three colour channels, yielding a 6720 dimensional
288"
EXPERIMENTS,0.617363344051447,"input. The pixels corresponding to the agent are set to zero since these otherwise act as near-constant
289"
EXPERIMENTS,0.6189710610932476,"bias inputs not present in our model. The agent is endowed with a shallow policy network with
290"
EXPERIMENTS,0.6205787781350482,"(a)
(b) 2
4 20 40 60"
EXPERIMENTS,0.6221864951768489,Episode
EXPERIMENTS,0.6237942122186495,% Successful Episodes
EXPERIMENTS,0.6254019292604501,"h = 1
h = 2
h = 3
h = 4 ·105 (c)"
EXPERIMENTS,0.6270096463022508,"0.01
(Left) −0.01"
EXPERIMENTS,0.6286173633440515,(Right)
EXPERIMENTS,0.6302250803858521,"0.1
(Left)"
EXPERIMENTS,0.6318327974276527,"−0.1
(Right) ω ω"
EXPERIMENTS,0.6334405144694534,"Figure 6: Empirical speed-accuracy tradeoff in Bossfight. (a) Top: Screenshot from a frame of
‘Bossfight.’ Bottom: Example observation provided to the agent’s policy network. In our variant, the
agent can move left or right and aims to survive for a given duration T. Collision with projectiles
or asteroids costs one life, and the agent has h lives before an episode terminates. (b) Performance
during training, measured on evaluation episodes with h = 3 lives. Agents trained in stringent
conditions (h = 1) learn slowly but eventually outperform agents trained in lax conditions (h = 4),
an instance of the speed-accuracy tradeoff. Shaded regions indicate SEM over 10 repetitions. (c)
Policy network weights for an agent with (top) h = 4 lives and (bottom) h = 1 life. For simplicity,
one colour channel (red) is shown. Training with fewer lives increases the weight placed on dodging
projectiles (see text). Parameters: T = 100, η1 = 8.2e −5, η2 = 0."
EXPERIMENTS,0.635048231511254,"logistic output unit that indicates the probability of left or right action. The weights of the policy
291"
EXPERIMENTS,0.6366559485530546,"network are trained using the policy gradient update of eq. (1) under a pure random policy.
292"
EXPERIMENTS,0.6382636655948553,"To study the speed-accuracy trade-off, we train agents with different numbers of lives. As seen
293"
EXPERIMENTS,0.639871382636656,"in fig. 6b, we observe a clear speed-accuracy trade-off mediated by agent health consistent with
294"
EXPERIMENTS,0.6414790996784566,"our theoretical findings (c.f. fig. 3c). Figure 6c shows the final policy weights for agents trained
295"
EXPERIMENTS,0.6430868167202572,"with h = 1 and h = 4. These show interpretable structure, roughly split into thirds vertically: the
296"
EXPERIMENTS,0.6446945337620579,"weights in the top third detect the position of the boss and centre the agent beneath it; this causes
297"
EXPERIMENTS,0.6463022508038585,"projectiles to arrive vertically rather than obliquely, making them easier to dodge. The weights
298"
EXPERIMENTS,0.6479099678456591,"in the middle third dodge projectiles. Finally, the weights in the bottom third avoid asteroids near
299"
EXPERIMENTS,0.6495176848874598,"the agent. Notably, the agent trained in the more stringent reward condition (h = 1) places greater
300"
EXPERIMENTS,0.6511254019292605,"weight on dodging projectiles, showing the qualitative impact of reward on learned policy. Hence
301"
EXPERIMENTS,0.6527331189710611,"similar qualitative phenomena as in our theoretical model can arise in more general settings.
302"
CONCLUDING PERSPECTIVES,0.6543408360128617,"4
Concluding perspectives
303"
CONCLUDING PERSPECTIVES,0.6559485530546624,"The RL perceptron provides a framework to investigate high-dimensional policy gradient learning in
304"
CONCLUDING PERSPECTIVES,0.657556270096463,"RL for a range of plausible sparse reward structures. We derive closed ODEs that capture the average-
305"
CONCLUDING PERSPECTIVES,0.6591639871382636,"case learning dynamics in high-dimensional settings. The reduction of the high-dimensional learning
306"
CONCLUDING PERSPECTIVES,0.6607717041800643,"dynamics to a low-dimensional set of differential equations permits a precise, quantitative analysis
307"
CONCLUDING PERSPECTIVES,0.662379421221865,"of learning behaviours: computing optimal hyper-parameter schedules, or tracing out phase diagrams
308"
CONCLUDING PERSPECTIVES,0.6639871382636656,"of learnability. Our framework offers a starting point to explore additional settings that are closer
309"
CONCLUDING PERSPECTIVES,0.6655948553054662,"to many real-world RL scenarios, such as those with conditional next states. Furthermore, the RL
310"
CONCLUDING PERSPECTIVES,0.6672025723472669,"perceptron offers a means to study common training practices, including curricula; and more advanced
311"
CONCLUDING PERSPECTIVES,0.6688102893890675,"algorithms, like actor-critic methods. We hope to extract more analytical insights from the ODEs,
312"
CONCLUDING PERSPECTIVES,0.6704180064308681,"particularly on how initialization and learning rate influence an agent’s learning regime. Our findings
313"
CONCLUDING PERSPECTIVES,0.6720257234726688,"emphasize the intricate interplay of task, reward, architecture, and algorithm in modern RL systems.
314"
REFERENCES,0.6736334405144695,"References
315"
REFERENCES,0.6752411575562701,"Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity
316"
REFERENCES,0.6768488745980707,"and representation learning of low rank mdps. Advances in neural information processing systems,
317"
REFERENCES,0.6784565916398714,"33:20095–20107, 2020.
318"
REFERENCES,0.680064308681672,"Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy
319"
REFERENCES,0.6816720257234726,"gradient methods: Optimality, approximation, and distribution shift. The Journal of Machine
320"
REFERENCES,0.6832797427652733,"Learning Research, 22(1):4431–4506, 2021.
321"
REFERENCES,0.684887459807074,"Andrea Agazzi and Jianfeng Lu. Global optimality of softmax policy gradient with single hidden layer
322"
REFERENCES,0.6864951768488746,"neural networks in the mean-field regime. In International Conference on Learning Representations,
323"
REFERENCES,0.6881028938906752,"2021. URL https://openreview.net/forum?id=bB2drc7DPuB.
324"
REFERENCES,0.6897106109324759,"Andrea Agazzi and Jianfeng Lu. Temporal-difference learning with nonlinear function approximation:
325"
REFERENCES,0.6913183279742765,"lazy training and mean field regimes. In Joan Bruna, Jan Hesthaven, and Lenka Zdeborova, editors,
326"
REFERENCES,0.6929260450160771,"Proceedings of the 2nd Mathematical and Scientific Machine Learning Conference, volume 145
327"
REFERENCES,0.6945337620578779,"of Proceedings of Machine Learning Research, pages 37–74. PMLR, 16–19 Aug 2022. URL
328"
REFERENCES,0.6961414790996785,"https://proceedings.mlr.press/v145/agazzi22a.html.
329"
REFERENCES,0.6977491961414791,"Luca Arnaboldi, Ludovic Stephan, Florent Krzakala, and Bruno Loureiro. From high-dimensional &
330"
REFERENCES,0.6993569131832797,"mean-field dynamics to dimensionless odes: A unifying approach to sgd in two-layers networks.
331"
REFERENCES,0.7009646302250804,"arXiv preprint arXiv:2302.05882, 2023.
332"
REFERENCES,0.702572347266881,"Haruka Asanuma, Shiro Takagi, Yoshihiro Nagano, Yuki Yoshida, Yasuhiko Igarashi, and Masato
333"
REFERENCES,0.7041800643086816,"Okada. Statistical mechanical analysis of catastrophic forgetting in continual learning with teacher
334"
REFERENCES,0.7057877813504824,"and student networks. Journal of the Physical Society of Japan, 90(10):104001, 2021.
335"
REFERENCES,0.707395498392283,"Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement
336"
REFERENCES,0.7090032154340836,"learning with value-targeted regression. In International Conference on Machine Learning, pages
337"
REFERENCES,0.7106109324758842,"463–474. PMLR, 2020.
338"
REFERENCES,0.7122186495176849,"Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for rein-
339"
REFERENCES,0.7138263665594855,"forcement learning. In International Conference on Machine Learning, pages 263–272. PMLR,
340"
REFERENCES,0.7154340836012861,"2017.
341"
REFERENCES,0.7170418006430869,"Yasaman Bahri, Jonathan Kadmon, Jeffrey Pennington, Sam S. Schoenholz, Jascha Sohl-Dickstein,
342"
REFERENCES,0.7186495176848875,"and Surya Ganguli. Statistical mechanics of deep learning. Annual Review of Condensed Matter
343"
REFERENCES,0.7202572347266881,"Physics, 11(1):501–528, 2020. doi: 10.1146/annurev-conmatphys-031119-050745. URL https:
344"
REFERENCES,0.7218649517684887,"//doi.org/10.1146/annurev-conmatphys-031119-050745.
345"
REFERENCES,0.7234726688102894,"Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
346"
REFERENCES,0.72508038585209,"structural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.
347"
REFERENCES,0.7266881028938906,"Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
348"
REFERENCES,0.7282958199356914,"Unifying count-based exploration and intrinsic motivation. Advances in neural information
349"
REFERENCES,0.729903536977492,"processing systems, 29, 2016.
350"
REFERENCES,0.7315112540192926,"Jalaj Bhandari and Daniel Russo. Global optimality guarantees for policy gradient methods. arXiv
351"
REFERENCES,0.7331189710610932,"preprint arXiv:1906.01786, 2019.
352"
REFERENCES,0.7347266881028939,"Michael Biehl and Holm Schwarze. Learning by on-line gradient descent. Journal of Physics A:
353"
REFERENCES,0.7363344051446945,"Mathematical and general, 28(3):643, 1995.
354"
REFERENCES,0.7379421221864951,"Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference learning
355"
REFERENCES,0.7395498392282959,"converges to global optima. Advances in Neural Information Processing Systems, 32, 2019.
356"
REFERENCES,0.7411575562700965,"Giuseppe Carleo, Ignacio Cirac, Kyle Cranmer, Laurent Daudet, Maria Schuld, Naftali Tishby, Leslie
357"
REFERENCES,0.7427652733118971,"Vogt-Maranto, and Lenka Zdeborová. Machine learning and the physical sciences. Reviews of
358"
REFERENCES,0.7443729903536977,"Modern Physics, 91(4):045002, 2019.
359"
REFERENCES,0.7459807073954984,"L. Chizat and F. Bach. On the global convergence of gradient descent for over-parameterized
360"
REFERENCES,0.747588424437299,"models using optimal transport. In Advances in Neural Information Processing Systems 31, pages
361"
REFERENCES,0.7491961414790996,"3040–3050, 2018.
362"
REFERENCES,0.7508038585209004,"Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
363"
REFERENCES,0.752411575562701,"trained with the logistic loss. In Conference on Learning Theory, pages 1305–1338. PMLR, 2020.
364"
REFERENCES,0.7540192926045016,"Lénaïc Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
365"
REFERENCES,0.7556270096463023,"In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché Buc, E. Fox, and R. Garnett, editors,
366"
REFERENCES,0.7572347266881029,"Advances in Neural Information Processing Systems 32, pages 2933–2943. Curran Associates, Inc.,
367"
REFERENCES,0.7588424437299035,"2019.
368"
REFERENCES,0.7604501607717041,"Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation
369"
REFERENCES,0.7620578778135049,"to benchmark reinforcement learning. arXiv preprint arXiv:1912.01588, 2019.
370"
REFERENCES,0.7636655948553055,"William M. Dabney. Adaptive step-sizes for reinforcement learning. 2014.
371"
REFERENCES,0.7652733118971061,"Gal Dalal, Balázs Szörényi, Gugan Thoppe, and Shie Mannor. Concentration bounds for two timescale
372"
REFERENCES,0.7668810289389068,"stochastic approximation with applications to reinforcement learning. CoRR, abs/1703.05376,
373"
REFERENCES,0.7684887459807074,"2017. URL http://arxiv.org/abs/1703.05376.
374"
REFERENCES,0.770096463022508,"Stéphane d’Ascoli, Maria Refinetti, and Giulio Biroli. Optimal learning rate schedules in high-
375"
REFERENCES,0.7717041800643086,"dimensional non-convex optimization problems, 2022.
376"
REFERENCES,0.7733118971061094,"Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
377"
REFERENCES,0.77491961414791,"Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex
378"
REFERENCES,0.7765273311897106,"optimization. Advances in neural information processing systems, 27, 2014.
379"
REFERENCES,0.7781350482315113,"Oussama Dhifallah and Yue M Lu. Phase transitions in transfer learning for high-dimensional
380"
REFERENCES,0.7797427652733119,"perceptrons. Entropy, 23(4):400, 2021.
381"
REFERENCES,0.7813504823151125,"Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. Provably efficient reinforcement learning with
382"
REFERENCES,0.7829581993569131,"aggregated states. arXiv preprint arXiv:1912.06366, 2019.
383"
REFERENCES,0.7845659163987139,"Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford.
384"
REFERENCES,0.7861736334405145,"Provably efficient rl with rich observations via latent state decoding. In International Conference
385"
REFERENCES,0.7877813504823151,"on Machine Learning, pages 1665–1674. PMLR, 2019a.
386"
REFERENCES,0.7893890675241158,"S.S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient descent provably optimizes over-parameterized
387"
REFERENCES,0.7909967845659164,"neural networks. In International Conference on Learning Representations, 2019b.
388"
REFERENCES,0.792604501607717,"Andreas Engel and Christian Van den Broeck. Statistical mechanics of learning. Cambridge
389"
REFERENCES,0.7942122186495176,"University Press, 2001.
390"
REFERENCES,0.7958199356913184,"Marylou Gabrié, Surya Ganguli, Carlo Lucibello, and Riccardo Zecchina. Neural networks: from the
391"
REFERENCES,0.797427652733119,"perceptron to deep nets. arXiv preprint arXiv:2304.06636, 2023.
392"
REFERENCES,0.7990353697749196,"David Gamarnik, Cristopher Moore, and Lenka Zdeborová . Disordered systems insights on compu-
393"
REFERENCES,0.8006430868167203,"tational hardness. Journal of Statistical Mechanics: Theory and Experiment, 2022(11):114015,
394"
REFERENCES,0.8022508038585209,"nov 2022. doi: 10.1088/1742-5468/ac9cc8. URL https://doi.org/10.1088%2F1742-5468%
395"
REFERENCES,0.8038585209003215,"2Fac9cc8.
396"
REFERENCES,0.8054662379421221,"Elizabeth Gardner and Bernard Derrida. Three unfinished works on the optimal storage capacity of
397"
REFERENCES,0.8070739549839229,"networks. Journal of Physics A: Mathematical and General, 22(12):1983, 1989.
398"
REFERENCES,0.8086816720257235,"Federica Gerace, Luca Saglietti, Stefano Sarao Mannelli, Andrew Saxe, and Lenka Zdeborová.
399"
REFERENCES,0.8102893890675241,"Probing transfer learning with a model of synthetic correlated datasets. Machine Learning: Science
400"
REFERENCES,0.8118971061093248,"and Technology, 3(1):015030, 2022.
401"
REFERENCES,0.8135048231511254,"Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy
402"
REFERENCES,0.815112540192926,"training of two-layers neural network. In Advances in Neural Information Processing Systems,
403"
REFERENCES,0.8167202572347267,"volume 32, pages 9111–9121, 2019.
404"
REFERENCES,0.8183279742765274,"Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural
405"
REFERENCES,0.819935691318328,"networks outperform kernel methods? In Advances in Neural Information Processing Systems,
406"
REFERENCES,0.8215434083601286,"volume 33, 2020.
407"
REFERENCES,0.8231511254019293,"Sebastian Goldt, Madhu Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborová. Dynamics
408"
REFERENCES,0.8247588424437299,"of stochastic gradient descent for two-layer neural networks in the teacher-student setup. Advances
409"
REFERENCES,0.8263665594855305,"in neural information processing systems, 32, 2019.
410"
REFERENCES,0.8279742765273312,"Vijaykumar Gullapalli and Andrew G Barto. Shaping as a method for accelerating reinforcement
411"
REFERENCES,0.8295819935691319,"learning. In Proceedings of the 1992 IEEE international symposium on intelligent control, pages
412"
REFERENCES,0.8311897106109325,"554–559. IEEE, 1992.
413"
REFERENCES,0.8327974276527331,"A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural
414"
REFERENCES,0.8344051446945338,"networks. In Advances in Neural Information Processing Systems 32, pages 8571–8580, 2018.
415"
REFERENCES,0.8360128617363344,"Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contex-
416"
REFERENCES,0.837620578778135,"tual decision processes with low bellman rank are pac-learnable. In International Conference on
417"
REFERENCES,0.8392282958199357,"Machine Learning, pages 1704–1713. PMLR, 2017.
418"
REFERENCES,0.8408360128617364,"Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
419"
REFERENCES,0.842443729903537,"learning with linear function approximation. In Conference on Learning Theory, pages 2137–2143.
420"
REFERENCES,0.8440514469453376,"PMLR, 2020.
421"
REFERENCES,0.8456591639871383,"Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl
422"
REFERENCES,0.8472668810289389,"problems, and sample-efficient algorithms. Advances in neural information processing systems,
423"
REFERENCES,0.8488745980707395,"34:13406–13418, 2021.
424"
REFERENCES,0.8504823151125402,"W. Kinzel and P. Ruján. Improving a Network Generalization Ability by Selecting Examples. EPL
425"
REFERENCES,0.8520900321543409,"(Europhysics Letters), 13(5):473–477, 1990.
426"
REFERENCES,0.8536977491961415,"Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rocktäschel. A Survey of Zero-shot
427"
REFERENCES,0.8553054662379421,"Generalisation in Deep Reinforcement Learning. Journal of Artificial Intelligence Research, 76:
428"
REFERENCES,0.8569131832797428,"201–264, January 2023. ISSN 1076-9757. doi: 10.1613/jair.1.14174. URL https://jair.org/
429"
REFERENCES,0.8585209003215434,"index.php/jair/article/view/14174.
430"
REFERENCES,0.860128617363344,"Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Pac reinforcement learning with rich
431"
REFERENCES,0.8617363344051447,"observations. Advances in Neural Information Processing Systems, 29, 2016.
432"
REFERENCES,0.8633440514469454,"Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and transfer
433"
REFERENCES,0.864951768488746,"learning in deep linear networks. arXiv preprint arXiv:1809.10374, 2018.
434"
REFERENCES,0.8665594855305466,"Sebastian Lee, Sebastian Goldt, and Andrew Saxe. Continual learning in the teacher-student setup:
435"
REFERENCES,0.8681672025723473,"Impact of task similarity. In International Conference on Machine Learning, pages 6109–6119.
436"
REFERENCES,0.8697749196141479,"PMLR, 2021.
437"
REFERENCES,0.8713826366559485,"Sebastian Lee, Stefano Sarao Mannelli, Claudia Clopath, Sebastian Goldt, and Andrew Saxe.
438"
REFERENCES,0.8729903536977492,"Maslow’s hammer for catastrophic forgetting: Node re-use vs node activation. arXiv preprint
439"
REFERENCES,0.8745980707395499,"arXiv:2205.09029, 2022.
440"
REFERENCES,0.8762057877813505,"Peter Marbach and John N. Tsitsiklis. Approximate gradient methods in policy-space optimization of
441"
REFERENCES,0.8778135048231511,"markov reward processes. Discrete Event Dynamic Systems, 13:111–148, 2003.
442"
REFERENCES,0.8794212218649518,"Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
443"
REFERENCES,0.8810289389067524,"layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665–E7671,
444"
REFERENCES,0.882636655948553,"2018.
445"
REFERENCES,0.8842443729903537,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
446"
REFERENCES,0.8858520900321544,"Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
447"
REFERENCES,0.887459807073955,"through deep reinforcement learning. nature, 518(7540):529–533, 2015.
448"
REFERENCES,0.8890675241157556,"Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement
449"
REFERENCES,0.8906752411575563,"learning using linearly combined model ensembles. In International Conference on Artificial
450"
REFERENCES,0.8922829581993569,"Intelligence and Statistics, pages 2010–2020. PMLR, 2020.
451"
REFERENCES,0.8938906752411575,"Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E. Taylor, and Peter Stone.
452"
REFERENCES,0.8954983922829582,"Curriculum learning for reinforcement learning domains: A framework and survey, 2020.
453"
REFERENCES,0.8971061093247589,"Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:
454"
REFERENCES,0.8987138263665595,"Theory and application to reward shaping. In Icml, volume 99, pages 278–287. Citeseer, 1999.
455"
REFERENCES,0.9003215434083601,"Matteo Pirotta, Marcello Restelli, and Luca Bascetta.
Adaptive step-size for policy gradient
456"
REFERENCES,0.9019292604501608,"methods. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, ed-
457"
REFERENCES,0.9035369774919614,"itors, Advances in Neural Information Processing Systems, volume 26. Curran Associates,
458"
REFERENCES,0.905144694533762,"Inc., 2013.
URL https://proceedings.neurips.cc/paper_files/paper/2013/file/
459"
REFERENCES,0.9067524115755627,"f64eac11f2cd8f0efa196f8ad173178e-Paper.pdf.
460"
REFERENCES,0.9083601286173634,"Maria Refinetti, Sebastian Goldt, Florent Krzakala, and Lenka Zdeborova.
Classifying high-
461"
REFERENCES,0.909967845659164,"dimensional gaussian mixtures: Where kernel methods fail and neural networks succeed. In Marina
462"
REFERENCES,0.9115755627009646,"Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine
463"
REFERENCES,0.9131832797427653,"Learning, volume 139 of Proceedings of Machine Learning Research, pages 8936–8947. PMLR,
464"
REFERENCES,0.9147909967845659,"18–24 Jul 2021. URL https://proceedings.mlr.press/v139/refinetti21b.html.
465"
REFERENCES,0.9163987138263665,"Federico Ricci-Tersenghi, Guilhem Semerjian, and Lenka Zdeborová. Typology of phase transitions
466"
REFERENCES,0.9180064308681672,"in bayesian inference problems. Physical Review E, 99(4):042109, 2019.
467"
REFERENCES,0.9196141479099679,"F. Rosenblatt. Principles ofNeurodynamics. Spartan, New York, 1962.
468"
REFERENCES,0.9212218649517685,"G.M. Rotskoff and E. Vanden-Eijnden. Parameters as interacting particles: long time convergence
469"
REFERENCES,0.9228295819935691,"and asymptotic error scaling of neural networks. In Advances in Neural Information Processing
470"
REFERENCES,0.9244372990353698,"Systems 31, pages 7146–7155, 2018. URL http://arxiv.org/abs/1805.00915.
471"
REFERENCES,0.9260450160771704,"Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic
472"
REFERENCES,0.927652733118971,"exploration. Advances in Neural Information Processing Systems, 26, 2013.
473"
REFERENCES,0.9292604501607717,"David Saad and Sara A Solla. On-line learning in soft committee machines. Physical Review E, 52
474"
REFERENCES,0.9308681672025724,"(4):4225, 1995.
475"
REFERENCES,0.932475884244373,"Luca Saglietti, Stefano Sarao Mannelli, and Andrew Saxe. An analytical theory of curriculum
476"
REFERENCES,0.9340836012861736,"learning in teacher–student networks. Journal of Statistical Mechanics: Theory and Experiment,
477"
REFERENCES,0.9356913183279743,"2022(11):114014, 2022.
478"
REFERENCES,0.9372990353697749,"John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
479"
REFERENCES,0.9389067524115756,"continuous control using generalized advantage estimation, 2015. URL https://arxiv.org/
480"
REFERENCES,0.9405144694533762,"abs/1506.02438.
481"
REFERENCES,0.9421221864951769,"Hyunjune Sebastian Seung, Haim Sompolinsky, and Naftali Tishby. Statistical mechanics of learning
482"
REFERENCES,0.9437299035369775,"from examples. Physical review A, 45(8):6056, 1992.
483"
REFERENCES,0.9453376205787781,"David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
484"
REFERENCES,0.9469453376205788,"Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
485"
REFERENCES,0.9485530546623794,"the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016.
486"
REFERENCES,0.9501607717041801,"Ben Sorscher, Surya Ganguli, and Haim Sompolinsky. Neural representational geometry under-
487"
REFERENCES,0.9517684887459807,"lies few-shot concept learning. Proceedings of the National Academy of Sciences, 119(43):
488"
REFERENCES,0.9533762057877814,"e2200800119, 2022. doi: 10.1073/pnas.2200800119. URL https://www.pnas.org/doi/abs/
489"
REFERENCES,0.954983922829582,"10.1073/pnas.2200800119.
490"
REFERENCES,0.9565916398713826,"R. S. Sutton, D. Mcallester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement
491"
REFERENCES,0.9581993569131833,"learning with function approximation. In Advances in Neural Information Processing Systems 12,
492"
REFERENCES,0.9598070739549839,"volume 12, pages 1057–1063. MIT Press, 2000.
493"
REFERENCES,0.9614147909967846,"Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of
494"
REFERENCES,0.9630225080385852,"events to their probabilities. Measures of complexity: festschrift for alexey chervonenkis, pages
495"
REFERENCES,0.9646302250803859,"11–30, 2015.
496"
REFERENCES,0.9662379421221865,"Rodrigo Veiga, Ludovic STEPHAN, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova.
497"
REFERENCES,0.9678456591639871,"Phase diagram of stochastic gradient descent in high-dimensional two-layer neural networks.
498"
REFERENCES,0.9694533762057878,"In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in
499"
REFERENCES,0.9710610932475884,"Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=
500"
REFERENCES,0.9726688102893891,"GL-3WEdNRM.
501"
REFERENCES,0.9742765273311897,"Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features.
502"
REFERENCES,0.9758842443729904,"In International Conference on Machine Learning, pages 6995–7004. PMLR, 2019.
503"
REFERENCES,0.977491961414791,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
504"
REFERENCES,0.9790996784565916,"learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–115,
505"
REFERENCES,0.9807073954983923,"2021.
506"
REFERENCES,0.9823151125401929,"Xuezhou Zhang, Yuda Song, Masatoshi Uehara, Mengdi Wang, Alekh Agarwal, and Wen Sun.
507"
REFERENCES,0.9839228295819936,"Efficient reinforcement learning in block mdps: A model-free representation learning approach. In
508"
REFERENCES,0.9855305466237942,"International Conference on Machine Learning, pages 26517–26547. PMLR, 2022.
509"
REFERENCES,0.9871382636655949,"Yufeng Zhang, Qi Cai, Zhuoran Yang, Yongxin Chen, and Zhaoran Wang. Can temporal-difference
510"
REFERENCES,0.9887459807073955,"and q-learning learn representation? a mean-field theory. In H. Larochelle, M. Ranzato, R. Hadsell,
511"
REFERENCES,0.9903536977491961,"M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,
512"
REFERENCES,0.9919614147909968,"pages 19680–19692. Curran Associates, Inc., 2020a. URL https://proceedings.neurips.
513"
REFERENCES,0.9935691318327974,"cc/paper_files/paper/2020/file/e3bc4e7f243ebc05d66a0568a3331966-Paper.pdf.
514"
REFERENCES,0.9951768488745981,"Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learningvia
515"
REFERENCES,0.9967845659163987,"reference-advantage decomposition. Advances in Neural Information Processing Systems, 33:
516"
REFERENCES,0.9983922829581994,"15198–15207, 2020b.
517"
