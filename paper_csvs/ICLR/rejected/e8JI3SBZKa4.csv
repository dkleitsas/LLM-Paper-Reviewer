Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0033222591362126247,"Recent works have derived neural networks with online correlation-based learn-
ing rules to perform kernel similarity matching. These works applied existing
linear similarity matching algorithms to nonlinear features generated with random
Fourier methods. In this paper attempt to perform kernel similarity matching by
directly learning the nonlinear features. Our algorithm proceeds by deriving and
then minimizing an upper bound for the sum of squared errors between output
and input kernel similarities. The construction of our upper bound leads to online
correlation-based learning rules which can be implemented with a 1 layer recur-
rent neural network. In addition to generating high-dimensional linearly separa-
ble representations, we show that our upper bound naturally yields representations
which are sparse and selective for speciﬁc input patterns. We compare the approx-
imation quality of our method to neural random Fourier method and variants of
the popular but non-biological “Nystr¨om” method for approximating the kernel
matrix. Our method appears to be comparable or better than randomly sampled
Nystr¨om methods when the outputs are relatively low dimensional (although still
potentially higher dimensional than the inputs) but less faithful when the outputs
are very high dimensional."
INTRODUCTION,0.006644518272425249,"1
INTRODUCTION"
INTRODUCTION,0.009966777408637873,"Brain inspired learning algorithms have a long history in the ﬁeld of neural networks and machine
learning (Rosenblatt, 1958; Olshausen & Field, 1996; Lee & Seung, 1999; Riesenhuber & Poggio,
1999; Hinton, 2007; Lillicrap et al., 2016). While many algorithms have diverged from their bi-
ological roots, the motivation to study biology remains clear: the human brain is such a powerful
learning agent, there must be insights to be gained by making our algorithms look “brain-like”. This
paper is focused on merging biological constraints with the well-established ﬁeld of kernel-based
machine learning."
INTRODUCTION,0.013289036544850499,"A common assumption in brain-inspired models of learning is that synaptic update rules should be
a) online, meaning the algorithm only has access to a single input pattern at a time and b) local,
meaning synapses should only be modiﬁed using information immediately available to the synapse,
often just the pre- and post-ﬁring rates of the neurons to which it is connected. Learning rules with
these properties are commonly referred to as Hebbian learning rules (Chklovskii, 2016)."
INTRODUCTION,0.016611295681063124,"Recent works have devised neural networks with Hebbian learning rules that perform linear simi-
larity matching. These networks map every input xt to a representation yt such that linear output
similarities match linear input similarities ys·yt ≈xs·xt. These networks are interesting as models
for real brains because they display a number of interesting biological properties: they are recurrent
networks with correlation-based learning rules (Pehlevan et al., 2018) and can be modiﬁed to in-
clude non-negativity (Pehlevan & Chklovskii, 2014), sparsity, and convolutional structure (Obeid
et al., 2019)."
INTRODUCTION,0.019933554817275746,"However there is a problem if one believes these networks should ultimately generate representations
which are useful for downstream tasks. If similarities are actually matched, that is if ys·yt = xs·xt,
then the outputs are simply an orthogonal transformation of the inputs, yt = Qxt, which is unlikely
to have signiﬁcant impact on downstream tasks. Bahroun et al. (2017) identiﬁed this problem and
proposed a solution: instead of matching linear input similarities, one can match nonlinear input"
INTRODUCTION,0.023255813953488372,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.026578073089700997,"similarities: ys · yt ≈f(xs, xt). The authors provided a method that can be applied to any shift-
invariant kernel. They applied the random Fourier feature method of Rahimi et al. (2007) to map
inputs to nonlinear feature vectors x →ψ and then applied the linear similarity matching framework
of Pehlevan et al. (2018) to these nonlinear features."
INTRODUCTION,0.029900332225913623,"In this paper, tackle the same neural kernel similarity matching problem with a different approach.
Instead of using random nonlinear features, we directly optimize for the features with Hebbian
learning rules that resemble the learning rules derived in the original works on linear similarity
matching. To derive our learning rules, we show that for any kernel we can upper bound the sum of
squared errors |ys · yt −f(xs, xt)|2 with a correlation-based energy. Gradient-based optimization
of our upper bound with will lead to a neural network with correlation-based learning rules."
CORRELATION-BASED BOUND FOR KERNEL SIMILARITY MATCHING,0.03322259136212625,"2
CORRELATION-BASED BOUND FOR KERNEL SIMILARITY MATCHING"
CORRELATION-BASED BOUND FOR KERNEL SIMILARITY MATCHING,0.036544850498338874,"Roadmap for this section We ﬁrst deﬁne the kernel similarity matching problem (Eq. 1). We
then derive a correlation-based optimization (Eq. 6) which is an upper bound for to Eq. 1 (up to a
constant that does not depend on the representations). We then use a Legendre transform to derive
an equivalent (except for the numerical stability parameter λ) optimization problem in Eq. 9 that
will lend itself towards online updates."
CORRELATION-BASED BOUND FOR KERNEL SIMILARITY MATCHING,0.03986710963455149,"Kernel similarity matching Assume we are given a set of input vectors {xt ∈RM}T
t=1 and a
positive semi-deﬁnite kernel function f : RM × RM →R which deﬁnes the similarity between
input vectors. The goal is to ﬁnd a corresponding set of representations {yt ∈RN}T
t=1 such that for
all pairs (s, t) we have ys ·yt ≈f(xs, xt). We will assume that T ≫N > M: the dimensionalities
of x, y are much lower than the number of samples T, but y are still higher dimensional than the
inputs. This is formalized by minimizing the sum of squared errors:"
CORRELATION-BASED BOUND FOR KERNEL SIMILARITY MATCHING,0.04318936877076412,"min
{yt}
1
T 2 T
X s,t"
CORRELATION-BASED BOUND FOR KERNEL SIMILARITY MATCHING,0.046511627906976744,"
f(xs, xt) −ys · yt2
(1)"
CORRELATION-BASED BOUND FOR KERNEL SIMILARITY MATCHING,0.04983388704318937,"This is known as the classical multidimensional scaling objective (Borg & Groenen, 2005). For
arbitrary nonlinearity f this can be solved exactly by ﬁnding the top N eigenvectors of the T × T
input similarity matrix (Borg & Groenen, 2005), and is therefore closely related to kernel PCA
(Sch¨olkopf et al., 1997). However, this requires computing and storing similarities for all pairs of
input vectors which breaks the online constraint that we require for biological realism. The purpose
of this paper is to ﬁnd an online algorithm, with correlation-based computations, that can at least
approximately minimize Eq. 1."
CORRELATION-BASED BOUND FOR KERNEL SIMILARITY MATCHING,0.053156146179401995,"Correlation based upper bound In this section we provide an upper bound to Eq. 1 which does not
require computing f(xs, xt) for any (s, t). The ﬁrst step is to expand the square in Eq (1) to yield: 1
T 2 T
X s,t"
CORRELATION-BASED BOUND FOR KERNEL SIMILARITY MATCHING,0.05647840531561462,"
f(xs, xt) −ys · yt2 = −2 T 2
X"
CORRELATION-BASED BOUND FOR KERNEL SIMILARITY MATCHING,0.059800664451827246,"s,t
f(xs, xt)ys · yt + 1 T 2
X"
CORRELATION-BASED BOUND FOR KERNEL SIMILARITY MATCHING,0.06312292358803986,"s,t
(ys · yt)2 + const
(2)"
CORRELATION-BASED BOUND FOR KERNEL SIMILARITY MATCHING,0.0664451827242525,"We will now show how to bound the ﬁrst term on the right hand side.
Theorem 1. If f is a positive semideﬁnite kernel function, then
1
2T 2
X"
CORRELATION-BASED BOUND FOR KERNEL SIMILARITY MATCHING,0.06976744186046512,"s,t
ysytf(xs, xt) ≥1 T X"
CORRELATION-BASED BOUND FOR KERNEL SIMILARITY MATCHING,0.07308970099667775,"t
qytf(xt, w) −1"
CORRELATION-BASED BOUND FOR KERNEL SIMILARITY MATCHING,0.07641196013289037,"2q2f(w, w)
(3)"
CORRELATION-BASED BOUND FOR KERNEL SIMILARITY MATCHING,0.07973421926910298,for all q and w.
CORRELATION-BASED BOUND FOR KERNEL SIMILARITY MATCHING,0.08305647840531562,"Proof. Because f is a positive semi-deﬁnite kernel, we can assign to any set of M-dimensional vec-
tors {w, x1, . . . , xT }, a corresponding set of (at most) T +1-dimensional vectors {φw, φ1, . . . , φT }
whose inner products yield the similarity deﬁned by f:
φt · φt′ = f(xt, xt′)
φt · φw = f(xt, w)
φw · φw = f(w, w)
(4)
Now consider the vector difference 1 T
P"
CORRELATION-BASED BOUND FOR KERNEL SIMILARITY MATCHING,0.08637873754152824,"t ytφt −qφw. The squared norm of this difference is of
course non-negative. Additionally we can expand out this square: 0 ≤1 2"
T,0.08970099667774087,"1
T X"
T,0.09302325581395349,"t
ytφt −qφw  2"
T,0.09634551495016612,"=
1
2T 2
X"
T,0.09966777408637874,"s,t
ysytφs · φt −1 T X"
T,0.10299003322259136,"t
qytφt · φw + 1"
T,0.10631229235880399,"2q2φw · φw
(5)"
T,0.10963455149501661,Under review as a conference paper at ICLR 2022
T,0.11295681063122924,"Figure 1: Neural network implementation of the optimization in Eq. 9 (a) network architecture
(b) recurrent network dynamics (c) steady state network response (d) Hebbian update rules for the
special case of Gaussian kernels (the precise form of these updates will be depend on the kernel)"
T,0.11627906976744186,"At this point we can simply replace all dot products with the equivalent nonlinear similarities f(·, ·)
in Equation 20 and rearrange the terms to yield our key inequality (Eq. 3)."
T,0.11960132890365449,"Our inequality still holds if we maximize the right hand side with respect to q and w. For every
index i of y, we ﬁnd the optimal wi, qi, and then replace the ﬁrst pairwise sum in Eq. (2) with our
upper bounds. Additionally we rearrange the order of the summations in second term on the right
hand side of Eq. (2) to yield the following upper bound for the y-dependent terms in Eq. (2):"
T,0.12292358803986711,"min
yt min
qi,wi −1 T T
X t=1 N
X i=1"
T,0.12624584717607973,"
qiyt
if(wi, xt) −1"
T,0.12956810631229235,"2q2
i f(wi, wi)

+ 1 4 N
X i,j=1"
T,0.132890365448505,"1
T T
X"
T,0.1362126245847176,"t=1
yt
iyt
j !2 (6)"
T,0.13953488372093023,"Online focused reformulation We can further remove the square of the correlation matrix
1
T
P"
T,0.14285714285714285,"t yt
iyt
j (another impediment to online learning) by introducing a Legendre transformation:
1
2C2
ij →maxLij CijLij −1"
T,0.1461794019933555,"2L2
ij:"
T,0.14950166112956811,"min
W,Y,q max
L
1
T T
X t=1  − N
X i=1"
T,0.15282392026578073,"
qiyt
if(wi, xt) −1"
T,0.15614617940199335,"2q2
i f(wi, wi)

+ 1 2 N
X i,j=1"
T,0.15946843853820597,"
Lijyt
iyt
j −1"
T,0.16279069767441862,"2L2
ij  (7)"
T,0.16611295681063123,"We can swap the order of the y and L optimizations, because the objective obeys the strong min-
max property with W, q ﬁxed (Appendix Section A of Pehlevan et al. (2018)). We add one ﬁnal
term
λ
NT
PT
t=1
PN
i=1(yt
i)2 to the objective, which can be important for numerical stability of our
resulting algorithm. In our experiments λ = 0.001. Finally, to better motivate our online algorithm,
we deﬁne the “per-sample-energy”: et := N
X"
T,0.16943521594684385,"i=1
−

qiyt
if(wi, xt) −1"
T,0.17275747508305647,"2q2
i f(wi, wi)

+ 1 2 N
X i,j=1"
T,0.1760797342192691,"
Lijyt
iyt
j −1"
T,0.17940199335548174,"2L2
ij 
+ λ 2 N
X"
T,0.18272425249169436,"i=1
(yt
i)2
(8)"
T,0.18604651162790697,"where et := e(yt, xt; W, q, L). The ﬁnal optimization we will perform, which is equivalent to the
optimization in Eq. 6, and is derived as an upper bound to Eq. 1, is thus:"
T,0.1893687707641196,"min
W,q max
L
min
Y
1
T T
X"
T,0.19269102990033224,"t=1
e(yt, xt; W, q, L)
(9)"
NEURAL NETWORK OPTIMIZATION,0.19601328903654486,"3
NEURAL NETWORK OPTIMIZATION"
NEURAL NETWORK OPTIMIZATION,0.19933554817275748,"Applying a stochastic gradient descent-ascent algorithm to Eq. (9) yields a neural network (Fig.
1) in which yt
i is the response of neuron i to input pattern t, wi is the vector of incoming connec-
tions to neuron i from the input layer, qi is a term which modulates the strength of these incoming
connections, and Lij is a matrix of lateral recurrent connections between outputs."
NEURAL NETWORK OPTIMIZATION,0.2026578073089701,"Speciﬁcally the neural algorithm procedes as follows. We initialize Wia ←N(0, 1), qi ←1 and
Lij ←Iij. At each iteration sample a minibatch of inputs {xb}. Using Eq.12 we compute the {yb}"
NEURAL NETWORK OPTIMIZATION,0.2059800664451827,Under review as a conference paper at ICLR 2022
NEURAL NETWORK OPTIMIZATION,0.20930232558139536,"which minimize Eq. 9 for ﬁxed synapses. Using these optimal {yb}, we compute the minibatch
energy e =
1
B
P"
NEURAL NETWORK OPTIMIZATION,0.21262458471760798,"b e(xb, yb; W, q, L) and take a gradient descent step for q, a rescaled gradient
descent step for w and a gradient ascent step for L:"
NEURAL NETWORK OPTIMIZATION,0.2159468438538206,"wi ←wi −ηw q2
i"
NEURAL NETWORK OPTIMIZATION,0.21926910299003322,"∂e
∂wi
qi ←qi −ηq
∂e
∂qi
Lij ←Lij + ηl
∂e
∂Lij
(10)"
NEURAL NETWORK OPTIMIZATION,0.22259136212624583,"Convergence of the neural algorithm We treat convergence of this gradient descent ascent al-
gorithm as an empirical issue. We adopt the ”two time scale” strategy that has shown empirical
successes for training generative adversarial networks (Heusel et al., 2017). We choose the learning
rates such that ηq, ηw ≪ηl. Intuitively when choosing ηl to be large, the Lij can approximately
maximize Eq. 9 for any particular q, W so that the min-max ordering is roughly preserved. In prac-
tice this ratio is important for convergence. We do not observe convergence when the ratios ηw/ηl
or ηq/ηl are large. Unfortunately it is an empirical question of what is “too large”. If we could show
that the objective were concave in L, it can be gradient descent ascent with smaller learning rates
for W, q would indeed converge to a saddle point (Lin et al., 2020; Seung, 2019). However, this
question will have to be left for future work."
NEURAL NETWORK OPTIMIZATION,0.22591362126245848,"Empirically it is sometimes observed that qi quickly shrinks to a small value early in training, which
subsequently leads to small gradients for w. The rescaling of the wi updates provides an adaptive
learning rate that appeared to improve training times in practice. We have attached the main portion
of the training code, written using PyTorch, in the appendix."
NETWORK DYNAMICS,0.2292358803986711,"3.1
NETWORK DYNAMICS"
NETWORK DYNAMICS,0.23255813953488372,"Assuming ﬁxed parameters q, W, L, the gradient for y can be computed for any input pattern x.
Gradient descent can be used to perform the inner loop minimization in Equation 9:"
NETWORK DYNAMICS,0.23588039867109634,˙yi = ηy 
NETWORK DYNAMICS,0.23920265780730898,"qif(wi, x) − N
X"
NETWORK DYNAMICS,0.2425249169435216,"j=1
Lijyj −λyi "
NETWORK DYNAMICS,0.24584717607973422,"
(11)"
NETWORK DYNAMICS,0.24916943521594684,"Like previous works on linear similarity matching, these dynamics can be interpreted as the dynam-
ics of a one-layer recurrent neural network with all-to-all inhibition PN
j=1 Lijyj between units. A
diagram of this network is shown in Figure 1. Note that we can analytically perform the inner loop
minimization with a non-neural algorithm:"
NETWORK DYNAMICS,0.25249169435215946,"yi ←
X"
NETWORK DYNAMICS,0.2558139534883721,"j
[L + λI]−1
ij qjf(wj, x)
(12)"
NETWORK DYNAMICS,0.2591362126245847,"This is useful both conceptually and for speeding up the training process in our experiments. This
formula shows us that y is a linear function of the non-linear feedforward input f(wi, xt). This
is different from Seung & Zung (2017), Pehlevan & Chklovskii (2014) where the the neurons are
non-linear functions (due to non-negativity constraints) of linear feedforward input wi · xt."
NETWORK DYNAMICS,0.26245847176079734,"3.2
SYNAPTIC LEARNING RULES: ARBITRARY KERNEL"
NETWORK DYNAMICS,0.26578073089701,"In the previous section we saw how the W could be interpreted as feedforward synapses, q as
feedforward regulatory terms, L as inhibitory synapses. Gradient descent on W, q and gradient
ascent on L provides an algorithm for performing the optimization in Equation 9. At each step, we
compute the optimal y. For simplicity, we consider the case with a single input, in which case we
drop the index b on xb, yb. The stochastic gradients for W lead to the update:"
NETWORK DYNAMICS,0.2691029900332226,"∆wi ∝yi∂f(wi, x)/∂wi −qi∂f(wi, wi)/∂wi
(13)"
NETWORK DYNAMICS,0.2724252491694352,"Classically Hebbian rules have been deﬁned so that the update is linear in the input x (although they
can be nonlinear in the output y which is a function of x) (Eq. 1 of Brito & Gerstner (2016)). This
rule is more general as it is a nonlinear function of the input vector ∂f(wi, x)/∂wi = h(x, wi).
However we note that the spirit of Hebb is still here as this is an online, local, correlation-based
learning rule."
NETWORK DYNAMICS,0.2757475083056478,Under review as a conference paper at ICLR 2022
NETWORK DYNAMICS,0.27906976744186046,"dataset
learned features
neuron 1 tuning
neuron 2 tuning
neuron 3 tuning"
NETWORK DYNAMICS,0.2823920265780731,"Figure 2: Overview of our Hebbian radial basis function network on the half moons dataset (a)
dataset, (b) features {wi}16
i=1 (c,d,e) response proﬁles of 3 neurons."
NETWORK DYNAMICS,0.2857142857142857,"The regulatory terms (essentially controlling the magnitude of the strength of feedforward input)
can be updated with:
∆qi ∝yif(wi, x) −f(wi, wi)qi
(14)
Here we have the correlation between the feedforward input and the neurons response. Finally
gradients for the inhibitory synapses are:"
NETWORK DYNAMICS,0.28903654485049834,"∆Lij ∝yiyj −Lij
(15)"
NETWORK DYNAMICS,0.292358803986711,"This is exactly the same “anti-hebbian” update seen in previous linear similarity matching works
Pehlevan et al. (2018). The inhibition grows in strength as the correlation between neurons grows."
NETWORK DYNAMICS,0.2956810631229236,"3.3
SYNAPTIC LEARNING RULES: RADIAL BASIS FUNCTION KERNEL"
NETWORK DYNAMICS,0.29900332225913623,"Before moving on, we’ll consider the form of the update rules in Eq. 13,14 when the kernel is a
radial basis function, i.e. when the kernel is a function of the Euclidean distance. For simplicity
we’ll also assume the kernel is normalized so that f(v, v) = 1:"
NETWORK DYNAMICS,0.3023255813953488,"f(u, v) := g(∥u −v∥)
and
g(0) = 1
(16)"
NETWORK DYNAMICS,0.30564784053156147,"In this case we get the gradient updates for wi, qi:"
NETWORK DYNAMICS,0.3089700996677741,"∆wi ∝[yig′
i]x −[yig′
i]w
∆qi ∝yigi −qi
(17)"
NETWORK DYNAMICS,0.3122923588039867,"The update for wi is proportional to the input x, but modulated by the output response (yi) and a
function of the feedforward input (g′
i). The updates for Lij do not depend on the form of the kernel."
EXPERIMENTS,0.31561461794019935,"4
EXPERIMENTS"
EXPERIMENTS,0.31893687707641194,"We train networks using a Gaussian kernel for the half moons dataset and a “power-cosine” kernel
(deﬁned in section 4.2) for the MNIST dataset. We compare the approximation error (Eq. 1) of
our method to the approximation error given by a) the optimal eigenvector-based solution (which
we label as kernel PCA) b) Nystr¨om approximation with uniformly sampled landmarks c) Nystr¨om
approximation using KMeans to generate the landmarks d) Nystr¨om approximation using our gen-
erated features (wi) as the landmarks and e) random Fourier feature method (This method is not
applicable to the cosine-based kernel we use for the MNIST dataset). The “dimensionality” refers
to the number of components for the PCA method, the number of landmarks for the Nystr¨om meth-
ods, and the number of Fourier features for the Fourier method. See the appendix for more details
regarding each of these 5 methods. Method (e) is the only other explicitly neural method."
HALF MOONS DATASET,0.3222591362126246,"4.1
HALF MOONS DATASET"
HALF MOONS DATASET,0.32558139534883723,"We train our algorithm on a simple half moons dataset, shown in Figure 2. It consists of 1600 input
vectors x = [x1, x2] drawn from a distribution of two noisy interleaving half circles. We use a"
HALF MOONS DATASET,0.3289036544850498,"Gaussian kernel with σ = 0.3 to measure input similarities: f(u, v) = e−∥u−v∥2"
HALF MOONS DATASET,0.33222591362126247,"2σ2
. We compare
various number of neurons n ∈{2, 4, 8, 16, 32, 64}. See the appendix for training details."
HALF MOONS DATASET,0.33554817275747506,"Emergence of sparse, template-tuned neurons In Fig. 2 we show the learned features {wi} when
we train our algorithm with 16 neurons. We observe that the features appear to tile the input space."
HALF MOONS DATASET,0.3388704318936877,Under review as a conference paper at ICLR 2022
HALF MOONS DATASET,0.34219269102990035,"Figure 3: Approximation error vs. dimensionality for the half moons dataset (a) learned features for
n = 4, 16, 64 (b) input-output similarities for 16 dimensional networks (c) normalized root-mean-
square error between true kernel matrix and various approximation methods. The neural method
(dashed blue) that we derive in Section 3 performs well for n <= 16, but the approximation actually
gets worse as we increase the dimensionality."
HALF MOONS DATASET,0.34551495016611294,"We also show the tuning properties of 3 of the output neurons over the dataset. To generate these
ﬁgures, we color code each sample in the dataset with the response of neuron yi. Gray indicates
zero response, red indicates a positive response and blue indicates a negative response. We observe
that neurons appear to respond with large positive values centered around a small localized region
of the input dataset. The features closely resemble the cluster centers return by KMeans."
HALF MOONS DATASET,0.3488372093023256,"Kernel approximation error In panel (a) of Fig.
3 we show the learned features for n =
{4, 16, 64}. In panel (b) we plot the input similarities vs. output similarities generated by our neural
algorithm with 16 dimensional outputs. In panel (c), we plot the normalized mean squared error
for our method compared to the neural random Fourier method of Bahroun et al. (2017), non-neural
Nystr¨om methods, and non-neural but optimal kernel PCA method."
HALF MOONS DATASET,0.3521594684385382,"We observe that for small dimensionality (n ≤16) our method actually seems to marginally outper-
form the Nystr¨om+KMeans method, which outperforms the Nystr¨om+randomly sampled landmarks
method. Additionally, using the Nystr¨om approximation with our features seems to be uniformly
better than the representations we generate with the neural net. Essentially, our algorithm leanrs
useful landmarks, but for most faithful representation, it is better to just throw away the neural re-
sponses and simply use the Nystr¨om approximation with our landmarks. It is worth mentioning that
as you increase the dimensionality higher, the Random Rourier method ultimately does converge to
zero error, unlike our method."
HALF MOONS DATASET,0.3554817275747508,"Utility of representations evaluated by KMeans clustering In Fig. 4 we visualize the principle
components of the inputs x and 16D representations y. Of course, the principle components of
x are not too interesting, they are just a reﬂected version of the original 2D dataset. The top two
components of y appear to be more linearly separable than the inputs and indicate that a strong
nonlinear transformation has occured. Additionally, we run KMeans on x and on y (we use the
implementation of scikit-learn, and take the lowest energy solution using 100 inits). We observe that
the clustering yields the nearly perfect labels when performed on y. The kernel similarity matching
vectors appear to be better suited for downstream learning tasks than the original inputs."
MNIST DATASET,0.3588039867109635,"4.2
MNIST DATASET"
MNIST DATASET,0.36212624584717606,"We train our algorithm on the MNIST handwritten digits dataset LeCun & Cortes (2010). The
dataset consists of 70,000 images of 28x28 handwritten digits, which we cropped by 4 pixels on
each side to yield 20x20 images (which become 400 dimensional inputs). We use kernels of the
form f(u, v) = ∥u∥∥v∥(ˆu · ˆv)α and varying number of neurons. Training details are provided in
the appendix."
MNIST DATASET,0.3654485049833887,"The linear kernel is recovered by setting α = 1. We are not aware of other works using this exact
“power-cosine” kernel before, however it is motivated by the arccosine kernel studied in the context
of wide random ReLU networks (Cho & Saul, 2009). An important property of our kernel network is
the linear input-output scaling, meaning that rescaling an input x′ ←ax will cause the correspond-"
MNIST DATASET,0.3687707641196013,Under review as a conference paper at ICLR 2022 pc1 pc2
MNIST DATASET,0.37209302325581395,pca on x pc1 pc2
MNIST DATASET,0.3754152823920266,pca on y x1 x2
MNIST DATASET,0.3787375415282392,kmeans on x x1 x2
MNIST DATASET,0.38205980066445183,kmeans on y
MNIST DATASET,0.3853820598006645,"Figure 4: Utility of kernel similarity matching for downstream tasks. (a) principle components of
the input vectors x (b) principle components of the 16 dimensional neural representations y (c)
clustering generated by kmeans on x (d) clustering generated by kmeans on y. For (a,b) the the
colors are given by ground truth labels while in (c,d) the colors are given by the KMeans clustering."
MNIST DATASET,0.38870431893687707,"0
200
400
600
800
1000
dimensionality 0.00 0.01 0.02 0.03 0.04 0.05 F
F
F"
MNIST DATASET,0.3920265780730897,"f(u, v) = u
v (u v)"
MNIST DATASET,0.3953488372093023,"0
200
400
600
800
1000
dimensionality 0.0 0.1 0.2 0.3 0.4"
MNIST DATASET,0.39867109634551495,"f(u, v) = u
v (u v)3"
MNIST DATASET,0.4019933554817276,"Our method
Nystrom-our landmarks
Nystrom-sampled landmarks
Nystrom-kmeans landmarks
Kernel PCA"
MNIST DATASET,0.4053156146179402,MNIST approximation error vs. dimensionality
MNIST DATASET,0.40863787375415284,"Figure 5: Approximation error vs. dimensionality for the MNIST dataset. (a) f(u, v) = u · v
(linear kernel) (b) f(u, v) = ∥u∥∥v∥(ˆu · ˆv)3 (a nonlinear kernel). For the linear kernel all methods
give relatively small approximation error once n > 100. Although yet again we see that the neural
method does not continue to decrease as the dimensionality increases beyond 200, even in the linear
setting."
MNIST DATASET,0.4119601328903654,"ing representation to also be rescaled by the same factor y′ ←ay. This will allow our nonlinear
networks to have the same “contrast-invariant-tuning” properties that are thought to be displayed by
simple cells in cat visual cortex (Skottun et al., 1987)."
MNIST DATASET,0.4152823920265781,"Approximation error We display the normalized approximation errors for α = 1 and α = 3 in Fig.
5. For the linear kernel (α = 1) all methods yield a relatively small error even for low dimensionality.
An error of 0.01 is hard to see by eye when plotting input-output similarity scatter plots as done in
Fig. 3. For both α = 1, 3 we observe again a strange behavior of our method: it seems to “bottom
out” and the error stops decreasing and even begins to increase as the dimensionality increases. This
may be related to unstable convergence properties of gradient descent ascent."
MNIST DATASET,0.4186046511627907,"For α = 3 we observe that the kernel PCA method largely outperforms all methods. We obesrve that
Nystr¨om with either our features or KMeans appears to outperform sampled Nystr¨om methods. The
sampled Nystr¨om method is worse than our representations for low dimensionality but eventually
catches up and surpasses ours neural representations."
MNIST DATASET,0.4219269102990033,"Emergence of sparse representations We train networks with α = {1, 2, 3, 4} and n = 800 neu-
rons (so the output dimensionality is exactly 2x the input dimensionality). There is a sign degeneracy
when α is odd: we can multiply both wi and yi by −1 and the objective is unchanged. When we
look at the response histogram for single neurons, we observe that for α = 3, the response tends
to be heavily skewed so that when the response has a high magnitude, it is either always positive"
MNIST DATASET,0.42524916943521596,Under review as a conference paper at ICLR 2022
MNIST DATASET,0.42857142857142855,"Figure 6: (a) response distribution (after the procedure we describe in the text for removing the sign
degeneracy). The nonlinear kernels (α = 2, 3, 4) naturally give rise to sparse distributions. (b) test
set accuracy of a linear classiﬁer classiﬁcation for MNIST (c) train set accuracy of the corresponding
linear classiﬁer. Interestingly all nonlinear kernels give nearly identical train and test set results. The
linear kernel gives nearly identical results to simply training the classiﬁer directly on the pixels."
MNIST DATASET,0.4318936877076412,"or always negative. We remove this degeneracy by multiplying both wi and yi by the sign of ⟨yi⟩.
After removing this degeneracy we plot the neuron responses over all patterns in Figure 6."
MNIST DATASET,0.43521594684385384,"For α = 1 (linear neurons), neuron responses are roughly centered around zero: neuron responses
are neither sparse not skewed. For α = 2, 3, 4, neurons appear to have a heavy tailed distribu-
tion, they frequently have small responses, but occasionally have large positive responses. Neurons
become increasingly sparse and heavy tailed as we increase α, although this effect is not that strong."
MNIST DATASET,0.43853820598006643,"Evaluating the representations via linear classiﬁcation We train a linear classiﬁer on the inputs
(x) and the outputs (y) for α = 1, 2, 3, 4 and n = 800. We train every conﬁguration using k ∈
{1, 3, 10, 30, 100, 300, 1000, 3000} labels per class. We train all conﬁguration with a weight decay
parameter λ ∈{1e −5, 1e −4, 1e −3, 1e −2, 1e −1, 1} which yields the highest test accuracy. We
average the accuracy for every conﬁguration over 5 random seeds. The results are show in Fig. 6."
MNIST DATASET,0.4418604651162791,"As expected, the performance of the inputs and α = 1 (linear similarity matching) is nearly identi-
cal on both test and train sets. Surprisingly, the test performance of α = 2, 3, 4 is nearly identical.
Perhaps these curves can be partially explained by the spectra of the output similarity matrix which
we show in Figure 10 of the Appendix. While the shapes of the spectra are different in every case,
α = 1 has roughly 200 nonzero eigenvalues while α = 2, 3, 4 all have nearly 800 nonzero eigen-
values. Perhaps the number of nonzero eigenvalues is more inﬂuential for the linear classiﬁcation
performance than the detailed shapes of these spectra."
RELATED WORK,0.44518272425249167,"5
RELATED WORK"
RELATED WORK,0.4485049833887043,"Kernel similarity matching with random Fourier features The most closely related work to ours
is kernel similarity matching with random Fourier features (Bahroun et al., 2017). The key difference
between our methods is that instead of learning the features w, they use random Fourier features
to directly generate nonlinear feature vectors φt =
p"
RELATED WORK,0.45182724252491696,"2/n cos(Wxt + b) which they then feed
into a standard linear similarity matching network. This leads them to a different architecture (one
feedforward layer + one recurrent layer, instead of our single recurrent layer net) and a different set
of learning rules. A beneﬁt of the random feature approach is that it will theoretically lead to perfect
matching, so long as the number of random features is sufﬁciently large."
RELATED WORK,0.45514950166112955,"However, the feature learning aspect of our algorithm naturally led to a sparse set of responses which
lends our model an added degree of biological plausibility. Additionally, our method generalizes to
non-shift invariant kernels and empirically it seemed that to yield better approximation error when
the dimensionality of the output is not too high. Our method can be seen as a biased method for ap-"
RELATED WORK,0.4584717607973422,Under review as a conference paper at ICLR 2022
RELATED WORK,0.46179401993355484,"proximation, which can be useful when the dimensionality is low, but ultimately will underperform
compared to non-biased methods such as random Fourier methods or Nystr¨om methods."
RELATED WORK,0.46511627906976744,"Nystr¨om Approximation While not obviously biological, Nystr¨om methods are perhaps the most
commonly used methods for approximating kernel matrices. The Nystr¨om approximation uses a set
of landmarks {wi : i = 1, 2, . . . , N} to construct a low rank approximation of the original kernel
matrix (Williams & Seeger, 2001). To more clearly see the relationship between this method to ours,
one can slightly modify the original formulation to generate “Nystr¨om features”:"
RELATED WORK,0.4684385382059801,"“Nystr¨om features” yt
j =
X"
RELATED WORK,0.4717607973421927,"i
f(xt, wi)Mij where Mij = [(B†)1/2]ij and Bij = f(wi, wj)"
RELATED WORK,0.4750830564784053,"(18)
B† indicates the psuedo-inverse. Multiplying two such vectors togethers yields the Nystr¨om approx-
imation ˆFst = ys · yt = P"
RELATED WORK,0.47840531561461797,"ij f(xs, wi)[B†]ijf(xt, wj). Our method produces representations of
the same functional form but our M matrix is derived from parameters learned by the correlations:"
RELATED WORK,0.48172757475083056,"Our features yt
j =
X"
RELATED WORK,0.4850498338870432,"i
f(xt, wi)Mij where Mij = [L + λI]−1
ij qj
(19)"
RELATED WORK,0.4883720930232558,"As measured by squared error, the Nystr¨om approximation was actually a better approximation than
our representations, when we used the same set of landmarks (Figs. 3, 5). The variation in Nystr¨om
methods primarily come from the method used to generate the landmarks. Two broad categories
of landmark selection can be deﬁned: template vs. pseudo-landmark. Template based approaches
choose landmarks as a subset of the inputs w ∈{x1, xw, . . . , xT } typically chosen via sampling
schemes (Williams & Seeger, 2001; Drineas et al., 2005; Musco & Musco, 2016). Pseudo-landmark
approaches do not require the landmarks to be inputs. Zhang et al. (2008) used the cluster centers
generated by KMeans as the landmarks. Fu (2014) formulate landmark selection as an optimization
problem in the reproducing Hilbert space. Our method can be seen as a pseudo-template approach
as our landmarks are directly generated via Hebbian learning rules and in general will not be exactly
equal to any particular input pattern. Our method is similar in spirit to the approach of Fu (2014).
A key difference is that we use a different objective, a correlation-based upper bound to the sum of
squared errors, which gives rise to correlation-based learning rules."
DISCUSSION,0.49169435215946844,"6
DISCUSSION"
DISCUSSION,0.4950166112956811,"We have extended the neural random Fourier feature method of Bahroun et al. (2017) for kernel
similarity matching to instead be applicable to arbitrary differentiable kernels. Rather than using
random nonlinear features, we learned the features with Hebbian learning rules. Both this work and
that of Bahroun et al. (2017) can be seen as extensions of the linear similarity matching works written
in Hu et al. (2014); Pehlevan & Chklovskii (2014); Pehlevan et al. (2015; 2018). By using a nonlinear
input similarity, the representations learned by our network are capable of learning high-dimensional
nonlinear functions of the input, without requiring any constraints such as non-negativity."
DISCUSSION,0.4983388704318937,"To our knowledge this is the ﬁrst work that attempts to directly optimize the sum of squared errors
(Eq. 1) without relying on sampling schemes or direct computation of the input similarity matrix. It
would be interesting to relax the correlation-based constraint we have imposed on ourselves. This
might allow for a variety of different types of bounds (Eq. 3) to be derived which in turn could lead
to more faithful approximations than the one presented in our paper."
DISCUSSION,0.5016611295681063,"A key obstacle faced by users of this algorithm is the stochastic gradient descent ascent proce-
dure. Empirically the convergence our algorithm is quite sensitive to the learning rate choices. This
method does not provide the same sorts of theoretically guarantees or empirically observed robust-
ness of sampling based methods. Generation of more robust ascent-descent optimization methods
could be useful for making this class of algorithms more accessible for the practitioner."
REPRODUCIBILITY STATEMENT,0.5049833887043189,"7
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.5083056478405316,"To aid with reproducibility, we have added the PyTorch code used to implement our main algorithm
in the Appendix. We have proven our core claims in the Appendix. We are also attaching as zip ﬁles
for the source code used to train our networks and produce our ﬁgures."
REPRODUCIBILITY STATEMENT,0.5116279069767442,Under review as a conference paper at ICLR 2022
REFERENCES,0.5149501661129569,REFERENCES
REFERENCES,0.5182724252491694,"Yanis Bahroun, Eug´enie Hunsicker, and Andrea Soltoggio. Neural networks for efﬁcient nonlinear
online clustering. In International Conference on Neural Information Processing, pp. 316–324.
Springer, 2017."
REFERENCES,0.521594684385382,"I. Borg and P. J. F. Groenen. Classical Scaling, pp. 261–267. Springer New York, New York, NY,
2005. ISBN 978-0-387-28981-6. doi: 10.1007/0-387-28981-X 12. URL https://doi.org/
10.1007/0-387-28981-X_12."
REFERENCES,0.5249169435215947,"Carlos SN Brito and Wulfram Gerstner. Nonlinear hebbian learning as a unifying principle in re-
ceptive ﬁeld formation. PLoS computational biology, 12(9):e1005070, 2016."
REFERENCES,0.5282392026578073,"Dmitri Chklovskii. The search for biologically plausible neural computation: The conventional
approach, 2016. URL http://www.offconvex.org/2016/11/03/MityaNN1/."
REFERENCES,0.53156146179402,"Youngmin Cho and Lawrence Saul. Kernel methods for deep learning. In Y. Bengio, D. Schuurmans,
J. Lafferty, C. Williams, and A. Culotta (eds.), Advances in Neural Information Processing Sys-
tems, volume 22. Curran Associates, Inc., 2009. URL https://proceedings.neurips.
cc/paper/2009/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf."
REFERENCES,0.5348837209302325,"Petros Drineas, Michael W Mahoney, and Nello Cristianini. On the nystr¨om method for approxi-
mating a gram matrix for improved kernel-based learning. journal of machine learning research,
6(12), 2005."
REFERENCES,0.5382059800664452,"Zhouyu Fu. Optimal landmark selection for nystr¨om approximation. In Chu Kiong Loo, Keem Siah
Yap, Kok Wai Wong, Andrew Teoh, and Kaizhu Huang (eds.), Neural Information Processing,
pp. 311–318, Cham, 2014. Springer International Publishing. ISBN 978-3-319-12640-1."
REFERENCES,0.5415282392026578,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
neural information processing systems, 30, 2017."
REFERENCES,0.5448504983388704,"Geoffrey E Hinton. Boltzmann machine. Scholarpedia, 2(5):1668, 2007."
REFERENCES,0.5481727574750831,"Tao Hu, Cengiz Pehlevan, and Dmitri B. Chklovskii. A hebbian/anti-hebbian network for online
sparse dictionary learning derived from symmetric matrix factorization. In 2014 48th Asilomar
Conference on Signals, Systems and Computers, pp. 613–619, 2014. doi: 10.1109/ACSSC.2014.
7094519."
REFERENCES,0.5514950166112956,"Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/."
REFERENCES,0.5548172757475083,"Daniel D. Lee and H. Sebastian Seung. Learning the parts of objects by nonnegative matrix factor-
ization. Nature, 401:788–791, 1999."
REFERENCES,0.5581395348837209,"Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random synaptic
feedback weights support error backpropagation for deep learning. Nature communications, 7(1):
1–10, 2016."
REFERENCES,0.5614617940199336,"Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave mini-
max problems. In International Conference on Machine Learning, pp. 6083–6093. PMLR, 2020."
REFERENCES,0.5647840531561462,"Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):
129–137, 1982."
REFERENCES,0.5681063122923588,"Cameron Musco and Christopher Musco. Recursive sampling for the nystr\” om method. arXiv
preprint arXiv:1605.07583, 2016."
REFERENCES,0.5714285714285714,"Dina Obeid, Hugo Ramambason, and Cengiz Pehlevan. Structured and Deep Similarity Matching
via Structured and Deep Hebbian Networks. Curran Associates Inc., Red Hook, NY, USA, 2019."
REFERENCES,0.574750830564784,"B.A. Olshausen and D.J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a
sparse code for natural images. Nature, 381:607–609, June 1996."
REFERENCES,0.5780730897009967,Under review as a conference paper at ICLR 2022
REFERENCES,0.5813953488372093,"Bruno A. Olshausen and David J. Field. Sparse coding with an overcomplete basis set: A strat-
egy employed by v1?
Vision Research, 37(23):3311–3325, 1997.
ISSN 0042-6989.
doi:
https://doi.org/10.1016/S0042-6989(97)00169-7.
URL https://www.sciencedirect.
com/science/article/pii/S0042698997001697."
REFERENCES,0.584717607973422,"Cengiz Pehlevan and Dmitri B. Chklovskii. A hebbian/anti-hebbian network derived from online
non-negative matrix factorization can cluster and discover sparse features. In 2014 48th Asilomar
Conference on Signals, Systems and Computers, pp. 769–775, 2014. doi: 10.1109/ACSSC.2014.
7094553."
REFERENCES,0.5880398671096345,"Cengiz Pehlevan, Tao Hu, and Dmitri B. Chklovskii. A hebbian/anti-hebbian neural network for
linear subspace learning: A derivation from multidimensional scaling of streaming data. Neural
Computation, 27(7):1461–1495, 2015. doi: 10.1162/NECO a 00745."
REFERENCES,0.5913621262458472,"Cengiz Pehlevan, Anirvan M. Sengupta, and Dmitri B. Chklovskii. Why do similarity matching
objectives lead to hebbian/anti-hebbian networks?
Neural Computation, 30(1):84–124, 2018.
doi: 10.1162/neco a 01018."
REFERENCES,0.5946843853820598,"Ali Rahimi, Benjamin Recht, et al. Random features for large-scale kernel machines. In NIPS,
volume 3, pp. 5. Citeseer, 2007."
REFERENCES,0.5980066445182725,"Maximilian Riesenhuber and Tomaso Poggio. Hierarchical models of object recognition in cortex.
Nature neuroscience, 2(11):1019–1025, 1999."
REFERENCES,0.6013289036544851,"Frank Rosenblatt. The perceptron: a probabilistic model for information storage and organization
in the brain. Psychological review, 65(6):386, 1958."
REFERENCES,0.6046511627906976,"Bernhard Sch¨olkopf, Alexander Smola, and Klaus-Robert M¨uller. Kernel principal component anal-
ysis. In International conference on artiﬁcial neural networks, pp. 583–588. Springer, 1997."
REFERENCES,0.6079734219269103,"H. Sebastian Seung. Convergence of gradient descent-ascent analyzed as a newtonian dynamical
system with dissipation, 2019."
REFERENCES,0.6112956810631229,"H. Sebastian Seung and Jonathan Zung. A correlation game for unsupervised learning yields com-
putational interpretations of hebbian excitation, anti-hebbian inhibition, and synapse elimination.
CoRR, abs/1704.00646, 2017. URL http://arxiv.org/abs/1704.00646."
REFERENCES,0.6146179401993356,"Bernt C Skottun, Arthur Bradley, Gary Sclar, Izumi Ohzawa, and Ralph D Freeman. The effects of
contrast on visual orientation and spatial frequency discrimination: a comparison of single cells
and behavior. Journal of neurophysiology, 57(3):773–786, 1987."
REFERENCES,0.6179401993355482,"Christopher Williams and Matthias Seeger. Using the nystr¨om method to speed up kernel machines.
In T. Leen, T. Dietterich, and V. Tresp (eds.), Advances in Neural Information Processing Sys-
tems, volume 13. MIT Press, 2001. URL https://proceedings.neurips.cc/paper/
2000/file/19de10adbaa1b2ee13f77f679fa1483a-Paper.pdf."
REFERENCES,0.6212624584717608,"Kai Zhang, Ivor W Tsang, and James T Kwok. Improved nystr¨om low-rank approximation and
error analysis. In Proceedings of the 25th international conference on Machine learning, pp.
1232–1239, 2008."
REFERENCES,0.6245847176079734,"A
APPENDIX"
REFERENCES,0.627906976744186,"Proof of Lemma 1. In this section we will derive a correlation-based upper bound for the nonlinear
pairwise sum P"
REFERENCES,0.6312292358803987,"t,t′ f(xt, xt′)yt · yt′ in Equation 2. The key to creating this bound will be to
replace all nonlinear similarities f(u, v) with the dot product between high dimensional vectors
φu · φv. This is allowed because we have assumed that f is a positive semideﬁnite kernel. Formally
this assumption means that for any set of M-dimensional vectors {w, x1, . . . , xT }, there exists a
corresponding set of (at most) T + 1-dimensional vectors {φw, φ1, . . . , φT } whose inner products
yield the similarity deﬁned by f:"
REFERENCES,0.6345514950166113,"φt · φt′ = f(xt, xt′) and φt · φw = f(xt, w) and φw · φw = f(w, w)
(20)"
REFERENCES,0.6378737541528239,Under review as a conference paper at ICLR 2022
REFERENCES,0.6411960132890365,"Assume we have some corresponding set of T + 1 scalars {q, y1, . . . , yT }. Now consider the vec-
tor difference 1 T
P"
REFERENCES,0.6445182724252492,"t ytφt −qφw. The squared norm of this difference is of course non-negative.
Additionally we can expand out this square: 0 ≤1 2"
T,0.6478405315614618,"1
T X"
T,0.6511627906976745,"t
ytφt −qφw  2"
T,0.654485049833887,"=
1
2T 2
X"
T,0.6578073089700996,"s,t
ysytφs · φt −1 T X"
T,0.6611295681063123,"t
qytφt · φw + 1"
T,0.6644518272425249,2q2φw · φw (21)
T,0.6677740863787376,"At this point we can simply replace all dot products with the equivalent nonlinear similarities f(·, ·)
in Equation 20 and rearrange the terms to yield our key inequality (Eq. 3) which we write again:"
T,0.6710963455149501,"1
2T 2
X"
T,0.6744186046511628,"s,t
ysytf(xs, xt) ≥1 T X"
T,0.6777408637873754,"t
qytf(xt, w) −1"
T,0.6810631229235881,"2q2f(w, w)
(22)"
T,0.6843853820598007,"A.1
INTERPRETATION USING RANK-1 NYSTR ¨OM APPROXIMATION"
T,0.6877076411960132,"The bound in Equation 3 can be interpreted using a rank-1 Nystr¨om approximation for f(xt, xt′).
By holding w ﬁxed and maximizing for q in the right hand side of Equation 3, we get q∗=
f(w, w)† P"
T,0.6910299003322259,"t ytf(xt, w) where f(w, w)† indicates the pseudo-inverse.1 We can insert this opti-
mal q∗back into the right hand side to yield: X"
T,0.6943521594684385,"t
q∗ytf(xt, w) −1"
T,0.6976744186046512,"2(q∗)2f(w, w) = 1 2 X"
T,0.7009966777408638,"t,t′
ytf Nystr¨om
t,t′
yt′
(23)"
T,0.7043189368770764,where we have deﬁned they Nystr¨om matrix:
T,0.707641196013289,"f Nystr¨om
t,t′
:= f(xt, w)f(w, w)†f(w, xt′)
(24)"
T,0.7109634551495017,"The matrix f Nystr¨om
t,t′
is a rank-1 Nystr¨om approximation for the full similarity matrix f(xt, xt′) ?.
Note that for every dimension i of the representation vector y, we have a different landmark vector
wi so we are using a different rank-1 approximation of the matrix f(xt, xt′) for every to the pairwise
sum 1"
P,0.7142857142857143,"2
P"
P,0.717607973421927,"t,t′ yi
tyi
t′f(xt, xt′)."
P,0.7209302325581395,"Typically the weight vector w , often called a “landmark”, used in the Nystr¨om approximation is set
either by setting it to a random input or by more sophistcated schemes like setting it with KMeans.
In our case, we are directly optimizing the landmarks via Equation 6. To our knowledge the only
other work to do this was performed in Fu (2014)."
P,0.7242524916943521,"A.2
PYTORCH CODE FOR TRAINING"
P,0.7275747508305648,The code used in the main training loop of our algorithm is shown in Fig. 7.
P,0.7308970099667774,"A.3
HOMOGENEOUS KERNELS"
P,0.7342192691029901,"Before moving on we note that a simpliﬁcation can be made if we have a homogenous (scale free)
kernel, i.e. if f(au, bv) = (ab)αf(u, v). Examples of such kernels are the linear kernel f(u, v) =
u·v, homogeneous polynomial kernels f(u, v) = (u·v)α, and the cosine-based kernel we will use
in one of our experiments f(u, v) = ∥y∥∥v∥(ˆu · ˆv)α. In this case, there is a degenecary between
qi and the norm of wi. This means we can actually eliminate the minimization over q by setting
qi = 1. We prove this fact in the appendix. In the case of a homogeneous kernel, we are left with
the simpler equivalent optimization:"
P,0.7375415282392026,"min
W max
L
min
Y − N
X i=1"
P,0.7408637873754153,"
⟨yif(wi, x)⟩−1"
P,0.7441860465116279,"2f(wi, wi)

+ 1 2 N
X i,j=1"
P,0.7475083056478405,"
Lij⟨yiyj⟩−1"
P,0.7508305647840532,"2L2
ij"
P,0.7541528239202658,"
+ λ⟨y2
i ⟩(25)"
P,0.7574750830564784,"1The pseudo-inverse of the scalar f(w, w) acts exactly like the regular inverse except it is deﬁned to be 0
when f(w, w) is zero, unlike the regular inverse which would be undeﬁned."
P,0.760797342192691,Under review as a conference paper at ICLR 2022
P,0.7641196013289037,Figure 7: Training loop to perform the GDA-based optimazation of Eq. 9 written using PyTorch
P,0.7674418604651163,"min
W max
L
min
Y
1
T T
X t=1  − N
X i=1"
P,0.770764119601329,"
yt
if(wi, xt) −1"
P,0.7740863787375415,"2f(wi, wi)

+ 1 2 N
X i,j=1"
P,0.7774086378737541,"
Lij⟨yiyj⟩−1"
P,0.7807308970099668,"2L2
ij 
+ λ 2 N
X"
P,0.7840531561461794,"i=1
(yt
i)2  "
P,0.7873754152823921,"(26)
This more clearly shows the relationship between the linear similarity matching objectives and the
more general kernel similarity matching objective. When f(wi, x) = wi · x, we are in fact left with
the exact objective studied in previous works on linear similarity matching Pehlevan et al. (2018).
This simpliﬁcation can be easily implemented in code by initializing qi = 1 and setting the learning
rate for ηq to be 0 for all iterations."
P,0.7906976744186046,"A.4
PROOF THAT THE BOUND IN EQUATION 6 IS MAXIMIZED WHEN q = 1 AND f IS A
HOMOGENEOUS KERNEL"
P,0.7940199335548173,"Assume that f is a homogenous kernel, so that f(λ1u, λ2v) = (λ1λ2)αf(u, v) for any λ1, λ2 > 0.
We will show that in this case we can simply set q = 1. Assume we have some pair q, w. Then
deﬁne q′ = 1 and w′ := q1/αw. Because our kernel is homogeneous, we have f(w′, xt) =
f(q1/αw, xt) = qf(w, xt) and similarly f(w′, w′) = q2f(w, w). In other words when we have a
homogenous kernel, we can always just rescale the features w′ ←q1/αw so the following holds for
any q:
X"
P,0.7973421926910299,"t
qytf(xt, w) −1"
P,0.8006644518272426,"2q2f(w, w) =
X"
P,0.8039867109634552,"t
ytf(xt, w′) −1"
P,0.8073089700996677,"2f(w′, w′)
(27)"
P,0.8106312292358804,"A.5
METHODS WE COMPARE TO IN OUR EXPERIMENTS"
P,0.813953488372093,"Kernel PCA The optimal (in terms of mean squared error) rank N approximation ˆf to the kernel
matrix f is given by the top n-dimensional subspace of the kernel matrix (Borg & Groenen, 2005).
Speciﬁcally, we perform an eigenvector decomposition on f then set ˆf via:"
P,0.8172757475083057,"f(xs, xt) = T
X"
P,0.8205980066445183,"i=1
λiviv⊤
i →ˆfst = N
X"
P,0.8239202657807309,"i=1
λiviv⊤
i
(28)"
P,0.8272425249169435,"For the mnist dataset,
the kernel matrix is 70k x 70k entries so we use a random-
ized svd algorithm to compute the top components,
rather than a full SVD. We use
the PyTorch implementation “torch.svdlowrank′′withq=1024+256(soweestimatethetop1024 +
256singularvaluesandvectors)andwesetniter = 4meaningwedo4poweriterations."
P,0.8305647840531561,Under review as a conference paper at ICLR 2022
P,0.8338870431893688,"Nystrom methods Given a set of landmarks {wi : i = 1, 2, . . . , N}, the nystrom method deﬁnes
two matrices:
Ati = f(xt, wi)
Bij = f(xi, wj)
(29)"
P,0.8372093023255814,These are used to approximate the kernel matrix via:
P,0.840531561461794,"ˆfst =
X"
P,0.8438538205980066,"ij
Asi[B†]ijA⊤
tj
(30)"
P,0.8471760797342193,"To calculate the pseudo-inverse of B we use double precision arithmetic and set ﬁrst set all singular
value of B smaller than 1e −10 to zero. We compare 3 different methods of landmark generation
in our paper."
P,0.8504983388704319,"Nystrom with uniformly sampled landmarks This is the simplest method, and was proposed in
he original paper using the Nystrom method to approximate kernel matrices (Williams & Seeger,
2001). We simply uniformly sample N landmarks without replacement from the dataset."
P,0.8538205980066446,"Nystrom with landmarks generated via KMeans This method was used by Zhang et al. (2008)
and instead uses the cluster centers given by KMeans as the landmarks. We initialize our means with
templates from the dataset and the use Lloyd’s method to update our cluster centers (Lloyd, 1982).
This is run either until convergence, or 100 iterations of the algorithm occurs, whichever happens
ﬁrst."
P,0.8571428571428571,"Nystrom with landmarks generated via our method We use the N features learned with Hebbian
update rules as the landmarks in thye Nystrom approximation."
P,0.8604651162790697,"Random Fourier features For the half moons dataset using the Gaussian kernel, we also compare
our method to the random Fourier feature method (Rahimi et al., 2007). The authors in Bahroun
et al. (2017) train a linear similarity matching on top of these features. But for simplicity, we just
use the random features themselves, rather than the subsequent neurally generated features. This
provides a best-case scenario for the neural random Fourier method. The neural algorithm is simply
trying to matching similarities min y∥ys · yt −φs · φt∥, and it should be able to provide zero error,
given the same output dimension as input dimension. Although it practice it can be challenging to
set the learning rates appropriately, so we evaluate φ instead of y to avoid any possible issues with
improper training."
P,0.8637873754152824,"To generate these features, we randomly sample wi ∼N(mean = 0, variance =
1
σ2 I) where and
bi ∈Uniform[0, 2π] as set the features as"
P,0.867109634551495,"φt
i = r"
P,0.8704318936877077,"2
n cos(wi · xt + bi)
(31)"
P,0.8737541528239202,"A.6
HALF MOONS EXPERIMENT"
P,0.8770764119601329,"A.6.1
TRAINING DETAILS"
P,0.8803986710963455,"We train with minibatch sizes of 64 input. We train for 10000 iterations with ηw = ηq = 0.01 and
ηl = 0.1. Then we anneal the learning rates by a factor of 10x and train for 10000 more iterations
ηw = ηq = 0.001 and ηl = 0.01."
P,0.8837209302325582,"A.7
MNIST EXPERIMENT"
P,0.8870431893687708,"A.7.1
TRAINING DETAILS"
P,0.8903654485049833,"We train with minibatch sizes of 64. After initialization and warmup, we set α and train for 10,000
iterations with ηw = 0.001, ηl = 0.01. We then decay the learning rates for W, L by 10x and train for
with 5k more iterations with ηw = 0.0001, ηl = 0.001. This whole procedure takes approximately
4 minutes on an NVIDIA GTX 1080 GPU."
P,0.893687707641196,"A.7.2
LEARNED FEATURES FOR MNIST DATASET"
P,0.8970099667774086,"In Figure 8 we show the weights wi, visualized as 20x20 images, that are learned. When α = 1
(linear similarity matching), the features appear as complicated linear combinations of input digits."
P,0.9003322259136213,Under review as a conference paper at ICLR 2022
P,0.9036544850498339,"= 1
= 2
= 3
= 4
Feedforward weights"
P,0.9069767441860465,"Figure 8: Feedforward weights (W) learned by the network for α = 1, 2, 3, 4. When α = 1, the
weights appear to be complicated linear combinations of input vectors. As α increases, the weights
begin to resemble “templates”, i.e. whole digits. In the main text, we argue this behavior results
from the increasing sharpness of neural tuning as α increases."
P,0.9102990033222591,"= 1
= 2
= 3
= 4
Linearized Neuron Responses"
P,0.9136212624584718,"Figure 9: “Linearized responses” for a subset of neurons from networks with α = {1, 2, 3, 4}.
Speciﬁcally, for each neuron yi we compute the vector si =

0.1 I + ⟨xx⊤⟩
−1 ⟨yix⟩and visualize
si as a 20 × 20 image. As α increases, it appears that neurons become increasingly selective to
whole input digits."
P,0.9169435215946844,"However, with α = 2 we see clear digits beginning to emerge. And with α = 4 nearly all the
features look like whole digits."
P,0.920265780730897,"A.8
RECEPTIVE FIELD ANALYSIS (AKA ”LINEARIZED NEURON RESPONSES”)"
P,0.9235880398671097,"A natural way to visualize what the networks learn is to examine the feedforward weights. However
these visualizations are not as interpretable in this experiment as they were for the simple halfmoons
dataset. In particular for α = 1, 2, 3 the weights appear to be a blend of templates (whole digits)"
P,0.9269102990033222,Under review as a conference paper at ICLR 2022
P,0.9302325581395349,"0
400
800
1200
eigenvalue index (i) 10
4 10
3 10
2 10
1 100"
P,0.9335548172757475,"eigenvalue (
i/
1)"
P,0.9368770764119602,similarity matrix eigenvalue spectrum
P,0.9401993355481728,"= 1 : f(x, x)"
P,0.9435215946843853,"= 2 : f(x, x)"
P,0.946843853820598,"= 3 : f(x, x)"
P,0.9501661129568106,"= 4 : f(x, x)"
P,0.9534883720930233,= 1 : yy
P,0.9568106312292359,= 2 : yy
P,0.9601328903654485,"= 3 : yy
= 4 : yy"
P,0.9634551495016611,"Figure 10: Eigenvalue spectrum of the input similarity matrix f(xt, xt′) and learned output similar-
ity matrix yt · yt′. If similarity matching were optimal, (i.e. we just performed uncentered kernel
pca on the input similarity matrix) the largest 800 eigenvalues would be exactly matched and sub-
sequent eigenvalues would be zero. We see that increasing α brings up the tails of the spectrum,
approximately ”whitening” the responses. For α = 1, because the inputs are 400 dimensional, the
spectrum only has at most 400 nonzero eigenvalues."
P,0.9667774086378738,"and more complicated linear combinations of digits. We show some examples from each network
conﬁguration in the appendix."
P,0.9700996677740864,"We can better understand and visualize the network responses by instead examining the lin-
earized neuron responses.
Speciﬁcally, for each neuron yi we compute the vector si
=

0.1 I + ⟨xx⊤⟩
−1 ⟨yix⟩. This vector can be thought of as a linear approximation to each neuron
yi ≈si · x. We show these vectors, again visualized as images, in Figure 9."
P,0.973421926910299,"These linearized responses actually highlight a behavior not seen by only considering feedforward
weights. We see for α = 2, it appears that many of the neurons appear to be selective for smaller
regions of the input, sometimes interpretable as strokes and edges. This behavior is likely com-
ing from some sort of cancellation between the feedforward input and lateral interactions. As α
increases, the linear ﬁlters appear to grow in size to resemble whole digits."
P,0.9767441860465116,"For α = 1 (aka linear similarity matching) the linearized responses do not in any way appear as
whole digits, rather they appear to be high spatial frequency images. This is not a failure of the
networks, as the input-output similarities are nearly perfectly matched. This behavior results from
the fact that linearity is not enough to encourage parts or whole templates to be learned."
P,0.9800664451827242,"A.8.1
SPECTRAL ANALYSIS OF THE REPRESENTATIONS"
P,0.9833887043189369,"We examine the eigenvalue spectrum of the input similarity matrix f(xt, xt′) and the output sim-
ilarity matrix yt · yt′. We plot these spectra in Figure 10. Note that we normalize the spectra by
dividing by the largest eigenvalue."
P,0.9867109634551495,"Without even considering the output representations, we can already observe interesting behavior
just by considering the spectrum of the input similarity matrix. As we increase α, the “sharpness”
of the kernel, the spectrum of f tends to ﬂatten out. The effective rank of this matrix increases
with increasing kernel sharpness. This observation is is in part a motivation for kernel similarity
matching. Matching a high rank matrix naturally requires high dimensional vectors. This increase
in dimensionality may be useful for downstream tasks such as linear classiﬁcation. It is also an
important part of brain inspired modeling to use overcomplete representations of the input Olshausen
& Field (1997)."
P,0.9900332225913622,"For α = 1, the spectrum of yt·yt′ closely matches the spectrum f(xt, xt′) for the larger eigenvalues.
However, it appears to fall off for smaller eigenvalues. This may be due in part to the training not
being fully converged. For α > 1, the spectrum of yt · yt′ approximately matches for the larger
eigenvalues (although not perfectly). However again the spectrum tends to fall off more rapidly for
the learned representations than for the input similarity matrix. Note that because the dimensionality"
P,0.9933554817275747,Under review as a conference paper at ICLR 2022
P,0.9966777408637874,"of y is 800 for all experiments, the spectrum necessarily must be zero for all eigenvalues smaller
than the 800th largest eigenvalue."
