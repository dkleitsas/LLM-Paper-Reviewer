Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001876172607879925,"Network clustering tackles the problem of identifying sets of nodes (communities)
that have similar connection patterns. However, in many scenarios, nodes also
have attributes that are correlated with the clustering structure. Thus, network
information (edges) and node information (attributes) can be jointly leveraged
to design high-performance clustering algorithms. Under a general model for
the network and node attributes, this work establishes an information-theoretic
criterion for the exact recovery of community labels and characterizes a phase
transition determined by the Chernoff-Hellinger divergence of the model. The
criterion shows how network and attribute information can be exchanged in order to
have exact recovery (e.g., more reliable network information requires less reliable
attribute information). This work also presents an iterative clustering algorithm that
maximizes the joint likelihood, assuming that the probability distribution of network
interactions and node attributes belong to exponential families. This covers a broad
range of possible interactions (e.g., edges with weights) and attributes (e.g., non-
Gaussian models), as well as sparse networks, while also exploring the connection
between exponential families and Bregman divergences. Extensive numerical
experiments using synthetic data indicate that the proposed algorithm outperforms
classic algorithms that leverage only network or only attribute information as well
as state-of-the-art algorithms that also leverage both sources of information. The
contributions of this work provide insights into the fundamental limits and practical
techniques for inferring community labels on node-attributed networks."
INTRODUCTION,0.00375234521575985,"1
Introduction"
INTRODUCTION,0.005628517823639775,"Community detection or network clustering–the task of identifying sets of similar nodes in a network–
is a fundamental problem in network analysis [4, 1, 18], with applications in diverse fields such
as digital humanities, data science and biology. In the classic formulation, a set of communities
must be determined from the connection patterns among the nodes of a single network. A simple
random graph model with community structure, the Stochastic Block Model (SBM), has been the
canonical model to characterise theoretical limitations and evaluate different community detection
algorithms [1]."
INTRODUCTION,0.0075046904315197,"However, nodes of many real-world networks have attributes or features that can reveal their identity
as an individual or within a group. For example, the age, gender and ethnicity of individuals in a
social network [27], the title, keywords and co-authors of papers in a citation network [32], or the
longitude and latitude of meteorological stations in weather forecast networks [8]. In some scenarios,
such attributes can be leveraged alone to identify node communities (clusters) without even using the
network."
INTRODUCTION,0.009380863039399626,"Thus, a modern formulation for community detection must consider network information (edges) and
node information (attributes). Indeed, recent works have designed community detection algorithms
that can effectively leverage both sources of information to improve performance. Various methods
have been proposed for clustering node-attributed networks, including modularity optimisation [14],
belief propagation [15], expected-maximisation algorithms [24, 34], information flow compres-
sion [33], semidefinite programming [37], spectral algorithms [2, 7, 25] and iterative likelihood based
methods [8]."
INTRODUCTION,0.01125703564727955,"A fundamental problem in this new formulation is fusing both sources of information: how important
is network information in comparison to node information given a problem instance? Intuitively,
this depends on the noise associated with network edges and node attributes. For example, if edges
are reliable then the clustering algorithm should prioritize them when determining the communities.
However, most prior approaches adopt some form of heuristic when merging the two sources of
information [14, 16, 37]. A rigorous approach to this problem requires a mathematical model, and
one has been recently proposed."
INTRODUCTION,0.013133208255159476,"The Contextual Stochastic Block Model (CSBM) is a generalization of the SBM where each node has
a random attribute that depends on its community label. While the model formulation is general (in
terms of distribution for edges and attributes), CSBM has only been rigorously studied in the restrictive
setting where the pairwise interactions are binary (edges are present or not) and the node attributes are
Gaussian [2, 8, 15]. In this scenario, the phase transitions for exactly recovering the community labels
and for detecting them better than a random guess has been established. Moreover, the comparison
with the respective phase transitions in SBM [1] and in Gaussian mixture model [11, 26] (with
no network) highlight the value of jointly leveraging network and node information in recovering
community labels."
INTRODUCTION,0.0150093808630394,"However, real networks often depart from binary edges and Gaussian attributes. Indeed, in many
scenarios network edges have weights that reveal information about that interaction and nodes have
discrete or non-Gaussian attributes. This work tackles this scenario by considering a CSBM where
edges have weights and nodes have attributes that follow some arbitrary distributions. Under this
general model, this work is the first to characterise the phase transition for the exact recovery of
community labels. In particular, the Chernoff-Hellinger divergence, initially defined just for binary
networks [3], is extended to this more general model. This divergence effectively captures the
difficulty of distinguishing different communities and thus plays a crucial role in determining the
limits of exact recovery. The analysis reveals an additional term in the divergence that quantifies the
information provided by the attributes of the nodes. Moreover, it quantifies the trade-off between
network and node information in meeting the threshold for exact recovery."
INTRODUCTION,0.016885553470919325,"The CSBM generates weighted networks that are complete (all possible edges are present) when
edge weights follow a continuous distribution. However, most weighted real networks are sparse.
To model sparse weighted networks and to provide a practical community detection algorithm,
we consider a CSBM whose weights belong to zero-inflated distributions. More precisely, we
suppose that conditioned on observing an edge, the distribution of the weight of this edge belongs
to an exponential family. Similarly, the node attribute distributions are also assumed to belong
to an exponential family. Working with exponential families is motivated by two factors. Firstly,
exponential families encompass a broad range of parametric distributions, including the commonly
used Bernoulli, Poisson, Gaussian, or Gamma distributions. Secondly, there exists an intricate
connection between exponential families of distributions and Bregman divergences, which has proven
to be a powerful tool for developing algorithms across a variety of problems such as clustering,
classification, and dimensionality reduction [6, 13]."
INTRODUCTION,0.01876172607879925,"This connection between Bregman divergences and exponential families has been previously explored
in the context of clustering dense networks (all possible edges are present) [23]. In contrast, this work
proposes an iterative algorithm that maximizes the log-likelihood of the model, for both dense and
sparse networks. This is a key difference with many previous works which either study only dense"
INTRODUCTION,0.020637898686679174,"weighted networks [9, 24] or binary networks with Gaussian attributes [2, 15, 34]. Simulations on
synthetic networks demonstrate that our algorithm outperforms state-of-the-art approaches in various
settings, providing practical techniques for achieving accurate clustering results."
INTRODUCTION,0.0225140712945591,"The article is structured as follows. The relevant related work is discussed in Section 2. Section 3
introduces the model under consideration along with the main theoretical contributions on exact
recovery. Section 4 focuses on sparse networks with edge weights and node attributes drawn from
exponential families and introduces an iterative algorithm for clustering such networks. Numerical
results and comparisons to prior works are presented in Section 5, and Section 6 concludes the paper."
INTRODUCTION,0.024390243902439025,"Notations
Let Ber(p) denote a Bernoulli random variable (r.v.) with parameter p, Nor(m, σ) a
Gaussian r.v. with mean m and standard deviation σ, and Exp(λ) an exponential r.v. with mean λ−1.
The notation [K] refers to the set {1, · · · , K}, while Ai· stands for the i-th row of matrix A."
RELATED WORK,0.02626641651031895,"2
Related work"
EXACT RECOVERY IN SBM WITH EDGE WEIGHTS AND NODE ATTRIBUTES,0.028142589118198873,"2.1
Exact recovery in SBM with edge weights and node attributes"
EXACT RECOVERY IN SBM WITH EDGE WEIGHTS AND NODE ATTRIBUTES,0.0300187617260788,"Community detection in classic SBM (binary edges) is a well-understood problem with strong
theoretical results concerning exact recovery and efficient algorithms with guaranteed accuracy
[1, 39]. However, extending the classic SBM to weighted networks (non-binary edges) with arbitrary
distributions is an ongoing research area. Most existing work in this scenario has been restricted to
the homogeneous model1, where edge weights within and across communities are determined by
two respective distributions. Moreover, existing works often restrict to categorical or real-valued
weights [22, 36], or to multiplex networks (multiple edge types) with independent and identically
distributed layers [29]. However, a recent work has provided a strong theoretical foundation the
homogeneous model with arbitrary distributions [5], highlighting the role of the Rényi divergence as
the key information-theoretic quantity for the homogeneous model."
EXACT RECOVERY IN SBM WITH EDGE WEIGHTS AND NODE ATTRIBUTES,0.03189493433395872,"In non-homogeneous models, a more complex divergence called the Chernoff-Hellinger divergence
is the appropriate information-theoretic quantity for exact community recovery [3]. However, the
expression of the Chernoff-Hellinger divergence as originally defined in [3] for binary networks
does not have an intuitive interpretation, and its extension to non-binary (weighted) networks is
challenging. For example, the exact recovery threshold for non-homogeneous SBM whose edges are
categorical random variables has been established [38], but this threshold is expressed as a condition
involving the minimization of a mixture of Kullback-Leibler divergences over the space of probability
distributions. Although the relationship between Kullback-Leibler and Chernoff divergences are
known (see for example [35, Theorem 30-32]), the specific technical lemma required to link them to
the Chernoff-Hellinger divergence is not straightforward (see [38, Claim 4])."
EXACT RECOVERY IN SBM WITH EDGE WEIGHTS AND NODE ATTRIBUTES,0.03377110694183865,"Another generalization of the SBM allows for nodes to have attributes that provide information
about their community, such as the Contextual SBM (CSBM) [15]. The CSBM has only been
rigorously studied in the setting where edges are binary and node attributes follow a Gaussian
distribution. In this scenario, the phase transition for exact recovery for the community labels has
been established [2, 8, 15]. A natural generalization is to investigate the model where network edges
have weights and nodes have attributes that follow arbitrary distributions. Indeed, this is one of the
main contributions of this work: Expression (3.4) gives a straightforward yet crucial formula for the
phase transition for exact recovery, also providing a natural interpretation for the influence of both
the network and node attributes. Moreover, Expression (3.4) also applies when no node attribute is
available, thus providing the exact recovery threshold for a non-homogeneous model and arbitrary
edge weight distribution, a significant advancement in the state of the art."
ALGORITHMS FOR CLUSTERING WEIGHTED NETWORKS WITH NODE ATTRIBUTES,0.03564727954971857,"2.2
Algorithms for clustering weighted networks with node attributes"
ALGORITHMS FOR CLUSTERING WEIGHTED NETWORKS WITH NODE ATTRIBUTES,0.0375234521575985,"Algorithms leveraging different approaches have been proposed to tackle community detection in
networks with edge weights and node attributes. A common principled approach is to determine the
community assignment that maximizes the likelihood function of a model for the data. However,
optimizing the likelihood function is computationally intractable even for binary networks. Thus,"
ALGORITHMS FOR CLUSTERING WEIGHTED NETWORKS WITH NODE ATTRIBUTES,0.039399624765478425,1Also known as the planted partition model.
ALGORITHMS FOR CLUSTERING WEIGHTED NETWORKS WITH NODE ATTRIBUTES,0.04127579737335835,"approximation schemes such as variational inference and pseudo-likelihood methods are often
adopted. For instance, [24] introduced a variational-EM algorithm for clustering non-homogeneous
weighted SBM with arbitrary distributions. Another approach for clustering node-attributed SBM
whose edge weights and attribute distributions belong to exponential families is [23]. These two
approaches assume that the network is dense (all edges are present and have non-zero edge weight).
However, most real networks are very sparse (most node pairs do not have an edge) and this work
focuses on this scenario. Another very recent work tackling sparse networks is the IR_sLs algorithm
from [8], although its theoretical guarantees assume binary networks with Gaussian attributes."
ALGORITHMS FOR CLUSTERING WEIGHTED NETWORKS WITH NODE ATTRIBUTES,0.043151969981238276,"The iterative clustering algorithm presented in this work maximizes the pseudo-likelihood likelihood
by assuming that the probability distribution of network edges and node attributes belong to exponen-
tial families. This yields a direct connection with Bregman divergences and establishes an elegant
expression for the likelihood function. This connection has also been leveraged in [23], however,
their model is restricted to dense weighted networks (all edges are present). This work (more specifi-
cally, Lemma 2), demonstrates that this connection can also be applied to sparse weighted networks
(using zero-inflated distributions to model the weights). This extension enhances the applicability
of pseudo-likelihood algorithms using Bregman divergence to a broader class of scenarios, namely
weighted sparse networks with node attributes."
MODEL AND EXACT RECOVERY IN NODE-ATTRIBUTED SBM,0.0450281425891182,"3
Model and exact recovery in node-attributed SBM"
MODEL DEFINITION,0.04690431519699812,"3.1
Model definition"
MODEL DEFINITION,0.04878048780487805,"Consider a population of n objects, called nodes, partitioned into K ≥2 disjoint sets, called blocks
or communities. A node-labelling vector z = (z1, · · · , zn) ∈[K]n represents this partitioning so
that zi indicates the block of node i. The labels (blocks) of nodes are random variables assumed to
be independent and identically distributed such that P(zi = k) = πk for some vector π ∈(0, 1)K
verifying that P"
MODEL DEFINITION,0.05065666041275797,"k πk = 1. The nodes interact in unordered pairs giving rise to undirected edges,
and X is the measurable space of all possible pairwise interactions. Additionally, each node has an
attribute that is an element of a measurable space Y. Let X ∈X N×N denote the symmetric matrix
such that Xij represents the interaction between node pair (ij), and by Y = (Y1, · · · , Yn) ∈Yn the
node attribute vector."
MODEL DEFINITION,0.0525328330206379,"Assume that interactions and attributes are independent conditionally on the community labels of the
nodes. Let fkℓ(x) denote the probability that two nodes in blocks k and ℓhave an interaction x ∈X,
and hk(y) denote the probability that a node in block k ∈[K] has an attribute y ∈Y. Thus,"
MODEL DEFINITION,0.054409005628517824,"P (X, Y | z) =
Y"
MODEL DEFINITION,0.05628517823639775,"1≤i<j≤n
fzizj(Xij) n
Y"
MODEL DEFINITION,0.058161350844277676,"i=1
hzi(Yi).
(3.1)"
MODEL DEFINITION,0.0600375234521576,"In the following, the interaction spaces X, Y might depend on n, as well as the respective interaction
probabilities f, h. The number of nodes n will increase to infinity while K and π are constant. For
an estimator ˆz ∈[K]n of z, we define the classification error as
loss(z, ˆz) = min
τ∈SK Ham(z, τ ◦ˆz),"
MODEL DEFINITION,0.06191369606003752,"where SK is the set of permutations of [K] and Ham(·, ·) is the hamming distance between two
vectors. An estimator ˆz = ˆz(X, Y ) achieves exact recovery if P (loss(z, ˆz) ≥1) = o(1)."
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.06378986866791744,"3.2
Exact recovery threshold in node-attributed SBM"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.06566604127579738,"The difficulty of classifying empirical data in one of K possible classes is traditionally measured by
the Chernoff information [12]. More precisely, in the context of network clustering, let CH(a, b) =
CH(a, b, π, f, h) denote the hardness of distinguishing nodes that belong to block a from block b.
This quantity is defined by
CH(a, b) =
sup
t∈(0,1)
CHt(a, b),
(3.2) where"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.0675422138836773,"CHt(a, b) = (1 −t) "" K
X"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.06941838649155722,"c=1
πc Dt (fbc∥fac) + 1"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.07129455909943715,n Dt (hb∥ha) # (3.3)
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.07317073170731707,"is the Chernoff coefficient of order t across blocks a and b, and Dt(f∥g) =
1
t−1 log
R
f t(x)g1−t(x)dx
is the Rényi divergence of order t between two probability densities f, g [35]. The key quantity
assessing the possibility or impossibility of exact recovery in SBM is then the minimal Chernoff
information across all pairs of clusters. We denote it by I = I(π, f, h), and it is defined by
I =
min
a,b∈[K]
a̸=b
CH(a, b).
(3.4)"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.075046904315197,"The following Theorem provides the information-theoretic threshold for exact recovery in node-
attributed SBM.
Theorem 1. Consider model (3.1) with πa > 0 for all a ∈[K]. Denote by a∗, b∗the two hardest
blocks to estimate, that is CH(a∗, b∗) = I. Suppose that t ∈(0, 1) 7→lim
n→∞
n
log nCHt(a∗, b∗) exists
and is strictly concave. Then the following holds:"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.07692307692307693,"(i) exact recovery is information-theoretically impossible if lim
n→∞
n
log nI < 1;"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.07879924953095685,"(ii) exact recovery is information-theoretically possible if lim
n→∞
n
log nI > 1."
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.08067542213883677,"The proof for Theorem 1 is provided in the supplemental material. The main ingredient of the
proof is the asymptotic study of log-likelihood ratios. More precisely, let L(z) = P(X, Y | z). The
application of Chernoff bounds provides an upper bound on −log P (log L(z) ≥log L(z′)), where z
denotes the correct block structure and z′ ∈[K]n is another node-labelling vector (see Lemma 1
in the supplement). We can use this upper bond to prove that the maximum likelihood estimator
(MLE) achieves exact recovery if I > n−1 log n. Reciprocally, if I < n−1 log n, we show that whp
there exist some ""bad"" nodes i for which zi ̸= arg maxa∈[k] P (X, Y | z−i, zi = a). In other words,
even in a setting where an oracle would reveal z−i (i.e., the correct block assignment of all nodes
except node i), the MLE would fail at recovering zi. Establishing this fact requires to lower bound
−log P (log L(z) ≥log L(z′)) where z′ ∈[K]n is such that Ham(z, z′) = 1 (i.e., z′ correctly labels
all nodes except one). This lower bound is obtained using large deviation results for general random
variables [10, 30]. To apply these results, the strict concavity of the limit n(log n)−1I is needed. In
most practical settings, this assumption is verified, except in some edge cases (see Examples 1 and 2).
Let us now provide some examples of applications of Theorem 1.
Example 1 (Binary SBM with no attributes). Suppose that fab ∼Ber(αabn−1 log n) where αab are
constants. A Taylor-expansion of the Rényi divergence between Bernoulli distributions leads to"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.0825515947467167,I = (1 + o(1))log n
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.08442776735459662,"n
min
a̸=b sup
t∈(0,1) X"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.08630393996247655,"c
πc

tαbc + (1 −t)αac −αt
bcα1−t
ac

! ,"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.08818011257035648,"which indeed coincides with the expression of the Chernoff-Hellinger divergence defined in [3]. We
also note that the limit n(log n)−1I is strictly concave as long as the αab are not all equals2.
Example 2 (Binary SBM with Gaussian attributes). Suppose that fab ∼Ber(αabn−1 log n) and
ha ∼Nor(µa log n, σ2Id), where αab and µa are independent of n. Then,"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.0900562851782364,I = (1 + o(1))log n
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.09193245778611632,"n
min
a̸=b sup
t∈(0,1) X"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.09380863039399624,"c
πc

tαbc + (1 −t)αac −αt
bcα1−t
ac

+ t∥µb −µa∥2
2
2σ2 ! ."
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.09568480300187618,"In particular, the technical conditions of Theorem 1 are verified if we rule out the uninformative case
where all the αab’s and the µa’s are equal to each other. Thus, exact recovery is possible if"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.0975609756097561,"min
a̸=b sup
t∈(0,1) X"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.09943714821763602,"c
πc

tαbc + (1 −t)αac −αt
bcα1−t
ac

+ t∥µb −µa∥2
2
2σ2 ! > 1."
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.10131332082551595,"Further assuming that αab = α1(a = b) + β1(a ̸= b) (homogeneous interactions) and π =
  1"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.10318949343339587,"K , · · · , 1"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.1050656660412758,"K

(uniform block probabilities), the expression of I simplifies to"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.10694183864915573,I = (1 + o(1))log n n
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.10881801125703565,""" √α −√β
2"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.11069418386491557,"K
+ ∆2 8σ2 # ,"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.1125703564727955,"2Indeed, since the matrix α = (αab) is symmetric, this implies that there exists a ̸= b such that ∃c∗∈
[k]: αac∗̸= αbc∗. The function f(t) = P"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.11444652908067542,"c πc

tαbc + (1 −t)αac −αt
bcα1−t
ac
 is continuous and strictly
concave, hence supt∈(0,1) f(t) is strictly concave."
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.11632270168855535,"where ∆= min
a̸=b ∥µa −µb∥2. This last scenario recovers the recently established threshold for exact"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.11819887429643527,"recovery in the Contextual SBM [8].
Example 3 (Semi-supervised clustering in SBM). Consider binary interactions given by"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.1200750469043152,"fab ∼
Ber(αn−1 log n)
if a = b,
Ber(βn−1 log n)
otherwise,"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.12195121951219512,"where α, β are independent of n. Consider a semi-supervised model in which the vector of attributes
Y is a noisy oracle of the true community labels z. More precisely, for a node i such that zi = k, let"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.12382739212007504,"hk(y) = P(Yi = y) = 
 "
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.12570356472795496,"1 −η
if y = 0,
η1
if y = k,
η0
K−1
if y ∈[K]\{k},"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.1275797373358349,"with η0 + η1 = η. A bit of algebra shows that exact recovery is possible if
√α −
p"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.1294559099437148,"β
2
−
2
log n log

1 −η + 2(K −1)−1/2√η0η1

> K."
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.13133208255159476,"When η0 = 0 (perfect oracle), the condition simplifies to (√a −
√"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.13320825515947468,b)2 −2 log(1−η)
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.1350844277673546,"log n
> K. Note that
the oracle term is non-negligible only if −log (1 −η) ≳log n, as previously established [31]. This
last condition is very strong since it implies η ≳1 −1/n, and hence the oracle must provide the
correct label for almost all nodes."
BREGMAN HARD CLUSTERING OF SPARSE WEIGHTED NODE-ATTRIBUTED NETWORKS,0.13696060037523453,"4
Bregman hard clustering of sparse weighted node-attributed networks"
BREGMAN HARD CLUSTERING OF SPARSE WEIGHTED NODE-ATTRIBUTED NETWORKS,0.13883677298311445,"In this section, we will propose an algorithm for clustering sparse, weighted networks with node
attributes. When present, the weights are sampled from an exponential family, and the node attributes
also belong to an exponential family. In Section 4.1, we provide some reminder of exponential families.
We derive the likelihood of the model in Section 4.2, and present the algorithm in Section 4.3."
EXPONENTIAL FAMILY,0.14071294559099437,"4.1
Exponential family"
EXPONENTIAL FAMILY,0.1425891181988743,"An exponential family Eψ is a parametric class of probability distributions whose densities can
be canonically written as pθ,ψ(x) = e<θ,x>−ψ(θ), where the density is taken with respect to an
appropriate measure, θ ∈Θ is a function of the parameters of the distribution that must belong to an
open convex space Θ, and ψ is a convex function."
EXPONENTIAL FAMILY,0.14446529080675422,"We consider the model defined in (3.1), such that fab are zero-inflated distributions and are given by"
EXPONENTIAL FAMILY,0.14634146341463414,"fab(x) = (1 −pab)δ0(x) + pab ˜fab(x),
(4.1)"
EXPONENTIAL FAMILY,0.14821763602251406,"where pab ∈[0, 1] is the interaction probability between blocks a and b, δ0(x) is the Dirac delta at
zero, and ˜fab is a probability density with no mass at zero. Note that this model can represent sparse
weighted networks, as edges between nodes in blocks a and b are absent with probability 1 −pab."
EXPONENTIAL FAMILY,0.150093808630394,"Finally, suppose that the distributions { ˜fab} and {ha} belong to exponential families. More precisely,"
EXPONENTIAL FAMILY,0.15196998123827393,"˜fab(x) = e<θab,x>−ψ(θab)
and
ha(y) = e<ηa,y>−ϕ(ηa),
(4.2)"
EXPONENTIAL FAMILY,0.15384615384615385,"for some parameters θab, ηa and functions ψ, ϕ. The following lemma provides the expression of the
Chernoff divergence of this model.
Lemma 1. Let fab and ha be defined as in (4.1)-(4.2). Suppose that pab = αabδ where αab is
constant and δ ≪1. We have"
EXPONENTIAL FAMILY,0.15572232645403378,"I = (1 + o(1)) min
a̸=b sup
t∈(0,1) 
  X"
EXPONENTIAL FAMILY,0.1575984990619137,"c∈[K]
πc
h
tpac + (1 −t)pbc −pt
acp1−t
bc
e−Jψ(θac∥θbc)i
+ Jϕ(ηa∥ηb) 
 ,"
EXPONENTIAL FAMILY,0.15947467166979362,where Jψ(θab∥θbc) = tψ(θac) + (1 −t)ψ(θbc) −ψ(tθac + (1 −t)θbc).
EXPONENTIAL FAMILY,0.16135084427767354,"In combination with Theorem 1, Lemma 1 provides the expression for the exact recovery threshold
for node-attributed networks with interaction and attribute distributions given by (4.1)-(4.2)."
LOG-LIKELIHOOD,0.16322701688555347,"4.2
Log-likelihood"
LOG-LIKELIHOOD,0.1651031894934334,"Given a convex function ψ, the Bregman divergence dψ : Rm × Rm →R+ is defined by"
LOG-LIKELIHOOD,0.1669793621013133,"dψ(x, y) = ψ(x) −ψ(y)−< x −y, ∇ψ(y) > ."
LOG-LIKELIHOOD,0.16885553470919323,"The log-likelihood of the density pψ,θ of an exponential family distribution is linked to the Bregman
divergence by the following relationship (see for example [6, Equation 13])"
LOG-LIKELIHOOD,0.17073170731707318,"log pψ,θ(x) = −dψ∗(x, µ) + ψ∗(x),
(4.3)"
LOG-LIKELIHOOD,0.1726078799249531,"where µ = Epψ,θ(X) is the mean of the distribution, and ψ∗denotes the Legendre transform of ψ,
defined by ψ∗(t) = supθ{< θ, t > −ψ(θ)}. The following Lemma provides an expression for the
log-likelihood of fab when fab is a distribution belonging to a zero-inflated exponential family."
LOG-LIKELIHOOD,0.17448405253283303,"Lemma 2. Let fab be a probability density as defined in (4.1)-(4.2). For x, y ∈(0, 1), let dKL(x, y)
be the Kullback-Leibler divergences between Ber(x) and Ber(y), and let H(x) = x log x + (1 −
x) log(1 −x), with the usual convention 0 log 0 = 1. Then,"
LOG-LIKELIHOOD,0.17636022514071295,"−log fab(x) = dKL (x∥pab) + sdψ∗(x, µab) −sψ∗(x) −H(s),"
LOG-LIKELIHOOD,0.17823639774859287,where s = 1(x ̸= 0).
LOG-LIKELIHOOD,0.1801125703564728,"Proof. To express log fab(x), we first note that"
LOG-LIKELIHOOD,0.18198874296435272,"log fab(x) = (1 −s) log(1 −pab) + s log pab + s log( ˜fab(x)),"
LOG-LIKELIHOOD,0.18386491557223264,"where c = 1(x ̸= 0). The result follows by adding and subtracting H(b) in the previous expression
and expressing log ˜fab(x) with a Bregman divergence as in (4.3)."
LOG-LIKELIHOOD,0.18574108818011256,"Suppose that X, Y follow the model (3.1) with probability distributions given by (4.1)-(4.2). Let A
be a binary matrix such that Aij = 1(Xij ̸= 0). We have"
LOG-LIKELIHOOD,0.18761726078799248,"−log P(X, Y | z) =
X i 
 
1
2 X j̸=i"
LOG-LIKELIHOOD,0.1894934333958724,"
dKL(Aij, pzizj) + Aijdψ∗ 
Xij, µzizj

+ dϕ∗(Yi, νzi) 
 + c,"
LOG-LIKELIHOOD,0.19136960600375236,"where the additional term c is a function of X, Y but does not depend on z. Denoting Z ∈{0, 1}n×K"
LOG-LIKELIHOOD,0.19324577861163228,"the one-hot membership matrix such that Zik = 1(zi = k), observe that pzizj =
 
ZpZT "
LOG-LIKELIHOOD,0.1951219512195122,ij where p
LOG-LIKELIHOOD,0.19699812382739212,"is a symmetric matrix with the interaction probabilities between different blocks, µzizj =
 
ZµZT "
LOG-LIKELIHOOD,0.19887429643527205,"ij
where µ is a symmetric matrix with the expected value of the interaction between different blocks
(edge weights), and νzi =
 
ZT ν
"
LOG-LIKELIHOOD,0.20075046904315197,"i where ν is a vector with the expected value of the attribute for
different blocks. Thus, up to some additional constants, the negative log-likelihood −log P(X, Y | Z)
is equal to X i 1"
DKL,0.2026266416510319,"2 dKL
 
Ai·,
 
ZpZT "
DKL,0.2045028142589118,"i·

+ 1"
DKL,0.20637898686679174,"2d′
ψ∗
 
Xi·,
 
ZµZT "
DKL,0.20825515947467166,"i·

+ dϕ∗ 
Yi,
 
ZT ν
"
DKL,0.2101313320825516,"i

+ c,
(4.4)"
DKL,0.21200750469043153,"where d′
ψ∗(B, C) = Pn
j=1 1(Bj ̸= 0)dψ∗(Bj, Cj) for two vectors B, C ∈Rn."
CLUSTERING BY ITERATIVE LIKELIHOOD MAXIMISATION,0.21388367729831145,"4.3
Clustering by iterative likelihood maximisation"
CLUSTERING BY ITERATIVE LIKELIHOOD MAXIMISATION,0.21575984990619138,"Following the log-likelihood expression derived in (4.4), we propose an iterative clustering algorithm
that places each node in the block maximising P (X, Y | z−i, zi = a) for 1 ≤a ≤K, the likelihood
that node i is in community a given the community labels of the other nodes, z−i. Let Z(ia) denote
the membership matrix obtained from Z by placing node i in block a, and let Lia(Z(ia)) denote the
contribution of node i to the negative log-likelihood when node i is placed in block a. Equation (4.4)
shows that"
CLUSTERING BY ITERATIVE LIKELIHOOD MAXIMISATION,0.2176360225140713,Lia(Z) = 1
DKL,0.21951219512195122,"2 dKL
 
Ai·,
 
ZpZT "
DKL,0.22138836772983114,"i·

+ 1"
DKL,0.22326454033771106,"2d′
ψ∗
 
Xi·,
 
ZµZT "
DKL,0.225140712945591,"i·

+ dϕ∗ 
Yi,
 
ZT ν
"
DKL,0.2270168855534709,"i

,
(4.5)"
DKL,0.22889305816135083,"where the p, µ and ν in the equation above must be estimated from X, Y , and the community
membership matrix Z. Let ˆp = ˆp(A, Z), ˆµ = ˆµ(X, Z), and ˆν = ˆν(Y, Z) denote the estimators for p,
µ and ν, respectively. Their values can be computed as follows:"
DKL,0.23076923076923078,"ˆp(A, Z) =
 
ZT Z
−1 ZT AZ
 
ZT Z
−1 ,"
DKL,0.2326454033771107,"ˆµ(X, Z) =
 
ZT AZ
−1 ZT XZ,"
DKL,0.23452157598499063,"ˆν(Y, Z) =
 
ZT Z
−1 ZT Y. (4.6)"
DKL,0.23639774859287055,"Note that the matrix inverse
 
ZT Z
−1 can be easily computed since ZT Z is a K-by-K diagonal
matrix. This approach is described in Algorithm 1."
DKL,0.23827392120075047,"Algorithm 1: Bregman hard clustering for node-attributed SBM.
Input: Interactions X ∈X n×n, attributes Y ∈Yn, convex functions ψ∗, ϕ∗, clustering Z0
1 Let Z = Z0
2 repeat"
DKL,0.2401500938086304,"3
Compute ˆp, ˆµ, ˆν according to (4.6)"
DKL,0.24202626641651032,"4
Let Znew = 0n×K
5
for i = 1, . . . , n do"
DKL,0.24390243902439024,"6
Let Z(ia) be the membership matrix obtained from Z by placing node i in community a"
DKL,0.24577861163227016,"7
Find k∗= arg max
a∈[K]
Lia
 
Z(ia)
, where Lia
 
Z(ia)
is defined in (4.5);"
"LET ZNEW
IK",0.24765478424015008,"8
Let Znew
ik
= 1(k = k∗) for all k = 1, . . . , K"
"LET ZNEW
IK",0.24953095684803,"9
Let Z = Znew"
"LET ZNEW
IK",0.25140712945590993,10 until convergence;
"LET ZNEW
IK",0.25328330206378985,Return: Node-membership matrix Z
"LET ZNEW
IK",0.2551594746716698,"A fundamental aspect of many likelihood maximization iterative algorithms such as Algorithm 1
is the initial membership assignment, Z0. This initial assignment often has a profound influence
on the final membership assignment, and thus, it is important to have an adequate initialization. In
the numerical section, we proceed as follows. We construct the matrix W ∈Rn×2K such that the
first K columns of W are the first K eigenvectors of the graph normalised Laplacian, while the
last K columns of W are the first K eigenvectors of the Gram matrix Y Y T ."
NUMERICAL EXPERIMENTS,0.2570356472795497,"5
Numerical experiments"
NUMERICAL EXPERIMENTS,0.2589118198874296,"5.1
Performance of Algorithm 1"
NUMERICAL EXPERIMENTS,0.2607879924953096,"We first compare in Figure 1 the performance of Algorithm 1 in terms of exact recovery (fraction of
times the algorithm correctly recovers the community of all nodes) with the theoretical threshold for
exact recovery proved in the paper (red curve in the plots) in two settings: Figure 1a shows binary
weight with Gaussian attributes, and Figure 1b shows zero-inflated Gaussian weights with Gaussian
attributes. A solid black (resp., white) square means that over 50 trials, the algorithms failed 50 times
(resp., succeeded 50 times) at exactly recovering the block structure."
COMPARISON WITH OTHER ALGORITHMS,0.2626641651031895,"5.2
Comparison with other algorithms"
COMPARISON WITH OTHER ALGORITHMS,0.26454033771106944,"In this section, we compare Algorithm 1 with other algorithms presented in the literature. We used
the Adjusted Rand Index (ARI) [21] between the predicted clusters and the ground truth ones to
evaluate the performance of each algorithm."
COMPARISON WITH OTHER ALGORITHMS,0.26641651031894936,"In Figure 2, we compare Algorithm 1 with the variational-EM algorithm of [24] and the algorithm
of [23] (which is also based on Bregman divergences, but tailored for dense networks). Because both
of these algorithms are designed for dense networks, we observe that Algorithm 1 has overall better
performance on sparse networks."
COMPARISON WITH OTHER ALGORITHMS,0.2682926829268293,"We also compare Algorithm 1 with the IR_sLs algorithm from [8]. This is one of the most recent
algorithms for node-attributed SBM and it comes with theoretical guarantees (for binary networks"
COMPARISON WITH OTHER ALGORITHMS,0.2701688555347092,"1
2
3
4
5
6
alpha 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 r 0.0 0.2 0.4 0.6 0.8 1.0"
COMPARISON WITH OTHER ALGORITHMS,0.27204502814258913,(a) Binary weights with Gaussian attributes
COMPARISON WITH OTHER ALGORITHMS,0.27392120075046905,0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 mu 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 r 0.0 0.2 0.4 0.6 0.8 1.0
COMPARISON WITH OTHER ALGORITHMS,0.275797373358349,"(b) zero-inflated Gaussian weights with Gaussian
attributes."
COMPARISON WITH OTHER ALGORITHMS,0.2776735459662289,"Figure 1: Phase transition of exact recovery. Each pixel represents the empirical probability that
Algorithm 1 succeeds at exactly recovering the clusters (over 50 runs), and the red curve shows the
theoretical threshold.
(a) n = 500, K = 2, fin = Ber(αn−1 log n), fout = Ber(n−1 log n). The attributes are 2d-
spherical Gaussian with radius (±r√log n, 0) and identity covariance matrix.
(b) n = 600, K = 3, fin = (1 −ρ)δ0 + ρ Nor(µ, 1), fout = (1 −ρ)δ0 + ρ Nor(0, 1) with
ρ = 5n−1 log n. The attributes are 2d-spherical Gaussian whose means are the vertices of a regular
polygon on the circle of radius r√log n."
COMPARISON WITH OTHER ALGORITHMS,0.2795497185741088,"0.03
0.05
0.07
0.09
0.11
p 0.00 0.25 0.50 0.75 1.00 ARI"
COMPARISON WITH OTHER ALGORITHMS,0.28142589118198874,"Algorithm 1
bregman [23]
vem [24]
initialisation"
COMPARISON WITH OTHER ALGORITHMS,0.28330206378986866,"1
3
5
7
9
11
muin 0.00 0.25 0.50 0.75 1.00 ARI"
COMPARISON WITH OTHER ALGORITHMS,0.2851782363977486,"Algorithm 1
bregman [23]
vem [24]
initialisation"
COMPARISON WITH OTHER ALGORITHMS,0.2870544090056285,"Figure 2: Comparison of Algorithm 1 with algorithms of [23] and [24]. Error bars show the standard
deviations over 25 realisations. Attributes are 2d-spherical Gaussian attributes with radius (±1, 0).
(a) n = 100, K = 2, fin = (1 −pin)δ0 + pin Poi(5), fout = (1 −0.03)δ0 + 0.03 Poi(1).
(b) n = 100, K = 2, fin = (1 −0.07)δ0 + 0.07 Poi(µin), fout = (1 −0.04)δ0 + 0.04 Poi(1)."
COMPARISON WITH OTHER ALGORITHMS,0.28893058161350843,"with Gaussian attributes). We also compare with the EM algorithm of [34], attSBM, which is
designed for binary networks with Gaussian attributes. Finally, we compare with baseline methods
for clustering using the network or the attributes alone. EM-GMM refers to fitting a Gaussian Mixture
Model via EM on attribute data Y , and sc refers to spectral clustering on network data X."
COMPARISON WITH OTHER ALGORITHMS,0.29080675422138835,"Figure 3 shows the results for binary networks with Gaussian attributes. Algorithm 1 successfully
learns from both the signal coming from the network and the attributes, even in scenarios where
one of them is non-informative. Moreover, Algorithm 1 has better performance than the two other
node-attributed clustering algorithms, and those algorithms also show a large variance3. We also note
that IR_sLs and attSBM are both tailor-made for binary edges and Gaussian attributes. Even in such
a setting, Algorithm 1 outperforms these two algorithms. We show in the supplement material that
when the network is weighted and the attributes non-Gaussian, IR_sLs and attSBM perform poorly."
EVALUATION USING REAL DATASETS,0.2926829268292683,"5.3
Evaluation using real datasets"
EVALUATION USING REAL DATASETS,0.2945590994371482,"The following three benchmark datasets were used to evaluate and compare the proposed algorithm:
CiteSeer (n = 3279, m = 9104, K = 6, d = 3703), Cora (n = 2708, m = 10556, K = 7,"
EVALUATION USING REAL DATASETS,0.2964352720450281,"3As the large variance makes the figures less readable, we provide all results in the supplemental material."
EVALUATION USING REAL DATASETS,0.29831144465290804,"(a) Varying edge distribution.
(b) Varying attribute distributions."
EVALUATION USING REAL DATASETS,0.300187617260788,"Figure 3: Performance on binary networks with Gaussian attributes. We take n = 600, K = 2,
fin = Ber(an−1 log n), fout = Ber(5n−1 log n), and 2-dimensional Gaussian attributes with unit
variance and mean (±r, 0). Results are averaged over 60 runs."
EVALUATION USING REAL DATASETS,0.30206378986866794,"d = 1433), and Cornell (n = 183, m = 298, K = 5, d = 1703) (all available in Pytorch Geometric).
For each network, the original node attribute vector was reduced to have dimension d = 10 by
selecting the 10 best features according to the chi-square test. Algorithm 1 assumed a multivariate
Gaussian distribution with d = 10 for node attributes and Bernoulli edges (these networks have no
edge weights). The initialization for Algorithm 1 and attSBM used spectral clustering of both the
node similarity matrix (using node attributes) and network edges."
EVALUATION USING REAL DATASETS,0.30393996247654786,"Dataset
CiteSeer
Cora
Cornell"
EVALUATION USING REAL DATASETS,0.3058161350844278,"Algorithm 1
0.20
0.12
0.49
attSBM
0.17
0.09
0.46
EM-GMM
0.13
0.06
0.37
sc
0.00
0.00
0.02
Table 1: Average ARI results (over independent runs) for the three benchmark datasets."
EVALUATION USING REAL DATASETS,0.3076923076923077,"Table 1 shows that Algorithm 1 outperformed the other three algorithms. Spectral clustering (sc) has
near zero performance, indicating that the network structure in these data sets has little information
concerning the clusters of the nodes. Moreover, both Algorithm 1 and attSBM (that leverage network
and node attributes) outperform EM-GMM that use only node attributes. These preliminary results
indicate that Algorithm 1 is promising even in real data sets with little pre-processing."
CONCLUSION,0.30956848030018763,"6
Conclusion"
CONCLUSION,0.31144465290806755,"This work made the following contributions to community detection in node-attributed networks: i)
extended the known thresholds for exact recovery in binary SBM to non-binary (weighted) networks
with node attributes, providing a clean expression for a new information-theoretic quantity, known
in the binary setting as the Chernoff-Hellinger divergence; ii) proposed an iterative algorithm based
on the likelihood function that can infer community memberships from a problem instance. The
algorithm leverages the framework of Bregman divergences and is simple and computationally
efficient. Numerical experiments indicate the superiority of this algorithm when compared to recent
state-of-the-art approaches."
CONCLUSION,0.3133208255159475,Acknowledgements
CONCLUSION,0.3151969981238274,The first author would like to thank Lasse Leskelä for helpful discussions and comments.
CONCLUSION,0.3170731707317073,"This work has been partially funded by the Brazilian-Swiss Joint Research Program (grant
IZBRZ2_186313), the Brazilian National Council for Scientific and Technological Development
(CNPq), and the Carlos Chagas Filho Research Foundation of the State of Rio de Janeiro (FAPERJ)."
REFERENCES,0.31894934333958724,References
REFERENCES,0.32082551594746717,"[1] Emmanuel Abbe. Community detection and stochastic block models: recent developments. The Journal of
Machine Learning Research, 18(1):6446–6531, 2017."
REFERENCES,0.3227016885553471,"[2] Emmanuel Abbe, Jianqing Fan, and Kaizheng Wang. An ℓp theory of pca and spectral clustering. The
Annals of Statistics, 50(4):2359–2385, 2022."
REFERENCES,0.324577861163227,"[3] Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block models: Fundamental
limits and efficient algorithms for recovery. In 2015 IEEE 56th Annual Symposium on Foundations of
Computer Science, pages 670–688, Los Alamitos, CA, USA, 2015. IEEE Computer Society."
REFERENCES,0.32645403377110693,"[4] Konstantin Avrachenkov and Maximilien Dreveton. Statistical Analysis of Networks. Now Publishers,
2022."
REFERENCES,0.32833020637898686,"[5] Konstantin Avrachenkov, Maximilien Dreveton, and Lasse Leskelä. Community recovery in non-binary
and temporal stochastic block models. arXiv preprint arXiv:2008.04790, 2022."
REFERENCES,0.3302063789868668,"[6] Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, Joydeep Ghosh, and John Lafferty. Clustering with
Bregman divergences. Journal of machine learning research, 6(10), 2005."
REFERENCES,0.3320825515947467,"[7] Norbert Binkiewicz, Joshua T Vogelstein, and Karl Rohe. Covariate-assisted spectral clustering. Biometrika,
104(2):361–377, 2017."
REFERENCES,0.3339587242026266,"[8] Guillaume Braun, Hemant Tyagi, and Christophe Biernacki. An iterative clustering algorithm for the
contextual stochastic block model with optimality guarantees. In International Conference on Machine
Learning, pages 2257–2291. PMLR, 2022."
REFERENCES,0.33583489681050654,"[9] Andressa Cerqueira and Elizaveta Levina. A pseudo-likelihood approach to community detection in
weighted networks. arXiv preprint arXiv:2303.05909, 2023."
REFERENCES,0.33771106941838647,"[10] Narasinga Rao Chaganty and Jayaram Sethuraman. Strong large deviation and local limit theorems. The
Annals of Probability, pages 1671–1690, 1993."
REFERENCES,0.3395872420262664,"[11] Xiaohui Chen and Yun Yang. Cutoff for exact recovery of Gaussian mixture models. IEEE Transactions
on Information Theory, 67(6):4223–4238, 2021."
REFERENCES,0.34146341463414637,"[12] Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of
observations. The Annals of Mathematical Statistics, pages 493–507, 1952."
REFERENCES,0.3433395872420263,"[13] Michael Collins, Sanjoy Dasgupta, and Robert E Schapire. A generalization of principal components
analysis to the exponential family. Advances in neural information processing systems, 14, 2001."
REFERENCES,0.3452157598499062,"[14] David Combe, Christine Largeron, Mathias Géry, and El˝od Egyed-Zsigmond. I-Louvain: An attributed
graph clustering method. In Advances in Intelligent Data Analysis XIV: 14th International Symposium,
IDA 2015, Saint Etienne. France, October 22-24, 2015. Proceedings 14, pages 181–192. Springer, 2015."
REFERENCES,0.34709193245778613,"[15] Yash Deshpande, Subhabrata Sen, Andrea Montanari, and Elchanan Mossel. Contextual stochastic block
models. Advances in Neural Information Processing Systems, 31, 2018."
REFERENCES,0.34896810506566606,"[16] Issam Falih, Nistor Grozavu, Rushed Kanawati, and Younès Bennani. Community detection in attributed
network. In Companion proceedings of the the web conference 2018, pages 1299–1306, 2018."
REFERENCES,0.350844277673546,"[17] Cees M Fortuin, Pieter W Kasteleyn, and Jean Ginibre. Correlation inequalities on some partially ordered
sets. Communications in Mathematical Physics, 22:89–103, 1971."
REFERENCES,0.3527204502814259,"[18] Santo Fortunato and Darko Hric. Community detection in networks: A user guide. Physics reports,
659:1–44, 2016."
REFERENCES,0.3545966228893058,"[19] Geoffrey Grimmett. Percolation. Springer, 1999."
REFERENCES,0.35647279549718575,"[20] Theodore E. Harris. A lower bound for the critical probability in a certain percolation process. In
Mathematical Proceedings of the Cambridge Philosophical Society, volume 56, pages 13–20. Cambridge
University Press, 1960."
REFERENCES,0.35834896810506567,"[21] Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of classification, 2:193–218, 1985."
REFERENCES,0.3602251407129456,"[22] Varun Jog and Po-Ling Loh. Recovering communities in weighted stochastic block models. In Proceedings
of the 53rd Annual Allerton Conference on Communication, Control, and Computing, Los Alamitos, CA,
USA, October 2015. IEEE Computer Society."
REFERENCES,0.3621013133208255,"[23] Bo Long, Zhongfei Mark Zhang, and Philip S Yu. A probabilistic framework for relational clustering. In
ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD), pages 470–479,
2007."
REFERENCES,0.36397748592870544,"[24] Mahendra Mariadassou, Stéphane Robin, and Corinne Vacher. Uncovering latent structure in valued graphs:
A variational approach. The Annals of Applied Statistics, 4(2):715 – 742, 2010."
REFERENCES,0.36585365853658536,"[25] Angelo Mele, Lingxin Hao, Joshua Cape, and Carey E Priebe. Spectral inference for large stochastic
blockmodels with nodal covariates. arXiv preprint arXiv:1908.06438, 2019."
REFERENCES,0.3677298311444653,"[26] Mohamed Ndaoud. Sharp optimal recovery in the two component Gaussian mixture model. The Annals of
Statistics, 50(4):2096–2126, 2022."
REFERENCES,0.3696060037523452,"[27] Mark EJ Newman and Aaron Clauset. Structure and inference in annotated networks. Nature communica-
tions, 7(1):11863, 2016."
REFERENCES,0.3714821763602251,"[28] Frank Nielsen. Chernoff information of exponential families. arXiv preprint arXiv:1102.2684, 2011."
REFERENCES,0.37335834896810505,"[29] Subhadeep Paul and Yuguo Chen. Consistent community detection in multi-relational data through
restricted multi-layer stochastic blockmodel. Electronic Journal of Statistics, 10(2):3807 – 3870, 2016."
REFERENCES,0.37523452157598497,"[30] Detlef Plachky and Joseph Steinebach. A theorem about probabilities of large deviations with an application
to queuing theory. Periodica Mathematica Hungarica, 6(4):343–345, 1975."
REFERENCES,0.3771106941838649,"[31] Hussein Saad and Aria Nosratinia. Community detection with side information: Exact recovery under the
stochastic block model. IEEE Journal of Selected Topics in Signal Processing, 12(5):944–958, 2018."
REFERENCES,0.3789868667917448,"[32] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93–93, 2008."
REFERENCES,0.3808630393996248,"[33] Laura M Smith, Linhong Zhu, Kristina Lerman, and Allon G Percus. Partitioning networks with node
attributes by compressing information flow. ACM Transactions on Knowledge Discovery from Data
(TKDD), 11(2):1–26, 2016."
REFERENCES,0.3827392120075047,"[34] Natalie Stanley, Thomas Bonacci, Roland Kwitt, Marc Niethammer, and Peter J Mucha. Stochastic block
models with multiple continuous attributes. Applied Network Science, 4(1):1–22, 2019."
REFERENCES,0.38461538461538464,"[35] Tim Van Erven and Peter Harremos. Rényi divergence and Kullback-Leibler divergence. IEEE Transactions
on Information Theory, 60(7):3797–3820, 2014."
REFERENCES,0.38649155722326456,"[36] Min Xu, Varun Jog, and Po-Ling Loh. Optimal rates for community estimation in the weighted stochastic
block model. The Annals of Statistics, 48(1):183 – 204, 2020."
REFERENCES,0.3883677298311445,"[37] Bowei Yan and Purnamrita Sarkar. Covariate regularized community detection in sparse graphs. Journal
of the American Statistical Association, 116(534):734–745, 2021."
REFERENCES,0.3902439024390244,"[38] Se-Young Yun and Alexandre Proutiere. Optimal cluster recovery in the labeled stochastic block model.
Advances in Neural Information Processing Systems, 29, 2016."
REFERENCES,0.3921200750469043,"[39] Anderson Y. Zhang and Harrison H. Zhou. Minimax rates of community detection in stochastic block
models. The Annals of Statistics, 44(5):2252 – 2280, 2016."
REFERENCES,0.39399624765478425,"A
Establishing the exact recovery threshold"
REFERENCES,0.39587242026266417,"The proof of Theorem 1 is structured as follows. We start by establishing some concentration
results on the block sizes in Section A.1. In Section A.2, we delve into the asymptotic analysis
of log-likelihood ratios, establishing fundamental results. Building upon these findings, we prove
the converse statement, demonstrating the impossibility of exact recovery below the threshold, in
Section A.3. Conversely, in Section A.4, we establish the positive statement, demonstrating the
possibility of exact recovery above the threshold. To complement the proof, Section A.5 presents a
set of technical combinatorial lemmas."
REFERENCES,0.3977485928705441,"Notations
In the following, we denote by z ∈[K]n the true block-labelling vector, and by z′ ∈
[K]n another block-labelling vector, and the conditional probabilities are denoted by Pz(·) = P(· | z)."
REFERENCES,0.399624765478424,"A.1
Preliminaries on the block sizes"
REFERENCES,0.40150093808630394,"For any z′ ∈[K]n, we for all c ∈[K] define ˆπc(z′), the empirical size of block c, as"
REFERENCES,0.40337711069418386,"ˆπc(z′) = |{i ∈[n]: z′
i = c}|
n
."
REFERENCES,0.4052532833020638,"For any 0 < ξ < 1, we define"
REFERENCES,0.4071294559099437,"Zξ = {z ∈[K]n : (1 −ξ)πc ≤ˆπc(z) ≤(1 + ξ)πc
∀c ∈[K]} .
(A.1)"
REFERENCES,0.4090056285178236,"Recalling that the true block-labelling vector z is sampled from π⊗n, we have by concentration of
multinomial distributions (see for example [5, Lemma A1])"
REFERENCES,0.41088180112570355,"P(Zξ) ≥1 −2K exp

−ξ2"
N MIN,0.41275797373358347,"3 n min
c∈[K] πc 
."
N MIN,0.4146341463414634,"In the following, we chose ξ = log log n
√n
, and hence P(Zξ) = 1 −o(1)."
N MIN,0.4165103189493433,"A.2
Asymptotic study of log-likelihood ratios"
N MIN,0.41838649155722324,"This section studies the asymptotic behaviour of Pz (L(z) ≥L(z′)), where L(z′) = Pz′(X, Y ) the
likelihood of the block-labelling vector z′ given the observed data X, Y ."
N MIN,0.4202626641651032,"A.2.1
Upper-bound"
N MIN,0.42213883677298314,"The following lemma provides an upper bound on Pz (L(z) ≥L(z′)).
Lemma 3. Let z, z′ ∈Z be two block labelling vectors such that Ham(z, z′) = m ≥1, and let
L(z) = P(X, Y | z) be the likelihood of labelling z. We have"
N MIN,0.42401500938086306,"Pz (L(z′) ≥L(z)) ≤e
−(1+o(1))

1−
m
nπmin"
N MIN,0.425891181988743,"
mnI."
N MIN,0.4277673545966229,"Moreover, let d = sup
t∈(0,1)
min
a̸=b∈[K]
c∈[K]
(1 −t) Dt(fac∥fbc). Then we also have"
N MIN,0.42964352720450283,Pz (L(z′) ≥L(z)) ≤e−m(n−m)d.
N MIN,0.43151969981238275,"Proof. Using Chernoff’s bound, we have for all t ≥0, Pz"
N MIN,0.4333958724202627,"
log Pz′(X, Y )"
N MIN,0.4352720450281426,"Pz(X, Y ) > ϵ

≤e−tϵ Ez"
N MIN,0.4371482176360225,"
et log
Pz′ (X,Y )"
N MIN,0.43902439024390244,"Pz(X,Y )

= e−tϵ−(1−t) Dt(Pz)∥Pz′),
(A.2)"
N MIN,0.44090056285178236,"where the probability measure Pz is defined on X × Y by (3.1) in the main text. The linearity of the
Rényi divergence with respect to product distributions implies"
N MIN,0.4427767354596623,Dt (Pz∥Pz′) = 1 2 X
N MIN,0.4446529080675422,"i̸=j
Dt

fzizj∥fz′
iz′
j 
+
X"
N MIN,0.44652908067542213,"1≤i≤n
Dt(hzi∥hz′
i) = 1 2 X"
N MIN,0.44840525328330205,"1≤a,b,c,d≤K
M(ab)(cd) Dt(fac∥fbd) +
X"
N MIN,0.450281425891182,"1≤a,b≤K
Nab Dt(ha∥hb),
(A.3) where"
N MIN,0.4521575984990619,"Nab = |{i ∈[n]: (zi, z′
i) = (a, b)}| ,"
N MIN,0.4540337711069418,"M(ab)(cd) =

i ̸= j : (zi, z′
i) = (a, b) and (zj, z′
j) = (c, d)
	 ."
N MIN,0.45590994371482174,"Moreover, Lemma 6 ensures that"
N MIN,0.45778611632270166,"M(ab)(cd) = Nab
 
Ncd −1(ab)=(cd)

= Ncd
 
Nab −1(ab)=(cd)

."
N MIN,0.4596622889305816,"Therefore,
X"
N MIN,0.46153846153846156,"1≤a,b,c,d≤K
M(ab)(cd) Dt(fac∥fbd) ≥
X a̸=b X"
N MIN,0.4634146341463415,"c
M(ab)(cc) Dt(fac∥fbc) +
X a X"
N MIN,0.4652908067542214,"c̸=d
M(aa)(cd) Dt(fac∥fad) = 2
X"
N MIN,0.46716697936210133,"a̸=b
Nab
X"
N MIN,0.46904315196998125,"c
Ncc Dt(fac∥fbc).
(A.4)"
N MIN,0.4709193245778612,"Using Ncc =
z−1(c)
 −P"
N MIN,0.4727954971857411,"d̸=c Ncd and that P c
P"
N MIN,0.474671669793621,"d̸=c Ncd = m, we obtain that 1
2 X"
N MIN,0.47654784240150094,"1≤a,b,c,d≤K
M(ab)(cd) Dt(fac∥fbd) ≥n
X"
N MIN,0.47842401500938087,"a̸=b
Nab
X"
N MIN,0.4803001876172608,"c
ˆπc(z)

1 − P"
N MIN,0.4821763602251407,"d̸=c Ncd
nˆπc(z)"
N MIN,0.48405253283302063,"
Dt(fac∥fbc)"
N MIN,0.48592870544090055,"≥n

1 −
m
n minc ˆπc(z)  X"
N MIN,0.4878048780487805,"a̸=b
Nab
X"
N MIN,0.4896810506566604,"c
ˆπc(z) Dt(fac∥fbc)."
N MIN,0.4915572232645403,"Hence,"
N MIN,0.49343339587242024,"(1 −t) Dt (Pz′∥Pz) ≥n

1 −
m
n minc ˆπc(z)  X"
N MIN,0.49530956848030017,"a̸=b
Nab(1 −t) X"
N MIN,0.4971857410881801,"c
ˆπc(z) Dt(fac∥fbc) + 1"
N MIN,0.49906191369606,n Dt(ha∥hb) !
N MIN,0.50093808630394,"= n

1 −
m
n minc ˆπc(z)  X"
N MIN,0.5028142589118199,"a̸=b
NabCHt(a, b, ˆπ(z))."
N MIN,0.5046904315196998,"Using the concentration of ˆz, we have ˆπc(z) = (1 + o(1))πc for all c, and hence"
N MIN,0.5065666041275797,"sup
t∈(0,1)
(1 −t) Dt (Pz′∥Pz) ≥(1 + o(1))

1 −
m
nπmin"
N MIN,0.5084427767354597,"
mnI,"
N MIN,0.5103189493433395,where we used P
N MIN,0.5121951219512195,"a̸=b Nab = m and I ≤CH(a, b, ˆπ(ˆz)). The first upper bound on Pz(L(z′) ≥L(z))
follows by taking ϵ = 0 and the supremum over t ∈(0, 1) in (A.2)."
N MIN,0.5140712945590994,"Finally, let d = sup
t∈(0,1)
min
a̸=b∈[K]
c∈[K]
(1 −t) Dt(fac∥fbc). Combining (A.3) and (A.4) leads to"
N MIN,0.5159474671669794,"sup
t∈(0,1)
(1 −t) Dt(Pz∥Pz′) ≥d
X"
N MIN,0.5178236397748592,"a̸=b
Nab
X"
N MIN,0.5196998123827392,"c
Ncc = dm(n −m),"
N MIN,0.5215759849906192,and this ends the proof.
N MIN,0.5234521575984991,"A.2.2
Lower bound"
N MIN,0.525328330206379,"We now focus on lower-bounding Pz (L(z) ≥L(z′)) when Ham(z, z′) = 1. In particular, the
condition Ham(z, z′) = 1 implies that there exists a unique node u ∈[n] such that zu ̸= z′
u. Let
a, b ∈[K] such that zu = a and z′
u = b. By definition of the likelihood, we have L(z) −L(z′) =
∆ub(X, Y, z) where"
N MIN,0.5272045028142589,"∆ub(X, Y, z) = n
X"
N MIN,0.5290806754221389,"v=1
v̸=u"
N MIN,0.5309568480300187,log fbzv
N MIN,0.5328330206378987,"fazv
(Xuv) + log hb"
N MIN,0.5347091932457786,"ha
(Yu).
(A.5)"
N MIN,0.5365853658536586,"Before studying the large deviation rates of likelihood ratios such as Pz (∆ub > 0) and
Pz (∆ub > 0, ∆vb > 0), we first recall some large deviation result for arbitrary sequences of random
variables [30, 10]."
N MIN,0.5384615384615384,"Proposition 1 (Strong large deviations for arbitrary sequences of random variables – Theorem 3.3
of [10]). Let Tn be a sequence of random variables whose moment generating function ϕn(t) =
E

etTn
is non-vanishing and analytic in the region Ω= {z ∈C: |z| < a} for some a > 0. Let kn
be a sequence of real numbers, and let ψn(t) = k−1
n log ϕn(t). Let (mn) be a bounded sequence of
real numbers such that there exists a sequence (τn) verifying ψ′
n(τn) = mn and 0 < τn < α0 < α.
Suppose that:"
N MIN,0.5403377110694184,1. there exists b > 0 such that |ψn(z)| ≤b for all z ∈Ω;
N MIN,0.5422138836772983,"2. there exists a > 0 such that ψ′′
n(τn) > c;"
N MIN,0.5440900562851783,"3. there exists δ0 > 0 such that supδ<|t|<λτn
 ϕn(τn+it)"
N MIN,0.5459662288930581,"ϕn(τn)
 = o(k−1/2
n
) for any given δ, λ such
that 0 < δ < δ0 < λ. Then,"
N MIN,0.5478424015009381,"P (Tn > anmn) =
1 + o(1) τn
p"
N MIN,0.549718574108818,"2πanψ′′(τn)
e−an(mnτn−ψn(τn))."
N MIN,0.551594746716698,"Proposition 1 is an extension of Cramer’s large deviation theorem for sums of iid r.v. to sequences of
arbitrary random variables (see [10, Remark 3.6]). We will apply it to the study of P (∆ub > 0).
Lemma 4. Let z ∈Zξ (where Zξ is defined in (A.1)) with ξ = o(1). Let u ∈[n] such that
zu = a and let ∆ub be given in (A.5). Suppose that there exists δn = ω(1) such that β : t ∈
(0, 1) 7→−lim
n→∞nδ−1
n CHt(a, b) exists and is strictly convex. Then, for any sequence ϵn such that"
N MIN,0.5534709193245778,"δnϵn →ϵ ∈{β′(t), t ∈(0, 1)} we have"
N MIN,0.5553470919324578,"Pz (∆ub > δnϵn) ∼
1 tϵ
p"
N MIN,0.5572232645403377,"2πδnβ′′(tϵ)
e−δnβ∗(tϵ),"
N MIN,0.5590994371482176,"where β∗(x) = sup
t>0
{tx −β(t)} is the Legendre transform of β, and tϵ is such that β′(tϵ) = ϵ. In"
N MIN,0.5609756097560976,"particular 0 ∈{β′(t), t ∈(0, 1)}, and therefore for any sequence ϵn = o(δ−1
n ) we have"
N MIN,0.5628517823639775,"Pz (∆ub > δnϵn) ∼
1 t0
p"
N MIN,0.5647279549718575,"2πδnβ′′(t0)
e−nCH(a,b)."
N MIN,0.5666041275797373,"Proof of Lemma 4. We will apply Proposition 1 with Ω= {z ∈C: |z| < 1} and kn = δ−1
n
to obtain
the stated results."
N MIN,0.5684803001876173,"Let us first compute ϕn(t) = Ez

et∆ub
and ψn(t) = δ−1
n log ϕn(t) for t ∈(0, 1). We first notice
that for all v ∈[n], Ez"
N MIN,0.5703564727954972,"
et log
fbzv
fazv (Xuv)

= E"
N MIN,0.5722326454033771,""" fbzv fazv"
N MIN,0.574108818011257,"t
(Xuv) #"
N MIN,0.575984990619137,"= e−(1−t) Dt(fbzv ∥fazv ),"
N MIN,0.5778611632270169,by definition of the Rényi divergence. Similar computations show that
N MIN,0.5797373358348968,"Ez
h
et log
hb
ha (Yu)i
= e−(1−t) Dt(hb∥ha)."
N MIN,0.5816135084427767,"Therefore,"
N MIN,0.5834896810506567,"ϕn(t) = et log
πb
πa −nCHt(a,b,z), where"
N MIN,0.5853658536585366,"CHt(a, b, z) = (1 −t) "" K
X"
N MIN,0.5872420262664165,"c=1
ˆπc Dt (fbc∥fac) + 1"
N MIN,0.5891181988742964,n Dt (hb∥ha) #
N MIN,0.5909943714821764,",
(A.6)"
N MIN,0.5928705440900562,"with ˆπc = n−1 z−1(k)
. We note that since z ∈Zξ we have CHt(a, b, z) = (1 + O(ξ))CHt(a, b).
Thus, ψn(t) = −(1 + o(1))nCHt(a, b). Moreover, the assumption CH(a, b) = Θ
 
n−1δn

implies"
N MIN,0.5947467166979362,"that CHt(a, b) = Θ
 
n−1δn

for all t ∈(0, 1) since the Rényi divergences of orders t ∈(0, 1) are
equivalent [35, Theorem 16]. Hence, δ−1
n ψn(t) = (1+o(1))β(t). Since β is well-defined and strictly
convex on (0, 1), this ensures that Assumptions 1 and 2 of Proposition 1 are verified. Moreover, we
notice that eβ(t) is the m.g.f of some r.v. X, and let φ(t) = EeitX be the characteristic function"
N MIN,0.5966228893058161,"of X. Then, ϕn(t∗+it)"
N MIN,0.5984990619136961,"ϕn(t∗)
=

φ(t∗+it)"
N MIN,0.600375234521576,"φ(t∗)
δn
. Since φ(t∗+it)"
N MIN,0.6022514071294559,"φ(t∗)
is the characteristic function of some r.v., its
module is strictly less than 1 on an interval not containing 0, and hence Assumption 3 of Proposition 1
is verified."
N MIN,0.6041275797373359,"Finally, Lemma 8 ensures that β′(0) ≤0 and β′(1) ≥0 and hence 0 ∈{β′(t), t ∈(0, 1)}."
N MIN,0.6060037523452158,"A.2.3
Asymptotic independence"
N MIN,0.6078799249530957,"Finally, the following lemma shows that the events ∆ub > 0 and ∆vb > 0 are asymptotically
independent.
Lemma 5. Let z ∈Zξ, u, v ∈z−1(a), and b ∈[K]\{a}. Then,"
N MIN,0.6097560975609756,"Pz (∆ub > 0, ∆vb > 0)
Pz (∆ub > 0) Pz (∆vb > 0) = 1 + o(1)."
N MIN,0.6116322701688556,Proof. Let ξ(Xuv) = log faa
N MIN,0.6135084427767354,fab (Xuv). We have
N MIN,0.6153846153846154,"∆ub = log πb πa
+ n
X"
N MIN,0.6172607879924953,"w=1
v̸∈{u,v}"
N MIN,0.6191369606003753,log fbzw
N MIN,0.6210131332082551,"fazw
(Xuw) + log hb"
N MIN,0.6228893058161351,"ha
(Yu) + log fba"
N MIN,0.624765478424015,"faa
(Xuv),"
N MIN,0.626641651031895,"∆vb = log πb πa
+ n
X"
N MIN,0.6285178236397748,"w=1
v̸∈{u,v}"
N MIN,0.6303939962476548,log fbzw
N MIN,0.6322701688555347,"fazw
(Xvw) + log hb"
N MIN,0.6341463414634146,"ha
(Yv) + log fba"
N MIN,0.6360225140712945,"faa
(Xuv)."
N MIN,0.6378986866791745,"Therefore,"
N MIN,0.6397748592870544,"∆ub = Uu −ξ(Xuv)
and
∆vb = Uv −ξ(Xuv)"
N MIN,0.6416510318949343,"where Uu and Uv are iid. Therefore,"
N MIN,0.6435272045028143,"Pz (∆ub > 0, ∆vb > 0) −Pz (∆ub > 0) Pz (∆vb > 0) = Covz(1(∆ub > 0), 1(∆vb > 0)."
N MIN,0.6454033771106942,"Conditioning on Uu and Uv, we observe that"
N MIN,0.6472795497185742,"Covz(1(∆ub > 0), 1(∆vb > 0) =
Z Z
Covz (ϕu(ξ), ϕu′(ξ)) µ(du)µ(du′),"
N MIN,0.649155722326454,"where ϕt(ξ) = 1(t > ξ) and µ = Law(ξ).
Because ϕu and ϕu′ are monotonous, the
Fortuin–Kasteleyn–Ginibre (FKG) inequality [20, 17] (see also [19, Section 2.2]) implies that
Cov (ϕu(ξ), ϕu′(ξ)) ≥0. Therefore,"
N MIN,0.651031894934334,"Pz (∆ub > 0, ∆vb > 0)
Pz (∆ub > 0) Pz (∆vb > 0) ≥1."
N MIN,0.6529080675422139,"Let us now derive an upper bound for this ratio. First, Lemma 4 with ϵn = 0 and δn = log n implies"
N MIN,0.6547842401500938,"P (∆ub > 0) =
1 + o(1) t∗p"
N MIN,0.6566604127579737,"2π log nβ′′(t∗)
e−nCH(a,b),"
N MIN,0.6585365853658537,"and the same relation holds for P (∆vb > 0). Moreover, conditionally on Xuv we have"
N MIN,0.6604127579737336,"Pz (∆ub > 0, ∆vb > 0) =
Z"
N MIN,0.6622889305816135,"x∈X
(γ (x))2 faa(x)dx,"
N MIN,0.6641651031894934,"where γ(x) = Pz (U > ξ(x)). Since CHt(a, b) = o(1), then fac and fbc are mutually contiguous4
for all c (see [35, Theorem 25]). In particular, this implies that ξ(x) < ∞for all x ∈X such that"
N MIN,0.6660412757973734,"4Let (pn) and (qn) be two sequences of distributions. Then pn is contiguous with respect to qn if for all
sequence of event An such that qn(An) = o(1) we also have pn(An) = o(1)."
N MIN,0.6679174484052532,"faa(x) > 0. Thus, we can apply Lemma 4 with ϵn = ξ(x) and δn = log n, verifying δnϵn →0 to
obtain"
N MIN,0.6697936210131332,"γ(x) =
1 + o(1) t∗p"
N MIN,0.6716697936210131,"2π log nβ′′(t∗)
e−nCH(a,b),"
N MIN,0.6735459662288931,and this ends the proof.
N MIN,0.6754221388367729,"A.3
Impossibility of exact recovery"
N MIN,0.6772983114446529,"For a, b ∈[K] two block indexes, we recall the quantity CH(a, b) defined in (3.2) of the main text
denotes the hardness to distinguish a node in block a from a node in block b. We suppose that
I = min
a̸=b CH(a, b) = Θ

log n"
N MIN,0.6791744840525328,"n

with lim
n→∞
n
log nI < 1. We prove the failure of the MAP estimator"
N MIN,0.6810506566604128,for exact recovery in three steps:
N MIN,0.6829268292682927,"(i) We start by showing that we can restrict the study to the node labelling vector z for which the
relative size of the communities are close to their expectations, i.e., |z−1(a)|"
N MIN,0.6848030018761726,"n
= (1 + o(1))πa for
all a ∈[K];"
N MIN,0.6866791744840526,"(ii) Let Mab(z) be the number of nodes in block a for which changing their community label to
block b results in an increase of likelihood. We show that EMab(z) ≫1 when a, b are the two
hardest blocks to distinguish."
N MIN,0.6885553470919324,(iii) We show that Mab(z) > 0 almost surely using the second-moment method.
N MIN,0.6904315196998124,"(i) Conditioning on well-behaving community sizes.
Recall the definition of Zξ in (A.1). We
established in Section A.1 that P(Zξ) = 1 −o(1) for ξ = log log n
√n
. Hence,"
N MIN,0.6923076923076923,"P (MAP fails ) =
X"
N MIN,0.6941838649155723,"z∈Z
Pz
 
zMAP ̸= z

P(z) ≥
X"
N MIN,0.6960600375234521,"z∈Zξ
Pz
 
zMAP ̸= z

P(z)"
N MIN,0.6979362101313321,"≥P (Zξ) inf
z∈Zξ Pz
 
zMAP ̸= z
"
N MIN,0.699812382739212,"≥(1 −o(1)) inf
z∈Zξ Pz
 
zMAP ̸= z

."
N MIN,0.701688555347092,"The rest of the proof is devoted to show that Pz
 
zMAP ̸= z

= 1 −o(1) for all z ∈Zξ."
N MIN,0.7035647279549718,"(ii) Expected number of bad nodes.
Given a block structure z ∈Zξ, an arbitrary node u ∈[n]
such that zu = a and a block b ̸= a. We define by ˜z the block structure obtained from z by swapping
the block of node u to b, i.e.,"
N MIN,0.7054409005628518,∀v ∈[n]: ˜zv = (1 −δvu)zv + δvub.
N MIN,0.7073170731707317,"Suppose that X, Y are generating from a true block structure z, and let"
N MIN,0.7091932457786116,"∆ub(X, Y, z) = log P(˜z | X, Y ) −log P(z | X, Y )
(A.7)"
N MIN,0.7110694183864915,"the change in the MAP estimation of z obtained by swapping the label of node u from its true block
a to a wrong block b. By the definition of ˜z, we have using Bayes’ law"
N MIN,0.7129455909943715,"∆ub(X, Y, z) = log
 P (zu = b | X, Y, z−u)"
N MIN,0.7148217636022514,"P (zu = a | X, Y, z−u) "
N MIN,0.7166979362101313,"= log
 πbP (X, Y | z−u, zu = b)"
N MIN,0.7185741088180112,"πaP (X, Y | z−u, zu = a) 
."
N MIN,0.7204502814258912,"Moreover,"
N MIN,0.7223264540337712,"log P (X, Y | z−u, zu = b) =
X"
N MIN,0.724202626641651,"1≤v<w≤n
log fzvzw(Xvw) + n
X"
N MIN,0.726078799249531,"v=1
log hzv(Yv) = n
X"
N MIN,0.7279549718574109,"v=1
v̸=u"
N MIN,0.7298311444652908,log fbzv(Xuv) + log hb(Yu) + C
N MIN,0.7317073170731707,where C does not depend on b. Hence
N MIN,0.7335834896810507,"∆ub(X, Y, z) = log πb πa
+ n
X"
N MIN,0.7354596622889306,"v=1
v̸=u"
N MIN,0.7373358348968105,log fbzv
N MIN,0.7392120075046904,"fazv
(Xuv) + log hb"
N MIN,0.7410881801125704,"ha
(Yu)."
N MIN,0.7429643527204502,"The number of nodes in block a for which updating the label to b would cause the change of likelihood
to be strictly positive is given by"
N MIN,0.7448405253283302,"Mab(z) =
X"
N MIN,0.7467166979362101,"u∈z−1(a)
1 (∆ub(X, Y, z) > 0) ."
N MIN,0.7485928705440901,"In the following, we select a, b as the indexes of the two hardest blocks to distinguish. Hence,
I = CH(a, b, π, f, h) and we have using Lemma 4 that"
N MIN,0.7504690431519699,"EzMab(z) ∼πane−(1+o(1))nCH(a,b,ˆπ,f,g)."
N MIN,0.7523452157598499,"Since z ∈Zξ we have that CH(a, b, ˆπ, f, h) = (1+o(1))I. Moreover, by assumption I < (1−ϵ) log n"
N MIN,0.7542213883677298,"n
for some ϵ > 0 and for any n large enough. Hence EzMab(z) ≫1."
N MIN,0.7560975609756098,"(iii) Conclusion.
We will conclude that Mab(z) > 0 almost surely using the second-moment
method. Denote by α = Pz(∆ub > 0) and β = P (∆ub > 0, ∆vb > 0) for two arbitrary distinct
nodes u, v ∈z−1(a). We have EzMab =
z−1(k)
 α and"
N MIN,0.7579737335834896,"Varz Mab =
X"
N MIN,0.7598499061913696,"u,v∈z−1(a)
Cov (1(∆uℓ> 0), 1(∆vℓ> 0))"
N MIN,0.7617260787992496,"=
z−1(a)
  
α −α2
+
z−1(a)
  z−1(a)
 −1
  
β −α2"
N MIN,0.7636022514071295,"≤EzMab + (EzMab)2 β −α2 α2
."
N MIN,0.7654784240150094,"Hence, the second-moment method implies that"
N MIN,0.7673545966228893,Pz(Mab = 0) ≤Varz Mab
N MIN,0.7692307692307693,"(EzMab)2 ≤
1
EzMab
+ β"
N MIN,0.7711069418386491,α2 −1.
N MIN,0.7729831144465291,We end the proof using Lemma 5.
N MIN,0.774859287054409,"A.4
Possibility of exact recovery"
N MIN,0.776735459662289,"To show that the MAP estimator achieves exact recovery up to the desired threshold, we need to show
that there is no possibility of reducing the likelihood by swapping m vertices from each community.
In all the following, we recall that z denotes the true community labelling, and for any z′ ∈[K] we
denote by L(z′) = P(X, Y | z′) the likelihood of labelling z′. For any m ≥1, let us denote the set of
node-labelling at a distance m of z by"
N MIN,0.7786116322701688,"Γm = {z′ ∈Z : loss(z, z′) = m} .
(A.8)"
N MIN,0.7804878048780488,The probability that there exists a labelling z′ ∈Γm with higher likelihood than z is
N MIN,0.7823639774859287,Pm = Pz (∃z′ ∈Γm : L(z′) ≥L(z)) .
N MIN,0.7842401500938087,Let ˆz be the block labelling estimated by the MAP. Lemma 7 ensures that |Γm| = 0 if m ≥n K−1
N MIN,0.7861163227016885,"K .
Thus, we have, using union bounds,"
N MIN,0.7879924953095685,"Pz (loss (z, ˆz) ≥1) ≤ 2K−1"
"K
X",0.7898686679174484,"2K
X"
"K
X",0.7917448405253283,"m=1
|Γm| max
z′∈Γm Pz (L(z′) ≥L(z)) ."
"K
X",0.7936210131332082,"Moreover, [39, Proposition 5.2] show that"
"K
X",0.7954971857410882,"|Γm| ≤min
enK m"
"K
X",0.797373358348968,"m
, Kn

,"
"K
X",0.799249530956848,"while Lemma 3 provides an upper bounds on max
z′∈Γm Pz (L(z′) ≥L(z)). Combining these upper"
"K
X",0.801125703564728,"bounds ensure that for some sequence η = o(1), we have"
"K
X",0.8030018761726079,"Pz (loss(z, ˆz) ≥1) ≤"
"K
X",0.8048780487804879,n 2K−1
"K
X",0.8067542213883677,"2K
X"
"K
X",0.8086303939962477,"m=1
(sm)m,
(A.9)"
"K
X",0.8105065666041276,"where sm
=
enK"
"K
X",0.8123827392120075,"m min

e
−(1−η)

1−
m
nπmin"
"K
X",0.8142589118198874,"
nI; e−(n−m)d

with d
=
sup
t∈(0,1)
min
a̸=b∈[K]
c∈[K]
(1 −"
"K
X",0.8161350844277674,t) Dt(fac∥fbc).
"K
X",0.8180112570356473,Let ϵ > 0 such that I > (1 + ϵ) log n
"K
X",0.8198874296435272,"n . We will now show that the sum on the right-hand side of
Equation (A.9) goes to zero, by considering the cases (i) m ≤πmin"
"K
X",0.8217636022514071,2 n and (ii) πmin
"K
X",0.8236397748592871,2 n ≤m ≤1 2n.
"K
X",0.8255159474671669,"(i) First of all, suppose that m ≤πmin"
"K
X",0.8273921200750469,"2 n. Then,"
"K
X",0.8292682926829268,"sm ≤
eK
nϵ(1−η)+η e(1−η)(1+ϵ)
m
πmin
log n"
"K
X",0.8311444652908068,"n
−log m"
"K
X",0.8330206378986866,"≤
eK
nϵ/2 e(1+ϵ)
m
πmin
log n"
"K
X",0.8348968105065666,"n
−log m."
"K
X",0.8367729831144465,Let f(m) = log m
"K
X",0.8386491557223265,"m
−1+ϵ"
"K
X",0.8405253283302064,"πmin
log n"
"K
X",0.8424015009380863,n . We will show that f(m) ≥log m
"K
X",0.8442776735459663,"2m for m ≥3. Indeed, for x ≥3, the
function x 7→log x"
"K
X",0.8461538461538461,"x
is decreasing. Therefore, for 3 ≤m ≤πmin"
N WE HAVE,0.8480300187617261,2 n we have
N WE HAVE,0.849906191369606,f(m) −log m
M,0.851782363977486,"2m
≥1"
LOG M,0.8536585365853658,"2
log m"
LOG M,0.8555347091932458,"m
−1 + ϵ πmin log n"
LOG M,0.8574108818011257,"n
≥
2
πmin"
LOG M,0.8592870544090057,log n −log(πmin/2)
LOG M,0.8611632270168855,"n
−1 + ϵ πmin log n n
> 0"
LOG M,0.8630393996247655,for n large enough. Hence sm ≤eKn−ϵ/2e−1
LOG M,0.8649155722326454,2 log m and P πmin
N,0.8667917448405253,"2
n
m=1
sm
m = o(1)."
N,0.8686679174484052,"(ii) Next, suppose that 1"
N,0.8705440900562852,2πminn ≤m ≤n K−1
N,0.8724202626641651,"K . Then, the assumption Dt(fac∥fbc) = Θ

log n n
"
N,0.874296435272045,implies that d > κ log n
N,0.8761726078799249,"n
for some positive constant κ. Hence,"
N,0.8780487804878049,sm ≤2eK
N,0.8799249530956847,"πmin
e−κ"
N,0.8818011257035647,"K log n,"
N,0.8836772983114447,"and thus K−1 K
n
P"
N,0.8855534709193246,m= πmin
"N
SM",0.8874296435272045,"2
n
sm
m = o(1)."
"N
SM",0.8893058161350844,"Hence, (A.9) shows that Pz (loss(z, ˆz) ≥1) = o(1), and thus the MAP estimator exactly recovers
the true community structure z."
"N
SM",0.8911819887429644,"A.5
Additional lemmas"
"N
SM",0.8930581613508443,"Lemma 6. Let z, z′ ∈[K]n and define for all a, b, c, d ∈[K]:"
"N
SM",0.8949343339587242,"Aab = |{i ∈[n]: (zi, z′
i) = (a, b)}| ,"
"N
SM",0.8968105065666041,"M(ab)(cd) =

1 ≤i < j ≤n: (zi, z′
i) = (a, b) and (zj, z′
j) = (c, d)
	 ."
"N
SM",0.8986866791744841,We have
"N
SM",0.900562851782364,"M(ab)(cd) = Aab
 
Acd −1(ab)=(cd)

= Acd
 
Aab −1(ab)=(cd)

."
"N
SM",0.9024390243902439,"Proof. Denote by Ca = z−1(a) and C′
a = z′−1(a), and let Cab = Ca ∩C′
b. Then, Aab = |Cab| and
M(ab)(cd) = | {1 ≤i < j ≤n: i ∈Cab and j ∈Ccd} |. This proves the lemma."
"N
SM",0.9043151969981238,"Lemma 7. For any z, z′ ∈[K]n, we have loss(z, z′) ≤n(1 −1 K )."
"N
SM",0.9061913696060038,"Proof. Without loss of generality, suppose that loss(z, z) = Ham(z, z′) (that is, the optimal per-
mutation in the definition of the loss is simply the identity). For any a ∈[K], let us denote by
Ca = z−1(a), C′
a = z−1(b). The confusion matrix of the two node labellings is the K-by-K matrix
having entries Nab = |Ca ∩Cb|. In particular, we have n = P"
"N
SM",0.9080675422138836,"a,b Nab and"
"N
SM",0.9099437148217636,"loss(z, z′) = K
X a=1 K
X"
"N
SM",0.9118198874296435,"b=1
b̸=a Nab"
"N
SM",0.9136960600375235,"= n −
X"
"N
SM",0.9155722326454033,"a
Naa."
"N
SM",0.9174484052532833,"[5, Lemma B.2] show that Nab ≤Naa + Nbb −Nba, and therefore"
"N
SM",0.9193245778611632,"loss(z, z′) ≤ K
X a=1 K
X"
"N
SM",0.9212007504690432,"b=1
b̸=a"
"N
SM",0.9230769230769231,"(Naa + Nbb −Nba) .
(A.10)"
"N
SM",0.924953095684803,"We notice that
X a X"
"N
SM",0.926829268292683,"b̸=a
Naa = (K −1)
X"
"N
SM",0.9287054409005628,"a
Naa = (K −1)(n −loss(z, z′)),"
"N
SM",0.9305816135084428,"and similarly, X a X"
"N
SM",0.9324577861163227,"b̸=a
Nbb =
X a X"
"N
SM",0.9343339587242027,"b
Nbb −Naa !"
"N
SM",0.9362101313320825,"= (K −1)(n −loss(z, z′))."
"N
SM",0.9380863039399625,"Finally, P a
P"
"N
SM",0.9399624765478424,"b̸=a Nba = loss(z, z′). Thus, going back to (A.10) leads to"
"N
SM",0.9418386491557224,"loss(z, z′) ≤2(K −1)n −(2K −1)loss(z, z′),"
"N
SM",0.9437148217636022,and this ends the proof.
"N
SM",0.9455909943714822,"Lemma 8. Let f, g be two probability distributions and denote c(t) = (1 −t) Dt(f∥g) the Chernoff
coefficient of order t between f and g. We have c′(0) = −dKL(g∥f) and c′(1) = dKL(f∥g)."
"N
SM",0.9474671669793621,"Proof. From c(t) = log
R
f tg1−t, we notice that c′(t) ="
"N
SM",0.949343339587242,"R
f tg1−t log f"
"N
SM",0.9512195121951219,"g
R
f tg1−t
, and the result follows."
"N
SM",0.9530956848030019,"B
Proof of Lemma 1"
"N
SM",0.9549718574108818,"Proof of Lemma 1. Using a Taylor expansion, we have"
"N
SM",0.9568480300187617,"(1 −t) Dt(fac∥fbc) = log

(1 −pac)t(1 −pbc)1−t + pt
acp1−t
bc"
"N
SM",0.9587242026266416,"Z
( ˜fab)t( ˜fbc)1−t
"
"N
SM",0.9606003752345216,"= tp + (1 −t)q −ptq1−t
Z
( ˜fac)t( ˜fbc)1−t + o(δ),"
"N
SM",0.9624765478424016,"and we finish the proof using
R
( ˜fac)t( ˜fbc)1−t = e−Jψ(θab∥θbc) and (1 −t) Dt(ha∥hb) = Jϕ(ηa∥ηb)
(see for example [28])."
"N
SM",0.9643527204502814,"C
Additional numerical results"
"N
SM",0.9662288930581614,"We present in Table 2 and 3 the numerical results that are drawn in Figure (4a) and (4b), respectively.
We observe that the variance of Algorithm 1 is very low, while all other algorithms (excepted
EM-GMM) exhibit a very large variance.
Table 2: Average ARI of the different algorithms over 60 trials with standard deviation in brackets
obtained for Figure (4a)."
"N
SM",0.9681050656660413,"PARAMETER a
5
7
9
11
13
15"
"N
SM",0.9699812382739212,"ALGORITHM 1
0.40 (0.15)
0.62 (0.04)
0.93 (0.02)
1 (0)
1 (0)
1 (0)
EM-GMM
0.45 (0.04)
0.46 (0.03)
0.46 (0.04)
0.46 (0.04)
0.46 (0.04)
0.45 (0.07)
SC
0 (0)
0.07 (0.05)
0.83 (0.15)
0.85 (0.33)
0.93 (0.24)
0.82 (0.37)
IR_LS
0 (0)
0.20 (0.13)
0.93 (0.14)
1 (0)
0.98 (0.12)
1 (0)
attSBM
0 (0)
0.07 (0.05)
0.82 (0.15)
0.86 (0.33)
0.42 (0.48)
0.59 (0.49)"
"N
SM",0.9718574108818011,"Table 3: Average ARI of the different algorithms over 60 trials with standard deviation in brackets
obtained for Figure (4b)."
"N
SM",0.9737335834896811,"PARAMETER r
0
1
2
3
4
5"
"N
SM",0.975609756097561,"ALGORITHM 1
0.47 (0.29)
0.82 (0.03)
0.97 (0.02)
1 (0)
1 (0)
1 (0)
EM-GMM
0 (0)
0.46 (0.04)
0.9 (0.02)
1 (0)
1 (0)
1 (0)
SC
0.36 (0.28)
0.38 (0.27)
0.35 (0.26)
0.40 (0.26)
0.37 (0.28)
0.35 (0.26)
IR_LS
0.42 (0.29)
0.65 (0.31)
0.91 (0.21)
0.97 (0.15)
0.97 (0.14)
0.98 (0.12)
attSBM
0.36 (0.28)
0.38 (0.27)
0.38 (0.30)
0.59 (0.43)
0.57 (0.46)
0.50 (0.43)"
"N
SM",0.9774859287054409,"We also provide in Figure 4 comparison of Algorithm 1 with IR_sLs and attSBM on weighted
networks with attributes exponentially distributed. We observe that contrary to Algorithm 1, IR_sLs
and attSBM do not perform well in that setting (which is expected as they are designed for binary
network and Gaussian attributes)."
"N
SM",0.9793621013133208,"(a) Varying λ−1
w , with λa = 1/2.
(b) Varying λ−1
a , with λw = 1/2."
"N
SM",0.9812382739212008,"Figure 4: Performance on weighted networks (n = 600, K = 2) with edge-weight distributions
fin = (1 −p)δ0 + p Exp(λw), fout = (1 −p)δ0 + p Exp(1) with p = 5n−1 log n, and exponentially
distributed attributes h1 ∼Exp(1) and h2 = Exp(λa). Results are averaged over 20 runs."
"N
SM",0.9831144465290806,"C.1
Robustness to the choice of dψ∗and dϕ∗"
"N
SM",0.9849906191369606,"We show in Figure 5 that using a divergence (distribution) for edge weights (Figure 5a and node
attributes (Figure 5b different from the distribution used to generate the data does not impact the
performance of Algorithm 1. We note that a similar observation was done in previous papers using
Bregman divergence for clustering [6, 23]."
"N
SM",0.9868667917448405,"5
7
9
11
13
15
muin 0.2 0.4 0.6 0.8 1.0 ARI"
"N
SM",0.9887429643527205,"euclidean
manhattan
Itakura-Saito
poisson"
"N
SM",0.9906191369606003,(a) Various dψ∗. Poisson is the correct model.
"N
SM",0.9924953095684803,"3
5
7
9
11
nu1 0.0 0.2 0.4 0.6 0.8 1.0 ARI"
"N
SM",0.9943714821763602,"euclidean
manhattan
Itakura-Saito
poisson"
"N
SM",0.9962476547842402,(b) Various dϕ∗. Poisson is the correct model.
"N
SM",0.99812382739212,"Figure 5: Performance of Algorithm 1 when dψ∗or dϕ∗do not correspond to the model that generated
the data. The different curves show the Adjusted Rand Index (ARI) [21] averaged over 20 realisations
with the standard deviations as error bars.
(a) n = 400, K = 4, fin = (1 −p)δ0(x) + pPoi(µin) and fout = (1 −q)δ0(x) + qPoi(5), with
p = 0.04 and q = 0.01. Attributes are 2d-Gaussians with unit variances and mean equally spaced the
circle of radius r = 2.
(b) n = 400, K = 2, fin = (1 −p)δ0(x) + pNor(2, 1) and fout = (1 −q)δ0(x) + qNor(0, 1), with
p = 0.04 and q = 0.01. Attributes are Poisson with means ν1 (for nodes in cluster 1) and 3 (for
nodes in cluster 2)."
