Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0013404825737265416,"Real-world data deviating from the independent and identically distributed (i.i.d.)
assumption of in-distribution training data poses security threats to deep networks,
thus advancing out-of-distribution (OOD) detection algorithms. Detection methods
in generative language models (GLMs) mainly focus on uncertainty estimation and
embedding distance measurement, with the latter proven to be most effective in
traditional linguistic tasks like summarization and translation. However, another
complex generative scenario mathematical reasoning poses significant challenges
to embedding-based methods due to its high-density feature of output spaces, but
this feature causes larger discrepancies in the embedding shift trajectory between
different samples in latent spaces. Hence, we propose a trajectory-based method TV
Score, which uses trajectory volatility for OOD detection in mathematical reasoning.
Experiments show that our method outperforms all traditional algorithms on GLMs
under mathematical reasoning scenarios and can be extended to more applications
with high-density features in output spaces, such as multiple-choice questions."
ABSTRACT,0.002680965147453083,§ https://github.com/Alsace08/OOD-Math-Reasoning
INTRODUCTION,0.004021447721179625,"1
Introduction"
INTRODUCTION,0.005361930294906166,"The rapid development of generative language models (GLMs) [40, 41, 5, 2, 50] has empowered
them to fit diverse and challenging datasets, showing strong generalization over in-distribution
(ID) test data satisfying the independent and identically distributed (i.i.d.) assumption. However,
unconstrained inputs in real-world settings frequently trigger distributional drifts, called out-of-
distribution (OOD) data. In such scenarios, model performance often deteriorates unexpectedly,
yielding harmful outcomes. Thus, OOD detection [38, 4] is critical in safeguarding model security."
INTRODUCTION,0.006702412868632708,"A highly scalable detection method must not rely on specific OOD data distributions, so simulating
scenarios where OOD data is unavailable is more promising. Most existing research mainly focuses
on vision and text classification tasks [12, 30, 55, 26, 48]. In contrast, studies addressing OOD
detection on GLMs remain relatively niche, despite the more severe risks associated with OOD
perils in GLMs due to the potential for error propagation in autoregressive generated sequences [43].
Existing methods on GLMs only focus on traditional text generation scenarios like summarization and
translation, and these methods did not step outside the research framework of uncertainty estimation
[33, 31] and embedding distance measure [43]. Among these, [43] has demonstrated that embedding-
based methods are currently the only optimal solution for text generation scenarios, they determine
whether a new sample is ID or OOD by calculating the Mahalanobis Distance [22] between the new
sample embedding and ID embedding distribution in the static input or output space."
INTRODUCTION,0.00804289544235925,"Recently, mathematical reasoning has emerged as a challenging generative task and a crucial
benchmark for evaluating model abilities. However, it presents unique phenomena in both input and
output spaces that render embedding-based methods inapplicable, as shown in Figure 1:"
INTRODUCTION,0.00938337801608579,"Input
Output"
INTRODUCTION,0.010723860589812333,"Input: Suppose that 𝑓𝑓is a function and 𝑓𝑓−1 is the inverse of 𝑓𝑓.  If 𝑓𝑓(1) = 2, 𝑓𝑓(2) = 6, and 𝑓𝑓(3) = 5, then what is 𝑓𝑓−1(𝑓𝑓−1(6))?
Output: 1"
INTRODUCTION,0.012064343163538873,Algebra
INTRODUCTION,0.013404825737265416,Input: The product of the proper positive integer factors of 𝑛𝑛can be written as 𝑛𝑛
INTRODUCTION,0.014745308310991957,"𝑎𝑎𝑎𝑎+𝑏𝑏
𝑐𝑐, where 𝑥𝑥is the number of positive divisors 𝑛𝑛
has, 𝑐𝑐is a positive integer, and the greatest common factor of the three integers 𝑎𝑎, 𝑏𝑏, and 𝑐𝑐is 1. What is 𝑎𝑎+ 𝑏𝑏+ 𝑐𝑐?
Output: 1"
INTRODUCTION,0.0160857908847185,Number Theory
INTRODUCTION,0.01742627345844504,Algebra
INTRODUCTION,0.01876675603217158,Geometry
INTRODUCTION,0.020107238605898123,Number Theory
INTRODUCTION,0.021447721179624665,Precalculus
INTRODUCTION,0.022788203753351208,(a) Mathematical Reasoning
INTRODUCTION,0.024128686327077747,"Input
Output
Bible"
INTRODUCTION,0.02546916890080429,WMT News
INTRODUCTION,0.02680965147453083,TED Talk
INTRODUCTION,0.028150134048257374,Health
INTRODUCTION,0.029490616621983913,"Input: Supporters say the tunnels would benefit the environment and offer Californians a more secure water supply.
Output:支持者们表示，这两条隧道将使环境受益并帮助加利福尼亚州确保供水更加安全。"
INTRODUCTION,0.030831099195710455,"Input: The White House Coronavirus Task Force was established on January 29.
Output:白宫冠状病毒工作组是成立于2020年1月29日的以应对美国COVID-19疫情的工作组。"
INTRODUCTION,0.032171581769437,Health
INTRODUCTION,0.03351206434316354,WMT News
INTRODUCTION,0.03485254691689008,(b) Text Generation (Translation)
INTRODUCTION,0.036193029490616625,"Figure 1: Embedding projection and cases of input and output spaces under mathematical reasoning
and text generation scenarios. We select MATH [6] dataset for mathematical reasoning and OPUS [49]
for text generation, each with four diverse domains. Different colors represent different domains, with
lighter and darker shades indicating input and output. We use SimCSE [9] for sentence embeddings
and UMAP [34] for dimensionality reduction. Appendix B shows detailed settings and examples."
INTRODUCTION,0.03753351206434316,"• Input space of mathematical reasoning exhibits vague clustering features across various domains,
in contrast to the more defined clusters observed in text generation. This indicates that embedding
may struggle to capture the complexity of mathematical questions.
• Output space of mathematical reasoning exhibits high-density characteristics with significant
overlap between different domains. we call this phenomenon Pattern Collapse. Since the output
is mathematically symbolic [53, 37], it compresses the search space, increasing the likelihood of
overlap between questions from disparate domains (As cases of Figure 1a). More importantly, the
sequence tokenization used in GLMs allows for substantial token sharing among mathematically
distinct expressions, as these mathematical tokens are primarily drawn from digits 0-9 and finite
special symbols such as decimal points and square roots, rather than diverse linguistic elements.
These scalar outputs lack distinctive features associated with specific domain distributions."
INTRODUCTION,0.0388739946380697,"More discussion of the two phenomena, especially the “pattern collapse”, will be presented in Section
6. Given the limitations of traditional methods, we aim to explore innovative OOD detection solutions
in mathematical reasoning scenarios. We transform our focus from static embedding space to the
dynamic embedding trajectory, this motivation stems from a theoretical insight, as shown in Figure 2:
“pattern collapse” causes the convergence of the trajectory endpoints of different samples, leading to
significant trajectory differences across samples. In Section 2.1, we model and prove this hypothesis
to elucidate the intuition of using trajectories as a measure. Subsequently, in Section 2.2, we perform
empirical experiments to investigate the underlying causes of trajectory differences between ID and
OOD samples. Our findings reveal a phenomenon we term Early Stabilization, wherein GLMs
achieve primary reasoning in later stages for ID samples, a pattern not observed in OOD samples.
This observation provides direct evidence for the rationale behind using trajectory as a measure."
INTRODUCTION,0.040214477211796246,"Based on these analyses, in Section 3, we propose the Trajectory Volatility detection algorithm (TV
Score) for mathematical reasoning. Then in Section 4 and 5, we conduct extensive OOD detection
experiments with diverse datasets and GLMs to validate the effectiveness of our method. We also
tackle the challenging scenario of OOD quality estimation, which raises higher precision demands on
the OOD scores. Results indicate that our method surpasses all traditional algorithms for GLMs under
the mathematical reasoning scenario. Additionally, we demonstrate the extension of our method to a
broader range of tasks with high-density features in output spaces, such as multiple-choice questions."
INTRODUCTION,0.04155495978552279,"Problem Statement: OOD Detection on GLMs.
We start by formalizing GLMs. Let X and Y be
the input and output spaces with PX and PY be the marginal distributions for respective space, and
PX,Y is the joint data distribution defined over X × Y. GLMs are trained given input sequence x =
x1x2...xt ∼PX of length t to autoregressively generate the next token in the corresponding output
sequence y = y1y2...yn ∼PY of length n over the likelihood model pθ(y|x) = Qn
i=1 pθ(yi|y≺i, x),
where each xi and yi are taken from vocabulary V and θ is sampled from the parameter space Θ."
INTRODUCTION,0.04289544235924933,"Mathematical Reasoning
Text Generation (e.g. Translation)"
INTRODUCTION,0.04423592493297587,Output Space
INTRODUCTION,0.045576407506702415,"Input Space
Input Space"
INTRODUCTION,0.04691689008042895,Output Space
INTRODUCTION,0.04825737265415549,"𝜙𝜙0 𝑠𝑠𝑖𝑖−𝜙𝜙0 𝑠𝑠𝑗𝑗
2 ≫0"
INTRODUCTION,0.049597855227882036,"𝜙𝜙𝐿𝐿𝑠𝑠𝑖𝑖−𝜙𝜙𝐿𝐿𝑠𝑠𝑗𝑗
2 ≈0"
INTRODUCTION,0.05093833780160858,𝜙𝜙𝐿𝐿/2 𝑠𝑠𝑖𝑖
INTRODUCTION,0.05227882037533512,𝜙𝜙𝐿𝐿/2 𝑠𝑠𝑗𝑗
INTRODUCTION,0.05361930294906166,"𝜙𝜙0 𝑠𝑠𝑖𝑖−𝜙𝜙0 𝑠𝑠𝑗𝑗
2 ≫0"
INTRODUCTION,0.054959785522788206,"𝜙𝜙𝐿𝐿𝑠𝑠𝑖𝑖−𝜙𝜙𝐿𝐿𝑠𝑠𝑗𝑗
2 ≫0"
INTRODUCTION,0.05630026809651475,𝜙𝜙𝐿𝐿/2 𝑠𝑠𝑗𝑗
INTRODUCTION,0.057640750670241284,𝜙𝜙𝐿𝐿/2 𝑠𝑠𝑖𝑖
INTRODUCTION,0.058981233243967826,𝜙𝜙𝑙𝑙⋅: Embedding in 𝑙𝑙-th layer
INTRODUCTION,0.06032171581769437,"𝑠𝑠𝑖𝑖, 𝑠𝑠𝑗𝑗: samples in the dataset"
INTRODUCTION,0.06166219839142091,"Figure 2: The “pattern collapse” phenomenon only exists in mathematical reasoning scenarios, where
two samples initially distant in distance will converge approximately at the endpoint after undergoing
embedding shifts, and does not occur in text generation scenarios. This produces a greater likelihood
of trajectory variation under different samples in mathematical reasoning."
INTRODUCTION,0.06300268096514745,"Assume that ePX,Y denote a distribution sufficiently different from PX,Y, the goal of OOD detection
in GLMs is to find a score function f(x, y, θ) for each sample and a threshold ϵ, which may rely on
the features of X, Y, and Θ, to achieve a high discrimination accuracy goal:"
INTRODUCTION,0.064343163538874,"max
f
P(x,y)∼PX,Y [f(x, y, θ) < ϵ] + P(ex,ey)∼e
PX,Y [f(ex, ey, θ) > ϵ] .
(1)"
"DYNAMIC EMBEDDING TRAJECTORY DISTINGUISHES ID AND OOD SAMPLES IN
MATHEMATICAL REASONING",0.06568364611260054,"2
Dynamic Embedding Trajectory Distinguishes ID and OOD Samples in
Mathematical Reasoning"
"DYNAMIC EMBEDDING TRAJECTORY DISTINGUISHES ID AND OOD SAMPLES IN
MATHEMATICAL REASONING",0.06702412868632708,"We begin by defining the embedding trajectory and its volatility features. Given a sample s,
we put its input to the language model pθ with L layers, and the output sequence consists of T
tokens. For the t-th token, we denote its output embedding at layer l as ht
l, with each embedding
a d-dimensional vector. Following the definitions in [43, 51], we define the average embedding
yl = 1"
"DYNAMIC EMBEDDING TRAJECTORY DISTINGUISHES ID AND OOD SAMPLES IN
MATHEMATICAL REASONING",0.06836461126005362,"T
PT
t=1 ht
l as the sentence embedding at layer l. Then, the embedding trajectory is formed
as a progressive chain of these embeddings: y0 →y1 →· · · →yl →· · · →yL−1 →yL. To
measure the change magnitudes of the embedding trajectory in the latent space, we define two types
of trajectory volatilities:"
"DYNAMIC EMBEDDING TRAJECTORY DISTINGUISHES ID AND OOD SAMPLES IN
MATHEMATICAL REASONING",0.06970509383378017,"• Dimension-independent volatility VI(s) ∈Rd: For sample s, we first obtain the embedding
difference vector |yl −yl−1| between each adjacent-layer pair l −1 and l. VI is the average of
all differences across layers, it captures the local trajectory changes across individual dimensions:"
"DYNAMIC EMBEDDING TRAJECTORY DISTINGUISHES ID AND OOD SAMPLES IN
MATHEMATICAL REASONING",0.07104557640750671,"VI(s) = 1 L · L
X"
"DYNAMIC EMBEDDING TRAJECTORY DISTINGUISHES ID AND OOD SAMPLES IN
MATHEMATICAL REASONING",0.07238605898123325,"l=1
|yl −yl−1| = 1 L · L
X l=1"
"DYNAMIC EMBEDDING TRAJECTORY DISTINGUISHES ID AND OOD SAMPLES IN
MATHEMATICAL REASONING",0.07372654155495978," y1
l −y1
l−1
 , · · · ,
yd
l −yd
l−1
⊤.
(2)"
"DYNAMIC EMBEDDING TRAJECTORY DISTINGUISHES ID AND OOD SAMPLES IN
MATHEMATICAL REASONING",0.07506702412868632,"• Dimension-joint volatility VJ(s) ∈R: For sample s, we first obtain the L2-norm of the embedding
difference ||yl −yl−1||2 between each adjacent-layer pair l −1 and l. VJ is the average of all
such differences across layers, it captures the global changes in the trajectory:"
"DYNAMIC EMBEDDING TRAJECTORY DISTINGUISHES ID AND OOD SAMPLES IN
MATHEMATICAL REASONING",0.07640750670241286,"VJ(s) = 1 L · L
X"
"DYNAMIC EMBEDDING TRAJECTORY DISTINGUISHES ID AND OOD SAMPLES IN
MATHEMATICAL REASONING",0.0777479892761394,"l=1
||yl −yl−1||2 = 1 L · L
X l=1"
"DYNAMIC EMBEDDING TRAJECTORY DISTINGUISHES ID AND OOD SAMPLES IN
MATHEMATICAL REASONING",0.07908847184986595,"v
u
u
t1 d · d
X i=1"
"DYNAMIC EMBEDDING TRAJECTORY DISTINGUISHES ID AND OOD SAMPLES IN
MATHEMATICAL REASONING",0.08042895442359249," 
yi
l −yi
l−1
2.
(3)"
"DYNAMIC EMBEDDING TRAJECTORY DISTINGUISHES ID AND OOD SAMPLES IN
MATHEMATICAL REASONING",0.08176943699731903,"In this section, we will clarify our motivation: Why Dynamic Embedding Trajectory As The Measure?
This stems from the phenomenon of “pattern collapse” in the output space, where the endpoints of
different sample trajectories converge to a high-density region. This constraint potentially increases
the likelihood of trajectory differences across samples. We will model and prove this theoretical
intuition in Section 2.1 through the Dimension-independent volatility. After gaining insight into
trajectory differences, we specifically explore the differences between ID and OOD sample trajectories.
We will empirically investigate this in Section 2.2 through the Dimension-joint volatility."
"DYNAMIC EMBEDDING TRAJECTORY DISTINGUISHES ID AND OOD SAMPLES IN
MATHEMATICAL REASONING",0.08310991957104558,"2.1
Theoretical Intuition: Trajectory Differences with Higher Likelihood"
"DYNAMIC EMBEDDING TRAJECTORY DISTINGUISHES ID AND OOD SAMPLES IN
MATHEMATICAL REASONING",0.08445040214477212,"Figure 1 has illustrated the “pattern collapse” phenomenon in the output space in mathematical
reasoning scenarios. We abstract this phenomenon in Figure 2, which compares the trajectory"
"DYNAMIC EMBEDDING TRAJECTORY DISTINGUISHES ID AND OOD SAMPLES IN
MATHEMATICAL REASONING",0.08579088471849866,"0
5
10
15
20
25
30
Layer Number 0 10 20 30 40 50"
-NORM VALUE,0.0871313672922252,2-norm value
-NORM VALUE,0.08847184986595175,OOD data: GSM8K
-NORM VALUE,0.08981233243967829,"In-Distribution (ID)
Out-of-Distribution (OOD)"
-NORM VALUE,0.09115281501340483,"0
5
10
15
20
25
30
Layer Number 0 10 20 30 40 50"
-NORM VALUE,0.09249329758713137,2-norm value
-NORM VALUE,0.0938337801608579,OOD data: AddSub
-NORM VALUE,0.09517426273458444,"In-Distribution (ID)
Out-of-Distribution (OOD)"
-NORM VALUE,0.09651474530831099,"0
5
10
15
20
25
30
Layer Number 0 20 40 60 80 100"
-NORM VALUE,0.09785522788203753,2-norm value
-NORM VALUE,0.09919571045576407,OOD data: SVAMP
-NORM VALUE,0.10053619302949061,"In-Distribution (ID)
Out-of-Distribution (OOD)"
-NORM VALUE,0.10187667560321716,"0
5
10
15
20
25
30
Layer Number 0 10 20 30 40 50"
-NORM VALUE,0.1032171581769437,2-norm value
-NORM VALUE,0.10455764075067024,OOD data: SingleEq
-NORM VALUE,0.10589812332439678,"In-Distribution (ID)
Out-of-Distribution (OOD)"
-NORM VALUE,0.10723860589812333,"0
5
10
15
20
25
30
Layer Number 0 10 20 30 40 50"
-NORM VALUE,0.10857908847184987,2-norm value
-NORM VALUE,0.10991957104557641,OOD data: SingleOp
-NORM VALUE,0.11126005361930295,"In-Distribution (ID)
Out-of-Distribution (OOD)"
-NORM VALUE,0.1126005361930295,"0
5
10
15
20
25
30
Layer Number 0 10 20 30 40 50 60 70 80"
-NORM VALUE,0.11394101876675604,2-norm value
-NORM VALUE,0.11528150134048257,OOD data: MATH-algebra
-NORM VALUE,0.11662198391420911,"In-Distribution (ID)
Out-of-Distribution (OOD)"
-NORM VALUE,0.11796246648793565,"0
5
10
15
20
25
30
Layer Number 0 10 20 30 40 50 60 70"
-NORM VALUE,0.1193029490616622,2-norm value
-NORM VALUE,0.12064343163538874,OOD data: MATH-geometry
-NORM VALUE,0.12198391420911528,"In-Distribution (ID)
Out-of-Distribution (OOD)"
-NORM VALUE,0.12332439678284182,"0
5
10
15
20
25
30
Layer Number 0 10 20 30 40 50 60"
-NORM VALUE,0.12466487935656836,2-norm value
-NORM VALUE,0.1260053619302949,OOD data: MATH-counting_and_probability
-NORM VALUE,0.12734584450402145,"In-Distribution (ID)
Out-of-Distribution (OOD)"
-NORM VALUE,0.128686327077748,"0
5
10
15
20
25
30
Layer Number 0 20 40 60 80 100"
-NORM VALUE,0.13002680965147453,2-norm value
-NORM VALUE,0.13136729222520108,OOD data: MATH-number_theory
-NORM VALUE,0.13270777479892762,"In-Distribution (ID)
Out-of-Distribution (OOD)"
-NORM VALUE,0.13404825737265416,"0
5
10
15
20
25
30
Layer Number 0 20 40 60 80 100 120"
-NORM VALUE,0.1353887399463807,2-norm value
-NORM VALUE,0.13672922252010725,OOD data: MATH-precalculus
-NORM VALUE,0.1380697050938338,"In-Distribution (ID)
Out-of-Distribution (OOD)"
-NORM VALUE,0.13941018766756033,"Figure 3: Trajectory volatility curve comparisons between one ID data and ten OOD data from diverse
mathematical domains. Each trajectory represents the average of all samples from the corresponding
datasets, with color shading being the sample standard deviation. Llama2-7B is used for the backbone."
-NORM VALUE,0.14075067024128687,"trend between different samples in the mathematical reasoning and text generation scenarios: In
mathematical reasoning, when initial points of two trajectories are separated by any distance in
the input space, they typically converge to a significantly closer distance in the output space after
undergoing an embedding shift. However, in text generation, outputs from different samples may
not exhibit this same convergence. This finding inspires the following theoretical intuition: Hard
constraints on trajectory endpoints in mathematical reasoning allow for a higher probability of
trajectory differences under different samples, as expressed by the key hypothesis:"
-NORM VALUE,0.14209115281501342,"Hypothesis 1 In scenarios characterized by pattern collapse in the output space, the probability of
trajectory volatility differences across samples increases."
-NORM VALUE,0.14343163538873996,"We now formalize this hypothesis. Assume that the output space is a Gaussian distribution N(c, Σ2),
so the output embedding yL ∼N(c, Σ2). For two different samples si and sj, their dimension-
independent volatilities are VI(si) and VI(sj), respectively. We want to show that when pattern
collapse exists, the probability of VI(si) −VI(sj) ̸= 0 will increase. According to the pattern
collapse features under different scenarios, we can constraint the yL as follows:"
-NORM VALUE,0.1447721179624665,"• For mathematical reasoning with pattern collapse, Σ →O, so we approximate that yL ≡c;
• For text generation without pattern collapse, Σ = diag(δ1, δ2, · · · , δD) ̸= O, so yL ̸≡c."
-NORM VALUE,0.14611260053619302,"With such formalized constraints, we model our Hypothesis 1 as a main theorem:"
-NORM VALUE,0.14745308310991956,"Theorem 2.1 (Main Theorem) We assume that {yl}L
l=1 are all independent variables sampling
from vector space Rd. For different samples si and sj, their embedding sets are {[yi]l}L
l=1 and
{[yj]l}L
l=1, respectively. The likelihood of trajectory volatility differences between si and sj under
mathematical reasoning scenarios is higher than that under text generation scenarios, which means:"
-NORM VALUE,0.1487935656836461,"E{[yi]l}L
l=1, {[yj]l}L
l=1 ∼U(Rd) {VI(si) −VI(sj) ̸= 0|[yi]L, [yj]L ≡c}"
-NORM VALUE,0.15013404825737264,"> E{[yi]l}L
l=1, {[yj]l}L
l=1 ∼U(Rd)

VI(si) −VI(sj) ̸= 0|[yi]L, [yj]L ∼N(c, Σ2)"
-NORM VALUE,0.15147453083109919,"Due to space limits, we present the Complete Proof in Appendix C. This demonstrates that when
pattern collapse occurs, the probability of trajectory volatility differences across samples increases."
-NORM VALUE,0.15281501340482573,"2.2
Empirical Analysis: Early Stabilization of ID samples"
-NORM VALUE,0.15415549597855227,"The theoretical analysis in Section 2.1 provides an intuition for using embedding trajectory as the
measure for distinguishing between different samples. However, the specific trajectory differences
between ID and OOD samples remain unclear. Thus, we investigate this empirically in this section."
-NORM VALUE,0.1554959785522788,"We select eleven mathematical reasoning datasets: one for ID data and ten for OOD data. The
MultiArith dataset serves as the ID data, while the OOD datasets include GSM8K, SVAMP, AddSub,
SingleEq, SingleOp, and MATH. The latter encompasses five tasks across various mathematical
domains: Algebra, Geometry, Counting and Probability, Number Theory, and Precalculus. This
selection includes varying task types and levels of difficulty. We first train a base Llama2-7B (32
layers) using the ID training set, then inference on the ID test set and all OOD test sets. Details
regarding the datasets, model, and implementation can be found in Section 4.1."
-NORM VALUE,0.15683646112600536,"We measure the dimension-joint volatility for all samples from each ID and OOD test set. Specifically,
we compute each adjacent-layer change magnitude ||yl −yl−1||2, and connect all values as a change
curve, with higher value means the higher volatility. Figure 3 shows ten curve comparisons, with
each sub-figure including the ID data and one OOD data. We find that the change magnitude slightly
until the 20th layer. After 20 layers, for ID data, the change magnitude is again suppressed after a
few layers of inference, while for OOD data, the magnitude is maintained at a relatively high level."
-NORM VALUE,0.1581769436997319,"We term this phenomenon “Early Stabilization”: For ID data, GLMs largely complete their reasoning
in the mid-to-late stages, and simple adjustments are sufficient after that. However, for OOD data,
GLMs can still not complete accurate reasoning at a later stage. They thus can only randomly switch
to a specific output pattern, i.e., the scalar mathematical expression pattern. This provides strong
evidence that using embedding trajectory for OOD detection may be effective."
-NORM VALUE,0.15951742627345844,"3
TV Score: Trajectory Volatility Score for OOD Detection"
-NORM VALUE,0.16085790884718498,"In Section 2.2, we have identified a significant difference in embedding trajectory volatilities between
ID and OOD samples in mathematical reasoning. We now aim to leverage this phenomenon to
develop a lightweight OOD detection solution tailored for mathematical reasoning scenarios."
-NORM VALUE,0.16219839142091153,"Inspired by static embedding methods that use the ID sample embedding cluster as the reference to
measure the Mahalanobis Distance (MaDis) [22], we similarly aim to use the ID sample trajectory
cluster as the reference to measure the difference between them and a new sample trajectory. While
measuring the difference between an embedding and an embedding cluster only requires a simple
MaDis calculation, quantifying the difference between a trajectory and a trajectory cluster is less
intuitive. Thus, we seek a transition idea starting from the static space Gaussian assumption."
-NORM VALUE,0.16353887399463807,"First, we obey the assumption under static embeddings [3, 43] to fit all ID embeddings at each layer l
to a Gaussian distribution Gl = N(µl, Σl). Next, for a new sample with yl be its embedding at layer
l, we map it to its MaDis f(yl) : Rd →R with Gl as follows:"
-NORM VALUE,0.1648793565683646,"f(yl) = (yl −µl)⊤(Σl)−1 (yl −µl)
(0 ≤l ≤L).
(4)"
-NORM VALUE,0.16621983914209115,"Finally, we treat the MaDis differences |f(yl) −f(yl−1)| between adjacent layer-pairs l −1 and l as
the adjacent-layer volatility of the new sample, and average all adjacent-layer volatilities as the final
trajectory volatility score (TV Score S) of this sample: S = 1 L · L
X"
-NORM VALUE,0.1675603217158177,"l=1
|f(yl) −f(yl−1)| .
(TV Score)"
-NORM VALUE,0.16890080428954424,"The anticipated trend is that when the two adjacent embeddings after MaDis mapping exhibit a greater
difference, the embedding change between the two layers is more volatile compared to ID data. This
enables us to identify new samples with trajectory volatility features that significantly deviate from
those of the ID samples, thus increasing the likelihood that they are OOD samples."
-NORM VALUE,0.17024128686327078,"Furthermore, as a trajectory, outliers in the trajectory may significantly impact feature extraction
[21, 28]. To mitigate this, we explore higher-order differential smoothing techniques to enhance
trajectory smoothness. We first define the k-order embedding ∇kyl and Gaussian distribution
∇kGl = ∇kN(µl, Σl) for all l ≤L −k based on the backward difference:"
-NORM VALUE,0.17158176943699732,"∇kyl = k
X"
-NORM VALUE,0.17292225201072386,"i=0
(−1)k+iCi
kyl+k,
∇kGl = N k
X"
-NORM VALUE,0.1742627345844504,"i=0
(−1)k+iCi
kµl+k, k
X"
-NORM VALUE,0.17560321715817695,"i=0
Ci
kΣl+k ! ,
(5)"
-NORM VALUE,0.1769436997319035,"where Ci
k =
k!
i!(k−i)!. Similarly, we mapping ∇kyl to its MaDis ∇kf(yl) : Rd →R with ∇kGl:"
-NORM VALUE,0.17828418230563003,∇kf(yl) = 
-NORM VALUE,0.17962466487935658,"∇kyl − k
X"
-NORM VALUE,0.18096514745308312,"i=0
(−1)k+iCi
kµl+k"
-NORM VALUE,0.18230563002680966,"!⊤ k
X"
-NORM VALUE,0.1836461126005362,"i=0
Ci
kΣl+k !−1"
-NORM VALUE,0.18498659517426275,"∇kyl − k
X"
-NORM VALUE,0.1863270777479893,"i=0
(−1)k+iCi
kµl+k ! . (6)"
-NORM VALUE,0.1876675603217158,"Following the definition of TV Score, we define the trajectory volatility score after differential
smoothing (TV Score w/ DiSmo ∇kS) as follows:"
-NORM VALUE,0.18900804289544235,"∇kS = 1 L · L
X l=1"
-NORM VALUE,0.1903485254691689,"∇kf(yl) −∇kf(yl−1)
 .
(TV Score w/ Dismo)"
-NORM VALUE,0.19168900804289543,The Algorithmic Process and Computational Complexity are detailed in Appendix D.
EXPERIMENTS,0.19302949061662197,"4
Experiments"
SETUP,0.19436997319034852,"4.1
Setup"
SETUP,0.19571045576407506,"Dataset Selection.
For the ID dataset, we use the MultiArith [44], which consists of Math Word
Problems on arithmetic reasoning. For the OOD datasets, we intuitively introduce two types of
detection scenarios following [43]: (i) Far-shift OOD scenario, we select the MATH [11] dataset
with five domains of algebra, geometry, counting and probability, number theory, and precalculus; (ii)
Near-shift OOD scenario, we select five independent datasets: GSM8K [6], SVAMP [39], AddSub
[13], SingleEq [18], and SingleOp [18]. We consider the ID data negative(-) and the OOD data
positive(+). Refer to Appendix E.1 for basic information and OOD features of these datasets."
SETUP,0.1970509383378016,"Data Split and Sampling.
Given the limited data size of MultiArith, totaling only 600 samples
and lacking a standard division, we allocate 360 samples for training and 240 for testing. However,
with such a small test set, randomness in evaluation becomes a concern. To mitigate this, we conduct
test sampling and set the sampling size as 1000. Specifically, we denote ID dataset as Din and OOD
dataset as Dout. For each sampling, the collection is {Din, eDout} where eDout ⊂Dout and |Din| = | eDout|,
this guarantees positive and negative sample balance. We report both the mean and standard variance
of the results to enhance the reliability of evaluations. Refer to Appendix E.2 for the ID dataset split."
SETUP,0.19839142091152814,"Implementation.
To measure the application value of our method used in cutting-edge GLMs,
we use Llama2-7B [50] and GPT2-XL (1.5B) [5] as our backbones for ID dataset training. Refer
to Appendix E.3 for training details. However, there exists uncertainty about the data used in the
pre-training phase, especially for Llama2 because its data is closed-source. Some research [57, 59]
have confirmed the absence of data leakage in Llama2 for the MATH and GSM8K datasets, we
still conduct pre-experiments to examine the rationality of the OOD data selection rigorously. Our
criterion is the claim that a dataset can be categorized as OOD if it exceeds the capabilities of the base
model, as proposed in prior studies [47, 27]. Results of the pre-experiments are shown in Appendix
E.4, and they can confirm that these datasets can be considered as OOD data for the two GLMs."
SETUP,0.19973190348525469,"Baseline.
We compare our method with five training-free baselines where OOD training data are
unavailable. We refer to the latest survey [20] to select them for the scarcity of OOD detection
methods on GLMs: (1) Maximum Softmax Probability (Prob.) [12]; (2) Monte-Carlo Dropout [8]; (3)
Sequence Perplexity [3]; (4) Input Embedding [43]; (5) Output Embedding [43]. Refer to Appendix
E.5 for details. Additionally, we set the smoothing order k ranges from 1 to 5 for TV Score w/ DiSmo
and report the highest among them when with smoothing."
SETUP,0.20107238605898123,"Evaluation.
We divide the OOD detection evaluation into two scenarios: (1) Offline Detection,
which classifies whether samples from a given list belong to OOD. For each collection {Din, eDout},
we report the AUROC [35] and FPR95 metrics. The former represents the area under the ROC curve
and the latter represents the value of FPR at 95% TPR. (2) Online Detection, which utilizes the offline
detection results to calculate an optimal classification threshold for a direct determination of whether
new samples belong to OOD. We introduce the metrics in the corresponding result sub-sections."
MAIN RESULTS,0.20241286863270777,"4.2
Main Results"
MAIN RESULTS,0.2037533512064343,"Offline Detection.
Table 1 presents the results of the offline detection scenarios.
• Performance Analysis: In the far-shift OOD setting, our average performance surpasses 98
(Llama2-7B) and 96 (GPT2-XL) under the AUROC metric, surpassing the optimal baseline
by 10+ points. Moreover, our performance stands at an impressive 5.21 (Llama2-7B) and 9.89
(GPT2-XL) under the FPR95 metric, representing a remarkable 80%+ reduction compared
to the optimal baseline, far surpassing all baseline methods. In the near-shift OOD setting, the
robustness of our method is even more impressive. All of the baseline methods show significant
performance degradation, especially in Llama2-7B, with the AUROC metric decreasing below 60
and the FPR95 elevating above 80. However, our method maintains excellent performances, with
AUROC scores surpassing 90 and FPR95 below 30. This indicates that for more fine-grained OOD
detection scenarios, our method demonstrates greater adaptability."
MAIN RESULTS,0.20509383378016086,"• Model Analysis: Comparing performances of Llama2-7B and GPT2-XL, we find two phenomena:
(i) Results on GPT2-XL are more stable, performance differences between GPT2-XL on far-
and near-drift settings are not significant, while Llama2-7B shows a significant performance"
MAIN RESULTS,0.2064343163538874,"Table 1: AUROC and FPR95 results of the Offline Detection scenario. Underline and bold denote
SOTA among all baselines and all methods, respectively. We report the average results under each
setting in the main text, results of each dataset are shown in Table 11 and 12 (Appendix F)."
MAIN RESULTS,0.20777479892761394,"Model
Llama2-7B [50]
GPT2-XL [5]"
MAIN RESULTS,0.20911528150134048,"Method
Metric
Far-shift OOD
Near-shift OOD
Far-shift OOD
Near-shift OOD"
MAIN RESULTS,0.21045576407506703,"AUROC ↑
FPR95 ↓
AUROC ↑
FPR95 ↓
AUROC ↑
FPR95 ↓
AUROC ↑
FPR95 ↓"
MAIN RESULTS,0.21179624664879357,"Max Softmax Prob. [12]
78.66±1.38
81.44±3.56
60.14±1.54
88.91±2.41
70.54±1.42
78.29±2.02
67.12±1.20
76.27±2.66
Monte-Carlo Dropout [8]
68.63±2.21
87.04±4.88
52.33±2.21
91.92±1.89
66.18±1.87
84.69±1.65
63.54±1.72
78.08±2.50
Perplexity [3]
85.64±1.46
53.06±4.36
59.35±1.89
86.09±1.89
80.82±1.04
64.53±2.10
73.74±1.12
72.39±1.27
Input Embedding [43]
75.89±1.03
67.87±3.69
60.33±1.37
84.65±2.53
86.26±0.84
49.33±2.10
83.22±0.88
52.90±3.16
Output Embedding [43]
74.86±1.39
75.21±2.16
44.50±1.06
86.46±1.59
77.95±1.16
65.64±3.42
79.28±1.24
64.70±2.72"
MAIN RESULTS,0.2131367292225201,"TV Score (Ours)
98.76±0.11
5.21±0.98
92.64±0.39
28.39±1.38
93.47±0.08
24.10±0.95
94.86±0.23
13.82±0.36
w/ DiSmo (Ours)
93.25±0.76
41.82±4.69
56.99±1.41
88.01±1.71
96.54±0.11
9.89±0.61
94.19±0.25
13.66±0.69"
MAIN RESULTS,0.21447721179624665,"∆(bold - underline)
+13.12
-47.85
+32.31
-56.26
+10.28
-39.44
+11.64
-39.24"
MAIN RESULTS,0.2158176943699732,"Table 2: Accuracy and Robustness results of the Online Detection scenario. We mainly compare our
method with embedding-based methods, and bold denotes the best among these methods."
MAIN RESULTS,0.21715817694369974,"Far-shift OOD Setting
Near-shift OOD Setting"
MAIN RESULTS,0.21849865951742628,"Dataset
Accuracy↑
Robustness↓
Dataset
Accuracy↑
Robustness↓"
MAIN RESULTS,0.21983914209115282,"I-Emb. / O-Emb. / TV (ours)
I-Emb. / O-Emb. / TV (ours)"
MAIN RESULTS,0.22117962466487937,"Algebra
76.43 / 45.42 / 93.88
5.27 / 6.94 / 0.97
GSM8K
81.49 / 75.32 / 93.39
10.08 / 3.36 / 2.05
Geometry
74.32 / 54.79 / 94.47
2.44 / 2.43 / 1.65
SVAMP
68.66 / 63.33 / 94.88
5.26 / 3.54 / 2.13
Cnt.&Prob
50.31 / 27.55 / 93.74
9.99 / 2.34 / 2.36
AddSub
79.16 / 78.09 / 74.11
3.21 / 6.98 / 2.77
Num.Theory
85.80 / 54.38 / 92.08
3.31 / 11.45 / 2.34
SingleEq
59.83 / 72.56 / 93.15
11.57 / 3.14 / 3.17
Precalculus
80.33 / 88.50 / 99.28
6.13 / 1.38 / 0.67
SingleOp
69.38 / 62.20 / 95.75
4.00 / 2.37 / 2.45"
MAIN RESULTS,0.2225201072386059,"Average
73.44 / 54.13 / 94.69
5.43 / 4.91 / 1.60
Average
71.70 / 70.30 / 90.26
6.82 / 3.88 / 2.51"
MAIN RESULTS,0.22386058981233245,"degradation (mainly for baselines) on near-shift setting; (ii) the DiSmo technique is more effective
on GPT2-XL, which suggests that there are more anomalous learning tendencies in latent spaces
of small models, and the smoothing helps to minimize these anomalies."
MAIN RESULTS,0.225201072386059,"In addition, we conduct significant tests (Details are shown in Table 11 - 14). We find that our
methods almost pass all significance tests, while the embedding-based methods have the lowest pass
rate among baselines, suggesting that their results are more susceptible to sampling error. We also
find that the performance of differential smoothing fluctuates greatly in different settings. Therefore,
we conduct the ablation of the smoothing order k. Results and analyses are shown in Appendix F.1."
MAIN RESULTS,0.22654155495978553,"Online Detection.
In this part, we utilize the TV score for online OOD discrimination. For each
collection {Din, eDout}, we obtain a detector and computer the optimal cut-off τi of Youden Index,
which is at the point in the AUROC curve where TPR −FPR is maximum. Then for all OOD
samples s ∈Dout −eDout, we donate t as the sampling size and computer the discrimination accuracy:"
MAIN RESULTS,0.22788203753351208,"Accuracy = 1 t t
X i=1 P"
MAIN RESULTS,0.2292225201072386,"s∈Dout−e
Dout I [TV-Score(s) ≥τi]
Dout −eDout

.
(7)"
MAIN RESULTS,0.23056300268096513,"In addition, the discrimination accuracy should vary less under different data collections, reflecting
the discriminator’s robustness. Therefore, we denote the Robustness metric as sampling variance."
MAIN RESULTS,0.23190348525469168,"Table 2 presents the results in Llama2-7B. Compared to the embedding-based methods, our TV
score obtains about an average of 20-point accuracy improvement in both far-shift OOD and
near-shift OOD settings, and on some datasets, such as Cnt.&Prob, our TV score achieves more than
40 points of improvement. These all imply that TV Score can perform online discrimination of OOD
samples more accurately. In addition, our TV score also possesses stronger robustness, which means
that in real scenarios, we can find the optimal threshold more consistently in the face of different
accessible ID and OOD data, reducing the potential riskiness due to uncontrollable data acquisition."
GENERALIZABILITY EXPLORATION,0.23324396782841822,"5
Generalizability Exploration"
GENERALIZABILITY EXPLORATION,0.23458445040214476,"5.1
Beyond Detection: OOD Quality Estimation"
GENERALIZABILITY EXPLORATION,0.2359249329758713,"In this part, we utilize the TV score for generative quality estimation (QE). For text generation, the
QE performance is usually measured by calculating the correlation coefficient between automatic
scores and human ratings. However, QE in mathematical reasoning scenarios is not a well-defined
problem. For one mathematical question, its answer is either right or wrong, the intermediate state
does not exist. For example, when the correct answer is 12.5, it is difficult to judge which is better
between generated answers of 1.25 and 13.6. The human approach may be to judge by comparing
the difference value or similarity like Rouge [24] and BertScore [60] between the generated answer
and the correct answer, which is unfair to the machine because there is a lot of randomness in the
intermediary process of computation, and the solution pattern of machines is case-based [14], so it is
not suitable to judge the machine-generated results with customized mathematical rules."
GENERALIZABILITY EXPLORATION,0.23726541554959785,"Therefore, we use the binary direct matching1 to compare the model-generated answers with the
correct answers. Considering the open-ended output of the GLMs, we give a loose matching condition,
i.e., as long as the correct answer is included in the generated answer by the model, the generated
answer is recognized as correct and the matching score is 1, otherwise the matching score is 0. We
compute the Kendall rank correlation coefficient τ [45] and Spearman rank correlation coefficient
[36] between each OOD score and the matching score."
GENERALIZABILITY EXPLORATION,0.2386058981233244,"Table 3: OOD Quality Estimation: Kendall’s τ and Spearman correlation between various OOD
scores and benchmark quality metric binary matching. Each column shows the correlation when ID
and OOD samples are merged. Underline denotes the SOTA among all baselines, and bold denotes
the SOTA among our methods. We report the average results under each setting in the main text,
results of each dataset are shown in Table 13 and 14 (Appendix F)."
GENERALIZABILITY EXPLORATION,0.23994638069705093,"Model
Llama2-7B [50]
GPT2-XL [5]"
GENERALIZABILITY EXPLORATION,0.24128686327077747,"Method
Metric
Far-shift OOD
Near-shift OOD
Far-shift OOD
Near-shift OOD"
GENERALIZABILITY EXPLORATION,0.24262734584450402,"Kendall ↑
Spearman ↑
Kendall ↑
Spearman ↑
Kendall ↑
Spearman ↑
Kendall ↑
Spearman ↑"
GENERALIZABILITY EXPLORATION,0.24396782841823056,"Max Softmax Prob.
0.024±0.020
0.038±0.020
0.038±0.018
0.026±0.018
0.066±0.015
0.044±0.016
0.057±0.018
0.057±0.022
Perplexity
0.050±0.015
0.045±0.016
0.074±0.017
0.050±0.018
0.036±0.014
0.038±0.017
0.035±0.018
0.058±0.019
Input Embedding
0.078±0.016
0.102±0.017
0.036±0.018
0.115±0.017
0.059±0.012
0.098±0.016
0.012±0.018
0.068±0.016
Output Embedding
0.058±0.018
0.025±0.017
0.038±0.015
0.012±0.017
0.050±0.012
0.016±0.017
0.036±0.017
0.029±0.021"
GENERALIZABILITY EXPLORATION,0.2453083109919571,"TV Score (Ours)
0.161±0.012
0.147±0.015
0.159±0.017
0.158±0.017
0.138±0.010
0.123±0.013
0.131±0.015
0.146±0.015
w/ DiSmo (Ours)
0.111±0.016
0.152±0.015
0.113±0.018
0.134±0.017
0.139±0.009
0.141±0.014
0.123±0.014
0.154±0.016"
GENERALIZABILITY EXPLORATION,0.24664879356568364,"∆(bold - underline)
+0.083
+0.050
+0.085
+0.043
+0.073
+0.043
+0.074
+0.086"
GENERALIZABILITY EXPLORATION,0.2479892761394102,"Table 3 presents the results. For Llama2-7B, when compared with Kendall correlation, the correlation
improvement of TV scores over SOTA baselines reaches up to 100% under both far-shift and
near-shift OOD settings. Compared with Spearman correlation, TV scores demonstrate a correlation
enhancement over SOTA baselines by up to 100% under far-shift OOD setting and 30% under
near-shift OOD setting. GPT2-XL also demonstrates excellent performance. These findings indicate
that our TV scores not only facilitate the binary discrimination of ID and OOD samples but also
substantially reflect the quality and precision of generated mathematical reasoning."
BEYOND MATHEMATICAL REASONING,0.24932975871313673,"5.2
Beyond Mathematical Reasoning"
BEYOND MATHEMATICAL REASONING,0.25067024128686327,"Apart from mathematical reasoning, our method also has a wider range of potential applications
that can be extended to any task where the output space exhibits the pattern collapse property.
An example would be multiple-choice questions, which is a popular evaluation tool in the era of
large language models and also display the pattern collapse property due to the limited output space
being confined to the “ABCD” four options. To verify the generalizability of our method, we conduct
experiments using the multiple-choice dataset MMLU [10], and our method also outperforms all
traditional algorithms in this setting. Results and analyses are shown in Appendix F.4."
BEYOND MATHEMATICAL REASONING,0.2520107238605898,"1Although direct matching is the most accurate solution, it suffers from two issues: (i) Generated answers
may include much noisy content, increasing the matching difficulty; (ii) Performances on GLMs of mathematical
reasoning is poor, which unbalances the positive and negative samples and increases the randomness."
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.25335120643431636,"6
Rethink Inapplicability of Static Embedding Methods"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.2546916890080429,"In Figure 1, we find that the static embedding space for both input and output fails to capture distinct
features across different domains. This renders traditional embedding-based methods unsuitable for
mathematical reasoning tasks. Experimental results in Section 4 also verify this disadvantage. In this
section, we conduct a detailed analysis of these phenomena, and reveal the root cause of the poor
performance of static embedding methods in mathematical reasoning scenarios."
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.25603217158176944,"6.1
Input Space: Representation Dilemma on Mathematical Expressions"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.257372654155496,"The input space in mathematical reasoning exhibits vague clustering features compared to text
generation scenarios as shown in Figure 1. We speculate that as semantic representations, embeddings
cannot accurately measure mathematical expressions in the mathematical sense."
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.2587131367292225,"Table 4: A toy example about cosine similarities
between different mathematical expressions."
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.26005361930294907,"Mathematical Expression
Cosine with Benchmark"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.2613941018766756,Benchmark: 2+3+4=?
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.26273458445040215,"8-0+9=?
0.65
xˆ2*yˆ3*xˆ4=?
0.83
234*2+345*4+456*4-243*3=?
0.78"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.2640750670241287,"We begin by considering a counter-intuitive toy
example. We construct a benchmark expression
“A: 2+3+4=?” and three diverse expressions:
“X: 8-0+9=?”, “Y: xˆ2*yˆ3*xˆ4=?”, and “Z:
234*2+345*4+456*4-243*3=?”.
From the
mathematical sense, X is ID for A, whereas Y and
Z are OOD for A since the difficulty and knowledge
used to solve Y and Z are largely different from
A compared to X. Table 4 presents the cosine
similarity between each of them and A. Notably, X exhibits the lowest similarity to A, primarily due
to having fewer token sharing with A. This is inconsistent with human perception of mathematics."
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.26541554959785524,"Table 5: AUROC score matrix produced after alternating the MATH dataset’s five domains as ID and
OOD data measured by (a) Input Embedding Mahalanobis Distance and (b) Output Embedding
(w/ CoT) Mahalanobis Distance. Darker colors represent better performances."
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.2667560321715818,OOD Domain
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.2680965147453083,"(a) Input Embedding MaDis
(b) Output Embedding (w/ CoT) MaDis"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.26943699731903487,"algebra
geometry
cnt.&prob
num.theory
precalculus
algebra
geometry
cnt.&prob
num.theory
precalculus"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.2707774798927614,ID Domain
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.27211796246648795,"algebra
-
63.77
80.01
58.79
50.80
-
71.37
84.24
65.06
56.71
geometry
85.68
-
88.14
86.55
69.03
72.12
-
92.42
86.08
70.89
cnt.&prob
49.95
44.94
-
38.02
51.76
46.19
53.61
-
40.15
51.63
num.theory
66.35
78.87
50.00
-
71.93
63.02
78.76
69.85
-
67.96
precalculus
85.14
79.31
86.18
89.34
-
79.84
85.65
86.77
90.35
-"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.2734584450402145,"To demonstrate this phenomenon, we use one of five domains from the MATH dataset as the ID and
the remaining four domains as OOD. We rotate this setting across five iterations. Under each ID-OOD
pair, we allocate half of the ID samples to compute the ID embedding distribution, while the other
half and all OOD samples constitute the test samples. The Mahalanobis distance is then calculated
between embeddings and the ID distributions for each sample input, yielding the OOD score. We
employ SimCSE [9] to generate sentence embeddings. Table 5(a) presents the AUROC score matrix,
in nearly half of the settings, the AUROC value hovers around 50, i.e., completely random. This
suggests that embeddings can not distinguish between domains in a mathematical sense."
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.27479892761394104,"6.2
Output Space: High-density “Pattern Collapse”"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.2761394101876676,"The output space in mathematical reasoning exhibits a unique high-density characteristic, which we
call “pattern collapse”. As shown in Figure 1, the distribution of output embeddings across various
domains in the mathematical reasoning scenarios is significantly concentrated and indistinguishable."
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.2774798927613941,"Two reasons cause this phenomenon: (1) Expression level: Output space of mathematical reasoning is
scalar [37], resulting in a compressed search space with a higher probability of overlap in two widely
divergent questions. For example, “1 + 3 =” and “
R 3
1 xdx =” are both “4”; (2) Token level: We
usually categorize different mathematical reasoning domains from a mathematical sense. However,
GLMs model real numbers or mathematical expressions not in a mathematical sense, but based
on the average embedding of all discrete token embeddings after sequence tokenization. Through"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.27882037533512066,"tokenization, two expressions that are very different in the mathematical sense (e.g., Two numbers
that are far apart on the real number line) may share many tokens because regular mathematical
expressions are only taken from 0-9 number tokens and a limited number of special symbols, such
as decimal points, slashes, and root signs. Thus, the collapse occurs at the token level during the
autoregressive prediction of each token."
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.2801608579088472,"Table 6: Statistics about output tokens on mathematical
reasoning (seven different task types) and text generation
(translation and summarization)."
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.28150134048257375,"Task Type
N
NT
D
C"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.2828418230563003,Mathematical Reasoning
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.28418230563002683,"Arithmetic(easy)
16136
14
99.9%
0.04%
Arithmetic(hard)
5663
16
99.7%
0.05%
Algebra
5234
107
98.0%
0.33%
Geometry
2615
75
97.1%
0.23%
Cnt.&Prob.
2524
43
98.3%
0.13%
Num.Theory
2395
71
97.1%
0.22%
Precalculus
3388
84
97.5%
0.26%"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.2855227882037534,"Average
5422
58
98.9%
0.18%"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.2868632707774799,Text Generation
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.28820375335120646,Translation
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.289544235924933,"2500
1065
57.4%
3.32%
5000
1832
63.3%
5.10%
10000
2980
70.2%
9.31%"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.29088471849865954,"Average
5833
1959
66.4%
6.12%"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.29222520107238603,Summarization
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.2935656836461126,"2500
1265
49.4%
4.01%
5000
1970
60.6%
6.16%
10000
3192
68.0%
9.98%"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.2949061662198391,"Average
5833
2142
63.2%
6.69%"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.29624664879356566,"We conduct the following statistical
analysis to demonstrate the universality
of “pattern collapse” across various
mathematical reasoning tasks:
We
categorize the mathematical tasks into
various types across different domains
and difficulties. In each task, we count
the token number N and the token type
number NT in the dataset, then compute
the token duplication rate D and the
vocab coverage C as follows: We use
Llama-2 tokenizator [50], whose token
type number in vocab NV is 32000. The
computation metric is:"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.2975871313672922,D = 1 −NT
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.29892761394101874,"N ,
C = NT"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.3002680965147453,"NV
.
(8)"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.30160857908847183,"We also test translation and summariza-
tion tasks by taking samples with the
same token size as the mathematical
reasoning dataset for a clear comparison.
Table 6 presents the statistics data,
we find that: (i) The average token
duplication rate was 98.9% on all math
tasks, and even a staggering 99.9% on
some easy arithmetic tasks; In contrast,
the token duplication rate on the text
generation task is only about 60%, with about 2000 token types, and still increasing as the total
number of tokens increases. These data and comparisons demonstrate that pattern collapse occurs on
mathematical reasoning and not on text generation. (ii) Token repetition rate exceeded 97% on all
seven math tasks of different difficulties and types. From these conclusions, we can demonstrate that
the “pattern collapse” occurs on generally all types of mathematical reasoning tasks."
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.30294906166219837,"6.3
Can Chain-of-Thought Address “pattern collapse”?"
RETHINK INAPPLICABILITY OF STATIC EMBEDDING METHODS,0.3042895442359249,"A straightforward approach to addressing “pattern collapse” in the output space is to leverage chain-
of-thought (CoT) techniques [54, 17, 62, 52, 61] to expand the output space size. Likewise, we
adopt the solution steps associated with each sample in the MATH dataset as the output and employ
SimCSE to derive embedding representations. The experimental setup aligns with Section 4.1, and
the results are shown in Table 5(b). We note a similar phenomenon as in Table 5(a), i.e., the detection
accuracy under different ID-OOD pairs varies greatly, and thus the detection randomness is more
pronounced. This suggests that despite that CoT expands the output space size, the output answer
is still essentially related to the difficulty and digit of mathematical reasoning, and the semantic
embedding representation cannot reflect these features accurately."
CONCLUSION,0.30563002680965146,"7
Conclusion"
CONCLUSION,0.306970509383378,"We propose the TV Score, a lightweight OOD detection method for mathematical reasoning, which
distinguishes between ID and OOD samples by the embedding trajectory volatility in the latent space.
We identify bottlenecks in OOD detection for mathematical reasoning and prove them empirically and
theoretically. Experiments show that our method substantially outperforms all traditional algorithms,
and can be extended to more application scenarios beyond mathematical reasoning."
CONCLUSION,0.30831099195710454,Limitations
CONCLUSION,0.3096514745308311,"Our limitation mainly lies in the relatively small sizes of datasets used in our experiments. Due to
the difficulty of collecting and labeling mathematical reasoning data, dataset sizes in this field are
generally small, mostly in the hundreds or thousands, making it difficult to obtain millions of data
for training and reasoning as in translation or summarization tasks. To address this, we adopt test
sampling to reduce the randomness under small-scale testing and mitigate the data imbalance."
ETHICS STATEMENT,0.3109919571045576,Ethics Statement
ETHICS STATEMENT,0.31233243967828417,"The data and models used in this work are sourced from the official version of the original paper, and
we strictly adhere to the provided usage protocol. Regarding the data, no modifications have been
made to the original dataset. Regarding the models, supervised fine-tuning and OOD data inference
are involved. To mitigate the risk of uncontrollable outputs, all generated outputs in the experiments
have been reviewed to ensure their safety. Furthermore, as our focus is solely on mathematical
reasoning and does not involve sensitive content, we would not cause any potential societal impact."
ETHICS STATEMENT,0.3136729222520107,Acknowledgment
ETHICS STATEMENT,0.31501340482573725,"This work is supported by the Alibaba Innovative Research Program. Yiming and Rui are partially
supported by the General Program of the National Natural Science Foundation of China (62176153),
the Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), and SMP-
Zhipu.AI Large Model Cross-Disciplinary Fund, the Science and Technology Development Fund.
Zhuosheng is partially supported by the National Natural Science Foundation of China (62406188).
This work is also partially supported by the Science and Technology Development Fund, Macau SAR
(Grant Nos. FDCT/060/2022/AFJ, FDCT/0070/2022/AMJ), and the Multi-year Research Grant from
the University of Macau (Grant No. MYRG-GRG2023-00006-FST-UMDF)."
REFERENCES,0.3163538873994638,References
REFERENCES,0.31769436997319034,"[1] Vahdat Abdelzad, Krzysztof Czarnecki, Rick Salay, Taylor Denounden, Sachin Vernekar, and
Buu Phan. Detecting out-of-distribution inputs in deep neural networks using an early-layer
output. arXiv preprint arXiv:1910.10307, 2019."
REFERENCES,0.3190348525469169,"[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4
technical report. arXiv preprint arXiv:2303.08774, 2023."
REFERENCES,0.3203753351206434,"[3] Udit Arora, William Huang, and He He. Types of out-of-distribution texts and how to detect
them. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing, pages 10687–10701, 2021."
REFERENCES,0.32171581769436997,"[4] Arsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov. Pitfalls of in-
domain uncertainty estimation and ensembling in deep learning. In International Conference
on Learning Representations, 2019."
REFERENCES,0.3230563002680965,"[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020."
REFERENCES,0.32439678284182305,"[6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solve math word problems. arXiv preprint arXiv:2110.14168, 2021."
REFERENCES,0.3257372654155496,"[7] Taylor Denouden, Rick Salay, Krzysztof Czarnecki, Vahdat Abdelzad, Buu Phan, and Sachin
Vernekar. Improving reconstruction autoencoder out-of-distribution detection with mahalanobis
distance. arXiv preprint arXiv:1812.02765, 2018."
REFERENCES,0.32707774798927614,"[8] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pages 1050–1059.
PMLR, 2016."
REFERENCES,0.3284182305630027,"[9] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence
embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing, pages 6894–6910, 2021."
REFERENCES,0.3297587131367292,"[10] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and
Jacob Steinhardt. Measuring massive multitask language understanding. In International
Conference on Learning Representations, 2020."
REFERENCES,0.33109919571045576,"[11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn
Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In
Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks
Track (Round 2), 2021."
REFERENCES,0.3324396782841823,"[12] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
examples in neural networks. In International Conference on Learning Representations, 2016."
REFERENCES,0.33378016085790885,"[13] Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to
solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 523–533, 2014."
REFERENCES,0.3351206434316354,"[14] Yi Hu, Xiaojuan Tang, Haotong Yang, and Muhan Zhang. Case-based or rule-based: How do
transformers do the math? arXiv preprint arXiv:2402.17709, 2024."
REFERENCES,0.33646112600536193,"[15] Rui Huang, Andrew Geng, and Yixuan Li. On the importance of gradients for detecting
distributional shifts in the wild. Advances in Neural Information Processing Systems, 34:677–
689, 2021."
REFERENCES,0.3378016085790885,"[16] Wenyu Jiang, Yuxin Ge, Hao Cheng, Mingcai Chen, Shuai Feng, and Chongjun Wang. Read:
Aggregating reconstruction error into out-of-distribution detection. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 37, pages 14910–14918, 2023."
REFERENCES,0.339142091152815,"[17] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. Advances in neural information processing systems,
35:22199–22213, 2022."
REFERENCES,0.34048257372654156,"[18] Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi.
Mawps: A math word problem repository. In Proceedings of the 2016 conference of the
north american chapter of the association for computational linguistics: human language
technologies, pages 1152–1157, 2016."
REFERENCES,0.3418230563002681,"[19] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable
predictive uncertainty estimation using deep ensembles. Advances in neural information
processing systems, 30, 2017."
REFERENCES,0.34316353887399464,"[20] Hao Lang, Yinhe Zheng, Yixuan Li, SUN Jian, Fei Huang, and Yongbin Li. A survey on
out-of-distribution detection in nlp. Transactions on Machine Learning Research, 2023."
REFERENCES,0.3445040214477212,"[21] Rikard Laxhammar and Göran Falkman. Online learning and sequential anomaly detection in
trajectories. IEEE transactions on pattern analysis and machine intelligence, 36(6):1158–1173,
2013."
REFERENCES,0.34584450402144773,"[22] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for
detecting out-of-distribution samples and adversarial attacks. Advances in neural information
processing systems, 31, 2018."
REFERENCES,0.34718498659517427,"[23] Shiyu Liang, Yixuan Li, and R Srikant. Enhancing the reliability of out-of-distribution image
detection in neural networks. In International Conference on Learning Representations, 2018."
REFERENCES,0.3485254691689008,"[24] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization
branches out, pages 74–81, 2004."
REFERENCES,0.34986595174262736,"[25] Zi Lin, Jeremiah Zhe Liu, and Jingbo Shang. Towards collaborative neural-symbolic graph
semantic parsing via uncertainty. Findings of the Association for Computational Linguistics:
ACL 2022, 2022."
REFERENCES,0.3512064343163539,"[26] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution
detection. Advances in neural information processing systems, 33:21464–21475, 2020."
REFERENCES,0.35254691689008044,"[27] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng,
Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: a survey and
guideline for evaluating large language models’ alignment. In Socially Responsible Language
Modelling Research, 2023."
REFERENCES,0.353887399463807,"[28] Yiding Liu, Kaiqi Zhao, Gao Cong, and Zhifeng Bao. Online anomalous trajectory detection
with deep generative sequence modeling. In 2020 IEEE 36th International Conference on Data
Engineering (ICDE), pages 949–960. IEEE, 2020."
REFERENCES,0.3552278820375335,"[29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International
Conference on Learning Representations, 2018."
REFERENCES,0.35656836461126007,"[30] Christos Louizos and Max Welling. Multiplicative normalizing flows for variational bayesian
neural networks. In International Conference on Machine Learning, pages 2218–2227. PMLR,
2017."
REFERENCES,0.3579088471849866,"[31] Denis Lukovnikov, Sina Daubener, and Asja Fischer.
Detecting compositionally out-of-
distribution examples in semantic parsing. In Findings of the Association for Computational
Linguistics: EMNLP 2021, pages 591–598, 2021."
REFERENCES,0.35924932975871315,"[32] Andrey Malinin, Neil Band, Yarin Gal, Mark Gales, Alexander Ganshin, German Chesnokov,
Alexey Noskov, Andrey Ploskonosov, Liudmila Prokhorenkova, Ivan Provilkov, et al. Shifts: A
dataset of real distributional shift across multiple large-scale tasks. In Thirty-fifth Conference
on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021."
REFERENCES,0.3605898123324397,"[33] Andrey Malinin and Mark Gales. Uncertainty estimation in autoregressive structured prediction.
In International Conference on Learning Representations, 2020."
REFERENCES,0.36193029490616624,"[34] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Großberger. Umap: Uniform manifold
approximation and projection. Journal of Open Source Software, 3(29):861, 2018."
REFERENCES,0.3632707774798928,"[35] Charles E Metz. Basic principles of roc analysis. In Seminars in nuclear medicine, volume 8,
pages 283–298. Elsevier, 1978."
REFERENCES,0.3646112600536193,"[36] Leann Myers and Maria J Sirois. Spearman correlation coefficients, differences between.
Encyclopedia of statistical sciences, 12, 2004."
REFERENCES,0.36595174262734587,"[37] Inderjeet Nair and Lu Wang. Midgard: Self-consistency using minimum description length for
structured commonsense reasoning, 2024."
REFERENCES,0.3672922252010724,"[38] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua
Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty?
evaluating predictive uncertainty under dataset shift. Advances in neural information processing
systems, 32, 2019."
REFERENCES,0.36863270777479895,"[39] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve
simple math word problems? In Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies,
pages 2080–2094, 2021."
REFERENCES,0.3699731903485255,"[40] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
understanding by generative pre-training. 2018."
REFERENCES,0.37131367292225204,"[41] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.3726541554959786,"[42] Jie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji
Lakshminarayanan. A simple fix to mahalanobis distance for improving near-ood detection.
arXiv preprint arXiv:2106.09022, 2021."
REFERENCES,0.3739946380697051,"[43] Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan,
and Peter J Liu. Out-of-distribution detection and selective generation for conditional language
models. In The Eleventh International Conference on Learning Representations, 2023."
REFERENCES,0.3753351206434316,"[44] Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of the
Conference on Empirical Methods in Natural Language Processing, pages 1743–1752, 2015."
REFERENCES,0.37667560321715815,"[45] Pranab Kumar Sen. Estimates of the regression coefficient based on kendall’s tau. Journal of
the American statistical association, 63(324):1379–1389, 1968."
REFERENCES,0.3780160857908847,"[46] Rupesh K Srivastava, Klaus Greff, and Jürgen Schmidhuber. Training very deep networks.
Advances in neural information processing systems, 28, 2015."
REFERENCES,0.37935656836461124,"[47] Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang,
Wenhan Lyu, Yixuan Zhang, Xiner Li, et al. Trustllm: Trustworthiness in large language models.
arXiv preprint arXiv:2401.05561, 2024."
REFERENCES,0.3806970509383378,"[48] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep
nearest neighbors. In International Conference on Machine Learning, pages 20827–20840.
PMLR, 2022."
REFERENCES,0.3820375335120643,"[49] Jörg Tiedemann. Parallel data, tools and interfaces in opus. In Proceedings of the Eighth
International Conference on Language Resources and Evaluation, pages 2214–2218, 2012."
REFERENCES,0.38337801608579086,"[50] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open
foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023."
REFERENCES,0.3847184986595174,"[51] Yiming Wang, Pei Zhang, Baosong Yang, Derek F Wong, and Rui Wang. Latent space chain-of-
embedding enables output-free llm self-evaluation. arXiv preprint arXiv:2410.13640, 2024."
REFERENCES,0.38605898123324395,"[52] Yiming Wang, Zhuosheng Zhang, and Rui Wang. Element-aware summarization with large
language models: Expert-aligned evaluation and chain-of-thought method. In Proceedings of
the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 8640–8665, 2023."
REFERENCES,0.3873994638069705,"[53] Yiming Wang, Zhuosheng Zhang, Pei Zhang, Baosong Yang, and Rui Wang. Meta-reasoning:
Semantics-symbol deconstruction for large language models. In Findings of the Association for
Computational Linguistics ACL 2024, pages 622–643, 2024."
REFERENCES,0.38873994638069703,"[54] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,
Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.
Advances in neural information processing systems, 35:24824–24837, 2022."
REFERENCES,0.3900804289544236,"[55] Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient pseudo-
independent weight perturbations on mini-batches. In International Conference on Learning
Representations, 2018."
REFERENCES,0.3914209115281501,"[56] Tim Z Xiao, Aidan Gomez, and Yarin Gal. Wat zei je? detecting out-of-distribution translations
with variational transformers. 2020."
REFERENCES,0.39276139410187666,"[57] Ruijie Xu, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu. Benchmarking benchmark leakage in
large language models. arXiv preprint arXiv:2404.18824, 2024."
REFERENCES,0.3941018766756032,"[58] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution
detection: A survey. arXiv preprint arXiv:2110.11334, 2021."
REFERENCES,0.39544235924932974,"[59] Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao,
Pranav Raja, Dylan Slack, Qin Lyu, et al. A careful examination of large language model
performance on grade school arithmetic. arXiv preprint arXiv:2405.00332, 2024."
REFERENCES,0.3967828418230563,"[60] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore:
Evaluating text generation with bert. In International Conference on Learning Representations,
2019."
REFERENCES,0.39812332439678283,"[61] Zhuosheng Zhang, Yao Yao, Aston Zhang, Xiangru Tang, Xinbei Ma, Zhiwei He, Yiming Wang,
Mark Gerstein, Rui Wang, Gongshen Liu, et al. Igniting language intelligence: The hitchhiker’s
guide from chain-of-thought reasoning to language agents. arXiv preprint arXiv:2311.11797,
2023."
REFERENCES,0.39946380697050937,"[62] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting
in large language models. arXiv preprint arXiv:2210.03493, 2022."
REFERENCES,0.4008042895442359,Appendix
REFERENCES,0.40214477211796246,"A
Related Work"
REFERENCES,0.403485254691689,"Data-unavailable OOD Detection: Generic Methods.
In scenarios where OOD data is unavailable.
Detection methods are categorized into three main types: (1) Output-based methods assess confidence
using predicted probabilities. (2) Ensemble-based methods assess the uncertainty of a collection of
supporting models, classical techniques are Monte-Carlo dropout [46, 8] based on Bayesian inference
and deep ensemble [19, 31]; (3) Feature-based methods assess the Mahalanobis Distance between
OOD samples and the distribution of the ID data feature space, usually considering the input and
output spaces [22, 42, 48], occasionally extending to specific hidden layer spaces [1]. Besides, there
are some fragmented methods, such as gradient methods [23, 15] and autoencoder reconstruction
methods [7, 16]. Still, these methods suffer from optimization and computational complexity with
serious performance bottlenecks [58] and thus are not mainstream detection methods."
REFERENCES,0.40482573726541554,"OOD Detection in GLMs.
Relatively few studies have explored OOD detection in GLMs, mainly
in semantic parsing [31, 25], speech recognition [33], machine translation [56, 32], summarization
[43], and they do not jump out of the frameworks of uncertainty estimation, ensemble-based methods,
and embedding-based methods. To our knowledge, we are the first to study OOD detection on
mathematical reasoning, and we have found the failure of traditional algorithms in this scenario.
Mathematical reasoning is an important and difficult research topic in the era of LLMs, and this
research is valuable for the scenario expansion of OOD detection algorithms on language models."
REFERENCES,0.4061662198391421,"B
Motivation Visualization Details of Figure 1"
REFERENCES,0.4075067024128686,"B.1
Projection Setting"
REFERENCES,0.40884718498659517,"We use UMAP [34] for projection visualization, which is a nonlinear dimensionality reduction
technique for mapping high-dimensional data into low-dimensional spaces. It uses optimization of
the local and global structure of the original data to produce a high-quality mapping that can preserve
the original data’s local and global characteristics."
REFERENCES,0.4101876675603217,"The UMAP algorithm consists of two steps: (i) Calculate the local similarity between each data point
and its immediate neighbors to construct a local similarity map. (ii) Map the high-dimensional data
to a low-dimensional space by optimizing an objective function that maintains the local and global
structure. The hyperparameter “n_neighbors” in the first step is key, we set it as 10 in our paper."
REFERENCES,0.41152815013404825,"B.2
More Examples"
REFERENCES,0.4128686327077748,"We select the MATH [6] dataset for mathematical reasoning and the OPUS [49] for text generation
(translation). In the MATH, we choose four domains: algebra, geometry, number theory, and
precalculus; In the OPUS, we also choose four domains: Bible, news, TED, and health. We present
examples of inputs and outputs under different domains for mathematical reasoning and translation
scenarios in Table 7 and Table 8, respectively. They correspond to the case projection of Figure 1.
Obviously, under mathematical reasoning, the outputs under different domains may appear exactly
the same, while it is impossible under translation."
REFERENCES,0.41420911528150134,"C
Theoretical Analysis of Section 2.1"
REFERENCES,0.4155495978552279,"In this section, we theoretically analyze Why pattern collapse in the output space leads to a greater
likelihood of volatility differences in trajectories under different samples, which corresponds to
Hypothesis 1 in the main paper."
REFERENCES,0.4168900804289544,"C.1
Problem Setup"
REFERENCES,0.41823056300268097,"Latent Space Embedding.
For a given sample si, [yi]l ∈Rd(1 ≤l ≤L) denote the hidden
embedding of the l-th layer. We define the embedding coordinate in the latent space as {(l, [yi]l)}L
l=1."
REFERENCES,0.4195710455764075,Table 7: Examples of inputs/outputs from different domains in the mathematical reasoning scenario.
REFERENCES,0.42091152815013405,"Input
Output"
REFERENCES,0.4222520107238606,Domain 1: Algebra
REFERENCES,0.42359249329758714,"Suppose that f is a function and f −1 is the inverse of f. If f(1) = 2, f(2) = 6, and
f(3) = 5, then what is f −1(f −1(6))?
1"
REFERENCES,0.4249329758713137,Domain 2: Geometry
REFERENCES,0.4262734584450402,"Two chords, AB and CD, meet inside a circle at P. If AP = CP = 7, then what is BP"
REFERENCES,0.42761394101876676,"DP ?
1"
REFERENCES,0.4289544235924933,Domain 3: Number Theory
REFERENCES,0.43029490616621985,"The product of the proper positive integer factors of n can be written as n(ax+b)/c, where
x is the number of positive divisors n has, c is a positive integer, and the greatest common
factor of the three integers a, b, and c is 1. What is a + b + c? 1"
REFERENCES,0.4316353887399464,Domain 4: Precalculus
REFERENCES,0.43297587131367293,"Simplify
1
1 −tan2 x +
1
1 −cot2 x. 1"
REFERENCES,0.4343163538873995,Table 8: Examples of inputs/outputs from different domains in the translation scenario.
REFERENCES,0.435656836461126,"Input
Output"
REFERENCES,0.43699731903485256,Domain 1: Bible
REFERENCES,0.4383378016085791,"The earth yielded grass, herbs yielding seed after their
kind, and trees bearing fruit, with its seed in it, after
their kind; and God saw that it was good."
REFERENCES,0.43967828418230565,"于是地发生了青草，和结种子的菜蔬，各从其
类；并结果子的树木，各从其类；果子都包着
核。神看着是好的。"
REFERENCES,0.4410187667560322,Domain 2: News
REFERENCES,0.44235924932975873,"Supporters say the tunnels would benefit the
environment and offer Californians a more secure
water supply."
REFERENCES,0.4436997319034853,"支持者们表示，这两条隧道将使环境受益并帮助
加利福尼亚州确保供水更加安全。"
REFERENCES,0.4450402144772118,Domain 3: TED
REFERENCES,0.44638069705093836,"(Applause) June Cohen: Frank, that was beautiful, so
touching.
（鼓掌）主持：Frank 刚刚真是太美丽、太感人
啦！"
REFERENCES,0.4477211796246649,Domain 4: Health
REFERENCES,0.44906166219839144,"The White House Coronavirus Task Force was
established on January 29.
白宫冠状病毒工作组（White House Coronavirus
Task Force）是成立于2020年1月29日的以应对美
国COVID-19疫情的工作组。"
REFERENCES,0.450402144772118,"Embedding Interpolation.
We assume the Fi(x) = [Fi1(x), Fi2(x), ..., Fid(x)]⊤: R →Rd with
d independent components fits the L coordinates, representing a continuous learning trajectory for
si, so [yi]l = Fi(l). Fi(x) is taken from the functional space X, so {Fi(l)}L
l=1 are all independent
variables. We constrain each component function of this Fi(x) to satisfy the m-order (m ≥3)
derivability property. Under this setting, the definition of dimension-independent trajectory volatility
(Eq. 2) equates to"
REFERENCES,0.4517426273458445,"V (si) = [V1(si), V2(si), ..., VL(si)]⊤ = 1 L L−1
X"
REFERENCES,0.45308310991957107,"l=1
[|Fi1(l) −Fi1(l −1)|, |Fi2(l) −Fi2(l −1)|, ..., |Fid(l) −Fid(l −1)|]⊤
(9)"
REFERENCES,0.4544235924932976,"Modeling.
We observe the “pattern collapse” phenomenon in the output space in Figure 1. We
abstract this phenomenon in Figure 2, which compares the trajectory trend between different samples
in the mathematical reasoning and text generation scenarios."
REFERENCES,0.45576407506702415,"We specify that [yi]L = Fi(L) ∼N(c, Σ2), where"
REFERENCES,0.4571045576407507,"c = [c1, c2, ..., cd]⊤,
Σ = diag(δ1, δ2, · · · , δd).
(10)"
REFERENCES,0.4584450402144772,"According to the pattern collapse property under different tasks, we can constraint the endpoint
embedding Fi(L) in the output space:"
REFERENCES,0.4597855227882037,"• For mathematical reasoning with pattern collapse, Σ →O, so we approximate that Fi(L) ≡c;
• For text generation without pattern collapse, Σ ̸= O, so Fi(L) ̸≡c."
REFERENCES,0.46112600536193027,"With such constraints, we model the main theorem:"
REFERENCES,0.4624664879356568,"Theorem C.1 (Main Theorem) For different samples si and sj, the likelihood of variations in
trajectory volatility under mathematical reasoning scenarios is higher than that under text generation
scenarios, which means:"
REFERENCES,0.46380697050938335,"E{Fi(l)}L
l=1, {Fj(l)}L
l=1 ∼U(Rd) {V (si) −V (sj) ̸= 0|Fi(L), Fj(L) ≡c}"
REFERENCES,0.4651474530831099,"> E{Fi(l)}L
l=1, {Fj(l)}L
l=1 ∼U(Rd)

V (si) −V (sj) ̸= 0|Fi(L), Fj(L) ∼N(c, Σ2)
	
,"
REFERENCES,0.46648793565683644,where U(Rd) denotes the uniform distribution defined in d-dimensional real number space.
REFERENCES,0.467828418230563,"C.2
Prelinimary"
REFERENCES,0.4691689008042895,"Next, we move on to the formal proofs. We begin with some propositions and lemmas that will be
useful in the main theorem."
REFERENCES,0.47050938337801607,Proposition C.1 (Lagrange Remainder Term) In the Taylor Expansion expression
REFERENCES,0.4718498659517426,f(x) = f(a) + df
REFERENCES,0.47319034852546915,dx(x −a) + 1
REFERENCES,0.4745308310991957,2! · d2f
REFERENCES,0.47587131367292224,dx2 (x −a)2 + · · · + 1
REFERENCES,0.4772117962466488,n! · dnf
REFERENCES,0.4785522788203753,"dxn (x −a)n + Rn+1(x),"
REFERENCES,0.47989276139410186,the remainder Rn+1(x) has the following property:
REFERENCES,0.4812332439678284,"|Rn+1(x)| =

1
(n + 1)! · dn+1f"
REFERENCES,0.48257372654155495,"dcn+1 · (x −a)n+1
 ≤
M
(n + 1)! ·
(x −a)n+1 ,"
REFERENCES,0.4839142091152815,"where c ∈[a, x] or c ∈[x, a], and M = sup
n dn+1f"
REFERENCES,0.48525469168900803,"dξn+1
 : ξ ∈[a, x] or ξ ∈[x, a]
o
> 0."
REFERENCES,0.4865951742627346,"Proof.
We consider the case of a < x with a > x identically. We use the Fundamental Theorem of
Calculus (FTC) for the most basic expansion of f(x):"
REFERENCES,0.4879356568364611,"f(x) = f(a) +
Z x a"
REFERENCES,0.48927613941018766,"df
dx1
dx1.
(11)"
REFERENCES,0.4906166219839142,Continue to use the FTC to expand the derivatives in integrals:
REFERENCES,0.49195710455764075,"f(x) = f(a) +
Z x a"
REFERENCES,0.4932975871313673,"df
dx1
dx1 = f(a) +
Z x a df"
REFERENCES,0.49463806970509383,"da +
Z x1 a"
REFERENCES,0.4959785522788204,"d2f
dx2
2
dx2 
dx1"
REFERENCES,0.4973190348525469,= f(a) + df
REFERENCES,0.49865951742627346,"dx(x −a) +
Z x a Z x1 a"
REFERENCES,0.5,"d2f
dx2
2
dx2 dx1"
REFERENCES,0.5013404825737265,"= · · · = n
X k=0"
REFERENCES,0.5026809651474531,"1
k! · dkf"
REFERENCES,0.5040214477211796,"dak · (x −a)k +
Z x a Z x1 a Z x2"
REFERENCES,0.5053619302949062,"a
· · ·
Z xn a"
REFERENCES,0.5067024128686327,"dn+1f
dxn+1
n+1
dxn+1 dxn · · · dx1. (12)"
REFERENCES,0.5080428954423593,"Therefore, the generalized remainder is known as"
REFERENCES,0.5093833780160858,"Rn+1(x) =
Z x a Z x1 a Z x2"
REFERENCES,0.5107238605898123,"a
· · ·
Z xn a"
REFERENCES,0.5120643431635389,"dn+1f
dxn+1
n+1
dxn+1 dxn · · · dx1 =
Z x a"
REFERENCES,0.5134048257372654,"1
n! · dn+1f"
REFERENCES,0.514745308310992,dtn+1 · (x −t)n dt. (13)
REFERENCES,0.5160857908847185,"We let mn+1 = mint∈[a,x]
dn+1f"
REFERENCES,0.517426273458445,"dtn+1 and Mn+1 = maxt∈[a,x]
dn+1f"
REFERENCES,0.5187667560321716,"dtn+1 , so mn+1 Z x"
REFERENCES,0.5201072386058981,"a
(x −t)n dt ≤
Z x a dn+1f"
REFERENCES,0.5214477211796247,dtn+1 · (x −t)n dt ≤Mn+1 Z x
REFERENCES,0.5227882037533512,"a
(x −t)n dt"
REFERENCES,0.5241286863270778,=⇒mn+1 ≤
REFERENCES,0.5254691689008043,"R x
a
dn+1f"
REFERENCES,0.5268096514745308,dtn+1 · (x −t)n
REFERENCES,0.5281501340482574,(x−a)n+1
REFERENCES,0.5294906166219839,"n+1
≤Mn+1.
(14)"
REFERENCES,0.5308310991957105,"According to the Lagrange’s Mean Value Theorem, there must exist a number c ∈[a, x] with dn+1f"
REFERENCES,0.532171581769437,dcn+1 =
REFERENCES,0.5335120643431636,"R x
a
dn+1f"
REFERENCES,0.5348525469168901,dtn+1 · (x −t)n
REFERENCES,0.5361930294906166,(x−a)n+1
REFERENCES,0.5375335120643432,"n+1
,
(15)"
REFERENCES,0.5388739946380697,"this gives us
Z x a dn+1f"
REFERENCES,0.5402144772117963,dtn+1 · (x −t)n = dn+1f
REFERENCES,0.5415549597855228,dcn+1 · (x −a)n+1 n + 1
REFERENCES,0.5428954423592494,"=⇒Rn+1(x) =
1
(n + 1)! · dn+1f"
REFERENCES,0.5442359249329759,"dcn+1 · (x −a)n+1,
(16)"
REFERENCES,0.5455764075067024,"and the equation on the left side of the proposition is proved completely. The inequality on the
right-hand side is clearly established by the Lagrange’s Mean Value Theorem.
□"
REFERENCES,0.546916890080429,Lemma C.1 (Error Bound for the Midpoint Rule) Suppose that f(x) is a m-th (m ≥2) order
REFERENCES,0.5482573726541555,"differentiable function on the interval (−∞, +∞), and K = sup
n d2f"
REFERENCES,0.5495978552278821,"dx2
 : x ∈[a, b]
o
∈R, then  Z b"
REFERENCES,0.5509383378016086,"a
f(x) dx −(b −a)f
a + b 2"
REFERENCES,0.5522788203753352, ≤K
REFERENCES,0.5536193029490617,24(b −a)3
REFERENCES,0.5549597855227882,"Proof.
We do the first order Taylor Expansion for f(x) at the midpoint x = a+b"
OF THE INTERVAL,0.5563002680965148,"2
of the interval
[a, b]: Z b"
OF THE INTERVAL,0.5576407506702413,"a
f(x) dx −(b −a)f
a + b 2"
OF THE INTERVAL,0.5589812332439679, =  Z b a
OF THE INTERVAL,0.5603217158176944,"
f(x) −f
a + b 2 
dx  =  Z b a"
OF THE INTERVAL,0.561662198391421,"""
df
d
  a+b"
OF THE INTERVAL,0.5630026809651475,"2
 ·

x −a + b 2"
OF THE INTERVAL,0.564343163538874,"
+ R1(x) # dx  ="
OF THE INTERVAL,0.5656836461126006,"0 +
Z b"
OF THE INTERVAL,0.5670241286863271,"a
R1(x) dx =  Z b"
OF THE INTERVAL,0.5683646112600537,"a
R1(x) dx  (17)"
OF THE INTERVAL,0.5697050938337802,"Following Theorem C.1, we do the inequality scaling as below: Z b"
OF THE INTERVAL,0.5710455764075067,"a
R1(x) dx ≤  Z b a K 2!"
OF THE INTERVAL,0.5723860589812333,"
x −a + b 2 2
dx = K 6"
OF THE INTERVAL,0.5737265415549598,"""b −a 2"
OF THE INTERVAL,0.5750670241286864,"3
−
a −b 2 3# = K"
OF THE INTERVAL,0.5764075067024129,24(b −a)3
OF THE INTERVAL,0.5777479892761395,"(18)
□"
OF THE INTERVAL,0.579088471849866,"Lemma C.2 (Differential-Integral Error Order Estimation) Suppose that f(x) is a m-th
(m
≥
3)
order
differentiable
function
on
the
interval
(−∞, +∞)
and
Ki
="
OF THE INTERVAL,0.5804289544235925,"sup
n dif"
OF THE INTERVAL,0.5817694369973191,"dxi
 : x ∈(−∞, +∞)
o
< +∞(i = 1, 2, · · · , m), then on the closed interval [a, b], we
have A := ( L
X i=1"
OF THE INTERVAL,0.5831099195710456,"f

a + b −a"
OF THE INTERVAL,0.5844504021447721,"L
i

−f

a + b −a"
OF THE INTERVAL,0.5857908847184986,"L
(i −1)
 −
Z b a df
dx dx  )"
OF THE INTERVAL,0.5871313672922251,≤R(L) ∼o( 1 L).
OF THE INTERVAL,0.5884718498659517,"Proof.
For each sub-interval
 
a + b−a"
OF THE INTERVAL,0.5898123324396782,"L (i −1), a + b−a"
OF THE INTERVAL,0.5911528150134048,"L i

, we perform a Taylor Expansion at the
midpoint x = a + b−a"
OF THE INTERVAL,0.5924932975871313,L (i −1
OF THE INTERVAL,0.5938337801608579,"2) to obtain the following expression: L
X i=1"
OF THE INTERVAL,0.5951742627345844,f(a + b −a
OF THE INTERVAL,0.596514745308311,"L
i) −f(a + b −a"
OF THE INTERVAL,0.5978552278820375,"L
(i −1)) = L
X i=1  """
OF THE INTERVAL,0.599195710455764,"f

a + b −a"
OF THE INTERVAL,0.6005361930294906,"L
(i −1"
OF THE INTERVAL,0.6018766756032171,"2)

+
df
d
 
a + b−a"
OF THE INTERVAL,0.6032171581769437,2L (i −1
OF THE INTERVAL,0.6045576407506702,"2)
 · b −a L
+ O"
OF THE INTERVAL,0.6058981233243967,"""b −a"
L,0.6072386058981233,"2L 2## − """
L,0.6085790884718498,"f

a + b −a"
L,0.6099195710455764,"L
(i −1"
L,0.6112600536193029,"2)

+
df
d
 
a + b−a"
L,0.6126005361930295,2L (i −1
L,0.613941018766756,"2)
 · a −b L
+ O"
L,0.6152815013404825,"""b −a"
L,0.6166219839142091,"2L 2## = L
X i=1"
L,0.6179624664879356,"df
d
 
a + b−a"
L,0.6193029490616622,L (i −1
L,0.6206434316353887,"2)
 · b −a"
L,0.6219839142091153,"L
+ O( 1 L2 )  ≤b −a L
· L
X i=1"
L,0.6233243967828418,"df
d
 
a + b−a"
L,0.6246648793565683,"L (i −1 2)
 + L
X"
L,0.6260053619302949,"i=1
O( 1 L2 ) =b −a L
· L
X i=1"
L,0.6273458445040214,"df
d
 
a + b−a"
L,0.628686327077748,"L (i −1 2)
"
L,0.6300268096514745,+ O( 1 L) (19)
L,0.631367292225201,"Following Lemma C.1, we do the inequality scaling for the first-order derivative summation: b −a L
· L
X i=1"
L,0.6327077747989276,"df
d
 
a + b−a"
L,0.6340482573726541,"L (i −1 2)
 ≤ L
X i=1"
L,0.6353887399463807,Z a+ b−a L i
L,0.6367292225201072,a+ b−a
L,0.6380697050938338,L (i−1)
L,0.6394101876675603,"""
df
dx + K3 24 b −a L 3# dx =
Z b a"
L,0.6407506702412868,"
df
dx"
L,0.6420911528150134,+ K3(b −a)3 24L2
L,0.6434316353887399,"
dx =
Z b a df
dx"
L,0.6447721179624665,dx + K3(b −a)4 24L2 (20)
L,0.646112600536193,A :≤K3(b −a)4
L,0.6474530831099196,"24L2
+ O( 1"
L,0.6487935656836461,L) = R(L) ∼o( 1
L,0.6501340482573726,"L)
(21) □"
L,0.6514745308310992,"C.3
Main Theorem Proof"
L,0.6528150134048257,"Finally, we formally prove our main theorem C.1 in this sub-section, thereby verifying the Hypotheses
1 in the main paper. Since the d components of V (si) −V (sj) are independent of each other, we
only consider the d-th dimensional component and the rest of the components Vd(si) −Vd(sj) can
be proved in the same way."
L,0.6541554959785523,"Proof.
According to Lemma C.2, we can approximate Vd(si) −Vd(sj) as"
L,0.6554959785522788,"Vd(si) −Vd(sj) = L
X"
L,0.6568364611260054,"l=1
|Fid(l) −Fid(l −1)| − L
X"
L,0.6581769436997319,"l=1
|Fjd(l) −Fjd(l −1)| =
Z L 1 dFid dx"
L,0.6595174262734584,"dx −
Z L 1 dFjd dx"
L,0.660857908847185,"dx + O( 1 L) ≈
Z L 1 dFid dx"
L,0.6621983914209115,"dx −
Z L 1 dFjd dx dx (22)"
L,0.6635388739946381,"Now we want to remove the absolute value of the integrated function. Due to the continuity of
the first-order derivative, there must be several zero points that are not extreme points, dividing
the domain of function definition into several open intervals with alternating constant positive and
negative function values. We first define such a set of zeros on the domain of definition D = [1, L]:"
L,0.6648793565683646,"Xi =

x | x ∈D; dFid"
L,0.6662198391420912,"dx
= 0

= {i1, i2, · · · , ip},"
L,0.6675603217158177,"Xj =

x | x ∈D; dFjd"
L,0.6689008042895442,"dx
= 0

= {j1, j2, · · · , jq}.
(23)"
L,0.6702412868632708,"For all zero points it ∈Xi or jt ∈Xj, they must satisfy the following properties to ensure that they
are not extreme points:"
L,0.6715817694369973,"∃ϵ > 0, ∀∆x < ϵ,
dFid
d(it −∆x) ·
dFid
d(it + ∆x) < 0,
(24)"
L,0.6729222520107239,"and jt is the same as it.
Therefore, in the sub-interval (1, i1), (i1, i2), · · · , (ip, L), Fid(x) is
alternately constant positive and constant negative, and the same as Fjd(x)."
L,0.6742627345844504,"We assume that the first interval (1, i1) and (1, j1) are constant positive intervals (the same can be
proven for constant negative intervals). In this setting, we can continue to simplify Eq.22:
Z L 1 dFid dx"
L,0.675603217158177,"dx −
Z L 1 dFjd dx dx = Z i1 1 dFid dx"
L,0.6769436997319035,"dx +
Z i2 i1 dFid dx"
L,0.67828418230563,"dx + · · · +
Z L ip dFid dx dx ! − Z j1 1 dFjd dx"
L,0.6796246648793566,"dx +
Z j2 j1 dFjd dx"
L,0.6809651474530831,"dx + · · · +
Z L jq dFjd dx dx !"
L,0.6823056300268097,"=
h
Fid(x)|i1
1 + (−1) · Fid(x)|i2
i1 + · · · + (−1)ip · Fid(x)|L
ip i"
L,0.6836461126005362,"−
h
Fjd(x)|j1
1 + (−1) · Fjd(x)|j2
j1 + · · · + (−1)jq · Fjd(x)|L
jq i"
L,0.6849865951742627,"= [−Fid(1) + Fjd(1)] + "" 2 p
X"
L,0.6863270777479893,"k=1
(−1)k−1 · Fid(ik) −2 q
X"
L,0.6876675603217158,"k=1
(−1)k−1 · Fjd(jk) #"
L,0.6890080428954424,"+

(−1)ip−1 · Fid(L) −(−1)jq−1 · Fjd(L)
 (25)"
L,0.6903485254691689,"Since Fid(x) and Fjd(x) are taken from the functional space, {Fid(1), {Fid(ik)}p
k=1} and
{Fjd(1), {Fjd(jk)}q
k=1} can be seen as independent variables. We let"
L,0.6916890080428955,"c =

(−1)ip−1 · Fid(L) −(−1)jq−1 · Fjd(L)

,
(26)"
L,0.693029490616622,"then we can rewrite Eq.25 to the matrix form:
Z L 1 dFid dx"
L,0.6943699731903485,"dx −
Z L 1 dFjd dx"
L,0.6957104557640751,"dx = Ax + c,
(27)"
L,0.6970509383378016,"where A is the coefficient matrix, and x is the unknown variable vector:"
L,0.6983914209115282,"A = [−1, 1, 2, −2, 2, −2, ..., (−1)p−1 · 2
|
{z
}
p dimensions"
L,0.6997319034852547,", −2, 2, −2, 2, ..., (−1)q · 2
|
{z
}
q dimensions"
L,0.7010723860589813,] ∈R1×(p+q+2)
L,0.7024128686327078,"x = [Fid(1), Fjd(1), Fid(i1), ..., Fid(ip), Fjd(j1), ..., Fjd(jq)]⊤∈R(p+q+2)×1
(28)"
L,0.7037533512064343,"We let ip ≡jq(mod 2), and return to the condition in the mathematical expectation on both sides of
the inequality in the main theorem C.1 for the following categorical discussion:"
L,0.7050938337801609,"• If Fid(L) = Fjd(L) = cd, then c = 0, so Vd(si) −Vd(sj) = 0 =⇒Ax = 0;"
L,0.7064343163538874,"• If Fid(L), Fjd(L) ∼N(cd, δ2
d), then c ̸≡0, so Vd(si) −Vd(sj) ̸= 0 =⇒Ax = −c."
L,0.707774798927614,"We denote N(A) as the solution space of {x|Ax = 0} and P(A) as the solution space of {x|Ax =
−c}. For x ∈P(A), x = xp or x = xp + xn where xp is the special solution and xn is the solution
in zero space, i.e., xn ∈N(A). Since rank(A) = 1 < p + q + 2, the special solution xp exists, so
the size of the solution space P(A) is larger than N(A). In this case, when the variable x is sampled
from the real number space, its probability of being in N(A) is smaller, namely its probability of not
being in N(A) is larger. This is exactly equivalent to the form of the proof of Main Theorem C.1, so
the proof is complete.
□"
L,0.7091152815013405,"C.4
Extended Conclusion"
L,0.710455764075067,"During the proof of the main theorem, we can unlock a hidden conclusion due to the embedding
interpolation: GLMs with a larger number of hidden layers may achieve more stable detection
performance, i.e., discrepancies in embedding volatility of different samples will be more obvious."
L,0.7117962466487936,"In Lemma C.2, we have proved the upper bound of differential-integral error order estimation is the
equivalent infinitesimals of 1/L. Actually, when L →+∞, we have"
L,0.7131367292225201,"lim
L→+∞ L
X i=1"
L,0.7144772117962467,"f

a + b −a"
L,0.7158176943699732,"L
i

−f

a + b −a"
L,0.7171581769436998,"L
(i −1)
 =
Z b a df
dx"
L,0.7184986595174263,"dx,
(29)"
L,0.7198391420911529,"which is clear according to the definition of Riemann Sum. This means that when the value of L
increases, the differential-integral approximation error will be reduced, so the conclusion of the main
theorem will be more accurate. In our experiments, we use GPT2-XL (48 layers) and Llama2-7B
(32 layers) as training backbones and find that the average performance of GPT2-XL is higher and
more stable than that of Llama2-7B, while the number of layers of GPT2-XL is 1.5 times that of
Llama2-7B. This further validates the correctness of our extended conclusions."
L,0.7211796246648794,"D
Algorithm: TV Score Computation"
L,0.7225201072386059,The pseudo-code of our TV Score computation pipeline is shown in Algorithm 1.
L,0.7238605898123325,Algorithm 1 Trajectory Volatility (TV) Score Computation
L,0.725201072386059,"Input: L: The number of hidden layers
N: The size of ID dataset
k: the smoothing differential order of TV score
{yl}1≤l≤L: the average hidden embedding of the OOD sample in each layer
{[ ˆyl]i}1≤l≤L,1≤i≤N: the average hidden embedding of all N ID samples in each layer
1: for l ←1 to L do
2:
Fitting ID samples {[ ˆyl]i}1≤l≤L,1≤i≤N to Gaussian distribution Gl = N(µl, Σl)"
L,0.7265415549597856,"3:
f(yl) ←(yl −µl)⊤(Σl)−1 (yl −µl)
4: end for
5: for i →1 to k do
6:
for l ←1 to L do
7:
∆(i)Gl = N(µ(i)
l , Σ(i)
l ) ←N(Pi
t=0(−1)i+tCt
iµl+i, Pi
t=0 Ct
iΣl+i)
8:
end for"
L,0.7278820375335121,"9:
f (i)(yl) ←
 
yl(i) −µl(i)⊤
Σ(i)
l
−1  
yl(i) −µl(i)"
L,0.7292225201072386,"10: end for
11: for i →1 to n do
12:
TVi ←average

f (i)(yl)
"
L,0.7305630026809652,"1≤l≤L
13: end for
Output: {TVi}1≤i≤k"
L,0.7319034852546917,"As for the computational complexity, once we have all yl, our TV score requires only scalar addition
and multiplication. During the ID distribution fitting phase, both operations are O(Ldn), where n is
the dataset size. During the score computation phase, both operations are O(Ldk). In practice, the
computation time is measured in milliseconds."
L,0.7332439678284183,"E
Experimental Setting Details"
L,0.7345844504021448,"E.1
Basic Information of Dataset"
L,0.7359249329758714,"For the ID dataset, we use the MultiArith [44], which consists of Math Word Problems on arithmetic
reasoning. For the OOD datasets, we intuitively introduce two types of detection scenarios following
[43]: (i) Far-shift OOD setting, we select the MATH [11] as the OOD data, which spans across"
L,0.7372654155495979,"distinct mathematical domains encompassing algebra, geometry, counting and probability, number
theory, and precalculus. It contains college difficulty level math problems, whereas MultiArith
has only elementary school difficulty, and thus can be considered as sourced from far-different
distributions; (ii) Near-shift OOD setting, we select five arithmetic reasoning datasets as the OOD
data: GSM8K [6], SVAMP [39], AddSub [13], SingleEq [18], and SingleOp [18], they all consist of
Math Word Problems like the MultiArith but require different reasoning hops and knowledge points
for solving the problems, and thus can be considered as sourced from near-different distributions. In
addition, we present the data sizes and examples of all ID and OOD datasets, as shown in Table 9."
L,0.7386058981233244,"E.2
ID Dataset Split"
L,0.739946380697051,"For the ID dataset MultiArith, we find that every 100 consecutive samples show a similar quadratic
operation pattern (e.g., samples with id 0-100 are a mixture of addition and subtraction, and id
100-200 are a mixture of addition and multiplication). Therefore, we divide it into 6 subsets (6*100)
in order. In each subset, we take the first 60 as training samples and the last 40 as test samples."
L,0.7412868632707775,"E.3
Training Implementation"
L,0.7426273458445041,"We train Llama2-7B [50] and GPT2-XL [5] models on the training split of MultiArith. Llama2-7B is
trained with AdamW optimizer [29] for 10K steps and 8 batch sizes in 4-card RTX 3090 (2 per card).
The learning rate is set to 1e-5, the warmup step to 10, and the maximum gradient normalization to 0.3.
GPT2-XL is trained for 3K steps and 128 batch sizes in a single RTX 3090, and other configurations
are the same as Llama2-7B."
L,0.7439678284182306,"E.4
OOD Dataset Selection Rationality"
L,0.7453083109919572,"In this part, we examine the rationality of the OOD data selection, ensuring that the OOD data
distribution significantly differs from the pre-trained data distribution and has not been fully learned
during the pre-training phase. Some research [57, 59] have confirmed the absence of data leakage in
Llama2 for MATH and GSM8K datasets, we still conduct experiments and analyses to ensure this."
L,0.7466487935656837,"For GPT2-XL, We can determine this from the time dimension. GPT-2 was released in 2019, but the
MATH and GSM8k datasets were released in 2021, so they are unlikely to appear in the pre-training
data. However, for Llama2-7B, due to the closed-source data, we cannot confirm which data the
model used in the pre-training phase, so we cannot fully determine whether the selected dataset was
OOD for the model from a data perspective."
L,0.7479892761394102,"Therefore, we argue that a dataset can be considered OOD when it is beyond the capability of the
base model as claimed by prior work [47, 27]. We test all ten datasets we select as the OOD data
in the pre-trained Llama2-7B and GPT2-XL model, and Table 10 shows the results. We find that
GPT2-XL cannot handle any of the ten mathematical reasoning tasks, and Llama2-7B performs with
very low accuracy. Therefore, from a capability standpoint, we can ensure that these datasets are
OOD for these two GLMs."
L,0.7493297587131368,"E.5
Baseline"
L,0.7506702412868632,"Let x represent the input sequence and y the output sequence, with lengths denoted as nx and ny
respectively. In addition, we assume that p(·; θ) represents the GLM parameterized by θ that has
been trained in ID dataset D, outputting a sequence of softmax probabilities. We compare some
training-free baseline methods as below:"
L,0.7520107238605898,• Maximum Softmax Probability [12]:
NY,0.7533512064343163,"1
ny
· ny
X"
NY,0.7546916890080428,"i=1
p(yi|y≺i, x; θ).
(30)"
NY,0.7560321715817694,"• Monte-Carlo Dropout [8]:
Z
p(y|x; θ)q(θ)dθ,
(31)"
NY,0.7573726541554959,where q(θ) ≈p(θ|D).
NY,0.7587131367292225,Table 9: ID and OOD datasets used in this paper.
NY,0.760053619302949,In-Distribution Dataset
NY,0.7613941018766756,MultiArith (Data Size: 600)
NY,0.7627345844504021,"Q: Kaleb was collecting cans for recycling. On Saturday he filled 5 bags up and on Sunday he filled 5 more
bags. If each bag had 4 cans in it, how many cans did he pick up total?"
NY,0.7640750670241286,A: 40.0
NY,0.7654155495978552,"Far Shift Out-of-Distribution Dataset
Near Shift Out-of-Distribution Dataset"
NY,0.7667560321715817,MATH-Algebra (Data Size: 1187)
NY,0.7680965147453083,"Q: How many real numbers are not in the domain of
the function"
NY,0.7694369973190348,"f(x) =
1
x −64 +
1
x2 −64 +
1
x3 −64 ? A: 4"
NY,0.7707774798927614,GSM8K (Data Size: 1318)
NY,0.7721179624664879,"Q: Judy teaches 5 dance classes, every day, on the
weekdays and 8 classes on Saturday. If each class has
15 students and she charges $15.00 per student, how
much money does she make in 1 week?"
NY,0.7734584450402144,A: 7425
NY,0.774798927613941,MATH-Geometry (Data Size: 479)
NY,0.7761394101876675,"Q: Suppose we are given seven points that are equally
spaced around a circle. If P, Q, and R are chosen to
be any three of these points, then how many different
possible values are there for m∠PQR? A: 5"
NY,0.7774798927613941,SVAMP (Data Size: 1000)
NY,0.7788203753351206,"Q: A mailman has to give 38 pieces of junk mail to
each of the 78 blocks. If there are 19 houses on a
block. How many pieces of junk mail should he give
each house?"
NY,0.7801608579088471,A: 2.0
NY,0.7815013404825737,MATH-Counting and Probability (Data Size: 474)
NY,0.7828418230563002,"Q: Amy’s grandmother gave her 3 identical chocolate
chip cookies and 4 identical sugar cookies. In how
many different orders can Amy eat the cookies such
that either she eats a chocolate chip cookie first, she
eats a chocolate chip cookie last, or both? A: 25"
NY,0.7841823056300268,AddSub (Data Size: 395)
NY,0.7855227882037533,"Q: While taking inventory at her pastry shop, Kelly
realizes that she had 0.4 box of baking powder
yesterday, but the supply is now down to 0.3 box. How
much more baking powder did Kelly have yesterday?"
NY,0.7868632707774799,A: 0.1
NY,0.7882037533512064,MATH-Number Theory (Data Size: 540)
NY,0.789544235924933,Q: Notice that
NY,0.7908847184986595,31 · 37 = 1147.
NY,0.792225201072386,Find some integer n with 0 ≤n < 2293 such that
NY,0.7935656836461126,"31n ≡3
(mod 2293)."
NY,0.7949061662198391,A: 222
NY,0.7962466487935657,SingleEq (Data Size: 508)
NY,0.7975871313672922,"Q: Fred had 7 dimes in his bank. His sister borrowed
3 of his dimes. How many dimes does Fred have now?"
NY,0.7989276139410187,A: 4.0
NY,0.8002680965147453,MATH-Precalculus (Data Size: 546)
NY,0.8016085790884718,Q: Let a = 
NY,0.8029490616621984,"
1
3
0 "
NY,0.8042895442359249,", b = "
NY,0.8056300268096515,"
−11
5
2 "
NY,0.806970509383378,", and c = "
NY,0.8083109919571045,"
1 +
√"
NY,0.8096514745308311,"5
4
−5 "
NY,0.8109919571045576,. Find k if the vectors a + b + c and
NY,0.8123324396782842,3(b × c) −8(c × a) + k(a × b)
NY,0.8136729222520107,are orthogonal. A: 5
NY,0.8150134048257373,SingleOp (Data Size: 562)
NY,0.8163538873994638,"Q: Pamela starts with 30 bottle caps. Jean takes 26
away. How many bottle caps does Pamela end with?"
NY,0.8176943699731903,A: 4.0
NY,0.8190348525469169,Table 10: Accuracies of all datasets we select as the OOD data in pre-trained GLMs.
NY,0.8203753351206434,"Far-shift OOD Setting
Near-shift OOD Setting"
NY,0.82171581769437,"Dataset
Llama2-7B
GPT2-XL
Dataset
Llama2-7B
GPT2-XL"
NY,0.8230563002680965,"Accuracy of pre-trained model
Accuracy of pre-trained model"
NY,0.824396782841823,"Algebra
6 / 1187
0 / 1187
GSM8K
0 / 1318
0 / 1318
Geometry
2 / 479
0 / 479
SVAMP
8 / 1000
0 / 1000
Cnt.&Prob
4 / 474
0 / 474
AddSub
13 / 395
0 / 1000
Num.Theory
0 / 540
0 / 540
SingleEq
5 / 395
0 / 508
Precalculus
1 / 546
0 / 540
SingleOp
7 / 508
0 / 508"
NY,0.8257372654155496,"• Sequence Perplexity [43]:
"" ny
Y"
NY,0.8270777479892761,"i=1
p(yi|y≺i, x; θ) #−1"
NY,0.8284182305630027,"ny
.
(32)"
NY,0.8297587131367292,"• Input Embedding [43]:
(x −µx)⊤Σ−1
x (x −µx),
(33)
where µx and Σx represent the mean and variance of the Gaussian distribution associated with the
first-layer hidden state.
• Output Embedding [43]:
(y −µy)⊤Σ−1
y (y −µy),
(34)
where µy and Σy represent the mean and variance of the Gaussian distribution associated with the
final-layer hidden state."
NY,0.8310991957104558,"F
Supplementary Experimental Results"
NY,0.8324396782841823,"F.1
Hyperparameter Ablation: Smoothing Order k"
NY,0.8337801608579088,"0
1
2
3
4
5
0 20 40 60 80 100 AUROC"
NY,0.8351206434316354,"Algebra
Geometry"
NY,0.8364611260053619,"Cnt.&Prob
Num.Theory"
NY,0.8378016085790885,Precalculus
NY,0.839142091152815,"0
1
2
3
4
5
0 20 40 60 80 100"
NY,0.8404825737265416,"GSM8K
SVAMP"
NY,0.8418230563002681,"AddSub
SingleEq"
NY,0.8431635388739946,SingleOp
NY,0.8445040214477212,"0
1
2
3
4
5
Smoothing Order k 0 20 40 60 80 100 FPR95"
NY,0.8458445040214477,"0
1
2
3
4
5
0 20 40 60 80 100"
NY,0.8471849865951743,"Figure 4: Smoothing order k analysis: k ranges from 0 −5 (k = 0 corresponds to the original
TV Score). The upper part is for the OOD detection scenario and the lower part is for the OOD
quality estimation scenario; the left part is for the far-shift OOD datasets and the right part is for the
near-shift OOD datasets."
NY,0.8485254691689008,"In the main experiments, we found that differential smoothing is not as effective as the basic TV
score, with excellent results occurring on only a few datasets. Figure 4 visualizes the results for the"
NY,0.8498659517426274,"OOD detection scenario with no smoothing (k = 0) and smoothing order k 1-5. We find that there
is a significant effect of differential smoothing in two cases: (1) very good performance without
smoothing, e.g., the precalculus dataset, where the AUROC results are almost close to 100%; and (2)
significantly poor performance without smoothing, e.g., the AddSub dataset, where the FPR95 results
on all other near-shift OOD datasets are 20 below, the results on this dataset are more than 80, when
differential smoothing helps to eliminate some of the noise features and helps better detection."
NY,0.8512064343163539,"In addition, for the case of k > 0, the peak performance basically occurs in the case of k = 1 or 2,
which indicates that when k is too large, the phenomenon of over-smoothing tends to occur. Too
much useful feature information is lost, leading to a decrease in detection accuracy."
NY,0.8525469168900804,"F.2
OOD Detection"
NY,0.853887399463807,Results of each dataset are shown in Table 11 (Llama2-7B) and Table 12 (GPT2-XL).
NY,0.8552278820375335,"Table 11: AUROC and FPR95 results in Llama2-7B of the Offline Detection scenario (p-value >
0.05 are grayed out). Underline and bold denote SOTA among all baselines and all methods."
NY,0.8565683646112601,Far-shift OOD Setting
NY,0.8579088471849866,"Method
Dataset
Algebra
Geometry
Counting and Probability
Number Theory
Precalculus
Average"
NY,0.8592493297587132,AUROC ↑/ FPR95 ↓
NY,0.8605898123324397,"Max Softmax Prob. [12]
79.97±1.60 / 80.97±4.90
82.60±1.25 / 82.57±4.62
63.87±1.64 / 96.19±1.57
76.20±1.46 / 85.20±2.69
90.68±0.94 / 62.27±4.03
78.66±1.38 / 81.44±3.56
Monte-Carlo Dropout [8]
72.12±1.68 / 85.43±6.60
74.23±1.74 / 81.17±6.28
55.64±2.87 / 97.15±1.34
67.53±2.08 / 89.09±5.41
73.61±2.66 / 82.36±4.78
68.63±2.21 / 87.04±4.88
Perplexity [3]
84.17±1.43 / 52.75±5.13
88.67±1.36 / 53.32±5.38
77.42±1.97 / 71.28±3.90
83.88±2.13 / 68.90±4.22
94.05±0.43 / 19.03±3.17
85.64±1.46 / 53.06±4.36
Input Embedding [43]
81.51±1.09 / 62.84±4.56
75.41±0.97 / 69.66±2.07
62.53±1.30 / 88.35±1.72
84.42±0.85 / 54.80±7.25
75.59±0.92 / 63.71±2.87
75.89±1.03 / 67.87±3.69
Output Embedding [43]
76.95±1.54 / 74.92±3.02
78.72±1.31 / 80.97±2.23
61.43±1.40 / 88.45±1.58
70.23±1.36 / 80.18±1.71
86.97±1.32 / 51.51±2.27
74.86±1.39 / 75.21±2.16"
NY,0.8619302949061662,"TV score (Ours)
98.87±0.16 / 4.67±1.41
99.03±0.09 / 3.70±0.42
97.70±0.15 / 8.83±1.33
98.43±0.13 / 7.37±1.46
99.78±0.02 / 1.47±0.20
98.76±0.11 / 5.21±0.98
w/ DiSmo (Ours)
94.71±0.93 / 39.65±7.28
94.08±0.80 / 50.52±6.14
83.08±1.28 / 80.07±1.57
94.57±0.75 / 37.74±7.58
99.82±0.02 / 1.11±0.19
93.25±0.76 / 41.82±4.69"
NY,0.8632707774798928,"∆(bold - underline)
+14.70 / -48.08
+10.36 / -49.62
+20.28 / -62.45
+14.01 / -47.43
+5.77 / -17.92
+13.12 / -47.85"
NY,0.8646112600536193,Near-shift OOD Setting
NY,0.8659517426273459,"Method
Dataset
GSM8K
SVAMP
AddSub
SingleEq
SingleOp
Average"
NY,0.8672922252010724,AUROC ↑/ FPR95 ↓
NY,0.868632707774799,"Max Softmax Prob. [12]
53.08±1.67 / 94.07±1.86
56.56±1.53 / 90.06±2.39
63.31±1.88 / 87.36±2.42
66.68±1.32 / 86.15±2.61
61.07±1.30 / 86.91±2.76
60.14±1.54 / 88.91±2.41
Monte-Carlo Dropout [8]
48.87±2.43 / 96.78±1.21
44.90±2.76 / 92.33±2.09
57.34±1.57 / 89.15±2.34
54.09±2.42 / 90.07±1.96
56.46±1.85 / 91.21±1.83
52.33±2.21 / 91.92±1.89
Perplexity [3]
52.24±2.57 / 95.56±1.21
55.12±1.80 / 89.24±2.09
62.88±1.76 / 80.96±2.34
67.14±1.34 / 81.38±1.96
59.39±1.98 / 83.30±1.83
59.35±1.89 / 86.09±1.89
Input Embedding [43]
45.68±1.50 / 95.05±1.17
60.92±1.34 / 86.97±4.13
66.28±0.92 / 76.09±2.93
61.18±1.22 / 87.36±1.89
67.60±1.20 / 77.78±2.53
60.33±1.37 / 84.65±2.53
Output Embedding [43]
35.39±1.24 / 91.22±1.27
36.77±1.14 / 90.41±1.61
63.08±0.92 / 77.12±3.11
43.70±1.02 / 86.69±0.91
43.55±0.99 / 86.87±1.04
44.50±1.06 / 86.46±1.59"
NY,0.8699731903485255,"TV score (Ours)
94.88±0.25 / 14.22±1.64
94.51±0.20 / 12.89±1.11
85.84±1.06 / 82.63±2.22
93.97±0.24 / 17.39±1.09
94.00±0.20 / 14.61±0.85
92.64±0.39 / 28.39±1.38
w/ DiSmo (Ours)
55.21±1.91 / 95.71±0.88
38.24±1.53 / 87.06±0.94
87.06±0.94 / 71.02±4.33
56.66±1.34 / 93.76±1.28
47.46±1.32 / 92.48±1.13
56.99±1.41 / 88.01±1.71"
NY,0.871313672922252,"∆(bold - underline)
+41.80 / -77.00
+33.59 / -74.08
+20.78 / -5.07
+26.83 / -63.99
+26.40 / -63.17
+32.31 / -56.26"
NY,0.8726541554959786,"Table 12: AUROC and FPR95 results in GPT2-XL of the Offline Detection scenario (p-value >
0.05 are grayed out). Underline and bold denote SOTA among all baselines and all methods."
NY,0.8739946380697051,Far-shift OOD Setting
NY,0.8753351206434317,"Method
Dataset
Algebra
Geometry
Counting and Probability
Number Theory
Precalculus
Average"
NY,0.8766756032171582,AUROC ↑/ FPR95 ↓
NY,0.8780160857908847,"Max Softmax Prob. [12]
72.13±1.42 / 78.35±2.35
64.09±1.78 / 87.42±1.24
68.45±1.56 / 82.35±1.57
69.37±1.43 / 82.09±1.90
78.67±0.89 / 61.23±3.02
70.54±1.42 / 78.29±2.02
Monte-Carlo Dropout [8]
62.41±2.10 / 88.02±1.79
66.63±1.82 / 82.49±1.12
59.32±2.03 / 92.34±1.54
63.88±1.92 / 86.71±1.95
70.19±1.47 / 73.90±1.87
66.18±1.87 / 84.69±1.65
Perplexity [3]
84.24±1.01 / 63.78±2.38
82.17±0.89 / 67.90±2.45
79.12±1.20 / 72.03±1.23
72.40±1.37 / 71.65±1.45
86.19±0.75 / 47.27±2.98
80.82±1.04 / 64.53±2.10
Input Embedding [43]
86.23±0.78 / 46.12±1.45
83.20±0.94 / 53.29±2.06
79.58±1.43 / 60.45±2.95
89.44±0.64 / 52.49±1.87
92.86±0.39 / 34.32±2.16
86.26±0.84 / 49.33±2.10
Output Embedding [43]
78.13±1.14 / 64.46±2.78
77.98±1.17 / 68.82±3.26
71.07±1.67 / 84.63±4.24
80.25±0.97 / 56.23±4.05
82.34±0.86 / 54.08±2.79
77.95±1.16 / 65.64±3.42"
NY,0.8793565683646113,"TV score (Ours)
98.35±0.07 / 6.24±0.24
97.27±0.11 / 9.23±0.35
85.52±0.12 / 52.51±1.78
92.96±0.06 / 29.86±1.26
93.24±0.04 / 22.68±1.14
93.47±0.08 / 24.10±0.95
w/ DiSmo (Ours)
93.68±0.19 / 23.24±1.45
94.39±0.15 / 12.17±0.79
95.83±0.13 / 9.86±0.46
99.17±0.04 / 2.42±0.22
99.62±0.02 / 1.75±0.13
96.54±0.11 / 9.89±0.61"
NY,0.8806970509383378,"∆(bold - underline)
+12.12 / -39.88
+14.07 / -44.06
+16.25 / -50.59
+9.27 / -50.07
+6.24 / -32.57
+10.28 / -39.44"
NY,0.8820375335120644,Near-shift OOD Setting
NY,0.8833780160857909,"Method
Dataset
GSM8K
SVAMP
AddSub
SingleEq
SingleOp
Average"
NY,0.8847184986595175,AUROC ↑/ FPR95 ↓
NY,0.886058981233244,"Max Softmax Prob. [12]
54.06±1.58 / 92.45±2.37
65.50±1.43 / 73.62±4.18
70.60±0.92 / 77.56±2.76
79.96±0.87 / 57.38±1.95
65.47±1.18 / 80.37±2.02
67.12±1.20 / 76.27±2.66
Monte-Carlo Dropout [8]
63.44±2.08 / 79.85±3.35
62.13±1.65 / 77.28±3.21
67.97±1.62 / 71.42±2.88
71.29±1.43 / 69.22±2.41
52.89±1.80 / 92.78±0.67
63.54±1.72 / 78.08±2.50
Perplexity [3]
72.89±1.56 / 74.63±1.52
70.79±1.38 / 79.65±1.21
67.50±1.04 / 87.76±0.92
87.14±0.57 / 43.09±1.23
70.40±1.06 / 76.80±1.45
73.74±1.12 / 72.39±1.27
Input Embedding [43]
87.58±1.14 / 48.45±2.06
84.81±1.20 / 62.75±5.68
80.34±0.64 / 46.62±2.17
81.44±0.49 / 49.05±3.24
81.91±0.93 / 57.65±2.67
83.22±0.88 / 52.90±3.16
Output Embedding [43]
82.05±1.36 / 60.44±3.96
81.42±2.02 / 68.43±4.38
72.98±0.85 / 66.72±1.77
80.00±1.02 / 70.23±1.87
79.94±0.94 / 57.68±1.60
79.28±1.24 / 64.70±2.72"
NY,0.8873994638069705,"TV score (Ours)
98.26±0.06 / 1.78±0.04
98.99±0.02 / 1.13±0.03
80.56±0.82 / 52.23±1.29
97.35±0.23 / 13.91±0.45
99.12±0.02 / 0.07±0.01
94.86±0.23 / 13.82±0.36
w/ DiSmo (Ours)
97.62±0.21 / 3.58±0.16
92.19±0.44 / 13.07±1.68
83.35±0.57 / 41.70±1.29
97.83±0.03 / 9.94±0.32
99.94±0.02 / 0.03±0.01
94.19±0.25 / 13.66±0.69"
NY,0.8887399463806971,"∆(bold - underline)
+10.68 / -46.67
+14.18 / -61.62
+3.01 / -4.92
+10.69 / -33.15
+18.03 / -57.62
+11.64 / -39.24"
NY,0.8900804289544236,"F.3
Beyond Detection: OOD Quality Estimation"
NY,0.8914209115281502,Results of each dataset are shown in Table 13 (Llama2-7B) and Table 14 (GPT2-XL).
NY,0.8927613941018767,"Table 13: OOD Quality Estimation (Llama2-7B): Kendall’s τ and Spearman correlation between
various OOD scores and benchmark quality metric binary matching. Each column shows the
correlation when ID and OOD samples are merged. Underline denotes the SOTA among all baselines,
and bold denotes the SOTA among our methods."
NY,0.8941018766756033,"Method
Dataset
ID + Far-shift OOD
ID + Near-shift OOD"
NY,0.8954423592493298,"Algebra
Geometry
Cnt.&Prob.
Num.Theory
Precalculus
Average
GSM8K
SVAMP
AddSub
SingleEq
SingleOp
Average"
NY,0.8967828418230563,Kendall Rank Correlation Coefficient
NY,0.8981233243967829,"Max Softmax Prob. [12]
0.035±0.025
0.024±0.020
0.084±0.022
-0.043±0.021
0.021±0.014
0.024±0.020
0.064±0.022
0.067±0.015
0.076±0.016
0.057±0.019
-0.075±0.017
0.038±0.018
Perplexity [3]
-0.027±0.020
0.071±0.013
0.112±0.015
0.035±0.016
0.036±0.011
0.050±0.015
0.073±0.019
0.027±0.017
0.082±0.015
0.091±0.019
0.079±0.017
0.074±0.017
Input Embedding [43]
0.074±0.020
0.119±0.015
0.054±0.018
0.111±0.016
0.034±0.013
0.078±0.016
0.042±0.020
-0.025±0.016
0.052±0.014
0.055±0.020
0.058±0.018
0.036±0.018
Output Embedding [43]
0.050±0.023
0.078±0.019
0.064±0.020
0.065±0.020
0.034±0.009
0.058±0.018
0.042±0.018
-0.001±0.013
0.056±0.015
0.032±0.015
0.061±0.015
0.038±0.015"
NY,0.8994638069705094,"TV score (Ours)
0.182±0.016
0.116±0.011
0.191±0.011
0.174±0.014
0.142±0.009
0.161±0.012
0.146±0.010
0.209±0.019
0.195±0.018
0.145±0.017
0.101±0.020
0.159±0.017
w/ DiSmo (Ours)
0.095±0.020
0.059±0.014
0.123±0.020
0.131±0.016
0.148±0.009
0.111±0.016
0.178±0.025
0.113±0.015
0.121±0.016
0.079±0.017
0.076±0.019
0.113±0.018"
NY,0.900804289544236,"∆(bold - underline)
+0.021
-0.003
+0.079
+0.109
+0.112
+0.083
+0.073
+0.142
+0.039
+0.055
+0.022
+0.085"
NY,0.9021447721179625,Spearman Rank Correlation Coefficient
NY,0.903485254691689,"Max Softmax Prob. [12]
0.027±0.021
0.057±0.019
0.039±0.021
0.067±0.021
0.004±0.017
0.038±0.020
0.071±0.021
-0.086±0.018
0.063±0.019
0.081±0.015
0.002±0.019
0.026±0.018
Perplexity [3]
0.039±0.014
0.018±0.018
0.073±0.017
0.051±0.015
0.045±0.015
0.045±0.016
0.086±0.019
0.090±0.019
0.016±0.019
0.039±0.015
0.017±0.016
0.050±0.018
Input Embedding [43]
0.150±0.021
0.136±0.018
0.061±0.015
0.038±0.016
0.127±0.016
0.102±0.017
0.042±0.022
0.128±0.016
0.154±0.016
0.110±0.017
0.140±0.015
0.115±0.017
Output Embedding [43]
0.002±0.018
0.001±0.020
0.015±0.015
0.033±0.016
0.012±0.018
0.025±0.017
-0.040±0.017
-0.019±0.016
0.045±0.019
0.101±0.018
-0.027±0.017
0.012±0.017"
NY,0.9048257372654156,"TV score (Ours)
0.122±0.012
0.169±0.015
0.102±0.016
0.126±0.016
0.216±0.014
0.147±0.015
0.124±0.016
0.188±0.017
0.127±0.017
0.165±0.017
0.188±0.019
0.158±0.017
w/ DiSmo (Ours)
0.139±0.014
0.148±0.013
0.176±0.017
0.178±0.017
0.121±0.016
0.152±0.015
0.071±0.018
0.150±0.020
0.167±0.014
0.131±0.017
0.153±0.017
0.134±0.017"
NY,0.9061662198391421,"∆(bold - underline)
-0.011
+0.033
+0.103
+0.127
+0.089
+0.050
+0.038
+0.060
+0.013
+0.055
+0.048
+0.043"
NY,0.9075067024128687,"Table 14: OOD Quality Estimation (GPT2-XL): Kendall’s τ and Spearman correlation between
various OOD scores and benchmark quality metric binary matching. Each column shows the
correlation when ID and OOD samples are merged. Underline denotes the SOTA among all baselines,
and bold denotes the SOTA among our methods."
NY,0.9088471849865952,"Method
Dataset
ID + Far-shift OOD
ID + Near-shift OOD"
NY,0.9101876675603218,"Algebra
Geometry
Cnt.&Prob.
Num.Theory
Precalculus
Average
GSM8K
SVAMP
AddSub
SingleEq
SingleOp
Average"
NY,0.9115281501340483,Kendall Rank Correlation Coefficient
NY,0.9128686327077749,"Max Softmax Prob. [12]
0.078±0.018
0.032±0.015
0.093±0.014
0.059±0.017
0.068±0.012
0.066±0.015
0.032±0.024
0.061±0.020
0.088±0.018
0.023±0.017
0.081±0.019
0.057±0.018
Perplexity [3]
0.023±0.014
0.086±0.015
0.043±0.016
-0.009±0.015
0.036±0.012
0.036±0.014
0.068±0.022
0.008±0.022
0.035±0.018
0.001±0.015
0.064±0.015
0.035±0.018
Input Embedding [43]
0.084±0.012
0.052±0.011
0.089±0.013
0.062±0.011
0.007±0.010
0.059±0.012
0.041±0.019
0.010±0.020
-0.008±0.016
0.019±0.015
0.000±0.018
0.012±0.018
Output Embedding [43]
0.089±0.012
0.023±0.012
0.058±0.013
0.014±0.012
0.067±0.011
0.050±0.012
0.068±0.018
0.036±0.020
0.033±0.018
0.022±0.014
0.021±0.017
0.036±0.017"
NY,0.9142091152815014,"TV score (Ours)
0.127±0.010
0.142±0.010
0.158±0.009
0.098±0.011
0.167±0.009
0.138±0.010
0.087±0.013
0.166±0.019
0.110±0.015
0.152±0.013
0.141±0.016
0.131±0.015
w/ DiSmo (Ours)
0.134±0.010
0.112±0.009
0.132±0.010
0.161±0.009
0.158±0.009
0.139±0.009
0.102±0.011
0.099±0.021
0.094±0.015
0.126±0.013
0.193±0.012
0.123±0.014"
NY,0.9155495978552279,"∆(bold - underline)
+0.045
+0.056
+0.065
+0.099
+0.099
+0.080
+0.034
+0.105
+0.022
+0.129
+0.112
+0.074"
NY,0.9168900804289544,Spearman Rank Correlation Coefficient
NY,0.9182305630026809,"Max Softmax Prob. [12]
0.001±0.015
0.035±0.017
0.054±0.016
0.076±0.016
0.056±0.018
0.044±0.016
0.032±0.023
0.066±0.024
0.031±0.021
0.067±0.021
0.087±0.020
0.057±0.022
Perplexity [3]
0.056±0.018
0.076±0.016
0.024±0.016
0.035±0.017
-0.002±0.016
0.038±0.017
0.023±0.021
0.094±0.022
0.065±0.016
0.088±0.019
0.019±0.019
0.058±0.019
Input Embedding [43]
0.141±0.016
0.135±0.016
0.087±0.014
0.097±0.017
0.029±0.016
0.098±0.016
0.043±0.018
0.084±0.020
0.098±0.016
0.024±0.016
0.090±0.017
0.068±0.016
Output Embedding [43]
0.012±0.018
-0.098±0.016
0.076±0.016
0.054±0.017
0.035±0.017
0.016±0.017
0.010±0.019
0.059±0.025
-0.077±0.019
0.084±0.019
0.067±0.023
0.029±0.021"
NY,0.9195710455764075,"TV score (Ours)
0.165±0.013
0.110±0.012
0.141±0.012
0.086±0.015
0.115±0.014
0.123±0.013
0.112±0.012
0.164±0.012
0.125±0.018
0.168±0.018
0.161±0.014
0.146±0.015
w/ DiSmo (Ours)
0.173±0.011
0.208±0.010
0.072±0.015
0.145±0.019
0.109±0.016
0.141±0.014
0.134±0.015
0.127±0.016
0.189±0.017
0.167±0.017
0.155±0.016
0.154±0.016"
NY,0.920911528150134,"∆(bold - underline)
+0.032
+0.073
+0.054
+0.048
+0.059
+0.043
+0.091
+0.070
+0.091
+0.080
+0.071
+0.086"
NY,0.9222520107238605,"F.4
Beyond Mathematical Reasoning"
NY,0.9235924932975871,"Our method has a wider range of application scenarios beyond mathematical reasoning. To verify
generalizability, we choose the multiple-choice quizzing task, which has the same “pattern collapse”
property as mathematical reasoning, since the output space is limited to the “ABCD” four options."
NY,0.9249329758713136,"We select the MMLU dataset [10] and choose eight domains among it: high school mathematics,
high school physics, high school chemistry, high school biology, high school geography, high school
government and politics, high school psychology, high school statistics. We test eight rounds, each
using one of the domains as the ID dataset and the remaining seven domains as the OOD datasets."
NY,0.9262734584450402,"We use Llama2-7B as the training backbone, each model is trained for 3K steps and 8 batch sizes in
4-card NVIDIA Tesla V100 (2 per card). The AUROC score matrices are shown in Figure 5(a)-(e),"
NY,0.9276139410187667,"presenting the results for TV score, input embedding, output embedding, and perplexity, respectively.
In the figures, the Roman numeral I - VIII are represented as:"
NY,0.9289544235924933,"• I = high school mathematics
• II = high school physics
• III = high school chemistry
• IV = high school biology
• V = high school geography
• VI = high school government and politics
• VII = high school psychology
• VIII = high school statistics"
NY,0.9302949061662198,"We find that MS-Prob and PPL nearly fail on the multiple-choice task and the output embedding is
not as excellent as expected, which is caused by the pattern collapse phenomenon."
NY,0.9316353887399463,"Our method is comparable to the input embedding method and has very good absolute performances.
For some far-shift OOD scenarios, e.g., mathematics-psychology (I-VII), physics-politics (II-VI),
etc., performances of the input embedding method and our method are basically the same, and there
exists a reasonable range of competing phenomena, e.g., our method performs more advantageously
under physics-biology (II-IV), and the input embedding method is better under physics-geography
(II-V). For some near-shift OOD scenarios, e.g., mathematics-statistics (I-VIII), where both domains
essentially belong to the category of math, our method will be more well-performed, indicating that
the embedding-based method produces performance degradation in fine-grained scenarios, while our
method possesses stronger robustness."
NY,0.9329758713136729,"Overall, our method is scalable and has greater advantages in fine-grained detection scenarios."
NY,0.9343163538873994,"(a) Maximum Probability
(b) Sequence Perplexity"
NY,0.935656836461126,"(c) Input Embedding
(d) Output Embedding"
NY,0.9369973190348525,(e) TV Score (Ours)
NY,0.938337801608579,"Figure 5: AUROC score matrix in MMLU dataset of different OOD scores. Rows represent ID data,
and columns represent OOD data."
NY,0.9396782841823056,NeurIPS Paper Checklist
CLAIMS,0.9410187667560321,1. Claims
CLAIMS,0.9423592493297587,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.9436997319034852,Answer: [Yes]
CLAIMS,0.9450402144772118,"Justification: [Abstract, Section 1]"
CLAIMS,0.9463806970509383,Guidelines:
CLAIMS,0.9477211796246648,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.9490616621983914,2. Limitations
LIMITATIONS,0.9504021447721179,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.9517426273458445,Answer: [Yes]
LIMITATIONS,0.953083109919571,Justification: [Limitations]
LIMITATIONS,0.9544235924932976,Guidelines:
LIMITATIONS,0.9557640750670241,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used
by reviewers as grounds for rejection, a worse outcome might be that reviewers
discover limitations that aren’t acknowledged in the paper. The authors should use
their best judgment and recognize that individual actions in favor of transparency play
an important role in developing norms that preserve the integrity of the community.
Reviewers will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.9571045576407506,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9584450402144772,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.9597855227882037,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.9611260053619303,"Justification: [Section 2, Appendix C]
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9624664879356568,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
THEORY ASSUMPTIONS AND PROOFS,0.9638069705093834,"Question: Does the paper fully disclose all the information needed to reproduce the
main experimental results of the paper to the extent that it affects the main claims and/or
conclusions of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: [Section 4, Appendix E]
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9651474530831099,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all
submissions to provide some reasonable avenue for reproducibility, which may depend
on the nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
THEORY ASSUMPTIONS AND PROOFS,0.9664879356568364,"Question: Does the paper provide open access to the data and code, with sufficient
instructions to faithfully reproduce the main experimental results, as described in
supplemental material?"
THEORY ASSUMPTIONS AND PROOFS,0.967828418230563,"Answer: [Yes]
Justification: [Section 4, Appendix E]
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9691689008042895,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
THEORY ASSUMPTIONS AND PROOFS,0.9705093833780161,"Question: Does the paper specify all the training and test details (e.g., data splits,
hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand
the results?
Answer: [Yes]
Justification: [Section 4, Appendix E]
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9718498659517426,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
THEORY ASSUMPTIONS AND PROOFS,0.9731903485254692,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: [Section 4, Appendix F]
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9745308310991957,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars,
confidence intervals, or statistical significance tests, at least for the experiments that
support the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean."
THEORY ASSUMPTIONS AND PROOFS,0.9758713136729222,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
THEORY ASSUMPTIONS AND PROOFS,0.9772117962466488,"Question: For each experiment, does the paper provide sufficient information on the
computer resources (type of compute workers, memory, time of execution) needed to
reproduce the experiments?
Answer: [Yes]
Justification: [Section 4, Appendix E]
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9785522788203753,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
THEORY ASSUMPTIONS AND PROOFS,0.9798927613941019,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: [Ethics Statement]
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9812332439678284,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special
consideration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
THEORY ASSUMPTIONS AND PROOFS,0.982573726541555,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: [Ethics Statement]
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9839142091152815,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to"
THEORY ASSUMPTIONS AND PROOFS,0.985254691689008,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
THEORY ASSUMPTIONS AND PROOFS,0.9865951742627346,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [Yes]
Justification: [Ethics Statement]
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9879356568364611,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
THEORY ASSUMPTIONS AND PROOFS,0.9892761394101877,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: [Section 4, Appendix E]
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9906166219839142,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
THEORY ASSUMPTIONS AND PROOFS,0.9919571045576407,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
THEORY ASSUMPTIONS AND PROOFS,0.9932975871313673,"Answer: [NA] .
Justification: [Our paper does not introduce new assets.]
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9946380697050938,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
THEORY ASSUMPTIONS AND PROOFS,0.9959785522788204,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA] .
Justification: [Our paper does not involve crowdsourcing experiments and research.]
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9973190348525469,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main
contribution of the paper involves human subjects, then as much detail as possible
should be included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA] .
Justification: [NA]
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9986595174262735,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
