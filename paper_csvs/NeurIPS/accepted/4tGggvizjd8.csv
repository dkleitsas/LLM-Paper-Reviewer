Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.004310344827586207,"Generalization analyses of deep learning typically assume that the training con-
verges to a fixed point. But, recent results indicate that in practice, the weights of
deep neural networks optimized with stochastic gradient descent often oscillate
indefinitely. To reduce this discrepancy between theory and practice, this paper
focuses on the generalization of neural networks whose training dynamics do not
necessarily converge to fixed points. Our main contribution is to propose a notion
of statistical algorithmic stability (SAS) that extends classical algorithmic stability
to non-convergent algorithms and to study its connection to generalization. This
ergodic-theoretic approach leads to new insights when compared to the traditional
optimization and learning theory perspectives. We prove that the stability of the
time-asymptotic behavior of a learning algorithm relates to its generalization and
empirically demonstrate how loss dynamics can provide clues to generalization
performance. Our findings provide evidence that networks that “train stably gener-
alize better” even when the training continues indefinitely and the weights do not
converge."
INTRODUCTION,0.008620689655172414,"1
Introduction"
INTRODUCTION,0.01293103448275862,"It is common practice that when the training loss of a neural networks converges close to some
value, the learning algorithm—typically some variant of Stochastic Gradient Descent (SGD)—is
terminated. Perhaps surprisingly, recent works indicate that the network function at termination time
is typically not a fixed point of the learning algorithm: if run longer, the gradient norm does not vanish
and the learning algorithm outputs functions with significantly different parameters [Cohen et al.,
2021, Zhang et al., 2022, Lobacheva et al., 2021]. This observation stands in contrast to common
generalization analyses that assume convergence of the training algorithm to a fixed point. It also
raises the question of when (and why) non-convergent learning algorithms should be expected to
generalize."
INTRODUCTION,0.017241379310344827,"Standard approaches for obtaining generalization bounds find ways to bound the complexity measure
of the function class expressible by a neural network in a data-independent e.g., [Vapnik, 1999,
Bartlett et al., 2017, Neyshabur et al., 2018, Golowich et al., 2018, Bartlett et al., 2019, Arora et al.,
2018] or data-dependent manner, e.g., [von Luxburg and Bousquet, 2004, Xu and Mannor, 2012,
Sokoli´c et al., 2017]. Nevertheless, even with recent technical improvements, by and large these"
INTRODUCTION,0.021551724137931036,"approaches do not explicitly account for the relationship between generalization and the dynamics of
the learning algorithm."
INTRODUCTION,0.02586206896551724,"A different approach to generalization analysis yields algorithm-dependent bounds by connecting the
generalization performance of the trained network to the stability of the training to data perturbations
[Bousquet and Elisseeff, 2002, Feldman and Vondrak, 2018, Kuzborskij and Lampert, 2018, Bousquet
et al., 2020, Zhang et al., 2021b, Rakhlin, 2006]. A key claim of these works has been that networks
that are trained fast generalize better. This claim, though intuitively meaningful, hinges on the premise
of a convergent algorithm, which deviates from the observed behavior of deep neural network training.
Further work has been done for settings where the algorithm provably converges to a fixed point, such
as two-layer overparameterized networks, deep linear networks, certain matrix factorizations, etc.
[Frei et al., 2019, Allen-Zhu et al., 2019, Soudry et al., 2018, Arora et al., 2019b, Gunasekar et al.,
2018, Haeffele and Vidal, 2015]. In a more general setting, Hardt et al. [2016] prove algorithmic
stability-based generalization bounds that worsen with increasing training time, whereas Loukas et al.
[2021] connect stable training dynamics near convergence under Dropout with good generalization.
For algorithms that do not converge (close) to a fixed point, the above analyses result in vacuous
bounds."
INTRODUCTION,0.03017241379310345,"Contributions. We start with the supposition that robustness of the learning algorithm’s fixed
points to small changes in the training set is not the mechanism underlying the good generalization
properties of deep networks. This is because, as is commonly known and many recent works indirectly
remark upon [Cohen et al., 2021, Ahn et al., 2022, Garipov et al., 2018, Zhang et al., 2022], training
can be linearly unstable: even if assuming the presence of isolated local minima that the learning
algorithm converges to, the weights encountered in training are not robust to small perturbations.
Small perturbations can accumulate over time leading to completely different orbits in weight space.
Yet, despite unstable dynamics, the generalization properties of two completely different fixed points
(e.g., obtained from two different random initializations or due to a different order of SGD updates)
are typically comparable."
INTRODUCTION,0.034482758620689655,"Our first contribution entails proposing a generalization of stability that applies to non-converging
algorithms. To achieve this, we depart from the typical optimization perspective of analyzing the
generalization properties of minima [Keskar et al., 2016, Ge et al., 2015, Sagun et al., 2016]. Instead,
we study the generalization performance of the learning algorithm on average (see section 2.2).
Adopting a statistical viewpoint, in Section 3 we define a new notion of statistical algorithmic
stability that measures the stability of this average performance to perturbations of the training data,
as opposed to stability of individual orbits in the weight space. Section 3 then proves upper bounds
for the average generalization error based on our new notion of statistical stability."
INTRODUCTION,0.03879310344827586,"We then move on to consider how statistical stability connects to the behavior of the loss function.
To this end, in Section 4 we provide conditions such that the spectral gap of a Markov operator
associated with the dynamics of the loss function is predictive of statistical algorithmic stability and
hence of generalization. Empirically, we estimate this spectral gap based on the rate of convergence
of ergodic averages of the loss function. We show via experiments that our estimate correlates with
generalization for image classification under corruption."
RELATED WORK,0.04310344827586207,"1.1
Related work"
RELATED WORK,0.04741379310344827,"Our work builds on previous analyses of the stability of learning algorithms that converge [Bousquet
and Elisseeff, 2002, Feldman and Vondrak, 2018, Kuzborskij and Lampert, 2018, Bousquet et al.,
2020, Zhang et al., 2021b]. Below, we also briefly discuss connections (beyond the literature on
stability) with previous analyses on the dynamics of learning algorithms."
RELATED WORK,0.05172413793103448,"Convergence and generalization of SGD. Exponential convergence rates to global minima are
known for SGD in the smooth and convex setting, with decaying learning rates and small constant
learning rates (e.g., [Moulines and Bach, 2011, Needell et al., 2014]). In the training of deep neural
networks, which is the focus of this paper, the loss function is non-convex and is typically not
well-approximated locally by convex functions [Ma et al., 2018]. However, in the overparameterized
regime, a Polyak-Lojasiewicz-type inequality that is automatically satisfied when the loss is smooth
and convex can be shown to hold in the non-convex setting as well [Liu et al., 2022, Ma et al., 2018],
allowing convergence to a global minimum. In certain non-convex problems, convergence to local
minima and even global minima has been proved, assuming a strict saddle property [Ge et al., 2015]"
RELATED WORK,0.05603448275862069,"or local convexity [Kleinberg et al., 2018]. Raginsky et al. [2017] show a favorable convergence for a
slightly different version of SGD by approximating the dynamics with a suitable continuous time
Langevin diffusion. A number of previous works [Safran and Shamir, 2016, Freeman and Bruna,
2016, Li and Yuan, 2017, Nguyen et al., 2018] have also provided arguments that SGD converges
to a solution that generalizes given suitable initialization and significant overparameterization. A
few works have studied the nonlinear dynamics of training [Kong and Tao, 2020] and provided
insights into generalization in the overparameterized regime [Dogra and Redman, 2020, Saxe et al.,
2013, Advani et al., 2020]. Another line of work considers the mean field limit of SGD dynamics to
show generalization [Mei et al., 2019, Gabrié et al., 2018, Chen et al., 2020]. The ergodic theoretic
perspective we adopt here deviates from the optimization perspectives but relates to the mean field
perspective in that we implicitly consider (see section 4) the evolution of probability distributions on
weight space."
RELATED WORK,0.0603448275862069,"Flat minima. In the generalization literature, flat minima correspond to large connected regions with
low empirical error in weight space. Flat minima have been argued to be related to the complexity
of the resulting hypothesis and, hence, can imply good generalization Hochreiter and Schmidhuber
[1997]. It has also been shown that SGD converges more frequently to flat minima when the batch
size is small or the learning rate and the number of iterations are suitably adjusted [Keskar et al.,
2016, Hoffer et al., 2017, Jastrz˛ebski et al., 2017, Smith and Le, 2018, Zhang et al., 2018]. Some
works consider the flat/sharp dichotomy an oversimplification [Dinh et al., 2017, Sagun et al., 2017,
He et al., 2019], e.g., using a reparameterization argument [Sagun et al., 2017]. The eigenvalues of
the hessian of the loss affect the local linear stability of optimization orbits. However, given that we
adopt the time-asymptotic/statistical picture, we do not explicitly make assumptions on local stability
(flatness/sharpness) or the topology of the loss landscape [Montanari and Zhong, 2020, Venturi et al.,
2019]."
RELATED WORK,0.06465517241379311,"SGD as a Bayesian sampler. The idea of looking at distributions of learners permeates PAC-Bayesian
analyses of generalization [McAllester, 1999, Dziugaite and Roy, 2017, Zhou et al., 2019, Pitas, 2020].
In contrast to Bayesian posteriors of the parameters, we study parameter distributions generated by
the learning algorithms. Previous empirical work has suggested that SGD operates almost like a
Bayesian sampler [Mingard et al., 2021] but the connection remains to be fully understood."
RELATED WORK,0.06896551724137931,"2
Local descent algorithms as dynamical systems: a statistical viewpoint"
RELATED WORK,0.07327586206896551,"This section lays out the basic definitions and assumptions of our work. We begin in Section 2.1
by introducing the learning problem and discussing learning algorithms from a dynamical systems
perspective. Section 2.2 then puts forth the notions of statistical convergence that give rise to our
main results."
LEARNING AS A DYNAMICAL SYSTEM,0.07758620689655173,"2.1
Learning as a dynamical system"
LEARNING AS A DYNAMICAL SYSTEM,0.08189655172413793,"We consider the supervised learning setup: a learner is given a training set S = {z1, . . . , zn}
consisting of n pairs zi ≡(xi, yi) of inputs xi ∈Rd and their corresponding labels yi ∈Y ⊆R,
drawn i.i.d. from a distribution D. A class of parametric models or hypotheses h : Rd × W →R on
a parameter space W gives possible input-to-output relationships, h(w, ·) : Rd →R. The learner is
given a risk or loss function ℓ: (Rd × R) × W →R+ that describes the error of a given hypothesis.
Common choices for loss functions are the mean-squared, hinge and cross-entropy loss. The learner
attempts to minimize the population risk RS(h(·, w)) = Ez∼Dℓ(z, w), by minimizing the empirical
risk"
LEARNING AS A DYNAMICAL SYSTEM,0.08620689655172414,"ˆRS(h(·, w)) = LS(w) = 1 n X"
LEARNING AS A DYNAMICAL SYSTEM,0.09051724137931035,"zi∈S
ℓ(zi, w).
(1)"
LEARNING AS A DYNAMICAL SYSTEM,0.09482758620689655,"This minimization is achieved using a local descent algorithm given by iterative updates ϕS : W →W
of the form"
LEARNING AS A DYNAMICAL SYSTEM,0.09913793103448276,"wt+1 = ϕS(wt) := wt −ηt ˆ∇LS(wt),
(2)"
LEARNING AS A DYNAMICAL SYSTEM,0.10344827586206896,where ηt is the learning rate or step size at time t.
LEARNING AS A DYNAMICAL SYSTEM,0.10775862068965517,"40
20
0
20
40 40 30 20 10 0 10 20 30 40 epoch 0 1000 2000 3000 4000"
LEARNING AS A DYNAMICAL SYSTEM,0.11206896551724138,"0
1000
2000
3000
4000
5000
epoch 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90"
LEARNING AS A DYNAMICAL SYSTEM,0.11637931034482758,test accuracy
LEARNING AS A DYNAMICAL SYSTEM,0.1206896551724138,"cumulative average
point-wise"
LEARNING AS A DYNAMICAL SYSTEM,0.125,"Figure 1: Although the training of neural networks depends on the initialization and does not
necessarily converge in the weight space, certain functionals of the learned hypothesis can converge
in distribution independently of initial conditions. Left: The orbits of the 2nd layer weights of four
different VGG16 models trained on a CIFAR10 dataset using SGD with step size 0.01. We embed
the orbits in the plane by performing PCA onto 40 dimensions followed by t-SNE. Colors indicate
epoch. Right: Corresponding test accuracy and cumulative average values as a function of epoch."
LEARNING AS A DYNAMICAL SYSTEM,0.12931034482758622,"In the gradient descent (GD) algorithm, ˆ∇LS(w) = ∇LS(w) is the gradient w.r.t. w, and the iterates
of ϕS represent a deterministic dynamical system. The critical points (including saddle points, local
and global minima) of LS are fixed points of ϕS (i.e., points w such that ϕSw = w). For stochastic
gradient descent (SGD), ˆ∇LS(w) is a random variable given by the sample mean of gradients
∇ℓ(·, w) over a batch of samples in S. In this case, ϕS is a random dynamical system and critical
points of LS are not necessarily fixed points of ϕS."
LEARNING AS A DYNAMICAL SYSTEM,0.1336206896551724,"To unify the definition of ϕS for both GD and SGD, we introduce the random variable Ξ that indicates
the choice of batch. Suppose the batch size m is fixed: m < n for SGD and m = n for GD. Denote
by [n] the set {1, 2, · · · , n} and let Ξt be a collection of m elements from [n] chosen uniformly at
random. Then, we may write ϕS as"
LEARNING AS A DYNAMICAL SYSTEM,0.13793103448275862,wt+1 = ϕS(wt) = wt −ηt ˆ∇LS(wt|Ξt) = wt −ηt m X
LEARNING AS A DYNAMICAL SYSTEM,0.14224137931034483,"i∈Ξt
∇ℓ(zi, wt),
(3)"
LEARNING AS A DYNAMICAL SYSTEM,0.14655172413793102,with Ξt being the set [n] at all t for GD.
LEARNING AS A DYNAMICAL SYSTEM,0.15086206896551724,"In our investigation, we opt for simplicity and consider a fixed learning rate ηt = η. More generally,
our analysis can be extended to learning rates that asymptotically converge to η. A rapidly vanish-
ing learning rate can obscure the learning dynamics by enforcing convergence to arbitrary points
irrespective of the points’ losses. In contrast, learning rates that do not decay to zero can induce
interesting transitions between neighborhoods of critical points and the iterates wt are generally more
exploratory of the loss landscape."
STATISTICAL CONVERGENCE OF THE LEARNING ALGORITHM,0.15517241379310345,"2.2
Statistical convergence of the learning algorithm"
STATISTICAL CONVERGENCE OF THE LEARNING ALGORITHM,0.15948275862068967,"In deep learning, any specific choice of wT , at some time T, is arbitrary as the loss landscape is
generally non-convex and stochastic learning algorithms are the norm. Hence, rather than focusing
on a single hypothesis, it is attractive to consider the average generalization error induced by the
stochastic dynamical system ϕS. We adopt this statistical perspective, rather than focus on a single
orbit of ϕS. Thus, the setting below applies to any asymptotic behavior of orbits of the (typically
nonlinear) dynamics ϕS including fixed points, periodic, quasiperiodic and chaotic orbits."
STATISTICAL CONVERGENCE OF THE LEARNING ALGORITHM,0.16379310344827586,"To analyze the generalization performance without making assumptions on the global stability
properties of individual orbits, we make an assumption about the ergodic properties of ϕS that
reasonably matches empirical evidence. To describe the assumption, we give a short primer on the
relevant concepts from ergodic theory of dynamical systems."
STATISTICAL CONVERGENCE OF THE LEARNING ALGORITHM,0.16810344827586207,"Invariant measures. Denote the Borel sigma algebra on W by B(W). A probability measure µS is
called invariant for ϕS if µS(A) = µS ◦ϕ−1
S (A) for all Borel sets A ∈B(W). Intuitively, for any
subset of the weight space, the probability that the dynamical system occupies it does not change.
See Liverani [2004] for an introduction to invariant measures."
STATISTICAL CONVERGENCE OF THE LEARNING ALGORITHM,0.1724137931034483,"If ϕS is a deterministic continuous function on the set W and W is compact, classical ergodic
theorems (see e.g., Theorem 4.1.1 of Katok and Hasselblatt [1997]) give the existence of at least
one invariant, ergodic measure for ϕS. Ergodic measures are a pertinent class of invariant measures
in practice, that allow us to understand the statistical properties of a system through evolution of
individual orbits. For an ergodic measure, sets that are invariant under the dynamics ϕS (such as a set
of periodic points, quasiperiodic or chaotic attractors) are trivial: they have measure 0 or 1."
STATISTICAL CONVERGENCE OF THE LEARNING ALGORITHM,0.17672413793103448,"When ϕS is an SGD update, Dieuleveut et al. [2020] show the existence of invariant distributions for
convex loss functions, whereas Fort and Pagès [1999] and Mattingly et al. [2002] show the existence
of invariant distributions in more general settings under mild assumptions on the gradient noise
(in the estimate ˆ∇LS). Several works have analyzed the asymptotic properties of such invariant
distributions, e.g., Chee and Toulis [2018] analyze the oscillation of the iterates wt about a mean
that is a critical point, as a phase separate from the transient phase of convergence to this invariant
distribution. Kong and Tao [2020] study the effect of large learning rates (see also [Wang et al.,
2021] for matrix factorization problems) and multiscale loss functions to show the existence of Gibbs
invariant measures in GD."
STATISTICAL CONVERGENCE OF THE LEARNING ALGORITHM,0.1810344827586207,"Ergodicity. When an invariant measure µS is ergodic for ϕS, time averages converge to ensemble
averages according to µS. That is, for µS-almost every initial state w0, and for all continuous scalar
functions h:"
T,0.1853448275862069,"1
T"
T,0.1896551724137931,"T −1
X"
T,0.1939655172413793,"t=0
h(wt) →Ew∼µSh(w)
(4)"
T,0.19827586206896552,and the dependence on initial conditions w0 is forgotten.
T,0.2025862068965517,"Unfortunately, we cannot generally expect the dynamics of the learning algorithm to have a unique
ergodic invariant measure on W. Instead, ϕS-invariant sets could be smaller subsets of W with
different ergodic invariant measures supported on them. As a result, starting from two different points
chosen Lebesgue almost everywhere (or sampled from any probability density on W), we do not
expect time averages to converge to the same limit. Indeed, as shown in Figure 1, the weights visited
by ϕS depend on the initial conditions and can vary significantly across different runs. In other words,
the limit of infinite time averages described in (4) does depend on w0."
T,0.20689655172413793,"Dynamics of the hypothesis. To circumvent the above issues, rather than focusing on the weights, we
consider the dynamics of the learned hypothesis h(·, wt). Specifically, suppose that the infinite-time
averages of the learned hypothesis are ergodic: that is, the time averages of h(z, wt) converge to the
same function irrespective of the initial function h(z, w0). Then, functionals that depend only on h
and not explicitly on the weights are also ergodic. This is one such scenario that leads to our main
assumption: the time averages of loss functions are ergodic.
Assumption 1. Given any S ∼Dn, there exists a map z →⟨ℓz⟩S, such that for Lebesgue-a.e. w0
and every z ∈Rd × R, the following holds:"
T,0.21120689655172414,"lim
T →∞
1
T"
T,0.21551724137931033,"T −1
X"
T,0.21982758620689655,"t=0
ℓ(z, wt) = ⟨ℓz⟩S ∈R+,
(5)"
T,0.22413793103448276,where {wt = ϕS(wt−1)} is an orbit of ϕS.
T,0.22844827586206898,"As an intuitive justification for Assumption 1, symmetries may result in several sets of parameters
(weights and biases) that represent the same network function h. Hence, assuming that there exists a
unique probability on the space of neural network functions that is ergodic is less restrictive and closer
to reality than assuming a unique ergodic measure on the parameter space. Furthermore, ergodicity
of the network functions is a stronger condition than our Assumption 1 and serves to illustrate one
possible sufficient condition for our assumption to hold."
T,0.23275862068965517,"Figure 1 provides evidence that Assumption 1 can hold in practice. It shows two observables
calculated along 4 different orbits of a VGG16 model trained using SGD with momentum (batches
of size 128, learning rate of 0.01, momentum of 0.9). On the left, we can see a low-dimensional
projection of the second layer weights obtained by projecting the weights onto 40 dimensions using
PCA and then employing t-SNE [Van der Maaten and Hinton, 2008], whereas the observable on the
right is the test accuracy, which is a functional of the hypothesis function. Here, the test accuracy can"
T,0.23706896551724138,"be seen to converge to a distribution independent of the initial conditions, whilst the weight orbits
vary significantly across different initializations. This experiment corroborates our hypothesis that
time averages of functionals on the loss space converge to distributions independent of the initial
condition, even if time averages of a generic observable on the weight space do not. See Remark 1 on
the idea that a larger learning rate induces ergodicity and Appendix A, which illustrates the idea with
a toy example."
STATISTICAL STABILITY IMPLIES GENERALIZATION,0.2413793103448276,"3
Statistical stability implies generalization"
STATISTICAL STABILITY IMPLIES GENERALIZATION,0.24568965517241378,"In Section 3.1, we generalize algorithmic stability beyond algorithms that converge to fixed points.
Section 3.2 then proceeds to examine the implications of our definition to generalization error."
STATISTICAL ALGORITHMIC STABILITY,0.25,"3.1
Statistical algorithmic stability"
STATISTICAL ALGORITHMIC STABILITY,0.2543103448275862,"Classically, the derivations of algorithmic stability-based generalization (see e.g., Hardt et al. [2016],
Chapter 14 of Mohri et al. [2018], Bousquet et al. [2020]) utilize an input perturbation of one element
in S by replacing it with a different element from the input distribution D:
Defnition 1 (Algorithmic stability, adapted from [Bousquet and Elisseeff, 2002]). Consider the
weights w∗
S and w∗
S′ obtained by running the learning algorithm on two training sets S, S′ sampled
from D that differ by exactly one sample. We say that the learning algorithm is algorithmically stable
(AS) with a stability coefficient β ≥0 if"
STATISTICAL ALGORITHMIC STABILITY,0.25862068965517243,"β = sup

|ℓ(z, w∗
S) −ℓ(z, w∗
S′)| : z ∈Rd × R
	
.
(6)"
STATISTICAL ALGORITHMIC STABILITY,0.2629310344827586,"We refer to this type of perturbation as a stochastic perturbation. Algorithmic stability does not
directly apply to learning algorithms that do not converge to a fixed point. Hence, next, we extend
algorithmic stability to loss statistics. The resulting notion of statistical algorithmic stability (SAS)
extends algorithmic stability to algorithms whose loss statistics converge as prescribed by Assumption
1, even if the weights do not converge."
STATISTICAL ALGORITHMIC STABILITY,0.2672413793103448,"Defnition 2 (Statistical algorithmic stability). We say that a learning algorithm with loss statistics
denoted by ⟨ℓz⟩S is statistically algorithmically stable (SAS) with a stability coefficient β ≥0 if"
STATISTICAL ALGORITHMIC STABILITY,0.27155172413793105,"β = sup

|⟨ℓz⟩S −⟨ℓz⟩S′| : z ∈Rd × R
	
,
(7)"
STATISTICAL ALGORITHMIC STABILITY,0.27586206896551724,"where S, S′ differ in exactly one element."
STATISTICAL ALGORITHMIC STABILITY,0.2801724137931034,"In the above definition, ⟨ℓz⟩S′ refers to the ergodic average of ℓ(z, ·) observed along almost every
orbit of ϕS′. A higher value of β indicates a lower SAS algorithm. The definition of SAS differs
from Definition 1 as we do not assume convergence of the learning algorithm. Nevertheless, when
every orbit of ϕS converges to a fixed point w∗
S, for almost every S, then Definition (7) reduces to the
standard notion of algorithmic stability since ergodic averages along almost every orbit converge to
ℓ(z, w∗
S)."
STATISTICAL ALGORITHMIC STABILITY,0.28448275862068967,"Crucially, and in line with the observations in Figure 1, we quantify stability based on the statistics of
the loss function and not the loss function at an ensemble mean or at any one point in the weight space
W. In other words, the definition of SAS does not use or provide information about the algorithmic
stability of Ew∈µSw for any invariant measure µS."
LEARNING THEORETIC IMPLICATIONS,0.28879310344827586,"3.2
Learning theoretic implications"
LEARNING THEORETIC IMPLICATIONS,0.29310344827586204,"It is well-known that Definition 1 leads to generalization bounds (see [Hardt et al., 2016, Bousquet
et al., 2020] and references therein). Next, we show that the more general statistical algorithmic
stability from Definition 2 also can imply a notion of generalization."
LEARNING THEORETIC IMPLICATIONS,0.2974137931034483,"To obtain generalization bounds, we first redefine the empirical and population risk to use the loss
statistics rather than loss values at fixed points:"
LEARNING THEORETIC IMPLICATIONS,0.3017241379310345,ˆRS = 1 n X
LEARNING THEORETIC IMPLICATIONS,0.30603448275862066,"z∈S
⟨ℓz⟩S
and
RS = Ez∼D⟨ℓz⟩S.
(8)"
LEARNING THEORETIC IMPLICATIONS,0.3103448275862069,"Although the definitions above appear deceivingly similar to the standard ones introduced in Sec-
tion 2.1, the two notions of risk are defined on different spaces, with the standard ones being maps
from the hypothesis space H to scalar values and (8) being maps from the space of learning algo-
rithms. That is, the latter risks only depend on algorithmic parameters and are not functions of H.
Using these definitions, the following generalization bound holds:
Theorem 1. Given a β-statistically algorithmically stable algorithm ϕS (see Definition 2), for any
δ ∈(0, 1), with probability greater than 1 −δ,"
LEARNING THEORETIC IMPLICATIONS,0.3146551724137931,RS ≤ˆRS + β + 2 (nβ + L) r
LEARNING THEORETIC IMPLICATIONS,0.31896551724137934,log(2/δ)
N,0.3232758620689655,"2n
,
(9)"
N,0.3275862068965517,"where L := supz supw∈W |ℓ(z, w)| is an upper bound on the loss."
N,0.33189655172413796,"The proof can be found in Appendix B and is based on applying Mcdiarmid’s inequality to the
generalization gap. Like Bousquet and Elisseeff [2002]’s bound, the SAS coefficient β has to decay
at least as fast as ∼O(1/√n) for the generalization bound to be non-vacuous. Note that our proof is
also naturally coupled with [Bousquet et al., 2020, Feldman and Vondrak, 2018]’s improved technique,
which provides tighter bounds when β ∼O(1/√n). (see Appendix B)."
DYNAMICAL SYSTEMS INTERPRETATION OF SAS,0.33620689655172414,"4
Dynamical systems interpretation of SAS"
DYNAMICAL SYSTEMS INTERPRETATION OF SAS,0.34051724137931033,"How can we distinguish between two algorithms with different SAS? Can the algorithm dynamics
provide clues to its SAS? These questions are of practical importance because a more (statistically)
stable algorithm generalizes better (via Theorem 1). Since the SAS coefficient β is defined as
a supremum over infinite pairs of training sets and inputs, a numerical estimation of it using its
definition only gives a rough lower bound and obtaining this lower bound is also computationally
expensive (see Figure 3 and section 5). Moreover, a numerical lower bound on β does not give us a
mechanistic understanding of statistical stability, which we seek here."
DYNAMICAL SYSTEMS INTERPRETATION OF SAS,0.3448275862068966,"Here we develop an operator-theoretic explanation for what makes an algorithm statistically stable.
We also derive a rough heuristic – albeit not a definitive predictor of generalization – that relates a
given training dynamics to its SAS coefficient β."
BOUNDING THE SAS COEFFICIENT,0.34913793103448276,"4.1
Bounding the SAS coefficient"
BOUNDING THE SAS COEFFICIENT,0.35344827586206895,"Before we present our bound, we discuss why we must deviate from the trajectory-based approach
that is commonly employed to study algorithmic stability. We recall that Hardt et al. [2016] base
their proofs of classical algorithmic stability (Theorems 3.7 and 3.8 in Hardt et al. [2016]) on the
accumulating differences between two orbits corresponding to S and S′, whenever the different
element is chosen in the mini-batch. The greater the difference in the weights, the greater the
difference in the (upper bound on the) loss functions at those weights, in the case of Lipschitz loss.
Thus, Hardt et al. [2016] argue that training faster (or stopping earlier) will lead to stabler algorithms.
Since SAS is about robustness of loss statistics or infinitely long time averages, the analysis à la Hardt
et al. [2016] generically leads to vacuous bounds for the SAS coefficient."
BOUNDING THE SAS COEFFICIENT,0.3577586206896552,"Our analysis is based on the realization that an algorithm can be SAS even if two orbits corresponding
to S and S′ diverge from each other (within W), but lead to similar loss statistics. Thus, to bound the
SAS coefficient, a perturbation analysis of transition operators (global information), rather than a
finite-time perturbation analysis of an orbit (local information), is needed. Since SAS only demands
robustness on loss space, we consider Markov transition operators on the loss space (⊆[0, L]), as
opposed to on the weight space (W). In general, at any z and w0, the loss process, {ℓ(z, wt)}t is not
Markovian. But, a family of Markov operators can be associated with the family of loss functions.
Lemma 1. (Markov operators) Let µS be an ergodic, invariant measure for ϕS. Assume that a loss
function ℓ(z, ·) : W →Iz ⊆[0, L] is such that the pushforward, νz
S : B(Iz) →R+, of µS on the
loss space is well-defined. Here, B(Iz) is the Borel sigma algebra on Iz and νz
S = ℓ(z, ·)♯µS, where
♯refers to the pushforward operation. Then, there exists a uniformly ergodic Markov operator Pz
µS
on the space of probability measures on Iz, with its invariant measure being νz
S."
BOUNDING THE SAS COEFFICIENT,0.3620689655172414,"This lemma (see Appendix C for a proof) parallels Chekroun et al. [2014]’s Theorem A, but in a
quite different setting, without using existence of a unique physical measure on the phase space (W)."
BOUNDING THE SAS COEFFICIENT,0.36637931034482757,"Combined with Assumption 1, perturbation results on the Markov operators, Pz
µS, defined by Lemma
1 lead us to SAS. Since Pz
µS is uniformly ergodic, there exist λz ∈(0, 1) and C > 0 such that for all
ξ ∈Iz, in the Wasserstein norm (∥· ∥W ),"
BOUNDING THE SAS COEFFICIENT,0.3706896551724138,"∥Pzt
µSδξ −νz
S∥W ≤Cλt
z.
(10)"
BOUNDING THE SAS COEFFICIENT,0.375,"Let λ := supz λz be the rate of mixing associated with the family of operators Pz
µS. We leverage the
perturbation theory of Markov operators to study the effect of stochastic perturbations on νz
S, and
subsequently, SAS. To see the connection with SAS, we recall that, by Assumption 1, for any choice
of µS, we have that ⟨ℓz⟩S = Eξ∼νz
Sξ. Thus, given any pair of µS and µS′, we have (see Appendix C
for details)"
BOUNDING THE SAS COEFFICIENT,0.3793103448275862,"|⟨ℓz⟩S −⟨ℓz⟩S′| = |Eξ∼νz
Sξ −Eξ∼νz
S′ξ| ≤∥νz
S −νz
S′∥W .
(11)"
BOUNDING THE SAS COEFFICIENT,0.38362068965517243,"Thus, a uniform upper bound on ∥νz
S −νz
S′∥W gives an upper bound for the SAS coefficient, β. Such
a bound is obtained from the straightforward use of an appropriate perturbation result on uniformly
ergodic Markov chains.
Theorem 2. Given a uniformly ergodic Markov operator Pz
µS constructed in Lemma 1, and that
the Lipschitz constant of ∇ℓ(·, w) on Rd is bounded above by LD for all w ∈W, ϕS is SAS with
stability coefficient"
BOUNDING THE SAS COEFFICIENT,0.3879310344827586,"β = O
 1"
BOUNDING THE SAS COEFFICIENT,0.3922413793103448,"n
LD
1 −λ 
,"
BOUNDING THE SAS COEFFICIENT,0.39655172413793105,"where λ = supz λz is the supremum over z of the mixing rates of Pz
µS."
BOUNDING THE SAS COEFFICIENT,0.40086206896551724,"Proof. (Sketch) The proof relies on the perturbation bounds of Rudolf and Schweizer [2018], Corol-
lary 3.2. Note that when (10) is satisfied, and if a stochastic perturbation S′ introduces a change
δPz
µS to Pz
µS, the perturbation bound from [Rudolf and Schweizer, 2018] gives"
BOUNDING THE SAS COEFFICIENT,0.4051724137931034,"∥νz
S −νz
S′∥W ≤C∥δPz
µS∥/(1 −λz),
(12)"
BOUNDING THE SAS COEFFICIENT,0.40948275862068967,"where ∥δPz
µS∥is the operator norm of δPz
µS induced by Wasserstein norm. From (11),"
BOUNDING THE SAS COEFFICIENT,0.41379310344827586,"|⟨ℓz⟩S −⟨ℓz⟩S′| ≤C∥δPz
µS∥/(1 −λz).
(13)"
BOUNDING THE SAS COEFFICIENT,0.41810344827586204,"Since this holds for all z, the right hand side of (13) is an upper bound for the stability coefficient β
in Definition (2), by taking the supremum over z. An upper bound on
δPz
µS
 can be derived when
the Lipschitz constant of ∇ℓ(·, w) is uniformly (in w) bounded on Rd, as shown in Appendix C)."
BOUNDING THE SAS COEFFICIENT,0.4224137931034483,"Theorem 2 implies that an algorithm that exhibits faster convergence to the stationary measure on
the loss space generalizes better. The bound in Theorem 2 implies that smaller λ (faster convergence
of the loss to the ergodic invariant measure) yields smaller upper bounds on β (more statistically
stable algorithm) and better generalization."
BOUNDING THE SAS COEFFICIENT,0.4267241379310345,"Strictly speaking, to determine λz, we must consider finite-dimensional approximations of the
operator Pz
µS (as done in the setting of chaotic systems in e.g., [Crimmins and Froyland, 2020,
Chekroun et al., 2014]) and compute its spectral decomposition. However, in our setting, it is difficult
in practice to generate samples in the loss space that obey the Markov process defined in Lemma 1.
Instead, we argue for using the readily available non-Markovian loss process to estimate convergence
to the equilibrium distribution. In particular, we hypothesize below that the auto-correlation function
of the test loss can separate algorithms based on their SAS."
BOUNDING THE SAS COEFFICIENT,0.43103448275862066,"Dropping the superscripts z, we define the auto-correlation Cℓ(τ) in a loss function ℓ(see also
Remark 3 in Appendix C) by"
BOUNDING THE SAS COEFFICIENT,0.4353448275862069,"Cℓ(τ) := | lim
T →∞
1
T X"
BOUNDING THE SAS COEFFICIENT,0.4396551724137931,"t≤T
ℓ(wt)ℓ(wt+τ) −⟨ℓ⟩2|/⟨ℓ⟩2.
(14)"
BOUNDING THE SAS COEFFICIENT,0.44396551724137934,"That is, Cℓ(τ) gives the correlation between the random variables ℓ(wt)/⟨ℓ⟩and ℓ(wt+τ)/⟨ℓ⟩, where
the randomness comes from the batch selection Ξt and the initialization of weights in SGD, and only
from the latter in GD."
BOUNDING THE SAS COEFFICIENT,0.4482758620689655,"Figure 2: SAS computed from the VGG16 model. Left: The change in cumulative time averages
of the test loss upon stochastic perturbation of the CIFAR10 dataset. Center: Lower bound on
the statistical stability coefficient β (Definition 2) computed using the test loss. Right: Test error
timeseries."
BOUNDING THE SAS COEFFICIENT,0.4525862068965517,"Figure 3: Predictors of generalization gaps on data with corrupted labels computed on the ResNet18
model. Left: Time series of the test loss. Center: Normalized auto-correlation of the test loss
timeseries on the left. The magnitude of the auto-correlation in the test loss is suggestive of the
generalization gap (right). For large percentage of noise, the gap decreases because the neural
network cannot fit the training data well."
BOUNDING THE SAS COEFFICIENT,0.45689655172413796,"Suppose that the loss process is Markov, i.e., ℓ(wt+1) conditioned on ℓ(wt) is independent of
{ℓ(wt′) : t′ ≤t −1}. Setting t = 0 after a sufficiently long runup time, the function Cℓ(τ) in (14) is
the auto-correlation function of a stationary Markov chain, which typically decays exponentially with
τ. This decay rate is lower (slower correlation decay/longer correlation times) when λ is closer to 1.
Qualitatively, comparing two different stationary Markov chains, the one with higher values of Cℓ
has longer correlation times and hence, larger λ."
BOUNDING THE SAS COEFFICIENT,0.46120689655172414,"In general, however, the loss process {ℓ(wt) : t ≥0} is non-Markovian. Thus, we do not expect
Cℓ(τ) to decay with τ. This is because the correlation function Cℓencodes both the Markovian and
non-Markovian components component of the loss dynamics (see Remark 4 in Appendix C; [Zwanzig,
2001, Kondrashov et al., 2015]). Hence, we expect the correlation times in the loss process to not
be equal to that of a Markov process generated according to PµS. For the purpose of qualitatively
comparing the SAS coefficients of two algorithms, however, the loss process can be considered a
heuristic for a Markov process generated according to PµS. Then, higher values of Cℓcomputed
from the loss process indicate larger λ, and hence less statistical stability."
NUMERICAL RESULTS,0.46551724137931033,"5
Numerical results"
NUMERICAL RESULTS,0.4698275862068966,"We numerically validate the main ideas of section 3 and 4 on VGG16 and ResNet18 models trained
on the CIFAR10 dataset (see Appendix D for further numerical results, [Chandramoorthy and Loukas,
2023] for the code). For all our experiments, ϕS is an SGD update with momentum 0.9, fixed
learning rate 0.01 and batch size of 128. In all figures, “time” indicates number of epochs. We
generate different versions of the training set Sp by corrupting CIFAR10’s labels with probability
p, with S0 being the original CIFAR10 dataset. Figures 2 and 3 show results corresponding to
p = 0, 0.1, 0.17, 0.25 and 0.5. Each line in Figure 3 is a sample mean over 10 random initializations."
NUMERICAL RESULTS,0.47413793103448276,"In Figure 2, we numerically estimate a lower bound on the SAS coefficient β using its definition
(see Definition 2). On the left, we plot the difference in test loss time-averages between between
random orbits of ϕSp and ϕS′p, with S′
p being a stochastic perturbation of Sp. The mean over 45 pairs
of orbits is shown as dots and the error bars indicate the standard error in mean. The cumulative time"
NUMERICAL RESULTS,0.47844827586206895,"average along orbits of length 1200 epochs are used as estimators for statistics. With these estimators
for statistics, in Figure 2(center), we compute an estimate of the stability coefficient as the difference
of the (estimated) test loss statistics at different values of p. This is hence an estimate on the lower
bound of β, and clearly increases with p. This illustrates that cases with worse generalization errors
(Figure 2 (right)) have larger lower bounds on β."
NUMERICAL RESULTS,0.4827586206896552,"In Figure 3 (left), we show the test loss timeseries obtained with the ResNet18 model. Again, greater
the noise corruption p, the larger is the generalization error (estimated by test error), consistent with
previous studies Zhang et al. [2021a], Loukas et al. [2021]). On the other hand, the generalization gap
– difference between training and test error – is bigger for intermediate levels of noise and decreases
when p = 0.5, as shown in Figure 3 (center). The gap does not always increase with p because, when
the labels are close to random, the network cannot fit the training data and thus the training error is
also large."
NUMERICAL RESULTS,0.4870689655172414,"In Figure 3 (right), we plot Cℓ(τ) (Section 4) where ℓis taken as the test loss. The test loss statistic
⟨ℓ⟩is estimated as a time average over 1200 epochs. At each p, we show the sample average of
the auto-correlations over 10 independent runs. We see that the magnitude of the test loss auto-
correlations preserves the same order (across p′s) as the generalization gap (absolute difference in test
and training losses) shown in Figure 3(center). Hence, we empirically observe that the loss process
codifies the phenomenological explanation for SAS that is expressed in Theorem 2."
DISCUSSION AND CONCLUSION,0.49137931034482757,"6
Discussion and conclusion"
DISCUSSION AND CONCLUSION,0.4956896551724138,"Predicting generalization is an active area of research and has implications for more reliable use of
machine learning. In this work, we introduce statistical algorithmic stability (SAS) and show how it
implies generalization bounds. Here, we add a few important remarks."
DISCUSSION AND CONCLUSION,0.5,"Statistical stability only requires robustness of statistics on loss space. A central challenge in the
analyses of non-convergent algorithms is the fact that there may be multiple, very different invariant
measures µS. For this reason, our stability criterion focuses on statistics of the loss function. This
way, an algorithm can be stable even if the measures µS and µS′ are not close in the total variation
norm. In fact, an algorithm can be stable even if µS and µS′ are mutually singular, if the loss functions
have similar statistics. Since our SAS analysis hinges on a reasonable yet nonrestrictive assumption
on ergodic properties of the algorithm, we believe it is broadly applicable."
DISCUSSION AND CONCLUSION,0.5043103448275862,"Algorithms that converge are special cases of the above analysis. In Appendix E, we show that the
proposed relationship (section 4) between statistical stability and convergence rates to stationary
measures can also be observed in the Neural Tangent Kernel (NTK) regime [Jacot et al., 2018]. In
this case, the training dynamics, ϕS, can be approximated by a linear function of the weights, which
converges to a fixed point. Thus, this provides an alternative, ergodic theoretic, interpretation of
generalization in the NTK regime [Arora et al., 2019b, Montanari and Zhong, 2020, Bartlett et al.,
2021]."
DISCUSSION AND CONCLUSION,0.5086206896551724,"Broader view. While our focus is on algorithmic stability-based generalization, this work gathers
more evidence to support the broader view [Wojtowytsch, 2021, Zhang et al., 2022] that exploiting
theoretically and empirically available dynamical information about the training algorithm is a fruitful
complement to understanding generalization from the optimization landscape and learning theory
perspectives."
DISCUSSION AND CONCLUSION,0.5129310344827587,"Funding: This work was partially funded by the NSF AI Institute TILOS, NSF award 2134108, and
ONR grant N00014-20-1-2023 (MURI ML-SCOPE)."
DISCUSSION AND CONCLUSION,0.5172413793103449,"Acknolwedgments: N.C. would like to thank Nandhini Chandramoorthy and Derek Lim for helping
with the experimental setup and Sven Wang, Benjamin Zhang, Matt Li and Youssef Marzouk for
valuable discussions. The authors also thank the reviewers for their constructive suggestions."
REFERENCES,0.521551724137931,References
REFERENCES,0.5258620689655172,"M. S. Advani, A. M. Saxe, and H. Sompolinsky. High-dimensional dynamics of generalization
error in neural networks. Neural Networks, 132:428–446, 2020. ISSN 0893-6080. doi: https:
//doi.org/10.1016/j.neunet.2020.08.022. URL https://www.sciencedirect.com/science/
article/pii/S0893608020303117."
REFERENCES,0.5301724137931034,"K. Ahn, J. Zhang, and S. Sra. Understanding the unstable convergence of gradient descent, 2022.
URL https://arxiv.org/abs/2204.01050."
REFERENCES,0.5344827586206896,"Z. Allen-Zhu, Y. Li, and Y. Liang. Learning and generalization in overparameterized neural networks,
going beyond two layers. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Cur-
ran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
62dad6e273d32235ae02b7d321578ee8-Paper.pdf."
REFERENCES,0.5387931034482759,"H. Arbabi and I. Mezic. Ergodic theory, dynamic mode decomposition, and computation of spectral
properties of the koopman operator. SIAM Journal on Applied Dynamical Systems, 16(4):2096–
2126, 2017."
REFERENCES,0.5431034482758621,"J. L. Aron and I. B. Schwartz. Seasonality and period-doubling bifurcations in an epidemic model.
Journal of theoretical biology, 110(4):665–679, 1984."
REFERENCES,0.5474137931034483,"S. Arora, R. Ge, B. Neyshabur, and Y. Zhang. Stronger generalization bounds for deep nets via a
compression approach. In International Conference on Machine Learning, pages 254–263. PMLR,
2018."
REFERENCES,0.5517241379310345,"S. Arora, S. Du, W. Hu, Z. Li, and R. Wang. Fine-grained analysis of optimization and generaliza-
tion for overparameterized two-layer neural networks. In International Conference on Machine
Learning, pages 322–332. PMLR, 2019a."
REFERENCES,0.5560344827586207,"S. Arora, S. S. Du, W. Hu, Z. Li, R. R. Salakhutdinov, and R. Wang. On exact computation with an
infinitely wide neural net. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Cur-
ran Associates, Inc., 2019b. URL https://proceedings.neurips.cc/paper/2019/file/
dbc4d84bfcfe2284ba11beffb853a8c4-Paper.pdf."
REFERENCES,0.5603448275862069,"P. L. Bartlett, D. J. Foster, and M. Telgarsky. Spectrally-normalized margin bounds for neural
networks. In Proceedings of the 31st International Conference on Neural Information Processing
Systems, pages 6241–6250, 2017."
REFERENCES,0.5646551724137931,"P. L. Bartlett, N. Harvey, C. Liaw, and A. Mehrabian. Nearly-tight vc-dimension and pseudodimension
bounds for piecewise linear neural networks. Journal of Machine Learning Research, 20(63):1–17,
2019. URL http://jmlr.org/papers/v20/17-612.html."
REFERENCES,0.5689655172413793,"P. L. Bartlett, A. Montanari, and A. Rakhlin. Deep learning: a statistical viewpoint. Acta numerica,
30:87–201, 2021."
REFERENCES,0.5732758620689655,"O. Bousquet and A. Elisseeff. Stability and generalization. The Journal of Machine Learning
Research, 2:499–526, 2002."
REFERENCES,0.5775862068965517,"O. Bousquet, Y. Klochkov, and N. Zhivotovskiy. Sharper bounds for uniformly stable algorithms. In
J. Abernethy and S. Agarwal, editors, Proceedings of Thirty Third Conference on Learning Theory,
volume 125 of Proceedings of Machine Learning Research, pages 610–626. PMLR, 09–12 Jul
2020. URL https://proceedings.mlr.press/v125/bousquet20b.html."
REFERENCES,0.5818965517241379,"M. Budiši´c, R. Mohr, and I. Mezi´c. Applied koopmanism. Chaos: An Interdisciplinary Journal of
Nonlinear Science, 22(4):047510, 2012. doi: 10.1063/1.4772195."
REFERENCES,0.5862068965517241,"N. Chandramoorthy and A. Loukas. ni-sha-c/astability: Paper, Jan. 2023. URL https://doi.org/
10.5281/zenodo.7525770."
REFERENCES,0.5905172413793104,"J. Chee and P. Toulis. Convergence diagnostics for stochastic gradient descent with constant learning
rate. In A. Storkey and F. Perez-Cruz, editors, Proceedings of the Twenty-First International
Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning
Research, pages 1476–1485. PMLR, 09–11 Apr 2018."
REFERENCES,0.5948275862068966,"M. D. Chekroun, J. D. Neelin, D. Kondrashov, J. C. McWilliams, and M. Ghil. Rough parameter
dependence in climate models and the role of ruelle-pollicott resonances. Proceedings of the
National Academy of Sciences, 111(5):1684–1690, 2014."
REFERENCES,0.5991379310344828,"Z. Chen, Y. Cao, Q. Gu, and T. Zhang. Mean-field analysis of two-layer neural networks: Non-
asymptotic rates and generalization bounds. arXiv preprint arXiv:2002.04026, 2020."
REFERENCES,0.603448275862069,"J. M. Cohen, S. Kaur, Y. Li, J. Z. Kolter, and A. Talwalkar. Gradient descent on neural networks
typically occurs at the edge of stability, 2021. URL https://arxiv.org/abs/2103.00065."
REFERENCES,0.6077586206896551,"H. Crimmins and G. Froyland. Fourier approximation of the statistical properties of anosov maps on
tori. Nonlinearity, 33(11):6244, 2020."
REFERENCES,0.6120689655172413,"M. Dellnitz, G. Froyland, and S. Sertl. On the isolated spectrum of the perron-frobenius operator.
Nonlinearity, 13(4):1171, 2000."
REFERENCES,0.6163793103448276,"A. Dieuleveut, A. Durmus, and F. Bach. Bridging the gap between constant step size stochastic
gradient descent and Markov chains. The Annals of Statistics, 48(3):1348 – 1382, 2020. doi:
10.1214/19-AOS1850. URL https://doi.org/10.1214/19-AOS1850."
REFERENCES,0.6206896551724138,"L. Dinh, R. Pascanu, S. Bengio, and Y. Bengio. Sharp minima can generalize for deep nets. In
International Conference on Machine Learning, pages 1019–1028. PMLR, 2017."
REFERENCES,0.625,"A. S. Dogra and W. Redman. Optimizing neural networks via koopman operator theory. Advances in
Neural Information Processing Systems, 33:2087–2097, 2020."
REFERENCES,0.6293103448275862,"G. K. Dziugaite and D. M. Roy. Computing nonvacuous generalization bounds for deep (stochastic)
neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008,
2017."
REFERENCES,0.6336206896551724,"V. Feldman and J. Vondrak.
Generalization bounds for uniformly stable algorithms.
In
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett, editors, Advances in Neural Information Processing Systems, volume 31. Curran
Associates, Inc., 2018.
URL https://proceedings.neurips.cc/paper/2018/file/
05a624166c8eb8273b8464e8d9cb5bd9-Paper.pdf."
REFERENCES,0.6379310344827587,"J.-C. Fort and G. Pagès. Asymptotic behavior of a markovian stochastic algorithm with constant step.
SIAM Journal on Control and Optimization, 37(5):1456–1482, 1999."
REFERENCES,0.6422413793103449,"C. D. Freeman and J. Bruna. Topology and geometry of half-rectified network optimization. arXiv
preprint arXiv:1611.01540, 2016."
REFERENCES,0.646551724137931,"S. Frei, Y. Cao, and Q. Gu. Algorithm-dependent generalization bounds for overparameterized
deep residual networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Cur-
ran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
6e2290dbf1e11f39d246e7ce5ac50a1e-Paper.pdf."
REFERENCES,0.6508620689655172,"M. Gabrié, A. Manoel, C. Luneau, N. Macris, F. Krzakala, L. Zdeborová, et al. Entropy and mutual
information in models of deep neural networks. Advances in Neural Information Processing
Systems, 31, 2018."
REFERENCES,0.6551724137931034,"T. Garipov, P. Izmailov, D. Podoprikhin, D. P. Vetrov, and A. G. Wilson. Loss surfaces, mode
connectivity, and fast ensembling of dnns. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/
2018/file/be3087e74e9100d4bc4c6268cdbe8456-Paper.pdf."
REFERENCES,0.6594827586206896,"R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points—online stochastic gradient for
tensor decomposition. In Conference on learning theory, pages 797–842. PMLR, 2015."
REFERENCES,0.6637931034482759,"N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks.
In Conference On Learning Theory, pages 297–299. PMLR, 2018."
REFERENCES,0.6681034482758621,"S. Gunasekar, J. D. Lee, D. Soudry, and N. Srebro. Implicit bias of gradient descent on linear
convolutional networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Cur-
ran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf."
REFERENCES,0.6724137931034483,"B. D. Haeffele and R. Vidal. Global optimality in tensor factorization, deep learning, and beyond,
2015. URL https://arxiv.org/abs/1506.07540."
REFERENCES,0.6767241379310345,"M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient
descent. In M. F. Balcan and K. Q. Weinberger, editors, Proceedings of The 33rd International
Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages
1225–1234, New York, New York, USA, 20–22 Jun 2016. PMLR. URL https://proceedings.
mlr.press/v48/hardt16.html."
REFERENCES,0.6810344827586207,"H. He, G. Huang, and Y. Yuan. Asymmetric valleys: Beyond sharp and flat local minima. arXiv
preprint arXiv:1902.00744, 2019."
REFERENCES,0.6853448275862069,"S. Hochreiter and J. Schmidhuber. Flat Minima. Neural Computation, 9(1):1–42, 01 1997. ISSN 0899-
7667. doi: 10.1162/neco.1997.9.1.1. URL https://doi.org/10.1162/neco.1997.9.1.1."
REFERENCES,0.6896551724137931,"E. Hoffer, I. Hubara, and D. Soudry. Train longer, generalize better: closing the generalization gap in
large batch training of neural networks. In NIPS, 2017."
REFERENCES,0.6939655172413793,"A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Cur-
ran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf."
REFERENCES,0.6982758620689655,"S. Jastrz˛ebski, Z. Kenton, D. Arpit, N. Ballas, A. Fischer, Y. Bengio, and A. Storkey. Three factors
influencing minima in sgd. arXiv preprint arXiv:1711.04623, 2017."
REFERENCES,0.7025862068965517,"T. Kato. Perturbation theory for linear operators, volume 132. Springer Science & Business Media,
2013."
REFERENCES,0.7068965517241379,"A. Katok and B. Hasselblatt. Introduction to the modern theory of dynamical systems. Number 54.
Cambridge university press, 1997."
REFERENCES,0.7112068965517241,"G. Keller and C. Liverani. Stability of the spectrum for transfer operators. Annali della Scuola
Normale Superiore di Pisa-Classe di Scienze, 28(1):141–152, 1999."
REFERENCES,0.7155172413793104,"N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training
for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016."
REFERENCES,0.7198275862068966,"B. Kleinberg, Y. Li, and Y. Yuan. An alternative view: When does SGD escape local minima?
In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine
Learning, volume 80 of Proceedings of Machine Learning Research, pages 2698–2707. PMLR,
10–15 Jul 2018. URL https://proceedings.mlr.press/v80/kleinberg18a.html."
REFERENCES,0.7241379310344828,"D. Kondrashov, M. D. Chekroun, and M. Ghil.
Data-driven non-markovian closure models.
Physica D: Nonlinear Phenomena, 297:33–55, 2015. ISSN 0167-2789. doi: https://doi.org/
10.1016/j.physd.2014.12.005. URL https://www.sciencedirect.com/science/article/
pii/S0167278914002413."
REFERENCES,0.728448275862069,"L. Kong and M. Tao.
Stochasticity of deterministic gradient descent: Large learning rate for
multiscale objective function. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin,
editors, Advances in Neural Information Processing Systems, volume 33, pages 2625–2638.
Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
1b9a80606d74d3da6db2f1274557e644-Paper.pdf."
REFERENCES,0.7327586206896551,"B. O. Koopman. Hamiltonian systems and transformation in hilbert space. Proceedings of the
national academy of sciences of the united states of america, 17(5):315, 1931."
REFERENCES,0.7370689655172413,"M. Korda and I. Mezi´c. On convergence of extended dynamic mode decomposition to the koopman
operator. Journal of Nonlinear Science, 28(2):687–710, 2018."
REFERENCES,0.7413793103448276,"I. Kuzborskij and C. Lampert. Data-dependent stability of stochastic gradient descent. In International
Conference on Machine Learning, pages 2815–2824. PMLR, 2018."
REFERENCES,0.7456896551724138,"A. Lasota and M. C. Mackey. Chaos, fractals, and noise: stochastic aspects of dynamics, volume 97.
Springer Science & Business Media, 1998. doi: 10.1007/978-1-4612-4286-4."
REFERENCES,0.75,"Y. Li and Y. Yuan. Convergence analysis of two-layer neural networks with relu activation. Advances
in neural information processing systems, 30, 2017."
REFERENCES,0.7543103448275862,"K. K. Lin and F. Lu. Data-driven model reduction, wiener projections, and the koopman-mori-
zwanzig formalism. Journal of Computational Physics, 424:109864, 2021. ISSN 0021-9991.
doi: https://doi.org/10.1016/j.jcp.2020.109864.
URL https://www.sciencedirect.com/
science/article/pii/S0021999120306380."
REFERENCES,0.7586206896551724,"C. Liu, L. Zhu, and M. Belkin. Loss landscapes and optimization in over-parameterized non-linear
systems and neural networks. Applied and Computational Harmonic Analysis, 2022."
REFERENCES,0.7629310344827587,"C. Liverani. Invariant measures and their properties. a functional analytic point of view. Dynamical
systems. Part II, pages 185–237, 2004."
REFERENCES,0.7672413793103449,"E. Lobacheva, M. Kodryan, N. Chirkova, A. Malinin, and D. P. Vetrov. On the periodic behavior
of neural network training with batch normalization and weight decay. Advances in Neural
Information Processing Systems, 34, 2021."
REFERENCES,0.771551724137931,"A. Loukas, M. Poiitis, and S. Jegelka. What training reveals about neural network complexity. In
A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information
Processing Systems, 2021. URL https://openreview.net/forum?id=RcjW7p7z8aJ."
REFERENCES,0.7758620689655172,"S. Ma, R. Bassily, and M. Belkin. The power of interpolation: Understanding the effectiveness
of SGD in modern over-parametrized learning. In J. Dy and A. Krause, editors, Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pages 3325–3334. PMLR, 10–15 Jul 2018. URL https://proceedings.
mlr.press/v80/ma18a.html."
REFERENCES,0.7801724137931034,"J. Mattingly, A. Stuart, and D. Higham. Ergodicity for sdes and approximations: locally lipschitz
vector fields and degenerate noise. Stochastic Processes and their Applications, 101(2):185–232,
2002. ISSN 0304-4149. doi: https://doi.org/10.1016/S0304-4149(02)00150-3. URL https:
//www.sciencedirect.com/science/article/pii/S0304414902001503."
REFERENCES,0.7844827586206896,"D. A. McAllester. Some pac-bayesian theorems. Machine Learning, 37(3):355–363, 1999."
REFERENCES,0.7887931034482759,"S. Mei, T. Misiakiewicz, and A. Montanari. Mean-field theory of two-layers neural networks:
dimension-free bounds and kernel limit. In Conference on Learning Theory, pages 2388–2464.
PMLR, 2019."
REFERENCES,0.7931034482758621,"C. Mingard, G. Valle-Pérez, J. Skalse, and A. A. Louis. Is sgd a bayesian sampler? well, almost.
Journal of Machine Learning Research, 22, 2021."
REFERENCES,0.7974137931034483,"M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of machine learning. MIT press, 2018."
REFERENCES,0.8017241379310345,"A. Montanari and Y. Zhong. The interpolation phase transition in neural networks: Memorization
and generalization under lazy training. arXiv preprint arXiv:2007.12826, 2020."
REFERENCES,0.8060344827586207,"E. Moulines and F. Bach.
Non-asymptotic analysis of stochastic approximation algorithms
for machine learning.
In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Wein-
berger, editors, Advances in Neural Information Processing Systems, volume 24. Curran
Associates, Inc., 2011.
URL https://proceedings.neurips.cc/paper/2011/file/
40008b9a5380fcacce3976bf7c08af5b-Paper.pdf."
REFERENCES,0.8103448275862069,"D. Needell, R. Ward, and N. Srebro. Stochastic gradient descent, weighted sampling, and the
randomized kaczmarz algorithm. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and
K. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Cur-
ran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper/2014/file/
f29c21d4897f78948b91f03172341b7b-Paper.pdf."
REFERENCES,0.8146551724137931,"Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003."
REFERENCES,0.8189655172413793,"B. Neyshabur, S. Bhojanapalli, and N. Srebro. A pac-bayesian approach to spectrally-normalized
margin bounds for neural networks. In International Conference on Learning Representations,
2018."
REFERENCES,0.8232758620689655,"Q. Nguyen, M. C. Mukkamala, and M. Hein. On the loss landscape of a class of deep neural networks
with no bad local valleys. In International Conference on Learning Representations, 2018."
REFERENCES,0.8275862068965517,"K. Pitas. Dissecting non-vacuous generalization bounds based on the mean-field approximation. In
International Conference on Machine Learning, pages 7739–7749. PMLR, 2020."
REFERENCES,0.8318965517241379,"T. Quail, A. Shrier, and L. Glass. Predicting the onset of period-doubling bifurcations in noisy car-
diac systems. Proceedings of the National Academy of Sciences, 112(30):9358–9363, 2015.
doi: 10.1073/pnas.1424320112.
URL https://www.pnas.org/doi/abs/10.1073/pnas.
1424320112."
REFERENCES,0.8362068965517241,"M. Raginsky, A. Rakhlin, and M. Telgarsky. Non-convex learning via stochastic gradient langevin
dynamics: a nonasymptotic analysis. In Conference on Learning Theory, pages 1674–1703. PMLR,
2017."
REFERENCES,0.8405172413793104,"A. Rakhlin. Applications of empirical processes in learning theory: algorithmic stability and
generalization bounds. PhD thesis, Massachusetts Institute of Technology, 2006."
REFERENCES,0.8448275862068966,"D. Rudolf and N. Schweizer. Perturbation theory for Markov chains via Wasserstein distance.
Bernoulli, 24(4A):2610 – 2639, 2018. doi: 10.3150/17-BEJ938. URL https://doi.org/10.
3150/17-BEJ938."
REFERENCES,0.8491379310344828,"I. Safran and O. Shamir. On the quality of the initial basin in overspecified neural networks. In
International Conference on Machine Learning, pages 774–782. PMLR, 2016."
REFERENCES,0.853448275862069,"L. Sagun, L. Bottou, and Y. LeCun. Eigenvalues of the hessian in deep learning: Singularity and
beyond. arXiv preprint arXiv:1611.07476, 2016."
REFERENCES,0.8577586206896551,"L. Sagun, U. Evci, V. U. Guney, Y. Dauphin, and L. Bottou. Empirical analysis of the hessian of
over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017."
REFERENCES,0.8620689655172413,"A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning
in deep linear neural networks, 2013. URL https://arxiv.org/abs/1312.6120."
REFERENCES,0.8663793103448276,"J. Sirignano and K. Spiliopoulos. Mean field analysis of deep neural networks. Mathematics of
Operations Research, 47(1):120–152, 2022."
REFERENCES,0.8706896551724138,"S. L. Smith and Q. V. Le. A bayesian perspective on generalization and stochastic gradient descent.
In International Conference on Learning Representations, 2018."
REFERENCES,0.875,"J. Sokoli´c, R. Giryes, G. Sapiro, and M. R. Rodrigues. Robust large margin deep neural networks.
IEEE Transactions on Signal Processing, 65(16):4265–4280, 2017."
REFERENCES,0.8793103448275862,"D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient
descent on separable data. The Journal of Machine Learning Research, 19(1):2822–2878, 2018."
REFERENCES,0.8836206896551724,"L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine learning research,
9(11), 2008."
REFERENCES,0.8879310344827587,"V. N. Vapnik. An overview of statistical learning theory. IEEE transactions on neural networks, 10
(5):988–999, 1999."
REFERENCES,0.8922413793103449,"L. Venturi, A. S. Bandeira, and J. Bruna. Spurious valleys in one-hidden-layer neural network
optimization landscapes. Journal of Machine Learning Research, 20:133, 2019."
REFERENCES,0.896551724137931,"U. von Luxburg and O. Bousquet. Distance-based classification with lipschitz functions. J. Mach.
Learn. Res., 5:669–695, 2004."
REFERENCES,0.9008620689655172,"Y. Wang, M. Chen, T. Zhao, and M. Tao. Large learning rate tames homogeneity: Convergence and
balancing effect. 2021. doi: 10.48550/ARXIV.2110.03677. URL https://arxiv.org/abs/
2110.03677."
REFERENCES,0.9051724137931034,"A. Wilkinson. What are lyapunov exponents, and why are they interesting? Bulletin of the American
Mathematical Society, 54(1):79–105, 2017."
REFERENCES,0.9094827586206896,"M. O. Williams, C. W. Rowley, and I. G. Kevrekidis. A kernel-based approach to data-driven
koopman spectral analysis. arXiv preprint arXiv:1411.2260, 2014."
REFERENCES,0.9137931034482759,"S. Wojtowytsch. Stochastic gradient descent with noise of machine learning type. part i: Discrete
time analysis, 2021. URL https://arxiv.org/abs/2105.01650."
REFERENCES,0.9181034482758621,"H. Xu and S. Mannor. Robustness and generalization. Machine learning, 86(3):391–423, 2012."
REFERENCES,0.9224137931034483,"C. Zhang, Q. Liao, A. Rakhlin, B. Miranda, N. Golowich, and T. Poggio. Theory of deep learning iib:
Optimization properties of sgd. arXiv preprint arXiv:1801.02254, 2018."
REFERENCES,0.9267241379310345,"C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning (still) requires
rethinking generalization. Communications of the ACM, 64(3):107–115, 2021a."
REFERENCES,0.9310344827586207,"J. Zhang, H. Li, S. Sra, and A. Jadbabaie. Neural network weights do not converge to stationary
points: An invariant measure perspective. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari,
G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference on Machine
Learning, volume 162 of Proceedings of Machine Learning Research, pages 26330–26346. PMLR,
17–23 Jul 2022. URL https://proceedings.mlr.press/v162/zhang22q.html."
REFERENCES,0.9353448275862069,"Y. Zhang, W. Zhang, S. Bald, V. Pingali, C. Chen, and M. Goswami. Stability of sgd: Tightness
analysis and improved bounds. arXiv preprint arXiv:2102.05274, 2021b."
REFERENCES,0.9396551724137931,"L. Zhao, D. Tang, F. Lin, and B. Zhao. Observation of period-doubling bifurcations in a femtosecond
fiber soliton laser with dispersion management cavity. Optics express, 12(19):4573–4578, 2004."
REFERENCES,0.9439655172413793,"W. Zhou, V. Veitch, M. Austern, R. P. Adams, and P. Orbanz. Non-vacuous generalization bounds
at the imagenet scale: a PAC-bayesian compression approach. In International Conference on
Learning Representations, 2019. URL https://openreview.net/forum?id=BJgqqsAct7."
REFERENCES,0.9482758620689655,"R. Zwanzig. Nonequilibrium statistical mechanics. Oxford university press, 2001."
REFERENCES,0.9525862068965517,Checklist
REFERENCES,0.9568965517241379,1. For all authors...
REFERENCES,0.9612068965517241,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes]"
REFERENCES,0.9655172413793104,"(c) Did you discuss any potential negative societal impacts of your work? [Yes]
(d) Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]"
REFERENCES,0.9698275862068966,2. If you are including theoretical results...
REFERENCES,0.9741379310344828,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
(b) Did you include complete proofs of all theoretical results? [Yes]
3. If you ran experiments..."
REFERENCES,0.978448275862069,"(a) Did you include the code, data, and instructions needed to reproduce the main ex-
perimental results (either in the supplemental material or as a URL)? [Yes] In the
supplementary material
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes] see section 5 and Appendix D
(c) Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [Yes] See section 5 and Appendix D
(d) Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [No]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets..."
REFERENCES,0.9827586206896551,"(a) If your work uses existing assets, did you cite the creators? [N/A]
(b) Did you mention the license of the assets? [N/A]"
REFERENCES,0.9870689655172413,(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
REFERENCES,0.9913793103448276,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identifiable
information or offensive content? [Yes]
5. If you used crowdsourcing or conducted research with human subjects..."
REFERENCES,0.9956896551724138,"(a) Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]"
