Section,Section Appearance Order,Paragraph
FAIR AT META,0.0,"1FAIR at Meta
2Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, France"
MCGILL UNIVERSITY,0.0023148148148148147,"3McGill University
4Mila, Quebec AI institute
5Canada CIFAR AI chair"
ABSTRACT,0.004629629629629629,Abstract
ABSTRACT,0.006944444444444444,"Large-scale training of latent diffusion models (LDMs) has enabled unprecedented
quality in image generation. However, the key components of the best performing
LDM training recipes are oftentimes not available to the research community,
preventing apple-to-apple comparisons and hindering the validation of progress
in the field. In this work, we perform an in-depth study of LDM training recipes
focusing on the performance of models and their training efficiency. To ensure
apple-to-apple comparisons, we re-implement five previously published models
with their corresponding recipes. Through our study, we explore the effects of
(i) the mechanisms used to condition the generative model on semantic information
(e.g., text prompt) and control metadata (e.g., crop size, random flip flag, etc.) on the
model performance, and (ii) the transfer of the representations learned on smaller
and lower-resolution datasets to larger ones on the training efficiency and model
performance. We then propose a novel conditioning mechanism that disentangles
semantic and control metadata conditionings and sets a new state-of-the-art in class-
conditional generation on the ImageNet-1k dataset – with FID improvements of
7% on 256 and 8% on 512 resolutions – as well as text-to-image generation on the
CC12M dataset – with FID improvements of 8% on 256 and 23% on 512 resolution."
INTRODUCTION,0.009259259259259259,"1
Introduction"
INTRODUCTION,0.011574074074074073,"Diffusion models have emerged as a powerful class of generative models and demonstrated unprece-
dented ability at generating high-quality and realistic images. Their superior performance is evident
across a spectrum of applications, encompassing image [7, 14, 39, 41] and video synthesis [35],
denoising [52], super-resolution [49] and layout-to-image synthesis [51]. The fundamental principle
underpinning diffusion models is the iterative denoising of an initial sample from a trivial prior distri-
bution, that progressively transforms it to a sample from the target distribution. The popularity of
diffusion models can be attributed to several factors. First, they offer a simple yet effective approach
for generative modeling, often outperforming traditional approaches such as Generative Adversarial
Networks (GANs) [3, 16, 24, 25] and Variational Autoencoders (VAEs) [29, 48] in terms of visual
fidelity and sample diversity. Second, diffusion models are generally more stable and less prone to
mode collapse compared to GANs, which are notoriously difficult to stabilize without careful tuning
of hyperparameters and training procedures [23, 30]."
INTRODUCTION,0.013888888888888888,"Funny realistic picture of a cat 
dressed in a suit counting dollars"
INTRODUCTION,0.016203703703703703,Husky puppies sleeping under
INTRODUCTION,0.018518518518518517,a blanket
INTRODUCTION,0.020833333333333332,"A car goes by on an icy road
a lion enjoys meat frozen in a block"
INTRODUCTION,0.023148148148148147,of ice during hot weather at a zoo
INTRODUCTION,0.02546296296296296,A small cat sitting in a
INTRODUCTION,0.027777777777777776,"windowsill
a starry night style painting"
INTRODUCTION,0.03009259259259259,"of the eiffel tower
A dancing human skeleton"
INTRODUCTION,0.032407407407407406,wearing a magician's hat
INTRODUCTION,0.034722222222222224,A beautiful statue of a horse
INTRODUCTION,0.037037037037037035,made of bronze
INTRODUCTION,0.03935185185185185,Figure 1: Qualitative examples. Images generated using our model trained on CC12M at 512 resolution.
INTRODUCTION,0.041666666666666664,"Despite the success of diffusion models, training such models at scale remains computationally
challenging, leading to a lack of insights on the most effective training strategies. Training recipes
of large-scale models are often closed (e.g., DALL-E, Imagen, Midjourney), and only a few studies
have analyzed training dynamics in detail [7, 14, 26, 27]. Moreover, evaluation often involves human
studies which are easily biased and hard to replicate [17, 56]. Due to the high computational costs,
the research community mostly focused on the finetuning of large text-to-image models for different
downstream tasks [1, 4, 54] and efficient sampling techniques [34, 36, 45]. However, there has
been less focus on ablating different mechanisms to condition on user inputs such as text prompts,
and strategies to pre-train using datasets of smaller resolution and/or data size. The benefits of
conditioning mechanisms are two-fold: allowing users to have better control over the content that
is being generated, and unlocking training on augmented or lower quality data by for example
conditioning on the original image size [39] and other metadata of the data augmentation. Improving
pre-training strategies, on the other hand, can allow for big cuts in the training cost of diffusion
models by significantly reducing the number of iterations necessary for convergence."
INTRODUCTION,0.04398148148148148,"Our work aims to disambiguate some of these design choices, and provide a set of guidelines that
enable the scaling of the training of diffusion models in an efficient and effective manner. Beyond
the main architectural choices (e.g., Unet vs. ViT), we focus on two other important aspects for
generative performance and efficiency of training. First, we enhance conditioning by decoupling
different conditionings based on their type: control metadata conditioning (e.g., crop size, random
flip, etc.), semantic-level conditioning based on class names or text-prompts. In this manner, we
disentangle the contribution of each conditioning and avoid undesired interference among them.
Second, we optimize the scaling strategy to larger dataset sizes and higher resolution by studying
the influence of the initialization of the model with weights from models pre-trained on smaller
datasets and resolutions. Here, we propose three improvements needed to seamlessly transition across
resolutions: interpolation of the positional embeddings, scaling of the noise schedule, and using a
more aggressive data augmentation strategy."
INTRODUCTION,0.046296296296296294,"In our experiments we evaluate models at 256 and 512 resolution on ImageNet-1k and Conceptual
Captions (CC12M), and also present results for ImageNet-22k at 256 resolution. We study the
following five architectures: Unet/LDM-G4 [39], DiT-XL2 w/ LN [38], mDT-v2-XL/2 w/ LN [15],
PixArt-α-XL/2, and mmDiT-XL/2 (SD3) [14]. We find that among the studied base architectures,
mmDiT-XL/2 (SD3) performs the best. Our improved conditioning approach further boosts the
performance of the best model consistently across metrics, resolutions, and datasets. In particular,
we improve the previous state-of-the-art DiT result of 3.04 FID on ImageNet-1k at 512 resolution
to 2.76. For CC12M at 512 resolution, we improve FID of 11.24 to 8.64 when using our improved
conditioning, while also obtaining a (small) improvement in CLIPscore from 26.01 to 26.17. See
Fig. 1 for qualitative examples of our model trained on CC12M."
INTRODUCTION,0.04861111111111111,"64
128
256
512
1024
Figure 2: Influence of control condi-
tions. Images generated using the same
latent sample. Top: Model trained with
constant weighting of the size condition-
ing as used in SDXL [39], introducing un-
desirable correlations between image con-
tent and size condition. Bottom: Model
trained using our cosine weighting of low-
level conditioning, disentangling the size
condition from the image content."
INTRODUCTION,0.05092592592592592,"In summary, our contributions are the following:
• We present a systematic study of five different diffusion architectures, which we train from scratch
using face-blurred ImageNet and CC12M datasets at 256 and 512 resolutions.
• We introduce a conditioning mechanism that disentangles different control conditionings and
semantic-level conditioning, improving generation and avoiding interference between conditions.
• To transfer weights from pre-trained models we propose to interpolate positional embeddings, scale
the noise schedule, and use stronger data augmentation, leading to improved performance.
• We obtain state-of-the-art results at 256 and 512 resolution for class-conditional generation on
ImageNet-1k and text-to-image generation and CC12M."
CONDITIONING AND PRE-TRAINING STRATEGIES FOR DIFFUSION MODELS,0.05324074074074074,"2
Conditioning and pre-training strategies for diffusion models"
CONDITIONING AND PRE-TRAINING STRATEGIES FOR DIFFUSION MODELS,0.05555555555555555,"In this section, we review and analyze the conditioning mechanisms and pre-training strategies
used in prior work (see more detailed discussion of related work in App. A), and propose improved
approaches based on the analysis."
CONDITIONING MECHANISMS,0.05787037037037037,"2.1
Conditioning mechanisms"
CONDITIONING MECHANISMS,0.06018518518518518,"Background. To control the generated content, diffusion models are usually conditioned on class
labels or text prompts. Adaptive layer norm is a lightweight solution to condition on class labels, used
for both UNets [21, 39, 41] and DiT models [38]. Cross-attention is used to allow more fine-grained
conditioning on textual prompts, where particular regions of the sampled image are affected only by
part of the prompt, see e.g. [41]. More recently, another attention based conditioning was proposed
in SD3 [14] within a transformer-based architecture that evolves both the visual and textual tokens
across layers. It concatenates the image and text tokens across the sequence dimension, and then
performs a self-attention operation on the combined sequence. Because of the difference between the
two modalities, the keys and queries are normalized using RMSNorm [53], which stabilizes training.
This enables complex interactions between the two modalities in one attention block instead of using
both self-attention and cross-attention blocks."
CONDITIONING MECHANISMS,0.0625,"Moreover, since generative models aim to learn the distribution of the training data, data quality is
important when training generative models. Having low quality training samples, such as the ones
that are poorly cropped or have unnatural aspect ratios, can result in low quality generations. Previous
work tackles this problem by careful data curation and fine-tuning on high quality data, see e.g. [7, 9].
However, strictly filtering the training data may deprive the model from large portions of the available
data [39], and collecting high-quality data is not trivial. Rather than treating them as nuisance factors,
SDXL [39] proposes an alternative solution where a UNet-based model is conditioned on parameters
corresponding to image size and crop parameters during training. In this manner, the model is aware
of these parameters and can account for them during training, while also offering users control over
these parameters during inference. These control conditions are transformed and additively combined
with the timestep embedding before feeding them to the diffusion model."
CONDITIONING MECHANISMS,0.06481481481481481,"Disentangled control conditions. Straightforward implementation of control conditions in DiT may
cause interference between the time-step, class-level and control conditions if their corresponding
embeddings are additively combined in the adaptive layer norm conditioning, e.g. causing changes in
high-level content of the generated image when modifying its resolution, see Fig. 2. To disentangle
the different conditions, we propose two modifications. First, we move the class embedding to be fed
through the attention layers present in the DiT blocks. Second, to ensure that the control embedding"
CONDITIONING MECHANISMS,0.06712962962962964,"does not overpower the timestep embedding when additively combined in the adaptive layer norm,
we zero out the control embedding in early denoising steps, and gradually increase its strength."
CONDITIONING MECHANISMS,0.06944444444444445,"0
250
500
750
1000
timestep t 0.0 0.2 0.4 0.6 0.8 1.0"
CONDITIONING MECHANISMS,0.07175925925925926,"γc(t) =

1−cos
 
π ·(1−t)α.
2"
CONDITIONING MECHANISMS,0.07407407407407407,"α = 1
α = 2
α = 4
α = 8"
CONDITIONING MECHANISMS,0.0763888888888889,"Figure 3: Weighting of low-level
control conditions. The weight
is zeroed out early on when im-
age semantics are defined, and in-
creased later when adding details."
CONDITIONING MECHANISMS,0.0787037037037037,"Control conditions can be used for different types of data augmenta-
tions: (i) high-level augmentations (ϕh) that affect the image compo-
sition – e.g. flipping, cropping and aspect ratio –, and (ii) low-level
augmentations (ϕl) that affect low-level details – e.g. image resolu-
tion and color. Intuitively, high-level augmentations should impact
the image formation process early on, while low-level augmenta-
tions should enter the process only once sufficient image details are
present. We achieve this by scaling the contribution of the low-level
augmentations, ϕl, to the control embedding using a cosine schedule
that downweights earlier contributions:"
CONDITIONING MECHANISMS,0.08101851851851852,"cemb(ϕh, ϕl, t) = Eh(ϕh) + γc(t) · El(ϕl),
(1)"
CONDITIONING MECHANISMS,0.08333333333333333,"where the embedding functions Eh, El are made of sinusoidal em-
beddings followed by a 2-layer MLP with SiLU activation, and
where γc is the cosine schedule illustrated in Fig. 3."
CONDITIONING MECHANISMS,0.08564814814814815,"Improved text conditioning. Most commonly used text encoders, like CLIP [40], output a constant
number of tokens T that are fed to the denoising model (usually T = 77). Consequently, when
the prompt has less than T tokens, the remaining positions are filled by zero-padding, but remain
accessible via cross-attention to the denoising network. To make better use of the conditioning vector,
we propose a noisy replicate padding mechanism where the padding tokens are replaced with copies
of the text tokens, thereby pushing the subsequent cross-attention layers to attend to all the tokens in
its inputs. As this might lead to redundant token embeddings, we improve the diversity of the feature
representation across the sequence dimension, by perturbing the embeddings with additive Gaussian
noise with a small variance βtxt. To ensure enough diversity in the token embeddings, we scale the
additive noise by σ(ϕtxt)√m −1, where m is the number of token replications needed for padding,
and σ(ϕtxt) is the per-channel standard deviation in the token embeddings."
CONDITIONING MECHANISMS,0.08796296296296297,"Integrating classifier-free guidance. Classifier-free guidance (CFG) [20] allows for training con-
ditional models by combining the output of the uncoditional generation with the output of the
conditional generation. Formally, given a latent diffusion model trained to predict the noise ϵ, CFG
reads as: ϵλ = λ · ϵs + (1 −λ) · ϵ∅, where ϵ∅is the uncoditional noise prediction, ϵs is the noise pre-
diction conditioned on the semantic conditioning s (e.g., text prompt), and λ is the hyper-parameter,
known as guidance scale, which regulates the strength of the conditioning. Importantly, during
training λ is set alternatively to 0 or 1, while at inference time it is arbitrarily changed in order to steer
the generation to be more or less consistent with the conditioning. In our case, we propose the control
conditioning to be an auxiliary guidance term, in order to separately regulate the strength of the
conditioning on the control variables c and semantic conditioning s at inference time. In particular,
we define the guided noise estimate as:"
CONDITIONING MECHANISMS,0.09027777777777778,"ϵλ,β = λ

β · ϵc,s + (1 −β) · ϵ∅,s

+ (1 −λ) · ϵ∅,∅,
(2)"
CONDITIONING MECHANISMS,0.09259259259259259,"where β sets the strength of the control guidance, and λ sets the strength of the semantic guidance."
ON TRANSFERRING MODELS PRE-TRAINED ON DIFFERENT DATASETS AND RESOLUTIONS,0.09490740740740741,"2.2
On transferring models pre-trained on different datasets and resolutions"
ON TRANSFERRING MODELS PRE-TRAINED ON DIFFERENT DATASETS AND RESOLUTIONS,0.09722222222222222,"Background.
Transfer learning has been a pillar of the deep learning community, enabling
generalization to different domains and the emergence of foundational models such as DINO [5, 10]
and CLIP [40]. Large pre-trained text-to-image diffusion models have also been re-purposed for
different tasks, including image compression [4] and spatially-guided image generation [1]. Here,
we are interested in understanding to which extent pre-training on other datasets and resolutions
can be leveraged to achieve a more efficient training of large text-to-image models. Indeed, training
diffusion models directly to generate high resolution images is computationally demanding, therefore,
it is common to either couple them with super-resolution models, see e.g. [44], or fine-tune them
with high resolution data, see e.g. [7, 14, 39]. Although most models can directly operate at a higher
resolution than the one used for training, fine-tuning is important to adjust the model to the different
statistics of high-resolution images. In particular, we find that the different statistics influence the
positional embedding of patches, the noise schedule, and the optimal guidance scale. Therefore,
we focus on improving the transferability of these components."
ON TRANSFERRING MODELS PRE-TRAINED ON DIFFERENT DATASETS AND RESOLUTIONS,0.09953703703703703,"Positional Embedding. Adapting to a higher resolution can be done in different ways. Interpolation
scales the – most often learnable – embeddings according to the new resolution [2, 47]. Extrapolation
simply replicates the embeddings of the original resolution to higher resolutions as illustrated in Fig. 4,
resulting in a mismatch between the positional embeddings and the image features when switching to
different resolutions. Most methods that use interpolation of learnable positional embeddings, e.g.
[2, 47], adopt either bicubic or bilinear interpolation to avoid the norm reduction associated with
the interpolation. In our case, we take advantage of the fact that our embeddings are sinusoidal and
simply adjust the sampling grid to have constants limit under every resolution, see App. C."
ON TRANSFERRING MODELS PRE-TRAINED ON DIFFERENT DATASETS AND RESOLUTIONS,0.10185185185185185,Low res.
ON TRANSFERRING MODELS PRE-TRAINED ON DIFFERENT DATASETS AND RESOLUTIONS,0.10416666666666667,"High res.
(extrapolated)"
ON TRANSFERRING MODELS PRE-TRAINED ON DIFFERENT DATASETS AND RESOLUTIONS,0.10648148148148148,"High res.
(interpolated)"
ON TRANSFERRING MODELS PRE-TRAINED ON DIFFERENT DATASETS AND RESOLUTIONS,0.1087962962962963,"Figure 4: Interpolation and extrapo-
lation of positional embeddings."
ON TRANSFERRING MODELS PRE-TRAINED ON DIFFERENT DATASETS AND RESOLUTIONS,0.1111111111111111,"Scaling the noise schedule. At higher resolution, the amount
of noise necessary to mask objects at the same rate changes [14,
22]. If we observe a spatial patch at low resolution under a
given uncertainty, upscaling the image by a factor s creates
s2 observations of this patch of the form y(i)
t
= xt + σtϵ(i) –
assuming the value of the patch is constant across the patch.
This increase in the number of observations reduces the un-
certainty around the value of that token, resulting in a higher
signal-to-noise (SNR) ratio than expected. This issue gets further accentuated when the scheduler
does not reach a terminal state with pure noise during training, i.e., a zero SNR [32], as the mismatch
between the non-zero SNR seen during training and the purely Gaussian initial state of the sampling
phase becomes significant. To resolve this, we scale the noise scheduler in order to recover the same
uncertainty for the same timestep.
Proposition 1. When going from a scale of s to a scale s′, we update the β scheduler according to
the following rule"
ON TRANSFERRING MODELS PRE-TRAINED ON DIFFERENT DATASETS AND RESOLUTIONS,0.11342592592592593,"¯αt′ =
s2 · ¯αt
s′2 + ¯αt · (s2 −s′2)
(3)"
ON TRANSFERRING MODELS PRE-TRAINED ON DIFFERENT DATASETS AND RESOLUTIONS,0.11574074074074074,"This increases the noise amplitude during intermediate denoising steps as illustrated in Fig. A1.
The final equation obtained is similar to the one obtained in [14] with the accompanying change of
variable t =
σ2
1+σ2 ."
ON TRANSFERRING MODELS PRE-TRAINED ON DIFFERENT DATASETS AND RESOLUTIONS,0.11805555555555555,"Global 
context"
ON TRANSFERRING MODELS PRE-TRAINED ON DIFFERENT DATASETS AND RESOLUTIONS,0.12037037037037036,"Target 
resolution vs."
ON TRANSFERRING MODELS PRE-TRAINED ON DIFFERENT DATASETS AND RESOLUTIONS,0.12268518518518519,"Figure 5: Low-resolution pre-
training. Crop size used for pre-
training impacts finetuning."
ON TRANSFERRING MODELS PRE-TRAINED ON DIFFERENT DATASETS AND RESOLUTIONS,0.125,"Pre-training cropping strategies. When pre-training and finetuning
at different resolutions, we can either first crop and then resize the
crops according to the training resolution, or directly take differently
sized crops from the training images. Using a different resizing
during pre-training and finetuning may introduce some distribution
shift, while using crops of different sizes may be detrimental to
low-resolution training as the model will learn the distribution of
smaller crops rather than full images, see Fig. 5. We experimentally
investigate which strategy is more effective for low-resolution pre-
training of high-resolution models."
ON TRANSFERRING MODELS PRE-TRAINED ON DIFFERENT DATASETS AND RESOLUTIONS,0.12731481481481483,"Guidance scale. We discover that the optimal guidance scale for both FID and CLIPScore varies
with the resolution of images. In App. D, we present a proof revealing that under certain conditions,
the optimal guidance scale adheres to a scaling law with respect to the resolution, as
λ′(s) = 1 + s · (λ −1).
(4)"
EXPERIMENTAL EVALUATION,0.12962962962962962,"3
Experimental evaluation"
EXPERIMENTAL SETUP,0.13194444444444445,"3.1
Experimental setup"
EXPERIMENTAL SETUP,0.13425925925925927,"Datasets. In our study, we train models on three datasets. To train class-conditional models, we use
ImageNet-1k [11], which has 1.3M images spanning 1,000 classes, as well as ImageNet-22k [43],
which contains 14.2M images spanning 21,841 classes. Additionally, we train text-to-image models
using Conceptual 12M (CC12M) [6], which contains 12M images with accompanying manually
generated textual descriptions. We pre-process both datasets by blurring all faces. Differently
from [7], we use the original captions for the CC12M dataset."
EXPERIMENTAL SETUP,0.13657407407407407,"Evaluation. For image quality, we evaluate our models using the common FID [19] metric. We
follow the standard evaluation protocol on ImageNet to have a fair comparison with the relevant"
EXPERIMENTAL SETUP,0.1388888888888889,"Table 1: Comparison between different model architectures. We compare results reported in the literature
(top, reporting available numbers) with our reimplementations of existing architectures (middle), and to our best
results obtained using architectural refinements and improved training. For 512 resolution, we trained models by
fine-tuning models pre-trained at 256 resolution. In each column, we bold the best results among those in the
first two blocks, and also those in the last row when they are equivalent or superior. ‘—’ denotes that numbers
are unavailable in the original papers, or architectures are incompatible with text-to-image generation in our
experiments. ‘✗’ indicates diverged runs. ‘*’ is used for Esser et al. [14] pre-trained on CC12M to denote that
FID is computed differently and some details about their evaluation are unclear."
EXPERIMENTAL SETUP,0.1412037037037037,"ImageNet-1k
CC12M"
EXPERIMENTAL SETUP,0.14351851851851852,"256
512
256
512"
EXPERIMENTAL SETUP,0.14583333333333334,"FIDtrain ↓
FIDtrain ↓
FIDval ↓
CLIPCOCO ↑
FIDval ↓
FIDCOCO ↓
CLIPCOCO ↑"
EXPERIMENTAL SETUP,0.14814814814814814,"Results taken from references
UNet (SD/LDM-G4) [39]
3.60
—
17.01
24
—
9.62
—
DiT-XL2 w/ LN [38]
2.27
3.04
—
—
—
—
—
mDT-v2-XL/2 w/ LN [15]
1.79
—
—
—
—
—
—
PixArt-α-XL/2 [7]
—
—
—
—
—
10.65
—
mmDiT-XL/2 (SD3) [14]
—
—
—
22.4
—
*
—"
EXPERIMENTAL SETUP,0.15046296296296297,"Our re-implementation of existing architectures
UNet (SDXL)
2.05
4.81
8.53
25.36
12.56
7.26
24.79
DiT-XL/2 w/ LN
1.95
2.85
—
—
—
—
—
DiT-XL/2 w/ Att
1.71
✗
✗
✗
✗
✗
✗
mDT-v2-XL/2 w/ LN
2.51
3.75
—
—
—
—
—
PixArt-α-XL/2
2.06
3.05
✗
✗
✗
✗
✗
mmDiT-XL/2 (SD3)
1.71
3.02
7.54
24.78
11.24
6.78
26.01"
EXPERIMENTAL SETUP,0.1527777777777778,"Our improved architecture and training
mmDiT-XL/2 (ours)
1.59
2.76
6.79
26.60
6.27
6.69
26.17"
EXPERIMENTAL SETUP,0.1550925925925926,"literature [3, 15, 38, 41]. Specifically, we compute the FID between the full training set and 50k
synthetic samples generated using 250 DDIM sampling steps. For image-text alignment, we compute
the CLIP [40] score similarly to [7, 14]. We measure conditional diversity, either using class-level
or text prompt conditioning, using LPIPS [55]. LPIPS is measured pairwise and averaged among
ten generations obtained with the same random seed, prompt, and initial noise, but different size
conditioning (we exclude sizes smaller than the target resolution); then we report the average over
10k prompts. In addition to ImageNet and CC12M evaluations, we provide FID and CLIPScore on
the COCO [33] validation set, which contains approximately 40k images with associated captions.
For COCO evaluation [33], we follow the same setting as [14] for computing the CLIP score, using
25 sampling steps and a guidance scale of 5.0."
EXPERIMENTAL SETUP,0.1574074074074074,"Training. To train our models we use the Adam [28] optimizer, with a learning rate of 10−4 and
β1, β2 = 0.9, 0.999. When training at 256×256 resolution, we use a batch size of 2, 048 images, a
constant learning rate of 10 × 10−4, train our models on two machines with eight A100 GPUs each.
In preliminary experiments with the DiT architecture we found that the FID metric on ImageNet-1k
at 256 resolution consistently improved with larger batches and learning rate, but that increasing the
learning rate by another factor of two led to diverging runs. We report these results in supplementary.
When training models at 512×512 resolution, we use the same approach but with a batch size of 384
distributed over 16 A100 GPUs. We train our ImageNet-1k models for 500k to 1M iterations and for
300k to 500k iterations for CC12M."
EXPERIMENTAL SETUP,0.1597222222222222,"Model architectures. We train different diffusion architectures under the same setting to provide a fair
comparison between model architectures. Specifically, we re-implement a UNet-based architecture
following Stable Diffusion XL (SDXL) [39] 1 and several transformer-based architectures: vanilla
DiT [38], masked DiT (mDiT-v2) [15], PixArt DiT (PixArt-α) [7], and multimodal DiT (mmDiT) as
in Stable Diffusion 3 [14]. For vanilla DiT, which only supports class-conditional generation, we
explore two variants one incorporating the class conditioning within LayerNorm and another one
within the attention layer. Also, for text-conditioned models, we use the text encoder and tokenizer
of CLIP (ViT-L/14) [40] having a maximum sequence length of T = 77. The final models share
similar number parameters, e.g. for DiTs we inspect the XL/2 variant [38], for UNet (SDXL) we
adopt similar size to the original LDM [41]. Similar to [14], we found the training of DiT with"
EXPERIMENTAL SETUP,0.16203703703703703,"1We only implement the base network, without the extra refiner as in [39]"
EXPERIMENTAL SETUP,0.16435185185185186,"Table 2: Control conditioning. We study different facets of control conditioning and their impact on the model
performance. (a-b) We report FIDtrain on ImageNet-1k@256 using 250 sampling steps. 120k training iterations."
EXPERIMENTAL SETUP,0.16666666666666666,"(a) Influence of the parametrization. LPIPS computa-
tion considers all resolutions [64, 128, 256, 512, 1024]
while LPIPS/HR exclude 64 and 128"
EXPERIMENTAL SETUP,0.16898148148148148,"Init.
t weighting
FID (↓)
LPIPS (↓)
LPIPS/HR (↓)"
EXPERIMENTAL SETUP,0.1712962962962963,"—
zero
3.29
—
—
zero.
unif.
3.08
0.33
0.210"
EXPERIMENTAL SETUP,0.1736111111111111,"zero.
cos(α = 1.0)
3.08
0.23
0.076
zero.
cos(α = 2.0)
3.09
0.18
0.045
zero.
cos(α = 4.0)
3.05
0.13
0.025
zero.
cos(α = 8.0)
3.04
0.04
0.009"
EXPERIMENTAL SETUP,0.17592592592592593,(b) Size conditioning effect on FID at inference.
EXPERIMENTAL SETUP,0.17824074074074073,"Size sampling (a, b)
baseline
α = 8.0"
EXPERIMENTAL SETUP,0.18055555555555555,"a = b = 512
4.48
4.12
a = b ∼U([512, 1024])
5.04
3.80
a, b ∼U([512, 1024])
4.51
3.90
a, b ∼Dtrain
3.08
3.04"
EXPERIMENTAL SETUP,0.18287037037037038,"(c) Influence of control conditioning. Models
trained on CC12M@256. (124k iterations)"
EXPERIMENTAL SETUP,0.18518518518518517,"Crop
R. flip
FID (↓)
CLIP (↑)"
EXPERIMENTAL SETUP,0.1875,"✓
✗
8.43
23.59
✓
✓
8.40
23.68"
EXPERIMENTAL SETUP,0.18981481481481483,"cross-attention to be unstable and had to resort to using RMSNorm [53] to normalize the key and
query in the attention layers. We detail the models sizes and computational footprints in Tab. A1."
EVALUATION OF MODEL ARCHITECTURES AND COMPARISON WITH THE STATE OF THE ART,0.19212962962962962,"3.2
Evaluation of model architectures and comparison with the state of the art"
EVALUATION OF MODEL ARCHITECTURES AND COMPARISON WITH THE STATE OF THE ART,0.19444444444444445,"In Tab. 1, we report results for models with different architectures trained at both 256 and 512
resolutions for ImageNet and CC12M, and compare our results (2nd block of rows) with those
reported in the literature, where available (1st block of rows). Where direct comparison is possible,
we notice that our re-implementation outperforms the one of existing references. Overall, we found the
mmDiT [14] architecture to perform best or second best in all settings compared to other alternatives.
For this reason, we apply our conditioning improvements on top of this architecture (last row of the
table), boosting the results as measured with FID and CLIPScore in all settings. Below, we analyse
the improvements due to our conditioning mechanisms and pre-training strategies."
CONTROL CONDITIONING,0.19675925925925927,"3.3
Control conditioning"
CONTROL CONDITIONING,0.19907407407407407,"Scheduling rate of control conditioning. In Tab. 2a we consider the effect of controlling the
conditioning on low-level augmentations via a cosine schedule for different decay rates α. We
compare to baselines (first two rows) with constant weighting (as in SDXL [39]) and without control
conditioning. We find that our cosine weighting schedule significantly reduces the dependence
between size control and image semantics as it drastically improves the instance specific LPIPS (0.33
vs. 0.04) in comparison to uniform weighting. In terms of FID, we observe a small gap with the
baseline (3.04 vs. 3.08), which increases (3.80 vs. 5.04) when computing FID by randomly sampling
the size conditioning in the range [512,1024], see Tab. 2b. Finally, the improved disentangling
between semantics and low-level conditioning is clearly visible in the qualitative samples in Fig. 2."
CONTROL CONDITIONING,0.2013888888888889,"Crop and random-flip control conditioning. A potential issue of horizontal flip data augmentations
is that it can create misalignment between the text prompt and corresponding image. For example the
prompt ""A teddy bear holding a baseball bat in their right arm"" will no longer be accurate when
an image is flipped – showing a teddy bear holding the bat in their left arm. Similarly, cropping
images can remove details mentioned in the corresponding caption. In Tab. 2c we evaluate models
trained on CC12M@256 with and without horizontal flip conditioning, and find that adding this
conditioning leads to slight improvements in both FID and CLIP as compared to using only crop
conditioing. We depict qualitative comparison in Fig. 6, where we observe that flip conditioning
improves prompt-layout consistency."
CONTROL CONDITIONING,0.2037037037037037,"Inference-time control conditioning of image size. High-level augmentations (ϕh) may affect
the image semantics. As a result they influence the learned distribution and modify the generation
diversity. For example, aspect ratio conditioning can harm the quality of generated images, when
images of a particular class or text prompt are unlikely to appear with a given aspect ratio. In Tab. 2b
we compare of different image size conditionings for inference. We find that conditioning on the same
size distribution as encountered during the training of the model yields a significant boost in FID"
CONTROL CONDITIONING,0.20601851851851852,"Flip cond
No flip cond"
CONTROL CONDITIONING,0.20833333333333334,"""a picture of a road showing a blue car on the left and a red car on the right"""
CONTROL CONDITIONING,0.21064814814814814,"Figure 6: Illustration of the
impact of flip conditioning.
Without the flip conditioning,
the model may confuse left-
right specifications. Including
flipping as a control condition
enables the model to properly
follow left-right instructions."
CONTROL CONDITIONING,0.21296296296296297,"0
1
2
3
4
5
Guidance scale 7.5 10.0 12.5"
CONTROL CONDITIONING,0.2152777777777778,FID@256
CONTROL CONDITIONING,0.2175925925925926,"0
1
2
3
4
5
Guidance scale 5 10 15"
CONTROL CONDITIONING,0.2199074074074074,FID@512
CONTROL CONDITIONING,0.2222222222222222,"0
1
2
3
4
Control guidance scale 2 4"
CONTROL CONDITIONING,0.22453703703703703,FID@256
CONTROL CONDITIONING,0.22685185185185186,"Figure 7: Guidance scales. Left + center: The optimal guidance scale varies with the image resolution. Right:
Decoupling the control guidance improves FID, the best reported performance is obtained with β = 1.375."
CONTROL CONDITIONING,0.22916666666666666,"as compared to generating all images with constant size conditioning or using uniformly randomly
sampled sizes. Note that in all cases images are generated at 256 resolution."
CONTROL CONDITIONING,0.23148148148148148,"Table 3: Text padding.
Our
noisy replication embedding vs.
baseline zero-padding. Models
trained on CC12M@256."
CONTROL CONDITIONING,0.2337962962962963,"Padding
βtxt
FID
CLIP"
CONTROL CONDITIONING,0.2361111111111111,"zero
—
7.19
26.25
replicate
0
6.93
26.47
replicate
0.02
6.79
26.60
replicate
0.05
6.82
26.58
replicate
0.1
7.01
26.47
replicate
0.2
7.02
26.41"
CONTROL CONDITIONING,0.23842592592592593,"Control conditioning and guidance. To understand how control
condition impacts the generation process, we investigate the influ-
ence of control guidance β (introduced in Sec. 2.1 ) on FID and
report the results in Fig. 7. We find that a higher control guidance
scale results in improved FID scores. However, note that this im-
provement comes at the cost of compute due to the additional control
term ϵc,s."
CONTROL CONDITIONING,0.24074074074074073,"Replication text padding.
We compare our noisy replication
padding to the baseline zero-padding in Tab. 3. We observe that
using a replication padding improves both FID and CLIP score, and
that adding scaled perturbations further improves the results – 0.35
point improvement in CLIP score and 0.4 point improvement in FID."
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.24305555555555555,"3.4
Transferring weights between datasets and resolutions"
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.24537037037037038,"Dataset shift. We evaluate the effect of pre-training on ImageNet-1k (at 256 resolution) when
training the models on CC12M or ImageNet-22k (at 512 resolution) by the time needed to achieve
the same performance as a model trained from scratch. In Tab. 4a, when comparing models trained
from scratch to ImageNet-1k pre-trained models (600k iterations) we observe two benefits: improved
training convergence and performance boosts. For CC12M, we find that after only 100k iterations,
both FID and CLIP scores improve over the baseline model trained with more than six times the"
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.24768518518518517,"Table 4: Effect of pre-training across datasets and res-
olutions. Number of (pre-)training iterations given in
thousands (k) per row. Relative improvements in FID and
CLIP score given as percentage in parenthesis."
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.25,(a) Pre-training models at 256 resolution on ImageNet-1k.
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.2523148148148148,"Pre-train
Finetune
FID
CLIP"
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.25462962962962965,"IN22k (375k)
—
5.80
—
IN1k
IN22k (80k)
5.29 (+8.67%)
—
IN1k
IN22k (110k)
4.67 (+17.82%)
—"
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.2569444444444444,"CC12M (600k)
—
7.54
24.78
IN1k
CC12M (60k)
7.59 (−0.66%)
25.09 (+1.24%)
IN1k
CC12M (100k)
7.27 (+3.71%)
25.62 (+3.43%)
IN1k
CC12M (120k)
7.25 (+3.85%)
25.69 (+3.71%)"
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.25925925925925924,"(b) Pre-training 512 resolution models on
ImageNet-22k before finetuning on CC12M."
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.26157407407407407,"Pre-train
Finetune
FIDIN22k
FIDCC12M
CLIPCC12M
200k
150k
5.80
7.15
25.79
300k
50k
5.41
7.79
25.30"
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.2638888888888889,"(c) Influence of pre-training at 256 resolution for
512 resolution models (ImageNet-1k)."
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.2662037037037037,"Model
pre-train
Finetune
FID"
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.26851851851851855,"UNet
—
1000k
6.80
mmDIT
—
500k
3.98"
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.2708333333333333,"UNet
750k
250k
4.81 (+29.51%)
mmDIT
350k
150k
3.02 (+24.12%)"
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.27314814814814814,"Figure 8: Resolution shift. Experiments are conducted on ImageNet-1k at 512 resolution, FID is reported using
50 DDIM steps with respect to the ImageNet-1k validation set."
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.27546296296296297,"25
50
75
100
125
Iteration (k) 5.00 5.25 5.50 5.75 6.00"
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.2777777777777778,FID@512
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.2800925925925926,"Default
Scaled"
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.2824074074074074,"(a) Influence of positional embed-
ding resampling on convergence."
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.2847222222222222,"0
10
20
30
40
Iteration (k) 4.0 4.5 5.0 5.5"
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.28703703703703703,FID@512
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.28935185185185186,"Default
Scaled"
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.2916666666666667,"(b) Influence of noise schedule
rescaling for training."
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.29398148148148145,"0
50
100
150
200
Iteration (k) 5.0 5.5 6.0 6.5 7.0"
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.2962962962962963,FID@512
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.2986111111111111,"Mixed
Local
Global"
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.30092592592592593,"(c) Influence of pretraining scale
on convergence."
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.30324074074074076,"amount of training iterations. For ImageNet-22k, which is closer in distribution to ImageNet-1k than
CC12M, the gains are even more significant, the finetuned model achieves an FID lower by 0.5 point
after only 80k training iterations. In Tab. 4b, we study the relative importance of pre-training vs
finetuning when the datasets have dissimilar distributions but similar sample sizes. We fix a training
budget in terms of number of training iterations N, we first train our model on ImageNet-22k for K
iterations before continuing the training on CC12M for the remaining N −K iterations. We see that
the model pretrained for 200k iterations and finetuned for 150k performs better than the one spending
the bulk of the training during pretraining phase. This validates the importance of domain specific
training and demonstrates that the bulk of the gains from the pretrained checkpoint come from the
representation learned during earlier stages."
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.3055555555555556,"Resolution change. We compare the performance boost obtained from training from scratch at 512
resolution vs. resuming from a 256-resolution trained model. According to our results in Tab. 4c,
pretraining on low resolution significantly boosts the performance at higher resolutions, both for
UNet and mmDiT, we find that higher resolution finetuning for short periods of time outperforms
high resolution training from scratch by a large margin (≈25%). These performance gains might in
part be due to the increased batch size when pre-training the 256 resolution model, which allows the
model to “see” more images as compared to training from-scratch at 512 resolution."
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.30787037037037035,"Positional Embedding. In Fig. 8a, we compare the influence of the adjustment mechanism for
the positional embedding. We find that our grid resampling approach outperforms the default
extrapolation approach, resulting in 0.2 point difference in FID after 130k training iterations."
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.3101851851851852,"Scaling the noise schedule. We conducted an evaluation to ascertain the influence of the noise
schedule by refining our mmDiT model post its low resolution training and report the results in
Fig. 8b. Remarkably, the application of the rectified schedule, for 40k iterations, resulted in an
improvement of 0.7 FID points demonstrating its efficacy at higher resolutions."
TRANSFERRING WEIGHTS BETWEEN DATASETS AND RESOLUTIONS,0.3125,"Pre-training cropping strategies. During pretraining, the model sees objects that are smaller
than what it sees during fine tuning, see Fig. 5. We aim to reduce this discrepancy by adopting
more agressive cropping during the pretraining phase. We experiment with three cropping ratios
for training: 0.9 −1 (global), 0.4 −0.6 (local), 0.4 −1 (mix). We report the results in Fig. 8c. On
ImageNet1K@256, the pretraining FID scores are 2.36, 245.55 and 2.21 for the local, global and
mixed strategies respectively. During training at 512 resolution, we observe that the global and mix
cropping strategies both outperform the local strategy. However, as reported in Fig. 8c, the local
strategy provides benefits at higher resolutions. Overall, training with the global strategy performs
the best at 256 resolution but lags behind for higher resolution adaptation. While local cropping
underperforms at lower resolutions, because it does not see any images in their totality, it outperforms
the other methods at higher resolutions – an improvement of almost 0.2 FID points is consistent after
the first 50k training steps at higher resolution."
CONCLUSION,0.3148148148148148,"4
Conclusion"
CONCLUSION,0.31712962962962965,"In this paper, we explored various approaches to enhance the conditional training of diffusion
models. Our empirical findings revealed significant improvements in the quality and control over
generated images when incorporating different coditioning mechanisms. Moreover, we conducted a"
CONCLUSION,0.3194444444444444,"comprehensive study on the transferability of these models across diverse datasets and resolutions.
Our results demonstrated that leveraging pretrained representations is a powerful tool to improve the
model performance while also cutting down the training costs. Furthermore, we provided valuable
insights into efficiently scaling up the training process for these models without compromising
performance. By adapting the schedulers and positional embeddings when scaling up the resolution,
we achieved substantial reductions in training time while boosting the quality of the generated images.
Additional experiments unveil the expected gains from different transfer strategies, making it easier
for researchers to explore new ideas and applications in this domain. In Appendix B we discuss
societal impact and limitations of our work."
REFERENCES,0.32175925925925924,References
REFERENCES,0.32407407407407407,"[1] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski,
Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable image generation. In CVPR,
2023."
REFERENCES,0.3263888888888889,"[2] Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai,
Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, and Filip Pavetic. FlexiViT: One model
for all patch sizes. In CVPR, 2023."
REFERENCES,0.3287037037037037,"[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural
image synthesis. In ICLR, 2019."
REFERENCES,0.33101851851851855,"[4] Marlène Careil, Matthew J. Muckley, Jakob Verbeek, and Stéphane Lathuilière. Towards image compres-
sion with perfect realism at ultra-low bitrates. In ICLR, 2024."
REFERENCES,0.3333333333333333,"[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand
Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021."
REFERENCES,0.33564814814814814,"[6] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale
image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021."
REFERENCES,0.33796296296296297,"[7] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok,
Ping Luo, Huchuan Lu, and Zhenguo Li. PixArt-α: Fast training of diffusion transformer for photorealistic
text-to-image synthesis. In ICLR, 2024."
REFERENCES,0.3402777777777778,"[8] Jaemin Cho, Abhay Zala, and Mohit Bansal. DALL-Eval: Probing the reasoning skills and social biases of
text-to-image generation models. In ICCV, 2023."
REFERENCES,0.3425925925925926,"[9] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon
Vandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek Kadian, Filip Radenovic, Dhruv
Mahajan, Kunpeng Li, Yue Zhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Motwani, Yi Wen, Yiwen
Song, Roshan Sumbaly, Vignesh Ramanathan, Zijian He, Peter Vajda, and Devi Parikh. Emu: Enhancing
image generation models using photogenic needles in a haystack. arXiv preprint, 2309.15807, 2023."
REFERENCES,0.3449074074074074,"[10] Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers.
In ICLR, 2024."
REFERENCES,0.3472222222222222,"[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical
image database. In ICCV, 2009."
REFERENCES,0.34953703703703703,"[12] Prafulla Dhariwal and Alex Nichol. Diffusion models beat GANs on image synthesis. In NeurIPS, 2021."
REFERENCES,0.35185185185185186,"[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and
Neil Houlsby. An image is worth 16×16 words: Transformers for image recognition at scale. In ICLR,
2021."
REFERENCES,0.3541666666666667,"[14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi,
Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey,
Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution
image synthesis. In ICML, 2024."
REFERENCES,0.35648148148148145,"[15] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a
strong image synthesizer. In ICCV, 2023."
REFERENCES,0.3587962962962963,"[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014."
REFERENCES,0.3611111111111111,"[17] Melissa Hall, Samuel J. Bell, Candace Ross, Adina Williams, Michal Drozdzal, and Adriana Romero
Soriano. Towards geographic inclusion in the evaluation of text-to-image models. In FAccT, 2024."
REFERENCES,0.36342592592592593,"[18] Melissa Hall, Candace Ross, Adina Williams, Nicolas Carion, Michal Drozdzal, and Adriana Romero-
Soriano. DIG in: Evaluating disparities in image generations with indicators for geographic diversity.
TMLR, 2024."
REFERENCES,0.36574074074074076,"[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs
trained by a two time-scale update rule converge to a local Nash equilibrium. In NeurIPS, 2017."
REFERENCES,0.3680555555555556,"[20] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshop on Deep
Generative Models and Downstream Applications, 2021."
REFERENCES,0.37037037037037035,"[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020."
REFERENCES,0.3726851851851852,"[22] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple diffusion: End-to-end diffusion for high
resolution images. In ICML, 2023."
REFERENCES,0.375,"[23] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park.
Scaling up GANs for text-to-image synthesis. In CVPR, 2023."
REFERENCES,0.3773148148148148,"[24] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and
improving the image quality of StyleGAN. In CVPR, 2020."
REFERENCES,0.37962962962962965,"[25] Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.
Alias-free generative adversarial networks. In NeurIPS, 2021."
REFERENCES,0.3819444444444444,"[26] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based
generative models. In NeurIPS, 2022."
REFERENCES,0.38425925925925924,"[27] Tero Karras, Janne Hellsten, Miika Aittala, Timo Aila, Jaakko Lehtinen, and Samuli Laine. Analyzing and
improving the training dynamics of diffusion models. In CVPR, 2024."
REFERENCES,0.38657407407407407,"[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015."
REFERENCES,0.3888888888888889,"[29] Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In ICLR, 2014."
REFERENCES,0.3912037037037037,"[30] Kwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang, Zhuowen Tu, and Ce Liu. ViTGAN: Training GANs
with vision transformers. In ICLR, 2022."
REFERENCES,0.39351851851851855,"[31] Mark Levy, Bruno Di Giorgi, Floris Weers, Angelos Katharopoulos, and Tom Nickson. Controllable music
production with diffusion models and guidance gradients. arXiv preprint, 2311.00613, 2023."
REFERENCES,0.3958333333333333,"[32] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample
steps are flawed. In WACV, 2024."
REFERENCES,0.39814814814814814,"[33] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, P. Dollár, and
C. Zitnick. Microsoft COCO: common objects in context. In ECCV, 2014."
REFERENCES,0.40046296296296297,"[34] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching
for generative modeling. In ICLR, 2023."
REFERENCES,0.4027777777777778,"[35] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang,
Hanchi Sun, Jianfeng Gao, Lifang He, and Lichao Sun. Sora: A review on background, technology,
limitations, and opportunities of large vision models. arXiv preprint, 2402.17177, 2024."
REFERENCES,0.4050925925925926,"[36] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-Solver: A fast ODE
solver for diffusion probabilistic model sampling in around 10 steps. In NeurIPS, 2022."
REFERENCES,0.4074074074074074,"[37] Alexandra Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and Yacine Jernite. Stable bias: Analyz-
ing societal representations in diffusion models. In NeurIPS, 2023."
REFERENCES,0.4097222222222222,"[38] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023."
REFERENCES,0.41203703703703703,"[39] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna,
and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In
ICLR, 2024."
REFERENCES,0.41435185185185186,"[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning
transferable visual models from natural language supervision. In ICML, 2021."
REFERENCES,0.4166666666666667,"[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
image synthesis with latent diffusion models. In CVPR, 2022."
REFERENCES,0.41898148148148145,"[42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical
image segmentation. In Medical Image Computing and Computer-Assisted Intervention, 2015."
REFERENCES,0.4212962962962963,"[43] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large
Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252,
2015."
REFERENCES,0.4236111111111111,"[44] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho,
David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language
understanding. In NeurIPS, 2022."
REFERENCES,0.42592592592592593,"[45] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021."
REFERENCES,0.42824074074074076,"[46] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
Score-based generative modeling through stochastic differential equations. In ICLR, 2021."
REFERENCES,0.4305555555555556,"[47] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé
Jégou. Training data-efficient image transformers & distillation through attention. In ICML, 2021."
REFERENCES,0.43287037037037035,"[48] A. Vahdat and J. Kautz. NVAE: A deep hierarchical variational autoencoder. In NeurIPS, 2020."
REFERENCES,0.4351851851851852,"[49] Chanyue Wu, Dong Wang, Yunpeng Bai, Hanyu Mao, Ying Li, and Qiang Shen. HSR-Diff: Hyperspectral
image super-resolution via conditional diffusion models. In ICCV, 2023."
REFERENCES,0.4375,"[50] Tong Wu, Zhihao Fan, Xiao Liu, Yeyun Gong, Yelong Shen, Jian Jiao, Hai-Tao Zheng, Juntao Li, Zhongyu
Wei, Jian Guo, Nan Duan, and Weizhu Chen. AR-Diffusion: Auto-regressive diffusion model for text
generation. In NeurIPS, 2023."
REFERENCES,0.4398148148148148,"[51] Han Xue, Zhiwu Huang, Qianru Sun, Li Song, and Wenjun Zhang. Freestyle layout-to-image synthesis. In
CVPR, 2023."
REFERENCES,0.44212962962962965,"[52] Cheng Yang, Lijing Liang, and Zhixun Su. Real-world denoising via diffusion model. arXiv preprint,
2305.04457, 2023."
REFERENCES,0.4444444444444444,"[53] Biao Zhang and Rico Sennrich. Root Mean Square Layer Normalization. In NeurIPS, 2019."
REFERENCES,0.44675925925925924,"[54] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion
models. In ICCV, 2023."
REFERENCES,0.44907407407407407,"[55] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In CVPR, 2018."
REFERENCES,0.4513888888888889,"[56] Sharon Zhou, Mitchell L. Gordon, Ranjay Krishna, Austin Narcomey, Durim Morina, and Michael S.
Bernstein. HYPE: a benchmark for Human eYe Perceptual Evaluation of generative models. In NeurIPS,
2019."
REFERENCES,0.4537037037037037,Appendix
REFERENCES,0.45601851851851855,Table of Contents
REFERENCES,0.4583333333333333,"A Related work
14"
REFERENCES,0.46064814814814814,"B
Societal impact and limitations
15"
REFERENCES,0.46296296296296297,"C Implementation details
15"
REFERENCES,0.4652777777777778,"D Derivations
18"
REFERENCES,0.4675925925925926,"E Additional results
19"
REFERENCES,0.4699074074074074,"F
NeurIPS Paper Checklist
22"
REFERENCES,0.4722222222222222,"A
Related work"
REFERENCES,0.47453703703703703,"Diffusion Models. Diffusion models have gained significant attention in recent years due to their
ability to model complex stochastic processes and generate high-quality samples. These models have
been successfully applied to a wide range of applications, including image generation [7, 21, 41],
video generation [35], music generation [31], and text generation [50]. One of the earliest diffusion
models was proposed in [21], which introduced denoising diffusion probabilistic models (DDPMs)
for image generation. This work demonstrated that DDPMs can generate high-quality images that
competitive with state-of-the-art generative models such as GANs [16]. Following this work, several
variants of DDPMs were proposed, including score-based diffusion models [46], conditional diffusion
models [12], and implicit diffusion models [45]. Overall, diffusion models have shown promising
results in various applications due to their ability to model complex stochastic processes and generate
high-quality samples [7, 14, 39, 41]. Despite their effectiveness, diffusion models also have some
limitations, including the need for a large amount of training data and the required computational
resources. Some works [26, 27] have studied and analysed the training dynamics of diffusion models,
but most of this work considers the pixel-based models and small-scale settings with limited image
resolution and dataset size. In our work we focus on the more scalable class of latent diffusion
models [41], and consider image resolutions up to 512 pixels, and 14M training images."
REFERENCES,0.47685185185185186,"Model architectures. Early work on diffusion models adopted the widely popular UNet arcchitec-
ture [39, 41]. The UNet is an encoder-decoder architecture where the encoder is made of residual
blocks that produce progressively smaller feature maps, and the decoder progressively upsamples the
feature maps and refines them using skip connections with the encoder [42]. For diffusion, UNets
are also equipped with cross attention blocks for cross-modality conditioning and adaptive layer
normalization that conditions the outputs of the model on the timestep [41]. More recently, vision
transformer architectures [13] were shown to scale more favourably than UNets for diffusion models
with the DiT architecture [38]. Numerous improvements have been proposed to the DiT in order
to have more efficient and stable training, see e.g. [7, 14, 15]. In order to reduce the computational
complexity of the model and train at larger scales, windowed attention has been proposed [7]. Latent
masking during training has been proposed to encourage better semantic understanding of inputs
in [15]. Others improved the conditioning mechanism by evolving the text tokens through the layers
of the transformer and replacing the usual cross-attention used for text conditioning with a variant
that concatenates the tokens from both the image and text modalities [14]."
REFERENCES,0.4791666666666667,"Large scale diffusion training. Latent diffusion models [41] unlocked training diffusion models at
higher resolutions and from more data by learning the diffusion model in the reduced latent space
of a (pre-trained) image autoencoder rather than directly in the image pixel space. Follow-up work
has proposed improved scaling of the architecture and data [39]. More recently, attention-based
architectures [7, 14, 15] have been adapted for large scale training, showing even more improvements"
REFERENCES,0.48148148148148145,"0
200
400
600
800
1000
timestep 0.0 0.2 0.4 0.6 0.8 1.0 t(s)"
REFERENCES,0.4837962962962963,"s=1
s=2
s=3
s=4"
REFERENCES,0.4861111111111111,"0
200
400
600
800
1000
timestep 0.0 0.2 0.4 0.6 0.8 1.0 t(s)"
REFERENCES,0.48842592592592593,"1 +
t(s)2"
REFERENCES,0.49074074074074076,"s=1
s=2
s=3
s=4"
REFERENCES,0.4930555555555556,"Figure A1: Noise schedule
scaling law. At higher resolu-
tions, keeping the same uncer-
tainty means spending more
time at higher noise levels,
thereby counteracting the un-
certainty reduction from the in-
crease in the observations for
the same patch."
REFERENCES,0.49537037037037035,"by scaling the model size further and achieving state-of-the-art performance on datasets such as
ImageNet-1k. Efficiency gains were also obtained by [7] by transferring ImageNet pre-trained models
to larger datasets."
REFERENCES,0.4976851851851852,"B
Societal impact and limitations"
REFERENCES,0.5,"Our research investigates the training of generative image models, which are widely employed to
generate content for creative or education and accessibility purposes. However, together with these
beneficial applications, these models are usually associated with privacy concerns (e.g., deepfake
generation) and misinformation spread. In our paper, we deepen the understanding of the training
dynamics of these modes, providing the community with additional knowledge that can be lever-
aged for safety mitigation. Moreover, we promote a safe and transparent/reproducible research by
employing only publicly available data, which we further mitigate by blurring human faces."
REFERENCES,0.5023148148148148,"Our work is mostly focused on training dynamics, and to facilitate reproducibility, we used publicly
available datasets and benchmarks, without applying any data filtering. We chose datasets relying on
the filtering/mitigation done by their respective original authors. In general, before releasing models
like the ones described in our paper to the public, we recommend conducting proper evaluation of
models trained using our method for bias, fairness and discrimination risks. For example, geographical
disparities due to stereotypical generations could be revealed with methods described by Hall et al.
[18], and social biases regarding gender and ethnicity could be captured with methods from Luccioni
et al. [37] and Cho et al. [8]."
REFERENCES,0.5046296296296297,"While our study provides valuable insights into control conditioning and the effectiveness of repre-
sentation transfer in diffusion models, there are several limitations that should be acknowledged. (i)
There are cases where these improvements can be less pronounced. For example, noisy replicates
for the text embeddings can become less pertinent if the model is trained exclusively with long
prompts. (ii) While low resolution pretraining with local crops on ImageNet resulted in better FID at
512 resolution (see Table 5c), it might not be necessary if pretraining on much larger datasets (e.g.
>100M samples, which we did not experiment in this work). Similarly, flip conditioning is only
pertinent if the training dataset contains position sensitive information (left vs. right in captions, or
rendered text in images), otherwise this condition will not provide any useful signal. (iii) We did not
investigate the impact of data quality on training dynamics, which could have implications for the
generalizability of our findings to datasets with varying levels of quality and diversity. (iv) As our
analysis primarily focused on control conditioning, other forms of conditioning such as timestep and
prompt conditioning were not explored in as much depth. Further research is needed to determine
the extent to which these conditionings interact with control conditioning and how that impacts the
quality of the models. (v) Our work did not include studies on other parts that are involved in the
training and sampling of diffusion models, such as the different sampling mechanisms and training
paradigms. This could potentially yield additional gains in performance and uncover new insights
about the state-space of diffusion models."
REFERENCES,0.5069444444444444,"C
Implementation details"
REFERENCES,0.5092592592592593,"Noise schedule scaling.
In Fig. A1 we depict the rescaling of the noise for higher resolutions,
following Eq. (3)."
REFERENCES,0.5115740740740741,"Table A1: Computational comparison between different models. We compute FLOPs for different resolutions
and the parameter count. FLOPS are computed with a batch size of 1."
REFERENCES,0.5138888888888888,"UNet-SDXL
PixArt-XL/2
mmDit-XL/2
Att-Dit-XL/2
mdT-XL/2 (IN1k)
Dit-XL/2"
REFERENCES,0.5162037037037037,"CC12M
(text)"
REFERENCES,0.5185185185185185,"FLOPS@256 (G)
189.78
314.68
397.06
407.88
—
FLOPS@512 (G)
819.56
1, 140
1, 260
1, 500
—
—
Params. (M)
864.31
610.12
791.64
940.49
—
—"
REFERENCES,0.5208333333333334,"ImageNet
(class-cond)"
REFERENCES,0.5231481481481481,"FLOPS@256 (G)
95.38
275.14
237.93
237.93
259.08
237.4
FLOPS@512 (G)
390
1, 200
1, 050
1, 050
1, 140
1, 050
Params. (M)
401.75
611.7
679.09
792.44
748.07
679.09"
REFERENCES,0.5254629629629629,"Positional embedding rescaling.
As illustrated in App. C, rescaling the positional embedding can
be integrated in two simple lines of code by changing the grid sampling to be based on reducing the
stepsize in the grid instead of extending its limits."
REFERENCES,0.5277777777777778,"# grid_h = arange(grid_size)
# grid_w = arange(grid_size)"
REFERENCES,0.5300925925925926,base_size = grid_size // scale
REFERENCES,0.5324074074074074,"grid_h = linspace(0, base_size , grid_size)
grid_w = linspace(0, base_size , grid_size)"
REFERENCES,0.5347222222222222,Pseudocode 1: Rectified grid sampling for positional embeddings.
REFERENCES,0.5370370370370371,"Computational costs.
In Tab. A1 we compare the model size and computational costs of the
different architectures studied in the main paper. All model architectures are based on the original
implemetations, but transposed to our codebase. Mainly, we use bfloat16 mixed precision and
memory-efficient attention 2 from PyTorch."
REFERENCES,0.5393518518518519,"Experimental details.
To ensure efficient training of our models, we benchmark the most widely
used hyperparameters and report the results of these experiments, which consist of the choice of
the optimizer and its hyperparameters, the learning rate and the batch size. We then transpose the
optimal settings to our other experiments. For FID evaluation, we use a guidance scale of 1.5 for 256
resolution and 2.0 for resolution 512. For evaluation on ImageNet-22k, we compute the FID score
between 50k generated images and a subset of 200k images from the training set."
REFERENCES,0.5416666666666666,"Training paradigm.
We use the EDM [26] abstraction to train our models for epsilon prediction
following DDPM paradigm. Differently from [15, 38], we do not use learned sigmas but follow
a standard schedule. Specifically, we use a quadratic beta schedule with βstart = 0.00085 and
βend = 0.012. In DDPM [21], a noising step is formulated as follows, with x0 being the data sample
and t ∈[[0, T]] a timestep:"
REFERENCES,0.5439814814814815,"xt = √¯αtx0 +
√"
REFERENCES,0.5462962962962963,"1 −¯αtϵ,
ϵ ∼N(0, I),
(5)"
REFERENCES,0.5486111111111112,"while in EDM, the cumulative alpha products are converted to corresponding signal-to-noise ratio
(SNR) proportions, the noising process is then reformulated as:"
REFERENCES,0.5509259259259259,"xt = x0 + σtϵ
p"
REFERENCES,0.5532407407407407,"1 + σ2
t
,
(6)"
REFERENCES,0.5555555555555556,"and the loss is weighted by the inverse SNR
1
σ2
t ."
REFERENCES,0.5578703703703703,"Scaling the training.
A recurrent question when training deep networks is the coupling between
the learning rate and the batch size. When multiplying the batch size by a factor γ, some works
recommend scaling the learning rate by the square root of γ, while others scale the learning rate by"
REFERENCES,0.5601851851851852,"2https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_
attention.html"
REFERENCES,0.5625,"Batch
Size"
REFERENCES,0.5648148148148148,"Learning
rate
10−3
5.10−4
10−4
10−5
10−6"
REFERENCES,0.5671296296296297,"w/ UNet
64
✗
126.68
57.56
59.50
104.73
512
✗
27.26
19.83
37.98
80.26
1024
✗
18.01
19.25
30.00
62.82
2048
✗
14.63
14.73
24.24
61.89
4096
✗
49.24
9.26
15.71
—"
REFERENCES,0.5694444444444444,"w/ DiT
64
✗
✗
59.26
104.55
—
256
✗
39.53
37.08
86.51
—
512
✗
22.48
23.61
83.6
—
1024
✗
12.03
17.13
76.15
—
2048
✗
10.19
12.36
82.62
—"
REFERENCES,0.5717592592592593,"Table A2: Influence of learning rate and batch
size on convergence. Training is performed on
ImageNet-1k@256. Results are reported on the
model without EMA after 70k training steps. FID
is computed using 250 sampling steps w.r.t. the
training set of ImageNet-1k@256. ✗: diverged.
For almost all learning rates, the optimal batch
size is the highest possible. The best performance
is obtained when using the highest learning rate
that does not diverge with biggest batch size pos-
sible."
REFERENCES,0.5740740740740741,"the factor γ itself. In the following we experiment with training a class-conditional DiT model with
different batch sizes and learning rates and report results after the same number of iterations."
REFERENCES,0.5763888888888888,"From Tab. A2 we observe an improved performance by increasing the batch size and the learning rate
in every learning rate setting. If the learning rate is too high the training diverges."
REFERENCES,0.5787037037037037,"0.5
0.95
0.99
0.999
β2 0.0 0.5 0.9 0.99 β1"
REFERENCES,0.5810185185185185,"24.02
25.52
24.28
26.11"
REFERENCES,0.5833333333333334,"28.38
25.93
21.79
24.7"
REFERENCES,0.5856481481481481,"25.64
21.65
26.98
26.43"
REFERENCES,0.5879629629629629,"23.4
25.88
27.43
22.42 22 23 24 25 26 27 28"
REFERENCES,0.5902777777777778,"Figure A2: Influence of momentum on
training dynamics of the UNet. We evalu-
ate ImageNet1k@256 FID using 50 sampling
steps after training the models for 70k steps.
The FID is reported w.r.t. the validation set
of ImageNet."
REFERENCES,0.5925925925925926,"Influence of momentum.
We conduct a grid search over
the momentum parameters of Adam optimizer, similar to
previous sections, we train a UNet model fo 70k steps
and compute FID with respect to the validation set of
ImageNet1k using 50 DDIM steps. From Fig. A2, we can
see that the default pytorch values (β1 = 0.9, β2 = 0.999)
are sub-optimal, resulting in an FID of 26.43 while the best
performance is obtained when setting (β1 = 0.9, β2 =
0.95) improves FID by 4.78 points in our experimental
setting."
REFERENCES,0.5949074074074074,Setting the optimal guidance scale.
REFERENCES,0.5972222222222222,"Proposition 2. The optimal guidance scale λ scales with
the upsampling factor s according to the law λ′(s) =
1 + s · (λ −1)."
REFERENCES,0.5995370370370371,"This is verified in Fig. 7 where the λ′(s = 2) = 1 + 2 ·
(1.5 −1.0) = 2.0 which is the optimal guidance scale at
512 resolution according to the figure."
REFERENCES,0.6018518518518519,"0
10 20 30 40 50 60 70"
REFERENCES,0.6041666666666666,Token index −2 −1 0 1 2
REFERENCES,0.6064814814814815,Token gradient
REFERENCES,0.6087962962962963,Number of tokens
REFERENCES,0.6111111111111112,"Figure A3: Contribution of Padding to-
kens. For short prompts, a large number of
text tokens do not contain useful information,
but may still contribute to the cross-attention,
as illustrated by the non-zero gradients w.r.t.
tokens after the ones coding the prompt (in-
dicated by the dashed vertical line)."
REFERENCES,0.6134259259259259,"Text padding mechanism.
In order to train at a large
scale, most commonly used text encoders output a constant
number of tokens T that are fed to the denoising model
(usually T = 77). Consequently, when the prompt has less
than T words, the remaining tokens are padding tokens
that do not contain useful information, but can still con-
tribute in the cross-attention, see Figure A3. This raises
the question of whether better use can be made of padding
text tokens to improve training performance and efficiency.
One common mitigation involves using recaptioning meth-
ods that provide longer captions. However, this creates an
inconsistency between training and sampling as users are
more likely to provide shorter prompts. Thus, to make bet-
ter use of the conditioning vector, we explore alternative
padding mechanisms for the text encoder. We explore a
‘replicate’ padding mechanism where the padding tokens
are replaced with copies of the text tokens, thereby push-
ing the subsequent cross-attention layers to attend to all 𝐾 𝑄 𝑉"
REFERENCES,0.6157407407407407,Padding tokens
REFERENCES,0.6180555555555556,Padding tokens
REFERENCES,0.6203703703703703,Image tokens
REFERENCES,0.6226851851851852,Text tokens
REFERENCES,0.625,"Image tokens
Text tokens ×"
REFERENCES,0.6273148148148148,(a) Zero padding. 𝐾 𝑄 ×
REFERENCES,0.6296296296296297,Padding tokens
REFERENCES,0.6319444444444444,Padding tokens
REFERENCES,0.6342592592592593,Image tokens
REFERENCES,0.6365740740740741,Text tokens
REFERENCES,0.6388888888888888,"Image tokens
Text tokens"
REFERENCES,0.6412037037037037,(b) Replicate padding. 𝐾 𝑄 𝑉 ×
REFERENCES,0.6435185185185185,Padding tokens
REFERENCES,0.6458333333333334,Padding tokens
REFERENCES,0.6481481481481481,Image tokens
REFERENCES,0.6504629629629629,Text tokens
REFERENCES,0.6527777777777778,"Image tokens
Text tokens"
REFERENCES,0.6550925925925926,(c) Noisy replicate padding.
REFERENCES,0.6574074074074074,"Figure A4: Illustration of the attention matrix under different padding mechanisms. With the zero padding
mechanism, a significant part of the attention can be used on “dead” padding tokens, potentially de-focusing
from the relevant information. Using a replicate padding instead results in redundant information. Noisy replicate
padding increases the diversity in the text token representations and therefore acts as a regularizer fostering the
model to be robust to local variations in the latent space of the conditioning, e.g., akin to data augmentation."
REFERENCES,0.6597222222222222,"the tokens in its inputs. To improve the diversity of the feature representation across the sequence
dimension, we perturb the embeddings with additive Gaussian noise with a small variance βtxt. For
shorter prompts with a high number of repeats m, we scale the additive noise by √m −1 to account
for the reduction in posterior uncertainty induced by these repetitions:"
REFERENCES,0.6620370370370371,"ϕtxt = ϕtxt +
√"
REFERENCES,0.6643518518518519,"m −1 · σch(ϕtxt) · ϵtxt,
with ϵtxt ∼N(0, I),
(7)"
REFERENCES,0.6666666666666666,"where ϕch is the standard deviation of the feature text embeddings over the feature dimension. See
Figure A4 for an illustration comparing zero-padding, replication padding, and our noisy replication
padding."
REFERENCES,0.6689814814814815,"D
Derivations"
REFERENCES,0.6712962962962963,Derivation for Eq. (2)
REFERENCES,0.6736111111111112,Proof. The formula can be obtained by iteratively applying the guidance across the conditions.
REFERENCES,0.6759259259259259,"ϵλ,β = λϵc + (1 −λ)ϵ∅
(8)"
REFERENCES,0.6782407407407407,"ϵλ,β = λ
 
βϵc,s + (1 −β)ϵc,∅

+ (1 −λ)ϵ∅,∅
(9)"
REFERENCES,0.6805555555555556,Derivation for Eq. (4)
REFERENCES,0.6828703703703703,"Proof. Assuming that the unconditional prediction ϵ∅is distributed around the conditional distribution
according to a normal law ϵ∅|ϵc ∼N(ϵc, δ2I)."
REFERENCES,0.6851851851851852,"ϵλ = λc + (1 −λ)(c + δϵ),
ϵ ∼N(0, I)
(10)
ϵλ = c + (1 −λ)δϵ
(11)"
REFERENCES,0.6875,"Var(ϵλ) = (1 −λ)2δ2
(12)"
REFERENCES,0.6898148148148148,"After upsampling by a scale factor of s, the same low resolution patch has s2 observations, hence the
variance is decreased by s2.
Var(ϵλ)hr = (1 −λ)2δ2/s2
(13)"
REFERENCES,0.6921296296296297,"Hence by equalizing the discrepancy between the conditional and unconditional predictions at low
and high resolutions we obtain:"
REFERENCES,0.6944444444444444,"Var(ϵλ)s=1 = Var(ϵ′
λ)s =⇒(1 −λ′)2δ2/s2 = (1 −λ)2δ2
(14)"
REFERENCES,0.6967592592592593,"λ′ = 1 + s · (λ −1)
(15)"
REFERENCES,0.6990740740740741,"Figure A5: Qualitative examples. Sample from mmDiT-XL/2 trained with our method on ImageNet1k at 512
resolution. Samples are generated with 50 DDIM steps and a guidance scale of 5."
REFERENCES,0.7013888888888888,Derivation for Eq. (3)
REFERENCES,0.7037037037037037,"Proof. At timestep t, the noisy observation is obtained as :"
REFERENCES,0.7060185185185185,"xt =
1
p"
REFERENCES,0.7083333333333334,"1 + σ2
t"
REFERENCES,0.7106481481481481," 
x0 + σtϵ

,
ϵ ∼N(0, I)
(16)"
REFERENCES,0.7129629629629629,Hence x0 can be estimated using the following formula:
REFERENCES,0.7152777777777778,"x0 =
q"
REFERENCES,0.7175925925925926,"1 + σ2
t xt −σtϵ
(17)"
REFERENCES,0.7199074074074074,"The statistics of the estimate of x0 are as follows.
(
E(x0) =
p"
REFERENCES,0.7222222222222222,"1 + σ2
t E(xt)
Var(x0) = (1 + σ2
t )Var(xt) + σ2
t
(18)"
REFERENCES,0.7245370370370371,"Consequently, if we already have xt, we have an estimate of x0 with an error bound given by:"
REFERENCES,0.7268518518518519,"Var
 
x0 −
p"
REFERENCES,0.7291666666666666,"1 + σ2xt

= σ2
t
(19)"
REFERENCES,0.7314814814814815,"At higher resolutions, we have s2 corresponding observations for the same patch such that the error
with respect to the estimate becomes:"
REFERENCES,0.7337962962962963,"Var

x0 −1 s2 s2
X i=1 q"
REFERENCES,0.7361111111111112,"1 + σ2
t x(i)
t

= σ2
t /s2
(20)"
REFERENCES,0.7384259259259259,"Hence, if we want to keep the same uncertainty with respect to the low resolution patches, the
following equality needs to be verified:"
REFERENCES,0.7407407407407407,"σ(t, s) = σ(t′, s′) =⇒σt/s = σ′
t/s′
(21)"
REFERENCES,0.7430555555555556,=⇒1 −¯αt
REFERENCES,0.7453703703703703,"¯αt
= ( s"
REFERENCES,0.7476851851851852,s′ )2 · 1 −¯αt′
REFERENCES,0.75,"¯
αt′
(22)"
REFERENCES,0.7523148148148148,"=⇒¯αt′ =
s2 · ¯αt
s′2 + ¯αt · (s2 −s′2)
(23)"
REFERENCES,0.7546296296296297,"E
Additional results"
REFERENCES,0.7569444444444444,"Qualitative results.
We provide additional qualitative examples on ImageNet-1k in Fig. A5."
REFERENCES,0.7592592592592593,"Conditioning
FIDIN1K
FIDCC12M
CLIPCOCO
Baseline (DiT)
1.95
✗
✗
+ mmDiT attention
1.81
—
—
+ Control cond.
1.76
—
—
+ Control cond. separation & scheduling
1.71
7.54
25.88
+ Disentangled control guidance
1.59
7.32
26.13"
REFERENCES,0.7615740740740741,"+ Flip cond.
—
7.19
26.25
+ Noised replicate padding for text conditioning
—
6.79
26.60"
REFERENCES,0.7638888888888888,"Resolution transfer
FIDCC12M
CLIPCOCO
Baseline (train from scratch)
11.24
24.23
+ Low res. pretraining
7.25
25.69
+ Local view pretraining
6.89
25.57
+ Positional emb. resampling
6.73
25.70
+ Noise schedule rescaling
6.27
25.91"
REFERENCES,0.7662037037037037,"Table A3: Effects of our changes. We summarize the improvements obtained by each change proposed in the
paper. left: Changes relevant to conditioning mechanisms – right: Changes relevant to representation transfer
across image resolutions."
REFERENCES,0.7685185185185185,"SD-XL [39]
SD3 [14]
Ours"
REFERENCES,0.7708333333333334,Sampling
REFERENCES,0.7731481481481481,"Schedule σi<N
q 1−¯αi"
REFERENCES,0.7754629629629629,"¯αi
i
N−1
q 1−¯αi"
REFERENCES,0.7777777777777778,"¯αi
Forward Process xt
 
x0 + σtϵ

/
p"
REFERENCES,0.7800925925925926,"1 + σ2
t
tx0 + (1 −t)ϵ
 
x0 + σtϵ

/
p"
REFERENCES,0.7824074074074074,"1 + σ2
t
Target
ϵ
vΘ = ϵ −x0
ϵ
Loss scaling
σ−2
t
1.0
σ−2
t
Timestep sampling
U([0, T])
π(t, m, s) =
1
s√wπ
1
t(1−t)e(logit(t)−m)2/2s2
U([0, T])
Guidance mechanism
λϵc + (1 −λ)ϵ∅
λϵc + (1 −λ)ϵ∅
λ

β · ϵc,s + (1 −β) · ϵ∅,s

+ (1 −λ) · ϵ∅,∅
Network and conditioning
Denoiser architecture
UNet
mmDiT
mmDiT++
Conditioning mechanism
Cross-attention
Augmented self-attention
Augmented self-attention
Attention Pre-Norm
✗
RMSNorm
RMSNorm
Control conditioning
✓
N/A
✓
Non-semantic cond. schedule γc(t)
1.0
1.0

1 −cos(π(1 −t)α)

/2
Flip conditioning
✗
✗
✓
Condition disentanglement
✗
✗
✓
Text token padding
zero
zero
noisy replicate"
REFERENCES,0.7847222222222222,"High resolution adaptation
Noise schedule resampling
¯αt′ = ¯αt
t′ =
st
s′+(s−s′)t
¯αt′ =
s2·¯αt
s′2+¯αt·(s2−s′2)
Positional embedding
✗
interpolated
resampled
Low res. pretrain. crop scale
[0.9, 1.0]
[0.9, 1.0]
[0.4, 0.6]
Guidance scale λ(s′)
λ(s)
λ(s)
1 + s′ · (λ(s) −1)"
REFERENCES,0.7870370370370371,"Optimization
Optimizer
Adam
AdamW
AdamW
Momentums
β1 = 0.9, β2 = 0.999
β1 = 0.9, β2 = 0.999
β1 = 0.9, β2 = 0.95
Schedule
βmin = 0.00085, βmax = 0.012
βmin = 0.00085, βmax = 0.012
βmin = 0.00085, βmax = 0.012
Weight decay
0.0
N/A
0.03
Table A4: Comparison with previous paradigms. We provide a comparative table with previous works,
notably SD-XL [39] and SD3 [14]."
REFERENCES,0.7893518518518519,"Summary of findings.
In Table A3 we summarize the improvements w.r.t. the DiT baseline
obtained by the changes to the model architecture and training. In Table A4 we compare our model
architecture and training recipe to that of SDXL and SD3. In Table A5, we provide a synopsis of
the research questions addressed in our study alongside a respective recommendation based on our
findings."
REFERENCES,0.7916666666666666,"Effectiveness of the power cosine schedule
We experiment with different function profiles for
controlling the conditioning on low-level augmentations. Specifically, we compare the power-cosine
profile with a linear and a piecewise constant profile. While the linear schedule manages an acceptable
performance in terms of reducing LPIPS (although still higher than the power-cosine profile), it still
achieves a higher FID than all the configurations with the cosine schedule. For the piecewise constant
profiles, they achieve a higher LPIPS while also having a higher FID. In conclusion, the proposed"
REFERENCES,0.7939814814814815,Table A5: Summary of the findings from our experiments in the form of a Q&A.
REFERENCES,0.7962962962962963,Conditioning
REFERENCES,0.7986111111111112,"How to condition on text ?
cross-domain self-attention [14].
Use control conditioning?
✓
Weight control conditioning with a power cosine schedule?
✓
Use a separate guidance scale for control conditions?
✓
Use replicate padding with gaussian perturbations for text embeddings?
✓"
REFERENCES,0.8009259259259259,"Distribution transfer
Pretrain on smaller datasets (ImageNet1k)?
✓
Pretrain on datasets of the same scale but with different distributions
✓
Pretrain until convergence?
Diminishing returns after a while"
REFERENCES,0.8032407407407407,Resolution transfer
REFERENCES,0.8055555555555556,"Rescale the noise scheduler
✓
Resample the grid coordinates of the positional embeddings
✓
Pretrain at smaller resolution
✓
Use more agressive cropping when pretraining at smaller resolutions?
✓"
REFERENCES,0.8078703703703703,"Table A6: Comparison of the power cosine schedule with other schedules. We report results for a linear and
step function schedules."
REFERENCES,0.8101851851851852,"Init.
t weighting
FID (↓)
LPIPS (↓)
LPIPS/HR (↓)"
REFERENCES,0.8125,"—
zero
3.29
—
—
zero.
unif.
3.08
0.33
0.210"
REFERENCES,0.8148148148148148,"zero.
cos(α = 1.0)
3.08
0.23
0.076
zero.
cos(α = 2.0)
3.09
0.18
0.045
zero.
cos(α = 4.0)
3.05
0.13
0.025
zero.
cos(α = 8.0)
3.04
0.04
0.009"
REFERENCES,0.8171296296296297,"zero.
linear
3.13
0.07
0.041"
REFERENCES,0.8194444444444444,"zero.
δ(σt ≤1.0)
3.15
0.09
0.046
zero.
δ(σt ≤2.0)
3.12
0.14
0.062
zero.
δ(σt ≤6.0)
3.09
0.26
0.170"
REFERENCES,0.8217592592592593,"power-cosine profile outperforms these simpler schedules in both FID and LPIPS, improving image
quality while better removing the unwanted distribution shift induced from choosing different samples
during training."
REFERENCES,0.8240740740740741,"F
NeurIPS Paper Checklist"
CLAIMS,0.8263888888888888,1. Claims
CLAIMS,0.8287037037037037,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction clearly set the claims made in the paper and
match theoretical and experimental results, which are addressed in Section 3.
Guidelines:"
CLAIMS,0.8310185185185185,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.8333333333333334,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We include discussion of limitations in Appendix B.
Guidelines:"
CLAIMS,0.8356481481481481,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.8379629629629629,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]"
CLAIMS,0.8402777777777778,"Justification: Proofs are provided Appendix D and contain cross-references to the relevant
parts in the main paper."
CLAIMS,0.8425925925925926,Guidelines:
CLAIMS,0.8449074074074074,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8472222222222222,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8495370370370371,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8518518518518519,"Answer: [Yes]
Justification: We include details required for result reproducibility in Section 3.1. These
include details about datasets used for training, evaluation metrics, training parameters, and
model architectures."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8541666666666666,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8564814814814815,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.8587962962962963,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.8611111111111112,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.8634259259259259,Answer: [No]
OPEN ACCESS TO DATA AND CODE,0.8657407407407407,"Justification: While we are not be able to include code, we do provide specific and thorough
detailing of our training and evaluation methods in Section 3.1 to enable experimental
reproduction."
OPEN ACCESS TO DATA AND CODE,0.8680555555555556,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8703703703703703,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.8726851851851852,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.875,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.8773148148148148,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8796296296296297,Justification: Details corresponding to training and testing are included in Section 3.1.
OPEN ACCESS TO DATA AND CODE,0.8819444444444444,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8842592592592593,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8865740740740741,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8888888888888888,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8912037037037037,Answer: [No]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8935185185185185,"Justification: Due to the computational cost of training and performing inference with
generative models, we do not include error bars."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8958333333333334,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8981481481481481,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9004629629629629,"• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9027777777777778,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9050925925925926,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9074074074074074,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9097222222222222,"Justification: Information about compute resources needed for experiment reproduction is
included in Section 3.1."
EXPERIMENTS COMPUTE RESOURCES,0.9120370370370371,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9143518518518519,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.9166666666666666,9. Code Of Ethics
CODE OF ETHICS,0.9189814814814815,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9212962962962963,Answer: [Yes]
CODE OF ETHICS,0.9236111111111112,"Justification: This research conforms to the Code of Ethics, including mitigation of potential
harms caused by the research process, considerations of social impact, and taking steps for
impact mitigation."
CODE OF ETHICS,0.9259259259259259,Guidelines:
CODE OF ETHICS,0.9282407407407407,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9305555555555556,10. Broader Impacts
BROADER IMPACTS,0.9328703703703703,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9351851851851852,Answer: [Yes]
BROADER IMPACTS,0.9375,"Justification: We discuss societal impacts and steps taken to mitigate potential harms in
Appendix B."
BROADER IMPACTS,0.9398148148148148,Guidelines:
BROADER IMPACTS,0.9421296296296297,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
BROADER IMPACTS,0.9444444444444444,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: We do not release models nor data in this work.
Guidelines:"
BROADER IMPACTS,0.9467592592592593,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
BROADER IMPACTS,0.9490740740740741,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: Use of existing assets is discussed, including which version, and cited through-
out the paper. No assets are released.
Guidelines:"
BROADER IMPACTS,0.9513888888888888,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset."
BROADER IMPACTS,0.9537037037037037,"• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9560185185185185,13. New Assets
NEW ASSETS,0.9583333333333334,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9606481481481481,Answer: [NA]
NEW ASSETS,0.9629629629629629,Justification: This paper does not release new assets.
NEW ASSETS,0.9652777777777778,Guidelines:
NEW ASSETS,0.9675925925925926,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9699074074074074,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9722222222222222,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9745370370370371,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9768518518518519,Justification: This paper does not include crowdsourcing or research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9791666666666666,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9814814814814815,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9837962962962963,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9861111111111112,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9884259259259259,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9907407407407407,Justification: This paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9930555555555556,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9953703703703703,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9976851851851852,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
