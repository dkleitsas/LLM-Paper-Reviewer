Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0021231422505307855,"We propose a surrogate function for efficient use of score-based priors for Bayesian
1"
ABSTRACT,0.004246284501061571,"inverse imaging. Recent work turned score-based diffusion models into probabilis-
2"
ABSTRACT,0.006369426751592357,"tic priors for solving ill-posed imaging problems by appealing to an ODE-based
3"
ABSTRACT,0.008492569002123142,"log-probability function. However, evaluating this function is computationally
4"
ABSTRACT,0.010615711252653927,"inefficient and inhibits posterior estimation of high-dimensional images. Our
5"
ABSTRACT,0.012738853503184714,"proposed surrogate prior is based on the evidence lower-bound of a score-based
6"
ABSTRACT,0.014861995753715499,"diffusion model. We demonstrate the surrogate prior on variational inference for
7"
ABSTRACT,0.016985138004246284,"efficient posterior sampling of large images. Compared to the exact prior used
8"
ABSTRACT,0.01910828025477707,"in previous work, our surrogate prior accelerates optimization of the variational
9"
ABSTRACT,0.021231422505307854,"distribution by at least two orders of magnitude. We also find that our principled ap-
10"
ABSTRACT,0.02335456475583864,"proach achieves higher-fidelity image-reconstruction than non-Bayesian baselines
11"
ABSTRACT,0.025477707006369428,"that involve hyperparameter-tuning at inference. Our work establishes a practical
12"
ABSTRACT,0.027600849256900213,"path forward for using score-based diffusion models as general-purpose priors for
13"
ABSTRACT,0.029723991507430998,"computational imaging.
14"
INTRODUCTION,0.03184713375796178,"1
Introduction
15"
INTRODUCTION,0.03397027600849257,"Ill-posed image reconstruction requires a prior to constrain the reconstruction according to desired
16"
INTRODUCTION,0.036093418259023353,"image statistics. From a Bayesian perspective, the prior influences both the uncertainty and the
17"
INTRODUCTION,0.03821656050955414,"richness of the estimated image. Although diffusion-based generative models represent rich image
18"
INTRODUCTION,0.040339702760084924,"priors, leveraging these priors for Bayesian image-reconstruction remains a challenge. True posterior
19"
INTRODUCTION,0.04246284501061571,"sampling with an unconditional diffusion model is intractable, so most previous methods heavily
20"
INTRODUCTION,0.044585987261146494,"approximate the posterior [9; 13; 14; 19] or disregard measurement noise [5; 7; 8; 6; 11; 24; 1].
21"
INTRODUCTION,0.04670912951167728,"Recent work demonstrated how to turn score-based diffusion models into probabilistic priors (score-
22"
INTRODUCTION,0.04883227176220807,"based priors) for Bayesian imaging [10]. However, this method requires the exact probability of
23"
INTRODUCTION,0.050955414012738856,"a proposed image to be evaluated with a computationally-expensive ordinary differential equation
24"
INTRODUCTION,0.05307855626326964,"(ODE), requiring days to a week to reconstruct even a 32 × 32 image [10]. We present a method for
25"
INTRODUCTION,0.055201698513800426,"Bayesian inference with a score-based prior that is both principled and computationally efficient.
26"
INTRODUCTION,0.05732484076433121,"Although computing exact probabilities under a diffusion model is inefficient or even intractable,
27"
INTRODUCTION,0.059447983014861996,"computing the evidence lower-bound [22; 12] is computationally efficient and feasible for high-
28"
INTRODUCTION,0.06157112526539278,"dimensional images. Thus we propose to use this evidence lower-bound as a surrogate for the
29"
INTRODUCTION,0.06369426751592357,"exact score-based prior. In particular, we use the evidence lower-bound of a score-based diffusion
30"
INTRODUCTION,0.06581740976645435,"model [22] as a substitute for the exact log-probability function. This function can be plugged into
31"
INTRODUCTION,0.06794055201698514,"any inference algorithm that requires the value or gradient of the posterior log-density. When it is
32"
INTRODUCTION,0.07006369426751592,"used in variational inference, we find at least two orders of magnitude in speedup of optimizing the
33"
INTRODUCTION,0.07218683651804671,"variational distribution. Furthermore, our approach reduces GPU memory requirements, as there
34"
INTRODUCTION,0.07430997876857749,"is no need to evaluate and backpropagate through an ODE. These efficiency improvements make it
35"
INTRODUCTION,0.07643312101910828,"practical to perform inference with score-based priors.
36"
INTRODUCTION,0.07855626326963906,"Observed
Posterior Samples
Original"
INTRODUCTION,0.08067940552016985,16x-accel.
INTRODUCTION,0.08280254777070063,"MRI
256x256"
INTRODUCTION,0.08492569002123142,4x-accel.
INTRODUCTION,0.0870488322717622,"MRI
256x256"
INTRODUCTION,0.08917197452229299,"U0Y="">AB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoseiF48VbC2oWy2m3bpZhN2X4QS+i+8eFDEq/Gm/GTZuDtg4sDPvsfMmSKQw6LrfTmldW19o7xZ2dre2d2r7h+0TZxqxlslrHuBNRwKRvoUDJO4nmNAokfwjGN7n/8MS1EbG6x0nC/YgOlQgFo2ilx15EcRSE2WTar9bcujsDWSZeQWpQoNmvfvUGMUsjrpBJakzXcxP0M6pRMmnlV5qeELZmA51JFI278bJZ4Sk6sMiBhrO1TSGbq742MRsZMosBO5gnNopeL/3ndFMrPxMqSZErNv8oTCXBmOTnk4HQnKGcWEKZFjYrYSOqKUNbUsW4C2evEzaZ3Xvou7endca10UdZTiCYzgFDy6hAbfQhBYwUPAMr/DmGOfFeXc+5qMlp9g5hD9wPn8AB+RIQ=</latexit>y"
INTRODUCTION,0.09129511677282377,RZdFNy4r2Ad0xpJM21oJhOSjFiG/oYbF4q49Wfc+Tdm2lo64HA4Zx7uScnlJxp47rfztLyuraemjvLm1vbNb2dtv6SRVhDZJwhPVCbGmnAnaNMxw2pGK4jktB2ObnK/UiVZom4N2NJgxgPBIsYwcZKvh9jMwyj7GnycNqrVN2aOwVaJF5BqlCg0at8+f2EpDEVhnCsdzpQkyrAwjnE7KfqpxGSEB7RrqcAx1UE2zTxBx1bpoyhR9gmDpurvjQzHWo/j0E7mGfW8l4v/ed3URFdBxoRMDRVkdihKOTIJygtAfaYoMXxsCSaK2ayIDLHCxNiayrYEb/7Li6R1VvMuau7debV+XdRgkM4ghPw4BLqcAsNaAIBCc/wCm9O6rw4787HbHTJKXYO4A+czx8fkZG8</latexit>x⇤
INTRODUCTION,0.09341825902335456,sA9oQplMJu3QmSTMTMQ8g9u/BU3LhRx68adf+O0jVBbDwycOede7r3HixmVyrK+jdLS8srqWnm9srG5tb1j7u61ZQITFo4YpHoekgSRkPSUlQx0o0FQdxjpONrsd+54ISaPwTqUxcTkahDSgGCkt9c0ThyM19ILsIXck5TCuzQic+r+/ND/um1Wrbk0AF4ldkCo0OybX4f4YSTUGpOzZVqzcDAlFMSN5xUkiREeoQHpaRoiTqSbTW7K4ZFWfBhEQr9QwYk625EhLmXKPV05XlHOe2PxP6+XqODSzWgYJ4qEeDoSBhUERwHBH0qCFYs1QRhQfWuEA+RQFjpGCs6BHv+5EXSPq3b53Xr9qzauCriKIMDcAhqwAYXoAFuQBO0AaP4Bm8gjfjyXgx3o2PaWnJKHr2wR8Ynz98d58q</latexit>x ⇠p(x | y)
INTRODUCTION,0.09554140127388536,"Figure 1: High-dimensional Bayesian inference with a surrogate score-based prior. We propose
a surrogate prior for efficient use of score-based diffusion models as priors for Bayesian imaging.
Here we show posterior samples (estimated with variational inference) for accelerated MRI of
256 × 256 knee images with a score-based diffusion-model prior. The first row shows reconstruction
from 16×-reduced MRI measurements. The second row shows reconstruction given more κ-space
measurements, i.e., 4×-reduced MRI. Bayesian imaging at this image resolution is computationally
infeasible with the previous ODE-based approach. Our proposed surrogate prior enables efficient yet
principled inference with diffusion-model priors, resulting in inferred posteriors where the true image
is within three standard deviations of the posterior mean for 96% and 99% of the pixels for 16×- and
4×-acceleration, respectively."
INTRODUCTION,0.09766454352441614,"In this paper, we describe our variational-inference approach to efficiently estimate a posterior with
37"
INTRODUCTION,0.09978768577494693,"a surrogate score-based prior. We provide experimental results to validate the proposed surrogate
38"
INTRODUCTION,0.10191082802547771,"prior, including high-dimensional posterior samples of sizes up to 256 × 256, a resolution infeasible
39"
INTRODUCTION,0.1040339702760085,"with the exact prior. In the setting of accelerated MRI, we quantify time- and memory-efficiency
40"
INTRODUCTION,0.10615711252653928,"improvements of the surrogate over the exact prior. We also demonstrate how our proposed approach
41"
INTRODUCTION,0.10828025477707007,"achieves higher-quality image reconstructions than methods that deviate from true Bayesian inference.
42"
RELATED WORK,0.11040339702760085,"2
Related work
43"
BAYESIAN INVERSE IMAGING,0.11252653927813164,"2.1
Bayesian inverse imaging
44"
BAYESIAN INVERSE IMAGING,0.11464968152866242,"Image reconstruction can be framed as an inverse problem: a hidden image x∗∈RD must be
45"
BAYESIAN INVERSE IMAGING,0.11677282377919321,"recovered from measurements y ∈RM, where
46"
BAYESIAN INVERSE IMAGING,0.11889596602972399,"y = f(x∗) + ϵ.
(1)"
BAYESIAN INVERSE IMAGING,0.12101910828025478,"It is usually assumed that the forward model f : RD →RM is known and that the measurement
47"
BAYESIAN INVERSE IMAGING,0.12314225053078556,"noise ϵ ∈RM is a random variable with a known distribution. With an ill-posed inverse problem,
48"
BAYESIAN INVERSE IMAGING,0.12526539278131635,"there is inherent uncertainty in image reconstruction.
49"
BAYESIAN INVERSE IMAGING,0.12738853503184713,"Bayesian imaging accounts for the uncertainty by formulating a posterior distribution p(x | y). The
50"
BAYESIAN INVERSE IMAGING,0.12951167728237792,"posterior can be decomposed into a likelihood term and a prior term:
51"
BAYESIAN INVERSE IMAGING,0.1316348195329087,"log p(x | y) = log p(y | x) + log p(x) + const.
(2)"
BAYESIAN INVERSE IMAGING,0.1337579617834395,"Given a log-likelihood function log p(y | x) and a prior log-probability function log p(x), we can
52"
BAYESIAN INVERSE IMAGING,0.13588110403397027,"use established techniques for sampling from the posterior, such as Markov chain Monte Carlo
53"
BAYESIAN INVERSE IMAGING,0.13800424628450106,"(MCMC) [3] or variational inference [2]. MCMC algorithms generate a Markov chain whose
54"
BAYESIAN INVERSE IMAGING,0.14012738853503184,"stationary distribution is the posterior, but they are generally slow to converge for high-dimensional
55"
BAYESIAN INVERSE IMAGING,0.14225053078556263,"data like images. Variational inference instead approximates the posterior with a tractable distribution
56"
BAYESIAN INVERSE IMAGING,0.14437367303609341,"(e.g., Gaussian). The variational distribution is usually parameterized and thus can be efficiently
57"
BAYESIAN INVERSE IMAGING,0.1464968152866242,"optimized to represent high-dimensional data distributions. Deep Probabilistic Imaging (DPI) [25; 26]
58"
BAYESIAN INVERSE IMAGING,0.14861995753715498,"proposed an efficient variational-inference approach specifically for computationtal imaging with
59"
BAYESIAN INVERSE IMAGING,0.15074309978768577,"traditional regularizers; in DPI, the variational distribution is a discrete normalizing flow [15], which
60"
BAYESIAN INVERSE IMAGING,0.15286624203821655,"is an invertible generative model capable of representing complex distributions.
61"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.15498938428874734,"2.2
Diffusion models for inverse problems
62"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.15711252653927812,"Primarily developed for image generation, diffusion models [18; 12; 20; 21; 23] learn to model a
63"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.1592356687898089,"rich image distribution that could be useful as a prior for image reconstruction. A diffusion model
64"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.1613588110403397,"generates an image by starting from an image of noise and gradually denoising it until it becomes a
65"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.16348195329087048,"clean image. We discuss this process, known as reverse diffusion, in more detail in Sec. 3.1.
66"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.16560509554140126,"Given an inverse problem, simply adapting a pretrained diffusion model to sample from the posterior
67"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.16772823779193205,"instead of the learned prior is intractable [10]. Therefore, most diffusion-based approaches do
68"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.16985138004246284,"not infer a true Bayesian posterior. Some methods project images onto a measurement-consistent
69"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.17197452229299362,"subspace [24; 8; 6; 5; 7], but the projection does not account for measurement noise and might
70"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.1740976645435244,"pull images away from a true posterior. Other methods follow a gradient toward higher likelihood
71"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.1762208067940552,"throughout reverse diffusion [9; 13; 11; 14; 1; 19; 17], but these methods heavily approximate the
72"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.17834394904458598,"posterior. Overall, these diffusion-based methods require hyperparameter-tuning to balance the
73"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.18046709129511676,"measurements and the prior. As soon as hyperparameters are introduced, there is no guarantee of
74"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.18259023354564755,"sampling from a posterior that represents the true uncertainty.
75"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.18471337579617833,"Score-based priors. Alternatively, a score-based diffusion model can be turned into a standalone,
76"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.18683651804670912,"probabilistic prior (score-based prior) that can be paired with any measurement-likelihood function
77"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.18895966029723993,"and plugged into established Bayesian-inference approaches. Feng et al. [10] proposed to do this
78"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.1910828025477707,"with a log-density function based on the ODE associated with reverse diffusion (see Sec. 3.2). This
79"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.1932059447983015,"function provides the log-probability of any image under the diffusion model’s generative distribution,
80"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.19532908704883228,"but it is computationally expensive to evaluate. When used in iterative optimization algorithms, it
81"
DIFFUSION MODELS FOR INVERSE PROBLEMS,0.19745222929936307,"incurs prohibitively high time and memory costs.
82"
BACKGROUND,0.19957537154989385,"3
Background
83"
BACKGROUND,0.20169851380042464,"In this section, we review background on score-based diffusion models with an emphasis on evaluating
84"
BACKGROUND,0.20382165605095542,"probabilities of images with a pretrained diffusion model. We then describe how a diffusion process
85"
BACKGROUND,0.2059447983014862,"gives rise to an efficient denoising-based lower-bound on these image probabilities.
86"
SCORE-BASED DIFFUSION MODELS,0.208067940552017,"3.1
Score-based diffusion models
87"
SCORE-BASED DIFFUSION MODELS,0.21019108280254778,"The core idea of a diffusion model is that it transforms a simple distribution π to a complex image
88"
SCORE-BASED DIFFUSION MODELS,0.21231422505307856,"distribution through a gradual process. In this work, we follow the popular framework of denoising
89"
SCORE-BASED DIFFUSION MODELS,0.21443736730360935,"diffusion models, which transform noise samples from π = N(0, I) to clean samples from the
90"
SCORE-BASED DIFFUSION MODELS,0.21656050955414013,"data distribution pdata through gradual denoising. With knowledge of the noise distribution and the
91"
SCORE-BASED DIFFUSION MODELS,0.21868365180467092,"denoising process, we can assess the probability of a novel image under this generative model.
92"
SCORE-BASED DIFFUSION MODELS,0.2208067940552017,"The transformation from a simple distribution to a complex one occurs over many steps. To determine
93"
SCORE-BASED DIFFUSION MODELS,0.2229299363057325,"how the data distribution should look at each step of the denoising process, we turn to a stochastic
94"
SCORE-BASED DIFFUSION MODELS,0.22505307855626328,"differential equation (SDE) that describes a diffusion process from clean images to noise. The
95"
SCORE-BASED DIFFUSION MODELS,0.22717622080679406,"diffusion SDE is defined on the time interval t ∈[0, T] and has the form
96"
SCORE-BASED DIFFUSION MODELS,0.22929936305732485,"dx = f(x, t) + g(t)dw,
(3)"
SCORE-BASED DIFFUSION MODELS,0.23142250530785563,"where w ∈RD denotes Brownian motion. g(t) ∈R is the diffusion coefficient, which controls the
97"
SCORE-BASED DIFFUSION MODELS,0.23354564755838642,"rate of noise increase. f(·, t) : RD →RD is the drift coefficient, which controls the deterministic
98"
SCORE-BASED DIFFUSION MODELS,0.2356687898089172,"evolution of x(t). By defining a stochastic trajectory {x(t)}t∈[0,T ], this SDE gives rise to a time-
99"
SCORE-BASED DIFFUSION MODELS,0.23779193205944799,"dependent probability distribution pt, which is the marginal distribution of x(t). We construct f(·, t)
100"
SCORE-BASED DIFFUSION MODELS,0.23991507430997877,"and g(t) so that if p0 = pdata, then pT ≈π. Image generation amounts to reversing the diffusion,
101"
SCORE-BASED DIFFUSION MODELS,0.24203821656050956,"which requires the gradient of the data log-density (score) at every noise level in order to nudge
102"
SCORE-BASED DIFFUSION MODELS,0.24416135881104034,"images toward high probability under pdata. A convolutional neural network sθ known as a score
103"
SCORE-BASED DIFFUSION MODELS,0.24628450106157113,"model is trained to approximate the true score: sθ(x, t) ≈∇x log pt(x).
104"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.2484076433121019,"3.2
Image probabilities under a score-based diffusion model
105"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.2505307855626327,"Once trained, sθ(x, t) is used in a reverse-diffusion process to generate clean images from noise.
106"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.2526539278131635,"The generated image distribution theoretically assigns a probability density to every possible image.
107"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.25477707006369427,"However, reverse diffusion does not lead to an image distribution with tractable probabilities. In this
108"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.25690021231422505,"subsection, we describe two workarounds: one based on an ordinary differential equation (ODE) and
109"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.25902335456475584,"the other based on a denoising score-matching objective.
110"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.2611464968152866,"Sampling with a reverse-time SDE. Reversing diffusion (Eq. 3) with a score model sθ(x, t) results
111"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.2632696390658174,"in a distribution pSDE
θ
, denoted as such because it is determined by a reverse-time SDE:
112"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.2653927813163482,"dx =

f(x, t) −g(t)2sθ(x, t)

dt + g(t)d¯w.
(4)"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.267515923566879,"¯w ∈RD denotes Brownian motion, and f(·, t) and g(t) are the same as in Eq. 3. To generate an
113"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.26963906581740976,"image, we first sample x(T) ∼N(0, I) and then numerically solve the reverse-time SDE for x(0).
114"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.27176220806794055,"pSDE
θ
is the marginal distribution of x(0), which for a well-trained score model is close to pdata.
115"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.27388535031847133,"To compute the probability of an image x under pSDE
θ
, we need to invert this image from x(0) = x to
116"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.2760084925690021,"x(T). However, this is not tractable through the SDE: just as it is intractable to reverse a random
117"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.2781316348195329,"walk, it is intractable to account for all the possible starting points x(T) that could have resulted in
118"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.2802547770700637,"x(0) through the stochastic process. Probability computation calls for an invertible process that lets
119"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.2823779193205945,"us map any point from pdata to N(0, I) and vice versa.
120"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.28450106157112526,"Computing probabilities with an ODE. The probability flow ODE [23] defines an invertible
121"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.28662420382165604,"sampling function for an image distribution pODE
θ
theoretically the same as pSDE
θ
. It is given by
122 dx"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.28874734607218683,"dt = f(x, t) −1"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.2908704883227176,"2g(t)2sθ(x, t) =: ˜fθ(x, t).
(5)"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.2929936305732484,"The absence of Brownian motion makes it possible to solve this ODE in both directions of time. To
123"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.2951167728237792,"compute the log-probability of an image x, we map x(0) = x to its corresponding noise image x(T).
124"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.29723991507430997,"Under the framework of neural ODEs [4], the log-probability is given by the log-probability of x(T)
125"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.29936305732484075,"under N(0, I) plus a normalization factor accounting for the change in density through time:
126"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.30148619957537154,"log pODE
θ
(x(0)) = log π(x(T)) +
Z T"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.3036093418259023,"0
∇· ˜fθ(x(t), t)dt,
x(0) = x,
(6)"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.3057324840764331,"Although tractable to evaluate with an ODE solver, this log-probability function is computationally
127"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.3078556263269639,"expensive, requiring hundreds to thousands of discrete ODE time steps to accurately evaluate.
128"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.3099787685774947,"Additional time and memory costs are incurred by backpropagation through the ODE and Hutchinson-
129"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.31210191082802546,"Skilling trace estimation of the divergence.
130"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.31422505307855625,"Equivalence of pSDE
θ
and pODE
θ
. Song et al. [22] proved that if sθ(x, t) ≡∇x log pt(x, t) for all t ∈
131"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.31634819532908703,"[0, T] and pT = π, then pODE
θ
= pSDE
θ
= pdata. In our work, we assume that sθ(x, t) ≈∇x log pt(x, t)
132"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.3184713375796178,"for almost all x ∈RD and t ∈[0, T] and that pT ≈N(0, I), so that pODE
θ
≈pSDE
θ
≈pdata. This
133"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.3205944798301486,"assumption empirically performed well in previous work that appealed to pODE
θ
as the exact probability
134"
IMAGE PROBABILITIES UNDER A SCORE-BASED DIFFUSION MODEL,0.3227176220806794,"distribution of the diffusion model [10; 23].
135"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.3248407643312102,"3.3
Evidence lower bound of a score-based diffusion model
136"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.32696390658174096,"In lieu of an exact log-probability function, Song et al. [22] derived an evidence lower-bound for
137"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.32908704883227174,"pSDE
θ
such that bSDE
θ
(x) ≤log pSDE
θ
(x) for any proposed image x. Essentially, this lower-bound
138"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.33121019108280253,"corresponds to how well the diffusion model is able to denoise a given image: an image with high
139"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.3333333333333333,"probability under the diffusion model is easy to denoise, whereas a low-probability image is difficult.
140"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.3354564755838641,"The lower-bound, or the negative “denoising score-matching loss” [22], is defined as
141"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.3375796178343949,"bSDE
θ
(x) := Ep0T (x′|x)

log π(x′)

−1 2 Z T"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.33970276008492567,"0
g(t)2h(t)dt,
(7)"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.34182590233545646,"where
142"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.34394904458598724,h(t) := Ep0t(x′|x)
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.346072186836518,"sθ(x′, t) −∇x′ log p0t(x′ | x)
2
2 −
∇x′ log p0t(x′ | x)
2
2 −
2
g(t)2 ∇x′ · f(x′, t)

. (8)"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.3481953290870488,"p0t(x′ | x) denotes the transition distribution from x(0) = x to x(t) = x′. For a drift coefficient
143"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.3503184713375796,"that is linear in x, this transition distribution is Gaussian: p0t(x′ | x) = N(x′; α(t)x, β(t)2I). This
144"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.3524416135881104,"means that the gradient ∇x′ log p0t(x′ | x) is directly proportional to the Gaussian noise that is
145"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.35456475583864117,"subtracted from x′ to get x. Eq. 7 is efficient to compute since we can evaluate it by adding Gaussian
146"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.35668789808917195,"noise to x without having to solve an initial-value problem as with the ODE. In fact, Eq. 7 is closely
147"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.35881104033970274,"related to the denoising score-matching objective used to efficiently train diffusion models [23].
148"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.3609341825902335,"Intuitively, we can interpret Eq. 7 as associating an image’s probability with how well the score model
149"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.3630573248407643,"sθ could denoise that image if it underwent diffusion. This is represented by the first term in h(t)
150"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.3651804670912951,"(Eq. 8). To assess the probability of an image x, we perturb it with Gaussian noise to get x′ and then
151"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.3673036093418259,"ask the score model to estimate the noise that was added. If sθ(x, t) accurately estimates the noise,
152"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.36942675159235666,"then ∥sθ(x′, t) −∇x′ log p0t(x′ | x)∥2
2 is small, and the value of bSDE
θ
(x) becomes larger.
153"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.37154989384288745,"The remaining terms in h(t) are normalizing factors independent of θ. The term Ep0T (x′|x) [log π(x′)]
154"
EVIDENCE LOWER BOUND OF A SCORE-BASED DIFFUSION MODEL,0.37367303609341823,"accounts for the probabilities of the noise images x(T) that could result from x being entirely diffused.
155"
METHOD,0.37579617834394907,"4
Method
156"
METHOD,0.37791932059447986,"Inspired by previous theoretical work [22], we propose bSDE
θ
as an efficient surrogate prior for the
157"
METHOD,0.38004246284501064,"exact score-based prior in Bayesian imaging. In this section, we describe our approach for efficient
158"
METHOD,0.3821656050955414,"posterior inference with a score-based prior.
159"
VARIATIONAL INFERENCE WITH A SURROGATE SCORE-BASED PRIOR,0.3842887473460722,"4.1
Variational inference with a surrogate score-based prior
160"
VARIATIONAL INFERENCE WITH A SURROGATE SCORE-BASED PRIOR,0.386411889596603,"Given measurements y ∈RM (with a known log-likelihood function) and a score-based diffusion
161"
VARIATIONAL INFERENCE WITH A SURROGATE SCORE-BASED PRIOR,0.3885350318471338,"model (parameterized by θ) as the prior, our goal is to sample from the image posterior pθ(x | y).
162"
VARIATIONAL INFERENCE WITH A SURROGATE SCORE-BASED PRIOR,0.39065817409766457,"We follow a variational-inference approach by optimizing the parameters of a variational distribution
163"
VARIATIONAL INFERENCE WITH A SURROGATE SCORE-BASED PRIOR,0.39278131634819535,"to closely approximate the target posterior.
164"
VARIATIONAL INFERENCE WITH A SURROGATE SCORE-BASED PRIOR,0.39490445859872614,"Let qϕ denote the variational distribution with parameters ϕ, and we assume qϕ to have tractable
165"
VARIATIONAL INFERENCE WITH A SURROGATE SCORE-BASED PRIOR,0.3970276008492569,"log-probabilities. We optimize ϕ to minimize the KL divergence from qϕ to the target posterior:
166"
VARIATIONAL INFERENCE WITH A SURROGATE SCORE-BASED PRIOR,0.3991507430997877,"ϕ∗= arg min
ϕ DKL(qϕ∥pθ(· | y)) = arg min
ϕ Ex∼qϕ
h
−log p(y | x) −log pODE
θ
(x) + log qϕ(x)
i
.
(9)"
VARIATIONAL INFERENCE WITH A SURROGATE SCORE-BASED PRIOR,0.4012738853503185,"qϕ can be various types of distributions. For example, it could be a Gaussian distribution with a
167"
VARIATIONAL INFERENCE WITH A SURROGATE SCORE-BASED PRIOR,0.4033970276008493,"diagonal covariance matrix so that ϕ := [µ⊤, σ⊤]⊤, where µ ∈RD and σ ∈RD (σ > 0) are
168"
VARIATIONAL INFERENCE WITH A SURROGATE SCORE-BASED PRIOR,0.40552016985138006,"the mean and pixel-wise standard deviation. As DPI showed [25], qϕ could also be a RealNVP
169"
VARIATIONAL INFERENCE WITH A SURROGATE SCORE-BASED PRIOR,0.40764331210191085,"normalizing flow with network parameters ϕ.
170"
VARIATIONAL INFERENCE WITH A SURROGATE SCORE-BASED PRIOR,0.40976645435244163,"To circumvent the computational challenges of evaluating the prior term log pODE
θ
(x), we replace it
171"
VARIATIONAL INFERENCE WITH A SURROGATE SCORE-BASED PRIOR,0.4118895966029724,"with the surrogate bSDE
θ
(x). This results in the following objective:
172"
VARIATIONAL INFERENCE WITH A SURROGATE SCORE-BASED PRIOR,0.4140127388535032,"ϕ∗= arg min
ϕ Ex∼qϕ

−log p(y | x) −bSDE
θ
(x) + log qϕ(x)

.
(10)"
VARIATIONAL INFERENCE WITH A SURROGATE SCORE-BASED PRIOR,0.416135881104034,"We can also think of bSDE
θ
as replacing the intractable log pSDE
θ
in Eq. 9. Since −log pSDE
θ
≤−bSDE
θ
,
173"
VARIATIONAL INFERENCE WITH A SURROGATE SCORE-BASED PRIOR,0.4182590233545648,"our surrogate objective minimizes the upper-bound of a valid KL divergence involving pSDE
θ
.
174"
IMPLEMENTATION DETAILS,0.42038216560509556,"4.2
Implementation details
175"
IMPLEMENTATION DETAILS,0.42250530785562634,"Evaluating bSDE
θ
(x). The formula for bSDE
θ
(x) (Eq. 7) contains a time integral and expectation
176"
IMPLEMENTATION DETAILS,0.42462845010615713,"over p0t(x′ | x) that can be estimated with numerical methods. Following Song et al. [22], we use
177"
IMPLEMENTATION DETAILS,0.4267515923566879,"importance sampling with time samples t ∼p(t) for the time integral and Monte-Carlo approximation
178"
IMPLEMENTATION DETAILS,0.4288747346072187,"with noisy images x′ ∼N(α(t)x, β(t)2I) for the expectation. The proposal distribution p(t) :=
179 g(t)2"
IMPLEMENTATION DETAILS,0.4309978768577495,"β(t)2Z was empirically verified to result in lower variance in the estimation of bSDE
θ
(x) [22]. We
180"
IMPLEMENTATION DETAILS,0.43312101910828027,"provide the following formula used in our implementation, which estimates the time integral with
181"
IMPLEMENTATION DETAILS,0.43524416135881105,"importance sampling and the expectation with Monte-Carlo approximation, for reference:
182"
IMPLEMENTATION DETAILS,0.43736730360934184,"bSDE
θ
(x) ≈
1
Nz Nz
X"
IMPLEMENTATION DETAILS,0.4394904458598726,"j=1
log π(x′
j)"
IMPLEMENTATION DETAILS,0.4416135881104034,"−
1
2NtNz Nt
X"
IMPLEMENTATION DETAILS,0.4437367303609342,"i=1
Zβ(t)2
Nz
X j=1"
IMPLEMENTATION DETAILS,0.445859872611465,"""sθ(x′
ij, ti) +
zij
β(ti)  2"
IMPLEMENTATION DETAILS,0.44798301486199577,"2
−

zij
β(ti)  2"
IMPLEMENTATION DETAILS,0.45010615711252655,"2
−
2
g(ti)2 ∇x′
ij · f(x′
ij, ti) #"
IMPLEMENTATION DETAILS,0.45222929936305734,"s.t.
ti ∼p(t), zij ∼N(0, I), x′
ij = α(ti)x + β(ti)zij, x′
j ∼N(α(T)x, β(T)2I)
∀i = 1, . . . , Nt, j = 1, . . . , Nz.
(11)"
IMPLEMENTATION DETAILS,0.4543524416135881,"Nt is the number of time samples used to approximate the time integral, and Nz is the number
183"
IMPLEMENTATION DETAILS,0.4564755838641189,"of noise samples taken to approximate the expectation over p0t(x′ | x). In our experiments, we
184"
IMPLEMENTATION DETAILS,0.4585987261146497,"set Nt = Nz = 1. Increasing the number of time and noise samples does not efficiently decrease
185"
IMPLEMENTATION DETAILS,0.4607218683651805,"variance in the estimated value of bSDE
θ
(x). We use the Variance Preserving (VP) SDE.
186"
IMPLEMENTATION DETAILS,0.46284501061571126,"Optimization. We use stochastic gradient descent to optimize ϕ, Monte-Carlo approximating the
187"
IMPLEMENTATION DETAILS,0.46496815286624205,"expectation in Eq. 10 with a batch of x ∼qϕ. We find that estimating bSDE
θ
(x) has higher variance
188"
IMPLEMENTATION DETAILS,0.46709129511677283,"than estimating log pODE
θ
(x). For example, in Fig. 4, bSDE
θ
(x) with Nt = 2048, Nz = 1 shows higher
189"
IMPLEMENTATION DETAILS,0.4692144373673036,"variance than log pODE
θ
(x) with 16 trace estimators. When optimizing a complex distribution like
190"
IMPLEMENTATION DETAILS,0.4713375796178344,"RealNVP, a lower learning-rate helps mitigate training instabilities caused by variance. For example,
191"
IMPLEMENTATION DETAILS,0.4734607218683652,"in Fig. 3b the learning rate with the exact prior was 0.0002, while the learning rate with the surrogate
192"
IMPLEMENTATION DETAILS,0.47558386411889597,"prior was 0.00001. Please refer to the supplemental text for more optimization details.
193"
EXPERIMENTS,0.47770700636942676,"5
Experiments
194"
EXPERIMENTS,0.47983014861995754,"We validate our proposed approach on the tasks of accelerated MRI, image denoising, and reconstruc-
195"
EXPERIMENTS,0.4819532908704883,"tion from low spatial frequencies. We highlight accelerated (or compressed sensing) MRI because in
196"
EXPERIMENTS,0.4840764331210191,"addition to being a real-world imaging problem that calls for accurate posterior estimation, it is the
197"
EXPERIMENTS,0.4861995753715499,"focus of much related work [24; 13]. In MRI, measurements in a spatial-frequency space (κ-space) are
198"
EXPERIMENTS,0.4883227176220807,"obtained to help reveal a hidden anatomical image. Accelerated MRI reduces the number of κ-space
199"
EXPERIMENTS,0.49044585987261147,"measurements, thus reducing the scan time but also making the image reconstruction ill-posed. The
200"
EXPERIMENTS,0.49256900212314225,"supplemental text provides details on how measurements were generated for all tasks.
201"
EFFICIENCY IMPROVEMENTS,0.49469214437367304,"5.1
Efficiency improvements
202"
EFFICIENCY IMPROVEMENTS,0.4968152866242038,"Image size
Surrogate
Exact
16 × 16
0.029
19.5
32 × 32
0.038
41.9
64 × 64
0.090
123
128 × 128
0.294
N/A
256 × 256
1.115
N/A"
EFFICIENCY IMPROVEMENTS,0.4989384288747346,"Table 1: Iteration time [sec/step].
Each iteration of gradient-based op-
timization of the variational distri-
bution is 2 to 3 orders of magnitude
faster with the surrogate prior."
EFFICIENCY IMPROVEMENTS,0.5010615711252654,"In Tab. 1 and Fig. 2, we quantify the efficiency improvements
203"
EFFICIENCY IMPROVEMENTS,0.5031847133757962,"of the surrogate prior for an accelerated MRI task at different
204"
EFFICIENCY IMPROVEMENTS,0.505307855626327,"image resolutions. We drew a test image from the fastMRI knee
205"
EFFICIENCY IMPROVEMENTS,0.5074309978768577,"dataset [27] and resized it to 16 × 16, 32 × 32, 64 × 64, 128 ×
206"
EFFICIENCY IMPROVEMENTS,0.5095541401273885,"128, and 256 × 256. For each image size, we trained a score
207"
EFFICIENCY IMPROVEMENTS,0.5116772823779193,"model on training images of the corresponding size from the
208"
EFFICIENCY IMPROVEMENTS,0.5138004246284501,"fastMRI dataset of single-coil knee scans. We then optimized a
209"
EFFICIENCY IMPROVEMENTS,0.5159235668789809,"Gaussian distribution with diagonal covariance to approximate
210"
EFFICIENCY IMPROVEMENTS,0.5180467091295117,"the posterior. The batch size was 64 for the surrogate and 32 for
211"
EFFICIENCY IMPROVEMENTS,0.5201698513800425,"the exact prior (a smaller batch size was needed to fit 64 × 64
212"
EFFICIENCY IMPROVEMENTS,0.5222929936305732,"optimization into GPU memory). Convergence was defined
213"
EFFICIENCY IMPROVEMENTS,0.524416135881104,"by setting a minimum acceptable change in the mean of the
214"
EFFICIENCY IMPROVEMENTS,0.5265392781316348,"estimated posterior between optimization steps.
215"
EFFICIENCY IMPROVEMENTS,0.5286624203821656,"We find at least two orders of magnitude in time improvement
216"
EFFICIENCY IMPROVEMENTS,0.5307855626326964,"with the surrogate prior. Tab. 1 compares the iteration time
217"
EFFICIENCY IMPROVEMENTS,0.5329087048832272,"between the two priors. Fig. 2 compares the total time it takes to optimize the variational distribution.
218"
EFFICIENCY IMPROVEMENTS,0.535031847133758,"The surrogate also significantly improves memory consumption, which in turn enables optimizing
219"
EFFICIENCY IMPROVEMENTS,0.5371549893842887,"higher-dimensional posteriors. Following standard practice, we just-in-time (JIT) compile the
220"
EFFICIENCY IMPROVEMENTS,0.5392781316348195,"optimization step to reduce time/step at the cost of GPU memory. Fig. 2 shows how the surrogate
221"
EFFICIENCY IMPROVEMENTS,0.5414012738853503,"prior significantly reduces memory requirements and scales better with image size. The exact prior
222"
EFFICIENCY IMPROVEMENTS,0.5435244161358811,"could only handle up to 32 × 32 before exceeding GPU memory (we tested on 4x 48GB GPUs).
223"
EFFICIENCY IMPROVEMENTS,0.5456475583864119,"While memory could be reduced with a smaller batch size, this would make optimization more time-
224"
EFFICIENCY IMPROVEMENTS,0.5477707006369427,"consuming. On the other hand, our surrogate prior supports much larger images, as we demonstrate in
225"
EFFICIENCY IMPROVEMENTS,0.5498938428874734,"Fig. 1 for 256 × 2561 MRI with a Gaussian-approximated posterior. This type of principled inference
226"
EFFICIENCY IMPROVEMENTS,0.5520169851380042,"of high-dimensional image posteriors was not possible before with the exact score-based prior.
227"
EFFICIENCY IMPROVEMENTS,0.554140127388535,"5.2
Posterior estimation under the surrogate vs. exact prior
228"
EFFICIENCY IMPROVEMENTS,0.5562632696390658,"We cannot expect the surrogate prior bSDE
θ
to be an identical substitute for the exact prior log pODE
θ
.
229"
EFFICIENCY IMPROVEMENTS,0.5583864118895966,"Importantly, though, we verify in Fig. 3a that both the surrogate and the exact prior recover a ground-
230"
EFFICIENCY IMPROVEMENTS,0.5605095541401274,"truth Gaussian posterior derived from a Gaussian likelihood and prior. The variational distribution
231"
EFFICIENCY IMPROVEMENTS,0.5626326963906582,"1Larger images may be feasible but are more memory-intensive, which imposes more restrictions on the
batch size and the complexity of the variational distribution."
EFFICIENCY IMPROVEMENTS,0.564755838641189,Dimensionality
EFFICIENCY IMPROVEMENTS,0.5668789808917197,Memory [GB]
EFFICIENCY IMPROVEMENTS,0.5690021231422505,Dimensionality
EFFICIENCY IMPROVEMENTS,0.5711252653927813,"Optimization Time
Optimization Memory"
EFFICIENCY IMPROVEMENTS,0.5732484076433121,"PSNR
SSIM"
EFFICIENCY IMPROVEMENTS,0.5753715498938429,Image-Restoration Quality
EFFICIENCY IMPROVEMENTS,0.5774946921443737,Dimensionality
EFFICIENCY IMPROVEMENTS,0.5796178343949044,39 hrs. 43 hrs.
EFFICIENCY IMPROVEMENTS,0.5817409766454352,126 hrs.
EFFICIENCY IMPROVEMENTS,0.583864118895966,"19 mins. 19 mins. 45 mins.
2 hrs.
9 hrs.
16 GB
31 GB"
GB,0.5859872611464968,89 GB
GB,0.5881104033970276,"8 GB
12 GB"
GB,0.5902335456475584,17 GB
GB,0.5923566878980892,44 GB
GB,0.5944798301486199,142 GB
GB,0.5966029723991507,Time [sec]
GB,0.5987261146496815,"= Exact
= Surrogate"
GB,0.6008492569002123,"= Exact
= Surrogate"
GB,0.6029723991507431,"Figure 2: Computational efficiency of our proposed surrogate prior (“Surrogate”) vs. exact prior
(“Exact”). For each image size, we estimated a posterior of images conditioned on 4×-accelerated
MRI measurements of a knee image, using a Gaussian distribution with diagonal covariance as the
variational distribution. The hardware is 4x NVIDIA RTX A6000. The surrogate prior allows for
variational inference of image sizes that are prohibitively large for the exact prior. For image sizes
supported by the exact prior, the surrogate improved total optimization time by over 120× while
using less memory and scaling better with image size. “Image-Restoration Quality” verifies that
optimization with the surrogate was done fairly, as the PSNR and SSIM of the converged posterior
(averaged over 128 samples) are at least as high as with the exact prior. -0.35"
GB,0.6050955414012739,"Mean
Std. Dev."
GB,0.6072186836518046,"True
Est."
GB,0.6093418259023354,"(Surrogate)
Est."
GB,0.6114649681528662,(Exact)
GB,0.613588110403397,"0.89 0.10
0.16"
GB,0.6157112526539278,(a) Ground-truth (Gaussian) posterior.
GB,0.6178343949044586,"Surrogate
Exact"
GB,0.6199575371549894,"Mean
Std. Dev."
GB,0.6220806794055201,"Original
Observed
Original
Observed"
GB,0.6242038216560509,"Mean
Std. Dev."
GB,0.6263269639065817,"0.01
0.12
0.01
0.1"
GB,0.6284501061571125,"(i) CelebA denoising
(ii) CIFAR-10 denoising"
GB,0.6305732484076433,(b) Complex posteriors.
GB,0.6326963906581741,"Figure 3: Estimated posteriors under surrogate vs. exact prior. For each task, the variational distri-
bution is a RealNVP, and the score model is the same between both prior functions. (a) Both prior
functions recover the correct (Gaussian) posterior. The score-based prior was trained on samples
from a known Gaussian distribution (originally fit to 16 × 16 face images), and the measurements are
the lowest 6.25% spatial frequencies of a test image from the prior. Since the prior and likelihood
are both Gaussian, we know the ground-truth Gaussian posterior. (b) We estimate posteriors for (i)
denoising a CelebA image and (ii) denoising a CIFAR-10 image. The score-based prior was trained
on CelebA in (i) and CIFAR-10 in (ii). Visual differences between the estimated posteriors appear
mostly in the image background, and the prior functions result in comparable image quality."
GB,0.6348195329087049,"used for inference is a RealNVP, and the score model (used by both the surrogate and exact prior)
232"
GB,0.6369426751592356,"was trained on samples from the known Gaussian prior.
233"
GB,0.6390658174097664,"Nonetheless, the surrogate could result in a different locally-optimal variational posterior, particularly
234"
GB,0.6411889596602972,"if the posterior is complex with various local minima in the variational objective. Fig. 3b compares
235"
GB,0.643312101910828,"posteriors (with unknown true distribution) approximated by a RealNVP under the surrogate versus
236"
GB,0.6454352441613588,"exact prior. For each task (CelebA denoising and CIFAR-10 denoising), both prior functions used the
237"
GB,0.6475583864118896,"same pretrained score model. We observe in these comparisons that most of the differences appear in
238"
GB,0.6496815286624203,"the image background and that both priors result in a plausible mean reconstruction and uncertainty.
239"
GB,0.6518046709129511,"Visualizing the bound bap throughout optimization helps shed light on why the two priors converge
240"
GB,0.6539278131634819,"to different solutions even if the underlying score model is the same. Fig. 4 shows probabilities of
241"
GB,0.6560509554140127,"samples generated by qϕ (in this case, a RealNVP) as optimization progresses. At each checkpoint
242"
GB,0.6581740976645435,"of qϕ, we plot log pODE
θ
(x) versus bSDE
θ
(x) (approximated with Nt = 2048 for reduced variance)
243"
GB,0.6602972399150743,"for samples x ∼qϕ coming from both the exact and surrogate optimization of qϕ. Importantly, we
244"
GB,0.6624203821656051,"find that the surrogate is a valid bound for the ODE log-density: bSDE
θ
(x) ≤log pODE
θ
(x) for all
245"
GB,0.6645435244161358,"x ∼qϕ(x), except for some outliers due to variance of bSDE
θ
(x). However, we find that optimization
246"
GB,0.6666666666666666,"follows a different trajectory depending on the prior. With the surrogate, samples x ∼qϕ tend toward
247"
GB,0.6687898089171974,"a region where the bound gap is small (i.e., bSDE
θ
(x) is close to log pODE
θ
(x)). Meanwhile, the exact
248"
GB,0.6709129511677282,"prior follows a loss landscape whose structure appears to be independent of the lower-bound. Note
249"
GB,0.673036093418259,"that samples from qϕ optimized under the exact prior obtain higher values of bSDE
θ
(x) than samples
250"
GB,0.6751592356687898,"obtained under the surrogate. The observations in Fig. 4 suggest that gradients under the surrogate
251"
GB,0.6772823779193206,"tend to push the qϕ distribution along the boundary of equality between bSDE
θ
and log pODE
θ
. This
252"
GB,0.6794055201698513,"constrains the path taken through gradient descent and subsequently the converged solution.
253"
GB,0.6815286624203821,"Exact
Surrogate"
GB,0.6836518046709129,Step 20000
GB,0.6857749469214437,Step 1000000
GB,0.6878980891719745,"Step 500
Step 100"
GB,0.6900212314225053,"Step 5000
Step 25000"
GB,0.692144373673036,Step 100
GB,0.6942675159235668,Step 500
GB,0.6963906581740976,Step 20K
GB,0.6985138004246284,Step 5K
GB,0.7006369426751592,Step 25K
GB,0.70276008492569,Step 1M
GB,0.7048832271762208,"Plotting 
trajectories on"
GB,0.7070063694267515,same scale
GB,0.7091295116772823,log pODE
GB,0.7112526539278131,"✓
(               )
bSDE"
GB,0.7133757961783439,"✓
(         )"
GB,0.7154989384288747,"log pODE ✓
(x) bSDE ✓
(x)"
GB,0.7176220806794055,log pODE
GB,0.7197452229299363,"✓
(x)
log pODE ✓
(x) bSDE ✓
(x) bSDE ✓
(x)"
GB,0.721868365180467,log pODE
GB,0.7239915074309978,"✓
(x)
log pODE"
GB,0.7261146496815286,"✓
(x)
log pODE ✓
(x) bSDE ✓
(x) bSDE ✓
(x) bSDE ✓
(x) bSDE"
GB,0.7282377919320594,"✓
(x)
bSDE ✓
(x)"
GB,0.7303609341825902,"amp6ZnZtfWMwtLa+srtnrGzc6ThWHCo9lrGo+0yBFBUKGWKGChL6Hqd08HfvUOlBZxdI29Boha0ciEJyhkZr2jifjNk2aHnYA2a2HcI/Z5dl5v+CFDt+kN395p23ik6Q9BJ4o5JnoxRbtqfXivmaQgRcsm0rtOgo2MKRcQj/npRoSxrusDXVDIxaCbmTDb/p01ygtGsTKVIR0qP6cyFiodS/0TefgRP3XG4j/efUg+NGJqIkRYj4aFGQSoxHURDW0IBR9kzhHElzK2Ud5hiHE2AOROC+/flSXKzX3QPi87VQb50Mo5jgWyRbVIgLjkiJXJByqRCOHkgT+SFvFqP1rP1Zr2PWqes8cwm+QXr4xuyVpv0</latexit>log pODE ✓
(x)"
GB,0.732484076433121,"log pODE ✓
(x)"
GB,0.7346072186836518,"Figure 4: bSDE
θ
(x) vs. log pODE
θ
(x) for samples x ∼qϕ as optimization of ϕ progresses. The task
is from Fig. 3b(i). For each plot, we took 128 samples x ∼qϕ and performed 20 estimates each
of bSDE
θ
(x) and log pODE
θ
(x). The density map is a KDE plot of all 128 · 20 = 2560 values; the 128
scatter points represent the mean estimate for each x. The black line indicates perfect agreement
between bSDE
θ
(x) and log pODE
θ
(x). We expect all points to lie below this black line for bSDE
θ
to be
a lower-bound. We find that bSDE
θ
(x) ≤log pODE
θ
(x) (up to variance error), but the optimization
progresses differently depending on the prior. Gradients under the surrogate push qϕ(x) along the
black line to increase bSDE
θ
(x) without exceeding log pODE
θ
(x). Optimization under the exact prior
proceeds more freely, although eventually achieves higher bSDE
θ
(x) at convergence. This visualization
may help explain differences in the posterior estimated with the surrogate vs. exact prior."
IMAGE-RECONSTRUCTION QUALITY,0.7367303609341825,"5.3
Image-reconstruction quality
254"
IMAGE-RECONSTRUCTION QUALITY,0.7388535031847133,"It would be reasonable to assume that diffusion-based approaches discussed in Sec. 2, although less
255"
IMAGE-RECONSTRUCTION QUALITY,0.7409766454352441,"principled, may lead to better visual quality than a Bayesian approach. However, we find that in
256"
IMAGE-RECONSTRUCTION QUALITY,0.7430997876857749,"addition to providing more-reliable uncertainty, our approach achieves higher-fidelity reconstructions.
257"
IMAGE-RECONSTRUCTION QUALITY,0.7452229299363057,"We note that similarity to a ground-truth image does not indicate a correct posterior. Still, for a good
258"
IMAGE-RECONSTRUCTION QUALITY,0.7473460721868365,"prior, it might be desirable for posterior samples to accurately reflect the true underlying image.
259"
IMAGE-RECONSTRUCTION QUALITY,0.7494692144373672,"We performed multiple MRI tasks at different acceleration rates and compared our approach to three
260"
IMAGE-RECONSTRUCTION QUALITY,0.7515923566878981,"baselines: SDE+Proj [24], Score-ALD [13], and Diffusion Posterior Sampling (DPS) [9]. SDE+Proj
261"
IMAGE-RECONSTRUCTION QUALITY,0.7537154989384289,"projects images onto a measurement subspace. Score-ALD and DPS approximate the posterior
262"
IMAGE-RECONSTRUCTION QUALITY,0.7558386411889597,"throughout reverse diffusion. All baselines involve at least one measurement-weight hyperparameter.
263"
IMAGE-RECONSTRUCTION QUALITY,0.7579617834394905,"The implementations and hyperparameter settings for SDE+Proj and Score-ALD were provided by
264"
IMAGE-RECONSTRUCTION QUALITY,0.7600849256900213,"Song et al. [24]. For DPS, we followed the implementation of Chung et al. [9] and performed a
265"
IMAGE-RECONSTRUCTION QUALITY,0.7622080679405521,"hyperparameter search on an 8×-acceleration test image to find the optimal PSNR.
266"
IMAGE-RECONSTRUCTION QUALITY,0.7643312101910829,"We simulated MRI at three different acceleration factors for ten test images, resulting in thirty
267"
IMAGE-RECONSTRUCTION QUALITY,0.7664543524416136,"posterior distributions to be estimated. As baseline implementations do not account for measurement
268"
IMAGE-RECONSTRUCTION QUALITY,0.7685774946921444,"noise, we gave the baselines noiseless measurements and set a near-zero measurement noise for our
269"
IMAGE-RECONSTRUCTION QUALITY,0.7707006369426752,"method. The test images were randomly sampled from the fastMRI dataset and resized to 64 × 64.
270"
IMAGE-RECONSTRUCTION QUALITY,0.772823779193206,Avg. PSNR
IMAGE-RECONSTRUCTION QUALITY,0.7749469214437368,= Ours
IMAGE-RECONSTRUCTION QUALITY,0.7770700636942676,"(DPI + Surrogate) 
= SDE+Proj
= DPS
= Score-ALD"
IMAGE-RECONSTRUCTION QUALITY,0.7791932059447984,Avg. SSIM
IMAGE-RECONSTRUCTION QUALITY,0.7813163481953291,"Acceleration factor
Acceleration factor"
IMAGE-RECONSTRUCTION QUALITY,0.7834394904458599,(a) Image-restoration metrics.
IMAGE-RECONSTRUCTION QUALITY,0.7855626326963907,"Original
Ours
(DPI + Surr.)
SDE+Proj
Score-ALD
0-Filled Recon."
IMAGE-RECONSTRUCTION QUALITY,0.7876857749469215,"(16x-accel.)
DPS"
IMAGE-RECONSTRUCTION QUALITY,0.7898089171974523,(b) Example image reconstructions for 16× acceleration.
IMAGE-RECONSTRUCTION QUALITY,0.7919320594479831,"Figure 5: Accelerated MRI of knee images. (a) For each acceleration factor (4×, 8×, 16×), we
estimated posteriors for ten images measured at that acceleration rate. Baseline methods do not
capture a true posterior: Score-ALD and DPS strongly approximate the posterior uncertainty, and
SDE+Proj is a non-Bayesian projection-based approach. For each method, we computed the average
PSNR and SSIM of 128 estimated posterior samples. The line plot shows the average result across
the ten tasks; the shaded region shows one std. dev. above and below the average. (b) An example of
16×-accel. MRI. The cropped region exemplifies how baselines hallucinate incorrect more features
than necessary. (a) and (b) are evidence that a principled Bayesian approach can capture a more
accurate posterior than previous unsupervised methods."
IMAGE-RECONSTRUCTION QUALITY,0.7940552016985138,"Our approach was DPI with the surrogate prior, meaning we optimized a RealNVP to approximate
271"
IMAGE-RECONSTRUCTION QUALITY,0.7961783439490446,"each posterior and used the lower-bound function bSDE
θ
as the prior log-density. The score model sθ
272"
IMAGE-RECONSTRUCTION QUALITY,0.7983014861995754,"was trained on 64 × 64 images of knee scans from fastMRI and stayed fixed across all methods.
273"
IMAGE-RECONSTRUCTION QUALITY,0.8004246284501062,"Our method achieves a marked improvement in PSNR and SSIM over the three baselines (Fig. 5).
274"
IMAGE-RECONSTRUCTION QUALITY,0.802547770700637,"Across all acceleration factors and baselines, our method improves PSNR by between 2.7 and 8.5 dB.
275"
IMAGE-RECONSTRUCTION QUALITY,0.8046709129511678,"Even though each method uses the same score model, restoration quality depends on how the prior is
276"
IMAGE-RECONSTRUCTION QUALITY,0.8067940552016986,"used for inference; whereas baselines loosely approximate the posterior and involve hyperparameters,
277"
IMAGE-RECONSTRUCTION QUALITY,0.8089171974522293,"our approach treats the diffusion model as a standalone prior in Bayesian inference.
278"
CONCLUSION,0.8110403397027601,"6
Conclusion
279"
CONCLUSION,0.8131634819532909,"We have presented a surrogate function that provides efficient access to score-based priors for
280"
CONCLUSION,0.8152866242038217,"Bayesian inference. We empirically verify that the evidence lower-bound bSDE
θ
(x) ≤log pSDE
θ
(x) can
281"
CONCLUSION,0.8174097664543525,"serve as a proxy for evaluating the log-prior of an image under a trained diffusion model. Paired
282"
CONCLUSION,0.8195329087048833,"with any log-likelihood function, bSDE
θ
(x) can be plugged into a Bayesian-inference algorithm. Our
283"
CONCLUSION,0.821656050955414,"experiments with variational inference show at least two orders of magnitude in runtime improvement
284"
CONCLUSION,0.8237791932059448,"and significant memory improvement over the ODE-based prior. This enables inference of images
285"
CONCLUSION,0.8259023354564756,"previously too large for a strictly Bayesian approach, such as 256 × 256 pixels. We also establish
286"
CONCLUSION,0.8280254777070064,"that a principled approach like ours outperforms baselines on image-restoration metrics, evidence
287"
CONCLUSION,0.8301486199575372,"that following a Bayesian approach results in more-reliable image reconstructions.
288"
CONCLUSION,0.832271762208068,"Limitations. A variational approach like ours depends on the expressiveness of the variational distri-
289"
CONCLUSION,0.8343949044585988,"bution. Improvements may be possible by using a diffusion model instead of a discrete normalizing
290"
CONCLUSION,0.8365180467091295,"flow as the variational distribution. We also note that there are open theoretical questions about bSDE
θ
291"
CONCLUSION,0.8386411889596603,"as it relates to pODE
θ
[16]. Broader impact. Our proposed framework for efficient estimation of
292"
CONCLUSION,0.8407643312101911,"high-dimensional, sophisticated posteriors has broad potential impact for computational imaging.
293"
CONCLUSION,0.8428874734607219,"Many imaging tasks, especially in science and medicine, would benefit from accurate uncertainty
294"
CONCLUSION,0.8450106157112527,"quantification with principled, data-driven priors.
295"
REFERENCES,0.8471337579617835,"References
296"
REFERENCES,0.8492569002123143,"[1] Alexandre Adam, Adam Coogan, Nikolay Malkin, Ronan Legin, Laurence Perreault-Levasseur, Yashar
297"
REFERENCES,0.851380042462845,"Hezaveh, and Yoshua Bengio. Posterior samples of source galaxies in strong gravitational lenses with
298"
REFERENCES,0.8535031847133758,"score-based priors. arXiv preprint arXiv:2211.03812, 2022.
299"
REFERENCES,0.8556263269639066,"[2] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians.
300"
REFERENCES,0.8577494692144374,"Journal of the American statistical Association, 112(518):859–877, 2017.
301"
REFERENCES,0.8598726114649682,"[3] Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Handbook of markov chain monte carlo.
302"
REFERENCES,0.861995753715499,"CRC press, 2011.
303"
REFERENCES,0.8641188959660298,"[4] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential
304"
REFERENCES,0.8662420382165605,"equations. NeurIPS, 31, 2018.
305"
REFERENCES,0.8683651804670913,"[5] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning
306"
REFERENCES,0.8704883227176221,"method for denoising diffusion probabilistic models. In ICCV. IEEE, 2021.
307"
REFERENCES,0.8726114649681529,"[6] Hyungjin Chung and Jong Chul Ye. Score-based diffusion models for accelerated mri. Medical Image
308"
REFERENCES,0.8747346072186837,"Analysis, 80:102479, 2022.
309"
REFERENCES,0.8768577494692145,"[7] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for inverse
310"
REFERENCES,0.8789808917197452,"problems using manifold constraints. arXiv preprint arXiv:2206.00941, 2022.
311"
REFERENCES,0.881104033970276,"[8] Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-closer-diffuse-faster: Accelerating conditional
312"
REFERENCES,0.8832271762208068,"diffusion models for inverse problems through stochastic contraction. In Proceedings of the IEEE/CVF
313"
REFERENCES,0.8853503184713376,"Conference on Computer Vision and Pattern Recognition, pages 12413–12422, 2022.
314"
REFERENCES,0.8874734607218684,"[9] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye.
315"
REFERENCES,0.8895966029723992,"Diffusion posterior sampling for general noisy inverse problems. In The Eleventh International Conference
316"
REFERENCES,0.89171974522293,"on Learning Representations, 2023. URL https://openreview.net/forum?id=OnD9zGAGT0k.
317"
REFERENCES,0.8938428874734607,"[10] Berthy T Feng, Jamie Smith, Michael Rubinstein, Huiwen Chang, Katherine L Bouman, and William T
318"
REFERENCES,0.8959660297239915,"Freeman.
Score-based diffusion models as principled priors for inverse imaging.
arXiv preprint
319"
REFERENCES,0.8980891719745223,"arXiv:2304.11751, 2023.
320"
REFERENCES,0.9002123142250531,"[11] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diffusion models as plug-
321"
REFERENCES,0.9023354564755839,"and-play priors. In Thirty-Sixth Conference on Neural Information Processing Systems, 2022. URL
322"
REFERENCES,0.9044585987261147,"https://arxiv.org/pdf/2206.09012.pdf.
323"
REFERENCES,0.9065817409766455,"[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural
324"
REFERENCES,0.9087048832271762,"Information Processing Systems, 33:6840–6851, 2020.
325"
REFERENCES,0.910828025477707,"[13] Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jonathan I Tamir. Robust
326"
REFERENCES,0.9129511677282378,"compressed sensing mri with deep generative priors. NeurIPS, 2021.
327"
REFERENCES,0.9150743099787686,"[14] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models.
328"
REFERENCES,0.9171974522292994,"In Advances in Neural Information Processing Systems, 2022.
329"
REFERENCES,0.9193205944798302,"[15] Ivan Kobyzev, Simon JD Prince, and Marcus A Brubaker. Normalizing flows: An introduction and review
330"
REFERENCES,0.921443736730361,"of current methods. IEEE transactions on pattern analysis and machine intelligence, 43(11):3964–3979,
331"
REFERENCES,0.9235668789808917,"2020.
332"
REFERENCES,0.9256900212314225,"[16] Cheng Lu, Kaiwen Zheng, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Maximum likelihood
333"
REFERENCES,0.9278131634819533,"training for score-based diffusion ODEs by high order denoising score matching. In Kamalika Chaudhuri,
334"
REFERENCES,0.9299363057324841,"Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the
335"
REFERENCES,0.9320594479830149,"39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning
336"
REFERENCES,0.9341825902335457,"Research, pages 14429–14460. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/
337"
REFERENCES,0.9363057324840764,"v162/lu22f.html.
338"
REFERENCES,0.9384288747346072,"[17] Morteza Mardani, Jiaming Song, Jan Kautz, and Arash Vahdat. A variational perspective on solving inverse
339"
REFERENCES,0.940552016985138,"problems with diffusion models. arXiv preprint arXiv:2305.04391, 2023.
340"
REFERENCES,0.9426751592356688,"[18] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning
341"
REFERENCES,0.9447983014861996,"using nonequilibrium thermodynamics. In Int. Conf. Machine Learning, pages 2256–2265. PMLR, 2015.
342"
REFERENCES,0.9469214437367304,"[19] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models
343"
REFERENCES,0.9490445859872612,"for inverse problems. In International Conference on Learning Representations, 2023. URL https:
344"
REFERENCES,0.9511677282377919,"//openreview.net/forum?id=9_gsMA8MRKQ.
345"
REFERENCES,0.9532908704883227,"[20] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In
346"
REFERENCES,0.9554140127388535,"NeurIPS, pages 11895–11907, 2019.
347"
REFERENCES,0.9575371549893843,"[21] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to
348"
REFERENCES,0.9596602972399151,"density and score estimation. In Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial
349"
REFERENCES,0.9617834394904459,"Intelligence, UAI, page 204, 2019. URL http://auai.org/uai2019/proceedings/papers/204.
350"
REFERENCES,0.9639065817409767,"pdf.
351"
REFERENCES,0.9660297239915074,"[22] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based
352"
REFERENCES,0.9681528662420382,"diffusion models. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.
353"
REFERENCES,0.970276008492569,"[23] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
354"
REFERENCES,0.9723991507430998,"Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. URL
355"
REFERENCES,0.9745222929936306,"https://openreview.net/forum?id=PxTIG12RRHS.
356"
REFERENCES,0.9766454352441614,"[24] Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging
357"
REFERENCES,0.9787685774946921,"with score-based generative models. In ICLR, 2022. URL https://openreview.net/forum?id=
358"
REFERENCES,0.9808917197452229,"vaRCHVj0uGI.
359"
REFERENCES,0.9830148619957537,"[25] He Sun and Katherine L Bouman. Deep probabilistic imaging: Uncertainty quantification and multi-modal
360"
REFERENCES,0.9851380042462845,"solution characterization for computational imaging. In AAAI, pages 2628–2637, 2021.
361"
REFERENCES,0.9872611464968153,"[26] He Sun, Katherine L Bouman, Paul Tiede, Jason J Wang, Sarah Blunt, and Dimitri Mawet. alpha-deep
362"
REFERENCES,0.9893842887473461,"probabilistic inference (alpha-dpi): efficient uncertainty quantification from exoplanet astrometry to black
363"
REFERENCES,0.9915074309978769,"hole feature extraction. arXiv preprint arXiv:2201.08506, 2022.
364"
REFERENCES,0.9936305732484076,"[27] Jure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan Huang, Matthew J Muckley, Aaron
365"
REFERENCES,0.9957537154989384,"Defazio, Ruben Stern, Patricia Johnson, Mary Bruno, et al. fastmri: An open dataset and benchmarks for
366"
REFERENCES,0.9978768577494692,"accelerated mri. arXiv preprint arXiv:1811.08839, 2018.
367"
