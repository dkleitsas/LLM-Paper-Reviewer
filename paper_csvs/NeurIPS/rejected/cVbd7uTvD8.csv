Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001148105625717566,"Existing single image-to-3D creation methods typically involve a two-stage pro-
1"
ABSTRACT,0.002296211251435132,"cess, first generating multi-view images, and then using these images for 3D
2"
ABSTRACT,0.003444316877152698,"reconstruction. However, training these two stages separately leads to significant
3"
ABSTRACT,0.004592422502870264,"data bias in the inference phase, thus affecting the quality of reconstructed results.
4"
ABSTRACT,0.0057405281285878304,"We introduce a unified 3D generation framework, named SC3D, which integrates
5"
ABSTRACT,0.006888633754305396,"diffusion-based multi-view image generation and 3D reconstruction through a
6"
ABSTRACT,0.008036739380022962,"self-conditioning mechanism. In our framework, these two modules are established
7"
ABSTRACT,0.009184845005740528,"as a cyclic relationship so that they adapt to the distribution of each other. During
8"
ABSTRACT,0.010332950631458095,"the denoising process of multi-view generation, we feed rendered color images and
9"
ABSTRACT,0.011481056257175661,"maps by SC3D itself to the multi-view generation module. This self-conditioned
10"
ABSTRACT,0.012629161882893225,"method with 3D aware feedback unites the entire process and improves geometric
11"
ABSTRACT,0.013777267508610792,"consistency. Experiments show that our approach enhances sampling quality, and
12"
ABSTRACT,0.014925373134328358,"improves the efficiency and output quality of the generation process.
13"
INTRODUCTION,0.016073478760045924,"1
Introduction
14"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.01722158438576349,"3D content creation from a single image have improved rapidly in recent years with the adoption of
15"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.018369690011481057,"large 3D datasets [1, 2, 3] and diffusion models [4, 5, 6]. A body of research [7, 8, 9, 10, 11, 12, 13, 14]
16"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.01951779563719862,"has focused on multi-view diffusion models, fine-tuning pretrained image or video diffusion models
17"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.02066590126291619,"on 3D datasets to enable consistent multi-view synthesis. These methods demonstrate generalizability
18"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.021814006888633754,"and produce promising results. Another group of works [15, 16, 17, 18, 19] propose generalizable
19"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.022962112514351322,"reconstruction models, generating 3D representation from one or few views in a feed-forward process.
20"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.024110218140068886,"Theses reconstruction models built upon convolutional network or transformer backbone, have led to
21"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.02525832376578645,"efficient image-to-3D creation.
22"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.02640642939150402,"Since single-view reconstruction models [15] trained on 3D datasets [1, 20] lack generalizability
23"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.027554535017221583,"and often produce blurring at unseen viewpoints, several works [21, 16, 18, 19] extend models to
24"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.02870264064293915,"sparse-view input, boosting the reconstruction quality. As shown in Fig. 1, these methods split 3D
25"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.029850746268656716,"generation into two stages: multi-view synthesis and 3D reconstruction. By combining generalizable
26"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.030998851894374284,"multi-view diffusion models and robust sparse-view reconstruction models, such pipelines achieve
27"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.03214695752009185,"high-quality image to 3D generation. However, combining the two independently designed models
28"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.03329506314580941,"introduces a significant ‚Äúdata bias‚Äù to the reconstruction model. The data bias is mainly reflected in
29"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.03444316877152698,"two aspects: (1) Multi-view bias. Multi-view diffusion models learn consistency at the image level,
30"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.03559127439724455,"struggle to ensure geometric consistency. When it comes to reconstruction, multi-view images that
31"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.03673938002296211,"lack geometric consistency affect the subsequent stage. (2) Limited data for reconstruction model.
32"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.03788748564867968,"Unlike multi-view diffusion models, reconstruction models which are trained from scratch on limited
33"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.03903559127439724,"3D dataset, lacks the generalization ability.
34"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.040183696900114814,"Recent works like IM-3D [22] and VideoMV [23] have attempted to aggregate the rendered views of
35"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.04133180252583238,"the reconstructed 3D model into previous-step multi-view synthesis, thus improving the capability
36"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.04247990815154994,"Input Image
Multi-view
Diffusion Model"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.04362801377726751,Reconstruction
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.04477611940298507,"Model
3D Representation"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.045924225028702644,T steps
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.04707233065442021,"Input Image
Multi-view
Diffusion Model"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.04822043628013777,Reconstruction
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.04936854190585534,"Model
3D Representation"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.0505166475315729,"SC3D
3D-aware Feedback"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.05166475315729047,"T steps ùë•""!"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.05281285878300804,(a) Two-stage pipeline
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.0539609644087256,(b) SC3D framework
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.05510907003444317,"Figure 1: Concept comparison between SC3D and previous two-stage methods. Instead of directly
combining multi-view diffusion model and reconstruction model, our self-conditioned framework
involves joint training of these two models and establish them as a cyclic association. During the
denoising process, rendered 3D-aware maps are fed to the multi-view generation module."
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.05625717566016074,"and consistency of the generated multi-view images. These methods integrate the aforementioned two
37"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.0574052812858783,"stages at the inference phase. But the models at both stages still lack joint training, which prevents
38"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.05855338691159587,"the reconstruction model from enhancing its robustness to the generated poor multiviews. Moreover,
39"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.05970149253731343,"these test-time aggregating methods cannot directly utilize geometric information such as depth maps,
40"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.060849598163030996,"normal maps, or position maps that can also be obtained from the reconstructed 3D. Notably, these
41"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.06199770378874857,"explicit 3D aware maps can better guide the multi-view generation.
42"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.06314580941446613,"To address these challenges, we propose a unified single image-to-3D creation framework, named
43"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.0642939150401837,"SC3D, which integrates multi-view generation and 3D reconstruction through a self-conditioning
44"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.06544202066590127,"mechanism. Our framework involves jointly training the multi-view diffusion model and the recon-
45"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.06659012629161883,"struction model. In SC3D, these two modules are established as a cyclic relationship so that they
46"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.0677382319173364,"adapt to the characteristics of each other, enabling robust generation at inference. Specifically, during
47"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.06888633754305395,"the denoising process, we feed rendered 3D-aware maps from the reconstructed 3D to the multi-view
48"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.07003444316877153,"generation module. By leveraging the color maps and spatial canonical coordinates maps from the
49"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.0711825487944891,"reconstruction 3D representation as condition, our multi-view diffusion model synthesizes multi-view
50"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.07233065442020666,"images that better conform to the actual 3D structure. This self-conditioned framework with 3D
51"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.07347876004592423,"aware feedback unites the 3D generation process and enhances the robustness for unseen complex
52"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.07462686567164178,"scenes. Experiments on the GSO dataset [24] validate that our SC3D reduces data bias between
53"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.07577497129735936,"training and inference, and enhances the overall efficiency and output quality.
54"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.07692307692307693,"Our key contributions are as follows:
55"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.07807118254879448,"‚Ä¢ We introduce SC3D, which unifies multi-view generation and 3D reconstruction in a single
56"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.07921928817451206,"framework and involves jointly training these two modules, enabling adaption to each other.
57"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.08036739380022963,"‚Ä¢ SC3D employs a self-conditioning mechanism with 3D-aware feedback, using rendered 3D-aware
58"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.08151549942594719,"maps to guide the multi-view generation, ensuring better geometric consistency and robustness.
59"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.08266360505166476,"‚Ä¢ Experiments show that SC3D significantly reduces data bias, improves the quality of 3D recon-
60"
D CONTENT CREATION FROM A SINGLE IMAGE HAVE IMPROVED RAPIDLY IN RECENT YEARS WITH THE ADOPTION OF,0.08381171067738231,"struction, and enhances overall efficiency in creating 3D content from a single image.
61"
RELATED WORK,0.08495981630309989,"2
Related Work
62"
RELATED WORK,0.08610792192881746,"Image/Video Diffusion for Multi-view Generation
Diffusion models [25, 26, 27, 28, 29, 30, 31,
63"
RELATED WORK,0.08725602755453502,"32, 33, 34] have demonstrated their powerful generative capabilities in image and video generation
64"
RELATED WORK,0.08840413318025259,"fields. Current research [7, 8, 9, 10, 11, 12, 13, 14, 35] fine-tunes pretrained image/video diffusion
65"
RELATED WORK,0.08955223880597014,"models on 3D datasets like Objaverse [1] and MVImageNet [20]. Zero123 [7] introduces relative
66"
RELATED WORK,0.09070034443168772,"view condition to image diffusion models, enabling novel view synthesis from a single image
67"
RELATED WORK,0.09184845005740529,"and preserving generalizability. Based on it, methods like SyncDreamer [9], ConsistNet [36] and
68"
RELATED WORK,0.09299655568312284,"EpiDiff [11] design attention modules to generate consistent multi-view images. These methods fine-
69"
RELATED WORK,0.09414466130884042,"tuned from image diffusion models produce generally promising results. By considering multi-view
70"
RELATED WORK,0.09529276693455797,"images as consecutive frames of a video (e.g., orbiting camera views), it naturally leads to the idea of
71"
RELATED WORK,0.09644087256027555,"applying video generation models to 3D generation [13]. However, since the diffusion model is not
72"
RELATED WORK,0.09758897818599312,"explicitly modeled in 3D space, the generated multi-view images often struggle to achieve consistent
73"
RELATED WORK,0.09873708381171067,"and robust details.
74"
RELATED WORK,0.09988518943742825,"Image to 3D Reconstruction
Recently, the task of reconstructing 3D objects has evolved from
75"
RELATED WORK,0.1010332950631458,"traditional multi-view reconstruction methods [37, 38, 39, 40] to feed-forward reconstruction mod-
76"
RELATED WORK,0.10218140068886337,"els [15, 41, 42, 16, 17, 18, 19]. Ultilizing one or few shot as input, these highly generalizable
77"
RELATED WORK,0.10332950631458095,"reconstruction models synthesize 3D representation, enabling the rapid generation of 3D objects.
78"
RELATED WORK,0.1044776119402985,"LRM [15] proposes a transformer-based model to effectively map image tokens to 3D triplanes.
79"
RELATED WORK,0.10562571756601608,"Instant3D [21] further extends LRM to sparse-view input, significantly boosting the reconstruction
80"
RELATED WORK,0.10677382319173363,"quality. LGM [16] and GRM [17] replace the triplane representation with 3D Gaussians [40] to enjoy
81"
RELATED WORK,0.1079219288174512,"its superior rendering efficiency. CRM [18] and InstantMesh [19] optimize on the mesh representation
82"
RELATED WORK,0.10907003444316878,"for high-quality geometry and texture modeling. These reconstrucion models built upon convolutional
83"
RELATED WORK,0.11021814006888633,"network architecture or transformer backbone, have led to efficient image-to-3D creation.
84"
RELATED WORK,0.1113662456946039,"Pipelines of 3D Generation
Early works propose to distill knowledge of image prior to create 3D
85"
RELATED WORK,0.11251435132032148,"models via Score Distillation Sampling (SDS) [43, 44, 45], limited by the low speed of per-scene
86"
RELATED WORK,0.11366245694603903,"optimization. Several works [9, 11, 14, 22] fine-tune image diffusion models to generate multi-view
87"
RELATED WORK,0.1148105625717566,"images, which are then utilized for 3D shape and appearance recovery with traditional reconstruction
88"
RELATED WORK,0.11595866819747416,"methods [46, 40]. More recently, several works [21, 16, 18, 19, 23] involve both multi-view diffusion
89"
RELATED WORK,0.11710677382319173,"models and feed-forward reconstruction models in the generation process. Such pipelines attempt
90"
RELATED WORK,0.1182548794489093,"to combine the processes into a cohesive two-stage approach, thus achieving highly generalizable
91"
RELATED WORK,0.11940298507462686,"and high-quality single-image to 3D generation. However, due to the lack of explicit 3D modeling,
92"
RELATED WORK,0.12055109070034443,"the results generated by the multi-view diffusion model cannot guarantee strong consistency, which
93"
RELATED WORK,0.12169919632606199,"will lead to data deviation for the reconstructed model between the testing phase and the training
94"
RELATED WORK,0.12284730195177956,"phase. Compared to them, we propose a unified pipeline, integrating the two stages through a
95"
RELATED WORK,0.12399540757749714,"self-conditioning mechanism at the training stage, with 3D aware feedback for high consistency.
96"
METHOD,0.1251435132032147,"3
Method
97"
METHOD,0.12629161882893225,"Given a single image, SC3D aims to generate multiview-consistent images with a reconstructed 3D
98"
METHOD,0.12743972445464982,"Gaussion model. To reduce the data bias and improve robustness of the generation, we propose SC3D,
99"
METHOD,0.1285878300803674,"a unified 3D generation framework which integrates multi-view synthesis and 3D reconstruction
100"
METHOD,0.12973593570608496,"through a self-conditioning mechanism. As illustrated in Fig. 2, the proposed framework involves a
101"
METHOD,0.13088404133180254,"video diffusion model (SVD [32]) as multi-view generator (refer to Section 3.1) and a feed-forward
102"
METHOD,0.13203214695752008,"reconstruction model to recover a 3D Gaussian Splatting (refer to Section 3.2. Moreover, we introduce
103"
METHOD,0.13318025258323765,"a self-conditioning mechanism, feeding the 3D-aware information obtained from the reconstruction
104"
METHOD,0.13432835820895522,"module back to the multi-view generation process (refer to Section 3.3). The 3D-aware denoising
105"
METHOD,0.1354764638346728,"sampling strategy iteratively refines the multi-view images and the 3d model, thus enhancing the final
106"
METHOD,0.13662456946039037,"production.
107"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.1377726750861079,"3.1
Video Diffusion Model as Multiview Generator
108"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.13892078071182548,"Recent video diffusion models such as those in [13, 34] have demonstrated a remarkable capability
109"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.14006888633754305,"to generate 3D-aware videos by scaling up both the model and dataset. Our research employs
110"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.14121699196326062,"the well-known Stable Video Diffusion (SVD) Model, which generates videos from image input.
111"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.1423650975889782,"Formally, given an image I ‚ààR3√óh√ów, the model is designed to generate a video V ‚ààRf√ó3√óh√ów.
112"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.14351320321469574,"Further details about SVD can be found in Appendix A.1.
113"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.1446613088404133,"We enhance the video diffusion model with camera control c to generate images from different
114"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.14580941446613088,"viewpoints. Traditional methods encode camera positions at the frame level, which results in all
115"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.14695752009184845,"pixels within one view sharing the same positional encoding [47, 13]. Building on the innovations
116"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.14810562571756603,"of previous work [11, 35], we integrate the camera condition c into the denoising network by
117"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.14925373134328357,"parameterizing the rays r = (o, o √ó d). Specifically, we use two-layered MLP to inject Pl√ºcker
118"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.15040183696900114,"ray embeddings for each latent pixel, enabling precise positional encoding at the pixel level. This
119"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.1515499425947187,"approach allows for more detailed and accurate 3D rendering, as pixel-specific embedding enhances
120"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.15269804822043628,"the model‚Äôs ability to handle complex variations in depth and perspective across the video frames.
121 ùíô! "" U-Net ùíô!#$ "" ‚Ä¶ ùíô""% """
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.15384615384615385,Multi-view Generation
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.1549942594718714,Decoder
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.15614236509758897,Reconstruction Model
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.15729047072330654,Reconstruction
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.1584385763490241,Feedback
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.15958668197474168,"+
color 
images"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.16073478760045926,"coordinates maps + ùíô% """
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.1618828932261768,Decoder
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.16303099885189437,Images
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.16417910447761194,"Color
Encoder"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.1653272101033295,"CCM
Encoder"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.16647531572904709,Input Image
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.16762342135476463,Ray Embeddings
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.1687715269804822,"Figure 2: Overview of SC3D. We adopt a video diffusion model as the multi-view generator by
incorporating the input image and relative camera poses. In the denoising sampling loop, we decode
the predicted exf
0 to noise-corrupted images, which are then used to recover 3D representation by
a feed-forward reconstruction model. Then the rendered color images and coordinates maps are
encoded and fed into the next denoising step. At inference, the 3D-aware denoising sampling strategy
iteratively refines the images by incorporating feedback from the reconstructed 3D into the denoising
loop, enhancing multi-view consistency and image quality."
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.16991963260619977,"In our framework, unlike existing two-stage methods, our multi-view diffusion model does not
122"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.17106773823191734,"complete multiple denoising steps independently. In contrast, in the denoising sampling loop, we
123"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.17221584385763491,"obtain the straightly predicted exf
0 at the current timestep, which will be used for subsequent 3D
124"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.17336394948335246,"reconstruction. Then we use rendered 3d-aware view maps as conditions to guide the next denoising
125"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.17451205510907003,"step. Therefore, at each sampling step, we do the reparameterization of the output from the denoising
126"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.1756601607347876,"network FŒ∏ to convert it into exf
0. Taking a single view as an example, we processes the denoised
127"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.17680826636050517,"image cin(œÉ)x and the associated noise level cnoise(œÉ), which œÉ indicates the standard deviation of
128"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.17795637198622274,"the noise. The reparameterization is formulated as follows:
129"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.1791044776119403,"Àúx0 = cskip(œÉ)x + cout(œÉ)FŒ∏(cin(œÉ)x; cnoise(œÉ)).
(1)"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.18025258323765786,"The above operation process adjusts the output of FŒ∏ to exf
0, which will be decoded into images and
130"
VIDEO DIFFUSION MODEL AS MULTIVIEW GENERATOR,0.18140068886337543,"passed to the subsequent 3D reconstruction module.
131"
FEED-FORWARD RECONSTRUCTION MODEL,0.182548794489093,"3.2
Feed-Forward Reconstruction Model
132"
FEED-FORWARD RECONSTRUCTION MODEL,0.18369690011481057,"In the SC3D framework, the feed-forward reconstruction model is designed to recover 3D models
133"
FEED-FORWARD RECONSTRUCTION MODEL,0.18484500574052812,"from pre-generated multi-view images, which can be images decoded from straightly predicted exf
0,
134"
FEED-FORWARD RECONSTRUCTION MODEL,0.1859931113662457,"or completely denoised images. We utilize Large Multi-View Gaussian Model (LGM) [16] G as our
135"
FEED-FORWARD RECONSTRUCTION MODEL,0.18714121699196326,"reconstruction module due to its real-time rendering capabilities that benefit from 3D representation of
136"
FEED-FORWARD RECONSTRUCTION MODEL,0.18828932261768083,"Gaussian Splatting. This method integrates seamlessly with our jointly training framework, allowing
137"
FEED-FORWARD RECONSTRUCTION MODEL,0.1894374282433984,"for quick adaptation and efficient processing.
138"
FEED-FORWARD RECONSTRUCTION MODEL,0.19058553386911595,"We pass four specific views from the reparameterized output exf
0 to the Large Gaussian Model (LGM)
139"
FEED-FORWARD RECONSTRUCTION MODEL,0.19173363949483352,"for 3D Gaussian Splatting reconstruction. To enhance the performance of LGM, particularly its
140"
FEED-FORWARD RECONSTRUCTION MODEL,0.1928817451205511,"sensitivity to different noise levels cnoise(œÉ) and image details, we introduce a zero-initialized time
141"
FEED-FORWARD RECONSTRUCTION MODEL,0.19402985074626866,"embedding layer within the original U-Net structure of the LGM. This innovative modification
142"
FEED-FORWARD RECONSTRUCTION MODEL,0.19517795637198623,"enables the LGM to dynamically adapt to the diverse outputs that arise at different stages of the
143"
FEED-FORWARD RECONSTRUCTION MODEL,0.19632606199770378,"denoising process, thereby substantially improving its capacity to accurately reconstruct 3D content
144"
FEED-FORWARD RECONSTRUCTION MODEL,0.19747416762342135,"from images that have undergone partial denoising.
145"
FEED-FORWARD RECONSTRUCTION MODEL,0.19862227324913892,"The loss function employed for the fine-tuning of the LGM is articulated as follows:
146"
FEED-FORWARD RECONSTRUCTION MODEL,0.1997703788748565,"LG = Lrgb(x0, G(Àúx0, cnoise(œÉ))) + ŒªLLPIPS(x0, G(Àúx0, cnoise(œÉ))).
(2)"
FEED-FORWARD RECONSTRUCTION MODEL,0.20091848450057406,"where we have utilized the mean square error loss Lrgb for the color channel and a VGG-based
147"
FEED-FORWARD RECONSTRUCTION MODEL,0.2020665901262916,"perceptual loss LLPIPS[43] for the LPIPS term. In practical applications, the weighting factor Œª is
148"
FEED-FORWARD RECONSTRUCTION MODEL,0.20321469575200918,"conventionally set to 1.
149"
FEED-FORWARD RECONSTRUCTION MODEL,0.20436280137772675,"Additionally, to maintain the model‚Äôs reconstruction capability for normal images, we also input the
150"
FEED-FORWARD RECONSTRUCTION MODEL,0.20551090700344432,"model without adding noise and calculate the corresponding loss. In this case, we set cnoise(œÉ) to 0.
151"
FEED-FORWARD RECONSTRUCTION MODEL,0.2066590126291619,"3.3
3D-Aware Feedback Mechanism
152"
FEED-FORWARD RECONSTRUCTION MODEL,0.20780711825487944,"As shown in Fig. 2, we adopt a 3D-aware feedback mechanism that involves the rendered color
153"
FEED-FORWARD RECONSTRUCTION MODEL,0.208955223880597,"images and geometric maps produced by our reconstruction module in a denoising loop to further
154"
FEED-FORWARD RECONSTRUCTION MODEL,0.21010332950631458,"improve the multi-view consistency of the resulting images and facilitate cyclic adaptation of the
155"
FEED-FORWARD RECONSTRUCTION MODEL,0.21125143513203215,"two stages. Instead of integrating multi-view generation and 3D reconstruction at the inference stage
156"
FEED-FORWARD RECONSTRUCTION MODEL,0.21239954075774972,"using re-sampling strategy [22, 23], we propose to train these two modules jointly to support more
157"
FEED-FORWARD RECONSTRUCTION MODEL,0.21354764638346727,"informative feedback. Specifically, in addition to the rendered color images, our flexible framework
158"
FEED-FORWARD RECONSTRUCTION MODEL,0.21469575200918484,"is able to derive additional geometric features to guide the generation process, which brings guidance
159"
FEED-FORWARD RECONSTRUCTION MODEL,0.2158438576349024,"of more explicit 3D information to multi-view generation.
160"
FEED-FORWARD RECONSTRUCTION MODEL,0.21699196326061998,"In practice, we obtain color images and canonical coordinates maps [48] from the reconstructed 3D
161"
FEED-FORWARD RECONSTRUCTION MODEL,0.21814006888633755,"model, and utilize them as condition to guide the next denoising step of multi-view generation. We
162"
FEED-FORWARD RECONSTRUCTION MODEL,0.21928817451205512,"use position maps instead of depth maps or normal maps as the representative of geometric maps
163"
FEED-FORWARD RECONSTRUCTION MODEL,0.22043628013777267,"because canonical coordinate maps record the vertex coordinate values after normalization of the
164"
FEED-FORWARD RECONSTRUCTION MODEL,0.22158438576349024,"overall 3D model, rather than the normalization of the relative self-view (such as depth maps). This
165"
FEED-FORWARD RECONSTRUCTION MODEL,0.2227324913892078,"operation enables the rendered maps to be characterized as cross-view alignment, providing the strong
166"
FEED-FORWARD RECONSTRUCTION MODEL,0.22388059701492538,"guidance of more explicit cross-view geometry relationship. The details of canonical coordinates
167"
FEED-FORWARD RECONSTRUCTION MODEL,0.22502870264064295,"map can be found in Appendix A.2.
168"
FEED-FORWARD RECONSTRUCTION MODEL,0.2261768082663605,"We adopt a 3D-aware self-conditioning [49] training and inference strategy that leverages reconstruc-
169"
FEED-FORWARD RECONSTRUCTION MODEL,0.22732491389207807,"tion stage results to enhance multi-view consistency and the quality of generated images. During
170"
FEED-FORWARD RECONSTRUCTION MODEL,0.22847301951779564,"training, the original denoising network FŒ∏(x; œÉ) is augmented with a 3D-aware feedback denoising
171"
FEED-FORWARD RECONSTRUCTION MODEL,0.2296211251435132,"network FŒ∏(G(Àúx0); œÉ), where G(Àúx0) is the output of the LGM reconstruction.
172"
FEED-FORWARD RECONSTRUCTION MODEL,0.23076923076923078,"To encode color images and coordinates maps into the denoising network of multi-view generation
173"
FEED-FORWARD RECONSTRUCTION MODEL,0.23191733639494833,"module, we design two simple and lightweight encoders for color images and coordinates maps using
174"
FEED-FORWARD RECONSTRUCTION MODEL,0.2330654420206659,"a series of convolutional neural networks, like T2I-Adapter [50]. The encoders are composed of four
175"
FEED-FORWARD RECONSTRUCTION MODEL,0.23421354764638347,"feature extraction blocks and three downsample blocks to change the feature resolution, so that the
176"
FEED-FORWARD RECONSTRUCTION MODEL,0.23536165327210104,"dimension of the encoded features is the same as the intermediate feature in the encoder of U-Net
177"
FEED-FORWARD RECONSTRUCTION MODEL,0.2365097588978186,"denoiser. The extracted features from the two conditional modalities are then added to the U-Net
178"
FEED-FORWARD RECONSTRUCTION MODEL,0.23765786452353616,"encoder at each scale.
179"
FEED-FORWARD RECONSTRUCTION MODEL,0.23880597014925373,"Training Strategy
As illustrated in Algorithm 1, to train a 3D-aware multi-view generation network,
180"
FEED-FORWARD RECONSTRUCTION MODEL,0.2399540757749713,"we use the rendered maps by the 3D reconstruction module as the self-conditioning input. In practice,
181"
FEED-FORWARD RECONSTRUCTION MODEL,0.24110218140068887,"we randomly use this self-conditioning mechanism with a probability of 0.5. When not using the 3D
182"
FEED-FORWARD RECONSTRUCTION MODEL,0.24225028702640644,"reconstruction result, we set G(Àúx0) = 0 as the input. This probabilistic approach ensures balanced
183"
FEED-FORWARD RECONSTRUCTION MODEL,0.24339839265212398,"learning, allowing the model to effectively incorporate 3D information without over-reliance on it.
184"
FEED-FORWARD RECONSTRUCTION MODEL,0.24454649827784156,"Algorithm 1 Training SC3D with the self-conditioned strategy.
def train_loss(x, cond_image):"
FEED-FORWARD RECONSTRUCTION MODEL,0.24569460390355913,"""""""Returns the loss on a training example x.""""""
# Sample sigma from a log-normal distribution
sigma = log_normal(P_mean, P_std)"
FEED-FORWARD RECONSTRUCTION MODEL,0.2468427095292767,"# Reparameterize sigma to obtain conditioning parameters
c_in, c_out, c_skip, c_noise, lambda_param = reparameterizing(sigma)"
FEED-FORWARD RECONSTRUCTION MODEL,0.24799081515499427,"# Add noise to input data
noise_x = x + sigma * normal(mean=0, std=1)
input_x = c_in * noise_x"
FEED-FORWARD RECONSTRUCTION MODEL,0.24913892078071181,"# Initial prediction without self-conditioning
self_cond = None
F_pred = net(input_x, c_noise, cond_image, self_cond)
pred_x = c_out * F_pred + c_skip * noise_x"
FEED-FORWARD RECONSTRUCTION MODEL,0.2502870264064294,"# Update self_cond using the reconstruction model
self_cond = recon_model(pred_x, c_noise)"
FEED-FORWARD RECONSTRUCTION MODEL,0.25143513203214696,"# Use rendered maps as condition and denoise
if self_cond and np.random.uniform(0, 1) > 0.5:
F_pred = net(input_x, t, cond_image, self_cond.detach())
pred_x = c_out * F_pred + c_skip * noise_x"
FEED-FORWARD RECONSTRUCTION MODEL,0.2525832376578645,"# Compute loss
loss = lambda_param * (pred_x - target) ** 2
recon_loss = recon_loss_fn(self_cond, x)"
FEED-FORWARD RECONSTRUCTION MODEL,0.2537313432835821,return loss.mean() + recon_loss
FEED-FORWARD RECONSTRUCTION MODEL,0.25487944890929964,"Inference/sampling strategy
At the inference stage, as shown in Algorithm 2, the 3D feedback
185"
FEED-FORWARD RECONSTRUCTION MODEL,0.25602755453501724,"G(Àúx0) is initially set to 0. At each timestep, this feedback is updated with the previous reconstruction
186"
FEED-FORWARD RECONSTRUCTION MODEL,0.2571756601607348,"result G(Àúx0). This iterative process refines the 3D representation, ensuring each frame benefits from
187"
FEED-FORWARD RECONSTRUCTION MODEL,0.25832376578645233,"prior reconstructions, leading to higher quality and more consistent 3D-aware images.
188"
FEED-FORWARD RECONSTRUCTION MODEL,0.25947187141216993,"Algorithm 2 Sampling algorithm of SC3D.
def generate(sigmas, cond_image):
self_cond = None
x_T = normal(mean=0, std=1)
# Initialize latent variable with Gaussian noise
for sigma in sigmas:"
FEED-FORWARD RECONSTRUCTION MODEL,0.2606199770378875,"# Reparameterize sigma to obtain conditioning parameters
c_in, c_out, c_skip, c_noise, lambda_param = reparameterizing(sigma)"
FEED-FORWARD RECONSTRUCTION MODEL,0.2617680826636051,"# Add noise to the latent variable
noise_x = x_T + sigma * normal(mean=0, std=1)
input_x = c_in * noise_x"
FEED-FORWARD RECONSTRUCTION MODEL,0.2629161882893226,"# Generate prediction
F_pred = net(input_x, t, cond_image, self_cond)
pred_x = c_out * F_pred + c_skip * noise_x"
FEED-FORWARD RECONSTRUCTION MODEL,0.26406429391504016,"# Update self_cond using the reconstruction model
self_cond = recon_model(pred_x, c_noise)"
FEED-FORWARD RECONSTRUCTION MODEL,0.26521239954075776,return pred_x
FEED-FORWARD RECONSTRUCTION MODEL,0.2663605051664753,Figure 3: Qualitative comparison with ImageDream-LGM and Our LGM.
FEED-FORWARD RECONSTRUCTION MODEL,0.2675086107921929,Figure 4: Qualitative comparison with no-feedback and 3d-aware feedback.
EXPERIMENTS,0.26865671641791045,"4
Experiments
189"
EXPERIMENTS,0.269804822043628,"We focus on 3D asset content synthesis, training our model on the G-Objaverse [1, 51] dataset and
190"
EXPERIMENTS,0.2709529276693456,"the LVIS subset of Objaverse, which consists of 300K high-quality 3D objects and is widely used in
191"
EXPERIMENTS,0.27210103329506313,"3D generation. We evaluate SC3D on the Google Scanned Object (GSO) dataset [24], which consists
192"
EXPERIMENTS,0.27324913892078073,"of approximately 1,000 scanned models, and we randomly select 100 samples for comparison. We
193"
EXPERIMENTS,0.2743972445464983,"adopt TripoSR[42], SyncDreamer[9], SV3D[13], ImageDream [8] combined with LGM [16] as the
194"
EXPERIMENTS,0.2755453501722158,"baseline approach [16] and VideoMV[23] as baseline methods. For each baseline, we report PSNR,
195"
EXPERIMENTS,0.2766934557979334,"SSIM, and LPIPS metrics.
196"
COMPARISON RESULTS,0.27784156142365096,"4.1
Comparison results
197"
COMPARISON RESULTS,0.27898966704936856,"For LGM, we utilize the official LGM single-image generation pipeline, which employs ImageDream
198"
COMPARISON RESULTS,0.2801377726750861,"[52] to transition from a single image input to multiple images. However, the conical coordinate
199"
COMPARISON RESULTS,0.28128587830080365,"system employed by ImageDream complicates the direct evaluation of the output. To address this,
200"
COMPARISON RESULTS,0.28243398392652125,"we use the official code to test on the GSO dataset, followed by manual calibration to assess the
201"
COMPARISON RESULTS,0.2835820895522388,"generated quality, as illustrated in Fig. 3. The misalignment between the two stages of ImageDream
202"
COMPARISON RESULTS,0.2847301951779564,"and LGM often results in generated models with blurred linear edges and geometric ambiguities.
203"
COMPARISON RESULTS,0.28587830080367393,"Nonetheless, our LGM, enhanced by a feedback mechanism, demonstrates significantly improved
204"
COMPARISON RESULTS,0.2870264064293915,"geometric and texture quality, producing results that closely approximate reality.
205"
COMPARISON RESULTS,0.2881745120551091,"As illustrate in 6, We find that although it can generate very continuous frames, the generated
206"
COMPARISON RESULTS,0.2893226176808266,"content tends to deviate from the given input image. This results in sub-optimal performance in
207"
COMPARISON RESULTS,0.2904707233065442,"Method
Resolution
PSNR‚Üë
SSIM‚Üë
LPIPS‚Üì"
COMPARISON RESULTS,0.29161882893226176,"TripoSR
256 √ó 256
18.481
0.8506
0.1357
SyncDreamer
256 √ó 256
20.056
0.8163
0.1596
SV3D
576 √ó 576
21.042
0.8497
0.1296
VideoMV(SD)
256 √ó 256
17.459
0.806
0.1446
VideoMV(GS)
256 √ó 256
17.577
0.807
0.1454"
COMPARISON RESULTS,0.2927669345579793,"SC3D (SVD)
512 √ó 512
21.625
0.9045
0.1011
SC3D (GS)
512 √ó 512
21.761
0.9094
0.0991
Table 1: Comparison of performance metrics across different models and configurations."
COMPARISON RESULTS,0.2939150401836969,"Input image
Rendered multi-views from Generated 3DGS"
COMPARISON RESULTS,0.29506314580941445,Figure 5: Out of distribution testing results.
COMPARISON RESULTS,0.29621125143513205,"the reconstruction metric. Additionally, VideoMV training the LGM separately with noisy images
208"
COMPARISON RESULTS,0.2973593570608496,"deteriorates, resulting in a visually noticeable reduction in its ability to generate texture details.
209"
ABLATION STUDY,0.29850746268656714,"4.2
Ablation study
210"
ABLATION STUDY,0.29965556831228474,"To validate the effectiveness of the proposed SC-3D framework, we conducted a series of ablation
211"
ABLATION STUDY,0.3008036739380023,"studies comparing PSNR, SSIM, and LPIPS metrics for different configurations (Table 2). We start
212"
ABLATION STUDY,0.3019517795637199,"with the base video diffusion model we trained, We then introduced 3D coordinates map feedback
213"
ABLATION STUDY,0.3030998851894374,"and RGB texture feedback from the reconstruction model to the diffusion model, which improved
214"
ABLATION STUDY,0.30424799081515497,"geometric consistency and texture detail across views. Combining both feedback mechanisms in the
215"
ABLATION STUDY,0.30539609644087257,"SVD + 3D-aware Feedback configuration resulted in the best performance, demonstrating significant
216"
ABLATION STUDY,0.3065442020665901,"improvements in the final 3D reconstruction quality by enhancing both geometric consistency and
217"
ABLATION STUDY,0.3076923076923077,"texture detail preservation.
218"
ABLATION STUDY,0.30884041331802525,"Method
Variant
PSNR ‚Üë
SSIM ‚Üë
LPIPS ‚Üì"
ABLATION STUDY,0.3099885189437428,"SVD
SVD
20.038
0.8745
0.1253
GS
20.549
0.8651
0.1183
SVD + Coordinates Map Feedback
SVD
21.021
0.8973
0.1110
GS
21.325
0.8937
0.1092
SVD + 3D-aware Feedback
SVD
21.752
0.9122
0.0993
GS
21.761
0.9094
0.0991
Table 2: Performance metrics of different feedback mechanisms."
ABLATION STUDY,0.3111366245694604,Figure 6: The Generation Example of VideoMV
ABLATION STUDY,0.31228473019517794,"We also demonstrate the impact of incorporating feedback mechanisms on the two models, as shown
219"
ABLATION STUDY,0.31343283582089554,"in Table 3. It can be observed that when no feedback mechanism is used, there is a significant
220"
ABLATION STUDY,0.3145809414466131,"discrepancy between the two models‚Äô modalities, which leads to a degradation in their combined
221"
ABLATION STUDY,0.3157290470723307,"performance.
222"
ABLATION STUDY,0.3168771526980482,"Method
Delta PSNR
Delta SSIM
Delta LPIPS"
ABLATION STUDY,0.31802525832376577,"SVD
0.511
0.0094
0.0070
SVD + Coordinates Map Feedback
0.304
0.0036
0.0018
SVD + 3D-aware Feedback
0.009
0.0028
0.0002
Table 3: The absolute differences in performance metrics between GS and SVD generation results.."
LIMITATIONS,0.31917336394948337,"4.3
Limitations
223"
LIMITATIONS,0.3203214695752009,"Current models utilize Gaussian splatting as a 3D representation, mapping and rendering coordinates
224"
LIMITATIONS,0.3214695752009185,"to textures for feedback. Although algorithms for converting Gaussian Splatting to meshe are under
225"
LIMITATIONS,0.32261768082663606,"development, achieving high quality in converting Gaussian models to general meshes remains
226"
LIMITATIONS,0.3237657864523536,"challenging. Directly employing a NeRF-based feed-forward model during the training process
227"
LIMITATIONS,0.3249138920780712,"significantly reduces training speed due to the computational demands of volumetric rendering. Our
228"
LIMITATIONS,0.32606199770378874,"model currently lacks the ability to generalize to the scene level, a limitation we intend to address in
229"
LIMITATIONS,0.32721010332950634,"future research.
230"
CONCLUSION,0.3283582089552239,"5
Conclusion
231"
CONCLUSION,0.32950631458094143,"In this paper, we introduce SC3D, a unified framework for 3D generation from a single image that
232"
CONCLUSION,0.330654420206659,"integrates multi-view image generation and 3D reconstruction through a self-conditioning mechanism.
233"
CONCLUSION,0.33180252583237657,"By establishing a cyclic relationship between these two stages, our approach effectively mitigates the
234"
CONCLUSION,0.33295063145809417,"data bias encountered in traditional methods. The self-conditioned method with 3D-aware feedback
235"
CONCLUSION,0.3340987370838117,"enhances geometric consistency throughout the generation process.
236"
CONCLUSION,0.33524684270952926,"Our experiments demonstrate that SC3D not only improves the quality and efficiency of the generation
237"
CONCLUSION,0.33639494833524686,"process but also achieves superior geometric consistency and detail in the reconstructed 3D models.
238"
CONCLUSION,0.3375430539609644,"By jointly training the multi-view diffusion model and the reconstruction model, SC3D adapts to the
239"
CONCLUSION,0.338691159586682,"inherent biases of each stage, resulting in more robust and accurate outputs.
240"
REFERENCES,0.33983926521239954,"References
241"
REFERENCES,0.3409873708381171,"[1] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt,
242"
REFERENCES,0.3421354764638347,"Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. In
243"
REFERENCES,0.34328358208955223,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13142‚Äì
244"
REFERENCES,0.34443168771526983,"13153, 2023.
245"
REFERENCES,0.3455797933409874,"[2] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan,
246"
REFERENCES,0.3467278989667049,"Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A universe of 10m+ 3d objects.
247"
REFERENCES,0.3478760045924225,"Advances in Neural Information Processing Systems, 36, 2024.
248"
REFERENCES,0.34902411021814006,"[3] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang,
249"
REFERENCES,0.35017221584385766,"Chen Qian, et al. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction
250"
REFERENCES,0.3513203214695752,"and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
251"
REFERENCES,0.35246842709529275,"pages 803‚Äì814, 2023.
252"
REFERENCES,0.35361653272101035,"[4] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning
253"
REFERENCES,0.3547646383467279,"using nonequilibrium thermodynamics. In Francis Bach and David Blei, editors, Proceedings of the 32nd
254"
REFERENCES,0.3559127439724455,"International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research,
255"
REFERENCES,0.35706084959816303,"pages 2256‚Äì2265, Lille, France, 07‚Äì09 Jul 2015. PMLR.
256"
REFERENCES,0.3582089552238806,"[5] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural
257"
REFERENCES,0.3593570608495982,"information processing systems, 33:6840‚Äì6851, 2020.
258"
REFERENCES,0.3605051664753157,"[6] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
259"
REFERENCES,0.3616532721010333,"Poole.
Score-based generative modeling through stochastic differential equations.
arXiv preprint
260"
REFERENCES,0.36280137772675086,"arXiv:2011.13456, 2020.
261"
REFERENCES,0.3639494833524684,"[7] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-
262"
REFERENCES,0.365097588978186,"1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF International Conference on
263"
REFERENCES,0.36624569460390355,"Computer Vision, pages 9298‚Äì9309, 2023.
264"
REFERENCES,0.36739380022962115,"[8] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view
265"
REFERENCES,0.3685419058553387,"diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023.
266"
REFERENCES,0.36969001148105624,"[9] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang.
267"
REFERENCES,0.37083811710677383,"Syncdreamer: Generating multiview-consistent images from a single-view image.
arXiv preprint
268"
REFERENCES,0.3719862227324914,"arXiv:2309.03453, 2023.
269"
REFERENCES,0.373134328358209,"[10] Jeong-gi Kwak, Erqun Dong, Yuhe Jin, Hanseok Ko, Shweta Mahajan, and Kwang Moo Yi. Vivid-1-to-3:
270"
REFERENCES,0.3742824339839265,"Novel view synthesis with video diffusion models. arXiv preprint arXiv:2312.01305, 2023.
271"
REFERENCES,0.37543053960964407,"[11] Zehuan Huang, Hao Wen, Junting Dong, Yaohui Wang, Yangguang Li, Xinyuan Chen, Yan-Pei Cao, Ding
272"
REFERENCES,0.37657864523536166,"Liang, Yu Qiao, Bo Dai, et al. Epidiff: Enhancing multi-view synthesis via localized epipolar-constrained
273"
REFERENCES,0.3777267508610792,"diffusion. arXiv preprint arXiv:2312.06725, 2023.
274"
REFERENCES,0.3788748564867968,"[12] Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas Chandra,
275"
REFERENCES,0.38002296211251435,"Yasutaka Furukawa, and Rakesh Ranjan. Mvdiffusion++: A dense high-resolution multi-view diffusion
276"
REFERENCES,0.3811710677382319,"model for single or sparse-view 3d object reconstruction. arXiv preprint arXiv:2402.12712, 2024.
277"
REFERENCES,0.3823191733639495,"[13] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian
278"
REFERENCES,0.38346727898966704,"Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from a
279"
REFERENCES,0.38461538461538464,"single image using latent video diffusion, 2024.
280"
REFERENCES,0.3857634902411022,"[14] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai
281"
REFERENCES,0.3869115958668197,"Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain
282"
REFERENCES,0.3880597014925373,"diffusion. arXiv preprint arXiv:2310.15008, 2023.
283"
REFERENCES,0.38920780711825487,"[15] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli,
284"
REFERENCES,0.39035591274397247,"Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint
285"
REFERENCES,0.39150401836969,"arXiv:2311.04400, 2023.
286"
REFERENCES,0.39265212399540755,"[16] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large
287"
REFERENCES,0.39380022962112515,"multi-view gaussian model for high-resolution 3d content creation. arXiv preprint arXiv:2402.05054,
288"
REFERENCES,0.3949483352468427,"2024.
289"
REFERENCES,0.3960964408725603,"[17] Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon
290"
REFERENCES,0.39724454649827784,"Wetzstein. Grm: Large gaussian reconstruction model for efficient 3d reconstruction and generation. arXiv
291"
REFERENCES,0.3983926521239954,"preprint arXiv:2403.14621, 2024.
292"
REFERENCES,0.399540757749713,"[18] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang
293"
REFERENCES,0.4006888633754305,"Su, and Jun Zhu. Crm: Single image to 3d textured mesh with convolutional reconstruction model. arXiv
294"
REFERENCES,0.4018369690011481,"preprint arXiv:2403.05034, 2024.
295"
REFERENCES,0.40298507462686567,"[19] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient
296"
REFERENCES,0.4041331802525832,"3d mesh generation from a single image with sparse-view large reconstruction models. arXiv preprint
297"
REFERENCES,0.4052812858783008,"arXiv:2404.07191, 2024.
298"
REFERENCES,0.40642939150401836,"[20] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming
299"
REFERENCES,0.40757749712973596,"Zhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: A large-scale dataset of multi-view images. In
300"
REFERENCES,0.4087256027554535,"Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9150‚Äì9161,
301"
REFERENCES,0.40987370838117104,"2023.
302"
REFERENCES,0.41102181400688864,"[21] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli,
303"
REFERENCES,0.4121699196326062,"Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large
304"
REFERENCES,0.4133180252583238,"reconstruction model. arXiv preprint arXiv:2311.06214, 2023.
305"
REFERENCES,0.41446613088404133,"[22] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, and
306"
REFERENCES,0.41561423650975887,"Filippos Kokkinos. Im-3d: Iterative multiview diffusion and reconstruction for high-quality 3d generation.
307"
REFERENCES,0.41676234213547647,"arXiv preprint arXiv:2402.08682, 2024.
308"
REFERENCES,0.417910447761194,"[23] Qi Zuo, Xiaodong Gu, Lingteng Qiu, Yuan Dong, Zhengyi Zhao, Weihao Yuan, Rui Peng, Siyu Zhu, Zilong
309"
REFERENCES,0.4190585533869116,"Dong, Liefeng Bo, et al. Videomv: Consistent multi-view generation based on large video generative
310"
REFERENCES,0.42020665901262916,"model. arXiv preprint arXiv:2403.12010, 2024.
311"
REFERENCES,0.4213547646383467,"[24] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann,
312"
REFERENCES,0.4225028702640643,"Thomas B McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset of 3d
313"
REFERENCES,0.42365097588978184,"scanned household items. In 2022 International Conference on Robotics and Automation (ICRA), pages
314"
REFERENCES,0.42479908151549944,"2553‚Äì2560. IEEE, 2022.
315"
REFERENCES,0.425947187141217,"[25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution
316"
REFERENCES,0.42709529276693453,"image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer
317"
REFERENCES,0.42824339839265213,"vision and pattern recognition, pages 10684‚Äì10695, 2022.
318"
REFERENCES,0.4293915040183697,"[26] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
319"
REFERENCES,0.4305396096440873,"Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-
320"
REFERENCES,0.4316877152698048,"image diffusion models with deep language understanding. Advances in neural information processing
321"
REFERENCES,0.43283582089552236,"systems, 35:36479‚Äì36494, 2022.
322"
REFERENCES,0.43398392652123996,"[27] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M√ºller, Joe Penna,
323"
REFERENCES,0.4351320321469575,"and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv
324"
REFERENCES,0.4362801377726751,"preprint arXiv:2307.01952, 2023.
325"
REFERENCES,0.43742824339839265,"[28] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rom-
326"
REFERENCES,0.43857634902411025,"bach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. arXiv preprint
327"
REFERENCES,0.4397244546498278,"arXiv:2403.12015, 2024.
328"
REFERENCES,0.44087256027554533,"[29] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet.
329"
REFERENCES,0.44202066590126293,"Video diffusion models. Advances in Neural Information Processing Systems, 35:8633‚Äì8646, 2022.
330"
REFERENCES,0.4431687715269805,"[30] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P
331"
REFERENCES,0.4443168771526981,"Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video
332"
REFERENCES,0.4454649827784156,"generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.
333"
REFERENCES,0.44661308840413316,"[31] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang,
334"
REFERENCES,0.44776119402985076,"Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv
335"
REFERENCES,0.4489092996555683,"preprint arXiv:2209.14792, 2022.
336"
REFERENCES,0.4500574052812859,"[32] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz,
337"
REFERENCES,0.45120551090700345,"Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video
338"
REFERENCES,0.452353616532721,"diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.
339"
REFERENCES,0.4535017221584386,"[33] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao.
340"
REFERENCES,0.45464982778415614,"Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024.
341"
REFERENCES,0.45579793340987373,"[34] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor,
342"
REFERENCES,0.4569460390355913,"Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as
343"
REFERENCES,0.4580941446613088,"world simulators. 2024.
344"
REFERENCES,0.4592422502870264,"[35] Chuanxia Zheng and Andrea Vedaldi. Free3d: Consistent novel view synthesis without 3d representation.
345"
REFERENCES,0.46039035591274396,"arXiv preprint arXiv:2312.04551, 2023.
346"
REFERENCES,0.46153846153846156,"[36] Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hongdong Li. Consistnet: Enforcing 3d consistency
347"
REFERENCES,0.4626865671641791,"for multi-view images diffusion. arXiv preprint arXiv:2310.10343, 2023.
348"
REFERENCES,0.46383467278989665,"[37] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren
349"
REFERENCES,0.46498277841561425,"Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM,
350"
REFERENCES,0.4661308840413318,"65(1):99‚Äì106, 2021.
351"
REFERENCES,0.4672789896670494,"[38] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P
352"
REFERENCES,0.46842709529276694,"Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In Proceedings
353"
REFERENCES,0.4695752009184845,"of the IEEE/CVF International Conference on Computer Vision, pages 5855‚Äì5864, 2021.
354"
REFERENCES,0.4707233065442021,"[39] Thomas M√ºller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives
355"
REFERENCES,0.4718714121699196,"with a multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):1‚Äì15, 2022.
356"
REFERENCES,0.4730195177956372,"[40] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler, and George Drettakis. 3d gaussian splatting for
357"
REFERENCES,0.47416762342135477,"real-time radiance field rendering. ACM Transactions on Graphics (TOG), 42(4):1‚Äì14, 2023.
358"
REFERENCES,0.4753157290470723,"[41] Hanwen Jiang, Zhenyu Jiang, Yue Zhao, and Qixing Huang. Leap: Liberate sparse-view 3d modeling from
359"
REFERENCES,0.4764638346727899,"camera poses. arXiv preprint arXiv:2310.01410, 2023.
360"
REFERENCES,0.47761194029850745,"[42] Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, and Song-Hai Zhang.
361"
REFERENCES,0.47876004592422505,"Triplane meets gaussian splatting: Fast and generalizable single-view 3d reconstruction with transformers.
362"
REFERENCES,0.4799081515499426,"arXiv preprint arXiv:2312.09147, 2023.
363"
REFERENCES,0.48105625717566014,"[43] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion.
364"
REFERENCES,0.48220436280137774,"arXiv preprint arXiv:2209.14988, 2022.
365"
REFERENCES,0.4833524684270953,"[44] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis,
366"
REFERENCES,0.4845005740528129,"Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In
367"
REFERENCES,0.4856486796785304,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300‚Äì309,
368"
REFERENCES,0.48679678530424797,"2023.
369"
REFERENCES,0.48794489092996557,"[45] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-Hao Chen,
370"
REFERENCES,0.4890929965556831,"Zi-Xin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: A unified framework for 3d
371"
REFERENCES,0.4902411021814007,"content generation. https://github.com/threestudio-project/threestudio, 2023.
372"
REFERENCES,0.49138920780711826,"[46] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang.
Neus:
373"
REFERENCES,0.4925373134328358,"Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint
374"
REFERENCES,0.4936854190585534,"arXiv:2106.10689, 2021.
375"
REFERENCES,0.49483352468427094,"[47] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-
376"
REFERENCES,0.49598163030998854,"1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF International Conference on
377"
REFERENCES,0.4971297359357061,"Computer Vision, pages 9298‚Äì9309, 2023.
378"
REFERENCES,0.49827784156142363,"[48] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweetdreamer: Aligning geometric priors in 2d diffusion
379"
REFERENCES,0.49942594718714123,"for consistent text-to-3d. arXiv preprint arXiv:2310.02596, 2023.
380"
REFERENCES,0.5005740528128588,"[49] Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion
381"
REFERENCES,0.5017221584385764,"models with self-conditioning. arXiv preprint arXiv:2208.04202, 2022.
382"
REFERENCES,0.5028702640642939,"[50] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-
383"
REFERENCES,0.5040183696900115,"adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In
384"
REFERENCES,0.505166475315729,"Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 4296‚Äì4304, 2024.
385"
REFERENCES,0.5063145809414467,"[51] Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong
386"
REFERENCES,0.5074626865671642,"Dong, Liefeng Bo, and Xiaoguang Han. Richdreamer: A generalizable normal-depth diffusion model for
387"
REFERENCES,0.5086107921928817,"detail richness in text-to-3d. arXiv preprint arXiv:2311.16918, 2023.
388"
REFERENCES,0.5097588978185993,"[52] Peng Wang and Yichun Shi. Imagedream: Image-prompt multi-view diffusion for 3d generation. arXiv
389"
REFERENCES,0.5109070034443168,"preprint arXiv:2312.02201, 2023.
390"
REFERENCES,0.5120551090700345,"[53] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based
391"
REFERENCES,0.513203214695752,"generative models. Advances in Neural Information Processing Systems, 35:26565‚Äì26577, 2022.
392"
REFERENCES,0.5143513203214696,"[54] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion
393"
REFERENCES,0.5154994259471871,"models.
394"
REFERENCES,0.5166475315729047,"[55] Wenzhe Shi, Jose Caballero, Ferenc Husz√°r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel
395"
REFERENCES,0.5177956371986223,"Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel
396"
REFERENCES,0.5189437428243399,"convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern
397"
REFERENCES,0.5200918484500574,"recognition, pages 1874‚Äì1883, 2016.
398"
REFERENCES,0.521239954075775,"[56] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
399"
REFERENCES,0.5223880597014925,"In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770‚Äì778, 2016.
400"
REFERENCES,0.5235361653272101,"[57] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations
401"
REFERENCES,0.5246842709529277,"enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM
402"
REFERENCES,0.5258323765786452,"SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505‚Äì3506, 2020.
403"
REFERENCES,0.5269804822043628,"A
Technical Details
404"
REFERENCES,0.5281285878300803,"A.1
Video model finetuning
405"
REFERENCES,0.529276693455798,"Based on the approach outlined in [32], the generation process employs the EDM framework[53].
406"
REFERENCES,0.5304247990815155,"Let pdata(x0) represent the video data distribution, and p(x; œÉ) be the distribution obtained by adding
407"
REFERENCES,0.5315729047072331,"Gaussian noise with variance œÉ2 to the data. For sufficiently large œÉmax, p(x; œÉ2
max) approximates
408"
REFERENCES,0.5327210103329506,"a normal distribution N(0, œÉ2
max). Diffusion models (DMs) leverage this property and begin with
409"
REFERENCES,0.5338691159586682,"high variance Gaussian noise, xM ‚àºN(0, œÉ2
max), and then iteratively denoise the data until reaching
410"
REFERENCES,0.5350172215843858,"œÉ0 = 0.
411"
REFERENCES,0.5361653272101033,"In practice, this iterative refinement process can be implemented through the numerical simulation of
412"
REFERENCES,0.5373134328358209,"the Probability Flow ordinary differential equation (ODE):
413"
REFERENCES,0.5384615384615384,"dx = ‚àíÀôœÉ(t)œÉ(t)‚àáx log p(x; œÉ(t)) dt
(3)"
REFERENCES,0.539609644087256,"where ‚àáx log p((x; œÉ) is called as score function.
414"
REFERENCES,0.5407577497129736,"DM training is to learn a model sŒ∏(x; œÉ) to approximate the score function ‚àáx log p((x; œÉ). The
415"
REFERENCES,0.5419058553386912,"model can be parameterized as:
416"
REFERENCES,0.5430539609644087,‚àáx log p((x; œÉ) ‚âàsŒ∏((x; œÉ) = DŒ∏(x; œÉ) ‚àíx
REFERENCES,0.5442020665901263,"œÉ2
,
(4)"
REFERENCES,0.5453501722158438,"where DŒ∏ is a learnable denoiser that aims to predict ground truth x0.
417"
REFERENCES,0.5464982778415615,"The denoiser DŒ∏ is trained via denoising score matching (DSM):
418"
REFERENCES,0.547646383467279,"Ex0‚àºpdata(x0),(œÉ,n)‚àºp(œÉ,n)

ŒªœÉ‚à•DŒ∏(x0 + n; œÉ) ‚àíx0‚à•2
2

,
(5)"
REFERENCES,0.5487944890929966,"where p(œÉ, n) = p(œÉ)N(n; 0, œÉ2), p(œÉ) is a distribution over noise levels œÉ, ŒªœÉ is a weighting
419"
REFERENCES,0.5499425947187141,"function. The learnable denoiser DŒ∏ is parameterized as:
420"
REFERENCES,0.5510907003444316,"DŒ∏(x; œÉ) = cskip(œÉ)x + cout(œÉ)FŒ∏(cin(œÉ)x; cnoise(œÉ)),
(6)"
REFERENCES,0.5522388059701493,"where FŒ∏ is the network to be trained.
421"
REFERENCES,0.5533869115958668,"We sample log œÉ ‚àºN(Pmean, P 2
std), with Pmean = 1.0 and Pstd = 1.6. Then we obtain all the
422"
REFERENCES,0.5545350172215844,"parameters as follows:
423"
REFERENCES,0.5556831228473019,"cin =
1
‚àö"
REFERENCES,0.5568312284730195,"œÉ2 + 1
(7) 424"
REFERENCES,0.5579793340987371,"cout =
‚àíœÉ
‚àö"
REFERENCES,0.5591274397244547,"œÉ2 + 1
(8) 425"
REFERENCES,0.5602755453501722,"cskip(œÉ) =
1
œÉ2 + 1
(9) 426"
REFERENCES,0.5614236509758898,"cnoise(œÉ) = 0.25 log œÉ
(10)
427"
REFERENCES,0.5625717566016073,Œª(œÉ) = 1 + œÉ2
REFERENCES,0.563719862227325,"œÉ2
(11)"
REFERENCES,0.5648679678530425,"We fine-tune the network backbone FŒ∏ on multi-view images of size 512 √ó 512. During training, for
428"
REFERENCES,0.56601607347876,"each instance in the dataset, we uniformly sample 8 views and choose the first view as the input view.
429"
REFERENCES,0.5671641791044776,"view images of size 512 √ó 512.
430"
REFERENCES,0.5683122847301951,Figure 7: The projection process of coordinates map.
REFERENCES,0.5694603903559128,"A.2
Coordinates Map
431"
REFERENCES,0.5706084959816303,"In conditional control models such as ControlNet[54], T2IAdapter, when depth maps are used as
432"
REFERENCES,0.5717566016073479,"input, their range needs to be normalized to the [0, 1] interval, typically using the formula: (p ‚àí
433"
REFERENCES,0.5729047072330654,"pmean)/(pmax ‚àípmin). However, this normalization process may introduce scale ambiguity, which
434"
REFERENCES,0.574052812858783,"can affect the multi-view generation performance. To avoid the issues caused by normalization, we use
435"
REFERENCES,0.5752009184845006,"coordinate maps. Coordinate maps transform the depth value d to a common world coordinate system
436"
REFERENCES,0.5763490241102182,"using the camera‚Äôs intrinsic and extrinsic parameters, represented as (X, Y, Z). The transformation
437"
REFERENCES,0.5774971297359357,"formula is:
438 X
Y
Z !"
REFERENCES,0.5786452353616532,"= K‚àí1 ¬∑ u
v
1 ! ¬∑ d"
REFERENCES,0.5797933409873708,"where (u, v) are the pixel coordinates, d is the corresponding depth value, and K is the camera
439"
REFERENCES,0.5809414466130884,"intrinsic matrix.
440"
REFERENCES,0.582089552238806,"A.3
3D Feedback
441"
REFERENCES,0.5832376578645235,"Figure 8: Architecture of the residual block
used in feedback stage."
REFERENCES,0.5843857634902411,"Input
inp ‚ààR3√ó512√ó512"
REFERENCES,0.5855338691159586,"PixelUnshuffle [55]
192 √ó 64 √ó 64
ResBlock √ó3
320 √ó 64 √ó 64
ResBlock √ó3
640 √ó 32 √ó 32
ResBlock √ó3
1280 √ó 16 √ó 16
ResBlock √ó3
1280 √ó 8 √ó 8
Table 4: The detailed structure of all layers in
the feedback injection network. 442"
REFERENCES,0.5866819747416763,"With reference to Section 3.3 in the main paper, Fig. 8 and Table 4 provide a detailed illustration of
443"
REFERENCES,0.5878300803673938,"the feedback injection netwrok. We use two networks to inject the coordinates map and RGB texture
444"
REFERENCES,0.5889781859931114,"map feedback into the score function. Each network consists of four feature extraction blocks and
445"
REFERENCES,0.5901262916188289,"three downsample blocks to adjust the feature resolution. The reconstruction coordinates map and
446"
REFERENCES,0.5912743972445464,"RGB texture map initially have a resolution of 512 √ó 512. We employ the pixel unshuffle operation
447"
REFERENCES,0.5924225028702641,"to downsample these maps to 64 √ó 64.
448"
REFERENCES,0.5935706084959816,"At each scale, three residual blocks[56] are used to extract the multi-scale feedback features,
449"
REFERENCES,0.5947187141216992,"denoted as FP
= {F 1
p , F 2
p , F 3
p , F 4
p } and FT
= {F 1
t , F 2
t , F 3
t , F 4
t } for the coordinates map
450"
REFERENCES,0.5958668197474167,"and RGB texture map, respectively. These feedback features match the intermediate features
451"
REFERENCES,0.5970149253731343,"Fenc = {F 1
enc, F 2
enc, F 3
enc, F 4
enc} in the encoder of the UNet denoiser. The feedback features FP
452"
REFERENCES,0.5981630309988519,"and FT are added to the intermediate features Fenc at each scale as described by the following
453"
REFERENCES,0.5993111366245695,"equations:
454"
REFERENCES,0.600459242250287,"Fp = F0(P)
(12)
455"
REFERENCES,0.6016073478760046,"Ft = F1(T)
(13)
456"
REFERENCES,0.6027554535017221,"Fi
enc = Fi
enc + Fi
p + Fi
t,
i ‚àà{1, 2, 3, 4}
(14)"
REFERENCES,0.6039035591274398,"where P represents the coordinates map feedback input, and T represents the RGB texture feedback
457"
REFERENCES,0.6050516647531573,"input. F0 and F1 denote the functions of the feedback inject network applied to the coordinates map
458"
REFERENCES,0.6061997703788748,"and RGB texture map, respectively.
459"
REFERENCES,0.6073478760045924,"B
Training Details and Experimental Settings
460"
REFERENCES,0.6084959816303099,"Implementation As illustrate in Table 5, all models are trained for 30,000 iterations using 8 A100
461"
REFERENCES,0.6096440872560276,"GPUs with a total batch size of 32. We clip the gradient with a maximum norm of 1.0. We use
462"
REFERENCES,0.6107921928817451,"the AdamW optimizer with a learning rate of 1 √ó 10‚àí5 and employ FP16 mixed precision with
463"
REFERENCES,0.6119402985074627,"DeepSeed[57] with Zero-2 for efficient training. We adjust the cameras in each batch so that the
464"
REFERENCES,0.6130884041331802,"initial input view consistently represents the reference frame, using an identity rotation matrix and a
465"
REFERENCES,0.6142365097588978,"fixed translation for alignment.
466"
REFERENCES,0.6153846153846154,"The inference settings are shown in Table 6.
467"
REFERENCES,0.616532721010333,"Hyperparameter
SVD (1.8 B)
LGM (424M)"
REFERENCES,0.6176808266360505,"Training
Optimizer
AdamW
AdamW
Learning rate
1e-5
1e-5
Batch size per GPU
4
4
# training steps
40k
40k
# GPUs
8
8
Training time (days)
4
4
Input Resolution
8 √ó 512 √ó 512 √ó 3
4 √ó 256 √ó 256 √ó 3
Output Resolution
8 √ó 512 √ó 512 √ó 3
‚àí√ó 512 √ó 512 √ó 3"
REFERENCES,0.618828932261768,"Diffusion setup
Pmean
1.0
-
Pstd
1.6
-
Table 5: Hyperparameters for the training stage."
REFERENCES,0.6199770378874856,"Hyperparameter
SC3D
VideoMV
SV3D
SyncDreamer"
REFERENCES,0.6211251435132032,"Sampling parameters
Sampler
Euler
DDIM
Euler
DDIM
steps
25
50
50
50
cfg gudiance
1.0 ‚àº3.0
6.0
6.0
2.0
Table 6: Hyperparameters for the inference stage."
REFERENCES,0.6222732491389208,"C
Additional Visualization Results
468"
REFERENCES,0.6234213547646383,"Figure 9: Visualization results generated by our SC3D. For each sample (3 rows), the 1st row is
ground truth, 2nd row is the generated multi-view images, while 3rd row is the rendered views from
reconstructed 3DGS. For each row, the first image is the input image."
REFERENCES,0.6245694603903559,"NeurIPS Paper Checklist
469"
CLAIMS,0.6257175660160735,"1. Claims
470"
CLAIMS,0.6268656716417911,"Question: Do the main claims made in the abstract and introduction accurately reflect the
471"
CLAIMS,0.6280137772675086,"paper‚Äôs contributions and scope?
472"
CLAIMS,0.6291618828932262,"Answer: [Yes]
473"
CLAIMS,0.6303099885189437,"Justification: The abstract and introduction clearly outline the primary contributions of the
474"
CLAIMS,0.6314580941446614,"paper. The claims made are directly supported by the experiments presented in the paper,
475"
CLAIMS,0.6326061997703789,"ensuring an accurate representation of the work‚Äôs contributions and limitations.
476"
CLAIMS,0.6337543053960965,"Guidelines:
477"
CLAIMS,0.634902411021814,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
478"
CLAIMS,0.6360505166475315,"made in the paper.
479"
CLAIMS,0.6371986222732492,"‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
480"
CLAIMS,0.6383467278989667,"contributions made in the paper and important assumptions and limitations. A No or
481"
CLAIMS,0.6394948335246843,"NA answer to this question will not be perceived well by the reviewers.
482"
CLAIMS,0.6406429391504018,"‚Ä¢ The claims made should match theoretical and experimental results, and reflect how
483"
CLAIMS,0.6417910447761194,"much the results can be expected to generalize to other settings.
484"
CLAIMS,0.642939150401837,"‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
485"
CLAIMS,0.6440872560275546,"are not attained by the paper.
486"
LIMITATIONS,0.6452353616532721,"2. Limitations
487"
LIMITATIONS,0.6463834672789897,"Question: Does the paper discuss the limitations of the work performed by the authors?
488"
LIMITATIONS,0.6475315729047072,"Answer: [Yes]
489"
LIMITATIONS,0.6486796785304249,"Justification: See in Section 4.3.
490"
LIMITATIONS,0.6498277841561424,"Guidelines:
491"
LIMITATIONS,0.6509758897818599,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
492"
LIMITATIONS,0.6521239954075775,"the paper has limitations, but those are not discussed in the paper.
493"
LIMITATIONS,0.653272101033295,"‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
494"
LIMITATIONS,0.6544202066590127,"‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
495"
LIMITATIONS,0.6555683122847302,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
496"
LIMITATIONS,0.6567164179104478,"model well-specification, asymptotic approximations only holding locally). The authors
497"
LIMITATIONS,0.6578645235361653,"should reflect on how these assumptions might be violated in practice and what the
498"
LIMITATIONS,0.6590126291618829,"implications would be.
499"
LIMITATIONS,0.6601607347876005,"‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
500"
LIMITATIONS,0.661308840413318,"only tested on a few datasets or with a few runs. In general, empirical results often
501"
LIMITATIONS,0.6624569460390356,"depend on implicit assumptions, which should be articulated.
502"
LIMITATIONS,0.6636050516647531,"‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.
503"
LIMITATIONS,0.6647531572904707,"For example, a facial recognition algorithm may perform poorly when image resolution
504"
LIMITATIONS,0.6659012629161883,"is low or images are taken in low lighting. Or a speech-to-text system might not be
505"
LIMITATIONS,0.6670493685419059,"used reliably to provide closed captions for online lectures because it fails to handle
506"
LIMITATIONS,0.6681974741676234,"technical jargon.
507"
LIMITATIONS,0.669345579793341,"‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms
508"
LIMITATIONS,0.6704936854190585,"and how they scale with dataset size.
509"
LIMITATIONS,0.6716417910447762,"‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
510"
LIMITATIONS,0.6727898966704937,"address problems of privacy and fairness.
511"
LIMITATIONS,0.6739380022962113,"‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
512"
LIMITATIONS,0.6750861079219288,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
513"
LIMITATIONS,0.6762342135476463,"limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
514"
LIMITATIONS,0.677382319173364,"judgment and recognize that individual actions in favor of transparency play an impor-
515"
LIMITATIONS,0.6785304247990815,"tant role in developing norms that preserve the integrity of the community. Reviewers
516"
LIMITATIONS,0.6796785304247991,"will be specifically instructed to not penalize honesty concerning limitations.
517"
THEORY ASSUMPTIONS AND PROOFS,0.6808266360505166,"3. Theory Assumptions and Proofs
518"
THEORY ASSUMPTIONS AND PROOFS,0.6819747416762342,"Question: For each theoretical result, does the paper provide the full set of assumptions and
519"
THEORY ASSUMPTIONS AND PROOFS,0.6831228473019518,"a complete (and correct) proof?
520"
THEORY ASSUMPTIONS AND PROOFS,0.6842709529276694,"Answer: [NA] .
521"
THEORY ASSUMPTIONS AND PROOFS,0.6854190585533869,"Justification: The paper does not include theoretical results.
522"
THEORY ASSUMPTIONS AND PROOFS,0.6865671641791045,"Guidelines:
523"
THEORY ASSUMPTIONS AND PROOFS,0.687715269804822,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
524"
THEORY ASSUMPTIONS AND PROOFS,0.6888633754305397,"‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
525"
THEORY ASSUMPTIONS AND PROOFS,0.6900114810562572,"referenced.
526"
THEORY ASSUMPTIONS AND PROOFS,0.6911595866819747,"‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
527"
THEORY ASSUMPTIONS AND PROOFS,0.6923076923076923,"‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
528"
THEORY ASSUMPTIONS AND PROOFS,0.6934557979334098,"they appear in the supplemental material, the authors are encouraged to provide a short
529"
THEORY ASSUMPTIONS AND PROOFS,0.6946039035591275,"proof sketch to provide intuition.
530"
THEORY ASSUMPTIONS AND PROOFS,0.695752009184845,"‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
531"
THEORY ASSUMPTIONS AND PROOFS,0.6969001148105626,"by formal proofs provided in appendix or supplemental material.
532"
THEORY ASSUMPTIONS AND PROOFS,0.6980482204362801,"‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
533"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6991963260619977,"4. Experimental Result Reproducibility
534"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7003444316877153,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
535"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7014925373134329,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
536"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7026406429391504,"of the paper (regardless of whether the code and data are provided or not)?
537"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.703788748564868,"Answer: [Yes]
538"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7049368541905855,"Justification: We provide the GSO generation result and code in the supplemental materials.
539"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7060849598163031,"Guidelines:
540"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7072330654420207,"‚Ä¢ The answer NA means that the paper does not include experiments.
541"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7083811710677382,"‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived
542"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7095292766934558,"well by the reviewers: Making the paper reproducible is important, regardless of
543"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7106773823191733,"whether the code and data are provided or not.
544"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.711825487944891,"‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
545"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7129735935706085,"to make their results reproducible or verifiable.
546"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7141216991963261,"‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
547"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7152698048220436,"For example, if the contribution is a novel architecture, describing the architecture fully
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7164179104477612,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7175660160734788,"be necessary to either make it possible for others to replicate the model with the same
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7187141216991964,"dataset, or provide access to the model. In general. releasing code and data is often
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7198622273249139,"one good way to accomplish this, but reproducibility can also be provided via detailed
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7210103329506314,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.722158438576349,"of a large language model), releasing of a model checkpoint, or other means that are
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7233065442020666,"appropriate to the research performed.
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7244546498277842,"‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7256027554535017,"sions to provide some reasonable avenue for reproducibility, which may depend on the
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7267508610792193,"nature of the contribution. For example
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7278989667049368,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7290470723306545,"to reproduce that algorithm.
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.730195177956372,"(b) If the contribution is primarily a new model architecture, the paper should describe
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7313432835820896,"the architecture clearly and fully.
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7324913892078071,"(c) If the contribution is a new model (e.g., a large language model), then there should
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7336394948335246,"either be a way to access this model for reproducing the results or a way to reproduce
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7347876004592423,"the model (e.g., with an open-source dataset or instructions for how to construct
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7359357060849598,"the dataset).
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7370838117106774,"(d) We recognize that reproducibility may be tricky in some cases, in which case
567"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7382319173363949,"authors are welcome to describe the particular way they provide for reproducibility.
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7393800229621125,"In the case of closed-source models, it may be that access to the model is limited in
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7405281285878301,"some way (e.g., to registered users), but it should be possible for other researchers
570"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7416762342135477,"to have some path to reproducing or verifying the results.
571"
OPEN ACCESS TO DATA AND CODE,0.7428243398392652,"5. Open access to data and code
572"
OPEN ACCESS TO DATA AND CODE,0.7439724454649828,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
573"
OPEN ACCESS TO DATA AND CODE,0.7451205510907003,"tions to faithfully reproduce the main experimental results, as described in supplemental
574"
OPEN ACCESS TO DATA AND CODE,0.746268656716418,"material?
575"
OPEN ACCESS TO DATA AND CODE,0.7474167623421355,"Answer: [Yes]
576"
OPEN ACCESS TO DATA AND CODE,0.748564867967853,"Justification: We provide the code in the supplemental materials.
577"
OPEN ACCESS TO DATA AND CODE,0.7497129735935706,"Guidelines:
578"
OPEN ACCESS TO DATA AND CODE,0.7508610792192881,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
579"
OPEN ACCESS TO DATA AND CODE,0.7520091848450058,"‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
580"
OPEN ACCESS TO DATA AND CODE,0.7531572904707233,"public/guides/CodeSubmissionPolicy) for more details.
581"
OPEN ACCESS TO DATA AND CODE,0.7543053960964409,"‚Ä¢ While we encourage the release of code and data, we understand that this might not be
582"
OPEN ACCESS TO DATA AND CODE,0.7554535017221584,"possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
583"
OPEN ACCESS TO DATA AND CODE,0.756601607347876,"including code, unless this is central to the contribution (e.g., for a new open-source
584"
OPEN ACCESS TO DATA AND CODE,0.7577497129735936,"benchmark).
585"
OPEN ACCESS TO DATA AND CODE,0.7588978185993112,"‚Ä¢ The instructions should contain the exact command and environment needed to run to
586"
OPEN ACCESS TO DATA AND CODE,0.7600459242250287,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
587"
OPEN ACCESS TO DATA AND CODE,0.7611940298507462,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
588"
OPEN ACCESS TO DATA AND CODE,0.7623421354764638,"‚Ä¢ The authors should provide instructions on data access and preparation, including how
589"
OPEN ACCESS TO DATA AND CODE,0.7634902411021814,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
590"
OPEN ACCESS TO DATA AND CODE,0.764638346727899,"‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
591"
OPEN ACCESS TO DATA AND CODE,0.7657864523536165,"proposed method and baselines. If only a subset of experiments are reproducible, they
592"
OPEN ACCESS TO DATA AND CODE,0.7669345579793341,"should state which ones are omitted from the script and why.
593"
OPEN ACCESS TO DATA AND CODE,0.7680826636050516,"‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
594"
OPEN ACCESS TO DATA AND CODE,0.7692307692307693,"versions (if applicable).
595"
OPEN ACCESS TO DATA AND CODE,0.7703788748564868,"‚Ä¢ Providing as much information as possible in supplemental material (appended to the
596"
OPEN ACCESS TO DATA AND CODE,0.7715269804822044,"paper) is recommended, but including URLs to data and code is permitted.
597"
OPEN ACCESS TO DATA AND CODE,0.7726750861079219,"6. Experimental Setting/Details
598"
OPEN ACCESS TO DATA AND CODE,0.7738231917336394,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
599"
OPEN ACCESS TO DATA AND CODE,0.7749712973593571,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
600"
OPEN ACCESS TO DATA AND CODE,0.7761194029850746,"results?
601"
OPEN ACCESS TO DATA AND CODE,0.7772675086107922,"Answer: [Yes]
602"
OPEN ACCESS TO DATA AND CODE,0.7784156142365097,"Justification: See in Appendix B.
603"
OPEN ACCESS TO DATA AND CODE,0.7795637198622273,"Guidelines:
604"
OPEN ACCESS TO DATA AND CODE,0.7807118254879449,"‚Ä¢ The answer NA means that the paper does not include experiments.
605"
OPEN ACCESS TO DATA AND CODE,0.7818599311136625,"‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
606"
OPEN ACCESS TO DATA AND CODE,0.78300803673938,"that is necessary to appreciate the results and make sense of them.
607"
OPEN ACCESS TO DATA AND CODE,0.7841561423650976,"‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
608"
OPEN ACCESS TO DATA AND CODE,0.7853042479908151,"material.
609"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7864523536165328,"7. Experiment Statistical Significance
610"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7876004592422503,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
611"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7887485648679678,"information about the statistical significance of the experiments?
612"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7898966704936854,"Answer: [No]
613"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7910447761194029,"Justification: The paper does not provide error bars or any statistical significance measures
614"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7921928817451206,"for the experimental results.
615"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7933409873708381,"Guidelines:
616"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7944890929965557,"‚Ä¢ The answer NA means that the paper does not include experiments.
617"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7956371986222732,"‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
618"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7967853042479908,"dence intervals, or statistical significance tests, at least for the experiments that support
619"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7979334098737084,"the main claims of the paper.
620"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.799081515499426,"‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for
621"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8002296211251435,"example, train/test split, initialization, random drawing of some parameter, or overall
622"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.801377726750861,"run with given experimental conditions).
623"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8025258323765786,"‚Ä¢ The method for calculating the error bars should be explained (closed form formula,
624"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8036739380022963,"call to a library function, bootstrap, etc.)
625"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8048220436280138,"‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
626"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8059701492537313,"‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error
627"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8071182548794489,"of the mean.
628"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8082663605051664,"‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
629"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8094144661308841,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
630"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8105625717566016,"of Normality of errors is not verified.
631"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8117106773823192,"‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or
632"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8128587830080367,"figures symmetric error bars that would yield results that are out of range (e.g. negative
633"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8140068886337543,"error rates).
634"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8151549942594719,"‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how
635"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8163030998851895,"they were calculated and reference the corresponding figures or tables in the text.
636"
EXPERIMENTS COMPUTE RESOURCES,0.817451205510907,"8. Experiments Compute Resources
637"
EXPERIMENTS COMPUTE RESOURCES,0.8185993111366245,"Question: For each experiment, does the paper provide sufficient information on the com-
638"
EXPERIMENTS COMPUTE RESOURCES,0.8197474167623421,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
639"
EXPERIMENTS COMPUTE RESOURCES,0.8208955223880597,"the experiments?
640"
EXPERIMENTS COMPUTE RESOURCES,0.8220436280137773,"Answer: [Yes]
641"
EXPERIMENTS COMPUTE RESOURCES,0.8231917336394948,"Justification: See Appendix B.
642"
EXPERIMENTS COMPUTE RESOURCES,0.8243398392652124,"Guidelines:
643"
EXPERIMENTS COMPUTE RESOURCES,0.8254879448909299,"‚Ä¢ The answer NA means that the paper does not include experiments.
644"
EXPERIMENTS COMPUTE RESOURCES,0.8266360505166476,"‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
645"
EXPERIMENTS COMPUTE RESOURCES,0.8277841561423651,"or cloud provider, including relevant memory and storage.
646"
EXPERIMENTS COMPUTE RESOURCES,0.8289322617680827,"‚Ä¢ The paper should provide the amount of compute required for each of the individual
647"
EXPERIMENTS COMPUTE RESOURCES,0.8300803673938002,"experimental runs as well as estimate the total compute.
648"
EXPERIMENTS COMPUTE RESOURCES,0.8312284730195177,"‚Ä¢ The paper should disclose whether the full research project required more compute
649"
EXPERIMENTS COMPUTE RESOURCES,0.8323765786452354,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
650"
EXPERIMENTS COMPUTE RESOURCES,0.8335246842709529,"didn‚Äôt make it into the paper).
651"
CODE OF ETHICS,0.8346727898966705,"9. Code Of Ethics
652"
CODE OF ETHICS,0.835820895522388,"Question: Does the research conducted in the paper conform, in every respect, with the
653"
CODE OF ETHICS,0.8369690011481056,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
654"
CODE OF ETHICS,0.8381171067738232,"Answer: [Yes]
655"
CODE OF ETHICS,0.8392652123995408,"Justification: We have reviewed the NeurIPS Code of Ethics.
656"
CODE OF ETHICS,0.8404133180252583,"Guidelines:
657"
CODE OF ETHICS,0.8415614236509759,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
658"
CODE OF ETHICS,0.8427095292766934,"‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
659"
CODE OF ETHICS,0.8438576349024111,"deviation from the Code of Ethics.
660"
CODE OF ETHICS,0.8450057405281286,"‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
661"
CODE OF ETHICS,0.8461538461538461,"eration due to laws or regulations in their jurisdiction).
662"
BROADER IMPACTS,0.8473019517795637,"10. Broader Impacts
663"
BROADER IMPACTS,0.8484500574052812,"Question: Does the paper discuss both potential positive societal impacts and negative
664"
BROADER IMPACTS,0.8495981630309989,"societal impacts of the work performed?
665"
BROADER IMPACTS,0.8507462686567164,"Answer: [Yes]
666"
BROADER IMPACTS,0.851894374282434,"Justification: In the Section 1, we discuss how 3D generation can accelerate various in-
667"
BROADER IMPACTS,0.8530424799081515,"dustries by enhancing design processes, improving simulations, and reducing production
668"
BROADER IMPACTS,0.8541905855338691,"costs.
669"
BROADER IMPACTS,0.8553386911595867,"Guidelines:
670"
BROADER IMPACTS,0.8564867967853043,"‚Ä¢ The answer NA means that there is no societal impact of the work performed.
671"
BROADER IMPACTS,0.8576349024110218,"‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
672"
BROADER IMPACTS,0.8587830080367393,"impact or why the paper does not address societal impact.
673"
BROADER IMPACTS,0.8599311136624569,"‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
674"
BROADER IMPACTS,0.8610792192881745,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
675"
BROADER IMPACTS,0.8622273249138921,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
676"
BROADER IMPACTS,0.8633754305396096,"groups), privacy considerations, and security considerations.
677"
BROADER IMPACTS,0.8645235361653272,"‚Ä¢ The conference expects that many papers will be foundational research and not tied
678"
BROADER IMPACTS,0.8656716417910447,"to particular applications, let alone deployments. However, if there is a direct path to
679"
BROADER IMPACTS,0.8668197474167624,"any negative applications, the authors should point it out. For example, it is legitimate
680"
BROADER IMPACTS,0.8679678530424799,"to point out that an improvement in the quality of generative models could be used to
681"
BROADER IMPACTS,0.8691159586681975,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
682"
BROADER IMPACTS,0.870264064293915,"that a generic algorithm for optimizing neural networks could enable people to train
683"
BROADER IMPACTS,0.8714121699196326,"models that generate Deepfakes faster.
684"
BROADER IMPACTS,0.8725602755453502,"‚Ä¢ The authors should consider possible harms that could arise when the technology is
685"
BROADER IMPACTS,0.8737083811710677,"being used as intended and functioning correctly, harms that could arise when the
686"
BROADER IMPACTS,0.8748564867967853,"technology is being used as intended but gives incorrect results, and harms following
687"
BROADER IMPACTS,0.8760045924225028,"from (intentional or unintentional) misuse of the technology.
688"
BROADER IMPACTS,0.8771526980482205,"‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
689"
BROADER IMPACTS,0.878300803673938,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
690"
BROADER IMPACTS,0.8794489092996556,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
691"
BROADER IMPACTS,0.8805970149253731,"feedback over time, improving the efficiency and accessibility of ML).
692"
SAFEGUARDS,0.8817451205510907,"11. Safeguards
693"
SAFEGUARDS,0.8828932261768083,"Question: Does the paper describe safeguards that have been put in place for responsible
694"
SAFEGUARDS,0.8840413318025259,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
695"
SAFEGUARDS,0.8851894374282434,"image generators, or scraped datasets)?
696"
SAFEGUARDS,0.886337543053961,"Answer: [NA]
697"
SAFEGUARDS,0.8874856486796785,"Justification: The paper does not involve the release of data or models that have a high risk
698"
SAFEGUARDS,0.8886337543053962,"for misuse.
699"
SAFEGUARDS,0.8897818599311137,"Guidelines: The paper focuses on foundational research and does not have direct societal
700"
SAFEGUARDS,0.8909299655568312,"implications. It does not address societal impacts.
701"
SAFEGUARDS,0.8920780711825488,"‚Ä¢ The answer NA means that the paper poses no such risks.
702"
SAFEGUARDS,0.8932261768082663,"‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
703"
SAFEGUARDS,0.894374282433984,"necessary safeguards to allow for controlled use of the model, for example by requiring
704"
SAFEGUARDS,0.8955223880597015,"that users adhere to usage guidelines or restrictions to access the model or implementing
705"
SAFEGUARDS,0.8966704936854191,"safety filters.
706"
SAFEGUARDS,0.8978185993111366,"‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
707"
SAFEGUARDS,0.8989667049368542,"should describe how they avoided releasing unsafe images.
708"
SAFEGUARDS,0.9001148105625718,"‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
709"
SAFEGUARDS,0.9012629161882894,"not require this, but we encourage authors to take this into account and make a best
710"
SAFEGUARDS,0.9024110218140069,"faith effort.
711"
LICENSES FOR EXISTING ASSETS,0.9035591274397244,"12. Licenses for existing assets
712"
LICENSES FOR EXISTING ASSETS,0.904707233065442,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
713"
LICENSES FOR EXISTING ASSETS,0.9058553386911596,"the paper, properly credited and are the license and terms of use explicitly mentioned and
714"
LICENSES FOR EXISTING ASSETS,0.9070034443168772,"properly respected?
715"
LICENSES FOR EXISTING ASSETS,0.9081515499425947,"Answer: [Yes]
716"
LICENSES FOR EXISTING ASSETS,0.9092996555683123,"Justification: The SVD model (https://huggingface.co/stabilityai/stable-video-diffusion-
717"
LICENSES FOR EXISTING ASSETS,0.9104477611940298,"img2vid) is intended for research purposes only. The following assets are used in the paper,
718"
LICENSES FOR EXISTING ASSETS,0.9115958668197475,"and their licenses are properly acknowledged:
719"
LICENSES FOR EXISTING ASSETS,0.912743972445465,"‚Ä¢ Gobjaverse: https://github.com/modelscope/richdreamer/tree/main/dataset/gobjaverse
720"
LICENSES FOR EXISTING ASSETS,0.9138920780711826,"‚Ä¢ LGM: https://github.com/3DTopia/LGM.git
721"
LICENSES FOR EXISTING ASSETS,0.9150401836969001,"‚Ä¢ Syncdreamer: https://github.com/liuyuan-pal/SyncDreamer.git
722"
LICENSES FOR EXISTING ASSETS,0.9161882893226176,"‚Ä¢ Objaverse: https://huggingface.co/datasets/allenai/objaverse
723"
LICENSES FOR EXISTING ASSETS,0.9173363949483353,"The use of the Objaverse dataset as a whole is licensed under the ODC-By v1.0 license.
724"
LICENSES FOR EXISTING ASSETS,0.9184845005740528,"Individual objects in Objaverse are licensed under various Creative Commons licenses,
725"
LICENSES FOR EXISTING ASSETS,0.9196326061997704,"including:
726"
LICENSES FOR EXISTING ASSETS,0.9207807118254879,"‚Ä¢ CC-BY 4.0 - 721K objects
727"
LICENSES FOR EXISTING ASSETS,0.9219288174512055,"‚Ä¢ CC-BY-NC 4.0 - 25K objects
728"
LICENSES FOR EXISTING ASSETS,0.9230769230769231,"‚Ä¢ CC-BY-NC-SA 4.0 - 52K objects
729"
LICENSES FOR EXISTING ASSETS,0.9242250287026407,"‚Ä¢ CC-BY-SA 4.0 - 16K objects
730"
LICENSES FOR EXISTING ASSETS,0.9253731343283582,"‚Ä¢ CC0 1.0 - 3.5K objects
731"
LICENSES FOR EXISTING ASSETS,0.9265212399540758,"Guidelines:
732"
LICENSES FOR EXISTING ASSETS,0.9276693455797933,"‚Ä¢ The answer NA means that the paper does not use existing assets.
733"
LICENSES FOR EXISTING ASSETS,0.928817451205511,"‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
734"
LICENSES FOR EXISTING ASSETS,0.9299655568312285,"‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
735"
LICENSES FOR EXISTING ASSETS,0.931113662456946,"URL.
736"
LICENSES FOR EXISTING ASSETS,0.9322617680826636,"‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
737"
LICENSES FOR EXISTING ASSETS,0.9334098737083811,"‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
738"
LICENSES FOR EXISTING ASSETS,0.9345579793340988,"service of that source should be provided.
739"
LICENSES FOR EXISTING ASSETS,0.9357060849598163,"‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
740"
LICENSES FOR EXISTING ASSETS,0.9368541905855339,"package should be provided. For popular datasets, paperswithcode.com/datasets
741"
LICENSES FOR EXISTING ASSETS,0.9380022962112514,"has curated licenses for some datasets. Their licensing guide can help determine the
742"
LICENSES FOR EXISTING ASSETS,0.939150401836969,"license of a dataset.
743"
LICENSES FOR EXISTING ASSETS,0.9402985074626866,"‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
744"
LICENSES FOR EXISTING ASSETS,0.9414466130884042,"the derived asset (if it has changed) should be provided.
745"
LICENSES FOR EXISTING ASSETS,0.9425947187141217,"‚Ä¢ If this information is not available online, the authors are encouraged to reach out to
746"
LICENSES FOR EXISTING ASSETS,0.9437428243398392,"the asset‚Äôs creators.
747"
NEW ASSETS,0.9448909299655568,"13. New Assets
748"
NEW ASSETS,0.9460390355912744,"Question: Are new assets introduced in the paper well documented and is the documentation
749"
NEW ASSETS,0.947187141216992,"provided alongside the assets?
750"
NEW ASSETS,0.9483352468427095,"Answer: [Yes]
751"
NEW ASSETS,0.9494833524684271,"Justification: We provide the code and generation results in supplemental materials.
752"
NEW ASSETS,0.9506314580941446,"Guidelines:
753"
NEW ASSETS,0.9517795637198623,"‚Ä¢ The answer NA means that the paper does not release new assets.
754"
NEW ASSETS,0.9529276693455798,"‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
755"
NEW ASSETS,0.9540757749712974,"submissions via structured templates. This includes details about training, license,
756"
NEW ASSETS,0.9552238805970149,"limitations, etc.
757"
NEW ASSETS,0.9563719862227325,"‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
758"
NEW ASSETS,0.9575200918484501,"asset is used.
759"
NEW ASSETS,0.9586681974741676,"‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
760"
NEW ASSETS,0.9598163030998852,"create an anonymized URL or include an anonymized zip file.
761"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9609644087256027,"14. Crowdsourcing and Research with Human Subjects
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9621125143513203,"Question: For crowdsourcing experiments and research with human subjects, does the paper
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9632606199770379,"include the full text of instructions given to participants and screenshots, if applicable, as
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9644087256027555,"well as details about compensation (if any)?
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.965556831228473,"Answer: [NA]
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9667049368541906,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9678530424799081,"Guidelines:
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9690011481056258,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9701492537313433,"human subjects.
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9712973593570609,"‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu-
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9724454649827784,"tion of the paper involves human subjects, then as much detail as possible should be
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9735935706084959,"included in the main paper.
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9747416762342136,"‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9758897818599311,"or other labor should be paid at least the minimum wage in the country of the data
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9770378874856487,"collector.
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9781859931113662,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9793340987370838,"Subjects
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9804822043628014,"Question: Does the paper describe potential risks incurred by study participants, whether
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.981630309988519,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9827784156142365,"approvals (or an equivalent approval/review based on the requirements of your country or
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.983926521239954,"institution) were obtained?
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9850746268656716,"Answer: [NA]
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9862227324913893,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9873708381171068,"Guidelines:
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9885189437428243,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
786"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9896670493685419,"human subjects.
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9908151549942594,"‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9919632606199771,"may be required for any human subjects research. If you obtained IRB approval, you
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9931113662456946,"should clearly state this in the paper.
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9942594718714122,"‚Ä¢ We recognize that the procedures for this may vary significantly between institutions
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9954075774971297,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9965556831228473,"guidelines for their institution.
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9977037887485649,"‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9988518943742825,"applicable), such as the institution conducting the review.
795"
