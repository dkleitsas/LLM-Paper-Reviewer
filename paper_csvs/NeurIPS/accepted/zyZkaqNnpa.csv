Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00099601593625498,"Common explanations for shortcut learning assume that the shortcut improves pre-
diction under the training distribution but not in the test distribution. Thus, models
trained via the typical gradient-based optimization of cross-entropy, which we call
default-ERM, utilize the shortcut. However, even when the stable feature deter-
mines the label in the training distribution and the shortcut does not provide any
additional information, like in perception tasks, default-ERM still exhibits short-
cut learning. Why are such solutions preferred when the loss for default-ERM
can be driven to zero using the stable feature alone? By studying a linear per-
ception task, we show that default-ERM’s preference for maximizing the margin
leads to models that depend more on the shortcut than the stable feature, even
without overparameterization. This insight suggests that default-ERM’s implicit
inductive bias towards max-margin is unsuitable for perception tasks. Instead, we
develop an inductive bias toward uniform margins and show that this bias guar-
antees dependence only on the perfect stable feature in the linear perception task.
We develop loss functions that encourage uniform-margin solutions, called mar-
gin control (MARG-CTRL). MARG-CTRL mitigates shortcut learning on a variety
of vision and language tasks, showing that better inductive biases can remove the
need for expensive two-stage shortcut-mitigating methods in perception tasks."
INTRODUCTION,0.00199203187250996,"1
Introduction"
INTRODUCTION,0.00298804780876494,"Shortcut learning is a phenomenon where a model learns to base its predictions on an unstable
correlation, or shortcut, that does not hold across data distributions collected at different times and/or
places [Geirhos et al., 2020]. A model that learns shortcuts can perform worse than random guessing
in settings where the label’s relationship with the shortcut feature changes [Koh et al., 2021, Puli
et al., 2022]. Such drops in performance do not occur if the model depends on features whose
relationship with the label does not change across settings; these are stable features."
INTRODUCTION,0.00398406374501992,"Shortcut learning is well studied in cases where models that use both shortcut and stable features
achieve lower loss than models that only use the stable feature [Arjovsky et al., 2019, Puli et al.,
2022, Geirhos et al., 2020]. These works consider cases where the Bayes-optimal classiﬁer — the
training conditional distribution of the label given the covariates — depends on both stable and
shortcut features. In such cases, shortcut learning occurs as the Bayes-optimal predictor is the target
of standard supervised learning algorithms such as the one that minimizes the log-loss via gradient
descent (GD), which we call default-ERM."
INTRODUCTION,0.0049800796812749,"However, in many machine learning tasks, the stable feature perfectly predicts the label, i.e. a perfect
stable feature. For example, in task of predicting hair color from images of celebrity faces in the"
INTRODUCTION,0.00597609561752988,∗1Corresponding email: aahlad@nyu.edu
INTRODUCTION,0.0069721115537848604,"CelebA dataset [Sagawa et al., 2020a], the color of the hair in the image determines the label. This
task is a perception task. In such classiﬁcation tasks, the label is independent of the shortcut feature
given the stable feature, and the Bayes-optimal predictor under the training distribution only depends
on the stable feature. Default-ERM can learn this Bayes-optimal classiﬁer which, by depending
solely on the stable feature, also generalizes outside the training distribution. But in practice, default-
ERM run on ﬁnite data yields models that depend on the shortcut and thus perform worse than chance
outside the training distribution [Sagawa et al., 2020a, Liu et al., 2021, Zhang et al., 2022]. The
question is, why does default-ERM prefer models that exploit the shortcut even when a model can
achieve zero loss using the stable feature alone?"
INTRODUCTION,0.00796812749003984,"To understand preferences toward shortcuts, we study default-ERM on a linear perception task with a
stable feature that determines the label and a shortcut feature that does not. The perfect linear stable
feature means that data is linearly separable. This separability means that default-ERM-trained linear
models classify in the same way as the minimum ℓ2-norm solution that has all margins greater than 1;
the latter is commonly called max-margin classiﬁcation [Soudry et al., 2018]. We prove that default-
ERM’s implicit inductive bias toward the max-margin solution is harmful in that default-ERM-trained
linear models depend more on the shortcut than the stable feature. In fact, such dependence on
the shortcut occurs even in the setting with fewer parameters in the linear model than data points,
i.e. without overparameterization. These observations suggest that a max-margin inductive bias is
unsuitable for perception tasks."
INTRODUCTION,0.008964143426294821,"Next, we study inductive biases more suitable for perception tasks with perfect stable features. We
ﬁrst observe that predicting with the perfect stable feature alone achieves uniform margins on all
samples. Formally, if the stable feature s(x) determines the label y via a function d, y = d ◦s(x),
one can achieve any positive b as the margin on all samples simultaneously by predicting with
b · d ◦s(x). We show that in the same setting without overparameterization where max-margin
classiﬁcation leads to shortcut learning, models that classify with uniform margins depend only on
the stable feature."
INTRODUCTION,0.0099601593625498,"Building on these observations, we identify alternative loss functions that are inductively biased
toward uniform margins, which we call margin control (MARG-CTRL). We empirically demonstrate
that MARG-CTRL mitigates shortcut learning on multiple vision and language tasks without the use
of annotations of the shortcut feature in training. Further, MARG-CTRL performs on par or better
than the more expensive two-stage shortcut-mitigating methods [Liu et al., 2021, Zhang et al., 2022].
We then introduce a more challenging setting where both training and validation shortcut annotations
are unavailable, called the nuisance-free setting. In the nuisance-free setting, MARG-CTRL always
outperforms default-ERM and the two-stage shortcut-mitigating methods. These empirical results
suggest that simply incorporating inductive biases more suitable for perception tasks is sufﬁcient to
mitigate shortcuts."
SHORTCUT LEARNING IN PERCEPTION TASKS DUE TO MAXIMIZING MARGINS,0.010956175298804782,"2
Shortcut learning in perception tasks due to maximizing margins"
SHORTCUT LEARNING IN PERCEPTION TASKS DUE TO MAXIMIZING MARGINS,0.01195219123505976,"Setup.
We use y, z, x to denote the label, the shortcut feature, and the covariates respectively. We
let the training and test distributions (ptr, pte) be members of a family of distributions indexed by
ρ, F = {pρ(y, z, x)}ρ, such that the shortcut-label relationship pρ(z, y) changes over the family.
Many common tasks in the spurious correlations literature have stable features s(x) that are perfect,
meaning that the label is a deterministic function d of the stable feature: y = d ◦s(x). For example,
in the Waterbirds task the bird’s body determines the label and in the CelebA task, hair color deter-
mines the label [Sagawa et al., 2020a]. As s(x) determines the label, it holds that y |="
SHORTCUT LEARNING IN PERCEPTION TASKS DUE TO MAXIMIZING MARGINS,0.012948207171314742,"pρ(x, z) | s(x).
Then, the optimal predictor on the training distribution is optimal on all distributions in the family
F, regardless of the shortcut because"
SHORTCUT LEARNING IN PERCEPTION TASKS DUE TO MAXIMIZING MARGINS,0.013944223107569721,ptr(y | x) = ptr(y | s(x)) = pte(y | s(x)) = pte(y | x).
SHORTCUT LEARNING IN PERCEPTION TASKS DUE TO MAXIMIZING MARGINS,0.014940239043824702,"The most common procedure to train predictive models to approximate ptr(y | x) is gradient-based
optimization of cross-entropy (also called log-loss); we call this default-ERM. Default-ERM targets
the Bayes-optimal predictor of the training distribution which, in tasks with perfect stable features,
also performs optimally under the test distribution. However, despite targeting the predictor that does
not depend on the shortcut, models built with default-ERM still rely on shortcut features that are often
less predictive of the label and are unstable, i.e. vary across distributions [Geirhos et al., 2020, Puli
et al., 2022]. We study default-ERM’s preference for shortcuts in a data generating process (DGP)
where both the shortcut and the perfect stable feature are linear functions of the covariates."
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.01593625498007968,"2.1
Shortcut learning in linear perception tasks"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.01693227091633466,"Let Rad be the uniform distribution over {1, −1}, N be the normal distribution, d be the dimension
of x, and ρ ∈(0, 1), B > 1 be scalar constants. The DGP for pρ(y, z, x) is:"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.017928286852589643,"y ∼Rad,
z ∼"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.018924302788844622,󰀝pρ(z = y | y = y) = ρ
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.0199203187250996,"pρ(z = −y | y = y) = (1 −ρ)
,
δ ∼N(0, Id−2),
x = [B ∗z, y, δ] . (1)"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.02091633466135458,"This DGP is set up to mirror the empirical evidence in the literature showing that shortcut features
are typically learned ﬁrst [Sagawa et al., 2020a]. The ﬁrst dimension of x, i.e. x1, is a shortcut
that is correlated with y according to ρ. The factor B in x1 scales up the gradients for parameters
that interact with x1 in predictions. For large enough B, model dependence on the shortcut feature
during default-ERM goes up faster than the stable feature [Idrissi et al., 2022]."
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.021912350597609563,(a) Average accuracy and loss curves.
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.022908366533864542,(b) Accuracy and loss on shortcut and leftover groups.
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.02390438247011952,"Figure 1: Accuracy and loss curves for training a linear
model with default-ERM on 1000 training samples from p0.9,
with B = 10, d = 300 (see eq. (1)), and testing on p0.1.
(a) The model achieves 100% train accuracy but < 40% test
accuracy. (b) The learned model achieves high test accuracy
(≈90%) on the shortcut group and low test accuracy on the
leftover group (≈30%). Models that depend more on the
stable feature than on the shortcut, achieve at least 50% ac-
curacy on both the shortcut and leftover groups. Hence the
learned model exploits the shortcut to classify the shortcut
group and overﬁts to the leftover group."
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.0249003984063745,"The training distribution is ptr = p0.9 and
the test distribution is one where the short-
cut’s relationship with the label is ﬂipped
pte = p0.1. Models achieve worse than
random test accuracy (50%) if they ex-
ploit the training shortcut relationship and
the predicted class ﬂips when the shortcut
feature ﬂips. We train with default-ERM
which uses log-loss: on a data point (x, y)
the log-loss is"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.025896414342629483,ℓlog(yfθ(x)) = log [1 + exp(−yfθ(x))] .
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.026892430278884463,"With d = 300 and B = 10, we train
a linear model on 1000 samples from the
training distribution pρ=0.9, and evaluate
on 1000 samples from pρ=0.1."
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.027888446215139442,"Observations.
Figure
1a
shows
that
when trained with default-ERM, the lin-
ear model does not do better than chance
(< 50%) on the test data even after 50, 000
epochs. So, even in the presence of the
perfect feature x2, the model relies on
other features like the shortcut x1. Since
the ﬁnal training loss is very small, on the
order of 10−9, this result is not due to op-
timization being stuck in a local minima
with high loss. These observations indi-
cate that, in the linear setting, gradient-
based optimization with log-loss prefers
models that depend more on the shortcut than the perfect stable feature."
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.02888446215139442,"To better understand this preference we focus on the errors in speciﬁc groups in the data. Con-
sider the classiﬁer that only uses the shortcut z and makes the Bayes-optimal prediction w.r.t ptr:
arg maxy ptr(y = y | z). We call instances that are classiﬁed correctly by this model the shortcut
group, and the rest the leftover group. We use these terms for instances in the training set as well as
the test set. In this experiment y is positively correlated with z, hence the shortcut group consists of
all instances with yi = zi and the leftover group of those with yi ∕= zi."
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.029880478087649404,"Figure 1b gives accuracy and loss curves on the shortcut and leftover groups for the ﬁrst 10000
epochs. The test accuracy for the shortcut group hits 90% while the leftover group test accuracy is
< 40%, meaning that the model exploits the shortcuts. Even though a model that relies solely on
the shortcut misclassiﬁes the leftover group, we see that the training loss of the learned model on
this group approaches 0. The model drives down training loss in the leftover group by depending
on noise, which results in larger test loss in the leftover group than the shortcut group. Thus,
ﬁg. 1b demonstrates that the default-ERM-trained model classiﬁes the training shortcut group
by using the shortcut feature while overﬁtting to the training leftover group."
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.030876494023904383,"Shortcut dependence like in ﬁg. 1 occurs even with ℓ2-regularization and when training neural net-
works; see appendix B.1 and appendix B.4 respectively. Next, we analyze the failure mode in ﬁg. 1,
showing that the shortcut dependence is due to default-ERM’s implicit bias to learn the max-margin
classiﬁer. Next, we study the failure mode in ﬁg. 1 theoretically, showing that the shortcut depen-
dence is due to default-ERM’s inductive bias toward learning the max-margin classiﬁer."
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.03187250996015936,"Max-margin classiﬁers depend more on the the shortcut than the stable feature.
We consider
training a linear model fθ(x) = w⊤x where w = [wz, wy, we] with default-ERM. Data from
eq. (1) is always linearly separable due to the perfect stable feature, but many hyperplanes that
separate the two classes exist.
When a linear model is trained with default-ERM on linearly
separable data, it achieves zero training loss and converges to the direction of a minimum ℓ2-norm
solution that achieves a margin of at least 1 on all samples [Soudry et al., 2018, Wang et al., 2021,
2022]; this is called the max-margin solution. We now show that for a small enough leftover group,
large enough scaling factor B and dimension d of the covariates, max-margin solutions depend
more on the shortcut feature than the stable feature:"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.03286852589641434,"Theorem 1. Let w∗be the max-margin predictor on n training samples from eq. (1) with a leftover
group of size k. There exist constants C1, C2, N0 > 0 such that"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.03386454183266932,"∀n > N0,
∀integers k ∈ 󰀓 0, n 10 󰀔"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.0348605577689243,",
∀d ≥C1k log(3n),
∀B > C2 󰁵"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.035856573705179286,"d
k ,
(2)"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.036852589641434265,"with probability at least 1 −1/3n over draws of the training data, it holds that Bw∗"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.037848605577689244,z > w∗ y.
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.03884462151394422,"The size of the leftover group k concentrates around (1 −ρ)n because each sample falls in the
leftover group with probability (1 −ρ). Thus, for ρ > 0.9, that is for a strong enough shortcut,
the condition in theorem 1 that k < n/10 will hold with probability close to 1; see appendix A.5 for
more details."
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.0398406374501992,"The proof is in appendix A. The ﬁrst bit of intuition is that using the shortcut can have lower norm
because of the scaling factor B. Using the shortcut only, however, misclassiﬁes the leftover group.
The next bit of intuition is that using noise from the leftover group increases margins in one group at
a rate that scales with the dimension d, while the cost in the margin for the other group only grows
as √"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.04083665338645418,"d. This trade-off in margins means the leftover group can be correctly classiﬁed using noise
without incorrectly classifying the shortcut group. The theorem then leverages convex duality to
show that this type of classiﬁer that uses the shortcut and noise has smaller ℓ2-norm than any linear
classiﬁer that uses the stable feature more."
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.04183266932270916,"The way the margin trade-off in the proof works is by constructing a linear classiﬁer whose weights
on the noise features are a scaled sum of the product of the label and the noise vector in the leftover
group: for a scalar γ, the weights we = γ 󰁓"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.04282868525896414,"i∈Sleftover yiδi. The margin change on the jth training
sample from using these weights is yjw⊤"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.043824701195219126,"e δj. For samples in the shortcut group, the margin change
looks like a sum of mean zero independent and identically distributed variables; the standard devi-
ation of this sum grows as √"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.044820717131474105,"d. For samples in the leftover group, the margin change is the sum
of mean one random variables; this sum grows as d and its standard deviation grows as √"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.045816733067729085,"d. The
difference in mean relative to the standard deviation is what provides the trade-off in margins."
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.046812749003984064,We now discuss three implications of the theorem.
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.04780876494023904,"First, theorem 1 implies that the leftover group sees worse than random accuracy (0.5). To see
this, note that for samples in the leftover group the margin y(w∗)⊤x = w∗"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.04880478087649402,y −Bw∗
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.049800796812749,z + (w∗
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.05079681274900399,"e)⊤yδ is
a Gaussian random variable centered at a negative number w∗"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.05179282868525897,y −Bw∗
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.052788844621513946,"z. Then, with Φe as the CDF
of the zero-mean Gaussian random variable (w∗"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.053784860557768925,"e)⊤δ, accuracy in the test leftover group is"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.054780876494023904,p(y(w∗)⊤x ≥0 | y ∕= z) = p[(w∗
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.055776892430278883,e)⊤δ > −(w∗
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.05677290836653386,y −Bw∗
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.05776892430278884,z)] = 1 −Φe(−(w∗
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.05876494023904383,y −Bw∗
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.05976095617529881,z)) ≤0.5.
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.060756972111553786,"Second, the leftover group in the training data is overﬁt in that the contribution of noise in prediction
(|(w∗"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.061752988047808766,"e)⊤δ|) is greater than the contribution from the stable and shortcut features. Formally, in the
training leftover group, w∗"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.06274900398406374,y −Bw∗
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.06374501992031872,"z < 0. Then, due to max-margin property, w∗"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.0647410358565737,y −Bw∗
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.06573705179282868,z + (w∗
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.06673306772908366,e)⊤yiδi > 1 =⇒(w∗
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.06772908366533864,e)⊤yiδi ≥1 −(w∗
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.06872509960159362,y −Bw∗
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.0697211155378486,z) > |w∗
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.07071713147410359,y −Bw∗ z|.
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.07171314741035857,"Third, many works point to overparameterization as one of the causes behind shortcut learning
[Sagawa et al., 2020a, Nagarajan et al., 2021, Wald et al., 2023], but in the setup in ﬁg. 1, the"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.07270916334661355,"linear model has fewer parameters than samples in the training data.
In such cases with non-
overparameterized linear models, the choice of default-ERM is typically not questioned, especially
when a feature exists that linearly separates the data.
Corollary 1 formally shows shortcut
learning for non-overparameterized linear models. In words, default-ERM — that is vanilla
logistic regression trained with gradient-based optimization — can yield models that rely
more on the shortcut feature even without overparameterization."
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.07370517928286853,"Corollary 1. For all n > N0 — where the constant N0 is from theorem 1 — with scalar τ ∈(0, 1)"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.07470119521912351,"such that the dimension of x is d = τn < n, for all integers k < n × min 󰁱"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.07569721115537849,"1
10,
τ
C1 log 3n 󰁲"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.07669322709163347,", a linear
model trained via default-ERM yields a predictor w∗such that Bw∗"
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.07768924302788845,z > w∗ y.
SHORTCUT LEARNING IN LINEAR PERCEPTION TASKS,0.07868525896414343,"If default-ERM produces models that suffer from shortcut learning even without overparameteriza-
tion, its implicit inductive bias toward max-margin classiﬁcation is inappropriate for perception tasks
in the presence of shortcuts. Next, we study inductive biases more suited to perception tasks."
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.0796812749003984,"3
Toward inductive biases for perception tasks with shortcuts"
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.08067729083665338,"The previous section formalized how default-ERM solutions, due to the max-margin inductive bias,
rely on the shortcut and noise to minimize loss on training data even in the presence of a different
zero-population-risk solution. Are there inductive biases more suitable for perception tasks?"
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.08167330677290836,"Given a perfect stable feature s(x) for a perception task, in that for a function d when y = d ◦s(x),
one can achieve margin b ∈(0, ∞) uniformly on all samples by predicting with the stable b·d◦s(x).
In contrast, max-margin classiﬁers allow for disparate margins as long as the smallest margin
crosses 1, meaning that it does not impose uniform margins. The cost of allowing disparate margins
is the preference for shortcuts even without overparamterization (corollary 1). In the same setting
however, any uniform-margin classiﬁer for the linear perception task (eq. (1)) relies only on the
stable feature:"
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.08266932270916334,"Theorem 2. Consider n samples of training data from DGP in eq. (1) with d < n. Consider a
linear classiﬁer fθ(x) = w⊤x such that for all samples in the training data yiw⊤xi = b for any
b ∈(0, ∞). With probability 1 over draws of samples, w = [0, b, 0d−2]."
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.08366533864541832,"Theorem 2 shows that uniform-margin classiﬁers only depend on the stable feature, standing in
contrast with max-margin classiﬁers which can depend on the shortcut feature (theorem 1). The
proof is in appendix A.6. Thus, inductive biases toward uniform margins are better suited for
perception tasks. Next, we identify several ways to encourage uniform margins."
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.0846613545816733,"Margin control (MARG-CTRL).
To produce uniform margins with gradient-based optimization,
we want the loss to be minimized at uniform-margin solutions and be gradient-optimizable. We iden-
tify a variety of losses that satisfy these properties, and we call them MARG-CTRL losses. MARG-
CTRL losses have the property that per-sample loss monotonically decreases for margins until a
threshold then increases for margins beyond it. In turn, minimizing loss then encourages all margins
to move to the threshold."
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.08565737051792828,"Mechanically, when models depend more on shortcuts than the stable feature during training, mar-
gins on samples in the shortcut group will be larger than those in the leftover group; see the right
panel in ﬁg. 1b where the train loss in the shortcut group is lower than the leftover group indicating
that the margins are smaller in the leftover group. This difference is margins is a consequence of the
shortcut matching the label in one group and not the other, thus, encouraging the model to have sim-
ilar margins across all samples pushes the model to depend less on the shortcut. In contrast, vanilla
log-loss can be driven to zero in a direction with disparate margins across the groups as long as the
margins on all samples go to ∞. We deﬁne MARG-CTRL losses for a model fθ with the margin on
a sample (x, y) deﬁned as yfθ(x)."
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.08665338645418327,"As the ﬁrst MARG-CTRL loss, we develop the σ-damped log-loss: we evaluate log-loss on a margin
multiplied by a monotonically decreasing function of the margin. In turn, the input to the loss
increases with the margin till a point and then decreases. For a temperature T and sigmoid function"
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.08764940239043825,"σ, the σ-damped loss modiﬁes the model output fθ and plugs it into log-loss:"
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.08864541832669323,"ℓσ-damp(y, fθ) = ℓlog 󰀕 y 󰀕 1 −σ 󰀕yfθ T 󰀖󰀖 fθ 󰀖"
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.08964143426294821,"Figure 2: Using σ-damped log-loss yields linear models that depend
on the perfect stable feature to achieve near perfect test accuracy. The
middle panel shows that σ-damping maintains similar margins in the
training shortcut and leftover groups unlike unconstrained log-loss,
and the right panel shows σ-damp achieves better leftover test-loss."
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.09063745019920319,"For
large
margin
predictions
yfθ > 0, the term 1−σ (yfθ(x)/T)
damps down the input to log-loss.
The largest the input to ℓlog can
get is 0.278T, found by setting
the derivative to zero, thus lower
bounding the loss.
As log-loss
is a decreasing function of its
input, the minimum of ℓσ-damp
occurs when the margin is 0.278T
on all samples.
To demonstrate
empirical advantage, we compare
standard log-loss to σ-damped loss on eq. (1); see ﬁg. 2. The left panel of ﬁgure ﬁg. 2 shows that
test accuracy is better for σ-damp. The middle and right panels shows the effect of controlling
margins in training, where losses on shortcut and leftover groups hover at the same value."
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.09163346613545817,"Second, we design the σ-stitch loss, which imitates log-loss when yfθ(x) < u and penalizes larger
margins (yfθ > u) by negating the sign of yfθ(x):"
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.09262948207171315,"ℓσ-stitch = ℓlog ( 1[yfθ(x) ≤u]yfθ(x) + 1[yfθ(x) > u](2u −yfθ(x)) )
(3)"
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.09362549800796813,"As the third MARG-CTRL loss, we directly penalize large margins via a log-penalty:"
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.0946215139442231,ℓmarg-log = ℓlog(yfθ(x)) + λ log 󰀃
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.09561752988047809,1 + |fθ(x)|2󰀄 (4)
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.09661354581673307,The fourth MARG-CTRL loss controls margins by penalizing |fθ(x)|2:
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.09760956175298804,"ℓSD = ℓlog(yfθ(x)) + λ|fθ(x)|2
(5)"
TOWARD INDUCTIVE BIASES FOR PERCEPTION TASKS WITH SHORTCUTS,0.09860557768924302,"This last penalty was called spectral decoupling (SD) by Pezeshki et al. [2021], who use it as a way
to decouple learning dynamics in the neural tangent kernel (NTK) regime. Instead, from the lens of
MARG-CTRL, SD mitigates shortcuts in eq. (1) because it encourages uniform margins, even though
SD was originally derived from different principles, as we discuss in section 4. In appendix B.2,
we plot all MARG-CTRL losses and show that MARG-CTRL improves over default-ERM on the linear
perception task; see ﬁgs. 6 to 8. We also run MARG-CTRL on a neural network and show that while
default-ERM achieves test accuracy worse than random chance, MARG-CTRL achieves 100% test
accuracy; see ﬁgs. 10 to 13 in appendix B.4."
RELATED WORK,0.099601593625498,"4
Related work"
RELATED WORK,0.10059760956175298,"A large body of work tackles shortcut learning under different assumptions [Arjovsky et al., 2019,
Wald et al., 2021, Krueger et al., 2020, Creager et al., 2021, Veitch et al., 2021, Puli et al., 2022,
Heinze-Deml and Meinshausen, 2021, Belinkov and Bisk, 2017]. A different line of work focuses on
learning in neural networks in idealized settings [Yang and Salman, 2019, Ronen et al., 2019, Jo and
Bengio, 2017, Baker et al., 2018, Saxe et al., 2013, Gidel et al., 2019, Advani et al., 2020]."
RELATED WORK,0.10159362549800798,"Shah et al. [2020] study simplicity bias [Valle-Perez et al., 2018] and show that neural networks
provably learn the linear function over a non-linear one, in the ﬁrst epoch of training. In a simi-
lar vein, Hermann and Lampinen [2020] show that neural networks can prefer a linearly-decodable
feature over a non-linear but more predictive feature, and Scimeca et al. [2021] make similar obser-
vations and use loss landscapes to empirically study which features are easier to learn. Simplicity
bias alone only describes neural biases early in training and does not explain why more predictive
stable features are not learned later. Unlike simplicity bias which focuses on linear versus non-
linear features, max-margin bias is the reason default-ERM prefers one linear feature, the shortcut,
over another, the stable feature, like in the synthetic experiment in section 2."
RELATED WORK,0.10258964143426295,"While Pezeshki et al. [2021] allow for perfect features, they hypothesize that shortcut learning occurs
because when one feature is learned ﬁrst, other features are gradient-starved and are not learned as"
RELATED WORK,0.10358565737051793,"well. They focus on a special setting where feature representations for different samples have inner
product equal to a small constant to show that models can depend more on the imperfect feature
than the perfect feature. In this special setting, they show that penalizing the magnitudes of what we
call the margin mitigates shortcuts; this method is called spectral decoupling (SD). However, as we
show in appendix B.5, the assumption in Lemma 1 [Pezeshki et al., 2021] is violated when using a
linear model to classify in the simple linear DGP in eq. (1). However, SD on a linear model mitigates
shortcuts in the DGP in eq. (1); see B.5. Thus, the theory in Pezeshki et al. [2021] fails to not explain
why SD works for eq. (1), but the uniform-margin property explains why all the MARG-CTRL losses,
including SD, mitigate shortcuts."
RELATED WORK,0.10458167330677291,"Nagarajan et al. [2021] consider tasks with perfect stable features and formalize geometric properties
of the data that make max-margin classiﬁers have non-zero weight on the shortcut feature (wz > 0).
In their set up, the linear models are overparameterized and it is unclear when wz > 0 leads to
worse-than-random accuracy in the leftover group because they do not separate the model’s depen-
dence on the stable feature from the dependence on noise. See ﬁg. 14 for an example where wz > 0
but test accuracy is 100%. In contrast to Nagarajan et al. [2021], theorem 1 gives DGPs where
the leftover group accuracy is worse than random, even without overparameterization. Ahuja et al.
[2021] also consider linear classiﬁcation with default-ERM with a perfect stable feature and con-
clude that default-ERM learns only the stable feature because they assume no additional dimensions
of noise in the covariates. We develop the necessary nuance by including noise in the problem and
showing default-ERM depends on the shortcut feature even without overparameterization."
RELATED WORK,0.10557768924302789,"Sagawa et al. [2020b] and Wald et al. [2023] both consider overparameterized settings where the
shortcut feature is informative of the label even after conditioning on the stable feature. In both cases,
the Bayes-optimal predictor also depends on the shortcut feature, which means their settings do not
allow for an explanation of shortcut dependence in examples like ﬁg. 1. In contrast, we show shortcut
dependence occurs even in the presence of a perfect stable feature and without overparameterization.
Li et al. [2019], Pezeshki et al. [2022] focus on relative feature complexity and discuss the effects
of large learning rate (LR) on which features are learned ﬁrst during training, but do not allow
for perfect features. Idrissi et al. [2022] empirically ﬁnd that tuning LR and weight decay (WD) gets
default-ERM to perform similar to two-stage shortcut-mitigating methods like Just Train Twice (JTT)
[Liu et al., 2021]. We view the ﬁndings of [Idrissi et al., 2022] through the lens of MARG-CTRL and
explain how large LR and WD approximate MARG-CTRL to mitigate shortcuts; see section 5."
RELATED WORK,0.10657370517928287,"MARG-CTRL is related to but different from methods proposed in Liu et al. [2017], Cao et al. [2019],
Kini et al. [2021]. These works normalize representations or the last linear layers and linearly
transform the logits to learn models with better margins under label imbalance. Next, methods like
Learning from Failure (LFF) [Nam et al., 2020], JTT [Liu et al., 2021], and Correct-n-Contrast (CNC)
[Zhang et al., 2022] build two-stage procedures to avoid shortcut learning without group annotations
in training. They assume that default-ERM produces models that depend more on the shortcut and
select hyperparamters of the two stage process using validation group annotations. In the nuisance-
free setting where there are no validation group annotations, the performance of these methods can
degrade below that of default-ERM. In contrast, better characterizing the source of shortcut learning
in perceptual problems leads to MARG-CTRL methods that are not as reliant on validation group
annotations (see nuisance-free results in Section 5). Without any group annotations, encouraging
uniform margins via MARG-CTRL mitigates shortcuts better than JTT and CNC."
RELATED WORK,0.10756972111553785,"Soudry et al. [2018] characterize the inductive bias of gradient descent to converge in direction to
max-margin solutions when using exponentially tailed loses; Wang et al. [2021, 2022] then prove
similar biases toward max-margin solutions for Adam and RMSProp. Ji et al. [2020] show that
for general losses that decrease in yfθ(x), gradient descent has an inductive bias to follow the ℓ2-
regularization path. All these inductive biases prefer shortcuts if using them leads to lower loss
within an ℓ2-norm-budget. MARG-CTRL provides a different inductive bias toward producing the
same margin on all samples, which means gradient descent veers models away from imperfect short-
cuts that lead to disparity in network outputs. Such inductive biases are suitable for tasks where a
feature determines the label (h(x) = y)."
VISION AND LANGUAGE EXPERIMENTS,0.10856573705179283,"5
Vision and language experiments"
VISION AND LANGUAGE EXPERIMENTS,0.10956175298804781,"We evaluate MARG-CTRL on common datasets with shortcuts: Waterbirds, CelebA [Sagawa et al.,
2020a], and Civilcomments [Koh et al., 2021]. First, MARG-CTRL always improves over default-
ERM.
Then, we show that MARG-CTRL performs similar to or better than two-stage shortcut-"
VISION AND LANGUAGE EXPERIMENTS,0.11055776892430279,"mitigating methods like JTT [Liu et al., 2021] and CNC [Zhang et al., 2022] in traditional evaluation
settings where group annotations are available in the validation data. Finally, we introduce a more
challenging setting that only provides class labels in training and validation, called the nuisance-
free setting. In contrast to the traditional setting that always assumes validation group annotations,
the nuisance-free setting does not provide group annotations in either training or in validation. In the
nuisance-free setting, MARG-CTRL outperforms JTT and CNC, even though the latter are supposed
to mitigate shortcuts without knowledge of the groups."
VISION AND LANGUAGE EXPERIMENTS,0.11155378486055777,"Datasets.
We use the Waterbirds and CelebA datasets from Sagawa et al. [2020a] and the Civil-
Comments dataset from Borkan et al. [2019], Koh et al. [2021]. In Waterbirds, the task is to classify
images of a waterbird or landbird, and the label is spuriously correlated with the image background
consisting of land or water. There are two types of birds and two types of background, leading to a
total of 4 groups deﬁned by values of y, z. In CelebA [Liu et al., 2015, Sagawa et al., 2020a], the
task is to classify hair color of celebrities as blond or not. The gender of the celebrity is a shortcut
for hair color. There are two types of hair color and two genders in this dataset, leading to a total of 4
groups deﬁned by values of y, z. In CivilComments-WILDS [Borkan et al., 2019, Koh et al., 2021],
the task is to classify whether an online comment is toxic or non-toxic, and the label is spuriously
correlated with mentions of certain demographic identities. There are 2 labels and 8 types of the
shortcut features, leading to 16 groups."
VISION AND LANGUAGE EXPERIMENTS,0.11254980079681275,"CelebA
WB
Civil"
VISION AND LANGUAGE EXPERIMENTS,0.11354581673306773,"ERM
72.8 ± 9.4
70.8 ± 2.4
60.1 ± 0.4
CNC
81.1 ± 0.6
68.0 ± 1.8
68.8 ± 0.2
JTT
75.2 ± 4.6
71.7 ± 4.0
69.9 ± 0.4"
VISION AND LANGUAGE EXPERIMENTS,0.1145418326693227,"marg-log
82.8 ± 1.1
78.2 ± 1.9
68.4 ± 1.8
σ-damp
79.4 ± 0.6
78.6 ± 1.1
69.6 ± 0.4
SD
81.4 ± 2.5
80.5 ± 1.4
69.9 ± 1.1
σ-stitch
81.1 ± 2.2
75.9 ± 3.4
67.8 ± 2.8"
VISION AND LANGUAGE EXPERIMENTS,0.11553784860557768,"Table 1: Mean and standard deviation of test worst-
group accuracies over two seeds for default-ERM, JTT,
CNC, σ-damp, σ-stitch, SD, and marg-log. Every MARG-
CTRL method outperforms default-ERM on every dataset.
On Waterbirds, MARG-CTRL outperforms JTT and CNC.
On CelebA, SD, marg-log, and σ-stitch beat JTT and
achieve similar or better performance than CNC. On Civil-
Comments, σ-damp and SD beat CNC and achieve similar
performance to JTT."
VISION AND LANGUAGE EXPERIMENTS,0.11653386454183266,"Metrics, model selection, and hyperpa-
rameters.
We report the worst-group test
accuracy for each method. The groups are
deﬁned based on the labels and shortcut fea-
tures.
The more a model depends on the
shortcut, the worse the worst-group error.
Due to the label imbalance in all the datasets,
we use variants of σ-damp, σ-stitch, MARG-
LOG, and SD with class-dependent hyperpa-
rameters; see appendix B.6.2. For all meth-
ods, we use the standard Adam optimizer
[Kingma and Ba, 2015] and let the learn-
ing rate and weight decay hyperparameters be
tuned along with the method’s hyperparame-
ters. We ﬁrst report results for all methods
using validation worst-group accuracy to se-
lect method and optimization hyperparame-
ters and early stop. For both JTT and CNC,
this is the evaluation setting that is used in
existing work [Liu et al., 2021, Idrissi et al., 2022, Zhang et al., 2022]. Finally, in the nuisance-free
setting where no group annotations are available, we select hyperparameters using label-balanced
average accuracy. Appendix B.6 gives further details about the training, hyperparameters, and ex-
perimental results."
MARG-CTRL MITIGATES SHORTCUTS IN THE DEFAULT SETTING,0.11752988047808766,"5.1
MARG-CTRL mitigates shortcuts in the default setting"
MARG-CTRL MITIGATES SHORTCUTS IN THE DEFAULT SETTING,0.11852589641434264,"Here, we experiment in the standard setting from Liu et al. [2021], Idrissi et al. [2022], Zhang et al.
[2022] and use validation group annotations to tune hyperparameters and early-stopping."
MARG-CTRL MITIGATES SHORTCUTS IN THE DEFAULT SETTING,0.11952191235059761,"MARG-CTRL improves over default-ERM.
We compare MARG-CTRL to default-ERM on
CelebA, Waterbirds, and Civilcomments. Table 1 shows that every MARG-CTRL method achieves
higher test worst-group accuracy than default-ERM on all datasets. Default-ERM achieves a mean
test worst-group accuracy of 70.8%, 72.8% and 60.1% on Waterbirds, CelebA, and Civilcomments
respectively. Compared to default-ERM, MARG-CTRL methods provide a 5 −10% improvement on
Waterbirds, 7 −10% improvement on CelebA, 7 −10% improvement on Civilcomments. These
improvements show the value of inductive biases more suitable for perception tasks."
MARG-CTRL MITIGATES SHORTCUTS IN THE DEFAULT SETTING,0.1205179282868526,"Large LR and WD may imitate MARG-CTRL in ERM.
Default-ERM’s performance varies greatly
across different values of LR and WD on, for instance, CelebA: the test worst-group accuracy im-
proves by more than 20 points over different LR and WD combinations. Why does tuning LR and WD
yield such improvements? We explain this phenomenon as a consequence of instability in optimiza-"
MARG-CTRL MITIGATES SHORTCUTS IN THE DEFAULT SETTING,0.12151394422310757,"Figure 3: Loss curves of default-ERM on CelebA for two combinations of LR and WD. The combination
with the larger learning rate (blue) achieves 72.8% test worst-group accuracy, beating the other combination by
20%. The model that achieves the best validation (and test) worst-group accuracy is the one at epoch 13 from
the blue run. This model achieves similar loss in both groups and the full data model suggesting that large LR
and WD controls margins from exploding (higher training loss in all panels) and avoids systematically smaller
margins in the leftover group compared to the shortcut group."
MARG-CTRL MITIGATES SHORTCUTS IN THE DEFAULT SETTING,0.12250996015936255,"tion induced by large LR and WD which prevents the model from maximizing margins and in turn
can control margins. Figure 3 provides evidence for this explanation by comparing default-ERM’s
loss curves for two LR and WD combinations."
MARG-CTRL MITIGATES SHORTCUTS IN THE DEFAULT SETTING,0.12350597609561753,"The blue curves in ﬁg. 3 correspond to the run with the larger LR and WD combination. The model
that achieves the best validation (and test) worst-group accuracy over all combinations of hyperpa-
rameters for default-ERM, including those not in the plot, is the one at epoch 13 on the blue curves.
This model achieves similar train and test losses (≈0.4) and thus similar margins in the shortcut
group, the leftover group, and the whole dataset. The red curves stand in contrast where the lower
LR results in the leftover group having higher training and test losses, and therefore smaller margins,
compared to the shortcut group. These observations together support the explanation that default-
ERM with large LR and WD mitigates shortcuts when controlling margins like MARG-CTRL."
MARG-CTRL MITIGATES SHORTCUTS IN THE DEFAULT SETTING,0.12450199203187251,"MARG-CTRL performs as well or better than two-stage shortcut-mitigating methods.
Two-
stage shortcut mitigating methods like Correct-n-Contrast (CNC) and Just Train Twice (JTT) aim to
mitigate shortcuts by using a model trained with default-ERM to approximate group annotations.
They rely on the assumption that a model trained via default-ERM either predicts with the shortcut
feature (like background in Waterbirds) or that the model’s representations separate into clusters
based on the shortcut feature. The methods then approximate group annotations using this default-
ERM-trained model and use them to mitigate shortcut learning in a second predictive model. JTT
upweights the loss on the approximate leftover group and CNC uses a contrastive loss to enforce the
model’s representations to be similar across samples that have the same label but different approxi-
mate group annotations. Appendix B.6.1 gives details."
MARG-CTRL MITIGATES SHORTCUTS IN THE DEFAULT SETTING,0.1254980079681275,"Table 1 compares MARG-CTRL to JTT and CNC on Waterbirds, Celeba, and CivilComments. On
CelebA, SD, marg-log, and σ-stitch perform similar to CNC while all MARG-CTRL techniques
outperform JTT. On Waterbirds, all MARG-CTRL methods outperform JTT and CNC. On CivilCom-
ments, σ-damp and SD perform similar to JTT and outperform CNC. CNC’s performance on Water-
birds differs from Zhang et al. [2022] because their reported performance requires unique large WD
choices (like WD set to 1) to build a ﬁrst-stage model that relies most on the shortcut feature without
overﬁtting to the training data."
MARG-CTRL MITIGATES SHORTCUTS IN THE DEFAULT SETTING,0.12649402390438247,"MARG-CTRL is faster than JTT and CNC.
MARG-CTRL takes the same time as default-ERM,
taking around 1, 20 and 60 minutes per epoch for Waterbirds, CelebA, and CivilComments respec-
tively on an RTX8000 GPU. In contrast, on average over runs, JTT takes around 6, 80, 120 minutes
per epoch and CNC takes around 8, 180, 360 minutes per epoch. Thus, MARG-CTRL performs as well
or better than JTT and CNC while being simpler to implement and computationally cheaper."
MARG-CTRL MITIGATES SHORTCUTS IN THE NUISANCE-FREE SETTING,0.12749003984063745,"5.2
MARG-CTRL mitigates shortcuts in the nuisance-free setting"
MARG-CTRL MITIGATES SHORTCUTS IN THE NUISANCE-FREE SETTING,0.12848605577689243,"Work like [Liu et al., 2021, Zhang et al., 2022] crucially require validation group annotations because
these methods push the work of selecting models for mitigating shortcuts to validation. Determining
shortcuts itself is a laborious manual process, which means group annotations will often be unavail-
able. Further, given a perfect stable feature that determines the label and a shortcut that does not,
only models that rely on the stable feature more than the shortcut achieve the highest validation ac-"
MARG-CTRL MITIGATES SHORTCUTS IN THE NUISANCE-FREE SETTING,0.1294820717131474,"curacy. Thus, we introduce a more challenging setting that only provides class labels in training and
validation, called the nuisance-free setting. In the nuisance-free setting, models are selected based
on label-balanced average accuracy: the average of the accuracies over samples of each class."
MARG-CTRL MITIGATES SHORTCUTS IN THE NUISANCE-FREE SETTING,0.13047808764940239,"CelebA
WB
Civil"
MARG-CTRL MITIGATES SHORTCUTS IN THE NUISANCE-FREE SETTING,0.13147410358565736,"ERM
57.5 ± 5.8
69.1 ± 2.1
60.7 ± 1.5
CNC
67.8 ± 0.6
60.0 ± 8.0
61.4 ± 1.9
JTT
53.3 ± 3.3
71.7 ± 4.0
53.4 ± 2.1"
MARG-CTRL MITIGATES SHORTCUTS IN THE NUISANCE-FREE SETTING,0.13247011952191234,"marg-log
74.2 ± 1.4
77.9 ± 0.3
66.8 ± 0.2
σ-damp
70.8 ± 0.3
74.8 ± 1.6
65.6 ± 0.2
SD
70.3 ± 0.3
78.7 ± 1.4
67.8 ± 1.3
σ-stitch
76.7 ± 0.6
74.5 ± 1.2
66.0 ± 1.0"
MARG-CTRL MITIGATES SHORTCUTS IN THE NUISANCE-FREE SETTING,0.13346613545816732,"Table 2: Average and standard deviation of test worst-
group accuracy over two seeds of MARG-CTRL, default-
ERM, JTT, and CNC in the nuisance-free setting. Hyper-
parameter selection and early stopping use label-balanced
average accuracy. All MARG-CTRL methods outperform
default-ERM, JTT, and CNC on all datasets."
MARG-CTRL MITIGATES SHORTCUTS IN THE NUISANCE-FREE SETTING,0.1344621513944223,"Table 2 reports test worst-group (WG) ac-
curacy in the nuisance-free setting. On all
the datasets, every MARG-CTRL outper-
forms default-ERM, JTT, and CNC.
On
average, the MARG-CTRL methods close at
least 61% of the gap between default-ERM
in the nuisance-free setting and the best per-
formance in table 1 on every dataset.
In
contrast, CNC and JTT sometimes perform
worse than default-ERM."
DISCUSSION,0.13545816733067728,"6
Discussion"
DISCUSSION,0.13645418326693226,"We study why default-ERM — gradient-based
optimization of log-loss — yields models that
depend on the shortcut even when the population minimum of log-loss is achieved by models that
depend only on the stable feature. By studying a linear task with perfect stable features, we show
that default-ERM’s preference toward shortcuts sprouts from an inductive bias toward maximizing
margins. Instead, inductive biases toward uniform margins improve dependence on the stable feature
and can be implemented via MARG-CTRL. MARG-CTRL improves over default-ERM on a variety
of perception tasks in vision and language without group annotations in training, and is competitive
with more expensive two-stage shortcut-mitigating methods. In the nuisance-free setting, where
even validation group annotations are unavailable, MARG-CTRL outperforms all the baselines. The
performance that MARG-CTRL yields demonstrates that changing inductive biases can remove the
need for expensive shortcut-mitigating methods in perception tasks."
DISCUSSION,0.13745019920318724,"Without overparameterization, uniform-margin classiﬁers are unique and learn stable features only,
while max-margin classiﬁers can depend more on shortcuts.
With overparameterization, max-
margin classiﬁers are still unique but uniform-margin solutions are not which necessitates choosing
between solutions. The experiments in section 5 suggest that choosing between uniform-margin
classiﬁers with penalties like ℓ2 improves over max-margin classiﬁers with ℓ2: all experiments use
overparameterized models trained with weight decay and MARG-CTRL outperforms default-ERM.
Further, our experiments suggest that uniform-margin classiﬁers are insensitive to the WD and LR
choices, unlike max-margin classiﬁers; appendix B.8 shows that MARG-CTRL achieves high perfor-
mance for all LR and WD choices but ERM requires tuning."
DISCUSSION,0.13844621513944222,"Theorem 1 also explains how balancing may or may not improve dependence on the stable features.
For example, a weighting-based approach produces the same max-margin solution as default-ERM
[Sagawa et al., 2020b, Rosset et al., 2003], but subsampling leads to a different solution that could
depend less on the shortcut. For the latter however, models are more prone to overﬁtting on the
smaller subsampled dataset. Similar observations were made in Sagawa et al. [2020b] but this
work extends the insight to tasks with perfect stable features. Comparing ERM and MARG-CTRL on
subsampled data would be fruitful."
DISCUSSION,0.1394422310756972,"Any exponentially tailed loss when minimized via gradient descent converges to the max-margin
solution in direction [Soudry et al., 2018]. Thus, theorem 1 characterizes shortcut learning for
any exponentially-tailed loss. However, losses with decreasing polynomial tails — for example,
ℓ(a) =
1
1+aK for some K > 0 — do not converge to the max-margin classiﬁer. One future direction
is to show shortcut-dependence results like theorem 1 for polynomial-tailed losses, which in turn
would mean that all common classiﬁcation losses with a decreasing tail impose inductive biases
unsuitable for perception tasks."
DISCUSSION,0.14043824701195218,"In the tasks we consider with perfect stable features, Bayes-optimal predictors rely only on the stable
feature. A weaker independence condition implies the same property of Bayes-optimal predictors
even when y is not determined by s(x): y |="
DISCUSSION,0.14143426294820718,"(x, z) | s(x). For example, in the CivilComments
dataset a few instances have ambiguous labels [Xenos et al., 2022] meaning that there may not be a
perfect stable feature. Studying uniform margins and other inductive biases under this independence
would be fruitful."
DISCUSSION,0.14243027888446216,"Acknowledgements.
This work was supported by the NIH/NHLBI Award R01HL148248, NSF
Award 1922658 NRT-HDR: FUTURE Foundations, Translation, and Responsibility for Data Sci-
ence, NSF CAREER Award 2145542, ONR N00014-23-1-2634, Google. Aahlad Puli is partly
supported by the Apple Scholars in AI/ML PhD fellowship."
REFERENCES,0.14342629482071714,References
REFERENCES,0.14442231075697212,"Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel,"
REFERENCES,0.1454183266932271,"Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature
Machine Intelligence, 2(11):665–673, 2020."
REFERENCES,0.14641434262948208,"Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal-"
REFERENCES,0.14741035856573706,"subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee,
Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure
Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang.
Wilds: A benchmark of in-the-wild distribution shifts. In Marina Meila and Tong Zhang, ed-
itors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of
Proceedings of Machine Learning Research, pages 5637–5664. PMLR, 18–24 Jul 2021. URL
https://proceedings.mlr.press/v139/koh21a.html."
REFERENCES,0.14840637450199204,"Aahlad Manas Puli, Lily H Zhang, Eric Karl Oermann, and Rajesh Ranganath. Out-of-distribution"
REFERENCES,0.14940239043824702,"generalization in the presence of nuisance-induced spurious correlations. In International Con-
ference on Learning Representations, 2022.
URL https://openreview.net/forum?id=
12RoR2o32T."
REFERENCES,0.150398406374502,"Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization."
REFERENCES,0.15139442231075698,"arXiv preprint arXiv:1907.02893, 2019."
REFERENCES,0.15239043824701196,"Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust"
REFERENCES,0.15338645418326693,"neural networks. In International Conference on Learning Representations, 2020a. URL https:
//openreview.net/forum?id=ryxGuJrFvS."
REFERENCES,0.15438247011952191,"Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa,"
REFERENCES,0.1553784860557769,"Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training
group information. In International Conference on Machine Learning, pages 6781–6792. PMLR,
2021."
REFERENCES,0.15637450199203187,"Michael Zhang, Nimit S Sohoni, Hongyang R Zhang, Chelsea Finn, and Christopher Re. Correct-n-"
REFERENCES,0.15737051792828685,"contrast: a contrastive approach for improving robustness to spurious correlations. In Kamalika
Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors,
Proceedings of the 39th International Conference on Machine Learning, volume 162 of Pro-
ceedings of Machine Learning Research, pages 26484–26516. PMLR, 17–23 Jul 2022. URL
https://proceedings.mlr.press/v162/zhang22z.html."
REFERENCES,0.15836653386454183,"Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-"
REFERENCES,0.1593625498007968,"plicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19
(1):2822–2878, 2018."
REFERENCES,0.1603585657370518,"Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data"
REFERENCES,0.16135458167330677,"balancing achieves competitive worst-group-accuracy. In Bernhard Schölkopf, Caroline Uhler,
and Kun Zhang, editors, Proceedings of the First Conference on Causal Learning and Reasoning,
volume 177 of Proceedings of Machine Learning Research, pages 336–351. PMLR, 11–13 Apr
2022. URL https://proceedings.mlr.press/v177/idrissi22a.html."
REFERENCES,0.16235059760956175,"Bohan Wang, Qi Meng, Wei Chen, and Tie-Yan Liu. The implicit bias for adaptive optimization"
REFERENCES,0.16334661354581673,"algorithms on homogeneous neural networks. In International Conference on Machine Learning,
pages 10849–10858. PMLR, 2021."
REFERENCES,0.1643426294820717,"Bohan Wang, Qi Meng, Huishuai Zhang, Ruoyu Sun, Wei Chen, Zhi-Ming Ma, and Tie-Yan Liu."
REFERENCES,0.16533864541832669,"Does momentum change the implicit regularization on separable data?
Advances in Neural
Information Processing Systems, 35:26764–26776, 2022."
REFERENCES,0.16633466135458166,"Vaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur.
Understanding the failure
modes of out-of-distribution generalization.
In International Conference on Learning Repre-
sentations, 2021. URL https://openreview.net/forum?id=fSTD6NFIW_b."
REFERENCES,0.16733067729083664,"Yoav Wald, Gal Yona, Uri Shalit, and Yair Carmon. Malign overﬁtting: Interpolation and invariance"
REFERENCES,0.16832669322709162,"are fundamentally at odds. In The Eleventh International Conference on Learning Representa-
tions, 2023. URL https://openreview.net/forum?id=dQNL7Zsta3."
REFERENCES,0.1693227091633466,"Mohammad Pezeshki, Sékou-Oumar Kaba, Yoshua Bengio, Aaron Courville, Doina Precup, and"
REFERENCES,0.17031872509960158,"Guillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. In Thirty-Fifth
Conference on Neural Information Processing Systems, 2021."
REFERENCES,0.17131474103585656,"Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit. On calibration and out-of-domain gener-"
REFERENCES,0.17231075697211157,"alization. arXiv preprint arXiv:2102.10395, 2021."
REFERENCES,0.17330677290836655,"David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai"
REFERENCES,0.17430278884462153,"Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapo-
lation (rex). arXiv preprint arXiv:2003.00688, 2020."
REFERENCES,0.1752988047808765,"Elliot Creager, Jörn-Henrik Jacobsen, and Richard Zemel.
Environment inference for invariant
learning. In International Conference on Machine Learning, pages 2189–2200. PMLR, 2021."
REFERENCES,0.17629482071713148,"Victor Veitch, Alexander D’Amour, Steve Yadlowsky, and Jacob Eisenstein. Counterfactual invari-"
REFERENCES,0.17729083665338646,"ance to spurious correlations: Why and how to pass stress tests. arXiv preprint arXiv:2106.00545,
2021."
REFERENCES,0.17828685258964144,Christina Heinze-Deml and Nicolai Meinshausen. Conditional variance penalties and domain shift
REFERENCES,0.17928286852589642,"robustness. Machine Learning, 110(2):303–348, 2021."
REFERENCES,0.1802788844621514,Yonatan Belinkov and Yonatan Bisk. Synthetic and natural noise both break neural machine trans-
REFERENCES,0.18127490039840638,"lation. arXiv preprint arXiv:1711.02173, 2017."
REFERENCES,0.18227091633466136,Greg Yang and Hadi Salman. A ﬁne-grained spectral perspective on neural networks. arXiv preprint
REFERENCES,0.18326693227091634,"arXiv:1907.10599, 2019."
REFERENCES,0.18426294820717132,"Basri Ronen, David Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of neural net-"
REFERENCES,0.1852589641434263,"works for learned functions of different frequencies. Advances in Neural Information Processing
Systems, 32, 2019."
REFERENCES,0.18625498007968128,Jason Jo and Yoshua Bengio. Measuring the tendency of cnns to learn surface statistical regularities.
REFERENCES,0.18725099601593626,"arXiv preprint arXiv:1711.11561, 2017."
REFERENCES,0.18824701195219123,"Nicholas Baker, Hongjing Lu, Gennady Erlikhman, and Philip J Kellman.
Deep convolutional
networks do not classify based on global object shape. PLoS computational biology, 14(12):
e1006613, 2018."
REFERENCES,0.1892430278884462,"Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-"
REFERENCES,0.1902390438247012,"ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013."
REFERENCES,0.19123505976095617,"Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient"
REFERENCES,0.19223107569721115,"dynamics in linear neural networks. Advances in Neural Information Processing Systems, 32,
2019."
REFERENCES,0.19322709163346613,"Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of gener-"
REFERENCES,0.1942231075697211,"alization error in neural networks. Neural Networks, 132:428–446, 2020."
REFERENCES,0.1952191235059761,"Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The"
REFERENCES,0.19621513944223107,"pitfalls of simplicity bias in neural networks. arXiv preprint arXiv:2006.07710, 2020."
REFERENCES,0.19721115537848605,"Guillermo Valle-Perez, Chico Q Camargo, and Ard A Louis. Deep learning generalizes because the"
REFERENCES,0.19820717131474103,"parameter-function map is biased towards simple functions. arXiv preprint arXiv:1805.08522,
2018."
REFERENCES,0.199203187250996,Katherine L Hermann and Andrew K Lampinen. What shapes feature representations? exploring
REFERENCES,0.20019920318725098,"datasets, architectures, and training. arXiv preprint arXiv:2006.12433, 2020."
REFERENCES,0.20119521912350596,"Luca Scimeca, Seong Joon Oh, Sanghyuk Chun, Michael Poli, and Sangdoo Yun. Which short-"
REFERENCES,0.20219123505976094,"cut cues will dnns choose?
a study from the parameter-space perspective.
arXiv preprint
arXiv:2110.03095, 2021."
REFERENCES,0.20318725099601595,"Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio,"
REFERENCES,0.20418326693227093,"Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-
distribution generalization. Advances in Neural Information Processing Systems, 34:3438–3450,
2021."
REFERENCES,0.2051792828685259,"Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang.
An investigation of why
overparameterization exacerbates spurious correlations. In International Conference on Machine
Learning, pages 8346–8356. PMLR, 2020b."
REFERENCES,0.2061752988047809,"Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large"
REFERENCES,0.20717131474103587,"learning rate in training neural networks. Advances in Neural Information Processing Systems,
32, 2019."
REFERENCES,0.20816733067729085,"Mohammad Pezeshki, Amartya Mitra, Yoshua Bengio, and Guillaume Lajoie. Multi-scale feature"
REFERENCES,0.20916334661354583,"learning dynamics: Insights for double descent. In International Conference on Machine Learn-
ing, pages 17669–17690. PMLR, 2022."
REFERENCES,0.2101593625498008,"Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep"
REFERENCES,0.21115537848605578,"hypersphere embedding for face recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 212–220, 2017."
REFERENCES,0.21215139442231076,"Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced"
REFERENCES,0.21314741035856574,"datasets with label-distribution-aware margin loss. Advances in neural information processing
systems, 32, 2019."
REFERENCES,0.21414342629482072,"Ganesh Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos Thrampoulidis. Label-"
REFERENCES,0.2151394422310757,"imbalanced and group-sensitive classiﬁcation under overparameterization. Advances in Neural
Information Processing Systems, 34:18970–18983, 2021."
REFERENCES,0.21613545816733068,"Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure:"
REFERENCES,0.21713147410358566,"De-biasing classiﬁer from biased classiﬁer. Advances in Neural Information Processing Systems,
33:20673–20684, 2020."
REFERENCES,0.21812749003984064,"Ziwei Ji, Miroslav Dudík, Robert E Schapire, and Matus Telgarsky. Gradient descent follows the"
REFERENCES,0.21912350597609562,"regularization path for general losses. In Conference on Learning Theory, pages 2109–2136.
PMLR, 2020."
REFERENCES,0.2201195219123506,"Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman.
Nuanced
metrics for measuring unintended bias with real data for text classiﬁcation. In Companion pro-
ceedings of the 2019 world wide web conference, pages 491–500, 2019."
REFERENCES,0.22111553784860558,"Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild."
REFERENCES,0.22211155378486055,"In Proceedings of the IEEE international conference on computer vision, pages 3730–3738, 2015."
REFERENCES,0.22310756972111553,"Diederik P. Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
In Yoshua
Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL
http://arxiv.org/abs/1412.6980."
REFERENCES,0.2241035856573705,"Saharon Rosset, Ji Zhu, and Trevor Hastie. Margin maximizing loss functions. Advances in neural"
REFERENCES,0.2250996015936255,"information processing systems, 16, 2003."
REFERENCES,0.22609561752988047,"Alexandros Xenos, John Pavlopoulos, Ion Androutsopoulos, Lucas Dixon, Jeffrey Sorensen, and"
REFERENCES,0.22709163346613545,"Léo Laugier. Toxicity detection sensitive to conversational context. First Monday, 2022."
REFERENCES,0.22808764940239043,"Roman Vershynin. High-dimensional probability: An introduction with applications in data science,"
REFERENCES,0.2290836653386454,"volume 47. Cambridge university press, 2018."
REFERENCES,0.2300796812749004,Xinlong Feng and Zhinan Zhang. The rank of a random matrix. Applied mathematics and compu-
REFERENCES,0.23107569721115537,"tation, 185(1):689–694, 2007."
REFERENCES,0.23207171314741035,"Ilya Loshchilov and Frank Hutter.
Decoupled weight decay regularization.
In International
Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=
Bkg6RiCqY7."
REFERENCES,0.23306772908366533,"Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher Ré. No subclass left"
REFERENCES,0.2340637450199203,"behind: Fine-grained robustness in coarse-grained classiﬁcation problems. Advances in Neural
Information Processing Systems, 33:19339–19352, 2020."
REFERENCES,0.2350597609561753,Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In International
REFERENCES,0.2360557768924303,"Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=
lQdXeXDoWtI."
REFERENCES,0.23705179282868527,"A
Appendix: Proof of Theorem 1, Corollary 1, and Theorem 2"
REFERENCES,0.23804780876494025,"A.1
Helper Lemmas"
REFERENCES,0.23904382470119523,"A.1.1
Bounding norms and inner products of isotropic random vectors."
REFERENCES,0.2400398406374502,"The main lemmas of this section are lemma 3 and lemma 4. We will then use these two to bound
norms of sums of random vectors and inner products between the sum and a single random vector
in lemma 5. We ﬁrst list some facts from [Vershynin, 2018] that we will use to bound the prob-
ability with which norms and inner products of Gaussian random vectors deviate far from their mean."
REFERENCES,0.2410358565737052,"Deﬁnition 1. (Sub-Gaussian norm) For an r.v. x, the sub-Gaussian norm, or ψ2-norm, is"
REFERENCES,0.24203187250996017,"󰀂x󰀂ψ2 = inf{t > 0, E[exp(x2/t2)] ≤2}."
REFERENCES,0.24302788844621515,An r.v. is called sub-Gaussian if its ψ2-norm is ﬁnite and for some ﬁxed constant c
REFERENCES,0.24402390438247012,p(|x| > t) ≤2 exp(−ct2/󰀂x󰀂ψ2).
REFERENCES,0.2450199203187251,"A Gaussian r.v. x ∼N(0, σ2) has an ψ2-norm of Gσ for a constant G = 󰁴 8
3.2"
REFERENCES,0.24601593625498008,"Deﬁnition 2. (Sub-exponential norm) For an r.v. x, the sub-exponential norm, or ψ1-norm, is"
REFERENCES,0.24701195219123506,"󰀂x󰀂ψ1 = inf{t > 0, E[exp(|x|/t)] ≤2}."
REFERENCES,0.24800796812749004,A sub-exponential r.v. is one that has ﬁnite ψ1-norm.
REFERENCES,0.24900398406374502,"Lemma 1. (Lemma 2.7.7 from [Vershynin, 2018]) Products of sub-Gaussian random variables x, y
is a sub-exponential random variable with it’s ψ1-norm bounded by the product of the ψ2-norm"
REFERENCES,0.25,󰀂xy󰀂ψ1 ≤󰀂x󰀂ψ2󰀂y󰀂ψ2
REFERENCES,0.250996015936255,"Lemma 1 implies that the product of two mean-zero standard normal vectors is a sub-exponential
random variable with ψ1-norm less than G2."
REFERENCES,0.25199203187250996,"Lemma 2. (Bernstein inequality, Theorem 2.8.2 [Vershynin, 2018]) For i.i.d sub-exponential ran-
dom variables x1, · · · , xd, for a ﬁxed constant c =
1
(2e)2 and K = 󰀂x1󰀂ψ1 p"
REFERENCES,0.25298804780876494,"󰀣󰀏󰀏󰀏󰀏󰀏 d
󰁛 i=1 xi"
REFERENCES,0.2539840637450199,󰀏󰀏󰀏󰀏󰀏> t 󰀤
REFERENCES,0.2549800796812749,≤2 exp 󰀕
REFERENCES,0.2559760956175299,−c min 󰀝t2
REFERENCES,0.25697211155378485,"K2d, t K 󰀞󰀖"
REFERENCES,0.25796812749003983,"Next, we apply these facts to bound the sizes of inner products between two unit-variance Gaussian
vectors."
REFERENCES,0.2589641434262948,"Lemma 3. (Bounds on inner products of Gaussian vectors) Let u, v be d-dimensional random
vectors where each coordinate is an i.i.d standard normal r.v. Then, for any scalar 󰂃> 0 such that
󰂃≤G2√"
REFERENCES,0.2599601593625498,"d, for a ﬁxed constant c =
1
(2e)2 p"
REFERENCES,0.26095617529880477,󰀓󰀏󰀏u⊤v 󰀏󰀏> 󰂃 √ d 󰀔
REFERENCES,0.26195219123505975,≤2 exp 󰀕 −c 󰂃2 G4 󰀖 . 2 G = 󰁴
REFERENCES,0.26294820717131473,"8
3. This follows from:"
REFERENCES,0.2639442231075697,"Ex∼N (0,σ2)[exp(x2/t2)] = 󰁝∞ −∞ 1
σ √ 2π"
REFERENCES,0.2649402390438247,"exp(−x2/2σ2) exp(x2/t2)dx = 󰁝∞ −∞ 1
σ √ 2π exp 󰀕"
REFERENCES,0.26593625498007967,"−x2 (t2 −2σ2) 2σ2t2 󰀖 dx =
1
σ √ 2π 󰁶 π"
REFERENCES,0.26693227091633465,(t2−2σ2) 2σ2t2
REFERENCES,0.2679282868525896,"=
1
σ√π 󰁶 πσ2t2"
REFERENCES,0.2689243027888446,(t2 −2σ2) = 󰁶 t2
REFERENCES,0.2699203187250996,"(t2 −2σ2)
󰁶 t2"
REFERENCES,0.27091633466135456,"(t2 −2σ2) ≤2 =⇒t2 ≤4(t2 −2σ2) =⇒8σ2 ≤3t2 =⇒inf{t : 8σ2 ≤3t2} = 󰁵 8
3σ."
REFERENCES,0.27191235059760954,"Proof. First, the inner product is u⊤v = 󰁓d"
REFERENCES,0.2729083665338645,"i uivi; it is the sum of products of i.i.d. standard
normal r.v. (σ = 1). Then, by lemma 1, each term in the sum is a sub-exponential r.v. with ψ1-norm
bounded as follows:"
REFERENCES,0.2739043824701195,"K = 󰀂uivi󰀂ψ1 ≤󰀂ui󰀂ψ2󰀂ui󰀂ψ2 = G × G = G2.
(6)
We can apply Bernstein inequality lemma 2 to sub-exponential r.v. to the inner product and then
upper bound the probability by replacing K with the larger G2 in eq. (6) p 󰀃"
REFERENCES,0.2749003984063745,|u⊤v| > t 󰀄
REFERENCES,0.27589641434262946,≤2 exp 󰀕
REFERENCES,0.27689243027888444,−c min 󰀝t2
REFERENCES,0.2778884462151394,"K2d, t K 󰀞󰀖"
REFERENCES,0.2788844621513944,≤2 exp 󰀕
REFERENCES,0.2798804780876494,−c min 󰀝t2
REFERENCES,0.28087649402390436,"G4d, t G2 󰀞󰀖"
REFERENCES,0.2818725099601594,Substituting t = 󰂃 √
REFERENCES,0.28286852589641437,d in the above gives us: p 󰀓
REFERENCES,0.28386454183266935,|u⊤v| > 󰂃 √ d 󰀔
REFERENCES,0.2848605577689243,≤2 exp 󰀣
REFERENCES,0.2858565737051793,−c min 󰀫
REFERENCES,0.2868525896414343,"󰂃2d
G4d, 󰂃 √ d
G2 󰀬󰀤"
REFERENCES,0.28784860557768926,Using the fact that 󰂃≤G2√
REFERENCES,0.28884462151394424,d to achieve the minimum concludes the proof: 󰂃≤G2√
REFERENCES,0.2898406374501992,d =⇒󰂃2 ≤󰂃G2√
REFERENCES,0.2908366533864542,"d =⇒
󰂃2 G4 ≤󰂃 √"
REFERENCES,0.2918326693227092,"d
G2
=⇒min 󰀫"
REFERENCES,0.29282868525896416,"󰂃2d
G4d, 󰂃 √ d
G2 󰀬 = 󰂃2 G4"
REFERENCES,0.29382470119521914,"Lemma 4. Let x be a Gaussian vector of size d where each element is a standard normal, meaning
that 󰀂xi󰀂ψ2 = G. Then, for any t > 0 and a ﬁxed constant c =
1
(2e)2 , the norm of the vector"
REFERENCES,0.2948207171314741,concentrates around √
REFERENCES,0.2958167330677291,d according to p
REFERENCES,0.2968127490039841,󰀓󰀏󰀏󰀏󰀂x󰀂− √ d
REFERENCES,0.29780876494023906,󰀏󰀏󰀏> t 󰀔
REFERENCES,0.29880478087649404,≤2 exp(−ct2/G4).
REFERENCES,0.299800796812749,"Proof. Equation 3.3 from the proof of theorem 3.1.1 in [Vershynin, 2018] shows that"
REFERENCES,0.300796812749004,p(|󰀂x󰀂− √
REFERENCES,0.301792828685259,d| > t) ≤2 exp(−ct2/(maxi 󰀂xi󰀂ψ2)
REFERENCES,0.30278884462151395,"4).
As x has i.i.d standard normal entries, maxi 󰀂xi󰀂ψ2 = G, concluding the proof."
REFERENCES,0.30378486055776893,"A.1.2
Concentration of norms of sums of random vectors and their inner products"
REFERENCES,0.3047808764940239,This is the main lemma that we will use in proving theorem 1.
REFERENCES,0.3057768924302789,"Lemma 5. Consider a set of vectors V = {δi} where δi ∈Rd of size TV ≥1 where each element
of each vector is drawn independently from the standard normal distribution N(0, 1). Then, for a
ﬁxed constant c =
1
(2e)2 and any 󰂃∈(0, G2√"
REFERENCES,0.30677290836653387,d) with probability ≥1 −2 exp(−󰂃2 c
REFERENCES,0.30776892430278885,"G4 )
󰀐󰀐󰀐󰀐󰀐 1
√TV 󰁛 i∈V δi"
REFERENCES,0.30876494023904383,󰀐󰀐󰀐󰀐󰀐≤ √
REFERENCES,0.3097609561752988,"d + 󰂃
(7)"
REFERENCES,0.3107569721115538,and with probability ≥1 −4TV exp(−󰂃2 c G4 )
REFERENCES,0.31175298804780877,"∀δj ∈V 󰀭 δj, 󰁛 i∈V δi 󰀮"
REFERENCES,0.31274900398406374,≥d −3󰂃 󰁳
REFERENCES,0.3137450199203187,"TV d
(8)"
REFERENCES,0.3147410358565737,"Further, consider any set U of vectors U = {δi} of size TU, where each vector also has co-
ordinates drawn i.i.d from the standard normal distribution N(0, 1).
Then, with probability
≥1 −2Tu exp(−󰂃2 c G4 )"
REFERENCES,0.3157370517928287,"∀δj ∈U 󰀏󰀏󰀏󰀏󰀏 󰀭 δj, 󰁛 i∈V δi"
REFERENCES,0.31673306772908366,󰀮󰀏󰀏󰀏󰀏󰀏≤󰂃 󰁳
REFERENCES,0.31772908366533864,"TV d,
(9)"
REFERENCES,0.3187250996015936,"By union bound, the three events above hold at once with a probability at least 1 −2(2TV + Tu +
1) exp(−󰂃2 c G4 )."
REFERENCES,0.3197211155378486,Proof. We split the proof into three parts one each for eqs. (7) to (9).
REFERENCES,0.3207171314741036,"Proof of eq. (7).
As δ is a vector of random i.i.d standard normal random variables, note that
1
√TV 󰁓"
REFERENCES,0.32171314741035856,"i δi is also a vector of i.i.d standard normal random variables. This follows from the fact
that the sum of TV standard normal random variables is a mean-zero Gaussian random variable with
standard deviation √TV . Thus dividing by the standard deviation makes the variance 1, making it
standard normal."
REFERENCES,0.32270916334661354,"Then, applying lemma 4 with t = 󰂃gives us the following bound: p"
REFERENCES,0.3237051792828685,"󰀣󰀐󰀐󰀐󰀐󰀐 1
√TV 󰁛 i δi"
REFERENCES,0.3247011952191235,󰀐󰀐󰀐󰀐󰀐> √ d + 󰂃 󰀤 ≤p
REFERENCES,0.3256972111553785,"󰀣󰀏󰀏󰀏󰀏󰀏 󰀐󰀐󰀐󰀐󰀐 1
√TV 󰁛 i δi"
REFERENCES,0.32669322709163345,󰀐󰀐󰀐󰀐󰀐− √ d
REFERENCES,0.32768924302788843,󰀏󰀏󰀏󰀏󰀏> 󰂃 󰀤
REFERENCES,0.3286852589641434,≤2 exp(−c󰂃2/G4)
REFERENCES,0.3296812749003984,"Proof of eq. (8)
We split the inner product into two cases: TV = 1 and TV ≥2."
REFERENCES,0.33067729083665337,"Case TV = 1.
First note that due to lemma 4,"
REFERENCES,0.33167330677290835,"∀j ∈V,
p 󰀓 󰀂δj󰀂< √ d −󰂃 󰀔 ≤p"
REFERENCES,0.33266932270916333,󰀓󰀏󰀏󰀏󰀂δj󰀂− √ d
REFERENCES,0.3336653386454183,󰀏󰀏󰀏> 󰂃 󰀔
REFERENCES,0.3346613545816733,≤2 exp(−c󰂃2/G4).
REFERENCES,0.33565737051792827,"Then, the following lower bound holds with probability at least 1 −2 exp(−c󰂃2/G4)"
REFERENCES,0.33665338645418325,"∀j ∈V, 󰀭 δj, 󰁛 i∈V δi 󰀮"
REFERENCES,0.3376494023904382,= 󰀂δj󰀂2 ≥( √
REFERENCES,0.3386454183266932,d −󰂃)2
REFERENCES,0.3396414342629482,≥d −2󰂃 √ d
REFERENCES,0.34063745019920316,"≥d −3󰂃 󰁳 TV d,"
REFERENCES,0.34163346613545814,"To summarize this case, with the fact that 1 −2 exp(−c󰂃2/G4) ≥1 −4TV exp(−c󰂃2/G4), we have
that"
REFERENCES,0.3426294820717131,"∀j ∈V, 󰀭 δj, 󰁛 i∈V δi 󰀮"
REFERENCES,0.3436254980079681,"≥d −3󰂃 󰁳 TV d,"
REFERENCES,0.34462151394422313,with probability at least 1 −4TV exp(−c󰂃2/G4).
REFERENCES,0.3456175298804781,"Case TV ≥2.
First note that, ∀j ∈V 󰀭 δj, 󰁛 i∈V δi 󰀮"
REFERENCES,0.3466135458167331,"= 󰀂δj󰀂2 + 󰀭 δj, 󰁛"
REFERENCES,0.34760956175298807,"i∈V,i∕=j δi 󰀮"
REFERENCES,0.34860557768924305,"For each of the TV different δj’s, using lemma 4 bounds the probability of the norm 󰀂δj󰀂being
larger than √ d −󰂃: p 󰀓 󰀂δj󰀂< √ d −󰂃 󰀔 ≤p"
REFERENCES,0.34960159362549803,󰀓󰀏󰀏󰀏󰀂δj󰀂− √ d
REFERENCES,0.350597609561753,󰀏󰀏󰀏> 󰂃 󰀔
REFERENCES,0.351593625498008,≤2 exp(−c󰂃2/G4).
REFERENCES,0.35258964143426297,"In the case where TV ≥2, we express the inner product of a vector and a sum of vectors as follows
󰀭 δj, 󰁛"
REFERENCES,0.35358565737051795,"i∈V,i∕=j δi 󰀮 = 󰁳 TV −1 󰀭"
REFERENCES,0.3545816733067729,"δj,
1
√TV −1 󰁛"
REFERENCES,0.3555776892430279,"i∈V,i∕=j δi 󰀮 ,"
REFERENCES,0.3565737051792829,"and noting that like above,
1
√TV −1 󰁓"
REFERENCES,0.35756972111553786,"i∈V,i∕=j δi is a vector of standard normal random variables, we
apply lemma 3 to get"
REFERENCES,0.35856573705179284,"∀i ∈V
p 󰀳 󰁃"
REFERENCES,0.3595617529880478,"󰀏󰀏󰀏󰀏󰀏󰀏 󰀭 δj, 󰁛"
REFERENCES,0.3605577689243028,"i∈V,i∕=j δi"
REFERENCES,0.3615537848605578,󰀮󰀏󰀏󰀏󰀏󰀏󰀏 ≥󰂃 󰁳
REFERENCES,0.36254980079681276,(TV −1)d 󰀴
REFERENCES,0.36354581673306774,󰁄≤2 exp 󰀕 −c 󰂃2 G4 󰀖 .
REFERENCES,0.3645418326693227,"Putting these together, by union bound over V p 󰀵"
REFERENCES,0.3655378486055777,󰀷∀j ∈V 󰀣 󰀂δj󰀂< √ d −󰂃 󰀤 or 󰀳 󰁃
REFERENCES,0.3665338645418327,"󰀏󰀏󰀏󰀏󰀏󰀏 󰀭 δj, 󰁛"
REFERENCES,0.36752988047808766,"i∈V,i∕=j δi"
REFERENCES,0.36852589641434264,󰀮󰀏󰀏󰀏󰀏󰀏󰀏 ≥󰂃 󰁳
REFERENCES,0.3695219123505976,(TV −1)d 󰀴 󰁄 󰀶 󰀸 ≤ 󰁛 j∈V p 󰀣 󰀂δj󰀂< √ d −󰂃 󰀤 + p 󰀳 󰁃
REFERENCES,0.3705179282868526,"󰀏󰀏󰀏󰀏󰀏󰀏 󰀭 δj, 󰁛"
REFERENCES,0.3715139442231076,"i∈V,i∕=j δi"
REFERENCES,0.37250996015936255,󰀮󰀏󰀏󰀏󰀏󰀏󰀏 ≥󰂃 󰁳
REFERENCES,0.37350597609561753,(TV −1)d 󰀴 󰁄 ≤ 󰁛 j∈V
EXP,0.3745019920318725,2 exp 󰀕 −c 󰂃2 G4 󰀖
EXP,0.3754980079681275,+ 2 exp 󰀕 −c 󰂃2 G4 󰀖
EXP,0.37649402390438247,≤4TV exp 󰀕 −c 󰂃2 G4 󰀖 .
EXP,0.37749003984063745,"Thus, with probability at least 1 −4TV exp 󰀓 −c 󰂃2 G4 󰀔"
EXP,0.3784860557768924,", none of the events happen and ∀j ∈V 󰀭 δj, 󰁛 i∈V δi 󰀮"
EXP,0.3794820717131474,"=
󰀂δj󰀂2 + 󰀭 δj, 󰁛"
EXP,0.3804780876494024,"i∈V,i∕=j δi 󰀮 ≥( √"
EXP,0.38147410358565736,d −󰂃)2 −󰂃 󰁳
EXP,0.38247011952191234,(TV −1)d
EXP,0.3834661354581673,= d −2󰂃 √
EXP,0.3844621513944223,d + 󰂃2 −󰂃 󰁳
EXP,0.3854581673306773,(TV −1)d
EXP,0.38645418326693226,≥d −2󰂃 󰁳
EXP,0.38745019920318724,(TV −1)d −󰂃 󰁳
EXP,0.3884462151394422,(TV −1)d
EXP,0.3894422310756972,≥d −3󰂃 󰁳 TV d
EXP,0.3904382470119522,"Thus, putting the analysis in the two cases together, as long as TV ≥1 ∀j ∈V 󰀭 δj, 󰁛 i∈V δi 󰀮"
EXP,0.39143426294820716,"≥d −3󰂃 󰁳 TV d,"
EXP,0.39243027888446214,with probability at least 1 −4TV exp 󰀓 −c 󰂃2 G4 󰀔 .
EXP,0.3934262948207171,"Proof of eq. (9)
Next, we apply lemma 3 again to the inner product of two vectors of i.i.d standard
normal random variables:"
EXP,0.3944223107569721,"∀j ∈U
p"
EXP,0.3954183266932271,"󰀣
󰀏󰀏󰀏󰀏󰀏 󰀭"
EXP,0.39641434262948205,"δj,
1
√TV 󰁛 i∈V δi"
EXP,0.39741035856573703,󰀮󰀏󰀏󰀏󰀏󰀏≥󰂃 √ d 󰀤
EXP,0.398406374501992,< 2 exp(−c󰂃2/G4).
EXP,0.399402390438247,By union bound over U p 󰀥 ∀j ∈U
EXP,0.40039840637450197,"󰀣
󰀏󰀏󰀏󰀏󰀏 󰀭"
EXP,0.40139442231075695,"δj,
1
√TV 󰁛 i∈V δi"
EXP,0.40239043824701193,󰀮󰀏󰀏󰀏󰀏󰀏≥󰂃 √ d 󰀤󰀦
EXP,0.4033864541832669,< 2Tu exp(−c󰂃2/G4).
EXP,0.4043824701195219,"Thus, with probability at least 1 −2Tu exp 󰀓 −c 󰂃2 G4 󰀔"
EXP,0.40537848605577687,", the following holds, concluding the proof ∀j ∈U 󰀏󰀏󰀏󰀏󰀏 󰀭"
EXP,0.4063745019920319,"δj,
1
√TV 󰁛 i∈V δi"
EXP,0.4073705179282869,󰀮󰀏󰀏󰀏󰀏󰀏≤󰂃 √ d.
EXP,0.40836653386454186,"Lemma 6. Let {xi, yi}i≤n be a collection of d dimensional covariates xi and label yi sampled
according to pρ in eq. (1). The covariates xi = [±Byi, yiδi], where +B in the middle coordinate
for i ∈Sshortcut and −B for i ∈Sleftover. The dual formulation of the following norm-minimization
problem"
EXP,0.40936254980079684,"wstable = arg min w
w2"
EXP,0.4103585657370518,y + w2
EXP,0.4113545816733068,z + 󰀂we󰀂2
EXP,0.4123505976095618,"s.t. i ∈Sshortcut
wy + Bwz + w⊤"
EXP,0.41334661354581675,e yiδi > 1
EXP,0.41434262948207173,"s.t. i ∈Sleftover
wy −Bwz + w⊤"
EXP,0.4153386454183267,e yiδi > 1
EXP,0.4163346613545817,wy ≥Bwz
EXP,0.41733067729083667,"is the following with ζ⊤= [−B, 1, 0d−2],"
EXP,0.41832669322709165,"max
λ≥0,ν≥0 −1"
EXP,0.41932270916334663,"4󰀂ζν + X⊤λ󰀂2 + 1⊤λ,
(10)"
EXP,0.4203187250996016,where X is a matrix with yixi as its rows.
EXP,0.4213147410358566,"Proof. We use Lagrange multipliers λ ∈Rn, ν ∈R to absorb the constraints and then use strong
duality. Letting ζ⊤= [−B, 1, 0d−2], X be a matrix where the ith row is xiyi, min"
EXP,0.42231075697211157,"w
󰀂w󰀂2
s.t.
Xw −1 ≥0
ζ⊤w ≥0"
EXP,0.42330677290836655,has the same solution as
EXP,0.4243027888446215,"max
λ≥0,ν≥0 min"
EXP,0.4252988047808765,"w
󰀂w󰀂2 −(Xw −1)⊤λ −νζ⊤w
(11)"
EXP,0.4262948207171315,"Now, we solve the inner minimization to write the dual problem only in terms of λ, ν. Solving the
inner minimization involves solving a quadratic program, which is done by setting its gradient to
zero, ∇w 󰀃"
EXP,0.42729083665338646,󰀂w󰀂2 −(Xw −1)⊤λ −νζ⊤w 󰀄
EXP,0.42828685258964144,= 2w −X⊤λ −νζ = 0
EXP,0.4292828685258964,=⇒w = 1
EXP,0.4302788844621514,2(ζν + X⊤λ)
EXP,0.4312749003984064,Substituting w = 1
EXP,0.43227091633466136,2(ζν + X⊤λ) in eq. (11)
EXP,0.43326693227091634,󰀂w󰀂2−(Xw −1)⊤λ −νζ⊤w =
EXP,0.4342629482071713,"1
4󰀂ζν + X⊤λ󰀂2 −(1"
EXP,0.4352589641434263,2(X(ζν + X⊤λ) −1)⊤λ −1
EXP,0.4362549800796813,2νζ⊤(ζν + X⊤λ) = 1
EXP,0.43725099601593626,4󰀂ζν + X⊤λ󰀂2 −(1
EXP,0.43824701195219123,2(X(ζν + X⊤λ) −1)⊤λ −1
EXP,0.4392430278884462,2ν2󰀂ζ󰀂2 −1
EXP,0.4402390438247012,2νζ⊤X⊤λ = 1
EXP,0.44123505976095617,4󰀂ζν + X⊤λ󰀂2 −1
EXP,0.44223107569721115,2(X(X⊤λ))⊤λ −1
EXP,0.44322709163346613,2(X(ζν))⊤λ + 1⊤λ −1
EXP,0.4442231075697211,2ν2󰀂ζ󰀂2 −1
EXP,0.4452191235059761,2νζ⊤X⊤λ = 1
EXP,0.44621513944223107,4󰀂ζν + X⊤λ󰀂2 − 󰀕1
EXP,0.44721115537848605,2(X(X⊤λ))⊤λ + 1
EXP,0.448207171314741,2ν2󰀂ζ󰀂2 + νζ⊤X⊤λ 󰀖 + 1⊤λ = 1
EXP,0.449203187250996,4󰀂ζν + X⊤λ󰀂2 − 󰀕1
EXP,0.450199203187251,2(X⊤λ)⊤X⊤λ + 1
EXP,0.45119521912350596,2ν2󰀂ζ󰀂2 + νζ⊤X⊤λ 󰀖 + 1⊤λ = 1
EXP,0.45219123505976094,4󰀂ζν + X⊤λ󰀂2 −1 2 󰀃
EXP,0.4531872509960159,󰀂X⊤λ󰀂2 + 󰀂νζ󰀂2 + 2νζ⊤X⊤λ 󰀄 + 1⊤λ = 1
EXP,0.4541832669322709,4󰀂ζν + X⊤λ󰀂2 −1
EXP,0.4551792828685259,2󰀂ζν + X⊤λ󰀂2 + 1⊤λ = −1
EXP,0.45617529880478086,4󰀂ζν + X⊤λ󰀂2 + 1⊤λ
EXP,0.45717131474103584,"A.2
Shortcut learning in max-margin classiﬁcation"
EXP,0.4581673306772908,We repeat the DGP from the linear perception task in eq. (1) here.
EXP,0.4591633466135458,"y ∼Rad,
z ∼"
EXP,0.4601593625498008,󰀝pρ(z = y | y = y) = ρ
EXP,0.46115537848605576,"pρ(z = −y | y = y) = (1 −ρ)
,
δ ∼N(0, Id−2),
x = [B ∗z, y, δ] . (12)"
EXP,0.46215139442231074,"Theorem 1. Let w∗be the max-margin predictor on n training samples from eq. (12) with a leftover
group of size k. There exist constants C1, C2, N0 > 0 such that"
EXP,0.4631474103585657,"∀integers k ∈ 󰀓 0, n 10 󰀔 (13)"
EXP,0.4641434262948207,"∀d ≥C1k log(3n),
(14)"
EXP,0.4651394422310757,∀B > C2 󰁳
EXP,0.46613545816733065,"d/k,
(15)"
EXP,0.46713147410358563,"with probability at least 1 −
1
3n over draws of the training data, it holds that Bw∗"
EXP,0.4681274900398406,z > w∗ y.
EXP,0.46912350597609564,"Before giving the proof of theorem 1, we ﬁrst give the corollary showing overparameterization is
not necessary for theorem 1 to hold."
EXP,0.4701195219123506,"Corollary 1. For all n > N0 — where the constant N0 is from theorem 1 — with scalar τ ∈(0, 1)
such that the dimension d = τn < n, theorem 1 holds."
EXP,0.4711155378486056,∀k ≤n × min 󰀝1
EXP,0.4721115537848606,"10,
τ
C1 log 3n 󰀞 ,"
EXP,0.47310756972111556,a linear model trained via default-ERM yields a predictor w∗such that Bw∗
EXP,0.47410358565737054,z > w∗ y.
EXP,0.4750996015936255,"Proof. We show that for a range of k, for all n ≥N0 theorem 1 holds for some d < n. Note that
theorem 1 holds for n ≥N0, d = C1k log(3n) and"
EXP,0.4760956175298805,∀k < n 10.
EXP,0.4770916334661355,"Setting d ≤τn for some τ ∈(0, 1) such that d < n means that theorem 1 holds if"
EXP,0.47808764940239046,"C1k log(3n) = d ≤τn =⇒k ≤
τn
C1 log(3n)."
EXP,0.47908366533864544,"Absorbing this new upper bound into the requirements on k for theorem 1 to hold, we get that for
any scalar n > N0, τ ∈(0, 1), d = τn, theorem 1 holds for"
EXP,0.4800796812749004,∀k < n × min 󰀝1
EXP,0.4810756972111554,"10,
τ
C1 log(3n) 󰀞 ."
EXP,0.4820717131474104,"In turn, even though d < n, a linear model trained via default-ERM converges in direction to a
max-margin classiﬁer such that w∗with Bw∗"
EXP,0.48306772908366535,z > w∗ y.
EXP,0.48406374501992033,"Proof. (of theorem 1) We consider two norm-minimization problems over w, one under constraint
wy ≥Bwz and another under wy < Bwz. We show that the latter achieves lower norm and
therefore, max-margin will achieve solutions wy < Bwz. The two minimization problems are as
follows:"
EXP,0.4850597609561753,"wstable = arg min w
w2"
EXP,0.4860557768924303,y + w2
EXP,0.48705179282868527,z + 󰀂we󰀂2
EXP,0.48804780876494025,"s.t. i ∈Sshortcut
wy + Bwz + w⊤"
EXP,0.48904382470119523,e yiδi > 1
EXP,0.4900398406374502,"s.t. i ∈Sleftover
wy −Bwz + w⊤"
EXP,0.4910358565737052,"e yiδi > 1
wy ≥Bwz (16)"
EXP,0.49203187250996017,"wshortcut = arg min w
w2"
EXP,0.49302788844621515,y + w2
EXP,0.4940239043824701,z + 󰀂we󰀂2
EXP,0.4950199203187251,"s.t. i ∈Sshortcut
wy + Bwz + w⊤"
EXP,0.4960159362549801,e yiδi > 1
EXP,0.49701195219123506,"i ∈Sleftover
wy −Bwz + w⊤"
EXP,0.49800796812749004,"e yiδi > 1
wy < Bwz (17)"
EXP,0.499003984063745,"From eq. (16), any w that satisfy the constraints of the dual maximization problem will lower bound
the value of the optimum of the primal, 󰀂wstable󰀂2 ≥Wstable. From the eq. (17), substituting a guess
in wshortcut that satisﬁes the constraints yields an upper bound, 󰀂wshortcut󰀂2 ≤Wshortcut. The actual
computation of the bounds Wshortcut, Wstable is in lemmas 7 and 8 which are proved in appendix A.3
and appendix A.4 respectively. We reproduce the lemmas here for convenience."
EXP,0.5,"Lemma. (7) Consider the following optimization problem from eq. (16) where n samples of xi, yi
come from eq. (1) where xi ∈Rd:"
EXP,0.500996015936255,"wstable = arg min w
w2"
EXP,0.50199203187251,y + w2
EXP,0.5029880478087649,z + 󰀂we󰀂2
EXP,0.5039840637450199,"s.t. i ∈Sshortcut
wy + Bwz + w⊤"
EXP,0.5049800796812749,e yiδi > 1
EXP,0.5059760956175299,"s.t. i ∈Sleftover
wy −Bwz + w⊤"
EXP,0.5069721115537849,"e yiδi > 1
wy ≥Bwz (18)"
EXP,0.5079681274900398,"Let k = |Sleftover| > 1. Then, for a ﬁxed constant c =
1
(2e)2 , with any scalar 󰂃< √"
EXP,0.5089641434262948,"d, with probability
at least 1 −2 exp(−c󰂃2/G4) and ∀integers M ∈ 󰀅 1, ⌊n 2k⌋ 󰀆 ,"
EXP,0.5099601593625498,"󰀂wstable󰀂2 ≥Wstable =
1 4 + ( √ d+󰂃) 2"
MK,0.5109561752988048,2Mk .
MK,0.5119521912350598,"Lemma. (8) Consider the following optimization problem from eq. (16) where n samples of xi, yi
come from eq. (1) where xi ∈Rd:"
MK,0.5129482071713147,"wshortcut = arg min w
w2"
MK,0.5139442231075697,y + w2
MK,0.5149402390438247,z + 󰀂we󰀂2
MK,0.5159362549800797,"s.t. i ∈Sshortcut
wy + Bwz + w⊤"
MK,0.5169322709163346,e yiδi > 1
MK,0.5179282868525896,"i ∈Sleftover
wy −Bwz + w⊤"
MK,0.5189243027888446,"e yiδi > 1
wy < Bwz (19)"
MK,0.5199203187250996,"Let k = |Sleftover| ≥1. Then, for a ﬁxed constant c =
1
(2e)2 , with any scalar 󰂃< 1 3 󰁴 d
k < √"
MK,0.5209163346613546,"d, with"
MK,0.5219123505976095,probability at least 1 −2(2k + (n −k) + 1) exp(−c 󰂃2
MK,0.5229083665338645,"G4 ), for γ =
2
d−4󰂃 √ kd,"
MK,0.5239043824701195,󰀂wshortcut󰀂2 ≤Wshortcut = γ2k( √
MK,0.5249003984063745,d + 󰂃)2 + 󰀓
MK,0.5258964143426295,1 + γ󰂃 √ dk 󰀔2 B2
MK,0.5268924302788844,"Together, the lemmas say that for any ∀integers M ∈ 󰀅 1, ⌊n 2k⌋ 󰀆"
MK,0.5278884462151394,and 󰂃< 1 3 󰁴
MK,0.5288844621513944,"d
k, with probability"
MK,0.5298804780876494,≥1 −2 exp(−c󰂃2/G4)
MK,0.5308764940239044,"󰀂wstable󰀂2 ≥Wstable =
1 4 + ( √ d+󰂃) 2"
MK,0.5318725099601593,2Mk .
MK,0.5328685258964143,and with probability at least 1 −2(2k + (n −k) + 1) exp(−c 󰂃2
MK,0.5338645418326693,"G4 ), for γ =
2
d−4󰂃 √"
MK,0.5348605577689243,"kd > 0,"
MK,0.5358565737051793,󰀂wshortcut󰀂2 ≤Wshortcut = γ2k( √
MK,0.5368525896414342,d + 󰂃)2 + 󰀓
MK,0.5378486055776892,1 + γ󰂃 √ dk 󰀔2 B2
MK,0.5388446215139442,"First, we choose 󰂃2 = 2 G4"
MK,0.5398406374501992,"c log(3n). This gives us the probability with which these bounds hold: as
k < 0.1n we have k + 2 < n"
AND,0.5408366533864541,2 and
AND,0.5418326693227091,1 −2(2k + (n −k) + 2) exp(−c 󰂃2
AND,0.5428286852589641,G4 ) = 1 −2(n + k + 2) exp(−2 log(3n))
AND,0.5438247011952191,≥1 −2(3n
AND,0.5448207171314741,2 ) exp(−2 log(3n))
AND,0.545816733067729,"= 1 −exp(−2 log(3n) + log(3n))
= 1 −exp(−log(3n))"
AND,0.546812749003984,= 1 −1 3n.
AND,0.547808764940239,"Next, we will instantiate the parameter M and set the constants C1, C2 and the upper bound on k in
theorem 1 to guarantee the following eq. (separation inequality):"
AND,0.548804780876494,Wshortcut = γ2k( √
AND,0.549800796812749,d + 󰂃)2 + 󰀓
AND,0.5507968127490039,1 + γ󰂃 √ dk 󰀔2
AND,0.5517928286852589,"B2
<
1 4 + ( √ d+󰂃) 2"
MK,0.5527888446215139,2Mk
MK,0.5537848605577689,"= Wstable,"
MK,0.5547808764940239,(separation inequality)
MK,0.5557768924302788,"which then implies that 󰀂wshortcut󰀂2 < 󰀂wstable󰀂2, concluding the proof."
MK,0.5567729083665338,"Invoking the conditions in theorem 1 and setting the upper bound on k.
We will keep the 󰂃as
is for simplicity of reading but invoke the inequalities satisﬁed by log(3n) from theorem 1:"
MK,0.5577689243027888,"∃constant C1,
d ≥C1k log(3n)."
MK,0.5587649402390438,Now we let C1 = 2 G4
MK,0.5597609561752988,"cC2 for a constant C ∈ 󰀃 0, 1 3"
MK,0.5607569721115537,"󰀄3, such that"
MK,0.5617529880478087,󰂃2 = 2G4
MK,0.5627490039840638,c log(3n) < C2 d
MK,0.5637450199203188,k =⇒󰂃< C 󰁵
MK,0.5647410358565738,"d
k and 󰂃 √"
MK,0.5657370517928287,"kd < Cd.
(20)"
MK,0.5667330677290837,"We next ﬁnd a C ∈ 󰀃 0, 1 3 󰀄"
MK,0.5677290836653387,"such that eq. (separation inequality) holds with M = 5, which upper
bounds k: M < n"
MK,0.5687250996015937,2k =⇒k
MK,0.5697211155378487,"n <
1
2M = 1"
MK,0.5707171314741036,10 =⇒k < n 10.
MK,0.5717131474103586,"Simplifying Wshortcut and Wstable.
To actually show Wshortcut < Wstable in eq. (separation inequal-
ity), we compare a simpliﬁed strict upper bound on the LHS Wshortcut and a simpliﬁed strict lower
bound on the RHS Wstable
For the simpliﬁcation of the RHS Wstable of eq. (separation inequality), we will use the fact that d ≥
2 G4"
MK,0.5727091633466136,"cC2 log(3n)k. Given the assumption n > N0, choosing N0 to be an integer such that log(3N0) ≥ 40cC2"
MK,0.5737051792828686,"G4
means that log(3n) > 40cC2"
MK,0.5747011952191236,"G4
and we have"
MK,0.5756972111553785,"d
k > 80 =⇒
d
10k > 8 =⇒1 2"
MK,0.5766932270916335,"d
10k > 4
(21)"
MK,0.5776892430278885,"which gives us, for M = 5,"
MK,0.5786852589641435,"Wstable =
1 4 + ( √ d+󰂃) 2"
MK,0.5796812749003984,"2Mk (22) =
1 4 + ( √ d+󰂃) 2"
K,0.5806772908366534,"10k (23) ≥
1 3
2 ( √ d+󰂃) 2"
K,0.5816733067729084,10k
K,0.5826693227091634,{4 < 1 2
K,0.5836653386454184,"d
10k < 1 2 ( √"
K,0.5846613545816733,d + 󰂃)2
K,0.5856573705179283,"10k
from eq. (21) }
(24)"
K,0.5866533864541833,"=
20k
3( √"
K,0.5876494023904383,"d + 󰂃)2
(25) ≥
20k 3( √ d + C √ d
√"
K,0.5886454183266933,"k)2
{󰂃< C √ d
√ k"
K,0.5896414342629482,"from eq. (20) }
(26)"
K,0.5906374501992032,"=
20k
3(1 +
C
√"
K,0.5916334661354582,"k)2d
(27)"
K,0.5926294820717132,">
20k
3(1 + C)2d
{k ≥1}
(28)"
K,0.5936254980079682,"Now, we produce a simpler upper bound on the ﬁrst part of the LHS of eq. (separation inequality):
recalling that γ =
2
d−4󰂃 √"
K,0.5946215139442231,"kd, and substituting in the upper bounds on 󰂃, γ2k( √"
K,0.5956175298804781,d + 󰂃)2 = 󰀣 2( √
K,0.5966135458167331,"d + 󰂃)
d −4󰂃 √ kd 󰀤2 k < 4 󰀳 󰁃( √ d + C 󰁴 d
k)"
K,0.5976095617529881,d −4Cd 󰀴 󰁄 2
K,0.598605577689243,"k
{󰂃< C √ d
√ k"
K,0.599601593625498,from eq. (20) } = 4k d
K,0.600597609561753,"󰀣(1 +
C
√ k) 1 −4C 󰀤2"
K,0.601593625498008,3The 1
K,0.602589641434263,3 comes from requiring that 󰂃< 1 3 󰁴
K,0.603585657370518,"d
k from lemma 8. ≤4k d"
K,0.6045816733067729,󰀕1 + C 1 −4C 󰀖2
K,0.6055776892430279,",
{k ≥1}
(29)"
K,0.6065737051792829,"Next is a simpler upper bound on the second part of the LHS of eq. (separation inequality). Again
with γ =
2
d−4󰂃 √ kd, 󰀓"
K,0.6075697211155379,"1 + γ󰂃 √ dk 󰀔2 B2
= 󰀓"
K,0.6085657370517928,"1 +
2󰂃 √"
K,0.6095617529880478,"dk
d−4󰂃 √ kd 󰀔2 B2 ≤ 󰀓"
K,0.6105577689243028,"1 +
2Cd
d−4Cd 󰀔2 B2 = 󰀓"
K,0.6115537848605578,"1 +
2C
1−4C 󰀔2 B2"
K,0.6125498007968128,Now setting B > √ 2 󰀓
K,0.6135458167330677,"1 +
2C
1−4C 󰀔 󰁴"
K,0.6145418326693227,4k d 󰀓
K,0.6155378486055777,"1+C
1−4C 󰀔"
K,0.6165338645418327,gives the lower bound on B from theorem 1:
K,0.6175298804780877,B > C2 󰁵
K,0.6185258964143426,"d
k ,
where
C2 = 󰀓"
K,0.6195219123505976,"1 +
2C
1−4C 󰀔 √ 2 󰀓"
K,0.6205179282868526,"1+C
1−4C"
K,0.6215139442231076,"󰀔=
(1 −2C)
√"
K,0.6225099601593626,2(1 + C).
K,0.6235059760956175,"Formally,"
K,0.6245019920318725,B > C2 󰁵
K,0.6254980079681275,"d
k =⇒ 󰀓"
K,0.6264940239043825,"1 +
2C
1−4C 󰀔2"
K,0.6274900398406374,"B2
< 1 2 󰀣󰁵"
K,0.6284860557768924,4k d
K,0.6294820717131474,󰀕1 + C 1 −4C 󰀖󰀤2 = 1 2
K,0.6304780876494024,4k d
K,0.6314741035856574,󰀕1 + C 1 −4C 󰀖2
K,0.6324701195219123,".
(30)"
K,0.6334661354581673,"By combining the upper bound from eq. (30) and the upper bound from eq. (29), we get an upper
bound on the whole of the LHS of eq. (separation inequality), which in turn provides an upper bound
on Wshortcut:"
K,0.6344621513944223,Wshortcut = γ2k( √
K,0.6354581673306773,d + 󰂃)2 + 󰀓
K,0.6364541832669323,1 + γ󰂃 √ dk 󰀔2
K,0.6374501992031872,"B2
< 3 2"
K,0.6384462151394422,4k d
K,0.6394422310756972,󰀕(1 + C) 1 −4C 󰀖2 ≤3 2
K,0.6404382470119522,4k d
K,0.6414342629482072,"󰀕(1 + C) 1 −4C 󰀖2 ,"
K,0.6424302788844621,because k ≥1. Note the upper bound is strict.
K,0.6434262948207171,"Concluding the proof.
Now, we show that a C exists such that the following holds, which implies
Wshortcut < Wstable, which in turn implies eq. (separation inequality) and the proof concludes:"
K,0.6444223107569721,Wshortcut < 3 2
K,0.6454183266932271,4k d
K,0.646414342629482,󰀕(1 + C) 1 −4C 󰀖2
K,0.647410358565737,"≤
20k
3(1 + C)2d < Wstable."
K,0.648406374501992,The above inequality holds when 6
K,0.649402390438247,󰀕(1 + C) 1 −4C 󰀖2
K,0.650398406374502,"≤
20
3(1 + C)2
⇐⇒
(1 + C)2 − 󰁵 10"
K,0.651394422310757,9 (1 −4C) ≤0.
K,0.6523904382470119,The right hand side holds when the quadratic equation (1 + C)2 − 󰁴 10
K,0.6533864541832669,"9 (1 −4C) is non-positive,
which holds between the roots of the equation. The equation’s positive solution is"
K,0.6543824701195219,"C =
−3 + √ 10 3 + 2 √ 10 + 󰁴"
K,0.6553784860557769,5(8 + 3 √ 10)
K,0.6563745019920318,≈0.008.
K,0.6573705179282868,"Setting C to this quantity satisﬁes the requirement that C ∈(0, 1 3)."
K,0.6583665338645418,"Thus, a C exists such that eq. (separation inequality) holds which concludes the proof of theorem 1
for the following constants and constraints implied by C and M = 5:"
K,0.6593625498007968,"C2 =
(1 −2C)
√"
K,0.6603585657370518,"2(1 + C)
C1 = 2 G4"
K,0.6613545816733067,"cC2
k < n 10,"
K,0.6623505976095617,"where G is the ψ2-norm of a standard normal r.v. and c is the absolute constant from the Bernstein
inequality in lemma 2."
K,0.6633466135458167,"A.3
Lower bounding the norm of solutions that rely more on the stable feature"
K,0.6643426294820717,"Lemma 7. Consider the following optimization problem from eq. (16) where n samples of xi, yi
come from eq. (1) where xi ∈Rd:"
K,0.6653386454183267,"wstable = arg min w
w2"
K,0.6663346613545816,y + w2
K,0.6673306772908366,z + 󰀂we󰀂2
K,0.6683266932270916,"s.t. i ∈Sshortcut
wy + Bwz + w⊤"
K,0.6693227091633466,e yiδi > 1
K,0.6703187250996016,"s.t. i ∈Sleftover
wy −Bwz + w⊤"
K,0.6713147410358565,"e yiδi > 1
wy ≥Bwz (31)"
K,0.6723107569721115,"Let k = |Sleftover| > 1. Then, for a ﬁxed constant c =
1
(2e)2 , with any scalar 󰂃< √"
K,0.6733067729083665,"d, with probability
at least 1 −2 exp(−c󰂃2/G4) and ∀integers M ∈ 󰀅 1, ⌊n 2k⌋ 󰀆 ,"
K,0.6743027888446215,"󰀂wstable󰀂2 ≥Wstable =
1 4 + ( √ d+󰂃) 2"
MK,0.6752988047808764,2Mk .
MK,0.6762948207171314,"Proof. By lemma 6, the dual of eq. (16) is the following for ζ = [−B, 1, 0d−2] and X is an n × d
matrix with rows yixi:"
MK,0.6772908366533864,"max
λ≥0,ν≥0 −1"
MK,0.6782868525896414,"4󰀂ζν + X⊤λ󰀂2 + 1⊤λ
(32)"
MK,0.6792828685258964,Now by duality
MK,0.6802788844621513,"󰀂wstable󰀂2 ≥
max
λ≥0,ν≥0 −1"
MK,0.6812749003984063,"4󰀂ζν + X⊤λ󰀂2 + 1⊤λ,"
MK,0.6822709163346613,which means any feasible candidate to eq. (32) gives a lower bound on 󰀂wstable󰀂2.
MK,0.6832669322709163,"Feasible Candidates for λ, ν.
We now deﬁne a set U ⊂[n], and let λi =
α
|U| > 0 for i ∈U and
0 otherwise. For M ∈(1, ⌊n"
MK,0.6842629482071713,"2k⌋], we take 2Mk samples from the training data to be included in U.
Formally,"
MK,0.6852589641434262,"U = Sleftover ∪(2M −1)k a random samples from Sshortcut,"
MK,0.6862549800796812,"which gives the size |U| = 2Mk. Then, we let ν = α 2(M−1)"
M,0.6872509960159362,"2M
> 0."
M,0.6882470119521913,"Note that for the above choice of λ, X⊤λ is a sum of the rows from U scaled by
α
|U|. Adding up
k rows from Sleftover and k rows from Sshortcut cancels out the Bs and, so in the B is accumulated
|U| −2k = 2(M −1)k times, and so X⊤λ = 󰀥"
M,0.6892430278884463,α ∗|U| −2k
M,0.6902390438247012,"|U|
B, α, α |U| 󰁛 i∈U δi 󰀦 = 󰀥"
M,0.6912350597609562,αB 2(M −1)
M,0.6922310756972112,"2M
, α, α |U| 󰁛 i∈U δi 󰀦 ."
M,0.6932270916334662,"As λ has
α
|U| on |U| elements and 0 otherwise, λ⊤1 = α"
M,0.6942231075697212,As we set ν = α 2(M−1)
M,0.6952191235059761,"2M
,"
M,0.6962151394422311,νζ + X⊤λ = 󰀥
M,0.6972111553784861,−αB 2(M −1)
M,0.6982071713147411,"2M
+ α2(M −1)"
M,0.6992031872509961,"2M
B, α2(M −1)"
M,0.700199203187251,"2M
+ α, 0 + α |U| 󰁛 i δi 󰀦 (33) = 󰀥 0
, α 󰀕"
M,0.701195219123506,1 + 2(M −1)
M,0.702191235059761,2M 󰀖
M,0.703187250996016,",
α
|U| 󰁛 i δi 󰀦 (34)"
M,0.704183266932271,"=⇒󰀂ζν + X⊤λ󰀂2 = 󰀐󰀐󰀐󰀐󰀐 󰀥 0, α 󰀕"
M,0.7051792828685259,1 + 2(M −1)
M,0.7061752988047809,"2M 󰀖 , α |U| 󰁛 i∈U δi"
M,0.7071713147410359,"󰀦󰀐󰀐󰀐󰀐󰀐 2 (35) = α2 󰀐󰀐󰀐󰀐󰀐 󰀥 0, 󰀕"
M,0.7081673306772909,1 + 2(M −1)
M,0.7091633466135459,"2M 󰀖 , 1 |U| 󰁛 i∈U δi"
M,0.7101593625498008,"󰀦󰀐󰀐󰀐󰀐󰀐
(36)"
M,0.7111553784860558,"For the chosen values of ν, λ the value of the objective in eq. (32) is −α2 4 󰀐󰀐󰀐󰀐󰀐 󰀥 0, 󰀕"
M,0.7121513944223108,1 + 2(M −1)
M,0.7131474103585658,"2M 󰀖 , 1 |U| 󰁛 i∈U δi"
M,0.7141434262948207,󰀦󰀐󰀐󰀐󰀐󰀐 2
M,0.7151394422310757,"+ α
(37)"
M,0.7161354581673307,"Letting Γ = 󰀐󰀐󰀐󰀐󰀐 󰀥 0, 󰀕"
M,0.7171314741035857,1 + 2(M −1)
M,0.7181274900398407,"2M 󰀖 , 1 |U| 󰁛 i∈U δi"
M,0.7191235059760956,"󰀦󰀐󰀐󰀐󰀐󰀐 2 ,"
M,0.7201195219123506,the objective is of the form α −α2Γ
M,0.7211155378486056,"4 . To maximize with respect to α, setting the derivative of the
objective w.r.t α to 0 gives:"
M,0.7221115537848606,1 −2αΓ
M,0.7231075697211156,"4
= 0 =⇒α = 2"
M,0.7241035856573705,"Γ =⇒α −α2Γ 4
= 2 Γ −4 Γ2 Γ 4 = 1 Γ."
M,0.7250996015936255,This immediately gives us
M,0.7260956175298805,"󰀂wstable󰀂2 ≥1 Γ,"
M,0.7270916334661355,and we lower bound this quantity by upper bounding Γ.
M,0.7280876494023905,"By concentration of gaussian norm as in lemma 4, with probability at least 1 −2 exp(−c 󰂃2"
M,0.7290836653386454,"G4 )
󰀐󰀐󰀐󰀐󰀐 1
|U| 󰁛 i∈U δi"
M,0.7300796812749004,"󰀐󰀐󰀐󰀐󰀐=
1
󰁳 |U| 󰀐󰀐󰀐󰀐󰀐 1
󰁳 |U| 󰁛 i∈U δi"
M,0.7310756972111554,"󰀐󰀐󰀐󰀐󰀐≤
1
󰁳 |U| ( √"
M,0.7320717131474104,d + 󰂃).
M,0.7330677290836654,"In turn, recalling that |U| = 2Mk Γ ≤"
M,0.7340637450199203,󰀕(2(M −1) + 2M)
M,0.7350597609561753,2M 󰀖2 + 󰀣√
M,0.7360557768924303,"d + 󰂃
󰁳 |U| 󰀤2 < 4 + 󰀣√"
M,0.7370517928286853,"d + 󰂃
󰁳 |U| 󰀤2 ≤4 + 󰀓√ d + 󰂃 󰀔2"
MK,0.7380478087649402,2Mk
MK,0.7390438247011952,The upper bound on Γ gives the following lower bound on 󰀂wstable󰀂2:
MK,0.7400398406374502,"󰀂wstable󰀂2 ≥1 Γ ≥
1 4 + ( √ d+󰂃) 2"
MK,0.7410358565737052,2Mk
MK,0.7420318725099602,"A.4
Upper bounding the norm of solutions that rely more on the shortcut."
MK,0.7430278884462151,"Lemma 8. Consider the following optimization problem from eq. (16) where n samples of xi, yi
come from eq. (1) where xi ∈Rd:"
MK,0.7440239043824701,"wshortcut = arg min w
w2"
MK,0.7450199203187251,y + w2
MK,0.7460159362549801,z + 󰀂we󰀂2
MK,0.7470119521912351,"s.t. i ∈Sshortcut
wy + Bwz + w⊤"
MK,0.74800796812749,e yiδi > 1
MK,0.749003984063745,"i ∈Sleftover
wy −Bwz + w⊤"
MK,0.75,"e yiδi > 1
wy < Bwz (38)"
MK,0.750996015936255,"Let k = |Sleftover| ≥1. Then, for a ﬁxed constant c =
1
(2e)2 , with any scalar 󰂃< 1 3 󰁴 d
k < √"
MK,0.75199203187251,"d, with"
MK,0.7529880478087649,probability at least 1 −2(2k + (n −k) + 1) exp(−c 󰂃2
MK,0.7539840637450199,"G4 ), for γ =
2
d−4󰂃 √ kd,"
MK,0.7549800796812749,󰀂wshortcut󰀂2 ≤Wshortcut = γ2k( √
MK,0.7559760956175299,d + 󰂃)2 + 󰀓
MK,0.7569721115537849,1 + γ󰂃 √ dk 󰀔2 B2
MK,0.7579681274900398,Proof. Let k = |Sleftover|. The candidate we will evaluate the objective for is w = 󰀵 󰀷β
MK,0.7589641434262948,"B , 0, γ 󰁛"
MK,0.7599601593625498,j∈Sleftover yjδj 󰀶
MK,0.7609561752988048,"󰀸.
(39)"
MK,0.7619521912350598,"High-probability bounds on the margin achieved by the candidate and norm of w
The mar-
gins on the shortcut group and the leftover group along with the constraints are as follows:"
MK,0.7629482071713147,"∀j ∈Sshortcut
mj = 0 + B ∗β B + 󰀭"
MK,0.7639442231075697,"yjδj, γ 󰁛"
MK,0.7649402390438247,i∈Sleftover yiδi 󰀮 ≥1
MK,0.7659362549800797,"∀j ∈Sleftover
mj = 0 −B ∗β B + 󰀭"
MK,0.7669322709163346,"yjδj, γ 󰁛"
MK,0.7679282868525896,i∈Sleftover yiδi 󰀮 ≥1. (40)
MK,0.7689243027888446,"Due to the standard normal distribution being isotropic, and yj ∈{−1, 1}, yjδj has the same
distribution as δj. Then, we apply lemma 5 with V = Sleftover, U = Sshortcut — which means Tv = k
and Tu = (n −k) — to bound the margin terms in eq. (40) and 󰀂w󰀂2 with probability at least"
MK,0.7699203187250996,1 −2(2k + (n −k) + 2) exp(−c 󰂃2 G4 ).
MK,0.7709163346613546,"Applying the bound in eq. (9) in lemma 5 between a sum of vectors and a different i.i.d vector,"
MK,0.7719123505976095,∀j ∈Sshortcut 󰀏󰀏󰀏󰀏󰀏 󰀭
MK,0.7729083665338645,"yjδj, γ 󰁛"
MK,0.7739043824701195,i∈Sleftover yiδi
MK,0.7749003984063745,󰀮󰀏󰀏󰀏󰀏󰀏≤γ󰂃 √
MK,0.7758964143426295,"kd
(41)"
MK,0.7768924302788844,Applying the bound in eq. (8) from lemma 5
MK,0.7778884462151394,∀j ∈Sleftover 󰀭
MK,0.7788844621513944,"yjδj, γ 󰁛"
MK,0.7798804780876494,i∈Sleftover yiδi 󰀮 ≥γ 󰀓 d −3󰂃 √ kd 󰀔 (42)
MK,0.7808764940239044,The margin constraints on the shortcut and leftover from eq. (40) respectively imply β −γ󰂃 √
MK,0.7818725099601593,"dk ≥1
−β + γ 󰀓 d −3󰂃 √ kd 󰀔 ≥1"
MK,0.7828685258964143,We choose β = 1 + γ󰂃 √
MK,0.7838645418326693,"dk, which implies an inequality that γ has to satisfy the following, which
is due to d −3󰂃 √"
MK,0.7848605577689243,"kd > 0,"
MK,0.7858565737051793,−(1 + γ󰂃 √
MK,0.7868525896414342,dk) + γ 󰀓 d −3󰂃 √ kd 󰀔
MK,0.7878486055776892,"≥1 =⇒γ ≥
2
d −4󰂃 √ kd"
MK,0.7888446215139442,"Now, we choose"
MK,0.7898406374501992,"γ =
2
d −4󰂃 √ kd ."
MK,0.7908366533864541,"Computing the upper bound on the value of the objective in the primal problem in eq. (17)
The feasible candidate’s norm 󰀂w󰀂2 is an upper bound on the solution’s norm 󰀂wshortcut󰀂2 and so"
MK,0.7918326693227091,󰀂wshortcut󰀂2 ≤󰀂w󰀂2 = 1
MK,0.7928286852589641,B2 β2 +
MK,0.7938247011952191,󰀐󰀐󰀐󰀐󰀐󰀐 γ 󰁛
MK,0.7948207171314741,j∈Sleftover yjδj
MK,0.795816733067729,󰀐󰀐󰀐󰀐󰀐󰀐 2 = γ2k
MK,0.796812749003984,"󰀐󰀐󰀐󰀐󰀐󰀐 1
√ k 󰁛"
MK,0.797808764940239,j∈Sleftover δj
MK,0.798804780876494,󰀐󰀐󰀐󰀐󰀐󰀐 2 + β2 B2
MK,0.799800796812749,"By lemma 5 which we invoked,"
MK,0.8007968127490039,"󰀐󰀐󰀐󰀐󰀐󰀐 1
√ k 󰁛"
MK,0.8017928286852589,j∈Sleftover δj
MK,0.8027888446215139,󰀐󰀐󰀐󰀐󰀐󰀐 2 ≤( √
MK,0.8037848605577689,d + 󰂃)2.
MK,0.8047808764940239,"To conclude the proof, substitute β = 1 + γ󰂃 √"
MK,0.8057768924302788,"dk and get the following upper bound with γ =
2
d−3󰂃 √ kd:"
MK,0.8067729083665338,󰀂wshortcut󰀂2 ≤γ2k( √
MK,0.8077689243027888,d + 󰂃)2 + β2
MK,0.8087649402390438,B2 = γ2k( √
MK,0.8097609561752988,d + 󰂃)2 + 󰀓
MK,0.8107569721115537,"1 + γ󰂃 √ dk 󰀔2 B2
."
MK,0.8117529880478087,"A.5
Concentration of k and intuition behind theorem 1"
MK,0.8127490039840638,"Concentration of k around (1 −ρ)n.
Denote the event that the ith sample lies in the leftover
group as Ii: then E[Ii] = 1 −ρ and the leftover group size is k = 󰁓"
MK,0.8137450199203188,"i Ii. Hoeffding’s inequality
(Theorem 2.2.6 in [Vershynin, 2018]) shows that for any t > 0, k is at most (1 −ρ)n + t√n with
probability at least 1 −exp(−2t2): p 󰀃"
MK,0.8147410358565738,k −(1 −ρ)n > t√n 󰀄 = p 󰀣󰁛 i
MK,0.8157370517928287,(Ii −(1 −ρ)) > t√n 󰀤 = p 󰀣󰁛 i
MK,0.8167330677290837,(Ii −E[Ii]) > t√n 󰀤
MK,0.8177290836653387,≤exp(−2t2).
MK,0.8187250996015937,Letting ρ = 0.9 + 󰁴
MK,0.8197211155378487,log 3n
MK,0.8207171314741036,"n
and t = √log 3n, gives us p 󰀃"
MK,0.8217131474103586,k −(1 −ρ)n > t√n 󰀄 = p 󰀓
MK,0.8227091633466136,k −0.1n + 󰁳
MK,0.8237051792828686,n log 3n > 󰁳
MK,0.8247011952191236,log 3n√n 󰀔
MK,0.8256972111553785,= p (k −0.1n > 0)
MK,0.8266932270916335,"≤exp(−2t2)
= exp(−2 log 3n). = 󰀕1"
N,0.8276892430278885,3n 󰀖2 < 1
N,0.8286852589641435,3n
N,0.8296812749003984,"To connect ρ to shortcut learning due to max-margin classiﬁcation, we take a union bound of the
event that k < 0.1n, which occurs with probability at least 1 −1"
N,0.8306772908366534,"3n and theorem 1 which occurs with
probability at least 1 −
1
3n. This union bound guarantees that with probability at least 1 −
2
3n over
sampling the training data, max-margin classiﬁcation on n training samples from eq. (1) relies more
on the shortcut feature if ρ is above a threshold; and this threshold converges to 0.9 at the rate of
󰁳"
N,0.8316733067729084,log 3n/n.
N,0.8326693227091634,"A.6
Bumpy losses improve ERM in the under-parameterized setting"
N,0.8336653386454184,"Theorem 2. Consider n samples of training data from DGP in eq. (1) with d < n. Consider a
linear classiﬁer fθ(x) = w⊤x such that for all samples in the training data yiw⊤xi = b for any
b ∈(0, ∞). With probability 1 over draws of samples, w = [0, b, 0d−2]."
N,0.8346613545816733,"Proof. Letting X be the matrix where each row is yixi, the theorem statement says the solution w∗"
N,0.8356573705179283,"Xw∗= b1
(43)"
N,0.8366533864541833,"First, split w∗= [w∗ z, w∗ y, w∗"
N,0.8376494023904383,"−y]. Equation (43) says that the margin of the model on any sample
satisﬁes"
N,0.8386454183266933,y(w∗)⊤x = w∗
N,0.8396414342629482,yy2 + w∗
N,0.8406374501992032,zyz + y(w∗
N,0.8416334661354582,"−y)⊤δ = b
=⇒
y(w∗"
N,0.8426294820717132,−y)⊤δ = b −w∗
N,0.8436254980079682,yy2 −w∗ zyz
N,0.8446215139442231,"We collect these equations for the whole training data by splitting X into columns: denoting Y, Z
as vectors of yi and zi and using · to denote element wise operation, split X into columns that
correspond to y, z and δ respectively as X = [Y · Y | Y · Z | Xδ]. Rearranging terms gives us w∗"
N,0.8456175298804781,zY · Z + w∗
N,0.8466135458167331,y1 + Xδw∗
N,0.8476095617529881,"δ = b1
=⇒
Xδw∗"
N,0.848605577689243,δ = (b −w∗
N,0.849601593625498,y)1 −w∗
N,0.850597609561753,zY · Z.
N,0.851593625498008,"The elements of Y · Z lie in {−1, 1} and, as the shortcut feature does not always equal the label, the
elements of Y · Z are not all the same sign."
N,0.852589641434263,Solutions do not exist when one non-zero element exists in (b −w∗
N,0.853585657370518,y)1 −w∗
N,0.8545816733067729,"zY · Z
By deﬁnition
of w∗ Xδw∗"
N,0.8555776892430279,δ = (b −w∗
N,0.8565737051792829,y)1 −w∗
N,0.8575697211155379,zY · Z.
N,0.8585657370517928,Denote r = (b −w∗
N,0.8595617529880478,y)1 −w∗
N,0.8605577689243028,"zY · Z. and A = Xδ. Now we show that w.p. 1 solutions do not exist for
the following system of linear equations:"
N,0.8615537848605578,Aw = r.
N,0.8625498007968128,"First, note that A = Xδ has yiδi for rows and as yi |="
N,0.8635458167330677,"δi and yi ∈{−1, 1}, each vector yiδi is
distributed identically to a vector of independent standard Gaussian random variables. Thus, A is a
matrix of IID standard Gaussian random variables."
N,0.8645418326693227,"Let U denote D −2 indices such that the corresponding rows of A form a matrix D −2 × D −2
matrix and rU has at least one non-zero element; let AU denote the resulting matrix. Now AU is
a D −2 × D −2 sized matrix where each element is a standard Gaussian random variable. Such
matrices have rank D −2 with probability 1 because square singular matrices form a measure zero
set under the Lebesgue measure over RD−2×D−2[Feng and Zhang, 2007]."
N,0.8655378486055777,"We use subscript ·−U to denote all but the indices in U. The equation Aw = r implies the following
two equations:"
N,0.8665338645418327,"AUw = rU
A−Uw = r−U.
As AU is has full rank (D −2), AUw = rU admits a unique solution w∗"
N,0.8675298804780877,"U ∕= 0 — because rU has at
least one non-zero element by construction. Then, it must hold that A−Uw∗"
N,0.8685258964143426,"U = r−U.
(44)"
N,0.8695219123505976,"For any row v⊤∈A−U, eq. (44) implies that v⊤w∗equals a ﬁxed constant. As v is a vector of
i.i.d standard normal random variables, v⊤w∗is a gaussian random variable with mean 󰁓(w∗"
N,0.8705179282868526,"i ) and
variance 󰀂w∗󰀂2. Then with probability 1, v⊤w∗will not equal a constant. Thus, w.p.1 A−Uw∗"
N,0.8715139442231076,"U =
r−U is not satisﬁed, which means w.p.1 there are no solutions to Aw = r."
N,0.8725099601593626,Case where (b −w∗
N,0.8735059760956175,y)1 −w∗
N,0.8745019920318725,"zY · Z is zero element-wise
As X has rank D −2, Xδw∗"
N,0.8754980079681275,"δ = 0 only
when w∗"
N,0.8764940239043825,δ = 0.
N,0.8774900398406374,Each element in (b −w∗
N,0.8784860557768924,y)1 −w∗
N,0.8794820717131474,zY · Z is either b −w∗
N,0.8804780876494024,y + w∗
N,0.8814741035856574,z or b −w∗ y −w∗
N,0.8824701195219123,"z. Thus,"
N,0.8834661354581673,(b −w∗
N,0.8844621513944223,y)1 −w∗
N,0.8854581673306773,"zY · Z = 0
=⇒"
N,0.8864541832669323,"󰀝
b −w∗"
N,0.8874501992031872,y + w∗
N,0.8884462151394422,"z = 0,
b −w∗ y −w∗"
N,0.8894422310756972,"z = 0
(45)"
N,0.8904382470119522,Adding and subtracting the two equations on the right gives
N,0.8914342629482072,2(b −w∗
N,0.8924302788844621,"y) = 0
and
2w∗"
N,0.8934262948207171,z = 0.
N,0.8944223107569721,"Thus, w∗"
N,0.8954183266932271,"δ = 0, w∗"
N,0.896414342629482,"z = 0, b = w∗ y."
N,0.897410358565737,"B
Appendix: further experimental details and results"
N,0.898406374501992,"B.1
Default-ERM with ℓ2-regularization."
N,0.899402390438247,"In section 3, we show default-ERM achieves zero training loss by using the shortcut to classify
the shortcut group and noise to classify the leftover group, meaning the leftover group is overﬁt.
The usual way to mitigate overﬁtting is via ℓ2-regularization, which, one can posit, may encourage
models to rely on the perfect stable feature instead of the imperfect shortcut and noise."
N,0.900398406374502,"We train the linear model from section 3 with default-ERM and ℓ2-regularization — implemented as
weight decay in the AdamW optimizer [Loshchilov and Hutter, 2019] — on data from eq. (1) with"
N,0.901394422310757,"Figure 4: Default-ERM with ℓ2-regularization with a penalty coefﬁcient of λ = 10−8 achieves a
test accuracy of ≈50% , outperforming default-ERM. The right panel shows that ℓ2-regularization
leads to lower test loss on the minority group, meaning that the regularization does mitigate some
overﬁtting. However, the difference between the shortcut and leftover test losses shows that the
model still relies on the shortcut."
N,0.9023904382470119,"Figure 5: Comparing log-loss with MARG-CTRL as functions of the margin. Each MARG-CTRL
loss has a ""bump"" which characterizes the loss function’s transition from a decreasing function of
the margin to an increasing one. These bumps push models to have uniform margins because the
loss function’s derivative after the bump is negative which discourages large margins. The hyperpa-
rameters (temperature in σ-damp or function output target in MARG-LOG.) affect the location of the
bump and the slopes of the function on either side of the bump."
N,0.9033864541832669,"d = 800, B = 10, n = 1000. Figure 4 plots accuracy and losses for the ℓ2-regularized default-ERM
with the penalty coefﬁcient set to 10−8; it shows that ℓ2-regularization leads default-ERM to build
models that only achieve ≈50% test accuracy."
N,0.9043824701195219,"For smaller penalty coefﬁcients, default-ERM performs similar to how it does without regularization,
and for larger ones, the test accuracy gets worse than default-ERM without regularization. We give
an intuitive reason for why larger ℓ2 penalties may lead to larger reliance on the shortcut feature.
Due to the scaling factor B = 10 in the synthetic experiment, for a ﬁxed norm budget, the model
achieves lower loss when using the shortcut and noise compared to using the stable feature. In turn,
heavy ℓ2-regularization forces the model to rely more on the shortcut to avoid the cost of larger
weight needed by the model to rely on the stable feature and the noise."
N,0.9053784860557769,"B.2
Margin control (MARG-CTRL)"
N,0.9063745019920318,"In ﬁg. 5, we plot the different MARG-CTRL losses along with log-loss. Each MARG-CTRL loss has a
""bump"" which characterizes the loss function’s transition from a decreasing function of the margin
to an increasing one. These bumps push models to have uniform margins because the loss function’s
derivative after the bump is negative which discourages large margins. The hyperparameters — like
temperature in σ-damp or function output target in MARG-LOG — affect the location of the bump
and the slopes of the function on either side of the bump."
N,0.9073705179282868,"B.3
MARG-CTRL on a linear model"
N,0.9083665338645418,"In ﬁg. 6, we compare default-ERM to σ-stitch. In ﬁg. 7 and ﬁg. 8, compare SD and MARG-LOG
respectively to default-ERM. The left panel of all ﬁgures shows that MARG-CTRL achieves better
test accuracy than default-ERM, while the right most panel shows that the test loss is better on the
leftover group using MARG-CTRL. Finally, the middle panel shows the effect of controlling margins"
N,0.9093625498007968,"Figure 6: A linear trained with σ-stitch depend on the perfect stable feature to achieve perfect test
accuracy, unlike default-ERM. The middle panel shows that σ-stitch does not let the loss on the
training shortcut group to go to zero, unlike default-ERM, and the right panel shows the test leftover
group loss is better."
N,0.9103585657370518,"Figure 7: A linear model trained with SD depend on the perfect stable feature to achieve perfect test
accuracy whereas default-ERM performs worse than random chance. The middle panel shows that
SD does not let the loss on the training shortcut group to go to zero, unlike vanilla default-ERM, and
the right panel shows the test-loss is better for the leftover group."
N,0.9113545816733067,"in training; namely, the margins on the training data do not go to ∞, evidenced by the training
loss being bounded away from 0. Depending on the shortcut feature leads to different margins and
therefore test losses between the shortcut and leftover groups; the right panel in each plot shows that
the the test losses on both groups reach similar values, meaning MARG-CTRL mitigates dependence
on the shortcut. While default-ERM fails to perform better than chance (50%) even after 100, 000
epochs (see ﬁg. 1), MARG-CTRL mitigates shortcut learning within 5000 epochs and achieves 100%
test accuracy."
N,0.9123505976095617,"B.4
MARG-CTRL vs. default-ERM with a neural network"
N,0.9133466135458167,"With d = 100 and B = 10 in eq. (1), we train a two layer neural network on 3000 samples from the
training distribution. The two layer neural network has a 200 unit hidden layer that outputs a scalar.
Figure 9 shows that a neural network trained via default-ERM fails to cross 50% test accuracy even
after 40, 000 epochs, while achieving less than 10−10 in training loss."
N,0.9143426294820717,"Figure 8: A linear model trained with MARG-LOG depend on the perfect stable feature to achieve
perfect test accuracy whereas default-ERM performs worse than random chance. The middle panel
shows that MARG-LOG does not let the loss on the training shortcut group to go to zero, unlike
default-ERM, and the right panel shows the test-loss is better for the leftover group."
N,0.9153386454183267,(a) Average accuracy and loss curves.
N,0.9163346613545816,(b) Accuracy and loss on shortcut and leftover groups.
N,0.9173306772908366,"Figure 9: Training a two-layer neural network with default-ERM on data from eq. (1). The model achieves
100% train accuracy but < 40% test accuracy even after 40, 000 epochs. The plot below zooms in on the ﬁrst
4000 epochs and shows that the model drives down loss on the test shortcut groups but not on the test leftover
group. This shows that the model uses the shortcut to classify the shortcut group and noise for the leftover."
N,0.9183266932270916,"In ﬁg. 10, we compare default-ERM to σ-stitch. In ﬁg. 12 and ﬁg. 13, compare SD and MARG-LOG
respectively to default-ERM. The left panel of all ﬁgures shows that MARG-CTRL achieves better
test accuracy than default-ERM, while the right most panel shows that the test loss is better on the
leftover group using MARG-CTRL. Finally, the middle panel shows the effect of controlling margins
in training; namely, the margins on the training data do not go to ∞, evidenced by the training loss
being bounded away from 0."
N,0.9193227091633466,"Figure 10: A neural network trained with σ-stitch depend on the perfect stable feature to achieve
perfect test accuracy, unlike default-ERM. The middle panel shows that σ-stitch does not let the loss
on the training shortcut group to go to zero, unlike default-ERM, and the right panel shows the test
leftover group loss is better."
N,0.9203187250996016,"B.5
Spectral decoupling for a linear model on the linear DGP in eq. (1)."
N,0.9213147410358565,"We ﬁrst show that a linear classiﬁer trained with SD achieves 100% test accuracy while default-
ERM performs worse than chance on the test data; so, SD builds models with more dependence on
the stable perfect feature, compared to Empirical Risk minimization (ERM). Next, we outline the
assumptions for the gradient starvation (GS) regime from Pezeshki et al. [2021] and then instantiate
it for a linear model under the data generating process in eq. (1), showing that the assumptions for
the GS-regime are violated."
N,0.9223107569721115,"Figure 7 shows the results of training a linear model with SD on training data of size 1000 sampled as
per eq. (1) from pρ=0.9 with d = 300; the test data also has a 1000 samples but comes from pρ=0.1."
N,0.9233067729083665,"Figure 11: A neural network trained with σ-damp depend on the perfect stable feature to achieve
perfect test accuracy whereas default-ERM performs worse than random chance. The middle panel
shows that σ-damp does not let the loss on the training shortcut group to go to zero, unlike vanilla
default-ERM, and the right panel shows the test-loss is better for the leftover group."
N,0.9243027888446215,"Figure 12: A neural network trained with SD depend on the perfect stable feature to achieve perfect
test accuracy whereas default-ERM performs worse than random chance. The middle panel shows
that SD does not let the loss on the training shortcut group to go to zero, unlike vanilla default-ERM,
and the right panel shows the test-loss is better for the leftover group."
N,0.9252988047808764,"Figure 7 shows that SD builds models with improved dependence on the perfect stable feature, as
compared to ERM, to achieve 100% test accuracy."
N,0.9262948207171314,"B.5.1
The linear example in Equation (1) violates the gradient starvation regime."
N,0.9272908366533864,"Background on Pezeshki et al. [2021].
With the aim of explaining why ERM-trained neural net-
works depend more on one feature over a more informative one, Pezeshki et al. [2021] derive solu-
tions to ℓ2-regularized logistic regression in the NTK; they let the regularization coefﬁcient be small
enough for the regularized solution to be similar in direction to the unregularized solution. Given
n samples yi, xi, let Y be a diagonal matrix with the labels on its diagonal, X be a matrix with
xi as its rows, and ˆy(X, θ) = fθ(X) be the n-dimensional vector of function outputs where each
element is ˆyi = fθ(xi). In gradient-based training in the NTK regime, the vector of function outputs
of the network with parameters θ can be approximated as ˆy = Φ0θ, where Φ0 is the neural-tangent-
random-feature (NTRF) matrix at initialization:"
N,0.9282868525896414,"Φ0 = ∂ˆy(X, θ0) ∂θ0"
N,0.9292828685258964,"To deﬁne the features, the strength (margin) of each feature, and how features appear in each sample,
Pezeshki et al. [2021] compute the singular value decomposition (SVD) of the NTRF Φ0 multiplied
by the diagonal-label matrix Y:"
N,0.9302788844621513,"YΦ0 = USV⊤.
(46)"
N,0.9312749003984063,"The rows of V are features, the diagonal elements of S are the strengths of each feature and the ith
row of U denotes how each feature appears in the NTRF representation of the ith sample."
N,0.9322709163346613,"To study issues with the solution to ℓ2-regularized logistic regression, Pezeshki et al. [2021] deﬁne
the gradient starvation (GS) regime. Under the GS regime, they assume U is a perturbed identity
matrix that is also unitary: for a small constant δ << 1, such a matrix has all diagonal elements
√"
N,0.9332669322709163,1 −δ2 and the rest of the elements are of the order δ such that the rows have unit ℓ2-norm.
N,0.9342629482071713,"Figure 13: A neural network trained with MARG-LOG depend on the perfect stable feature to achieve
perfect test accuracy whereas default-ERM performs worse than random chance. The middle panel
shows that MARG-LOG does not let the loss on the training shortcut group to go to zero, unlike
default-ERM, and the right panel shows the test-loss is better for the leftover group."
N,0.9352589641434262,"The GS regime is violated in eq. (1).
When fθ is linear, fθ(x) = θ⊤x, the NTRF matrix is"
N,0.9362549800796812,"∂ˆy(X, θ0) ∂θ0"
N,0.9372509960159362,= ∂Xθ0 ∂θ0 = X.
N,0.9382470119521913,"In this case, let us look at an implication of U being a perturbed identity matrix that is also unitary,
as Pezeshki et al. [2021] assume. With (ui)⊤as the ith row of U, the transpose of ith sample can
be written as (xi)⊤= (ui)⊤SV. Pezeshki et al. [2021] assume that δ << 1 in that the off-diagonal
terms of U are small perturbations such that off-diagonal terms of U(S2 + λI)U⊤have magnitude
much smaller than 1, meaning that the terms |(ui)⊤S2(uj) + λ| << 1 for i ∕= j and positive and
small λ << 1. Then,"
N,0.9392430278884463,"|yiyj(xi)⊤xj| = |(xi)⊤xj|
(47)"
N,0.9402390438247012,"= |(ui)⊤SV⊤VSuj|
(48)"
N,0.9412350597609562,"= |(ui)⊤S2uj|
(49)
<< 1
(50)"
N,0.9422310756972112,"In words, this means that any two samples xi, xj are nearly orthogonal. Now, for samples from
eq. (1), for any i, j such that zj = zi and yi = yj,"
N,0.9432270916334662,󰀏󰀏(xi)⊤xj󰀏󰀏=
N,0.9442231075697212,"󰀏󰀏B2zizj + yiyj + (δi)⊤δj󰀏󰀏≥|100 + 1 + (δi)⊤δj|
(51)"
N,0.9452191235059761,"As δ are isotropic Gaussian vectors, around half the pairs i, j will have (δi)⊤δj > 0 meaning
󰀏󰀏(xi)⊤xj󰀏󰀏> 101. This lower bound implies that U is not a perturbed identity matrix for sam-
ples from eq. (1). This violates the setup of the gradient starvation regime from [Pezeshki et al.,
2021]."
N,0.9462151394422311,"Thus, the linear DGP in eq. (1) does not satisfy the conditions for the GS regime that is proposed
in [Pezeshki et al., 2021]. The GS regime blames the coupled learning dynamics for the different
features as the cause for default-ERM-trained models depending more on the less informative feature.
Pezeshki et al. [2021] derive spectral decoupling (SD) to avoid coupling the training dynamics, which
in turn can improve a model’s dependence on the perfect feature. SD adds a penalty to the function
outputs which Pezeshki et al. [2021] show decouples training dynamics for the different features as
deﬁned by the NTRF matrix:"
N,0.9472111553784861,"ℓSD(y, fθ(x)) = log(1 + exp(yfθ)) + λ|fθ(x)|2"
N,0.9482071713147411,"As eq. (1) lies outside the GS regime, the success of SD on data from eq. (1) cannot be explained as
a consequence of avoiding the coupled training dynamics in the GS regime Pezeshki et al. [2021].
However, looking at SD as MARG-CTRL, the success of SD, as in ﬁg. 7, is explained as a consequence
encouraging uniform margins."
N,0.9492031872509961,"An example of perfect test accuracy even with dependence on the shortcut.
In ﬁg. 14, we train
a linear model with default-ERM on data from eq. (1), showing that even when shortcut dependence"
N,0.950199203187251,"Figure 14: With d = 200 and n = 1000, a linear classiﬁer can still depend on the shortcut feature and
achieve 100% test accuracy. Nagarajan et al. [2021] consider linearly separable data and formalize geometric
properties of the data that make max-margin classiﬁers give non-zero weight to the shortcut feature (wz > 0).
In their example, it is unclear when wz > 0 leads to poor accuracy in the leftover group because Nagarajan
et al. [2021] do not separate the model’s dependence on the stable feature from the dependence on noise. The
example here gives an example where wz > 0 but test accuracy is 100%. demonstrating that guarantees on
test leftover group error require comparing wy and wz; the condition wz > 0 alone is insufﬁcient."
N,0.951195219123506,"is non-zero, test leftover group accuracy can be 100%. Nagarajan et al. [2021] consider linearly
separable data and formalize geometric properties of the data that make max-margin classiﬁers give
non-zero weight to the shortcut feature (wz > 0). In their example, it is unclear when wz > 0 leads
to poor accuracy in the leftover group because Nagarajan et al. [2021] do not separate the model’s
dependence on the stable feature from the dependence on noise. The example in ﬁg. 14 gives an
example where wz > 0 but test accuracy is 100%, demonstrating that guarantees on test leftover
group error require comparing wy and wz; the condition wz > 0 alone is insufﬁcient. In contrast,
theorem 1 characterizes cases where leftover group accuracy is worse than random even without
overparameterization."
N,0.952191235059761,"B.6
Experimental details"
N,0.953187250996016,"B.6.1
Background on Just Train Twice (JTT) and Correct-n-Contrast (CNC)"
N,0.954183266932271,"JTT
Liu et al. [2021] develop JTT with the aim of building models robust to subgroup shift, where
the mass of disjoint subgroups of the data changes between training and test times. To work without
training group annotations, JTT assumes ERM builds models with high worst-group error. With
this assumption, JTT ﬁrst builds an ""identiﬁcation"" model via ERM to pick out samples that are
misclassiﬁed due to model’s dependence on the shortcut. Then, JTT trains a second model again via
ERM on the same training data with the loss for the misclassiﬁed samples upweighted (by constant
λ). As Liu et al. [2021] point out, the number of epochs to train the identiﬁcation model and the
upweighting constant are hyperparameters that require tuning using group annotations. As Liu et al.
[2021], Zhang et al. [2022] show that JTT and CNC outperforms LFF and other two-stage shortcut-
mitigating methods ([Zhang et al., 2022]), so we do not compare against them."
N,0.9551792828685259,"Correct-n-Contrast (CNC)
In a fashion similar to JTT, the ﬁrst stage of CNC is to train a model
with regularized ERM to predict based on spurious attributes, i.e. shortcut features. Zhang et al.
[2022] develop a contrastive loss to force the model to have similar representations across samples
that share a label but come from different groups (approximately inferred by the ﬁrst-stage ERM
model). Formally, the ﬁrst-stage model is used to approximate the spurious attributes in one of two
ways: 1) predict the label with the model, 2) cluster the representations into as many clusters as
there are classes, and then use the cluster identity. The latter technique was ﬁrst proposed in [Sohoni
et al., 2020]. For an anchor sample (yi, xi) of label y = y, positive samples Pi are those than have
the same label but have the predicted spurious attribute is a different value: ˆz ∕= y. Negatives Ni are
those that have a different label but the spurious attribution is the same: ˆz = y. For a temperature
parameter τ and representation function rθ, the per-sample contrastive loss for CNC is:"
N,0.9561752988047809,"ℓcont(rθ, i) = Exp∼Pi 󰀥"
N,0.9571713147410359,"−log
exp(rθ(xi)⊤rθ(xp)/τ)
󰁓"
N,0.9581673306772909,n∈Ni exp 󰀃
N,0.9591633466135459,rθ(xi)⊤rθ(xn)/τ 󰀄 + 󰁓
N,0.9601593625498008,p∈Pi exp 󰀃
N,0.9611553784860558,rθ(xi)⊤rθ(xp)/τ 󰀄 󰀦 .
N,0.9621513944223108,"The samples i are called anchors. For a scalar λ to trade off between contrastive and predictive loss,
the overall per-sample loss in the second-stage in CNC is"
N,0.9631474103585658,"λℓcont(rθ, i) + (1 −λ)ℓlog−loss(yiw⊤rθ(xi))."
N,0.9641434262948207,"CNC uses hyperparameters informed by dataset-speciﬁc empirical results from prior work.
The original implementation of CNC from Zhang et al. [2022] uses speciﬁc values of ﬁrst-stage
hyperparameters like weight decay and early stopping epoch for each dataset by using empirical
results from prior work [Sagawa et al., 2020a, Liu et al., 2021]. The prior work ﬁnds weight-decay
and early stopping epoch which lead default-ERM models to achieve low test worst-group accuracy,
implying that the model depends on the spurious attribute. This means the ﬁrst-stage models built
in CNC are pre-selected to pay attention to the spurious attributes. For example, [Zhang et al., 2022]
point out that the ﬁrst-stage model they use for Waterbirds predicts the spurious feature with an
accuracy of 94.7%."
N,0.9651394422310757,"Without using dataset-speciﬁc empirical results from prior work, choosing LR and WD requires
validating through the whole CNC procedure. We let CNC use the same LR and WD for both stages
and then validate the choice using validation performance of the second-stage model. This choice
of hyperparameter validation leads to a similar number of validation queries for all methods that
mitigate shortcuts."
N,0.9661354581673307,"B.6.2
Training details"
N,0.9671314741035857,"Variants of MARG-CTRL to handle label imbalance.
The three datasets that we use in our ex-
periments — Waterbirds, CelebA, and Civilcomments — all have an imbalanced (non-uniform)
marginal distribution over the label; for each dataset,"
N,0.9681274900398407,"max
class∈{−1,1} p(y = class) > 0.75."
N,0.9691235059760956,"When there is sufﬁciently large imbalance, restricting the margins on all samples could bias the
training to reduce loss on samples in the most-frequent class ﬁrst and overﬁt on the rest of the
samples. This could force a model to predict the most frequent class for all samples, resulting in
high worst-group error."
N,0.9701195219123506,"To prevent such a failure mode, we follow [Pezeshki et al., 2021] and deﬁne variants of σ-damp,
MARG-LOG, and σ-stitch that have either 1) different maximum margins for different classes or 2)
different per-class loss values for the same margin value. Mechanically, these variants encourage
uniform margins within each class, thus encouraging the model to rely less on the shortcut feature.
We give the variants here for labels taking values in {−1, 1}:"
N,0.9711155378486056,"1. With per-class temperatures T−1, T1 > 0 the variant of σ-damp is"
N,0.9721115537848606,with fθ = wf
N,0.9731075697211156,"⊤rθ(x),
ℓσ-damp(y, fθ) = ℓlog [Ty ∗1.278yfθ (1 −σ (1.278 ∗yfθ))]
The 1.278 comes in to make sure the maximum input to log-loss occurs at fθ = 1. How-
ever, due to the different temperatures T1 ∕= T−1, achieving the same margin on all samples
produces lower loss on the class with the larger temperature."
N,0.9741035856573705,"2. With per-class temperatures T−1, T1 > 0 the variant of σ-stitch is"
N,0.9750996015936255,with fθ = wf
N,0.9760956175298805,"⊤rθ(x),
ℓσ-stitch(yfθ) = ℓlog (Ty [
1[yfθ(x) < 1] × yfθ(x) + 1[yfθ(x) > 1] × (2 −yfθ(x)) ])"
N,0.9770916334661355,"3. With per-class function output targets γ−1, γ1 > 0 the variant of MARG-LOG is"
N,0.9780876494023905,with fθ = wf
N,0.9790836653386454,"⊤rθ(x),"
N,0.9800796812749004,ℓMARG-LOG(yfθ) = ℓlog(yfθ) + λ log(1 + |fθ −γy|2).
N,0.9810756972111554,"These per-class variants are only for training; at test time, the predicted label is sign(fθ)."
N,0.9820717131474104,"Details of the vision and language experiments.
We use the same datasets from Liu et al.
[2021], downloaded via the scripts in the code from [Idrissi et al., 2022]; see [Idrissi et al., 2022]
for sample sizes and the group proportions. For the vision datasets, we ﬁnetune a resnet50 from
Imagenet-pretrained weights and for Civilcomments, we ﬁnetune a BERT model."
N,0.9830677290836654,"Figure 15: Images mis-classiﬁed by a model trained on CelebA data with equal group sizes, i.e.
without a shortcut. Samples with blonde as the true label have a white strip at the bottom while
samples with non-blonde as the true label have a black strip at the bottom. The ﬁgure demonstrates
that many images with blonde people in the image have the non-blonde label, thus demonstrating
label noise. For example, see a blonde man in the ﬁrst row that is labelled non-blonde and a non-
blonde lady in the third row that is lablled blonde. Yet, MARG-CTRL improves over ERM for many
LR and WD combinations; see ﬁg. 16."
N,0.9840637450199203,"Optimization details.
For all methods and datasets, we tune over the following weight decay
(WD) parameters: 10−1, 10−2, 10−3, 10−4 For the vision datasets, we tune learning rate (LR) over
10−4, 10−5 and for CivilComments, we tune over 10−5, 10−6. For CivilComments, we use the
AdamW optimizer while for the vision datasets, we use the Adam optimizer; these are the standard
optimizers for their respective tasks [Puli et al., 2022, Gulrajani and Lopez-Paz, 2021]. We use a
batch size of 128 for both CelebA and Waterbirds, and train for 20 and 100 epochs respectively. For
CivilComments we train for 10 epochs with a batch size of 16."
N,0.9850597609561753,"Per-method Hyperparameters.
Like in [Pezeshki et al., 2021], the per-class temperatures
T−1, T1 for σ-damp and σ-stitch, and the function output targets γ−1, γ1 for MARG-LOG are hyper-
parameters that we tune using the worst-group accuracy or label-balanced average accuracy com-
puted on the validation dataset, averaged over 2 seeds."
N,0.9860557768924303,"1. For σ-stitch, we select from T−1 ∈{1, 2} and T1 ∈{2, 4, 8, 12} such that T1 > T−1."
N,0.9870517928286853,"2. For σ-damp, we search over T−1 ∈{1, 2} and T1 ∈{2, 4} such that T1 > T−1."
N,0.9880478087649402,"3. For SD and MARG-LOG, we search over γ−1 ∈{−1, 0, 1} and γ1 ∈{1, 2, 2.5, 3} for the"
N,0.9890438247011952,"image datasets and γ1 ∈{1, 2} for the text dataset, and the penalty coefﬁcient is set to be
λ = 0.1"
N,0.9900398406374502,"4. For JTT, we search over the following parameters: the number of epochs T ∈{1, 2} for"
N,0.9910358565737052,"CelebA and Civilcomments and T ∈{10, 20, 30} for Waterbirds, and the upweighting
constant λ ∈{20, 50, 100} for the vision datasets and λ ∈{4, 5, 6} for Civilcomments."
N,0.9920318725099602,"5. For CNC, we search over the same hyperparameter as [Zhang et al., 2022] : the temperature"
N,0.9930278884462151,"in τ ∈{0.05, 0.1}, the contrastive weight λ ∈{0.5, 0.75}, and the gradient accumulation
steps s ∈{32, 64}. For the language task in Civilcomments, we also try one additional
s = 128."
N,0.9940239043824701,"B.7
MARG-CTRL improves over default-ERM on CelebA even without the stable feature
being perfect."
N,0.9950199203187251,"CelebA is a perception task in that the stable feature is the color of the hair in the image. But
unlike the synthetic experiments, MARG-CTRL does not achieve a 100% test accuracy on CelebA.
We investigated this and found that CelebA in fact has some label noise."
N,0.9960159362549801,"We trained a model via the MARG-CTRL method σ-damp on CelebA data with no shortcut; this
data is constructed by subsampling the groups to all equal size, (5000 samples). This achieves a
test worst-group accuracy of 89%. We visualized the images that were misclassiﬁed by this model
and found that many images with blond-haired people were classiﬁed as having non-blonde hair.
Figure 15 shows 56 misclassiﬁed images where samples with blonde as the true label have a white
strip at the bottom while samples with non-blonde as the true label have a black strip at the bottom.
The ﬁgure shows that images with blonde people can have the non-blonde label, thus demonstrating
label noise. Thus, MARG-CTRL improves over ERM even on datasets like CelebA where the stable
features do not determine the label."
N,0.9970119521912351,"Figure 16: Test worst-group accuracy on CelebA of default-ERM and MARG-CTRL for different
values of LR and WD. Default-ERM’s performance changes more with LR and WD than MARG-
CTRL, which shows that default-ERM is more sensitive than MARG-CTRL. Only 2 combinations of
LR and WD improve ERM beyond a test worst-group accuracy of 60%, while every MARG-CTRL
method achieves more than 70% test worst-group accuracy for every combination of LR and WD."
N,0.99800796812749,"B.8
Sensitivity of ERM and MARG-CTRL to varying LR and WD"
N,0.999003984063745,"In ﬁg. 16, we compare the test worst-group accuracy of default-ERM and MARG-CTRL on CelebA,
for different values of LR and WD. There are 8 combinations of LR and WD for which ERM is run.
For each combination of LR and WD, the hyperparameters of the MARG-CTRL method (values of
λ, T, v) are tuned using validation group annotations, and the test worst-group accuracy corresponds
to the best method hyperparameters. Default-ERM’s performance changes more with LR and WD
than MARG-CTRL, which shows that default-ERM is more sensitive than MARG-CTRL. Only 2 com-
binations of LR and WD improve ERM beyond a test worst-group accuracy of 60%, while every
MARG-CTRL method achieves more than 70% test worst-group accuracy for every combination of
LR and WD."
