Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00411522633744856,"The discovery of the disentanglement properties of the latent space in GANs mo-
tivated a lot of research to ﬁnd the semantically meaningful directions on it. In
this paper, we suggest that the disentanglement property is closely related to the
geometry of the latent space. In this regard, we propose an unsupervised method
for ﬁnding the semantic-factorizing directions on the intermediate latent space of
GANs based on the local geometry. Intuitively, our proposed method, called Lo-
cal Basis, ﬁnds the principal variation of the latent space in the neighborhood of
the base latent variable. Experimental results show that the local principal vari-
ation corresponds to the semantic factorization and traversing along it provides
strong robustness to image traversal. Moreover, we suggest an explanation for
the limited success in ﬁnding the global traversal directions in the latent space,
especially W-space of StyleGAN2. We show that W-space is warped globally by
comparing the local geometry, discovered from Local Basis, through the metric
on Grassmannian Manifold. The global warpage implies that the latent space is
not well-aligned globally and therefore the global traversal directions are bound
to show limited success on it."
INTRODUCTION,0.00823045267489712,"1
INTRODUCTION"
INTRODUCTION,0.012345679012345678,"Generative Adversarial Networks (GANs, Goodfellow et al. (2014)), such as ProGAN (Karras et al.,
2018), BigGAN (Brock et al., 2018), and StyleGANs (Karras et al., 2019; 2020b;a), have shown
tremendous performance in generating high-resolution photo-realistic images that are often indistin-
guishable from natural images. However, despite several recent efforts (Goetschalckx et al., 2019;
Jahanian et al., 2019; Plumerault et al., 2020; Shen et al., 2020) to investigate the disentanglement
properties (Bengio et al., 2013) of the latent space in GANs, it is still challenging to ﬁnd meaningful
traversal directions in the latent space corresponding to the semantic variation of an image."
INTRODUCTION,0.01646090534979424,"The previous approaches to ﬁnd the semantic-factorizing directions are categorized into local and
global methods. The local methods (e.g. Ramesh et al. (2018), Latent Mapper in StyleCLIP (Patash-
nik et al., 2021), and attribute-conditioned normalizing ﬂow in StyleFlow (Abdal et al., 2021))
suggest a sample-wise traversal direction. By contrast, the global methods, such as GANSpace
(H¨ark¨onen et al., 2020) and SeFa (Shen & Zhou, 2021), propose a global direction for the particular
semantics (e.g. glasses, age, and gender) that works on the entire latent space. Throughout this pa-
per, we refer to these global methods as the global basis. These global methods showed promising
results. However, these methods are successful on the limited area, and the image quality is sensi-
tive to the perturbation intensity. In fact, if a latent space does not satisfy the global disentanglement
property itself, all global methods are bound to show a limited performance on it. Nevertheless, to
the best of our knowledge, the global disentanglement property of a latent space has not been inves-
tigated except for the empirical observation of generated samples. In this regard, we need a local
method that describes the local disentanglement property and an evaluation scheme for the global
disentanglement property from the collected local information."
INTRODUCTION,0.0205761316872428,∗Equal contribution
INTRODUCTION,0.024691358024691357,Published as a conference paper at ICLR 2022
INTRODUCTION,0.02880658436213992,"In this paper, we suggest that the semantic property of the latent space in GANs (i.e. disentangle-
ment of semantics and image collapse) is closely related to its geometry, because of the sample-wise
optimization nature of GANs. In this respect, we propose an unsupervised method to ﬁnd a traver-
sal direction based on the local structure of the intermediate latent space W, called Local Basis (Fig
1a). We approximate W with its submanifold representing its local principal variation, discovered in
terms of the tangent space TwW. Local Basis is deﬁned as an ordered basis of TwW corresponding
to the approximating submanifold. Moreover, we show that Local Basis is obtained from the simple
closed-form algorithm, that is the singular vectors of the Jacobian matrix of the subnetwork. The ge-
ometric interpretation of Local Basis provides an evaluation scheme for the global disentanglement
property through the global warpage of the latent manifold. Our contributions are as follows:"
INTRODUCTION,0.03292181069958848,"1. We propose Local Basis, a set of traversal directions that can reliably traverse without
escaping from the latent space to prevent image collapse. The latent traversal along Local
Basis corresponds to the local coordinate mesh of local-geometry-describing submanifold.
2. We show that Local Basis leads to stable variation and better semantic factorization than
global approaches. This result veriﬁes our hypothesis on the close relationship between the
semantic and geometric properties of the latent space in GANs.
3. We propose Iterative Curve-Traversal method, which is a way to trace the latent space in
the curved trajectory. The trajectory of the images with this method shows a more stable
variation compared to the linear traversal.
4. We introduce the metrics on the Grassmannian manifold to analyze the global geometry of
the latent space through Local Basis. Quantitative analysis demonstrates that the W-space
of StyleGAN2 is still globally warped. This result provides an explanation for the limited
success of the global basis and proves the importance of local approaches."
RELATED WORK,0.037037037037037035,"2
RELATED WORK"
RELATED WORK,0.0411522633744856,"Style-based Generators.
In recent years, GANs equipped with style-based generators (Karras
et al., 2019; 2020b) have shown state-of-the-art performance in high-ﬁdelity image synthesis. The
style-based generator consists of two parts: a mapping network and a synthesis network. The map-
ping network encodes the isotropic Gaussian noise z ∈Z to an intermediate latent vector w ∈W.
The synthesis network takes w and generates an image while controlling the style of the image
through w. Here, W-space is well known for providing a better disentanglement property compared
to Z (Karras et al., 2019). However, there is still a lack of understanding about the effect of latent
perturbation in a speciﬁc direction on the output image."
RELATED WORK,0.04526748971193416,"Latent Traversal for Image Manipulation.
The impressive success of GANs in producing high-
quality images has led to various attempts to understand their latent space. Early approaches (Rad-
ford et al., 2016; Upchurch et al., 2017) show that vector arithmetic on the latent space for the se-
mantics holds, and StyleGAN (Karras et al., 2019) shows that mixing two latent codes can achieve
style transfer. Some studies have investigated the supervised learning of latent directions while as-
suming access to the semantic attributes of images (Goetschalckx et al., 2019; Jahanian et al., 2019;
Shen et al., 2020; Yang et al., 2021; Abdal et al., 2021). In contrast to these supervised methods,
some recent studies have suggested novel approaches that do not use the prior knowledge of training
dataset, such as the labels of human facial attributes. In Voynov & Babenko (2020), an unsupervised
optimization method is proposed to jointly learn a candidate matrix and a corresponding reconstruc-
tor, which identiﬁes the semantic direction in the matrix. GANSpace (H¨ark¨onen et al., 2020) ﬁnds
a global basis for W in StyleGAN using a PCA, enabling a fast image manipulation. SeFa (Shen &
Zhou, 2021) focuses on the ﬁrst weight parameter right after the latent code, suggesting that it con-
tains essential knowledge of an image variation. SeFa proposes singular vectors of the ﬁrst weight
parameter as meaningful global latent directions. StyleCLIP (Patashnik et al., 2021) achieves a state-
of-the-art performance in the text-driven image manipulation of StyleGAN. StyleCLIP introduces
an additional training to minimize the CLIP loss (Radford et al., 2021)."
RELATED WORK,0.04938271604938271,"Jacobian Decomposition.
Some works use the Jacobian matrix to analyze the latent space of
GAN (Zhu et al., 2021; Wang & Ponce, 2021; Chiu et al., 2020; Ramesh et al., 2018). However,
these methods focus on the Jacobian of the entire model, from the input noise z to the output image.
Ramesh et al. (2018) suggested the right singular vectors of the Jacobian as local disentangled direc-"
RELATED WORK,0.053497942386831275,Published as a conference paper at ICLR 2022
RELATED WORK,0.05761316872427984,"(a) Concept Diagram of Local Basis
(b) Latent Traversal Methods"
RELATED WORK,0.06172839506172839,"Figure 1: (a) Concept diagram of Local Basis. The global basis reﬂects the global variation of
latent space. Hence, traversing along the global basis may result in the escape from the latent space
(shaded region). On the other hand, Local Basis closely follows the latent space. (b) Comparison of
Latent Traversal Methods (Global methods: GANSpace (H¨ark¨onen et al., 2020) and SeFa (Shen
& Zhou, 2021), Local methods: Ramesh et al. (2018) and Local Basis (Ours))"
RELATED WORK,0.06584362139917696,"tions in the Z space. Zhu et al. (2021) proposed a latent perturbation vector that can change only a
particular area of the image. The perturbation vector is discovered by taking the principal vector of
the Jacobian to the target area and projecting it into the null space of the Jacobian to the complemen-
tary region. On the other hand, our Local Basis utilizes the Jacobian matrix of the partial network,
from the input noise z to the intermediate latent code w, and investigates the black-box intermedi-
ate latent space from it. The top-k Local Basis corresponds to the best-local-geometry-describing
submanifolds. This intuition leads to exploiting Local Basis to assess the global geometry of the
intermediate latent space."
TRAVERSING A CURVED LATENT SPACE,0.06995884773662552,"3
TRAVERSING A CURVED LATENT SPACE"
TRAVERSING A CURVED LATENT SPACE,0.07407407407407407,"In this section, we introduce a method for ﬁnding a local-geometry-aware traversal direction in the
intermediate latent space W. The traversal direction is refered to as the Local Basis at w ∈W. In
addition, we evaluate the proposed Local Basis by observing how the generated image changes as
we traverse the intermediate latent variable. Throughout this paper, we assess Local Basis of the W-
space in StyleGAN2 (Karras et al., 2020b). However, our methodology is not limited to StyleGAN2.
See appendix for the results on StyleGAN (Karras et al., 2019) and BigGAN (Brock et al., 2018)."
FINDING A LOCAL BASIS,0.07818930041152264,"3.1
FINDING A LOCAL BASIS"
FINDING A LOCAL BASIS,0.0823045267489712,"Given a pretrained GAN model M : Z →X, from the input noise space Z to the image space
X, we choose the intermediate layer ˜
W to discover Local Basis. We refer to the former part of the
GAN model as the mapping network f : Z →˜
W. The image of the mapping network is denoted as
W = f(Z) ⊂˜
W. The latter part, a non-linear mapping from ˜
W to the image space X, is denoted
by G : ˜
W →X. Local Basis at w ∈W is deﬁned as the basis of the tangent space TwW. This
basis can be interpreted as a local-geometry-aware linear traversal direction starting from w."
FINDING A LOCAL BASIS,0.08641975308641975,"To deﬁne the tangent space of the intermediate latent space W properly, we assume that W is a
differentiable manifold. Note that the support of the isotropic Gaussian prior Z = RdZ and the
ambient space ˜
W = Rd ˜
W are already differentiable manifolds. The tangent space at w, denoted by
TwW, is a vector space consisting of tangent vectors of curves passing through point w. Explicitly,"
FINDING A LOCAL BASIS,0.09053497942386832,"TwW = { ˙γ(0) | γ : (−ϵ, ϵ) →W, γ(0) = w, for ϵ > 0}.
(1)"
FINDING A LOCAL BASIS,0.09465020576131687,"Then, the differentiable mapping network f gives a linear map dfz between the two tangent spaces
TzZ and Tw ˜
W where w = f(z)."
FINDING A LOCAL BASIS,0.09876543209876543,"dfz : TzZ −→TwW ,−→Tw ˜
W,
˙γ(0) 7−→
˙
(f ◦γ)(0)
(2)"
FINDING A LOCAL BASIS,0.102880658436214,"We utilize the linear map dfz, called the differential of f at z, to ﬁnd the basis of TwW. Based
on the manifold hypothesis in representation learning, we posit that the latent space of the image"
FINDING A LOCAL BASIS,0.10699588477366255,Published as a conference paper at ICLR 2022
FINDING A LOCAL BASIS,0.1111111111111111,"space X in ˜
W is a lower-dimensional manifold embedded in W. In this approach, we estimate
the latent manifold as a lower-dimensional approximation of W describing its principal variations.
The approximation manifold can be obtained by solving the low-rank approximation problem of
dfz. The manifold hypothesis is supported by the empirical distribution of singular values σz
i . The
analysis is provided in Fig 9 in the appendix."
FINDING A LOCAL BASIS,0.11522633744855967,"The low-rank approximation problem has an analytic solution deﬁned by Singular Value Decompo-
sition (SVD). Because the matrix representation of dfz is a Jacobian matrix (∇zf)(z) ∈Rd ˜
W×dZ,
Local Basis is obtained as the following: For the i-th right singular vector uz
i ∈RdZ, i-th left
singular vector vw
i ∈Rd ˜
W, and i-th singular value σz
i ∈R of (∇zf)(z) with σz
1 ≥· · · ≥σz
n,"
FINDING A LOCAL BASIS,0.11934156378600823,"dfz(uz
i ) = σz
i · vw
i for ∀i,
(3)
Local Basis(w = f(z)) = {vw
i }1≤i≤n.
(4)"
FINDING A LOCAL BASIS,0.12345679012345678,"Then, the k-dimensional approximation of W around w is described as the following because Z =
RdZ (if σz
k > 0). Note that Wk
w is a submanifold1 of W corresponding to the k components of Local
Basis, i.e. TwWk
w = span{vw
i : 1 ≤i ≤k}."
FINDING A LOCAL BASIS,0.12757201646090535,"Wk
w = ( f  z +
X"
FINDING A LOCAL BASIS,0.13168724279835392,"i
ti · uz
i !"
FINDING A LOCAL BASIS,0.13580246913580246,"| ti ∈(−ϵi, ϵi), for 1 ≤i ≤k ) (5)"
FINDING A LOCAL BASIS,0.13991769547325103,"Locally afﬁne mapping network
In this paragraph, we focus on the locally afﬁne mapping net-
work f, which is one of the most widely adopted GAN structures, such as MLP or CNN layers with
ReLU or leaky-ReLU activation functions. This type of mapping network has several well-suited
properties for Local Basis.
f(z) =
X"
FINDING A LOCAL BASIS,0.1440329218106996,"p∈Ω
1z∈p (Apz + bp)
(6)"
FINDING A LOCAL BASIS,0.14814814814814814,"where Ωdenotes a partition of Z, and Ap and bp are the parameters of the local afﬁne map. With this
type of mapping network f, it is clear that the intermediate latent space W satisﬁes a differentiable
manifold property at least locally on the interior of each p ∈Ω. The region where the property may
not hold, the intersection of several closure of p’s in Ω, has measure zero in Z."
FINDING A LOCAL BASIS,0.1522633744855967,"Moreover, the Jacobian matrix (∇zf)(z) becomes a locally constant matrix. Then, the approximat-
ing manifold Wk
w (Eq 5) satisﬁes the submanifold condition, and is consistent locally for each p,
avoiding being deﬁned for each w. In addition, the linear traversal of the latent variable w along vw
i
can be described as the curve on W (Eq 7). Most importantly, these curves on W (Eq 7), starting
from w in the direction of Local Basis, corresponds to the local coordinate mesh of Wk
w."
FINDING A LOCAL BASIS,0.15637860082304528,"Traversal(w = f(z), vw
i ) : (−ϵ, ϵ) −→Z
f
−−→W,
t 7→

z + t"
FINDING A LOCAL BASIS,0.16049382716049382,"σz
i
· uz
i"
FINDING A LOCAL BASIS,0.1646090534979424,"
7→(w + t · vw
i )
(7)"
FINDING A LOCAL BASIS,0.16872427983539096,"Equivalence to Local PCA
To provide additional intuition about Local Basis, we prove the fol-
lowing proposition. The proposition shows that Local Basis is equivalent to applying a PCA on the
samples on W around w.
Proposition 1 (Equivalence to Local PCA). Consider the Local PCA problem around the base
latent variable wb = f(zb) on W, i.e. PCA of the latent variable samples w′ around wb."
FINDING A LOCAL BASIS,0.1728395061728395,"w′ = T1f(zb + c · ϵ)
with ϵ ∼N(0, I) and for some small c > 0.
(8)"
FINDING A LOCAL BASIS,0.17695473251028807,"where T1f(z) = wb + (∇zbf)(z −zb) is the linear approximation of f around zb. Then, the
principal components discovered in the Local PCA problem are equivalent to Local Basis at wb."
ITERATIVE CURVE-TRAVERSAL,0.18106995884773663,"3.2
ITERATIVE CURVE-TRAVERSAL"
ITERATIVE CURVE-TRAVERSAL,0.18518518518518517,"We suggest a natural curve-traversal that can keep track of the W-manifold and an iterative method
to implement it. We divide the long curved trajectory into small pieces and approximate each piece"
ITERATIVE CURVE-TRAVERSAL,0.18930041152263374,"1Strictly speaking, Wk
w may not satisfy the conditions of the submanifold. The injectivity of dfz on the
domain

z + P"
ITERATIVE CURVE-TRAVERSAL,0.1934156378600823,"i ti · uz
i | ti ∈(−ϵi, ϵi), for 1 ≤i ≤k
	
is a sufﬁcient condition for the submanifold. As
described below, this sufﬁcient condition is satisﬁed under the locally afﬁne mapping network f and σz
k > 0."
ITERATIVE CURVE-TRAVERSAL,0.19753086419753085,Published as a conference paper at ICLR 2022
ITERATIVE CURVE-TRAVERSAL,0.20164609053497942,Figure 2: Illustration of Iterative Curve-Traversal (for N = 2).
ITERATIVE CURVE-TRAVERSAL,0.205761316872428,"by the local curves using Local Basis. We call this curve-traversal Iterative Curve-Traversal method.
Consistent with the linear traversal method, we consider Iterative Curve-Traversal γ departing in the
direction of a Local Basis. Explicitly, for a sufﬁciently large c > 0,"
ITERATIVE CURVE-TRAVERSAL,0.20987654320987653,"γ : (−c, c) −→W,
γ(0) = w, ˙γ(0) = vw
k
for some 1 ≤k ≤dW
(9)"
ITERATIVE CURVE-TRAVERSAL,0.2139917695473251,"where {vw
i }i = Local Basis(w). We split the curve-traversal γ into N pieces γn and denote each
n-th iterate in W and Z as wn and zn for 1 ≤n ≤N. The starting point of the traversal is denoted
as the 0-th iterate w = w0, z = z0, and w0 = f(z0). (Fig 2) Note that to ﬁnd Local Basis at wn,
we need a corresponding zn ∈Z such that wn = f(zn)."
ITERATIVE CURVE-TRAVERSAL,0.21810699588477367,"Below, we describe the positive part γ+ = γ|[0,c) of Iterative Curve-Traversal. For the negative part,
we repeat the same procedure using the reversed tangent vector −vw
k . The ﬁrst step γ+
1 of Iterative
Curve-Traversal method with perturbation intensity I is as follows:"
ITERATIVE CURVE-TRAVERSAL,0.2222222222222222,"γ+
1 : [0, I/(N · σz0
k )] −→Z
f
−−→W,
t 7−→(z0 + t · uz0
k ) 7−→f(z0 + t · uz0
k )
(10)"
ITERATIVE CURVE-TRAVERSAL,0.22633744855967078,"z1 = z0 +
I
(N · σz0
k )uz0
k ,
w1 = f(z1)
(11)"
ITERATIVE CURVE-TRAVERSAL,0.23045267489711935,"Note that w1 is the endpoint of the curve γ+
1 and ˙γ+
1 (0) = vw
k . We scale the step size in Z by 1/σz0
k
to ensure each piece of curve has a similar length of (I/N). To preserve the variation in semantics
during the traversal, the departure direction of γ+
2 is determined by comparing the similarity between
the previous departure direction vw0
k
and Local Basis at w1. The above process is repeated N-times.
(The algorithm for Iterative Curve-Traversal can be found in the appendix.)"
ITERATIVE CURVE-TRAVERSAL,0.2345679012345679,"˙γ+
2 (0) = vw1
j
where
j = argmax
1≤i≤dW
|⟨vw0
k , vw1
i
⟩|
(12)"
RESULTS OF LOCAL BASIS TRAVERSAL,0.23868312757201646,"3.3
RESULTS OF LOCAL BASIS TRAVERSAL"
RESULTS OF LOCAL BASIS TRAVERSAL,0.24279835390946503,"We evaluate Local Basis by observing how the generated image changes as we traverse W-space in
StyleGAN2 and by measuring FID score for each perturbation intensity. The evaluation is based on
two criteria: Robustness and Semantic Factorization."
RESULTS OF LOCAL BASIS TRAVERSAL,0.24691358024691357,"Robustness
Fig 3 and 4 present the Robustness Test results2. In Fig 3, the traversal image of Local
Basis is compared with those of the global methods (GANSpace (H¨ark¨onen et al., 2020) and SeFa
(Shen & Zhou, 2021)) under the strong perturbation intensity of 12 along the 1st and 2nd direction of
each method. The perturbation intensity is deﬁned as the traversal path length in W. The two global
methods show severe degradation of the image compared to Local Basis. Moreover, we perform
a quantitative assessment of robustness. We measure the FID score for 10,000 traversed images
for each perturbation intensity. In Fig 4, the global methods show the relatively small FID under
the small perturbation. But, as we impose the stronger perturbation, the FID scores on the global
methods increase sharply, implying the image collapse in Fig 3. By contrast, Local Basis achieves
much smaller FID scores with and without Iterative Curve-Traversal."
RESULTS OF LOCAL BASIS TRAVERSAL,0.25102880658436216,"2Ramesh et al. (2018) is not compared because it took hours to get a traversal direction of an image. See
appendix for the Qualitative Robustness Test results of Ramesh et al. (2018)."
RESULTS OF LOCAL BASIS TRAVERSAL,0.2551440329218107,Published as a conference paper at ICLR 2022
RESULTS OF LOCAL BASIS TRAVERSAL,0.25925925925925924,"I = 0
I = 2
I = 4
I = 6
I = 8
I = 10
I = 12"
RESULTS OF LOCAL BASIS TRAVERSAL,0.26337448559670784,(a) GANSpace
RESULTS OF LOCAL BASIS TRAVERSAL,0.2674897119341564,"I = 0
I = 2
I = 4
I = 6
I = 8
I = 10
I = 12"
RESULTS OF LOCAL BASIS TRAVERSAL,0.2716049382716049,(b) SeFa
RESULTS OF LOCAL BASIS TRAVERSAL,0.2757201646090535,"(c) Local Basis (Ours)
(d) Iterative Curve-Traversal (Ours)"
RESULTS OF LOCAL BASIS TRAVERSAL,0.27983539094650206,"Figure 3: Qualitative Robustness Test on the W-space of the StyleGAN2 (Karras et al., 2020b)
trained on FFHQ. Each traversal image is generated by the linear traversal on W except for (d)
under the strong perturbation intensity I of up to 12. The intensity is linearly increased from 0 to 12
for each column. We infer the deterioration of the traversal image along the global method is due
to the escape of the latent traversal from the latent manifold. (See the appendix for the additional
Robustness Test results along the ﬁrst 10 components of Local Basis.)"
RESULTS OF LOCAL BASIS TRAVERSAL,0.2839506172839506,"0
2
4
6
8
10
12
Perturbation Intensity 0 20 40 60 80 100 FID"
RESULTS OF LOCAL BASIS TRAVERSAL,0.2880658436213992,"Without Traverse
GANSpace
SeFa
Local Basis
Iterative Curve-Traversal"
RESULTS OF LOCAL BASIS TRAVERSAL,0.29218106995884774,"0
2
4
6
8
10
12
Perturbation Intensity 0 20 40 60 80 100 120 140 FID"
RESULTS OF LOCAL BASIS TRAVERSAL,0.2962962962962963,"Without Traverse
GANSpace
SeFa
Local Basis
Iterative Curve-Traversal"
RESULTS OF LOCAL BASIS TRAVERSAL,0.3004115226337449,"Figure 4: Quantitative Robustness Test on the W-space of the StyleGAN2 (Karras et al., 2020b)
trained on FFHQ. Fr´echet Inception Distance (FID) (Heusel et al., 2017) is measured for 10,000
traversed images for each perturbation intensity. Left: 1st direction, Right: 2nd direction"
RESULTS OF LOCAL BASIS TRAVERSAL,0.3045267489711934,"We interpret the degradation of image as due to the deviation of trajectory from W. The theoretical
interpretation shows that the linear traversal along Local Basis corresponds to a local coordinate axis
on W, at least locally. Therefore, the traversal along Local Basis is guaranteed to stay close to W
even under the longer traversal. However, we cannot expect the same property on the global basis
because it is based on the global geometry. Iterative Curve-Traversal shows more stable traversal
because of its stronger tracing to the latent manifold. This further supports our interpretation."
RESULTS OF LOCAL BASIS TRAVERSAL,0.30864197530864196,"Semantic Factorization
Local Basis is discovered in terms of singular vectors of dfz. The dis-
entangled correspondence, between Local Basis and the corresponding singular vectors in the prior
space, induces a semantic-factorization in Local Basis. Fig 5 and 6 presents the semantics of the
image discovered by Local Basis. In Fig 5, we compare the semantic factorizations of Local Ba-
sis and GANSpace (H¨ark¨onen et al., 2020) for the particular semantics discovered by GANSpace.
For each interpretable traversal direction of GANSpace provided by the authors, the corresponding
Local Basis is chosen by the one with the highest cosine similarity. For a fair comparison, each
traversal is applied to the speciﬁc subset of layers in the synthesis network (Karras et al., 2020b)
provided by the authors of GANSpace with the same perturbation intensity. In particular, as we
impose the stronger perturbation (from left to right), GANSpace shows the image collapse in Fig 5a
and entanglement of semantics (Glasses + Head Raising) in Fig 5d. However, Local Basis does not
show any of those problems. Fig 6 provides additional examples of semantic factorization where the
latent traversal is applied to a subset of layers predeﬁned in StyleGAN. The subset of the layers is
selected as one of four, i.e. coarse, middle, ﬁne, or all styles. Local Basis shows decent factorization
of semantics such as Body Length of car and Age of cat in LSUN (Yu et al., 2015) in Fig 6."
RESULTS OF LOCAL BASIS TRAVERSAL,0.31275720164609055,Published as a conference paper at ICLR 2022
RESULTS OF LOCAL BASIS TRAVERSAL,0.3168724279835391,(a) Face length
RESULTS OF LOCAL BASIS TRAVERSAL,0.32098765432098764,"GANSpace
Local Basis"
RESULTS OF LOCAL BASIS TRAVERSAL,0.32510288065843623,(b) Open mouth
RESULTS OF LOCAL BASIS TRAVERSAL,0.3292181069958848,"(c) Sunlight on face
(d) Glasses"
RESULTS OF LOCAL BASIS TRAVERSAL,0.3333333333333333,"Figure 5: Comparison of Semantic Factorization between Local Basis and GANSpace on pre-
trained StyleGAN2-FFHQ. We compare the semantic-factorizing directions of GANSpace provided
by the authors (H¨ark¨onen et al., 2020) with Local Basis of the highest cosine similarity. Local Basis
factorizes semantics of image better, notably without collapsing compared to the GANSpace."
RESULTS OF LOCAL BASIS TRAVERSAL,0.3374485596707819,"(a) StyleGAN2 LSUN Car
(b) StyleGAN2 LSUN Cat"
RESULTS OF LOCAL BASIS TRAVERSAL,0.34156378600823045,"Figure 6: Additional Semantic Factorization examples of Local Basis. The examples are dis-
covered by manual inspection due to the unsupervised nature while applying the latent traversal to a
subset of the layers predeﬁned in StyleGAN (Karras et al., 2019): coarse, middle, ﬁne, and all styles.
(See the appendix for the additional examples of Semantic Factorization without layer restriction.)"
EXPLORATION INSIDE ABSTRACT SEMANTICS,0.345679012345679,"3.4
EXPLORATION INSIDE ABSTRACT SEMANTICS"
ABSTRACT,0.3497942386831276,"Abstract semantics of image often consists of several lower-level semantics. For instance, Old can be
represented as the correlated distribution of Hair color, Wrinkle, Face length, etc. In this section, we
show that the adaptation of Iterative Curve-Traversal can explore the variation of abstract semantics,
which is represented by the cone-shaped region of the generative factors (Tr¨auble et al., 2021)."
ABSTRACT,0.35390946502057613,"Because of its text-driven nature, we utilize the global basis3 from StyleCLIP (Patashnik et al., 2021)
corresponding to the abstract semantics of Old. Then, we consider the modiﬁcation of Iterative
Curve-Traversal following the given global basis vglobal. To be more speciﬁc, the departure direction
of each piece of curve γi in Eq 12 is chosen by the similarity to vglobal, not by the similarity to
previous departure direction. The results for old are provided in Fig 7. (See the appendix for other
examples.) Step size denotes the length of each piece of curve, i.e. (I/N) in Sec 3.2. For a fair
comparison, the overall perturbation intensity I is ﬁxed to 4 by adjusting the number of steps N.
The linear traversal along the global basis adds only wrinkles to the image and the image collapses
shortly. On the contrary, both Iterative Curve-Traversal methods obtain the diverse and high-ﬁdelity
image manipulation for the target semantics old. In particular, the diversity is greatly increased as we
add stochasticity to the step size. We interpret this diversity as a result of the increased exploration
area from the stochastic step size while exploiting the high-ﬁdelity of Iterative Curve-Traversal."
ABSTRACT,0.35802469135802467,"3We use the global basis deﬁned on W+ (Tov et al., 2021). See the appendix for detail."
ABSTRACT,0.36213991769547327,Published as a conference paper at ICLR 2022
ABSTRACT,0.3662551440329218,"Figure 7: Iterative Curve-Traversal guided by global basis from StyleCLIP for the semantics
of old. Left: Linear traversal along global basis. Middle: Iterative Curve-Traversal of ﬁxed step
size (Stepsize = (0.02, 0.04, 0.08, 0.16)). Right: Stochastic Iterative Curve-Traversal (Step size is
sampled from Uniform Noise on [0.05, 0.15])"
EVALUATING WARPAGE OF W-MANIFOLD,0.37037037037037035,"4
EVALUATING WARPAGE OF W-MANIFOLD"
EVALUATING WARPAGE OF W-MANIFOLD,0.37448559670781895,"In this section, we provide an explanation for the limited success of the global basis in W-space of
StyleGAN2. In Sec 3, we showed that Local Basis corresponds to the generative factors of data.
Hence, the linear subspace spanned by Local Basis, which is the tangent space TwWk
w in Eq 5,
describes the local principal variation of image. In this regard, we assess the global disentangle-
ment property by evaluating the consistency of the tangent space at each w ∈W. We refer to the
inconsistency of the tangent space as the warpage of the latent manifold. Our evaluation proves
that W-manifold is warped globally. In this section, we present the quantitative evaluation of the
global warpage by introducing the Grassmannian Metric. The qualitative evaluation by observing
the subspace traversal is provided in the appendix. The subspace traversal denotes a simultaneous
traversal in multiple directions."
EVALUATING WARPAGE OF W-MANIFOLD,0.3786008230452675,"Grassmannian Manifold
Let V be the vector space.
The Grassmannian manifold Gr(k, V )
(Boothby, 1986) is deﬁned as the set of all k-dimensional linear subspaces of V . We evaluate
the global warpage of W-manifold by measuring the Grassmannian distance between the linear sub-
spaces spanned by top-k Local Basis of each w ∈W. The reason for measuring the distance for
top-k Local Basis is the manifold hypothesis. The linear subspace spanned by top-k Local Basis
corresponds to the tangent space of the k-dimensional approximation of W (Eq 5). From this per-
spective, a large Grassmannian distance means that the k-dimensional local approximation of W
severely changes. Likewise, we consider the subspace spanned by the top-k components for the
global basis. In this study, two types of metrics (i.e. Projection metric and Geodesic metric) are
adopted as metrics of the Grassmannian manifold."
EVALUATING WARPAGE OF W-MANIFOLD,0.38271604938271603,"Grassmannian Metric
First, for two subspaces W, W ′ ∈Gr(k, V ), let the projection into each
subspace be PW and PW ′, respectively. Then the Projection Metric (Karrasch, 2017) on Gr(k, V )
is deﬁned as follows.
dproj (W, W ′) = ∥PW −PW ′∥
(13)"
EVALUATING WARPAGE OF W-MANIFOLD,0.3868312757201646,where || · || denotes the operator norm.
EVALUATING WARPAGE OF W-MANIFOLD,0.39094650205761317,"Second, let MW , MW ′ ∈RdV ×k be the column-wise orthonormal matrix of which columns span
W, W ′ ∈Gr(k, V ), respectively. Then, the Geodesic Metric (Ye & Lim, 2016) on Gr(k, V ), which
is induced by canonical Riemannian structure, is formulated as follows."
EVALUATING WARPAGE OF W-MANIFOLD,0.3950617283950617,"dgeo(W, W ′) = k
X"
EVALUATING WARPAGE OF W-MANIFOLD,0.3991769547325103,"i=1
θ2
i !1/2 (14)"
EVALUATING WARPAGE OF W-MANIFOLD,0.40329218106995884,"where θi = cos−1(σi(M ⊤
W MW ′)) denotes the i-th principal angle between W and W ′."
EVALUATING WARPAGE OF W-MANIFOLD,0.4074074074074074,Published as a conference paper at ICLR 2022
EVALUATING WARPAGE OF W-MANIFOLD,0.411522633744856,(a) Projection Metric
EVALUATING WARPAGE OF W-MANIFOLD,0.4156378600823045,"0
10
20
30
40
50
Subspace Dimension 0 2 4 6 8"
EVALUATING WARPAGE OF W-MANIFOLD,0.41975308641975306,Metric
EVALUATING WARPAGE OF W-MANIFOLD,0.42386831275720166,"Close w
Random O(d)
Random w
To global GANSpace
To global SeFa"
EVALUATING WARPAGE OF W-MANIFOLD,0.4279835390946502,(b) Geodesic Metric
EVALUATING WARPAGE OF W-MANIFOLD,0.43209876543209874,"Figure 8: Grassmannian metric. The shaded region illustrates (mean ± standard deviation) inter-
vals of each score. Above all, Random w metric is much larger than the Close w. This means a
large variation of Local Basis on W, which demonstrates that W-space is globally warped. More-
over, the metric result shows the local consistency of Local Basis and the existence of limited global
alignment on W. (See Sec 4 for detail)"
EVALUATING WARPAGE OF W-MANIFOLD,0.43621399176954734,"Evaluation
We evaluate the global warpage of the W-manifold by comparing the ﬁve distances
as we vary the subspace dimension."
EVALUATING WARPAGE OF W-MANIFOLD,0.4403292181069959,"1. Random O(d): Between two random basis of RdW uniformly sampled from O(dW)
2. Random w: Between two Local Basis from two random w ∈W
3. Close w: Between two Local Basis from two close w′, w ∈W (See appendix for the
Grassmannian metric with various ϵ = |z′ −z|.)"
EVALUATING WARPAGE OF W-MANIFOLD,0.4444444444444444,"w′ = f(z′),
w = f(z)
where |z′ −z| = 0.1
(15)"
EVALUATING WARPAGE OF W-MANIFOLD,0.448559670781893,"4. To global GANSpace: Between Local Basis and the global basis from GANSpace
5. To global SeFa: Between Local Basis and the global basis from SeFa"
EVALUATING WARPAGE OF W-MANIFOLD,0.45267489711934156,"Fig 8 shows the above ﬁve Grassmannian metrics. We report the metric results from 100 samples for
the Random O(dW) and 1,000 samples for the others. The Projection metric increases in order of
Close w, To global GANSpace, Random w, To global SeFa, and Random O(dW). For the Geodesic
metric, the order is reversed for Random w and To global SeFa. Most importantly, the Random w
metric is much larger than Close w. This shows that there is a large variation of Local Basis on W,
which proves that W-space is globally warped. In addition, Close w metric is always signiﬁcantly
smaller than the others, which implies the local consistency of Local Basis on W. Finally, the
metric results prove the existence of limited global disentanglement on W. Random w is smaller
than Random O(d). This order shows that Local Basis on W is not completely random, which
implies the existence of a global alignment. In this regard, both To global results prove that the
global basis ﬁnds the global alignment to a certain degree. To global GANSpace lies in between
Close w and Random w. To global SeFa does so on the Geodesic metric and is similar to Random
w on the Projection metric. However, the large gap between Close w and both To global implies
that the discovered global alignment is limited."
CONCLUSION,0.4567901234567901,"5
CONCLUSION"
CONCLUSION,0.4609053497942387,"In this work, we proposed a method for ﬁnding a meaningful traversal direction based on the local-
geometry of the intermediate latent space of GANs, called Local Basis. Motivated by the theoretical
explanation of Local Basis, we suggest experiments to evaluate the global geometry of the latent
space and an iterative traversal method that can trace the latent space. The experimental results
demonstrate that Local Basis factorizes the semantics of images and provides a more stable trans-
formation of images with and without the proposed iterative traversal. Moreover, the suggested
evaluation of the W-space in StyleGAN2 proves that the W-space is globally distorted. Therefore,
the global method can ﬁnd a limited global consistency from W-space."
CONCLUSION,0.46502057613168724,Published as a conference paper at ICLR 2022
CONCLUSION,0.4691358024691358,ACKNOWLEDGEMENT
CONCLUSION,0.4732510288065844,"This work was supported by the NRF grant [2021R1A2C3010887], the ICT R&D program of
MSIT/IITP [2021-0-00077] and MOTIE [P0014715]."
ETHICS STATEMENT,0.4773662551440329,ETHICS STATEMENT
ETHICS STATEMENT,0.48148148148148145,"The limitations and the potential negative societal impacts of our work are that Local Basis would
reﬂect the bias of data. The GANs learn the probability distribution of data through samples from
it. Thus, unlike the likelihood-based method such as Variational Autoencoder (Kingma & Welling,
2014) and Flow-based models (Kingma & Dhariwal, 2018), the GANs are more likely to amplify
the dependence between the semantics of data, even the bias of it. Because Local Basis ﬁnds a
meaningful traversal direction based on the local-geometry of latent space, Local Basis would show
the bias of data as it is. Moreover, if Local Basis is applied to real-world problems like editing
images, Local Basis may amplify the bias of society. However, in order to ﬁx a problem, we have to
ﬁnd a method to analyze it. In this respect, Local Basis can serve as a tool to analyze the bias."
REPRODUCIBILITY STATEMENT,0.48559670781893005,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.4897119341563786,"To ensure the reproducibility of this study, we attached the entire source code in the supplementary
material. Every ﬁgure can be reproduced by running the jupyter notebooks in notebooks/*. In
addition, the proof of Proposition 1 is included in the appendix."
REFERENCES,0.49382716049382713,REFERENCES
REFERENCES,0.49794238683127573,"Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the
stylegan latent space? In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 4432–4441, 2019."
REFERENCES,0.5020576131687243,"Rameen Abdal, Peihao Zhu, Niloy J Mitra, and Peter Wonka. Styleﬂow: Attribute-conditioned
exploration of stylegan-generated images using conditional continuous normalizing ﬂows. ACM
Transactions on Graphics (TOG), 40(3):1–21, 2021."
REFERENCES,0.5061728395061729,"Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828,
2013."
REFERENCES,0.5102880658436214,"William M Boothby. An introduction to differentiable manifolds and Riemannian geometry. Aca-
demic press, 1986."
REFERENCES,0.51440329218107,"Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ﬁdelity natural
image synthesis. In International Conference on Learning Representations, 2018."
REFERENCES,0.5185185185185185,"Chia-Hsing Chiu, Yuki Koyama, Yu-Chi Lai, Takeo Igarashi, and Yonghao Yue. Human-in-the-loop
differential subspace search in high-dimensional latent space. ACM Transactions on Graphics
(TOG), 39(4):85–1, 2020."
REFERENCES,0.522633744855967,"Lore Goetschalckx, Alex Andonian, Aude Oliva, and Phillip Isola. Ganalyze: Toward visual deﬁ-
nitions of cognitive image properties. In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pp. 5744–5753, 2019."
REFERENCES,0.5267489711934157,"Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014."
REFERENCES,0.5308641975308642,"Erik H¨ark¨onen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. Ganspace: Discovering
interpretable gan controls. Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.5349794238683128,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
neural information processing systems, 30, 2017."
REFERENCES,0.5390946502057613,Published as a conference paper at ICLR 2022
REFERENCES,0.5432098765432098,"Ali Jahanian, Lucy Chai, and Phillip Isola. On the” steerability” of generative adversarial networks.
In International Conference on Learning Representations, 2019."
REFERENCES,0.5473251028806584,"Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-
proved quality, stability, and variation. In International Conference on Learning Representations,
2018."
REFERENCES,0.551440329218107,"Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks.
In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 4401–4410, 2019."
REFERENCES,0.5555555555555556,"Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training
generative adversarial networks with limited data. arXiv preprint arXiv:2006.06676, 2020a."
REFERENCES,0.5596707818930041,"Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-
ing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 8110–8119, 2020b."
REFERENCES,0.5637860082304527,Daniel Karrasch. An introduction to grassmann manifolds and their matrix representation. 2017.
REFERENCES,0.5679012345679012,"Diederik P Kingma and Prafulla Dhariwal. Glow: generative ﬂow with invertible 1× 1 convolutions.
In Proceedings of the 32nd International Conference on Neural Information Processing Systems,
pp. 10236–10245, 2018."
REFERENCES,0.5720164609053497,"Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the
International Conference on Learning Representations (ICLR), 2014."
REFERENCES,0.5761316872427984,"Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-
driven manipulation of stylegan imagery. arXiv preprint arXiv:2103.17249, 2021."
REFERENCES,0.5802469135802469,"David Pfau, Irina Higgins, Aleksandar Botev, and S´ebastien Racani`ere. Disentangling by subspace
diffusion. arXiv preprint arXiv:2006.12982, 2020."
REFERENCES,0.5843621399176955,"Antoine Plumerault, Herv´e Le Borgne, and C´eline Hudelot. Controlling generative models with
continuous factors of variations. In International Conference on Learning Representations, 2020.
URL https://openreview.net/forum?id=H1laeJrKDB."
REFERENCES,0.588477366255144,"Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In Proceedings of the International Conference on
Learning Representations (ICLR), 2016."
REFERENCES,0.5925925925925926,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. Image, 2:T2, 2021."
REFERENCES,0.5967078189300411,"Aditya Ramesh, Youngduck Choi, and Yann LeCun. A spectral regularizer for unsupervised disen-
tanglement. arXiv preprint arXiv:1812.01161, 2018."
REFERENCES,0.6008230452674898,"Yujun Shen and Bolei Zhou. Closed-form factorization of latent semantics in gans. In CVPR, 2021."
REFERENCES,0.6049382716049383,"Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of gans for
semantic face editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 9243–9252, 2020."
REFERENCES,0.6090534979423868,"Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. Designing an encoder
for stylegan image manipulation. ACM Transactions on Graphics (TOG), 40(4):1–14, 2021."
REFERENCES,0.6131687242798354,"Frederik Tr¨auble, Elliot Creager, Niki Kilbertus, Francesco Locatello, Andrea Dittadi, Anirudh
Goyal, Bernhard Sch¨olkopf, and Stefan Bauer. On disentangled representations learned from
correlated data. In International Conference on Machine Learning, pp. 10401–10412. PMLR,
2021."
REFERENCES,0.6172839506172839,"Paul Upchurch, Jacob Gardner, Geoff Pleiss, Robert Pless, Noah Snavely, Kavita Bala, and Kilian
Weinberger. Deep feature interpolation for image content changes. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 7064–7073, 2017."
REFERENCES,0.6213991769547325,Published as a conference paper at ICLR 2022
REFERENCES,0.6255144032921811,"Andrey Voynov and Artem Babenko. Unsupervised discovery of interpretable directions in the gan
latent space. In International Conference on Machine Learning, pp. 9786–9796. PMLR, 2020."
REFERENCES,0.6296296296296297,"Binxu Wang and Carlos R Ponce. The geometry of deep generative image models and its applica-
tions. arXiv preprint arXiv:2101.06006, 2021."
REFERENCES,0.6337448559670782,"Ceyuan Yang, Yujun Shen, and Bolei Zhou. Semantic hierarchy emerges in deep generative repre-
sentations for scene synthesis. International Journal of Computer Vision, pp. 1–16, 2021."
REFERENCES,0.6378600823045267,"Ke Ye and Lek-Heng Lim. Schubert varieties and distances between subspaces of different dimen-
sions. SIAM Journal on Matrix Analysis and Applications, 37(3):1176–1197, 2016."
REFERENCES,0.6419753086419753,"Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao.
Lsun: Construction of
a large-scale image dataset using deep learning with humans in the loop.
arXiv preprint
arXiv:1506.03365, 2015."
REFERENCES,0.6460905349794238,"Jiapeng Zhu, Ruili Feng, Yujun Shen, Deli Zhao, Zhengjun Zha, Jingren Zhou, and Qifeng Chen.
Low-rank subspaces in gans. arXiv preprint arXiv:2106.04488, 2021."
REFERENCES,0.6502057613168725,Published as a conference paper at ICLR 2022
REFERENCES,0.654320987654321,"A
PROPOSITION PROOF"
REFERENCES,0.6584362139917695,"Denote (∇zbf) by J. Then, from w′ = T1f(zb + c · ϵ),"
REFERENCES,0.6625514403292181,"w′ = wb + c · Jϵ ∼N(wb, c2 · JJ⊺)
(16)"
REFERENCES,0.6666666666666666,"The ﬁrst principal component v1 is the vector such that v⊺
1(c · Jϵ) has the maximum variance."
REFERENCES,0.6707818930041153,"Var(v⊺
1(c · Jϵ)) = c2 · ||v⊺
1J||2
2
(17)"
REFERENCES,0.6748971193415638,"Therefore,
v1 = argmax
||v||2=1
Var(v⊺(c · Jϵ)) = argmax
||v||2=1
||J⊺· v||2
(18)"
REFERENCES,0.6790123456790124,"Clearly, v1 corresponds to the ﬁrst right singular vector of J⊺, i.e. the ﬁrst left singular vector of J,
from the linear operator norm maximizing property of singular vectors. Inductively, k-th principal
component vk is the vector such that"
REFERENCES,0.6831275720164609,"vk = argmax
||v||2=1
Var(v⊺(c · Jϵ))
where
vk ⊥{v1, v2, · · · , vk−1}
(19)"
REFERENCES,0.6872427983539094,"Thus, vk becomes the k-th left singular vector of J. Therefore, the principal components from the
Local PCA problem are equivalent to Local Basis at wb."
REFERENCES,0.691358024691358,"B
ALGORITHM"
REFERENCES,0.6954732510288066,Algorithm 1 Local Basis
REFERENCES,0.6995884773662552,Input:
REFERENCES,0.7037037037037037,"1: z ∈RdZ is the input code.
2: f : RdZ →RdW is the mapping network."
REFERENCES,0.7078189300411523,"Output: LOCALBASIS(z, f)"
REFERENCES,0.7119341563786008,"3: w ←f(z)
4: J ∈RdW×dZ ←JACOBIAN(z, w)
5: U, S, V ←SVD(J)
6: return {U, S, V }"
REFERENCES,0.7160493827160493,Algorithm 2 Iterative Curve-Traversal along positive direction
REFERENCES,0.720164609053498,Input:
REFERENCES,0.7242798353909465,"1: z ∈RdZ is the input code.
2: f : RdZ →RdW is the mapping network.
3: k ∈[1, min {dZ, dW}] is the ordinal number of direction to traverse.
4: I is the total perturbation intensity.
5: N ≥1 is the number of iterations."
REFERENCES,0.7283950617283951,"Output: ITERATIVETRAVERSAL(z, f, k, I, N)"
REFERENCES,0.7325102880658436,"6: z0 ←z
7: c ←ones(dZ, 1)
8: for i ∈[0, N) do
9:
U, S, V ←LOCALBASIS(zi, f)
10:
if i ¿ 0 then
11:
c ←U T · ui−1
12:
k ←arg max(|c|)
▷The row number most similar to the previously selected basis.
13:
end if
14:
ui, vi ←sign(ck)Uk, sign(ck)Vk
▷Aligns with previous orientation
15:
si ←Skk
16:
zi+1 ←zi +
I
si·N vi
17: end for
18: return {z0, ..., zN}"
REFERENCES,0.7366255144032922,Published as a conference paper at ICLR 2022
REFERENCES,0.7407407407407407,"C
MODEL AND COMPUTATION RESOURCE DETAILS"
REFERENCES,0.7448559670781894,"Model
We evaluate GANSpace (H¨ark¨onen et al., 2020), SeFa (Shen & Zhou, 2021), and Local
Basis on StyleGAN2 models for FFHQ (Karras et al., 2019) and LSUN (Yu et al., 2015) provided
by the authors (Karras et al., 2020b)."
REFERENCES,0.7489711934156379,"Computation Resource
We generated Latent traversal results on the environment of TITAN RTX
with Intel(R) Xeon(R) Gold 5220 CPU @ 2.20GHz. However, it requires a low computational cost
to get a Local Basis. For example, on the environment of GTX 1660 with Ryzen 5 2600, computing
a Local Basis takes about 0.05 seconds."
REFERENCES,0.7530864197530864,"D
CODE LICENSE"
REFERENCES,0.757201646090535,"The ﬁles models/wrappers.py, notebooks/ganspace utils.py and notebooks/notebook utils.py are a
derivative of the GANSpace, and are provided under the Apache 2.0 license. The directory netdissect
is a derivative of the GAN Dissection project, and is provided under the MIT license. The directories
models/biggan and models/stylegan2 are provided under the MIT license."
REFERENCES,0.7613168724279835,Published as a conference paper at ICLR 2022
REFERENCES,0.7654320987654321,"E
DISTRIBUTION OF SIGULAR VALUES OF JACOBIAN"
REFERENCES,0.7695473251028807,"(a) Singluar values σz
i with 0 ≤σz
i ≤2
(b) All Singluar values σz
i"
REFERENCES,0.7736625514403292,"Figure 9: Histogram of the Singular Values σz
i of dfz for three random z and the random matrix.
The random matrix is sampled from the Gaussian distribution, then transformed to have the mean
and standard deviation of the 100 Jacobian matrix. The sharp peak around zero demonstrates that
most of the linear perturbation from z collapses. This observation proves our manifold hypothesis.
To better represent the sparsity of singular values, we provide the histogram of sigular values σz
i
with 0 ≤σz
i ≤2 separately."
REFERENCES,0.7777777777777778,"F
GRASSMANNIAN METRIC"
REFERENCES,0.7818930041152263,"(a) Projection Metric
(b) Geodesic Metric"
REFERENCES,0.7860082304526749,"Figure 10: Grassmannian metric between two close w, w′ ∈W as we vary ϵ. We denote ϵ =
|z′ −z| where w′ = f(z′), w = f(z). As expected, the Grassmannian metric monotonically
increases as we increase ϵ. However, even for the case of ϵ = 0.5, the evaluated metric is much
smaller than To global GANSpace. Therefore, regardless of ϵ, every metric for Close w supports our
claim for the global warpage of W-space. In the main text, we present only the case of ϵ = 0.1. The
reported Grassmannian metrics, Fig 10 in the supplementary material and Fig 8 in the main text, are
evaluated on the SytleGAN2 model trained on FFHQ."
REFERENCES,0.7901234567901234,Published as a conference paper at ICLR 2022
REFERENCES,0.7942386831275721,"G
MORE LATENT TRAVERSAL EXAMPLES"
REFERENCES,0.7983539094650206,(a) GANSpace
REFERENCES,0.8024691358024691,(b) SeFa
REFERENCES,0.8065843621399177,(c) Ramesh et al. (2018)
REFERENCES,0.8106995884773662,(d) Local Basis (Ours)
REFERENCES,0.8148148148148148,(e) Iterative Curve-Traversal (Ours)
REFERENCES,0.8189300411522634,"Figure 11: Enlarged ﬁgure for Fig 3. Each row represents latent traversal on the W-space of the
StyleGAN2-FFHQ, except for (c). Ramesh et al. (2018) provides the local traversal directions on
Z. Except for (e), each traversal image is generated by the linear traversal. The latent code w is
perturbed up to 12 along the 1st and 2nd direction of the corresponding method. The perturbation
intensity is linearly increased from 0 to 12 for each column. Since Ramesh et al. (2018) is deﬁned
on Z, we downscaled the perturbation intensity by the singular values from Local Basis for a fair
comparison. In the case of the existing methods, the quality of the image gets severely degraded as
we perturb stronger. On the other hand, Local Basis shows a relatively stable traversal."
REFERENCES,0.823045267489712,Published as a conference paper at ICLR 2022
REFERENCES,0.8271604938271605,"(a) Local basis (Ours)
(b) Iterative Curve-Traversal (Ours)"
REFERENCES,0.831275720164609,"(c) GANSpace
(d) SeFa"
REFERENCES,0.8353909465020576,"Figure 12: Additional Robustness Test results of each Latent Traversal methods along the ﬁrst 10
components. For each traversal methods, each row corresponds to a latent traversal of perturbation
up to 12. Compared to the global methods, GANSpace and SeFa, even Local Basis with linear
traversal (Fig 12a) shows more stable traversal on images. Moreover, Local Basis with Iterative
Curve-Traversal (Fig 12b) rarely shows any collapse under the latent traversal of 12 along the curve."
REFERENCES,0.8395061728395061,Published as a conference paper at ICLR 2022
REFERENCES,0.8436213991769548,"(a) StyleGAN2 FFHQ
(b) StyleGAN2 FFHQ"
REFERENCES,0.8477366255144033,"(c) StyleGAN2 LSUN Cat
(d) StyleGAN2 LSUN Car"
REFERENCES,0.8518518518518519,"Figure 13: Additional examples of Semantic Factorization without layer restriction, i.e. Latent
traversal along the ﬁrst 10 components of Local Basis with a moderate perturbation of up to 5. Local
Basis ﬁnds diverse and natural-looking semantic variations on each dataset."
REFERENCES,0.8559670781893004,Published as a conference paper at ICLR 2022
REFERENCES,0.8600823045267489,"Figure 14: Iterative Curve-Traversal guided by global basis only at departure from StyleCLIP
for the semantics of old. Contrary to Fig 7, Iterative Curve-Traversal follows the global basis only
at departure. After that, the departure direction is chosen by the similarity to the previous departure
direction. Left: Linear traversal along global basis. Middle: Iterative Curve-Traversal of ﬁxed
stepsize (Stepsize = (0.02, 0.04, 0.08, 0.16)). Right: Stochastic Iterative Curve-Traversal (Stepsize
is sampled from Uniform Noise on [0.05, 0.15])"
REFERENCES,0.8641975308641975,"H
IMPLEMENTATION DETAILS FOR SEC 3.4"
REFERENCES,0.8683127572016461,"In Sec 3.4, we utilize the global basis from StyleCLIP (Patashnik et al., 2021) deﬁned on W+, the
layer-wise extension of W introduced in (Abdal et al., 2019; Patashnik et al., 2021). To be more
speciﬁc, since the synthesis network in StyleGAN has 18 layers, we obtain an extended latent code
w+ ∈W+ deﬁned by the concatenation of latent codes wi ∈W of dimension 512 for each i-th
layer and it can be described as follows:"
REFERENCES,0.8724279835390947,"w+ = (w1, w2, · · · , w18) ∈R512×18.
(20)"
REFERENCES,0.8765432098765432,"Note that our Iterative Curve-Traversal originally deﬁned on W has a canonical extension to W+
without additional changes in structures or methodologies."
REFERENCES,0.8806584362139918,"For implementing the stochastic Iterative Curve-Traversal introduced in Sec 3.4, we ﬁrstly ﬁnd
a global basis on W+ using StyleCLIP (Patashnik et al., 2021) which implies a given semantic
attribute in the form of text (e.g. old). Now denote the global basis by v+
global, which can be
represented as follows:
v+
global = (vglobal
1
, vglobal
2
, · · · , vglobal
18
).
(21)"
REFERENCES,0.8847736625514403,"Then we perform the (extended) Iterative Curve-Traversal following v+
global, equipped with a
stochastic movement for each step. In practice, we consider two options to choose a traversal di-
rection for each step; First, follow the direction most similar to the previously selected basis (as
Algorithm 2), except for the ﬁrst iteration. Note that we compute the similarity between the local
basis and the global basis only at once when choosing the ﬁrst traversal direction. Second, follow
the direction most similar to the given global basis. This is slightly different from our Algorithm
2, however, we empirically verify that setting the exploration in that way leads to a more desirable
image change."
REFERENCES,0.8888888888888888,"Fig 14 shows that the ﬁrst method still preserves the image quality well, but it does not guarantee that
the desired direction of image change, namely ‘old’. We speculate the reason why such phenomenon
occurs is that most of the information contained in the meaningful global basis disappears after the
ﬁrst step (a unique, direct comparison to the global basis), although our methodology guarantees that
the latent code does not escape from the manifold and achieve a high image quality. Nevertheless,
Fig 15 shows that the second method for the stochastic Iterative Curve-Traversal can change a given
facial image in a very high quality and various ways."
REFERENCES,0.8930041152263375,Published as a conference paper at ICLR 2022
REFERENCES,0.897119341563786,"Figure 15: Additional Examples of Stochastic Iterative Curve Traversal guided by the global
basis from StyleCLIP for the semantics of Old. Left: Linear traversal along global basis. Right:
Stochastic Iterative Curve-Traversal"
REFERENCES,0.9012345679012346,Published as a conference paper at ICLR 2022
REFERENCES,0.9053497942386831,"(a) GANSpace
(b) SeFa"
REFERENCES,0.9094650205761317,(c) Local Basis (Ours)
REFERENCES,0.9135802469135802,"Figure 16: Subspace traversal with two directions on W-space of the StyleGAN2. The horizontal
(red box) and vertical (green box) axes correspond to the 1st and 2nd directions of each method."
REFERENCES,0.9176954732510288,"I
SUBSPACE TRAVERSAL"
REFERENCES,0.9218106995884774,"In Section 4, we proved that the W-space in StyleGAN2 is warped globally. Speciﬁcally, the sub-
space of traversal direction generating principal variation in the image changes severely as we vary
the starting latent variable w. To verify the claim further, we visualize the subspace traversal on the
latent space W. The subspace traversal denotes a simultaneous traversal in multiple directions. In
this paper, we visualize the two-dimensional traversal,"
REFERENCES,0.9259259259259259,"Subspace Traversalw
(i,j)(x, y) = G

w + x"
REFERENCES,0.9300411522633745,"N vw
i + y"
REFERENCES,0.934156378600823,"N vw
j

(22)"
REFERENCES,0.9382716049382716,"where w = f(z) and G denotes a subnetwork of the given GAN model from W to the images space
X. Since the disentanglement into the linear subspace implies the commutativity of transformation
(Pfau et al., 2020), the subspace traversal can be a more challenging version of linear traversal
experiments."
REFERENCES,0.9423868312757202,"Fig 16 and Fig 17 show results of the subspace traversal for the global basis and Local Basis. Starting
from the center, the horizontal and vertical traversals correspond to the 1st and 2nd directions of each
method. The same perturbation intensity per step is applied for both directions. When restricted to
the linear traversal (red and green box), the GANSpace shows relatively stable traversals. However,
the traversal image deteriorates at the corner of the subspace traversal. By contrast, Local Basis
shows a stable variation during the entire subspace traversal. This result proves that the global basis
is not well-aligned with the local-geometry of the W manifold."
REFERENCES,0.9465020576131687,Published as a conference paper at ICLR 2022
REFERENCES,0.9506172839506173,"(a) GANSpace
(b) SeFa"
REFERENCES,0.9547325102880658,(c) Local Basis (Ours)
REFERENCES,0.9588477366255144,"Figure 17: Subspace traversal with two directions on W-space of the StyleGAN2. The horizontal
(red box) and vertical (green box) axes correspond to the 1st and 2nd directions of each method."
REFERENCES,0.9629629629629629,Published as a conference paper at ICLR 2022
REFERENCES,0.9670781893004116,"J
LOCAL BASIS ON OTHER MODELS"
REFERENCES,0.9711934156378601,"(a) GANSpace
(b) Local Basis (Ours)"
REFERENCES,0.9753086419753086,"(c) GANSpace
(d) Local Basis (Ours)"
REFERENCES,0.9794238683127572,"Figure 18: Comparison of GANSpace and Local Basis on StyleGAN-FFHQ (Karras et al.,
2019) Each traversal image is generated along the ﬁrst 10 components of each method with a per-
turbation of up to 5."
REFERENCES,0.9835390946502057,Published as a conference paper at ICLR 2022
REFERENCES,0.9876543209876543,"(a) GANSpace
(b) Local Basis (Ours)"
REFERENCES,0.9917695473251029,"(c) GANSpace
(d) Local Basis (Ours)"
REFERENCES,0.9958847736625515,"Figure 19: Comparison of GANSpace and Local Basis on BigGAN-512 (Brock et al., 2018) Each
traversal image is generated along the ﬁrst 10 components of each method with a perturbation of up
to 3."
