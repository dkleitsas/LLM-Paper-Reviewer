Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0007158196134574087,"Recently, Wong et al. (2020) showed adversarial training with single-step FGSM
leads to a characteristic failure mode named catastrophic overﬁtting (CO), in
which a model becomes suddenly vulnerable to multi-step attacks. Moreover,
they showed adding a random perturbation prior to FGSM (RS-FGSM) seemed
to be sufﬁcient to prevent CO. However, Andriushchenko & Flammarion (2020)
observed that RS-FGSM still leads to CO for larger perturbations and argue that
the only contribution of the random step is to reduce the magnitude of the at-
tacks. They suggest a regularizer (GradAlign) that avoids CO but is signiﬁcantly
more expensive than RS-FGSM. In this work, we methodically revisit the role
of noise and clipping in single-step adversarial training. Contrary to previous in-
tuitions, we ﬁnd that not clipping the perturbation around the clean sample and
using a stronger noise is highly effective in avoiding CO for large perturbation
radii, despite leading to an increase in the magnitude of the attacks. Based on
these observations, we propose a method called Noise-FGSM (N-FGSM), which
attacks noise-augmented samples directly using a single-step. Empirical analyses
on a large suite of experiments show that N-FGSM is able to match or surpass the
performance of GradAlign while achieving a 3x speed-up."
INTRODUCTION,0.0014316392269148174,"1
INTRODUCTION"
INTRODUCTION,0.0021474588403722263,"Deep neural networks have achieved remarkable performance on a variety of tasks (He et al., 2015;
Silver et al., 2016; Devlin et al., 2019). However, it is well known that they are vulnerable to
small worst-case perturbations around the input data – commonly referred to as adversarial exam-
ples (Szegedy et al., 2014). The existence of such adversarial examples poses a security threat to
deploying models in sensitive environments (Biggio & Roli, 2018). This has motivated a large body
of work towards improving the adversarial robustness of neural networks (Goodfellow et al., 2015;
Papernot et al., 2016; Tram`er et al., 2018)."
INTRODUCTION,0.002863278453829635,"The most popular family of solutions to obtain robust neural networks is based on the concept
of adversarial training (Goodfellow et al., 2015; Madry et al., 2018). In a nutshell, adversarial
training can be posed as a min-max problem where instead of minimizing some loss over a dataset
of clean samples, we augment the inputs with worst-case perturbations that are generated online
during training. However, obtaining such perturbations is NP-hard (Weng et al., 2018) and hence,
different approaches have been suggested to approximate them. They are commonly referred to as
adversarial attacks. In their seminal work, Goodfellow et al. (2015) proposed the Fast Gradient Sign
Method (FGSM), that generates adversarial attacks by running one step of gradient ascent on the loss
function. However, while FGSM-based adversarial training provides robustness against single-step
FGSM adversaries, Madry et al. (2018); Tram`er et al. (2018) showed that these models were still
vulnerable to multi-step attacks, namely those allowed to perform multiple gradient ascent steps
instead of a single one. Notably, Madry et al. (2018) introduced the multi-step Projected Gradient
Descent (PGD) attack."
INTRODUCTION,0.0035790980672870437,"PGD-based attacks have now become the de facto standard for adversarial training; yet, their cost
increases linearly with the number of steps. As a result, several works have focused on reducing
the cost of adversarial training by approximating the worst-case perturbations with single-step at-
tacks (Wong et al., 2020; Shafahi et al., 2019; Vivek & Babu, 2020). In particular, Wong et al.
(2020) studied FGSM adversarial training and discovered that it suffers from a characteristic failure"
INTRODUCTION,0.004294917680744453,Under review as a conference paper at ICLR 2022 ε k
INTRODUCTION,0.005010737294201861,"FGSM
N-FGSM"
INTRODUCTION,0.00572655690765927,RS-FGSM
INTRODUCTION,0.006442376521116679,"2
4
6
8
10
12
14
16
for training and evaluation 0 20 40 60 80"
INTRODUCTION,0.0071581961345740875,Adversarial Accuracy
INTRODUCTION,0.007874015748031496,CIFAR10 Dataset
INTRODUCTION,0.008589835361488905,"N-FGSM (ours)
GradAlign
MultiGrad
ZeroGrad"
INTRODUCTION,0.009305654974946313,"Free-AT
Kim et. al.
RS-FGSM
FGSM"
INTRODUCTION,0.010021474588403722,N-FGSM
INTRODUCTION,0.010737294201861132,GradAlign
INTRODUCTION,0.01145311381531854,MultiGrad
INTRODUCTION,0.012168933428775949,ZeroGrad
INTRODUCTION,0.012884753042233358,Free-AT
INTRODUCTION,0.013600572655690766,Kim et. al.
INTRODUCTION,0.014316392269148175,RS-FGSM FGSM 0 1 2 3
INTRODUCTION,0.015032211882605583,Train cost relative to FGSM
INTRODUCTION,0.015748031496062992,"Figure 1: Left: Visualization of FGSM (Goodfellow et al., 2015), RS-FGSM (Wong et al., 2020)
and N-FGSM (ours) attacks. While RS-FGSM is limited to noise in the ϵ−l∞ball, N-FGSM draws
noise from an arbitrary k −l∞ball. Moreover, N-FGSM does not clip the perturbation around the
clean sample. Middle: Comparison of single-step methods on CIFAR-10 with PreactResNet18 over
different perturbation radii (ϵ is divided by 255). Our method, N-FGSM, can match or surpass state-
of-the-art results while reducing the cost by a 3× factor. Adversarial accuracy is based on PGD-
50-10 and experiments are averaged over 3 seeds. Right: Comparison of training costs relative to
FGSM baseline based on the number of Forward-Backward passes, see Appendix K for details."
INTRODUCTION,0.0164638511095204,"mode, in which a model suddenly becomes vulnerable to multi-step attacks despite remaining robust
to single-step attacks. This phenomenon is referred to as catastrophic overﬁtting. Moreover, they
argued that adding a random perturbation prior to FGSM (RS-FGSM) seemed sufﬁcient to prevent
catastrophic overﬁtting and yield robust models. Recently, Andriushchenko & Flammarion (2020)
observed that RS-FGSM still leads to catastrophic overﬁtting as we increase the perturbation radii.
They suggested a regularizer (GradAlign) that, on the one hand avoids catastrophic overﬁtting in all
the settings they considered, but on the other hand requires the computation of a double derivative
– which signiﬁcantly increases the computational cost compared to RS-FGSM. This has motivated
other works that aim at achieving the same level of robustness with a lower computational overhead
(Golgooni et al., 2021; Kim et al., 2021)."
INTRODUCTION,0.01717967072297781,"In this paper, we revisit two key components that are common among previous works combining
noise and FGSM (Tram`er et al., 2018; Wong et al., 2020): the role of noise, i.e. the random step,
and the role of the clipping step. In Section 4.1, we study how these two components affect model
robustness; our experiments suggest that adding noise with a large magnitude in the random step and
removing the clipping step improves model robustness and prevents catastrophic overﬁtting, even
against large perturbation radii. We combine these observations and propose a new method called
Noise-FGSM (N-FGSM), an illustration of which is presented in Figure 1 (left). N-FGSM allows to
match, or even surpass, the robust accuracy of the regularized FGSM introduced by Andriushchenko
& Flammarion (2020), while providing a 3× speed-up."
INTRODUCTION,0.01789549033643522,"To corroborate the effectiveness of our solution, we present an experimental survey of recently
proposed single-step attacks and empirically demonstrate that N-FGSM trades-off robustness and
computational cost better than other single-step approaches, evaluated over a large spectrum of
perturbation radii (see Figure 1, middle and right panels), over several datasets (CIFAR-10, CIFAR-
100, and SVHN) and architectures (PreActResNet18 and WideResNet28-10). We will release our
code reproducing all experiments."
RELATED WORK,0.018611309949892626,"2
RELATED WORK"
RELATED WORK,0.019327129563350035,"Since the discovery of adversarial examples, many defense mechanisms have been proposed. Pre-
processing techniques try to modify the input image to neutralize adversarial attacks (Guo et al.,
2018; Buckman et al., 2018; Song et al., 2018). Adversarial detection methods focus on detecting
and rejecting adversarial attacks (Carlini & Wagner, 2017; Ma et al., 2018; Yang et al., 2020; Tian
et al., 2021). Certiﬁable defenses provide theoretical guarantees for the lower bound performance
of networks subjected to worst-case adversarial attacks, however, they incur additional costs dur-
ing inference and, empirically, they yield sub-optimal performance (Cohen et al., 2019; Wong &"
RELATED WORK,0.020042949176807445,Under review as a conference paper at ICLR 2022
RELATED WORK,0.020758768790264854,"Kolter, 2018; Raghunathan et al., 2018; Balunovic & Vechev, 2020). Adversarial training methods
are based on a special form of data augmentation designed to make the network robust to worst-case
perturbations (Zhang et al., 2019; Athalye et al., 2018; Kurakin et al., 2017). However, comput-
ing a worst-case perturbation is an NP-hard problem that needs to be solved at every iteration. To
minimize the overhead of adversarial training, Goodfellow et al. (2015) proposed FGSM which re-
quires one additional gradient step per iteration. Tram`er et al. (2018) ﬁrst proposed performing a
random step before taking the adversarial step (R+FGSM), but they observed that neither method
yields robust models against PGD attacks (Madry et al., 2018). Since then, augmenting the training
with PGD attacks has been one of the most popular approaches for robustness, but its cost increases
linearly with the number of steps, which presents a severe practical limitation."
RELATED WORK,0.021474588403722263,"To reduce the cost of PGD, Shafahi et al. (2019) proposed Free Adversarial Training (Free-AT),
that exploits a single back-propagation step to both update the network parameters and compute the
attack. Wong et al. (2020) explored a variation of R+FGSM, namely RS-FGSM, and showed it can
yield robust networks against multi-step attacks. Andriushchenko & Flammarion (2020) found that
RS-FGSM only works for limited perturbation radii and introduced GradAlign – a regularizer to
linearize the loss surface. However, optimizing GradAlign triplicates the computational cost. This
motivated a new series of works that aim at matching the performance of GradAlign without the
additional computational overhead (Golgooni et al., 2021; Kim et al., 2021). Other strategies that
attempted to improve FGSM included introducing dropout in every layer (Vivek & Babu, 2020) and
perturbing intermediate feature maps together with the input (Park & Lee, 2021). Li et al. (2020)
suggested combining RS-FGSM and PGD attacks during training, however, the proposed strategy
requires a frequent monitoring of the PGD robust accuracy and, in the worst-case, is computationally
equivalent to PGD training."
RELATED WORK,0.02219040801717967,"Gilmer et al. (2019); Fawzi et al. (2018) suggested a strong link between robustness to adversarial
attacks and to random noise. Motivated by this, we revisit the idea of combining noise and FGSM
and propose N-FGSM. Our method is closely related to RS-FGSM, however, we ﬁnd that using a
larger amount of noise and removing the constraint that attacks must lie in the ϵ −l∞ball is key
to obtaining robust models. We note that Kang & Moosavi-Dezfooli (2021) concurrently studied
RS-FGSM without clipping, however, as opposed to our work, they did not investigate and provide
insights on the impact of noise, and the learned models were not robust against large perturbations."
PRELIMINARIES ON SINGLE-STEP ADVERSARIAL TRAINING,0.02290622763063708,"3
PRELIMINARIES ON SINGLE-STEP ADVERSARIAL TRAINING"
PRELIMINARIES ON SINGLE-STEP ADVERSARIAL TRAINING,0.023622047244094488,"Given a classiﬁer fθ : X →Y parameterized by θ and a perturbation set S, the classiﬁer fθ is said
to be robust at x ∈X under S if the following holds for all δ ∈S: fθ(x + δ) = fθ(x). One of the
most popular deﬁnitions for S is the ϵ −ℓ∞ball, i.e. S = {δ : ∥δ∥∞≤ϵ}. This is known as the l∞
threat model and is the setting we adopt throughout this work."
PRELIMINARIES ON SINGLE-STEP ADVERSARIAL TRAINING,0.024337866857551897,"To train networks that are robust against ℓ∞threat models, adversarial training modiﬁes the classical
training procedure of minimizing a loss function over a dataset D = {(xi, yi)}i=1:N of images
xi ∈X and labels yi ∈Y. In particular, adversarial training instead minimizes the worst-case loss
over the perturbation set S, i.e. trains on adversarially perturbed samples {(xi+δi, yi)}i=1:N. When
using the l∞threat model, we can formalize adversarial training as solving the following problem: min
θ N
X"
PRELIMINARIES ON SINGLE-STEP ADVERSARIAL TRAINING,0.025053686471009307,"i=1
max
δ
L(fθ(xi + δ), yi),
subject to ∥δ∥∞≤ϵ,
(1)"
PRELIMINARIES ON SINGLE-STEP ADVERSARIAL TRAINING,0.025769506084466716,"where L is typically the cross-entropy loss for image-classiﬁcation models. Due to the difﬁculty
of ﬁnding the exact inner maximizer, the most common procedure for adversarial training is to
approximate the worst-case perturbation through several PGD steps (Madry et al., 2018). While this
has been shown to yield robust models, it comes at a cost of a linear increase in the computational
overhead with the number of PGD steps. As a result, several works have focused on reducing the
cost of adversarial training by approximating the inner maximization with single-step attacks."
PRELIMINARIES ON SINGLE-STEP ADVERSARIAL TRAINING,0.026485325697924122,"If we assume that the loss function is locally linear with respect to changes in the input, then the inner
maximization of Equation (1) enjoys a closed form solution. Goodfellow et al. (2015) leveraged
this result to propose the FGSM method, which takes one step in the direction of the sign of the
gradient. Tram`er et al. (2018) proposed adding a random initialization prior to FGSM. However, both"
PRELIMINARIES ON SINGLE-STEP ADVERSARIAL TRAINING,0.02720114531138153,Under review as a conference paper at ICLR 2022
PRELIMINARIES ON SINGLE-STEP ADVERSARIAL TRAINING,0.02791696492483894,"methods were later shown to be vulnerable against multi-step attacks, such as PGD (Madry et al.,
2018). Contrary to prior intuition, recent work from Wong et al. (2020) observed that combining a
random step with FGSM can actually lead to a promising robustness performance. In particular, we
note that most recent single-step methods approximate the worst-case perturbation that results from
solving the inner maximization problem in Equation (1) with the following general form:"
PRELIMINARIES ON SINGLE-STEP ADVERSARIAL TRAINING,0.02863278453829635,"δ = ψ

η + α · sign
 
∇xiL(fθ(xi + η), yi)

, where η ∼Ω
(2)"
PRELIMINARIES ON SINGLE-STEP ADVERSARIAL TRAINING,0.02934860415175376,"and Ωis the distribution from which we draw noise perturbations. For example, when ψ is the
projection operator onto the ℓ∞ball and Ωis the uniform distribution in [−ϵ, ϵ], this recovers RS-
FGSM with the following update:"
PRELIMINARIES ON SINGLE-STEP ADVERSARIAL TRAINING,0.030064423765211165,"δRS-FGSM = Proj∥δ∥∞≤ϵ

η + α · sign
 
∇xiL(f(xi + η), yi)

, where η ∼U[−ϵ, ϵ]d.
(3)"
PRELIMINARIES ON SINGLE-STEP ADVERSARIAL TRAINING,0.030780243378668574,"On the other hand, with a different noise setting where Ω= (ϵ−α)·sign (N(0, I)) and by choosing
the step size α to be in [0, ϵ] we recover R+FGSM by Tram`er et al. (2018) that initially explored
the idea of combining noise with FGSM but reported no improvement over adversarial training
with FGSM. If we consider Ωto be deterministically 0 and ψ to be the identity map, we recover
the FGSM. Finally, if we adjust the choice of the loss function L to include a gradient alignment
regularizer, this recovers the GradAlign algorithm by Andriushchenko & Flammarion (2020)."
NOISE AND FGSM,0.031496062992125984,"4
NOISE AND FGSM"
NOISE AND FGSM,0.03221188260558339,"A common practice when performing adversarial training is to restrict the perturbations used during
training to the same ϵ −ℓ∞ball that will be considered at test time. The rationale behind it is that
increasing the magnitude of perturbations could “unnecessarily” decrease the clean accuracy, since
perturbations outside the ball will not be evaluated at test time. For instance, R+FGSM combines the
noise step, with magnitude (ϵ−α), and the FGSM step, with magnitude α, in a convex combination
manner, thereof, restricting the perturbation to ϵ. On the other hand, Wong et al. (2020) apply
a clipping operation after the FGSM step to the ϵ −ℓ∞ball. In the following, we experimentally
challenge this common practice and explore the two key components in previous single-step methods
that limit the magnitude of the perturbations. In particular, we explore (i) the role of the clipping
operation, i.e. ψ as a projection to ℓ∞ball; and (ii) the source and magnitude of noise for the random
step, i.e. Ω. We thoroughly revisit the role of both components on the robustness attained in single-
step methods. Throughout this work, unless stated otherwise, we consider noise perturbations η
sampled from a symmetric Uniform distribution, i.e. Ω= U[−k, k]d, where d is the dimension of
X and we refer to k as the “noise magnitude”."
THE ROLE OF NOISE AND CLIPPING ON ROBUSTNESS IN SINGLE-STEP METHODS,0.0329277022190408,"4.1
THE ROLE OF NOISE AND CLIPPING ON ROBUSTNESS IN SINGLE-STEP METHODS"
THE ROLE OF NOISE AND CLIPPING ON ROBUSTNESS IN SINGLE-STEP METHODS,0.03364352183249821,"Clipping Reduces the Effectiveness of Single-Step Perturbations. To study the impact clip-
ping has on model robustness, we consider the training of PreActResNet18 (He et al., 2016) on
CIFAR-10 (Krizhevsky & Hinton, 2009) with an instance of Equation (2) as a single-step ad-
versarial training. In particular, we consider the case where ψ is a projection to the ℓ∞ball of
size ϵclip ∈{ϵ, 2ϵ, 3ϵ, ∞}, where ∞denotes that ψ is an identity function, i.e. no clipping is
performed. Moreover, we consider noise sampled from a symmetric uniform distribution where
k ∈{ϵ, 2ϵ, 3ϵ, 4ϵ}. We report in Figure 2 the robust accuracy using PGD-50-10 (i.e. PGD attack
with 50 iterations and 10 restarts) with ϵ = 8/255. We observe in Figure 2 (left), that for all choices
of noise magnitude k, as we expand the clipping ℓ∞ball, i.e. we increase ϵclip, the adversarial ro-
bustness improves. We believe that this is due to the fact that more aggressive clipping, i.e. smaller
ϵclip, reduces the strength of the computed single-step perturbations during training. To support this
intuition, we report in Figure 2 (middle) the distribution of the loss measured at perturbed points
prior to applying the clipping step and after applying the clipping step, with ϵclip = ϵ. Moreover, the
negative impact of clipping during training is more prominent as we increase the noise magnitude k.
This is to be expected since, for a ﬁxed α, increasing the noise magnitude k will lead to a prevalence
of the noise component over the sign gradient direction in Equation (2)."
THE ROLE OF NOISE AND CLIPPING ON ROBUSTNESS IN SINGLE-STEP METHODS,0.03435934144595562,Under review as a conference paper at ICLR 2022
THE ROLE OF NOISE AND CLIPPING ON ROBUSTNESS IN SINGLE-STEP METHODS,0.03507516105941303,"1
2
3
Radius to clip the perturbation 20 30 40 50"
THE ROLE OF NOISE AND CLIPPING ON ROBUSTNESS IN SINGLE-STEP METHODS,0.03579098067287044,Adversarial Accuracy
THE ROLE OF NOISE AND CLIPPING ON ROBUSTNESS IN SINGLE-STEP METHODS,0.03650680028632784,"Noise: 1
Noise: 2
Noise: 3
Noise: 4"
THE ROLE OF NOISE AND CLIPPING ON ROBUSTNESS IN SINGLE-STEP METHODS,0.03722261989978525,"1.2
1.4
1.6
1.8
Loss at perturbed point
0.000 0.025 0.050 0.075 0.100 0.125"
THE ROLE OF NOISE AND CLIPPING ON ROBUSTNESS IN SINGLE-STEP METHODS,0.03793843951324266,Density
THE ROLE OF NOISE AND CLIPPING ON ROBUSTNESS IN SINGLE-STEP METHODS,0.03865425912670007,"After clip
Before clip"
THE ROLE OF NOISE AND CLIPPING ON ROBUSTNESS IN SINGLE-STEP METHODS,0.03937007874015748,"2
4
6
8
10 12 14 16
for trainining and evaluation 0 20 40 60 80"
THE ROLE OF NOISE AND CLIPPING ON ROBUSTNESS IN SINGLE-STEP METHODS,0.04008589835361489,Adversarial Accuracy
THE ROLE OF NOISE AND CLIPPING ON ROBUSTNESS IN SINGLE-STEP METHODS,0.0408017179670723,"FGSM (0 )
Noise 1
Noise 2
Noise 4"
THE ROLE OF NOISE AND CLIPPING ON ROBUSTNESS IN SINGLE-STEP METHODS,0.04151753758052971,"Figure 2: Left: N-FGSM + Clipping to different radii (∞means no clipping). As we constraint
perturbations by reducing the clipping radius, adversarial accuracy drops. This effect is stronger as
we increase the noise magnitude. Thus, clipping seems to have a negative impact on robustness.
Middle: Histogram of the loss value for perturbations before and after clipping to the ϵ −ℓ∞
ball. There is a clear shift in the distributions, which indicates that clipping reduces the adversarial
effect of perturbations. Right: N-FGSM when varying the noise magnitude k (ϵ is divided by 255).
Increasing the amount of noise is key to avoiding catastrophic overﬁtting. For (left) and (right) plots,
adversarial accuracy is based on PGD-50-10 and experiments are averaged over 3 seeds."
THE ROLE OF NOISE AND CLIPPING ON ROBUSTNESS IN SINGLE-STEP METHODS,0.04223335719398712,"Thus, overall, we observe that clipping during training has negative impact on the robust accuracy.
Despite unclipped perturbations may lie outside the ϵ−ℓ∞ball, we could not observe any signiﬁcant
drop in clean accuracy. In fact, we obtain comparable clean accuracy to GradAlign as later reported
in Section 5.5. Further analyses can be found in Appendix C."
THE ROLE OF NOISE AND CLIPPING ON ROBUSTNESS IN SINGLE-STEP METHODS,0.04294917680744453,"The role of noise in single-step adversarial training. Neither Tram`er et al. (2018) nor Wong
et al. (2020) explored settings with an increased noise magnitude. Moreover, Andriushchenko &
Flammarion (2020) argue that noise in RS-FGSM is not important per se, claiming that its main
purpose is only reducing the ℓ2 norm of the ﬁnal perturbation δRS-FGSM so that the loss is still in the
linear regime. However, we empirically ﬁnd that increasing the noise magnitude is key to avoiding
catastrophic overﬁtting – see Figure 2 (right)."
THE ROLE OF NOISE AND CLIPPING ON ROBUSTNESS IN SINGLE-STEP METHODS,0.043664996420901936,"Investigating further, similarly to Kim et al. (2021), we plot the loss surface at the end of train-
ing (see Figure 12 in Appendix J) and ﬁnd that, as observed by Kim et al. (2021), the loss surface
of models trained via FGSM or RS-FGSM appears distorted at the end of training, i.e. the loss
increases sharply along the FGSM direction, but then it rapidly decreases back to the same loss val-
ues associated with the clean sample. This is consistent with the gradient obfuscation observed by
Tram`er et al. (2018). However, when training without clipping with an increased noise magnitude,
we observe a non-distorted loss surface, i.e. the loss gradually increases along the FGSM direction.
Interestingly, we observe a similar effect when training with the GradAlign regularizer. Thus, com-
bining noise with FGSM seems to have a regularizing effect that encourages the loss surface to be
locally linear in a similar spirit to GradAlign. Note this is based on the empirical analyses and visual
inspections that we performed. We do not provide theoretical justiﬁcation behind this."
THE ROLE OF NOISE AND CLIPPING ON ROBUSTNESS IN SINGLE-STEP METHODS,0.04438081603435934,"Despite the clear beneﬁts of increasing the noise magnitude, we do observe a slight but consistent
decrease in robustness as we keep on increasing the noise, which we hypothesize is due to over-
regularization. We ﬁnd k = 2ϵ to be the sweet spot in most of our experiments; we do not exclude
that a more extensive hyperparameter tuning procedure may lead to improved results."
OUR APPROACH,0.04509663564781675,"4.2
OUR APPROACH"
OUR APPROACH,0.04581245526127416,"In the previous section, we provided experimental analyses to show that increasing the noise magni-
tude and not clipping prevents catastrophic overﬁtting and improves robustness signiﬁcantly. Based
on these observations, we propose the following, simple and efﬁcient, single-step method for adver-
sarial training that we denote as Noise-FGSM (N-FGSM):"
OUR APPROACH,0.046528274874731566,"δN-FGSM = η + α · sign
 
∇xiL(fθ(xi + η), yi)

, η ∼U[−k, k]d.
(4)"
OUR APPROACH,0.047244094488188976,We detail our full adversarial training procedure in Algorithm 1.
OUR APPROACH,0.047959914101646385,"Theoretical insights
We now theoretically analyse N-FGSM to understand the role of noise in
single-step approaches. According to Andriushchenko & Flammarion (2020), the main contribution"
OUR APPROACH,0.048675733715103794,Under review as a conference paper at ICLR 2022
OUR APPROACH,0.049391553328561204,Algorithm 1 N-FGSM adversarial training
OUR APPROACH,0.05010737294201861,"1: Inputs: epochs T, batches M, radius ϵ, step-size α (default:ϵ), noise magnitude k (default:2ϵ).
2: for t = 1, . . . , T do
3:
for i = 1, . . . , M do
4:
// Perform N-FGSM adversarial attack
5:
η ∼Uniform[−k, k]d"
OUR APPROACH,0.05082319255547602,"6:
δ = η + α · sign
 
∇xiL(fθ(xi + η), yi)
"
OUR APPROACH,0.05153901216893343,"7:
∇θ = ∇θL(fθ(xi + δ), yi)
8:
θ = optimizer(θ, ∇θ) // standard weight update, (e.g. SGD)
9:
end for
10: end for"
OUR APPROACH,0.052254831782390834,"of noise in single-step adversarial training is to reduce the effective ℓ2 norm of the perturbation δ.
Since N-FGSM does not involve clipping, we ﬁnd the expected norm squared of N-FGSM perturba-
tions to be larger than RS-FGSM perturbations. In fact, it is even larger than the FGSM perturbations
which always lie on the ϵ −ℓ∞ball. These observations are formalized in Theorem 1."
OUR APPROACH,0.052970651395848244,"Theorem 1. Let δN-FGSM be our proposed single-step method deﬁned by Equation (4), δFGSM be the
FGSM method (Goodfellow et al., 2015) and δRS-FGSM be the RS-FGSM method (Wong et al., 2020).
Then, with default hyperparameter values and for any ϵ > 0, we have that"
OUR APPROACH,0.05368647100930565,"Eη

∥δN-FGSM∥2
2

> Eη

∥δFGSM∥2
2

> Eη

∥δRS-FGSM∥2
2

."
OUR APPROACH,0.05440229062276306,"We present the Proof in Appendix F where we also compute an empirical estimation of the expected
ℓ2 norm of different methods via Monte Carlo sampling and ﬁnd them to align with Theorem 1.
Differently from the hypothesis of Andriushchenko & Flammarion (2020), we ﬁnd that despite that
N-FGSM perturbations have larger ℓ2 norms, they yield robust models even under larger ϵ radii,
for which RS-FGSM or FGSM fail to catastrophic overﬁtting. We believe that these contradic-
tory observations can lead to a better understanding on the role of noise in adversarial training and
catastrophic overﬁtting in future work."
EXPERIMENTS AND ANALYSES,0.05511811023622047,"5
EXPERIMENTS AND ANALYSES"
EXPERIMENTS AND ANALYSES,0.05583392984967788,"We compare N-FGSM against several adversarial training methods, considering a broad range of
ϵ −l∞radii. Following Wong et al. (2020); Andriushchenko & Flammarion (2020), we measure
adversarial robustness on CIFAR-10/100 (Krizhevsky & Hinton, 2009) and SVHN (Netzer et al.,
2011) datasets with PGD-50-10 attack – PGD (Madry et al., 2018) with 50 iterations and 10 restarts."
COMPARISON TO OTHER SINGLE-STEP METHODS,0.05654974946313529,"5.1
COMPARISON TO OTHER SINGLE-STEP METHODS"
COMPARISON TO OTHER SINGLE-STEP METHODS,0.0572655690765927,"We start by comparing N-FGSM against other single-step methods. Note that not all single-step
methods are equally expensive, since they may involve more or less computationally demanding
operations. For instance, GradAlign (Andriushchenko & Flammarion, 2020) relies on a regularizer
that is considerably expensive, while, MultiGrad (Golgooni et al., 2021) requires evaluating the
input gradients on multiple random points. For a comparison of training cost over different single-
step methods, we refer to Figure 1 (right). Following the standard practice (Wong et al., 2020;
Andriushchenko & Flammarion, 2020), we use a PreactResNet18 architecture (He et al., 2016)."
COMPARISON TO OTHER SINGLE-STEP METHODS,0.05798138869005011,"We use RS-FGSM and Free-AT with the settings recommended by Wong et al. (2020). We apply
GradAlign following the hyperparameters reported in the ofﬁcial repository 1. Golgooni et al. (2021)
recommend applying MultiGrad with n = 3 random samples, but do not provide a default hyperpa-
rameter setting for what concerns the ZeroGrad variant. Also Kim et al. (2021) do not recommend
a set of hyperparameters; for a fair comparison, we ablate them and select the ones that provide the
highest adversarial accuracy (for every combination of ϵ and dataset). We train on CIFAR-10/100
for 30 epochs and on SVHN for 15 epochs with a cyclic learning rate. Only for Free-AT, we use 96
and 48 epochs for CIFAR-10/100 and SVHN, respectively, to obtain comparable results following"
COMPARISON TO OTHER SINGLE-STEP METHODS,0.05869720830350752,1https://github.com/tml-epﬂ/understanding-fast-adv-training/
COMPARISON TO OTHER SINGLE-STEP METHODS,0.05941302791696493,Under review as a conference paper at ICLR 2022
COMPARISON TO OTHER SINGLE-STEP METHODS,0.06012884753042233,"2
4
6
8
10
12
14
16
for training and evaluation 0 10 20 30 40 50"
COMPARISON TO OTHER SINGLE-STEP METHODS,0.06084466714387974,Adversarial Accuracy
COMPARISON TO OTHER SINGLE-STEP METHODS,0.06156048675733715,CIFAR100 Dataset
COMPARISON TO OTHER SINGLE-STEP METHODS,0.06227630637079456,"N-FGSM (ours)
GradAlign
MultiGrad
ZeroGrad"
COMPARISON TO OTHER SINGLE-STEP METHODS,0.06299212598425197,"Free-AT
Kim et. al.
RS-FGSM
FGSM"
COMPARISON TO OTHER SINGLE-STEP METHODS,0.06370794559770938,"2
4
6
8
10
12
for training and evaluation 0 20 40 60 80"
COMPARISON TO OTHER SINGLE-STEP METHODS,0.06442376521116679,Adversarial Accuracy
COMPARISON TO OTHER SINGLE-STEP METHODS,0.06513958482462419,SVHN Dataset
COMPARISON TO OTHER SINGLE-STEP METHODS,0.0658554044380816,"Figure 3: Comparison of single-step methods on CIFAR-100 (left) and SVHN (right) with Preac-
tResNet18 over different perturbation radius (ϵ is divided by 255). Our method, N-FGSM, can match
or surpass state-of-the-art results while reducing the cost by a 3× factor. Adversarial accuracy is
based on PGD-50-10 and experiments are averaged over 3 seeds. Legend is shared among plots."
COMPARISON TO OTHER SINGLE-STEP METHODS,0.06657122405153901,"Wong et al. (2020) and Andriushchenko & Flammarion (2020). CIFAR-10 results are presented
in Figure 1 (middle), whereas CIFAR-100 and SVHN results are reported in Figure 3."
COMPARISON TO OTHER SINGLE-STEP METHODS,0.06728704366499642,"As previously observed, FGSM and RS-FGSM both suffer from catastrophic overﬁtting on larger
ϵ attacks. In contrast, N-FGSM prevents catastrophic overﬁtting and enjoys robustness properties
comparable or superior to GradAlign for all ϵ, while being 3 times faster. With appropriate hyper-
parameters, ZeroGrad is able to avoid catastrophic overﬁtting but obtains sub-obtimal robustness –
especially for large perturbations. Neither MultiGrad nor the method proposed by Kim et al. (2021)
can avoid catastrophic overﬁtting in all settings. We also observe that Free-AT cannot overcome
catastrophic overﬁtting as also observed by Andriushchenko & Flammarion (2020)."
RANDOMIZED ALPHA,0.06800286327845383,"5.2
RANDOMIZED ALPHA"
RANDOMIZED ALPHA,0.06871868289191124,"Kim et al. (2021) evaluate intermediate points along the RS-FGSM direction in order to pick the
“optimal” perturbation size. However, we ﬁnd that increasing the number of intermediate evaluated
points does not necessarily lead to increased adversarial accuracy. Moreover, for large perturbations
we could not prevent catastrophic overﬁtting even with twice the number of evaluations tested by
Kim et al. (2021). This motivates us to test a very simple baseline where instead of evaluating in-
termediate steps, the RS-FGSM perturbation size is randomly selected as: δ = t · δRS-FGSM where
t ∼U[0, 1]d. Interestingly, as reported in Figure 4 (left), we ﬁnd that this very simple baseline,
dubbed RandAlpha, is able to avoid catastrophic overﬁtting for all values of ϵ and outperforms Kim
et al. (2021) on CIFAR-10. This is aligned with our main ﬁnding that combining noise with adversar-
ial attacks is indeed a powerful tool that should be explored more thoroughly before developing more
expensive solutions. We reach the same conclusions for CIFAR-100 and SVHN in Appendix H."
HYPERPARAMETER SELECTION,0.06943450250536864,"5.3
HYPERPARAMETER SELECTION"
HYPERPARAMETER SELECTION,0.07015032211882606,"While FGSM relies on a ﬁxed step-size (equal to the maximum radius of perturbation to be used at
test time, i.e. α = ϵ), Wong et al. (2020) explored different values for α during the development
of RS-FGSM ﬁnding that an increase of the step-size improves the adversarial accuracy – up to
a magnitude before catastrophic overﬁtting occurs. We also ablate the value of α for N-FGSM
in Figure 4 (middle). We ﬁnd that by increasing the noise magnitude, N-FGSM can use larger α
values than RS-FGSM, without suffering from catastrophic overﬁtting. As observed by Wong et al.
(2020), this in turn leads to an increase in the adversarial accuracy at the expense of a decrease in
the clean accuracy. In light of this trade-off and following FGSM, we also use α = ϵ for N-FGSM."
HYPERPARAMETER SELECTION,0.07086614173228346,"Regarding N-FGSM noise hyperparameter, we ﬁnd k = 2ϵ works in all but one SVHN experiment
(ϵ = 12, in which we set k = 3ϵ), reducing the need for expensive hyperparameter tuning. In
comparison, GradAlign regularizer hyperparameter or ZeroGrad quantile value need to be deﬁned
for every radius with a noticeable shift between CIFAR-10 and SVHN hyperparameters, suggesting
they may require additional tuning when applied to novel datasets."
HYPERPARAMETER SELECTION,0.07158196134574088,Under review as a conference paper at ICLR 2022
HYPERPARAMETER SELECTION,0.07229778095919828,"2
4
6
8
10 12 14 16
 for trainining and evaluation 0 20 40 60 80"
HYPERPARAMETER SELECTION,0.07301360057265568,Adversarial Accuracy
HYPERPARAMETER SELECTION,0.0737294201861131,"N-FGSM
RandAlpha
Kim et. al."
HYPERPARAMETER SELECTION,0.0744452397995705,"4
6
8
10
12
14
N-FGSM step size  ( = 8) 40 50 60 70 80 90"
HYPERPARAMETER SELECTION,0.07516105941302792,Adversarial Accuracy
HYPERPARAMETER SELECTION,0.07587687902648532,"N-FGSM robust
N-FGSM clean"
HYPERPARAMETER SELECTION,0.07659269863994274,"Comparison of Training Schedules
Clean Acc
Robust Acc
Long schedule: Final model
83.18 ± 0.11
36.56 ± 0.26
Long schedule: Best model
80.8 ± 0.36
48.48 ± 0.27
Fast schedule: Final model
80.58 ± 0.22
48.12 ± 0.07"
HYPERPARAMETER SELECTION,0.07730851825340014,"Figure 4: Left: Comparison of Kim et al. (2021) with RandAlpha, our baseline where we multiply
the RS-FGSM perturbation by a value drawn uniformly in [0, 1], on CIFAR-10 and PreActResNet18
(ϵ is divided by 255). RandAlpha does not incur the extra cost of evaluating intermediate steps and
does not require hyperparameter tuning. Middle: Ablation of step size α in N-FGSM for ϵ = 8. As
we increase the magnitude of the FGSM perturbation we observe an increase in robustness coupled
with a drop on the clean accuracy. Right: Comparison of the “fast” training schedule from Wong
et al. (2020) and “long” training schedule described in Rice et al. (2020). N-FGSM shows robust
oberﬁtting but not catastrophic overﬁtting with the long schedule. Adversarial accuracy is based on
PGD-50-10 and experiments are averaged over 3 seeds."
LONG VS FAST TRAINING SCHEDULE,0.07802433786685756,"5.4
LONG VS FAST TRAINING SCHEDULE"
LONG VS FAST TRAINING SCHEDULE,0.07874015748031496,"Throughout our experiments, we used the RS-FGSM training setting introduced in Wong et al.
(2020). However, Rice et al. (2020) suggest that a longer training schedule coupled with early
stopping may lead to a boost in performance. Kim et al. (2021) and Li et al. (2020) report that
longer training schedules increase the chances of catastrophic overﬁtting for RS-FGSM and that
this limits its performance. We test the longer training schedule with N-FGSM and ﬁnd that it
presents robust overﬁtting, i.e. the adversarial accuracy on the training set keeps increasing but it
decreases when evaluated on the test set as described in Rice et al. (2020). However, it does not
suffer from catastrophic overﬁtting. In Figure 4 (right), we show the results for ϵ = 8. Although
we do observe a slight increase in performance when using the long training schedule, we ﬁnd the
performance remarkably competitive when considering the fast one which seems to avoid robust
overﬁtting (when relying on early stopping, the best models are usually found at the end). Thus,
it might be preferable to consider the fast training schedule if computational cost is an important
factor. In Appendix E, we report a robust accuracy of 47.86 ± 0.1 for GradAlign with the longer
schedule compared to 48.48 ± 0.27 for N-FGSM. Note that, in order to prevent GradAlign from
suffering from catastrophic overﬁtting for ϵ = 8, we had to increase the regularizer hyperparameter
(compared to the fast schedule), while N-FGSM is able to prevent catastrophic overﬁtting with the
same settings."
COMPARISON TO MULTI-STEP ATTACKS,0.07945597709377238,"5.5
COMPARISON TO MULTI-STEP ATTACKS"
COMPARISON TO MULTI-STEP ATTACKS,0.08017179670722978,"In Section 5.1, we compared the performance of single-step methods and observed that N-FGSM
is able to match or surpass the state-of-the-art method, i.e. GradAlign, while reducing the com-
putational cost by a factor of 3×. In this section, we compare the performance of N-FGSM with
multi-step attacks. We use PGD-2 with α = ϵ/2 and PGD-10 with α = 2 following Wong et al.
(2020) and keep the same training settings as described in Section 5.1 (note that PGD-x denotes
PGD attack with x iterations and no restarts)."
COMPARISON TO MULTI-STEP ATTACKS,0.08088761632068718,"In Figure 5, we observe that PGD-2 also presents catastrophic overﬁtting when we increase the
perturbation radius ϵ, which is consistent with results reported by Andriushchenko & Flammarion
(2020). On the other hand, despite all methods achieving comparable clean accuracy, there is a gap
on adversarial accuracy between PGD-10 and single-step methods which grows with the perturba-
tion size. This can be partially expected since the search space grows exponentially with ϵ and PGD
can explore it more thoroughly as we increase the number of iterations. Nevertheless, computing a
PGD-10 attack is 10× more expensive than computing an N-FGSM one. An important direction for
future work should be addressing this gap and analyse, both theoretically and empirically, whether
single-step methods can actually match the performance of their multi-step counterparts."
COMPARISON TO MULTI-STEP ATTACKS,0.0816034359341446,Under review as a conference paper at ICLR 2022
COMPARISON TO MULTI-STEP ATTACKS,0.082319255547602,"2
4
6
8
10
12
14
16
 for trainining and evaluation 0 20 40 60 80"
COMPARISON TO MULTI-STEP ATTACKS,0.08303507516105942,Clean and Adv. Acc.
COMPARISON TO MULTI-STEP ATTACKS,0.08375089477451682,CIFAR10 Dataset
COMPARISON TO MULTI-STEP ATTACKS,0.08446671438797423,"N-FGSM
GradAlign
PGD-10
PGD-2"
COMPARISON TO MULTI-STEP ATTACKS,0.08518253400143164,"2
4
6
8
10
12
 for trainining and evaluation 0 20 40 60 80 100"
COMPARISON TO MULTI-STEP ATTACKS,0.08589835361488905,Clean and Adv. Acc.
COMPARISON TO MULTI-STEP ATTACKS,0.08661417322834646,SVHN Dataset
COMPARISON TO MULTI-STEP ATTACKS,0.08732999284180387,"N-FGSM
GradAlign
PGD-10
PGD-2"
COMPARISON TO MULTI-STEP ATTACKS,0.08804581245526127,"Figure 5: Comparison of N-FGSM and GradAlign with multi-step methods on CIFAR-10 (Left) and
SVHN (Right) with PreactResNet18 over different perturbation radii (ϵ is divided by 255). Despite
all methods achieving comparable clean accuracy (dashed lines), there is a gap in robust accuracy
between PGD-10 and single-step methods. However, note that PGD-10 is 10× more expensive than
N-FGSM. Adversarial accuracy is based on PGD-50-10 and experiments are averaged over 3 seeds."
COMPARISON TO MULTI-STEP ATTACKS,0.08876163206871868,"5.6
EXPERIMENTS WITH WIDERESNET28-10 ARCHITECTURE"
COMPARISON TO MULTI-STEP ATTACKS,0.08947745168217609,"We also compare the performance of all methods on WideResNet28-10 (Zagoruyko & Komodakis,
2016) architecture in Figure 7 and Figure 8 (Appendix B). As in the experiments with the PreAc-
tResNet18 architecture, N-FGSM obtains state-of-the-art PGD-50-10 accuracy among single-step
methods. Nevertheless, as a general trend, we observe that catastrophic overﬁtting seems to be more
difﬁcult to prevent when using WideResNet. For instance, FGSM is able to consistently yield robust
models up to ϵ = 6 for PreActResNet18 on CIFAR-10, however, for some runs the same radius
can lead to catastrophic overﬁtting for WideResNet models. We hypothesize this is because it is
more over-parametrized –WideResNet28-10 has 36.5 M parameters, whereas PreActResNet18 has
11.2M. Regarding GradAlign, we had to increase the regularizer hyperparameter (compared to the
settings for PreActResNet18) in order to prevent catastrophic overﬁtting on CIFAR-100. Note that,
to our surprise, we could not ﬁnd a competitive hyperparameter setting for GradAlign on the SVHN
dataset for ϵ ≥6. We tried both increasing the regularizer hyperparameter and decreasing the step
size α, but some or all runs led to models close to a constant classiﬁer for each setting. We do not
claim that GradAlign will not work, but ﬁnding a good conﬁguration might require further tuning."
COMPARISON TO MULTI-STEP ATTACKS,0.0901932712956335,"On the other hand, as observed earlier, the default conﬁguration for N-FGSM (α = ϵ, k = 2ϵ)
works well in all settings except for ϵ = 16 on CIFAR-10 and ϵ = 10, 12 on SVHN. For CIFAR-10,
we increase the noise magnitude to k = 4ϵ. For SVHN, we ﬁnd that decreasing α as we tried for
GradAlign works better than increasing the noise. However, in both cases N-FGSM can obtain more
than a trivial adversarial accuracy. Results are presented in Appendix B."
DISCUSSION,0.09090909090909091,"6
DISCUSSION"
DISCUSSION,0.09162491052254831,"In this work, we explore the role of noise and clipping in single-step adversarial training. Contrary to
previous intuitions, we show that increasing the noise magnitude and removing the ϵ−ℓ∞constraint
allows improving adversarial robustness, while maintaining a competitive clean accuracy. These
ﬁndings led us to propose N-FGSM, a simple and effective approach that can match or surpass the
performance of GradAlign (Andriushchenko & Flammarion, 2020), while achieving a 3× speed-up."
DISCUSSION,0.09234073013600573,"We perform an extensive comparison with other relevant single-step methods, observing that all of
them achieve sub-optimal performance and most of them are not able to avoid catastrophic overﬁt-
ting for larger ϵ attacks. Moreover, we also analyze recent single-step methods and – inspired by
Kim et al. (2021) – observe that uniformly choosing a step-size in [0, α] avoids catastrophic over-
ﬁtting for RS-FGSM, which reinforces our intuition that random noise is a powerful tool towards
learning robust models. However, despite impressive improvements of single-step adversarial train-
ing methods, there is still a gap between single-step and multi-step methods such as PGD-10 as we
increase the ϵ radius. Therefore, future work should put an emphasis on formally understanding the
limitations of single-step adversarial training and explore how, if possible, this gap can be reduced."
DISCUSSION,0.09305654974946313,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.09377236936292055,"Ethics Statement.
The existence of adversarial examples poses a potential threat to the deploy-
ment of deep learning systems into the real world. Therefore, ﬁnding fast and effective methods
to train models robust against this threat is of utmost importance. On the other hand, adversarial
training methods are based on augmenting samples with adversarial perturbations, thus, they are
partially based on building methods to attack neural networks. Research on adversarial attacks is
naturally a sensitive path, since it can potentially be exploited for unethical purposes. However, the
scope of our work is not designing stronger attacks; rather, the methods we propose are designed ad
hoc to improve the adversarial robustness of learning systems, not to break other models’ defenses.
Therefore, although we are aware that we are carrying out research within a sensitive topic, we are
not particularly concerned that the research presented in this paper can lead to harmful applications
– on the contrary, we believe that it can help deploying safer machine learning applications."
REPRODUCIBILITY STATEMENT,0.09448818897637795,"Reproducibility Statement.
In this paper, we compare against several adversarial training meth-
ods. The general training and evaluation settings used are described at the beginning of Section 5.
Moreover, different hyperparameters have been used for each method depending on the dataset and
network. These are detailed in the manuscript, where corresponding results are discussed. For all
the reported experiments, we employ different random seeds to ensure replicability of our results,
and report performances indicating the standard deviation values. To facilitate further research on
this topic, as stated in Section 1, we will release the code to reproduce all experiments. Regarding
our theoretical results, we deferred the proofs to Appendix F."
REFERENCES,0.09520400858983537,REFERENCES
REFERENCES,0.09591982820329277,"Maksym Andriushchenko and Nicolas Flammarion. Understanding and improving fast adversarial
training. In Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.09663564781675017,"Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense
of security: Circumventing defenses to adversarial examples. In International Conference on
Machine Learning (ICML), 2018."
REFERENCES,0.09735146743020759,"Mislav Balunovic and Martin Vechev. Adversarial training and provable defenses: Bridging the gap.
In International Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.09806728704366499,"Battista Biggio and Fabio Roli.
Wild patterns: Ten years after the rise of adversarial machine
learning. Pattern Recognition, 2018."
REFERENCES,0.09878310665712241,"Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot
way to resist adversarial examples. In International Conference on Learning Representations
(ICLR), 2018."
REFERENCES,0.09949892627057981,"Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods.
In Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and
Security, 2017."
REFERENCES,0.10021474588403723,"Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certiﬁed adversarial robustness via randomized
smoothing. In International Conference on Machine Learning (ICML), 2019."
REFERENCES,0.10093056549749463,"Jacob Devlin, Ming-Wei Changm, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. In Annual Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Language Technologies
(NAACL HLT), 2019."
REFERENCES,0.10164638511095204,"Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, and Stefano Soatto. Empirical
study of the topology and geometry of deep networks. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018."
REFERENCES,0.10236220472440945,"Justin Gilmer, Nicolas Ford, Nicholas Carlini, and Ekin Cubuk. Adversarial examples are a natural
consequence of test error in noise. In International Conference on Machine Learning (ICML),
2019."
REFERENCES,0.10307802433786686,Under review as a conference paper at ICLR 2022
REFERENCES,0.10379384395132427,"Zeinab Golgooni, Mehrdad Saberi, Masih Eskandar, and Mohammad Hossein Rohban. Zerograd:
Mitigating and explaining catastrophic overﬁtting in fgsm adversarial training. arXiv:2103.15476
[cs.LG], 2021."
REFERENCES,0.10450966356478167,"Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. International Conference on Learning Representations (ICLR), 2015."
REFERENCES,0.10522548317823908,"Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering adversarial
images using input transformations. In International Conference on Learning Representations
(ICLR), 2018."
REFERENCES,0.10594130279169649,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpass-
ing human-level performance on imagenet classiﬁcation. In IEEE International Conference on
Computer Vision (ICCV), 2015."
REFERENCES,0.1066571224051539,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European Conference on Computer Vision (ECCV), 2016."
REFERENCES,0.1073729420186113,"Peilin Kang and Seyed-Mohsen Moosavi-Dezfooli. Understanding catastrophic overﬁtting in adver-
sarial training. arXiv:2105.02942 [cs.LG], 2021."
REFERENCES,0.10808876163206872,"Hoki Kim, Woojin Lee, and Jaewook Lee. Understanding catastrophic overﬁtting in single-step
adversarial training. In AAAI Conference on Artiﬁcial Intelligence (AAAI), 2021."
REFERENCES,0.10880458124552612,"Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Mas-
ter’s thesis, Department of Computer Science, University of Toronto, 2009."
REFERENCES,0.10952040085898354,"Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. 2017."
REFERENCES,0.11023622047244094,"Bai Li, Shiqi Wang, Suman Jana, and Lawrence Carin. Towards understanding fast adversarial
training. arXiv:2006.03089 [cs.LG], 2020."
REFERENCES,0.11095204008589836,"Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck,
Dawn Song, Michael E Houle, and James Bailey. Characterizing adversarial subspaces using
local intrinsic dimensionality. In International Conference on Learning Representations (ICLR),
2018."
REFERENCES,0.11166785969935576,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations (ICLR), 2018."
REFERENCES,0.11238367931281316,"Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In Neural Information Processing
Systems (NeurIPS), Workshops, 2011."
REFERENCES,0.11309949892627058,"Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In IEEE symposium on security
and privacy (SP), 2016."
REFERENCES,0.11381531853972798,"Geon Yeong Park and Sang Wan Lee. Reliably fast adversarial training via latent adversarial pertur-
bation. In International Conference on Learning Representations (ICLR), Workshops, 2021."
REFERENCES,0.1145311381531854,"Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against adversarial exam-
ples. In International Conference on Learning Representations (ICLR), 2018."
REFERENCES,0.1152469577666428,"Leslie Rice, Eric Wong, and Zico Kolter. Overﬁtting in adversarially robust deep learning. In
International Conference on Machine Learning (ICML), 2020."
REFERENCES,0.11596277738010022,"Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph
Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free!
Neu-
ral Information Processing Systems (NeurIPS), 2019."
REFERENCES,0.11667859699355762,"David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 2016."
REFERENCES,0.11739441660701504,Under review as a conference paper at ICLR 2022
REFERENCES,0.11811023622047244,"Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend:
Leveraging generative models to understand and defend against adversarial examples. In Inter-
national Conference on Learning Representations (ICLR), 2018."
REFERENCES,0.11882605583392986,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations (ICLR), 2014."
REFERENCES,0.11954187544738726,"Jinyu Tian, Jiantao Zhou, Yuanman Li, and Jia Duan. Detecting adversarial examples from sensi-
tivity inconsistency of spatial-transform domain. In AAAI Conference on Artiﬁcial Intelligence
(AAAI), 2021."
REFERENCES,0.12025769506084466,"Florian Tram`er, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. In International Conference on
Learning Representations (ICLR), 2018."
REFERENCES,0.12097351467430208,"BS Vivek and R Venkatesh Babu. Single-step adversarial training with dropout scheduling. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2020."
REFERENCES,0.12168933428775948,"Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning,
and Inderjit Dhillon. Towards fast computation of certiﬁed robustness for relu networks. In
International Conference on Machine Learning (ICML), 2018."
REFERENCES,0.1224051539012169,"Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning (ICML), 2018."
REFERENCES,0.1231209735146743,"Eric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than free: Revisiting adversarial training.
In International Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.12383679312813171,"Puyudi Yang, Jianbo Chen, Cho-Jui Hsieh, Jane-Ling Wang, and Michael Jordan. Ml-loo: Detect-
ing adversarial examples with feature attribution. In AAAI Conference on Artiﬁcial Intelligence
(AAAI), 2020."
REFERENCES,0.12455261274158912,"Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC British Machine Vision
Conference (BMVC), 2016."
REFERENCES,0.12526843235504653,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning (ICML), 2019."
REFERENCES,0.12598425196850394,Under review as a conference paper at ICLR 2022
REFERENCES,0.12670007158196134,"A
ADDITIONAL PLOTS FOR PREACTRESNET18 EXPERIMENTS"
REFERENCES,0.12741589119541877,"In the main paper we compare N-FGSM with other single-step methods and multi-step methods
separately and remove clean accuracies for better visualization. In this section we present the curves
for all methods with both the clean and robust accuracy. The tendency in the three datasets is for
N-FGSM PGD-50-10 accuracy to be slightly above that of GradAlign, while the opposite happens
to the clean accuracy. We also observe that clean accuracy becomes signiﬁcantly more noisy when
catastrophic overﬁtting happens. Exact numbers for all the curves are in Appendix L."
REFERENCES,0.12813171080887617,"2
4
6
8
10
12
14
16
 for trainining and evaluation 0 20 40 60 80"
REFERENCES,0.12884753042233357,Standard and Adversarial Accuracy
REFERENCES,0.12956335003579098,CIFAR10 Dataset
REFERENCES,0.13027916964924838,"N-FGSM (ours)
GradAlign
PGD-10
PGD-2
MultiGrad
ZeroGrad
Free-AT
Kim et. al.
RS-FGSM
FGSM"
REFERENCES,0.1309949892627058,"2
4
6
8
10
12
14
16
for trainining and evaluation 0 20 40 60"
REFERENCES,0.1317108088761632,Standard and Adversarial Accuracy
REFERENCES,0.1324266284896206,CIFAR100 Dataset
REFERENCES,0.13314244810307801,"2
4
6
8
10
12
 for trainining and evaluation 0 20 40 60 80 100"
REFERENCES,0.13385826771653545,Standard and Adversarial Accuracy
REFERENCES,0.13457408732999285,SVHN Dataset
REFERENCES,0.13528990694345025,"Figure 6: Comparison of all methods on CIFAR-10, CIFAR-100 and SVHN with PreactResNet18
over different perturbation radius (ϵ is divided by 255). We plot both the robust (solid line) and the
clean (dashed line) accuracy for each method. Our method, N-FGSM, is able to match or surpass the
state-of-the-art single-step method GradAlign while reducing the cost by a 3× factor. Adversarial
accuracy is based on PGD-50-10 and experiments are averaged over 3 seeds. Legend is shared
among all plots."
REFERENCES,0.13600572655690765,"B
EXPERIMENTS WITH WIDERESNET28-10 ARCHITECTURE"
REFERENCES,0.13672154617036505,"In this section we present the plots of our experiments with WideResNet28-10. We report the re-
sults in two ﬁgures. In Figure 7 we compare all single-step methods and we do not plot the clean
accuracy for better visualization. In Figure 8 we plot all methods, including multi-step methods, and
report the clean accuracy as well with dashed lines. Since we observed that our baseline, RandAl-
pha, outperformed Kim et al. (2021) in all settings for PreActResNet18, we only report RandAlpha
for WideResNet. As mentioned in the main paper, we observe that catastrophic overﬁtting seems
to be more difﬁcult to prevent for WideResNet. In particular, for GradAlign we observed the regu-
larizer hyperparameter settings proposed by Andriushchenko & Flammarion (2020) for CIFAR-10
(searched for a PreActResNet18) worked well. However, those parameters led to Catastrophic Over-
ﬁtting for 6 ≤ϵ ≤12 in CIFAR-100. Since ϵ = 14, 16 did not show Catastrophic Overﬁtting, we
increased the GradAlign regularizer hyperparameter λ for CIFAR-100 so that each 6 ≤ϵ ≤12
would have the default value corresponding to ϵ + 2, for instance, λ for ϵ = 6 would be the default
λ in Andriushchenko & Flammarion (2020) for ϵ = 8."
REFERENCES,0.13743736578382248,Under review as a conference paper at ICLR 2022
REFERENCES,0.1381531853972799,"2
4
6
8
10
12
14
16
for training and evaluation 0 20 40 60 80"
REFERENCES,0.1388690050107373,Adversarial Accuracy
REFERENCES,0.1395848246241947,CIFAR10 Dataset
REFERENCES,0.14030064423765212,"N-FGSM (ours)
GradAlign
MultiGrad
ZeroGrad"
REFERENCES,0.14101646385110952,"Free-AT
RandAlpha
RS-FGSM
FGSM"
REFERENCES,0.14173228346456693,"2
4
6
8
10
12
14
16
for training and evaluation 0 10 20 30 40 50"
REFERENCES,0.14244810307802433,Adversarial Accuracy
REFERENCES,0.14316392269148176,CIFAR100 Dataset
REFERENCES,0.14387974230493916,"2
4
6
8
10
12
for training and evaluation 0 20 40 60 80"
REFERENCES,0.14459556191839656,Adversarial Accuracy
REFERENCES,0.14531138153185397,SVHN Dataset
REFERENCES,0.14602720114531137,"Figure 7:
Comparison of single-step methods on CIFAR-10, CIFAR-100 and SVHN with
WideResNet28-10 over different perturbation radius (ϵ is divided by 255). Our method, N-FGSM,
is able to match or surpass the state-of-the-art single-step method GradAlign while reducing the cost
by a 3× factor. Moreover, we could not ﬁnd any competitive hyperparameter setting for GradAlign
for ϵ ≥6 in SVHN dataset. Adversarial accuracy is based on PGD-50-10 and experiments are
averaged over 3 seeds. Legend is shared among all plots."
REFERENCES,0.1467430207587688,"For SVHN we observed that the default values for λ led to models close to a constant classiﬁer
for ϵ ≥6. We tried to increase the lambda for those ϵ values to 1.25λ but observed the same
result. Since the model did not show typical catastrophic overﬁtting but rather it seemed as it was
underﬁtting, we tried to reduce the step-size to α = 0.75ϵ and also both decreasing α and increasing
λ. When reducing the step size we obtain accuracies above those of a constant classiﬁer for some
radii, however, some or all seeds converge to a constant classiﬁer for each setting, hence the large
standard deviations. For N-FGSM, the default conﬁguration of N-FGSM (α = ϵ, k = 2ϵ) works
well in all settings except for ϵ = 16 on CIFAR-10 and ϵ = 10, 12 on SVHN. For CIFAR-10,
we increase the noise magnitude to k = 4ϵ. For SVHN we ﬁnd that decreasing α as we tried for
GradAlign works better than increasing the noise. We use α = 8 for both ϵ radii. Exact numbers for
all the curves are in Appendix L"
REFERENCES,0.1474588403722262,"C
FURTHER ABLATION OF NOISE AND STEP SIZE IN N-FGSM"
REFERENCES,0.1481746599856836,"In Section 4.1, we observed that both removing clipping and increasing noise were necessary to
avoid catastrophic overﬁtting. However, when doing that, we increase the magnitude of the pertur-
bations. In this section, we study more closely the interplay between the step-size α and the noise
level k to ensure that it is indeed the increase in noise level, rather than merely increasing the mag-
nitude of perturbations, what helps stabilize adversarial training and avoid catastrophic overﬁtting."
REFERENCES,0.148890479599141,"We ﬁx ϵtest = 8/255 for evaluation, and report the clean and robust accuracy of N-FGSM under
different combinations of noise level k and step size α. Note that when the noise level k = 0
N-FGSM recovers plain FGSM, thus, as we increase α it is equivalent to using FGSM with an
increased ϵtrain. On the other hand, when α = 0, this is equivalent to training with only random
noise augmentation. Based on the results reported in Table 1, we make the following observations:"
REFERENCES,0.14960629921259844,Under review as a conference paper at ICLR 2022
REFERENCES,0.15032211882605584,"2
4
6
8
10
12
14
16
 for trainining and evaluation 0 20 40 60 80"
REFERENCES,0.15103793843951324,Standard and Adversarial Accuracy
REFERENCES,0.15175375805297064,CIFAR10 Dataset
REFERENCES,0.15246957766642805,"N-FGSM (ours)
GradAlign
PGD-10
PGD-2
MultiGrad
ZeroGrad
Free-AT
RandAlpha
RS-FGSM
FGSM"
REFERENCES,0.15318539727988548,"2
4
6
8
10
12
14
16
 for trainining and evaluation 0 20 40 60"
REFERENCES,0.15390121689334288,Standard and Adversarial Accuracy
REFERENCES,0.15461703650680028,CIFAR100 Dataset
REFERENCES,0.15533285612025768,"2
4
6
8
10
12
 for trainining and evaluation 0 20 40 60 80 100"
REFERENCES,0.15604867573371511,Standard and Adversarial Accuracy
REFERENCES,0.15676449534717252,SVHN Dataset
REFERENCES,0.15748031496062992,"Figure 8: Comparison of all methods on CIFAR-10, CIFAR-100 and SVHN with WideResNet28-10
over different perturbation radius (ϵ is divided by 255). We plot both the robust (solid line) and the
clean (dashed line) accuracy for each method. Legend is shared among all plots."
REFERENCES,0.15819613457408732,"1) Increasing the perturbation size is not enough to avoid catastrophic overﬁtting. For instance,
as observed in the ﬁrst column of Table 1, training with an increasing α without noise, i.e. (k = 0
which is equivalent to FGSM), leads to catastrophic overﬁtting despite the clear increase in pertur-
bation size due to the increase of α."
REFERENCES,0.15891195418754475,"2) Catastrophic overﬁtting leads to a vulnerable model against smaller perturbations too. For
instance, looking at the experiments with α = 10 or α = 12 in the ﬁrst column, we observe that the
resultant models are vulnerable to adversarial attacks for ϵtest = 8/255 even though the perturbation
radius used in training is larger than 8/255. This indicates that once a model catastrophically overﬁts
to perturbations of a given radius, it can be vulnerable to smaller perturbations too."
REFERENCES,0.15962777380100215,"3) Increasing the level of noise is necessary to stabilize training for larger perturbations. As
we increase the step size α of the attack, we observe that we need to increase the ratio between noise
k and step-size α in order to avoid catastrophic overﬁtting. This again further suggests that it is
indeed increasing the perturbation size by increasing the noise level (and not simply increasing the
perturbation budget) what mitigates catastrophic overﬁtting."
REFERENCES,0.16034359341445956,"4) Training with noise perturbations has a much milder effect on the clean accuracy than
adversarial training. Despite N-FGSM perturbations have a larger radius, it does not result in
a signiﬁcant drop of the clean accuracy, see Figure 5. Although it might seem counter-intuitive,
we note that while N-FGSM has a larger perturbation size, this increase is not only merely in the
adversary direction but also in the random noise direction. That is to say, while the perturbation size
is larger for N-FGSM, this is not necessarily equivalent to augmenting with adversaries with an a
similarly larger perturbation size of adversaries due to the bias in the noise. In Table 1, we observe
how augmenting training samples with noise alone (when α = 0) has a much milder effect on the
clean accuracy. In general, moving right on the table (increasing noise) is more forgiving on the clean
accuracy than moving downwards (increasing FGSM step size). This is not so surprising considering
that moving in random directions along the input space has a much lower impact on the loss than
moving along the FGSM direction (see Figure 12) and that training with noise alone does not provide
any signiﬁcant robustness against larger attacks (for a more detailed ablation see Figure 11)."
REFERENCES,0.16105941302791696,Under review as a conference paper at ICLR 2022
REFERENCES,0.16177523264137436,"k = 0 (no noise)
k = 4/255 (0.5ϵtrain)
k = 8/255 (1ϵtrain)
k = 16/255 (2ϵtrain)"
REFERENCES,0.1624910522548318,"α = 0
93.8 ± 0.14"
REFERENCES,0.1632068718682892,0.0 ± 0.0
REFERENCES,0.1639226914817466,93.53 ± 0.12
REFERENCES,0.164638511095204,0.0 ± 0.0
REFERENCES,0.16535433070866143,93.06 ± 0.02
REFERENCES,0.16607015032211883,0.0 ± 0.0
REFERENCES,0.16678596993557623,91.76 ± 0.07
REFERENCES,0.16750178954903364,0.01 ± 0.0
REFERENCES,0.16821760916249104,"α = 4
88.77 ± 0.04"
REFERENCES,0.16893342877594847,35.88 ± 0.55
REFERENCES,0.16964924838940587,88.61 ± 0.07
REFERENCES,0.17036506800286327,36.12 ± 0.09
REFERENCES,0.17108088761632068,88.42 ± 0.03
REFERENCES,0.1717967072297781,36.7 ± 0.23
REFERENCES,0.1725125268432355,87.79 ± 0.05
REFERENCES,0.1732283464566929,37.27 ± 0.23
REFERENCES,0.1739441660701503,"α = 6
85.58 ± 0.11"
REFERENCES,0.17465998568360774,43.85 ± 0.12
REFERENCES,0.17537580529706515,85.52 ± 0.23
REFERENCES,0.17609162491052255,44.14 ± 0.24
REFERENCES,0.17680744452397995,85.03 ± 0.09
REFERENCES,0.17752326413743735,44.44 ± 0.13
REFERENCES,0.17823908375089478,84.49 ± 0.1
REFERENCES,0.17895490336435219,44.44 ± 0.15
REFERENCES,0.1796707229778096,"α = 8
86.41 ± 0.7"
REFERENCES,0.180386542591267,0.0 ± 0.0
REFERENCES,0.18110236220472442,81.54 ± 0.19
REFERENCES,0.18181818181818182,47.93 ± 0.11
REFERENCES,0.18253400143163923,81.57 ± 0.07
REFERENCES,0.18324982104509663,48.16 ± 0.21
REFERENCES,0.18396564065855406,80.58 ± 0.22
REFERENCES,0.18468146027201146,48.12 ± 0.07
REFERENCES,0.18539727988546886,"α = 10
82.08 ± 1.62"
REFERENCES,0.18611309949892627,0.0 ± 0.0
REFERENCES,0.18682891911238367,82.81 ± 1.11
REFERENCES,0.1875447387258411,0.0 ± 0.0
REFERENCES,0.1882605583392985,77.32 ± 0.14
REFERENCES,0.1889763779527559,49.68 ± 0.25
REFERENCES,0.1896921975662133,76.49 ± 0.14
REFERENCES,0.19040801717967074,49.77 ± 0.37
REFERENCES,0.19112383679312814,"α = 12
80.6 ± 2.59"
REFERENCES,0.19183965640658554,0.0 ± 0.0
REFERENCES,0.19255547602004294,81.75 ± 1.1
REFERENCES,0.19327129563350035,0.0 ± 0.0
REFERENCES,0.19398711524695778,82.0 ± 1.65
REFERENCES,0.19470293486041518,0.0 ± 0.0
REFERENCES,0.19541875447387258,72.52 ± 0.16
REFERENCES,0.19613457408732998,50.17 ± 0.22
REFERENCES,0.1968503937007874,"Table 1: Ablation of the clean (top) and PGD-50-10 (bottom) accuracy when changing N-FGSM
hyperparameters – noise level k and FGSM step size α. Results are averaged over 3 seeds. All
models are evaluated with PGD-50-10 attack and ϵtest = ϵtrain = 8/255."
REFERENCES,0.19756621331424482,"D
ABLATION OF BASELINES INCREASING THE TRAINING EPSILON"
REFERENCES,0.19828203292770222,"We have seen in Theorem 1 and Appendix F that the magnitude of N-FGSM perturbations will (on
expectation) be larger than that of other baselines. Moreover, since N-FGSM does not use clipping
to ensure perturbations are within the ϵ −ℓ∞ball, they could have ℓ∞−norm of up to k + α; which
for the default hyperparameter values (α = ϵ, k = 2ϵ) add up to 3ϵ. In this section, we study the
performance of all single-step baselines as we increase the training epsilon."
REFERENCES,0.19899785254115962,"In Table 2, we present the result of an experiment where we ﬁx ϵtest = 8/255, then for each base-
line we increase the ϵtrain from 8/255 to 16/255 (while always evaluating the ﬁnal model at 8/255).
We use the same training hyperparameters as reported in Section 5.1. Results lead to three main
observations:"
REFERENCES,0.19971367215461705,"1) Increasing ϵtrain ends up hurting test robust accuracy. Even though in some methods we ob-
serve an increase in adversarial accuracy as we start to increase ϵtrain, it ends up decreasing. More-
over, this increase in adversarial accuracy (see GradAlign, MultiGrad or N-FGSM) is at the expense
of a decrase in clean accuracy."
REFERENCES,0.20042949176807445,"2) Catastrophic overﬁtting leads to a vulnerable model to smaller perturbations as well. As
observed in Appendix C, those models which suffer catastrophic overﬁtting become vulnerable to
attacks with smaller perturbation radius than used during training. Here, we observe the same effect
for various single-step baselines, which indicates this is likely a general trend."
REFERENCES,0.20114531138153185,"3) Changing N-FGSM step-size α leads to similar result to changing ϵtrain in other baselines.
Interestingly, we observe that although the presence of noise will lead to perturbations much larger
than those of other methods, it is the step-size in the direction of the FGSM attack that will have a
larger impact on robust and clean accuracy. Thus, for larger perturbations the role of noise seems to
be mainly to somehow mitigate catastrophic overﬁtting rather than strongly contributing to model
robustness."
REFERENCES,0.20186113099498926,"In light of the previous observations, we conclude that using a larger perturbation for N-FGSM
does not lead to an unfair comparison to other baselines, on the contrary, the best values for these
baselines (considering the trade-off between clean and robust accuracy) are when using the same
radius of perturbations for training and test (ϵtest = ϵtrain). On the other hand, this result highlights
the particularity of increasing the perturbation size with noise rather than following the adversarial"
REFERENCES,0.20257695060844666,Under review as a conference paper at ICLR 2022
REFERENCES,0.2032927702219041,"ϵtrain = 8/255 (1ϵtest)
ϵtrain = 12/255 (1.5ϵtest)
ϵtrain = 16/255 (2ϵtest)"
REFERENCES,0.2040085898353615,"FGSM
86.41 ± 0.7"
REFERENCES,0.2047244094488189,0.0 ± 0.0
REFERENCES,0.2054402290622763,80.6 ± 2.59
REFERENCES,0.20615604867573373,0.0 ± 0.0
REFERENCES,0.20687186828919113,77.14 ± 2.46
REFERENCES,0.20758768790264853,0.0 ± 0.0
REFERENCES,0.20830350751610593,"RS-FGSM
84.05 ± 0.13"
REFERENCES,0.20901932712956334,46.08 ± 0.18
REFERENCES,0.20973514674302077,65.22 ± 23.23
REFERENCES,0.21045096635647817,0.0 ± 0.0
REFERENCES,0.21116678596993557,76.66 ± 0.38
REFERENCES,0.21188260558339297,0.0 ± 0.0
REFERENCES,0.2125984251968504,"Kim et. al.
89.02 ± 0.1"
REFERENCES,0.2133142448103078,33.01 ± 0.09
REFERENCES,0.2140300644237652,88.35 ± 0.31
REFERENCES,0.2147458840372226,27.36±0.31
REFERENCES,0.21546170365068004,90.45 ± 0.08
REFERENCES,0.21617752326413744,9.28 ± 0.12
REFERENCES,0.21689334287759485,"AT Free
78.41 ± 0.18"
REFERENCES,0.21760916249105225,46.03 ± 0.36
REFERENCES,0.21832498210450965,73.91 ± 4.19
REFERENCES,0.21904080171796708,32.4 ± 22.91
REFERENCES,0.21975662133142448,71.64 ± 3.89
REFERENCES,0.2204724409448819,0.0 ± 0.0
REFERENCES,0.2211882605583393,"ZeroGrad
82.62 ± 0.05"
REFERENCES,0.22190408017179672,47.08 ± 0.1
REFERENCES,0.22261989978525412,78.11 ± 0.2
REFERENCES,0.22333571939871152,46.43 ± 0.37
REFERENCES,0.22405153901216893,75.42 ± 0.13
REFERENCES,0.22476735862562633,45.63 ± 0.39
REFERENCES,0.22548317823908376,"MultiGrad
82.33 ± 0.14"
REFERENCES,0.22619899785254116,47.29 ± 0.07
REFERENCES,0.22691481746599856,75.28 ± 0.2
REFERENCES,0.22763063707945597,50.0 ± 0.79
REFERENCES,0.2283464566929134,71.42 ± 5.63
REFERENCES,0.2290622763063708,16.01±22.64
REFERENCES,0.2297780959198282,"GradAlign
81.9 ± 0.22"
REFERENCES,0.2304939155332856,48.14 ± 0.15
REFERENCES,0.23120973514674303,73.29 ± 0.23
REFERENCES,0.23192555476020044,50.6 ± 0.45
REFERENCES,0.23264137437365784,61.3 ± 0.15
REFERENCES,0.23335719398711524,46.67 ± 0.29
REFERENCES,0.23407301360057264,"N-FGSM
80.58 ± 0.22"
REFERENCES,0.23478883321403007,48.12 ± 0.07
REFERENCES,0.23550465282748748,71.46 ± 0.14
REFERENCES,0.23622047244094488,50.23 ± 0.31
REFERENCES,0.23693629205440228,63.18 ± 0.49
REFERENCES,0.2376521116678597,46.46 ± 0.1
REFERENCES,0.2383679312813171,"Table 2: Ablation of the PGD-50-10 accuracy for single-step methods when increasing the ϵtrain. All
models are evaluated with PGD-50-10 attack and ϵtest = 8/255. Note that considering the trade-off
between clean and robust accuracy, all methods perform best when training with the same epsilon to
be applied at test time."
REFERENCES,0.23908375089477452,"direction. Gaining a deeper understanding of the role of noise in avoiding catastrophic overﬁtting is
a promising direction for future work."
REFERENCES,0.23979957050823192,"E
LONGER TRAINING SCHEDULE"
REFERENCES,0.24051539012168932,"In our experiments, we have followed the “fast” training schedule introduced by Wong et al. (2020).
However, Rice et al. (2020) suggest that a longer training schedule coupled with early stopping may
lead to a boost in performance. We also use the long training schedule for N-FGSM and observe that
it does not lead to catastrophic overﬁtting. In Table 3 we compare the performance of N-FGSM and
GradAlign for the long training schedule. We observe that GradAlign does not seem to beneﬁt from
the long training schedule. On the other hand, although N-FGSM seems to obtain a slight increase
in performance, the “fast” schedule provides comparable performance. It is worth mentioning that
for GradAlign, the default regularizer hyperparameter for ϵ = 8/255 and CIFAR-10 (λ = 0.2) does
not prevent catastrophic overﬁtting. We do a hyperparameter search and keep the value with the
largest ﬁnal robust accuracy (λ = 0.632)."
REFERENCES,0.24123120973514675,Under review as a conference paper at ICLR 2022
REFERENCES,0.24194702934860415,"N-FGSM
Grad Align"
REFERENCES,0.24266284896206156,"Clean Acc
Robust Acc
Clean Acc
Robust Acc"
REFERENCES,0.24337866857551896,Long schedule: Final model
REFERENCES,0.2440944881889764,"83.18 ± 0.11
36.56 ± 0.26
84.13 ± 0.24
36.17 ± 0.19"
REFERENCES,0.2448103078024338,Long schedule: Best model
REFERENCES,0.2455261274158912,"80.8 ± 0.36
48.48 ± 0.27
81.57 ± 0.44
47.86 ± 0.1"
REFERENCES,0.2462419470293486,fast schedule: Final model
REFERENCES,0.24695776664280603,"80.58 ± 0.22
48.12 ± 0.07
81.9 ± 0.22
48.14 ± 0.15"
REFERENCES,0.24767358625626343,"Table 3: Comparison of “long” (Rice et al., 2020) and “fast” (Wong et al., 2020) training schedules
for N-FGSM and GradAlign. GradAlign does not seem to beneﬁt from the long training schedule.
Although N-FGSM seems to obtain a slight increase in performance, the “fast” schedule provides
comparable performance."
REFERENCES,0.24838940586972083,Under review as a conference paper at ICLR 2022
REFERENCES,0.24910522548317823,"F
MAGNITUDE OF N-FGSM PERTURBATIONS"
REFERENCES,0.24982104509663564,"Lemma 1 (Expected perturbation). Consider the N-FGSM perturbation as deﬁned in Equation (4)
δN-FGSM = η + α · sign (∇xℓ(f(x + η), y)) , where η ∼Ω."
REFERENCES,0.25053686471009307,"Let the distribution Ωbe the uniform distribution U

[−kϵ, kϵ]d
and α > 0. Then,"
REFERENCES,0.25125268432355047,"Eη

∥δN-FGSM∥|2
2

= d
k2ϵ2"
REFERENCES,0.25196850393700787,"3
+ α2
 and"
REFERENCES,0.2526843235504653,Eη [∥δN-FGSM∥|2] ≤ s
REFERENCES,0.2534001431639227,"d
k2ϵ2"
REFERENCES,0.2541159627773801,"3
+ α2
"
REFERENCES,0.25483178239083754,"Proof. By Jensen’s inequality, we have"
REFERENCES,0.25554760200429494,"Eη [∥δN-FGSM∥2] ≤
q"
REFERENCES,0.25626342161775234,"Eη [∥δN-FGSM∥2
2]"
REFERENCES,0.25697924123120974,"Then let us consider the term Eη

∥δN-FGSM∥2
2

and use the shorthand ∇(η)i = (∇xℓ(f(x + η), y))i."
REFERENCES,0.25769506084466715,"Eη

∥δN-FGSM∥2
2

=Eη∥η + α · sign (∇xℓ(f(x + η), y)) ∥2
2 =Eη "" d
X"
REFERENCES,0.25841088045812455,"i=1
(ηi + α · sign(∇(η)i))2
# = d
X"
REFERENCES,0.25912670007158195,"i=1
Eη
h
(ηi + α · sign(∇(η)i))2i = d
X"
REFERENCES,0.25984251968503935,"i=1
Eη
h
(ηi + α · sign(∇(η)i))2 |sign(∇(η)i) = 1
i
Pη [sign(∇(η)i) = 1] + d
X"
REFERENCES,0.26055833929849676,"i=1
Eη
h
(ηi + α · sign(∇(η)i))2 |sign(∇(η)i) = −1
i
Pη [sign(∇(η)i) = −1] = d
X i=1 1
2kϵ Z kϵ"
REFERENCES,0.2612741589119542,"−kϵ
(ηi + α)2 dηi · Pη [sign(∇(η)i) = 1]"
REFERENCES,0.2619899785254116,"+
1
2kϵ d
X i=1 Z kϵ"
REFERENCES,0.262705798138869,"−kϵ
(ηi −α)2 dηi · Pη [sign(∇(η)i) = −1] = d
X i=1 1
2kϵ"
REFERENCES,0.2634216177523264,Z α+kϵ
REFERENCES,0.2641374373657838,"α−kϵ
z2dz · Pη [sign(∇(η)i) = 1]"
REFERENCES,0.2648532569792412,"+
1
2kϵ d
X i=1"
REFERENCES,0.2655690765926986,Z −α+kϵ
REFERENCES,0.26628489620615603,"−α−kϵ
z2dz · Pη [sign(∇(η)i) = −1] = d
X i=1 1
2kϵ"
REFERENCES,0.26700071581961343,Z α+kϵ
REFERENCES,0.2677165354330709,"α−kϵ
z2dz · Pη [sign(∇(η)i) = 1]"
REFERENCES,0.2684323550465283,"+
1
2kϵ d
X i=1"
REFERENCES,0.2691481746599857,Z α+kϵ
REFERENCES,0.2698639942734431,"α−kϵ
z2dz · Pη [sign(∇(η)i) = −1] = 1 2kϵ"
REFERENCES,0.2705798138869005,Z α+kϵ
REFERENCES,0.2712956335003579,"α−kϵ
z2dz d
X"
REFERENCES,0.2720114531138153,"i=1
(Pη [sign(∇(η)i) = 1] + Pη [sign(∇(η)i) = −1]) = d"
REFERENCES,0.2727272727272727,"6kϵ

(α + kϵ)3 −(α −kϵ)3
= dk2ϵ2"
REFERENCES,0.2734430923407301,"3
+ dα2"
REFERENCES,0.27415891195418757,Under review as a conference paper at ICLR 2022
REFERENCES,0.27487473156764497,"Therefore,"
REFERENCES,0.2755905511811024,Eη [∥δN-FGSM∥|2] ≤ s
REFERENCES,0.2763063707945598,"d
k2ϵ2"
REFERENCES,0.2770221904080172,"3
+ α2

."
REFERENCES,0.2777380100214746,"We state again Theorem 1 and present the proof.
Theorem 1. Let δN-FGSM be our proposed single-step method deﬁned by Equation (4), δFGSM be the
FGSM method (Goodfellow et al., 2015) and δRS-FGSM be the RS-FGSM method (Wong et al., 2020).
Then, with default hyperparameter values and for any ϵ > 0, we have that"
REFERENCES,0.278453829634932,"Eη

∥δN-FGSM∥2
2

> Eη

∥δFGSM∥2
2

> Eη

∥δRS-FGSM∥2
2

."
REFERENCES,0.2791696492483894,Proof. From Lemma 1 we have that
REFERENCES,0.27988546886184684,"Eη

∥δN-FGSM∥|2
2

= d
k2ϵ2"
REFERENCES,0.28060128847530424,"3
+ α2

."
REFERENCES,0.28131710808876165,"On the other hand, Andriushchenko & Flammarion (2020) showed that"
REFERENCES,0.28203292770221905,"Eη

∥δRS-FGSM∥2
2

= d

−1"
REFERENCES,0.28274874731567645,6ϵα3 + 1
REFERENCES,0.28346456692913385,2α2 + 1
REFERENCES,0.28418038654259126,"3ϵ2

."
REFERENCES,0.28489620615604866,"Finally, we note that
Eη

∥δFGSM∥2
2

= ∥δFGSM∥2
2 = dϵ2."
REFERENCES,0.28561202576950606,"The default hyperparameters for N-FGSM are k = 2, α = ϵ and RS-FGSM uses α = 5ϵ/4. With
these hyperparameters and any ϵ > 0 we have"
REFERENCES,0.2863278453829635,"Eη

∥δN-FGSM∥|2
2

= 7"
REFERENCES,0.2870436649964209,"3dϵ2 > Eη

∥δFGSM∥|2
2

= dϵ2 > Eη

∥δRS-FGSM∥|2
2

= 101"
REFERENCES,0.2877594846098783,128dϵ2
REFERENCES,0.2884753042233357,"In Lemma 1 we compute the expected value of the squared ℓ2 norm of N-FGSM perturbations and by
Jensen’s inequality we obtain an upper bound for the expected ℓ2 norm of N-FGSM perturbations.
However, obtaining the exact expected magnitude is more complex. To compliment our analytic
results, we approximate the ℓ2 norm of FGSM, RS-FGSM and N-FGSM via Monte Carlo sampling.
Results are presented in Figure 9. We observe that the empirical estimations are very close to the
analytical upper bounds and that indeed, N-FGSM has a magnitude signiﬁcantly above that of FGSM
or RS-FGSM."
REFERENCES,0.28919112383679313,"G
N-FGSM WITH GAUSSIAN NOISE"
REFERENCES,0.28990694345025053,"In the main paper we have only explored noise sources coming from a Uniform distribution. Since
we are measuring robustness against l∞−attacks, the Uniform distribution is a natural choice be-
cause the random perturbations will be bounded to the l∞ball deﬁned by the span of the distribution.
However, for the sake of completeness, we also explore the performance of augmenting the samples
from a Gaussian distribution where we choose its standard deviation to match that of the uniform
distribution. In Table 4 we present a comparison of the clean (top) and PGD-50-10 (bottom) accu-
racy for different values of α and noise magnitude with ϵ = 8/255. Recall that by default we use
Uniform distribution U[−k, k], therefore hyperparameter k sets the noise magnitude."
REFERENCES,0.29062276306370793,"Increasing the FGSM step size without increasing the amount of noise leads to catastrophic over-
ﬁtting. Note results for k = 0.5ϵ. More importantly, results are very similar when the two noise
distributions share the same standard deviation. Thus, using Gaussian instead of Uniform noise does
not seem to alter the results. Although this might be expected, we remark that the Gaussian is an
unbounded noise distribution and the common practice in adversarial training is to always restrict
the norm of the perturbations."
REFERENCES,0.29133858267716534,Under review as a conference paper at ICLR 2022
REFERENCES,0.29205440229062274,"2
4
6
8
10
12
14
16
Perturbation radius 5 10 15 20"
REFERENCES,0.2927702219040802,"Estimated l2
norm"
REFERENCES,0.2934860415175376,"N-FGSM
FGSM / GradAlign
RS-FGSM
Upper bound N-FGSM
Upper bound RS-FGSM"
REFERENCES,0.294201861130995,"Figure 9: Monte Carlo estimations of the expected l2−norm of perturbations from different meth-
ods and corresponding analytical upper bounds. As mentioned in Andriushchenko & Flammarion
(2020), we observe that RS-FGSM perturbations have lower l2 norm than FGSM. However, N-
FGSM perturbations have a signiﬁcantly higher l2−norm than both RS-FGSM and FGSM. This
seems to indicate that the role of random step is not simply to lower the l2 norm as previously
suggested (Andriushchenko & Flammarion, 2020)."
REFERENCES,0.2949176807444524,"Uniform Noise
Gaussian Noise"
REFERENCES,0.2956335003579098,"α = 6/255 (0.75ϵ)
α = 8/255 (1ϵ)
α = 10/255 (1.25ϵ)
α = 6/255 (0.75ϵ)
α = 8/255 (1ϵ)
α = 10/255 (1.25ϵ)"
REFERENCES,0.2963493199713672,"k = 0.5ϵ
85.52 ± 0.23"
REFERENCES,0.2970651395848246,44.14 ± 0.24
REFERENCES,0.297780959198282,81.54 ± 0.19
REFERENCES,0.2984967788117394,47.93 ± 0.11
REFERENCES,0.2992125984251969,82.81 ± 1.11
REFERENCES,0.2999284180386543,0.0 ± 0.0
REFERENCES,0.3006442376521117,85.27 ± 0.11
REFERENCES,0.3013600572655691,44.23 ± 0.17
REFERENCES,0.3020758768790265,81.71 ± 0.27
REFERENCES,0.3027916964924839,47.98 ± 0.14
REFERENCES,0.3035075161059413,83.34 ± 1.48
REFERENCES,0.3042233357193987,0.0 ± 0.0
REFERENCES,0.3049391553328561,"k = 1ϵ
85.03 ± 0.09"
REFERENCES,0.30565497494631355,44.44 ± 0.13
REFERENCES,0.30637079455977095,81.57 ± 0.07
REFERENCES,0.30708661417322836,48.16 ± 0.21
REFERENCES,0.30780243378668576,77.32 ± 0.14
REFERENCES,0.30851825340014316,49.68 ± 0.25
REFERENCES,0.30923407301360056,85.01 ± 0.17
REFERENCES,0.30994989262705797,44.41 ± 0.04
REFERENCES,0.31066571224051537,81.35 ± 0.14
REFERENCES,0.3113815318539728,48.21 ± 0.11
REFERENCES,0.31209735146743023,77.22 ± 0.32
REFERENCES,0.31281317108088763,49.83 ± 0.1
REFERENCES,0.31352899069434503,"k = 2ϵ
84.49 ± 0.1"
REFERENCES,0.31424481030780244,44.44 ± 0.15
REFERENCES,0.31496062992125984,80.58 ± 0.22
REFERENCES,0.31567644953471724,48.12 ± 0.07
REFERENCES,0.31639226914817464,76.49 ± 0.14
REFERENCES,0.31710808876163205,49.77 ± 0.37
REFERENCES,0.3178239083750895,84.35 ± 0.24
REFERENCES,0.3185397279885469,44.59 ± 0.22
REFERENCES,0.3192555476020043,80.44 ± 0.31
REFERENCES,0.3199713672154617,48.34 ± 0.1
REFERENCES,0.3206871868289191,76.33 ± 0.37
REFERENCES,0.3214030064423765,49.77 ± 0.23
REFERENCES,0.3221188260558339,"Table 4: Comparison of the clean (top) and PGD-50-10 (bottom) accuracy across different values of
step-size α and noise magnitude for the Uniform and Gaussian distributions with ϵ = 8/255. For
every value of k, we use a Gaussian with matching standard deviation. We observe that when we
match the standard deviation, both distribution perform similarly."
REFERENCES,0.3228346456692913,"H
FURTHER RESULTS WITH RANDALPHA"
REFERENCES,0.3235504652827487,"In Section 5.2 we analyze the method presented by Kim et al. (2021) and suggest a baseline where,
instead of evaluating intermediate points to determine the “optimal” step size, we simply choose
it randomly. That is, we multiply the RS-FGSM perturbation by a random scalar sampled from a
uniform distribution in [0, 1]. Interestingly we ﬁnd that it outperforms Kim et al. (2021) without the
additional cost of intermediate evaluations and without the need to perform hyperparameter selection
to ﬁnd the optimal number of intermediate evaluations."
REFERENCES,0.3242662848962062,"2
4
6
8
10 12 14 16
 for trainining and evaluation 0 20 40 60 80"
REFERENCES,0.3249821045096636,Adversarial Accuracy
REFERENCES,0.325697924123121,"N-FGSM
RandAlpha
Kim et. al."
REFERENCES,0.3264137437365784,"2
4
6
8
10 12 14 16
 for trainining and evaluation 0 10 20 30 40 50"
REFERENCES,0.3271295633500358,Adversarial Accuracy
REFERENCES,0.3278453829634932,"N-FGSM
RandAlpha
Kim et. al."
REFERENCES,0.3285612025769506,"2
4
6
8
10
12
 for trainining and evaluation 0 20 40 60 80"
REFERENCES,0.329277022190408,Adversarial Accuracy
REFERENCES,0.3299928418038654,"N-FGSM
RandAlpha
Kim et. al."
REFERENCES,0.33070866141732286,"Figure 10: Comparison of Kim et al. (2021) with RandomAlpha, our baseline where we multiply the
RS-FGSM perturbation by a scalar uniformly sampled in [0, 1]. We present results on CIFAR-10,
CIFAR-100 and SVHN with PreActResNet18."
REFERENCES,0.33142448103078026,Under review as a conference paper at ICLR 2022
REFERENCES,0.33214030064423766,"1
2
4
8
Magnitude of uniform noise 0 10 20 30 40"
REFERENCES,0.33285612025769507,Adversarial Accuracy
REFERENCES,0.33357193987115247,"= 2
= 4
= 6
= 8"
REFERENCES,0.33428775948460987,"Figure 11: Training with uniform noise augmented samples improves adversarial accuracy for small
perturbations but is not effective to protect against larger l∞radius ϵ. This motivates us to further
augment the noisy samples with FGSM. All experiments are averaged over 3 runs."
REFERENCES,0.3350035790980673,"I
TRAINING WITH NOISE AUGMENTED SAMPLES"
REFERENCES,0.3357193987115247,"Gilmer et al. (2019) and Fawzi et al. (2018) report a close link between robustness to adversarial
attacks and robustness to random noise. Actually, Gilmer et al. (2019) report that training with noise-
augmented samples can improve adversarial accuracy and vice-versa. We note that N-FGSM can
actually be seen as a combination of noise-augmentation and adversarial attacks. Here we perform
an ablation where we train models with samples augmented with uniform noise U[−k, k] and then
test the PGD-50-10 accuracy. We observe, that indeed random noise can increase the robustness to
wort-case perturbations for small ϵ −l∞balls. However, as we increase ϵ, noise augmentation is no
longer very effective. However, with N-FGSM, we apply a weak attack to these noise-augmented
samples and this seems to be enough to make them effective for adversarial training."
REFERENCES,0.3364352183249821,"J
VISUALIZATION OF THE LOSS SURFACE"
REFERENCES,0.33715103793843954,"In this section we present a visualization of the loss surface. We adapted the code from Kim et al.
(2021) to analyse the shape of the loss surface at the end of training for different methods. Kim
et al. (2021) reported that after adversarial training catastrophic overﬁtting, the loss surface would
become non-linear. In particular, they found that the FGSM perturbation seems to be misguided by
local maxima very close to the clean image that result in ineffective attacks. We note this was already
reported by Tram`er et al. (2018) which proposed to perform a random step to escape those maxima.
We argue that adding noise to the random step, when properly implemented, actually prevents those
maxima to appear in the ﬁrst place."
REFERENCES,0.33786685755189694,"K
COMPARISON OF ADVERSARIAL TRAINING COST"
REFERENCES,0.33858267716535434,"In this section we describe how we compute the relative training cost for single-step methods shown
in Figure 1 (right). We approximate the cost based on the number of forward/backward passes each
method uses, disregarding the cost of other additional operations such as adding a random step for
RS-FGSM or N-FGSM. We understand these operations have a negligible cost compared to a full
forward or backward pass."
REFERENCES,0.33929849677881174,"FGSM: FGSM is the cheapest of all methods since it only uses one forward/backward to compute
the attack and an additional forward/backward to compute the weight update. Hence, Cost FGSM =
2 F/B."
REFERENCES,0.34001431639226914,"RS-FGSM: As previously mentioned, we do not take into account the cost of random steps or
clipping, hence we consider RS-FGSM to have the same cost as standard FGSM. Cost RS-FGSM =
2 F/B."
REFERENCES,0.34073013600572655,"N-FGSM: Idem as before, cost of N-FGSM = 2 F/B."
REFERENCES,0.34144595561918395,"ZeroGrad: For ZeroGrad they need to do an additional sorting operation to ﬁnd the smallest gradi-
ent components. This could be potentially expensive, however, since the size of the input image is"
REFERENCES,0.34216177523264135,Under review as a conference paper at ICLR 2022 FGSM 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.3428775948460988,Random 0.0 0.2 0.4 0.6 0.8 1.0 Loss 0.6 0.8 1.0 1.2
REFERENCES,0.3435934144595562,N-FGSM AT model wrong
REFERENCES,0.3443092340730136,correct FGSM 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.345025053686471,Random 0.0 0.2 0.4 0.6 0.8 1.0 Loss
REFERENCES,0.3457408732999284,"0.5
0.6
0.7
0.8
0.9
1.0
1.1"
REFERENCES,0.3464566929133858,GradAlign AT model
REFERENCES,0.3471725125268432,correct FGSM 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.3478883321403006,"Random 0.0 0.2 0.4 0.6 0.8 1.0 Loss 1
2
3 4 5"
REFERENCES,0.34860415175375803,RS-FGSM AT model wrong
REFERENCES,0.3493199713672155,correct FGSM 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.3500357909806729,Random 0.0 0.2 0.4 0.6 0.8 1.0 Loss 1 2 3 4
REFERENCES,0.3507516105941303,FGSM AT model wrong
REFERENCES,0.3514674302075877,correct
REFERENCES,0.3521832498210451,"Figure 12: Visualization of the loss surface for models trained using different methods. Given a
clean sample from the test set in coordinate (0, 0), we compute the FGSM perturbation and evaluate
the loss on the subspace generated by the FGSM perturbation direction and a random direction. That
is, we evaluate xclean+t1·δFGSM+t2·δRandom, where t1, t2 ∈[0, 1]. Note that FGSM and RS-FGSM
both have catastrophic overﬁtting and the ﬁnal models present a highly non-linear loss surface, on
the other hand, both N-FGSM and GradAlign produce ﬁnal models with a very linear loss surface
which is key to obtain meaningful perturbations."
REFERENCES,0.3528990694345025,"several orders of magnitude smaller than that of the network, we also ignore this cost. Cost ZeroGrad
= 2 F/B."
REFERENCES,0.3536148890479599,"MultiGrad: MultiGrad computes 3 random steps and evaluates the gradient in all of them. There-
fore, it needs to do 3 F/B to compute the attack and an additional one to update the weights. Cost
MultiGrad = 4 F/B."
REFERENCES,0.3543307086614173,"Kim et al. (2021): Kim et al. (2021) compute the RS-FGSM perturbation and evaluate the model
on c points along this direction. Therefore, they will spend 1F/B on the RS-FGSM attack, c−1 F on
the evaluations since the clean image has already been evaluated; and 1 F/B for the weight update.
In our plot, we used c = 3 since it was the most chosen setting. Kim et al. (2021) assume the cost of
a forward is similar to that of a backward pass, following this assumption, cost of Kim et al. (2021)
is 1 F/B + 2 F + 1 F/B = 3 F/B"
REFERENCES,0.3550465282748747,"Free-AT: Shafahi et al. (2019) re-use the gradient from the previous backward pass to compute
the FGSM perturbation of the current iteration. Hence, the cost of their training is only 1 F/B per
iteration. However, Wong et al. (2020) observed they needed a longer training schedule to produce
comparable results. Therefore, the total training cost per iteration (1 F/B) is scaled by 96 in the case
of Free-AT, while it is only scaled by 30 for other methods. Relative cost Free = (96 · 1 F/B) / (30 ·
2 F/B)."
REFERENCES,0.35576234788833216,"GradAlign: Finally, GradAlign uses FGSM with a regularizer. However, this regularizer needs to
compute second-order derivatives via double backpropagation, which does not have the same cost"
REFERENCES,0.35647816750178957,Under review as a conference paper at ICLR 2022
REFERENCES,0.35719398711524697,"as regular backpropagation. Andriushchenko & Flammarion (2020) report that the cost of using
GradAlign regularizer increased the cost of FGSM by 3."
REFERENCES,0.35790980672870437,"L
DETAILED RESULTS FOR SECTION 5.1 AND SECTION 5.6"
REFERENCES,0.3586256263421618,"In this section we present the tables with the exact numbers used in plots comparing adversarial
training methods. For each method and ϵ −l∞radius, the top number is the clean accuracy while
the bottom number is the PGD-50-10 accuracy. We separate single-step from multi-step methods
with a double line."
REFERENCES,0.3593414459556192,PreActResNet18 – CIFAR-10 Dataset
REFERENCES,0.3600572655690766,"ϵ = 2/255
ϵ = 4/255
ϵ = 6/255
ϵ = 8/255
ϵ = 10/255
ϵ = 12/255
ϵ = 14/255
ϵ = 16/255"
REFERENCES,0.360773085182534,"N-FGSM
91.48 ± 0.17"
REFERENCES,0.3614889047959914,79.43 ± 0.21
REFERENCES,0.36220472440944884,88.44 ± 0.09
REFERENCES,0.36292054402290624,67.09 ± 0.31
REFERENCES,0.36363636363636365,84.72 ± 0.04
REFERENCES,0.36435218324982105,56.62 ± 0.26
REFERENCES,0.36506800286327845,80.58 ± 0.22
REFERENCES,0.36578382247673585,48.12 ± 0.07
REFERENCES,0.36649964209019326,75.98 ± 0.1
REFERENCES,0.36721546170365066,41.56 ± 0.16
REFERENCES,0.3679312813171081,71.46 ± 0.14
REFERENCES,0.3686471009305655,36.43 ± 0.16
REFERENCES,0.3693629205440229,67.11 ± 0.37
REFERENCES,0.3700787401574803,32.11 ± 0.2
REFERENCES,0.3707945597709377,63.18 ± 0.49
REFERENCES,0.37151037938439513,27.67 ± 0.93
REFERENCES,0.37222619899785253,"Grad Align
91.73 ± 0.04"
REFERENCES,0.37294201861130993,79.16 ± 0.03
REFERENCES,0.37365783822476734,88.76 ± 0.0
REFERENCES,0.3743736578382248,67.13 ± 0.26
REFERENCES,0.3750894774516822,85.67 ± 0.02
REFERENCES,0.3758052970651396,56.27 ± 0.31
REFERENCES,0.376521116678597,81.9 ± 0.22
REFERENCES,0.3772369362920544,48.14 ± 0.15
REFERENCES,0.3779527559055118,77.54 ± 0.06
REFERENCES,0.3786685755189692,40.75 ± 0.28
REFERENCES,0.3793843951324266,73.29 ± 0.23
REFERENCES,0.380100214745884,34.51 ± 0.63
REFERENCES,0.38081603435934147,68.01 ± 0.32
REFERENCES,0.3815318539727989,30.36 ± 0.27
REFERENCES,0.3822476735862563,61.3 ± 0.15
REFERENCES,0.3829634931997137,26.64 ± 0.27
REFERENCES,0.3836793128131711,"FGSM
91.6 ± 0.1"
REFERENCES,0.3843951324266285,79.35 ± 0.06
REFERENCES,0.3851109520400859,88.77 ± 0.04
REFERENCES,0.3858267716535433,67.11 ± 0.09
REFERENCES,0.3865425912670007,85.58 ± 0.11
REFERENCES,0.38725841088045815,56.33 ± 0.41
REFERENCES,0.38797423049391555,86.41 ± 0.7
REFERENCES,0.38869005010737295,0.0 ± 0.0
REFERENCES,0.38940586972083036,82.08 ± 1.62
REFERENCES,0.39012168933428776,0.0 ± 0.0
REFERENCES,0.39083750894774516,80.6 ± 2.59
REFERENCES,0.39155332856120256,0.0 ± 0.0
REFERENCES,0.39226914817465997,76.04 ± 2.37
REFERENCES,0.39298496778811737,0.0 ± 0.0
REFERENCES,0.3937007874015748,77.14 ± 2.46
REFERENCES,0.39441660701503223,0.0 ± 0.0
REFERENCES,0.39513242662848963,"RS-FGSM
92.09 ± 0.05"
REFERENCES,0.39584824624194703,78.64 ± 0.08
REFERENCES,0.39656406585540444,89.69 ± 0.01
REFERENCES,0.39727988546886184,66.12 ± 0.22
REFERENCES,0.39799570508231924,87.0 ± 0.12
REFERENCES,0.39871152469577664,54.87 ± 0.22
REFERENCES,0.3994273443092341,84.05 ± 0.13
REFERENCES,0.4001431639226915,46.08 ± 0.18
REFERENCES,0.4008589835361489,85.21 ± 0.51
REFERENCES,0.4015748031496063,0.0 ± 0.0
REFERENCES,0.4022906227630637,65.22 ± 23.23
REFERENCES,0.4030064423765211,0.0 ± 0.0
REFERENCES,0.4037222619899785,43.59 ± 25.01
REFERENCES,0.4044380816034359,0.0 ± 0.0
REFERENCES,0.4051539012168933,76.66 ± 0.38
REFERENCES,0.4058697208303508,0.0 ± 0.0
REFERENCES,0.4065855404438082,"Kim et. al.
92.85 ± 0.11"
REFERENCES,0.4073013600572656,74.74 ± 0.35
REFERENCES,0.408017179670723,91.1 ± 0.04
REFERENCES,0.4087329992841804,60.51 ± 0.4
REFERENCES,0.4094488188976378,89.34 ± 0.05
REFERENCES,0.4101646385110952,48.95 ± 0.45
REFERENCES,0.4108804581245526,89.02 ± 0.1
REFERENCES,0.41159627773801,33.01 ± 0.09
REFERENCES,0.41231209735146745,88.27 ± 0.14
REFERENCES,0.41302791696492486,24.43 ± 0.84
REFERENCES,0.41374373657838226,88.35 ± 0.31
REFERENCES,0.41445955619183966,13.11 ± 0.63
REFERENCES,0.41517537580529706,90.01 ± 0.25
REFERENCES,0.41589119541875447,5.86 ± 0.57
REFERENCES,0.41660701503221187,90.45 ± 0.08
REFERENCES,0.41732283464566927,1.88 ± 0.05
REFERENCES,0.4180386542591267,"AT Free
87.99 ± 0.16"
REFERENCES,0.41875447387258413,74.27 ± 0.33
REFERENCES,0.41947029348604153,84.98 ± 0.13
REFERENCES,0.42018611309949894,62.47 ± 0.25
REFERENCES,0.42090193271295634,81.77 ± 0.11
REFERENCES,0.42161775232641374,53.18 ± 0.15
REFERENCES,0.42233357193987114,78.41 ± 0.18
REFERENCES,0.42304939155332855,46.03 ± 0.36
REFERENCES,0.42376521116678595,74.79 ± 0.22
REFERENCES,0.42448103078024335,39.87 ± 0.07
REFERENCES,0.4251968503937008,73.91 ± 4.19
REFERENCES,0.4259126700071582,22.99 ± 16.26
REFERENCES,0.4266284896206156,61.92 ± 14.94
REFERENCES,0.427344309234073,0.0 ± 0.0
REFERENCES,0.4280601288475304,71.64 ± 3.89
REFERENCES,0.4287759484609878,0.0 ± 0.0
REFERENCES,0.4294917680744452,"ZeroGrad
91.71 ± 0.08"
REFERENCES,0.4302075876879026,79.36 ± 0.05
REFERENCES,0.4309234073013601,88.8 ± 0.11
REFERENCES,0.4316392269148175,67.32 ± 0.02
REFERENCES,0.4323550465282749,85.71 ± 0.1
REFERENCES,0.4330708661417323,56.14 ± 0.21
REFERENCES,0.4337866857551897,82.62 ± 0.05
REFERENCES,0.4345025053686471,47.08 ± 0.1
REFERENCES,0.4352183249821045,79.91 ± 0.12
REFERENCES,0.4359341445955619,37.58 ± 0.2
REFERENCES,0.4366499642090193,78.11 ± 0.2
REFERENCES,0.43736578382247676,27.41 ± 0.27
REFERENCES,0.43808160343593416,75.66 ± 0.46
REFERENCES,0.43879742304939157,21.29 ± 0.97
REFERENCES,0.43951324266284897,75.42 ± 0.13
REFERENCES,0.44022906227630637,13.06 ± 0.22
REFERENCES,0.4409448818897638,"MultiGrad
91.57 ± 0.16"
REFERENCES,0.4416607015032212,79.34 ± 0.02
REFERENCES,0.4423765211166786,88.74 ± 0.12
REFERENCES,0.443092340730136,66.81 ± 0.02
REFERENCES,0.44380816034359344,85.75 ± 0.05
REFERENCES,0.44452397995705084,56.02 ± 0.3
REFERENCES,0.44523979957050824,82.33 ± 0.14
REFERENCES,0.44595561918396565,47.29 ± 0.07
REFERENCES,0.44667143879742305,78.73 ± 0.16
REFERENCES,0.44738725841088045,40.11 ± 0.24
REFERENCES,0.44810307802433785,75.28 ± 0.2
REFERENCES,0.44881889763779526,33.87 ± 0.17
REFERENCES,0.44953471725125266,80.94 ± 5.94
REFERENCES,0.4502505368647101,9.55 ± 13.5
REFERENCES,0.4509663564781675,71.42 ± 5.63
REFERENCES,0.4516821760916249,16.35 ± 11.57
REFERENCES,0.4523979957050823,"PGD-2
91.4 ± 0.07"
REFERENCES,0.4531138153185397,79.55 ± 0.15
REFERENCES,0.45382963493199713,88.46 ± 0.13
REFERENCES,0.45454545454545453,67.62 ± 0.03
REFERENCES,0.45526127415891193,85.14 ± 0.13
REFERENCES,0.4559770937723694,57.39 ± 0.13
REFERENCES,0.4566929133858268,81.41 ± 0.05
REFERENCES,0.4574087329992842,49.58 ± 0.08
REFERENCES,0.4581245526127416,77.18 ± 0.15
REFERENCES,0.458840372226199,43.3 ± 0.11
REFERENCES,0.4595561918396564,72.9 ± 0.26
REFERENCES,0.4602720114531138,38.13 ± 0.15
REFERENCES,0.4609878310665712,70.39 ± 2.71
REFERENCES,0.4617036506800286,22.89 ± 15.26
REFERENCES,0.46241947029348607,64.81 ± 11.58
REFERENCES,0.46313528990694347,9.6 ± 13.37
REFERENCES,0.4638511095204009,"PGD-10
91.25 ± 0.04"
REFERENCES,0.4645669291338583,79.47 ± 0.13
REFERENCES,0.4652827487473157,88.34 ± 0.11
REFERENCES,0.4659985683607731,68.29 ± 0.24
REFERENCES,0.4667143879742305,84.79 ± 0.11
REFERENCES,0.4674302075876879,58.85 ± 0.18
REFERENCES,0.4681460272011453,80.71 ± 0.14
REFERENCES,0.46886184681460275,51.33 ± 0.31
REFERENCES,0.46957766642806015,76.13 ± 0.35
REFERENCES,0.47029348604151755,45.02 ± 0.49
REFERENCES,0.47100930565497495,71.24 ± 0.3
REFERENCES,0.47172512526843235,39.93 ± 0.5
REFERENCES,0.47244094488188976,66.7 ± 0.39
REFERENCES,0.47315676449534716,36.02 ± 0.67
REFERENCES,0.47387258410880456,62.11 ± 0.62
REFERENCES,0.47458840372226196,32.22 ± 0.64
REFERENCES,0.4753042233357194,Under review as a conference paper at ICLR 2022
REFERENCES,0.4760200429491768,PreActResNet18 – CIFAR-100 Dataset
REFERENCES,0.4767358625626342,"ϵ = 2/255
ϵ = 4/255
ϵ = 6/255
ϵ = 8/255
ϵ = 10/255
ϵ = 12/255
ϵ = 14/255
ϵ = 16/255"
REFERENCES,0.47745168217609163,"N-FGSM
69.12 ± 0.27"
REFERENCES,0.47816750178954903,51.02 ± 0.34
REFERENCES,0.47888332140300643,64.0 ± 0.06
REFERENCES,0.47959914101646384,39.5 ± 0.12
REFERENCES,0.48031496062992124,59.53 ± 0.02
REFERENCES,0.48103078024337864,32.06 ± 0.37
REFERENCES,0.4817465998568361,54.9 ± 0.2
REFERENCES,0.4824624194702935,26.46 ± 0.22
REFERENCES,0.4831782390837509,50.6 ± 0.16
REFERENCES,0.4838940586972083,22.23 ± 0.17
REFERENCES,0.4846098783106657,46.06 ± 0.14
REFERENCES,0.4853256979241231,18.95 ± 0.15
REFERENCES,0.4860415175375805,41.67 ± 0.25
REFERENCES,0.4867573371510379,16.33 ± 0.15
REFERENCES,0.4874731567644954,37.91 ± 0.11
REFERENCES,0.4881889763779528,14.34 ± 0.07
REFERENCES,0.4889047959914102,"Grad Align
68.96 ± 0.15"
REFERENCES,0.4896206156048676,51.31 ± 0.12
REFERENCES,0.490336435218325,64.71 ± 0.16
REFERENCES,0.4910522548317824,39.37 ± 0.25
REFERENCES,0.4917680744452398,60.42 ± 0.23
REFERENCES,0.4924838940586972,31.91 ± 0.28
REFERENCES,0.4931997136721546,56.53 ± 0.31
REFERENCES,0.49391553328561205,25.8 ± 0.14
REFERENCES,0.49463135289906945,54.06 ± 0.44
REFERENCES,0.49534717251252686,18.7 ± 1.92
REFERENCES,0.49606299212598426,48.87 ± 0.32
REFERENCES,0.49677881173944166,17.86 ± 0.04
REFERENCES,0.49749463135289906,43.84 ± 0.14
REFERENCES,0.49821045096635647,15.51 ± 0.16
REFERENCES,0.49892627057981387,38.93 ± 0.21
REFERENCES,0.49964209019327127,13.62 ± 0.19
REFERENCES,0.5003579098067287,"FGSM
69.01 ± 0.13"
REFERENCES,0.5010737294201861,51.3 ± 0.19
REFERENCES,0.5017895490336435,64.47 ± 0.15
REFERENCES,0.5025053686471009,39.7 ± 0.16
REFERENCES,0.5032211882605584,63.85 ± 2.18
REFERENCES,0.5039370078740157,10.93 ± 14.64
REFERENCES,0.5046528274874732,53.42 ± 0.65
REFERENCES,0.5053686471009305,0.0 ± 0.0
REFERENCES,0.506084466714388,45.06 ± 2.29
REFERENCES,0.5068002863278454,0.0 ± 0.0
REFERENCES,0.5075161059413028,46.14 ± 2.58
REFERENCES,0.5082319255547602,0.0 ± 0.0
REFERENCES,0.5089477451682176,41.66 ± 0.88
REFERENCES,0.5096635647816751,0.0 ± 0.0
REFERENCES,0.5103793843951324,44.68 ± 1.74
REFERENCES,0.5110952040085899,0.0 ± 0.0
REFERENCES,0.5118110236220472,"RS-FGSM
69.83 ± 0.29"
REFERENCES,0.5125268432355047,50.13 ± 0.32
REFERENCES,0.513242662848962,65.9 ± 0.36
REFERENCES,0.5139584824624195,38.36 ± 0.19
REFERENCES,0.5146743020758768,62.15 ± 0.23
REFERENCES,0.5153901216893343,30.82 ± 0.08
REFERENCES,0.5161059413027917,55.26 ± 6.86
REFERENCES,0.5168217609162491,0.01 ± 0.01
REFERENCES,0.5175375805297066,32.33 ± 12.12
REFERENCES,0.5182534001431639,0.0 ± 0.0
REFERENCES,0.5189692197566214,36.07 ± 2.59
REFERENCES,0.5196850393700787,0.0 ± 0.0
REFERENCES,0.5204008589835362,21.52 ± 5.56
REFERENCES,0.5211166785969935,0.0 ± 0.0
REFERENCES,0.521832498210451,20.38 ± 6.15
REFERENCES,0.5225483178239084,0.0 ± 0.0
REFERENCES,0.5232641374373658,"Kim et. al.
72.92 ± 0.41"
REFERENCES,0.5239799570508232,44.19 ± 0.25
REFERENCES,0.5246957766642806,70.16 ± 0.07
REFERENCES,0.525411596277738,30.63 ± 0.28
REFERENCES,0.5261274158911954,67.98 ± 0.19
REFERENCES,0.5268432355046528,22.0 ± 0.02
REFERENCES,0.5275590551181102,68.07 ± 0.1
REFERENCES,0.5282748747315676,12.75 ± 0.21
REFERENCES,0.5289906943450251,68.37 ± 0.21
REFERENCES,0.5297065139584825,6.98 ± 0.23
REFERENCES,0.5304223335719399,74.09 ± 0.06
REFERENCES,0.5311381531853973,0.0 ± 0.0
REFERENCES,0.5318539727988547,74.06 ± 0.34
REFERENCES,0.5325697924123121,0.0 ± 0.0
REFERENCES,0.5332856120257695,74.01 ± 0.36
REFERENCES,0.5340014316392269,0.0 ± 0.0
REFERENCES,0.5347172512526843,"AT Free
63.01 ± 0.19"
REFERENCES,0.5354330708661418,45.7 ± 0.33
REFERENCES,0.5361488904795991,59.41 ± 0.27
REFERENCES,0.5368647100930566,35.95 ± 0.09
REFERENCES,0.5375805297065139,55.43 ± 0.37
REFERENCES,0.5382963493199714,29.37 ± 0.21
REFERENCES,0.5390121689334287,51.91 ± 0.08
REFERENCES,0.5397279885468862,24.32 ± 0.4
REFERENCES,0.5404438081603435,48.11 ± 0.09
REFERENCES,0.541159627773801,20.64 ± 0.22
REFERENCES,0.5418754473872585,43.48 ± 1.25
REFERENCES,0.5425912670007158,5.71 ± 8.05
REFERENCES,0.5433070866141733,18.33 ± 4.86
REFERENCES,0.5440229062276306,0.0 ± 0.0
REFERENCES,0.5447387258410881,20.43 ± 11.25
REFERENCES,0.5454545454545454,0.0 ± 0.0
REFERENCES,0.5461703650680029,"ZeroGrad
69.35 ± 0.36"
REFERENCES,0.5468861846814602,51.1 ± 0.09
REFERENCES,0.5476020042949177,64.59 ± 0.32
REFERENCES,0.5483178239083751,39.38 ± 0.15
REFERENCES,0.5490336435218325,60.69 ± 0.09
REFERENCES,0.5497494631352899,31.72 ± 0.21
REFERENCES,0.5504652827487473,56.94 ± 0.13
REFERENCES,0.5511811023622047,25.87 ± 0.09
REFERENCES,0.5518969219756621,54.55 ± 0.17
REFERENCES,0.5526127415891195,19.49 ± 0.08
REFERENCES,0.5533285612025769,52.97 ± 0.34
REFERENCES,0.5540443808160344,14.32 ± 0.08
REFERENCES,0.5547602004294918,50.87 ± 0.26
REFERENCES,0.5554760200429492,10.92 ± 0.59
REFERENCES,0.5561918396564066,50.73 ± 0.3
REFERENCES,0.556907659269864,7.3 ± 0.16
REFERENCES,0.5576234788833214,"MultiGrad
69.01 ± 0.16"
REFERENCES,0.5583392984967788,51.15 ± 0.03
REFERENCES,0.5590551181102362,64.44 ± 0.11
REFERENCES,0.5597709377236937,39.16 ± 0.03
REFERENCES,0.560486757337151,60.65 ± 0.26
REFERENCES,0.5612025769506085,31.73 ± 0.09
REFERENCES,0.5619183965640658,56.84 ± 0.2
REFERENCES,0.5626342161775233,25.96 ± 0.11
REFERENCES,0.5633500357909806,53.62 ± 0.25
REFERENCES,0.5640658554044381,21.37 ± 0.16
REFERENCES,0.5647816750178954,53.05 ± 1.85
REFERENCES,0.5654974946313529,9.57 ± 7.32
REFERENCES,0.5662133142448104,48.28 ± 0.66
REFERENCES,0.5669291338582677,3.2 ± 4.49
REFERENCES,0.5676449534717252,45.28 ± 11.14
REFERENCES,0.5683607730851825,0.0 ± 0.0
REFERENCES,0.56907659269864,"PGD-2
69.18 ± 0.1"
REFERENCES,0.5697924123120973,51.36 ± 0.03
REFERENCES,0.5705082319255548,64.32 ± 0.14
REFERENCES,0.5712240515390121,40.06 ± 0.14
REFERENCES,0.5719398711524696,60.21 ± 0.13
REFERENCES,0.572655690765927,32.99 ± 0.24
REFERENCES,0.5733715103793844,55.8 ± 0.16
REFERENCES,0.5740873299928418,27.38 ± 0.16
REFERENCES,0.5748031496062992,51.68 ± 0.1
REFERENCES,0.5755189692197566,23.39 ± 0.19
REFERENCES,0.576234788833214,48.2 ± 0.1
REFERENCES,0.5769506084466715,19.83 ± 0.29
REFERENCES,0.5776664280601288,46.14 ± 1.24
REFERENCES,0.5783822476735863,10.55 ± 7.51
REFERENCES,0.5790980672870437,37.97 ± 10.52
REFERENCES,0.5798138869005011,4.79 ± 6.75
REFERENCES,0.5805297065139585,"PGD-10
68.83 ± 0.07"
REFERENCES,0.5812455261274159,51.51 ± 0.27
REFERENCES,0.5819613457408733,63.87 ± 0.09
REFERENCES,0.5826771653543307,40.59 ± 0.36
REFERENCES,0.5833929849677881,59.37 ± 0.07
REFERENCES,0.5841088045812455,33.65 ± 0.02
REFERENCES,0.5848246241947029,54.79 ± 0.38
REFERENCES,0.5855404438081604,28.55 ± 0.27
REFERENCES,0.5862562634216177,50.53 ± 0.15
REFERENCES,0.5869720830350752,24.17 ± 0.12
REFERENCES,0.5876879026485325,46.05 ± 0.21
REFERENCES,0.58840372226199,21.2 ± 0.12
REFERENCES,0.5891195418754474,41.76 ± 0.07
REFERENCES,0.5898353614889048,18.72 ± 0.06
REFERENCES,0.5905511811023622,37.81 ± 0.14
REFERENCES,0.5912670007158196,16.59 ± 0.16
REFERENCES,0.5919828203292771,PreActResNet18 – SVHN Dataset
REFERENCES,0.5926986399427344,"ϵ = 2/255
ϵ = 4/255
ϵ = 6/255
ϵ = 8/255
ϵ = 10/255
ϵ = 12/255"
REFERENCES,0.5934144595561919,"N-FGSM
96.01 ± 0.04"
REFERENCES,0.5941302791696492,86.44 ± 0.1
REFERENCES,0.5948460987831067,94.54 ± 0.15
REFERENCES,0.595561918396564,72.53 ± 0.19
REFERENCES,0.5962777380100215,92.25 ± 0.33
REFERENCES,0.5969935576234788,58.42 ± 0.14
REFERENCES,0.5977093772369363,89.56 ± 0.49
REFERENCES,0.5984251968503937,45.63 ± 0.11
REFERENCES,0.5991410164638511,86.74 ± 0.86
REFERENCES,0.5998568360773086,33.96 ± 0.49
REFERENCES,0.6005726556907659,81.48 ± 1.64
REFERENCES,0.6012884753042234,26.13 ± 0.81
REFERENCES,0.6020042949176807,"Grad Align
96.02 ± 0.05"
REFERENCES,0.6027201145311382,86.43 ± 0.1
REFERENCES,0.6034359341445955,94.56 ± 0.21
REFERENCES,0.604151753758053,72.12 ± 0.19
REFERENCES,0.6048675733715104,92.53 ± 0.24
REFERENCES,0.6055833929849678,57.34 ± 0.24
REFERENCES,0.6062992125984252,90.1 ± 0.34
REFERENCES,0.6070150322118826,43.85 ± 0.14
REFERENCES,0.60773085182534,87.23 ± 0.75
REFERENCES,0.6084466714387974,32.87 ± 0.33
REFERENCES,0.6091624910522548,84.01 ± 0.46
REFERENCES,0.6098783106657122,23.62 ± 0.41
REFERENCES,0.6105941302791696,"FGSM
96.04 ± 0.07"
REFERENCES,0.6113099498926271,86.5 ± 0.05
REFERENCES,0.6120257695060844,95.67 ± 0.07
REFERENCES,0.6127415891195419,13.61 ± 5.83
REFERENCES,0.6134574087329993,93.73 ± 0.68
REFERENCES,0.6141732283464567,0.56 ± 0.72
REFERENCES,0.6148890479599141,91.74 ± 0.86
REFERENCES,0.6156048675733715,0.26 ± 0.36
REFERENCES,0.6163206871868289,90.76 ± 0.63
REFERENCES,0.6170365068002863,0.07 ± 0.1
REFERENCES,0.6177523264137438,87.17 ± 0.43
REFERENCES,0.6184681460272011,0.0 ± 0.0
REFERENCES,0.6191839656406586,"RS-FGSM
96.18 ± 0.11"
REFERENCES,0.6198997852541159,86.16 ± 0.14
REFERENCES,0.6206156048675734,95.09 ± 0.09
REFERENCES,0.6213314244810307,71.28 ± 0.4
REFERENCES,0.6220472440944882,95.11 ± 0.44
REFERENCES,0.6227630637079457,0.11 ± 0.08
REFERENCES,0.623478883321403,94.46 ± 0.16
REFERENCES,0.6241947029348605,0.0 ± 0.0
REFERENCES,0.6249105225483178,93.88 ± 0.24
REFERENCES,0.6256263421617753,0.0 ± 0.0
REFERENCES,0.6263421617752326,92.74 ± 0.5
REFERENCES,0.6270579813886901,0.0 ± 0.0
REFERENCES,0.6277738010021474,"Kim et. al.
96.35 ± 0.02"
REFERENCES,0.6284896206156049,83.26 ± 0.24
REFERENCES,0.6292054402290623,95.25 ± 0.08
REFERENCES,0.6299212598425197,66.32 ± 0.63
REFERENCES,0.6306370794559771,94.83 ± 0.02
REFERENCES,0.6313528990694345,48.27 ± 0.52
REFERENCES,0.6320687186828919,94.88 ± 0.29
REFERENCES,0.6327845382963493,31.8 ± 1.1
REFERENCES,0.6335003579098067,96.61 ± 0.09
REFERENCES,0.6342161775232641,0.18 ± 0.21
REFERENCES,0.6349319971367215,96.61 ± 0.01
REFERENCES,0.635647816750179,0.0 ± 0.0
REFERENCES,0.6363636363636364,"AT Free
95.01 ± 0.09"
REFERENCES,0.6370794559770938,84.55 ± 0.27
REFERENCES,0.6377952755905512,93.66 ± 0.12
REFERENCES,0.6385110952040086,71.61 ± 0.75
REFERENCES,0.639226914817466,91.72 ± 0.29
REFERENCES,0.6399427344309234,59.31 ± 1.0
REFERENCES,0.6406585540443808,91.29 ± 4.07
REFERENCES,0.6413743736578382,0.01 ± 0.0
REFERENCES,0.6420901932712957,91.86 ± 3.66
REFERENCES,0.642806012884753,0.0 ± 0.0
REFERENCES,0.6435218324982105,92.36 ± 1.0
REFERENCES,0.6442376521116678,0.0 ± 0.0
REFERENCES,0.6449534717251253,"ZeroGrad
96.06 ± 0.03"
REFERENCES,0.6456692913385826,86.43 ± 0.1
REFERENCES,0.6463851109520401,94.81 ± 0.16
REFERENCES,0.6471009305654974,71.59 ± 0.22
REFERENCES,0.6478167501789549,93.53 ± 0.26
REFERENCES,0.6485325697924124,51.72 ± 0.53
REFERENCES,0.6492483894058697,92.42 ± 1.29
REFERENCES,0.6499642090193272,35.93 ± 2.73
REFERENCES,0.6506800286327845,90.34 ± 0.32
REFERENCES,0.651395848246242,21.34 ± 0.31
REFERENCES,0.6521116678596993,88.09 ± 0.4
REFERENCES,0.6528274874731568,14.14 ± 0.32
REFERENCES,0.6535433070866141,"MultiGrad
96.01 ± 0.08"
REFERENCES,0.6542591267000716,86.4 ± 0.08
REFERENCES,0.654974946313529,94.71 ± 0.17
REFERENCES,0.6556907659269864,71.98 ± 0.26
REFERENCES,0.6564065855404438,95.75 ± 0.58
REFERENCES,0.6571224051539012,28.1 ± 18.85
REFERENCES,0.6578382247673586,94.86 ± 0.97
REFERENCES,0.658554044380816,11.49 ± 16.19
REFERENCES,0.6592698639942735,94.7 ± 0.12
REFERENCES,0.6599856836077308,0.0 ± 0.0
REFERENCES,0.6607015032211883,94.48 ± 0.19
REFERENCES,0.6614173228346457,0.0 ± 0.0
REFERENCES,0.6621331424481031,"PGD-2
96.03 ± 0.14"
REFERENCES,0.6628489620615605,86.72 ± 0.06
REFERENCES,0.6635647816750179,94.66 ± 0.1
REFERENCES,0.6642806012884753,73.29 ± 0.29
REFERENCES,0.6649964209019327,93.77 ± 0.61
REFERENCES,0.6657122405153901,60.53 ± 0.73
REFERENCES,0.6664280601288475,94.63 ± 1.29
REFERENCES,0.6671438797423049,20.68 ± 18.56
REFERENCES,0.6678596993557624,84.09 ± 14.99
REFERENCES,0.6685755189692197,0.41 ± 0.29
REFERENCES,0.6692913385826772,94.16 ± 0.54
REFERENCES,0.6700071581961345,0.02 ± 0.03
REFERENCES,0.670722977809592,"PGD-10
95.92 ± 0.08"
REFERENCES,0.6714387974230493,86.94 ± 0.14
REFERENCES,0.6721546170365068,94.37 ± 0.13
REFERENCES,0.6728704366499642,74.76 ± 0.19
REFERENCES,0.6735862562634216,92.46 ± 0.25
REFERENCES,0.6743020758768791,63.9 ± 0.48
REFERENCES,0.6750178954903364,89.67 ± 0.34
REFERENCES,0.6757337151037939,53.95 ± 0.55
REFERENCES,0.6764495347172512,85.75 ± 0.65
REFERENCES,0.6771653543307087,44.91 ± 0.45
REFERENCES,0.677881173944166,80.08 ± 0.93
REFERENCES,0.6785969935576235,37.65 ± 0.53
REFERENCES,0.6793128131710809,Under review as a conference paper at ICLR 2022
REFERENCES,0.6800286327845383,WideResNet28-10 – CIFAR-10 Dataset
REFERENCES,0.6807444523979957,"ϵ = 2/255
ϵ = 4/255
ϵ = 6/255
ϵ = 8/255
ϵ = 10/255
ϵ = 12/255
ϵ = 14/255
ϵ = 16/255"
REFERENCES,0.6814602720114531,"N-FGSM
92.51 ± 0.11"
REFERENCES,0.6821760916249106,81.43 ± 0.3
REFERENCES,0.6828919112383679,89.65 ± 0.09
REFERENCES,0.6836077308518254,69.11 ± 0.24
REFERENCES,0.6843235504652827,85.8 ± 0.23
REFERENCES,0.6850393700787402,58.29 ± 0.14
REFERENCES,0.6857551896921976,81.59 ± 0.32
REFERENCES,0.686471009305655,49.53 ± 0.25
REFERENCES,0.6871868289191124,76.92 ± 0.04
REFERENCES,0.6879026485325698,42.37 ± 0.36
REFERENCES,0.6886184681460272,72.13 ± 0.15
REFERENCES,0.6893342877594846,36.85 ± 0.2
REFERENCES,0.690050107372942,67.82 ± 0.43
REFERENCES,0.6907659269863994,31.66 ± 0.6
REFERENCES,0.6914817465998568,56.73 ± 0.42
REFERENCES,0.6921975662133143,25.01 ± 0.23
REFERENCES,0.6929133858267716,"Grad Align
92.59 ± 0.05"
REFERENCES,0.6936292054402291,81.33 ± 0.4
REFERENCES,0.6943450250536864,89.95 ± 0.3
REFERENCES,0.6950608446671439,69.81 ± 0.47
REFERENCES,0.6957766642806013,86.98 ± 0.06
REFERENCES,0.6964924838940587,59.0 ± 0.13
REFERENCES,0.6972083035075161,83.19 ± 0.26
REFERENCES,0.6979241231209735,50.0 ± 0.05
REFERENCES,0.698639942734431,79.35 ± 0.26
REFERENCES,0.6993557623478883,41.48 ± 0.51
REFERENCES,0.7000715819613458,73.79 ± 0.72
REFERENCES,0.7007874015748031,35.06 ± 0.74
REFERENCES,0.7015032211882606,66.38 ± 0.53
REFERENCES,0.7022190408017179,30.83 ± 0.39
REFERENCES,0.7029348604151754,57.75 ± 0.75
REFERENCES,0.7036506800286327,26.26 ± 0.13
REFERENCES,0.7043664996420902,"FGSM
92.65 ± 0.17"
REFERENCES,0.7050823192555477,81.38 ± 0.22
REFERENCES,0.705798138869005,90.06 ± 0.18
REFERENCES,0.7065139584824625,69.59 ± 0.25
REFERENCES,0.7072297780959198,87.99 ± 1.3
REFERENCES,0.7079455977093773,38.69 ± 26.54
REFERENCES,0.7086614173228346,86.46 ± 0.45
REFERENCES,0.7093772369362921,0.0 ± 0.0
REFERENCES,0.7100930565497494,82.67 ± 1.78
REFERENCES,0.7108088761632069,0.0 ± 0.0
REFERENCES,0.7115246957766643,80.14 ± 1.2
REFERENCES,0.7122405153901217,0.0 ± 0.0
REFERENCES,0.7129563350035791,74.54 ± 4.01
REFERENCES,0.7136721546170365,0.0 ± 0.0
REFERENCES,0.7143879742304939,71.56 ± 3.78
REFERENCES,0.7151037938439513,0.0 ± 0.0
REFERENCES,0.7158196134574087,"RS-FGSM
92.85 ± 0.1"
REFERENCES,0.7165354330708661,80.9 ± 0.13
REFERENCES,0.7172512526843235,90.73 ± 0.2
REFERENCES,0.717967072297781,68.23 ± 0.17
REFERENCES,0.7186828919112384,88.24 ± 0.19
REFERENCES,0.7193987115246958,57.21 ± 0.17
REFERENCES,0.7201145311381532,83.64 ± 1.74
REFERENCES,0.7208303507516106,0.0 ± 0.0
REFERENCES,0.721546170365068,82.1 ± 1.45
REFERENCES,0.7222619899785254,0.0 ± 0.0
REFERENCES,0.7229778095919828,78.62 ± 0.7
REFERENCES,0.7236936292054402,0.0 ± 0.0
REFERENCES,0.7244094488188977,73.25 ± 8.16
REFERENCES,0.725125268432355,0.0 ± 0.0
REFERENCES,0.7258410880458125,68.64 ± 4.3
REFERENCES,0.7265569076592698,0.0 ± 0.0
REFERENCES,0.7272727272727273,"RandAlpha
93.37 ± 0.22"
REFERENCES,0.7279885468861846,77.67 ± 0.66
REFERENCES,0.7287043664996421,92.17 ± 0.21
REFERENCES,0.7294201861130994,63.73 ± 0.31
REFERENCES,0.7301360057265569,90.71 ± 0.14
REFERENCES,0.7308518253400144,50.4 ± 0.14
REFERENCES,0.7315676449534717,89.16 ± 0.19
REFERENCES,0.7322834645669292,39.37 ± 0.42
REFERENCES,0.7329992841803865,87.44 ± 0.31
REFERENCES,0.733715103793844,30.13 ± 0.9
REFERENCES,0.7344309234073013,85.69 ± 0.28
REFERENCES,0.7351467430207588,23.13 ± 0.33
REFERENCES,0.7358625626342162,83.98 ± 0.24
REFERENCES,0.7365783822476736,16.0 ± 0.22
REFERENCES,0.737294201861131,83.23 ± 0.46
REFERENCES,0.7380100214745884,8.47 ± 0.66
REFERENCES,0.7387258410880458,"AT Free
90.66 ± 0.25"
REFERENCES,0.7394416607015032,77.0 ± 0.27
REFERENCES,0.7401574803149606,88.37 ± 0.15
REFERENCES,0.740873299928418,64.25 ± 0.33
REFERENCES,0.7415891195418755,86.11 ± 0.29
REFERENCES,0.7423049391553329,53.76 ± 0.48
REFERENCES,0.7430207587687903,83.5 ± 0.27
REFERENCES,0.7437365783822477,44.85 ± 0.39
REFERENCES,0.7444523979957051,80.52 ± 0.32
REFERENCES,0.7451682176091625,31.87 ± 5.53
REFERENCES,0.7458840372226199,83.59 ± 1.35
REFERENCES,0.7465998568360773,0.0 ± 0.0
REFERENCES,0.7473156764495347,39.58 ± 15.8
REFERENCES,0.7480314960629921,0.0 ± 0.0
REFERENCES,0.7487473156764496,42.59 ± 27.96
REFERENCES,0.7494631352899069,0.0 ± 0.0
REFERENCES,0.7501789549033644,"ZeroGrad
92.62 ± 0.11"
REFERENCES,0.7508947745168217,81.42 ± 0.28
REFERENCES,0.7516105941302792,90.17 ± 0.05
REFERENCES,0.7523264137437365,69.28 ± 0.29
REFERENCES,0.753042233357194,86.98 ± 0.28
REFERENCES,0.7537580529706513,58.4 ± 0.14
REFERENCES,0.7544738725841088,84.25 ± 0.28
REFERENCES,0.7551896921975663,48.29 ± 0.16
REFERENCES,0.7559055118110236,81.72 ± 0.29
REFERENCES,0.7566213314244811,36.08 ± 0.29
REFERENCES,0.7573371510379384,79.24 ± 0.82
REFERENCES,0.7580529706513959,28.24 ± 1.79
REFERENCES,0.7587687902648532,78.14 ± 0.46
REFERENCES,0.7594846098783107,18.54 ± 0.31
REFERENCES,0.760200429491768,75.34 ± 0.12
REFERENCES,0.7609162491052255,14.6 ± 0.12
REFERENCES,0.7616320687186829,"MultiGrad
92.64 ± 0.1"
REFERENCES,0.7623478883321403,81.19 ± 0.28
REFERENCES,0.7630637079455977,90.18 ± 0.13
REFERENCES,0.7637795275590551,69.3 ± 0.2
REFERENCES,0.7644953471725126,87.11 ± 0.36
REFERENCES,0.7652111667859699,57.98 ± 0.08
REFERENCES,0.7659269863994274,83.87 ± 0.46
REFERENCES,0.7666428060128847,48.74 ± 0.09
REFERENCES,0.7673586256263422,80.89 ± 0.14
REFERENCES,0.7680744452397996,41.22 ± 0.57
REFERENCES,0.768790264853257,82.88 ± 2.85
REFERENCES,0.7695060844667144,4.46 ± 6.09
REFERENCES,0.7702219040801718,86.6 ± 1.52
REFERENCES,0.7709377236936292,0.0 ± 0.0
REFERENCES,0.7716535433070866,85.46 ± 3.73
REFERENCES,0.772369362920544,0.0 ± 0.0
REFERENCES,0.7730851825340014,"PGD-2
92.69 ± 0.14"
REFERENCES,0.7738010021474588,81.54 ± 0.18
REFERENCES,0.7745168217609163,90.18 ± 0.19
REFERENCES,0.7752326413743736,69.87 ± 0.26
REFERENCES,0.7759484609878311,86.87 ± 0.18
REFERENCES,0.7766642806012884,59.4 ± 0.19
REFERENCES,0.7773801002147459,83.31 ± 0.16
REFERENCES,0.7780959198282033,50.88 ± 0.16
REFERENCES,0.7788117394416607,79.61 ± 0.47
REFERENCES,0.7795275590551181,43.94 ± 0.24
REFERENCES,0.7802433786685755,75.81 ± 0.24
REFERENCES,0.780959198282033,37.77 ± 0.57
REFERENCES,0.7816750178954903,71.41 ± 1.38
REFERENCES,0.7823908375089478,21.06 ± 13.39
REFERENCES,0.7831066571224051,67.2 ± 14.94
REFERENCES,0.7838224767358626,0.0 ± 0.0
REFERENCES,0.7845382963493199,"PGD-10
92.24 ± 0.31"
REFERENCES,0.7852541159627774,81.18 ± 0.57
REFERENCES,0.7859699355762347,89.65 ± 0.33
REFERENCES,0.7866857551896922,70.34 ± 0.26
REFERENCES,0.7874015748031497,86.91 ± 0.51
REFERENCES,0.788117394416607,60.59 ± 0.21
REFERENCES,0.7888332140300645,82.82 ± 0.7
REFERENCES,0.7895490336435218,52.58 ± 0.2
REFERENCES,0.7902648532569793,78.63 ± 0.66
REFERENCES,0.7909806728704366,45.92 ± 0.38
REFERENCES,0.7916964924838941,74.0 ± 0.67
REFERENCES,0.7924123120973514,40.44 ± 0.17
REFERENCES,0.7931281317108089,68.6 ± 0.58
REFERENCES,0.7938439513242663,35.98 ± 0.56
REFERENCES,0.7945597709377237,64.17 ± 0.72
REFERENCES,0.7952755905511811,32.5 ± 0.61
REFERENCES,0.7959914101646385,WideResNet28-10 – CIFAR-100 Dataset
REFERENCES,0.7967072297780959,"ϵ = 2/255
ϵ = 5/255
ϵ = 6/255
ϵ = 8/255
ϵ = 10/255
ϵ = 12/255
ϵ = 14/255
ϵ = 16/255"
REFERENCES,0.7974230493915533,"N-FGSM
71.56 ± 0.13"
REFERENCES,0.7981388690050107,52.23 ± 0.33
REFERENCES,0.7988546886184682,66.49 ± 0.46
REFERENCES,0.7995705082319255,39.93 ± 0.37
REFERENCES,0.800286327845383,61.38 ± 0.68
REFERENCES,0.8010021474588404,30.97 ± 0.21
REFERENCES,0.8017179670722978,56.23 ± 0.59
REFERENCES,0.8024337866857552,26.77 ± 0.65
REFERENCES,0.8031496062992126,51.54 ± 0.63
REFERENCES,0.80386542591267,23.03 ± 0.54
REFERENCES,0.8045812455261274,46.43 ± 0.61
REFERENCES,0.8052970651395849,19.3 ± 0.59
REFERENCES,0.8060128847530422,42.11 ± 0.32
REFERENCES,0.8067287043664997,16.67 ± 0.4
REFERENCES,0.807444523979957,38.34 ± 0.47
REFERENCES,0.8081603435934145,14.27 ± 0.33
REFERENCES,0.8088761632068718,"Grad Align
71.68 ± 0.33"
REFERENCES,0.8095919828203293,51.5 ± 0.45
REFERENCES,0.8103078024337866,67.09 ± 0.19
REFERENCES,0.8110236220472441,39.9 ± 0.42
REFERENCES,0.8117394416607016,62.86 ± 0.1
REFERENCES,0.8124552612741589,32.0 ± 0.22
REFERENCES,0.8131710808876164,58.55 ± 0.41
REFERENCES,0.8138869005010737,26.9 ± 0.62
REFERENCES,0.8146027201145312,53.85 ± 0.73
REFERENCES,0.8153185397279885,22.63 ± 0.62
REFERENCES,0.816034359341446,46.94 ± 0.86
REFERENCES,0.8167501789549033,19.9 ± 0.65
REFERENCES,0.8174659985683608,42.63 ± 0.5
REFERENCES,0.8181818181818182,16.93 ± 0.12
REFERENCES,0.8188976377952756,36.17 ± 0.45
REFERENCES,0.819613457408733,14.03 ± 0.24
REFERENCES,0.8203292770221904,"FGSM
71.92 ± 0.33"
REFERENCES,0.8210450966356478,52.83 ± 0.37
REFERENCES,0.8217609162491052,67.34 ± 0.36
REFERENCES,0.8224767358625626,39.83 ± 0.31
REFERENCES,0.82319255547602,64.72 ± 1.12
REFERENCES,0.8239083750894775,0.0 ± 0.0
REFERENCES,0.8246241947029349,56.87 ± 1.24
REFERENCES,0.8253400143163923,0.03 ± 0.05
REFERENCES,0.8260558339298497,52.31 ± 2.11
REFERENCES,0.8267716535433071,0.0 ± 0.0
REFERENCES,0.8274874731567645,48.99 ± 1.17
REFERENCES,0.8282032927702219,0.0 ± 0.0
REFERENCES,0.8289191123836793,44.27 ± 1.4
REFERENCES,0.8296349319971367,0.0 ± 0.0
REFERENCES,0.8303507516105941,42.05 ± 1.03
REFERENCES,0.8310665712240516,0.0 ± 0.0
REFERENCES,0.8317823908375089,"RS-FGSM
72.65 ± 0.28"
REFERENCES,0.8324982104509664,51.63 ± 0.52
REFERENCES,0.8332140300644237,68.26 ± 0.2
REFERENCES,0.8339298496778812,39.57 ± 0.09
REFERENCES,0.8346456692913385,65.58 ± 0.69
REFERENCES,0.835361488904796,26.63 ± 2.8
REFERENCES,0.8360773085182533,54.25 ± 5.85
REFERENCES,0.8367931281317108,0.0 ± 0.0
REFERENCES,0.8375089477451683,46.08 ± 4.87
REFERENCES,0.8382247673586256,0.0 ± 0.0
REFERENCES,0.8389405869720831,35.84 ± 0.17
REFERENCES,0.8396564065855404,0.0 ± 0.0
REFERENCES,0.8403722261989979,24.4 ± 1.25
REFERENCES,0.8410880458124552,0.0 ± 0.0
REFERENCES,0.8418038654259127,21.37 ± 5.04
REFERENCES,0.84251968503937,0.0 ± 0.0
REFERENCES,0.8432355046528275,"RandAlpha
73.9 ± 0.15"
REFERENCES,0.8439513242662849,49.13 ± 0.91
REFERENCES,0.8446671438797423,71.17 ± 0.12
REFERENCES,0.8453829634931997,34.3 ± 0.54
REFERENCES,0.8460987831066571,68.65 ± 0.22
REFERENCES,0.8468146027201146,25.5 ± 0.33
REFERENCES,0.8475304223335719,66.42 ± 0.13
REFERENCES,0.8482462419470294,20.27 ± 0.98
REFERENCES,0.8489620615604867,64.05 ± 0.5
REFERENCES,0.8496778811739442,16.3 ± 0.14
REFERENCES,0.8503937007874016,61.99 ± 0.6
REFERENCES,0.851109520400859,12.4 ± 0.29
REFERENCES,0.8518253400143164,59.74 ± 0.57
REFERENCES,0.8525411596277738,6.93 ± 0.19
REFERENCES,0.8532569792412312,58.9 ± 0.78
REFERENCES,0.8539727988546886,3.63 ± 0.12
REFERENCES,0.854688618468146,"AT Free
67.62 ± 0.24"
REFERENCES,0.8554044380816035,48.07 ± 0.31
REFERENCES,0.8561202576950608,63.27 ± 0.72
REFERENCES,0.8568360773085183,37.93 ± 0.69
REFERENCES,0.8575518969219756,59.53 ± 0.31
REFERENCES,0.8582677165354331,29.7 ± 0.51
REFERENCES,0.8589835361488904,55.77 ± 0.28
REFERENCES,0.8596993557623479,24.43 ± 0.37
REFERENCES,0.8604151753758053,47.02 ± 3.83
REFERENCES,0.8611309949892627,3.23 ± 4.43
REFERENCES,0.8618468146027202,33.52 ± 9.24
REFERENCES,0.8625626342161775,0.0 ± 0.0
REFERENCES,0.863278453829635,7.87 ± 1.78
REFERENCES,0.8639942734430923,0.0 ± 0.0
REFERENCES,0.8647100930565498,20.92 ± 21.48
REFERENCES,0.8654259126700071,0.0 ± 0.0
REFERENCES,0.8661417322834646,"ZeroGrad
71.68 ± 0.07"
REFERENCES,0.8668575518969219,52.63 ± 0.61
REFERENCES,0.8675733715103794,67.2 ± 0.14
REFERENCES,0.8682891911238368,39.57 ± 0.33
REFERENCES,0.8690050107372942,63.69 ± 0.14
REFERENCES,0.8697208303507517,30.27 ± 0.54
REFERENCES,0.870436649964209,60.77 ± 0.26
REFERENCES,0.8711524695776665,23.7 ± 0.08
REFERENCES,0.8718682891911238,61.05 ± 0.38
REFERENCES,0.8725841088045813,15.1 ± 0.49
REFERENCES,0.8732999284180386,58.39 ± 0.16
REFERENCES,0.8740157480314961,11.13 ± 0.68
REFERENCES,0.8747315676449535,56.19 ± 0.11
REFERENCES,0.8754473872584109,8.8 ± 0.36
REFERENCES,0.8761632068718683,56.38 ± 0.18
REFERENCES,0.8768790264853257,4.9 ± 0.36
REFERENCES,0.8775948460987831,"MultiGrad
71.8 ± 0.15"
REFERENCES,0.8783106657122405,51.9 ± 0.29
REFERENCES,0.8790264853256979,67.73 ± 0.48
REFERENCES,0.8797423049391553,39.7 ± 0.37
REFERENCES,0.8804581245526127,63.24 ± 0.33
REFERENCES,0.8811739441660702,31.5 ± 0.62
REFERENCES,0.8818897637795275,60.05 ± 0.79
REFERENCES,0.882605583392985,26.03 ± 0.09
REFERENCES,0.8833214030064424,56.39 ± 0.49
REFERENCES,0.8840372226198998,20.8 ± 0.29
REFERENCES,0.8847530422333572,56.79 ± 8.27
REFERENCES,0.8854688618468146,0.0 ± 0.0
REFERENCES,0.886184681460272,59.8 ± 3.77
REFERENCES,0.8869005010737294,0.0 ± 0.0
REFERENCES,0.8876163206871869,52.96 ± 5.58
REFERENCES,0.8883321403006442,0.0 ± 0.0
REFERENCES,0.8890479599141017,"PGD-2
71.62 ± 0.15"
REFERENCES,0.889763779527559,51.73 ± 0.48
REFERENCES,0.8904795991410165,67.25 ± 0.43
REFERENCES,0.8911954187544738,40.27 ± 0.7
REFERENCES,0.8919112383679313,63.18 ± 0.36
REFERENCES,0.8926270579813886,32.23 ± 0.19
REFERENCES,0.8933428775948461,59.02 ± 0.4
REFERENCES,0.8940586972083036,27.13 ± 0.37
REFERENCES,0.8947745168217609,54.47 ± 0.45
REFERENCES,0.8954903364352184,23.43 ± 0.31
REFERENCES,0.8962061560486757,50.91 ± 0.35
REFERENCES,0.8969219756621332,20.23 ± 0.39
REFERENCES,0.8976377952755905,41.03 ± 3.18
REFERENCES,0.898353614889048,0.03 ± 0.05
REFERENCES,0.8990694345025053,40.13 ± 3.66
REFERENCES,0.8997852541159628,0.0 ± 0.0
REFERENCES,0.9005010737294202,"PGD-10
71.11 ± 0.62"
REFERENCES,0.9012168933428776,52.5 ± 0.59
REFERENCES,0.901932712956335,66.9 ± 0.57
REFERENCES,0.9026485325697924,40.73 ± 0.56
REFERENCES,0.9033643521832498,62.05 ± 0.47
REFERENCES,0.9040801717967072,32.8 ± 0.29
REFERENCES,0.9047959914101646,57.64 ± 0.81
REFERENCES,0.905511811023622,27.97 ± 0.59
REFERENCES,0.9062276306370795,52.84 ± 0.88
REFERENCES,0.9069434502505369,24.7 ± 0.36
REFERENCES,0.9076592698639943,48.14 ± 0.73
REFERENCES,0.9083750894774517,21.8 ± 0.57
REFERENCES,0.9090909090909091,43.14 ± 0.87
REFERENCES,0.9098067287043665,18.87 ± 0.6
REFERENCES,0.9105225483178239,39.2 ± 0.62
REFERENCES,0.9112383679312813,16.8 ± 0.57
REFERENCES,0.9119541875447388,Under review as a conference paper at ICLR 2022
REFERENCES,0.9126700071581961,WideResNet28-10 – SVHN Dataset
REFERENCES,0.9133858267716536,"ϵ = 2/255
ϵ = 4/255
ϵ = 6/255
ϵ = 8/255
ϵ = 10/255
ϵ = 12/255"
REFERENCES,0.9141016463851109,"N-FGSM
95.64 ± 0.09"
REFERENCES,0.9148174659985684,84.1 ± 0.73
REFERENCES,0.9155332856120257,93.66 ± 0.41
REFERENCES,0.9162491052254832,66.9 ± 0.86
REFERENCES,0.9169649248389405,91.77 ± 0.42
REFERENCES,0.917680744452398,53.0 ± 0.36
REFERENCES,0.9183965640658555,88.89 ± 0.58
REFERENCES,0.9191123836793128,40.5 ± 0.37
REFERENCES,0.9198282032927703,88.07 ± 0.59
REFERENCES,0.9205440229062276,30.47 ± 0.76
REFERENCES,0.9212598425196851,87.52 ± 0.49
REFERENCES,0.9219756621331424,22.43 ± 0.53
REFERENCES,0.9226914817465999,"Grad Align
95.41 ± 0.06"
REFERENCES,0.9234073013600572,84.57 ± 0.56
REFERENCES,0.9241231209735147,93.9 ± 0.48
REFERENCES,0.9248389405869721,67.27 ± 0.54
REFERENCES,0.9255547602004295,68.36 ± 34.49
REFERENCES,0.9262705798138869,39.53 ± 14.89
REFERENCES,0.9269863994273443,42.62 ± 32.73
REFERENCES,0.9277022190408017,24.7 ± 9.34
REFERENCES,0.9284180386542591,19.3 ± 0.21
REFERENCES,0.9291338582677166,17.63 ± 0.62
REFERENCES,0.9298496778811739,19.53 ± 0.08
REFERENCES,0.9305654974946314,18.13 ± 0.52
REFERENCES,0.9312813171080888,"FGSM
95.83 ± 0.1"
REFERENCES,0.9319971367215462,85.03 ± 0.37
REFERENCES,0.9327129563350036,95.0 ± 0.24
REFERENCES,0.933428775948461,31.53 ± 6.57
REFERENCES,0.9341445955619184,94.23 ± 0.79
REFERENCES,0.9348604151753758,1.7 ± 1.36
REFERENCES,0.9355762347888332,91.11 ± 1.36
REFERENCES,0.9362920544022906,0.13 ± 0.19
REFERENCES,0.937007874015748,88.83 ± 1.71
REFERENCES,0.9377236936292055,0.0 ± 0.0
REFERENCES,0.9384395132426628,86.74 ± 0.7
REFERENCES,0.9391553328561203,0.0 ± 0.0
REFERENCES,0.9398711524695776,"RS-FGSM
95.81 ± 0.25"
REFERENCES,0.9405869720830351,83.8 ± 0.43
REFERENCES,0.9413027916964924,94.53 ± 0.4
REFERENCES,0.9420186113099499,66.67 ± 0.65
REFERENCES,0.9427344309234073,95.23 ± 0.26
REFERENCES,0.9434502505368647,0.53 ± 0.26
REFERENCES,0.9441660701503222,94.68 ± 0.62
REFERENCES,0.9448818897637795,0.0 ± 0.0
REFERENCES,0.945597709377237,93.9 ± 0.52
REFERENCES,0.9463135289906943,0.0 ± 0.0
REFERENCES,0.9470293486041518,91.64 ± 2.98
REFERENCES,0.9477451682176091,0.0 ± 0.0
REFERENCES,0.9484609878310666,"RandAlpha
96.02 ± 0.23"
REFERENCES,0.9491768074445239,82.5 ± 0.45
REFERENCES,0.9498926270579814,95.47 ± 0.18
REFERENCES,0.9506084466714388,63.33 ± 0.53
REFERENCES,0.9513242662848962,94.69 ± 0.26
REFERENCES,0.9520400858983536,47.7 ± 0.99
REFERENCES,0.952755905511811,93.72 ± 0.44
REFERENCES,0.9534717251252685,35.73 ± 0.34
REFERENCES,0.9541875447387258,93.08 ± 1.45
REFERENCES,0.9549033643521833,23.17 ± 1.97
REFERENCES,0.9556191839656406,93.96 ± 0.68
REFERENCES,0.9563350035790981,11.1 ± 3.05
REFERENCES,0.9570508231925555,"AT Free
94.85 ± 0.39"
REFERENCES,0.9577666428060129,83.13 ± 0.17
REFERENCES,0.9584824624194703,92.95 ± 0.65
REFERENCES,0.9591982820329277,68.67 ± 0.53
REFERENCES,0.9599141016463851,91.62 ± 1.93
REFERENCES,0.9606299212598425,54.93 ± 2.58
REFERENCES,0.9613457408732999,93.74 ± 0.69
REFERENCES,0.9620615604867573,0.03 ± 0.05
REFERENCES,0.9627773801002147,92.47 ± 0.97
REFERENCES,0.9634931997136722,0.0 ± 0.0
REFERENCES,0.9642090193271295,90.5 ± 1.41
REFERENCES,0.964924838940587,0.0 ± 0.0
REFERENCES,0.9656406585540444,"ZeroGrad
95.78 ± 0.21"
REFERENCES,0.9663564781675018,84.47 ± 0.83
REFERENCES,0.9670722977809592,94.06 ± 0.52
REFERENCES,0.9677881173944166,66.1 ± 0.37
REFERENCES,0.968503937007874,92.13 ± 0.98
REFERENCES,0.9692197566213314,47.3 ± 0.62
REFERENCES,0.9699355762347889,91.04 ± 0.4
REFERENCES,0.9706513958482462,29.33 ± 0.56
REFERENCES,0.9713672154617037,88.85 ± 0.92
REFERENCES,0.972083035075161,20.77 ± 0.63
REFERENCES,0.9727988546886185,89.8 ± 1.36
REFERENCES,0.9735146743020758,9.33 ± 0.76
REFERENCES,0.9742304939155333,"MultiGrad
95.63 ± 0.16"
REFERENCES,0.9749463135289907,84.37 ± 0.59
REFERENCES,0.9756621331424481,94.27 ± 0.38
REFERENCES,0.9763779527559056,67.27 ± 0.31
REFERENCES,0.9770937723693629,93.64 ± 1.21
REFERENCES,0.9778095919828204,50.1 ± 0.9
REFERENCES,0.9785254115962777,94.83 ± 1.55
REFERENCES,0.9792412312097352,1.77 ± 1.72
REFERENCES,0.9799570508231925,95.26 ± 0.34
REFERENCES,0.98067287043665,0.0 ± 0.0
REFERENCES,0.9813886900501074,95.22 ± 0.15
REFERENCES,0.9821045096635648,0.0 ± 0.0
REFERENCES,0.9828203292770222,"PGD-2
95.88 ± 0.35"
REFERENCES,0.9835361488904796,86.25 ± 0.7
REFERENCES,0.984251968503937,94.66 ± 0.1
REFERENCES,0.9849677881173944,73.29 ± 0.25
REFERENCES,0.9856836077308518,93.77 ± 0.61
REFERENCES,0.9863994273443092,60.53 ± 0.72
REFERENCES,0.9871152469577666,92.99 ± 1.11
REFERENCES,0.9878310665712241,40.77 ± 4.39
REFERENCES,0.9885468861846815,88.81 ± 0.93
REFERENCES,0.9892627057981389,34.33 ± 2.76
REFERENCES,0.9899785254115963,83.17 ± 4.78
REFERENCES,0.9906943450250537,26.8 ± 3.31
REFERENCES,0.9914101646385111,"PGD-10
95.92 ± 0.08"
REFERENCES,0.9921259842519685,86.94 ± 0.13
REFERENCES,0.9928418038654259,94.36 ± 0.13
REFERENCES,0.9935576234788833,74.46 ± 0.54
REFERENCES,0.9942734430923408,92.46 ± 0.25
REFERENCES,0.9949892627057981,63.87 ± 0.49
REFERENCES,0.9957050823192556,89.67 ± 0.34
REFERENCES,0.9964209019327129,53.95 ± 0.55
REFERENCES,0.9971367215461704,85.98 ± 0.59
REFERENCES,0.9978525411596277,44.59 ± 0.14
REFERENCES,0.9985683607730852,80.08 ± 0.93
REFERENCES,0.9992841803865425,37.64 ± 0.49
