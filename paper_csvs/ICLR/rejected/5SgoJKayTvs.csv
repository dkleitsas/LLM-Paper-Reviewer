Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005780346820809248,"In this paper we propose a new method to stabilize the training process of the latent
variables of adversarial auto-encoders, which we name Intervention Adversarial
auto-encoder (IVAAE). The main idea is to introduce a sequence of distributions
that bridge the distribution of the learned latent variable and its prior distribution.
We theoretically and heuristically demonstrate that such bridge-like distributions,
realized by a multi-output discriminator, have an effect on guiding the initial la-
tent distribution towards the target one and hence stabilizing the training process.
Several different types of the bridge distributions are proposed. We also apply
a novel use of Stein variational gradient descent (SVGD) (Liu & Wang, 2016),
by which point assemble develops in a smooth and gradual fashion. We conduct
experiments on multiple real-world datasets. It shows that IVAAE enjoys a more
stable training process and achieves a better generating performance compared to
the vanilla Adversarial auto-encoder (AAE) (Makhzani & Shlens, 2015)."
INTRODUCTION,0.011560693641618497,"1
INTRODUCTION"
INTRODUCTION,0.017341040462427744,"Generative models are widely used for image and texture production. Among them there are two
base models which are most appealing to scholars for their elegant theoretical foundation and close
combination with neural network, namely Variational Auto-encoders (VAE)(Kingma & Welling,
2013) and Generative adversarial network (GAN)(Goodfellow et al., 2014). VAE maximizes a
lower bound of the log-likelihood called ELBO which decomposes to a reconstruction loss term
and a regularization term within an auto-encoder structure. GAN, however, goes beyond the like-
lihood concern, trying to generate data directly using a learned map from the latent space that is
trained in an adversarial manner. Both models have some weakness. For example, GAN’s training
is quite unstable and sometimes may have mode collapse problem. VAE usually produces less sharp
pictures. The regularity of the latent space is also an important concern for VAE that affects the
quality of the generated images."
INTRODUCTION,0.023121387283236993,"There’re many attempts to alleviate the above drawbacks. Adversarial Auto-encoders (AAE) com-
bines the techniques of both VAE and GAN network. It imposes a discriminator on the latent
space of an auto-encoder to classify the latent distribution p(z) and q(z), where the adversar-
ial term serves as the regularizer.
Such choice improves the regularization effect of the latent
variable and alleviates the mode collapse problem due to an auto-encoder structure. Wasserstein
Auto-Encoders(WAE)(Tolstikhin et al., 2017) generalize different regularization approaches on la-
tent space, proposing adversarial method (WAE-GAN) and kernel based method (WAE-MMD). It
provides AAE with theoretical support as well. Intervention Generative Adversarial Network (IV-
GAN)(Liang et al., 2020), another attempt to combine GAN with the encoder structure, stabilizes
GAN’s training process by intervening on the latent distribution so that the reconstructed data dis-
tribution could approach the target more robustly."
INTRODUCTION,0.028901734104046242,"In this work, we will propose this novel regularization method on AAE models. As basically an auto-
encoder structure, the key question is how to let the original distribution in latent space approach
the prior distribution. We intervened on the encoded data, which inherently brings intervened dis-
tribution. We will show that a loss term in our model can be transformed to a certain measure of
the overlap among multiple distributions. When minimizing the loss in terms of these constructed
variables, we expect not only the original distribution to approach the target one, but the distribution
of the intervened data (we call bridge distribution) to serve as a guidance to have such approaching
process more fast and robust as well."
INTRODUCTION,0.03468208092485549,Under review as a conference paper at ICLR 2022
METHODOLOGY,0.04046242774566474,"2
METHODOLOGY"
METHODOLOGY,0.046242774566473986,"Training a GAN model has always been a challenge (e.g., the process is known to be unstable and
prone to mode collapse). AAE’s auto-encoder structure alleviates the problem with a reconstruc-
tion term. It ﬁrst updates the auto-encoder upon the reconstruction loss, then use the discriminator
to distinguish the encoded latent variables and generated ones following Gaussian distribution, up-
dating the discriminator and encoder in an adversarial manner. However, such architecture doesn’t
completely solve GAN’s problem. Decreasing the JS divergence has a relatively low approaching
efﬁciency between two distributions. Adversarial training on latent space doesn’t avoid this problem
at all. Therefore there’s a desire to apply a more powerful regularization term to have the approach-
ing process more efﬁciently and stable. We consider to construct a series of “bridge distributions”
by certain transformations so that they in a sense lie between the encoded distribution and target one.
We let the discriminator to classify these multiple distributions, and update the encoder to let them
approach each other. Intuitively such bridge distributions would serve as a guidance to help the la-
tent variables follow the target distribution more quickly and robustly, but we need more theoretical
arguments ﬁrst."
METHODOLOGY,0.05202312138728324,"Before we expound on further details, we want to turn our attention to how to construct the
transformations mentioned above. We ﬁrst explain the meaning of intervention. It is similar to what
is exhibited in IVGAN(Liang et al., 2020). For completeness we show it here in our context."
METHODOLOGY,0.057803468208092484,"Deﬁnition 1 Let X be the set of all random variables in Rd on the probability space (Ω, X , P).
T is the set of all mappings from Rd to Rd. For a distribution function with support in Rd, we
call T ∈T “P-intervention” if X ∼P ⇒T(X) ∼P, for all X ∈X . If S ∈T satisﬁes that
T(X) ∼P, ∀T ∈S ⇒X ∼P, we denote S “Complete Intervention Group”."
METHODOLOGY,0.06358381502890173,"A rough understanding of the above deﬁnition is that complete intervention group let the meet of
subsequent multiple distributions embed their ﬁnal distributing pattern, i.e. P(x). Combining the
idea in our model, the intervention can be performed on our encoded data. We hope when these
originally different distributions approach each other, they all approach the prior distribution, which
is exactly the feature of the transformation proposed above. Therefore the complete intervention
group can serve to ensure the identiﬁability of our method."
METHODOLOGY,0.06936416184971098,"To be more concrete, we set the P to be standard Gaussian and propose several intervention patterns
that possess the above feature."
METHODOLOGY,0.07514450867052024,"A simplest example of P-intervention is to replace some of the dimension with normal distributed
one. Therefore we have our Blockwise substitution: Rd space is divided equally by t parts, where
t|d. , For 0 ≤k ≤t, let Tk be the transformation that substitute the ﬁrst k blocks with random
variables following standard normal distribution independent with the original variable. Then we
get a complete intervention group {Tk}."
METHODOLOGY,0.08092485549132948,"There’s other ways to create intervention groups. For example, we let x be replaced by xcos( kπ"
METHODOLOGY,0.08670520231213873,"2t ) +
zsin( kπ"
METHODOLOGY,0.09248554913294797,"2t ), for k = 0, 1, ..., t, where z follows standard normal distribution independent with x. We
call such construction pattern Radial substitution."
METHODOLOGY,0.09826589595375723,"Except for the two substitution method, we also apply Stein variational gradient descent (SVGD)
(Liu & Wang, 2016) to construct the bridge distribution by iteratively updating points. By setting
the target distribution P the updated particles will approximately approach P. More details will be
speciﬁed in the next section."
METHODOLOGY,0.10404624277456648,"Now we propose our model called Intervention Adversarial Auto-Encoder (IVAAE). [Figure 1]
shows its speciﬁc structure, which contains an encoder E, decoder G and a discriminator D.
Images x is fed into the E to get the encoded latent code z, which is decoded by G to obtain the
reconstructed images x. The encoded z is transformed by a complete intervention group Tk (T0
represent the identical mapping) to get zk which is discriminated by D with output of size t + 1.
Through maximizing and minimizing alternatively a cross entropy loss, D and E is both updated."
METHODOLOGY,0.10982658959537572,Intervention Loss
METHODOLOGY,0.11560693641618497,Under review as a conference paper at ICLR 2022
METHODOLOGY,0.12138728323699421,"x~pdata
E
z
D
Intervention G
x'"
METHODOLOGY,0.12716763005780346,"Reconstruction
Loss T0(z) T1(z) Tt(z) ..."
METHODOLOGY,0.1329479768786127,Intervention loss
METHODOLOGY,0.13872832369942195,Figure 1: The structure of our model
METHODOLOGY,0.14450867052023122,"The key distinction of our model from AAE is the intervention loss. More speciﬁcally, when encoded
latent z = E(x) is obtained, for every k, 0 ≤k ≤t, we intervene z by Tk to get zk. Then we feed
zk into the discriminator to obtain a t + 1 size output d, and compute the cross entropy between d
and labels ek, averaging on k, ﬁnally to update discriminator by gradient ﬂow. The encoder update
in the same manner."
METHODOLOGY,0.15028901734104047,Therefore the intervention loss is an unbiased estimator of its theoretical form as follow:
METHODOLOGY,0.15606936416184972,"LIV (E, D) = −Ek∼U[t]Ex∼pdata[log(Dk(Tk(E(x))))]"
METHODOLOGY,0.16184971098265896,"= −Ek∼U[t]Ez∼pk[log(Dk(z))]
(1)"
METHODOLOGY,0.1676300578034682,"where Dk, pk represent the (k + 1)th digit of D output, the distribution of zk = Tk(E(x)), respec-
tively."
METHODOLOGY,0.17341040462427745,Then we give theoretical results that evidence the rationality of our proposed models.
METHODOLOGY,0.1791907514450867,"Theorem 2 Given t + 1 latent variables {zk}t
k=0 with density function support in Rd, and their
probabilistic density function {pk}t
k=0, the classiﬁer D is trained to minimize the cross entropy loss
as equation 1 under the constraint that Pt
i=0 Dj(z) = 1. Then the optimal classiﬁer satisﬁes:"
METHODOLOGY,0.18497109826589594,"D∗
i (z) =
pi(z)
Pt
j=0 pj(z)"
METHODOLOGY,0.1907514450867052,And the corresponding optimal intervention loss can be expressed as follows:
METHODOLOGY,0.19653179190751446,"LIV (E, D) = −JS(p0, p2, ..., pt) + log(t + 1)"
METHODOLOGY,0.2023121387283237,"where JS(q1, q2, ..., qk) = 1"
METHODOLOGY,0.20809248554913296,"k
Pk
j=1(KL(qj||q)), q = 1"
METHODOLOGY,0.2138728323699422,"k
Pk
j=1 qj."
METHODOLOGY,0.21965317919075145,Proof: See Appendix.
METHODOLOGY,0.2254335260115607,"Theorem 2 tells us that when the discriminator reaches optimal, the intervention loss can be re-
garded as a certain distance among multiple distributions. We know when t = 1, the loss becomes
the original JS divergence which conforms with GAN. However, it is known that JS divergence
isn’t a perfect measure because the gradient would be zero if two variables have distributions with
disjoint support. According to the theorem, situation for multiple distributions will suffer less from
such problem, for bridge distributions provide greater chances for intersection which consequently
produces gradient. Therefore it would be more stable to update the encoder."
METHODOLOGY,0.23121387283236994,"We now further illustrate the advantage of the multi-object adversarial training. Although GAN’s
convergence has been theoretically proved(Goodfellow et al., 2014), the training process, is well-
known delicate and unstable. One important reason is that the optimal discriminator can always
achieve too perfect no matter how close it is between the generated data manifold and the real data
manifold (Arjovsky & Bottou, 2017). One cause for this is that two manifolds is disjoint with
probability 1 due to their low dimension(Arjovsky & Bottou, 2017) on the data space."
METHODOLOGY,0.23699421965317918,Under review as a conference paper at ICLR 2022
METHODOLOGY,0.24277456647398843,Algorithm 1 Intervention Adversarial Auto-Encoder (IVAAE)
METHODOLOGY,0.24855491329479767,"Input: learning rate: α, dimension of the space: d, number of bridge distributions: t Interventions:
Tk, 0 ≤k ≤t, hyperparameters: λ, µ1, µ2, minibatch size: m.
Output: θD, θE, θG"
METHODOLOGY,0.2543352601156069,"1: for number of training iterations do
2:
Sample {xj}m
j=1 from the training set
3:
Sample {zj}m
j=1 from the prior P(z)
4:
Compute ˆzj = E(xj), j = 1, ..., m
5:
Compute ˆxj = G(ˆzj), j = 1, ..., m
6:
for k = 0, ..., t do
7:
Compute the intervened latent variables
8:
ˆzjk = Tk(ˆzj), j = 1, ..., m
9:
end for
10:
Update the parameters of D by:
11:
θD ←θD +
α
mtµ1∇θD
Pm
j=1
Pt
k=0 logDk(ˆzjk)"
METHODOLOGY,0.26011560693641617,"12:
Calculate ˆLrecon, ˆLIV
13:
Update the parameters of G by:
14:
θG ←θG + α"
METHODOLOGY,0.2658959537572254,"m∇θGλ ˆLrecon
15:
Update the parameters of E by:
16:
θE ←θE + α"
METHODOLOGY,0.27167630057803466,"m∇θG{λ ˆLrecon + µ2 ˆLIV }
17: end for
18: return θD, θE, θG"
METHODOLOGY,0.2774566473988439,"Different from GAN, AAE applies discriminator on the latent distribution, therefore what it learns
isn’t a low-rank manifold, but a continuous distribution with full dimension. But the problem doesn’t
get solved in AAE setting. We address it in a heuristic way. We run AAE on real-world dataset for
50 epochs, then ﬁx the encoder and decoder and update the discriminator alone. We observe how
perfect the discriminator will develop. Figure 2 records the changing curve of adversarial loss every
iteration. According to Figrue 2, the adversarial loss still decreases remarkably toward 0, especially
when latent dimension goes high. That’s because of the limit batch size. IVAAE could alleviate
this problem by boosting the difﬁculty for discriminator to classify more than two distributions and
equivalently introducing more particles. We apply our models IVAAE. We choose t = 4, block-
wise substitution, with the rest of structure all the same as the AAE model. The loss is recorded
with ﬁxed encoder and decoder after 50 epochs, too. We note that although the adversarial loss is
decreasing as well, the drop is much more moderate than AAE. We also examine the consequence of
such loss decline. We choose the adversarial loss for encoder: −Epdata[log(D(E(x)))], and record
the gradient norm of the encoder after the encoder and decoder ﬁxed. According to Figure 3, AAE
is less stable compared to IVAAE."
METHODOLOGY,0.2832369942196532,"0
1000
2000
3000
4000
5000
6000
training iterations 10−5 10−4 10−3 10−2 10−1 100"
METHODOLOGY,0.28901734104046245,adversarial loss AAE
METHODOLOGY,0.2947976878612717,"dim = 16
dim = 32
dim = 64"
METHODOLOGY,0.30057803468208094,"0
1000
2000
3000
4000
5000
6000
training iterations 10−1 100"
METHODOLOGY,0.3063583815028902,adversarial loss IVAAE
METHODOLOGY,0.31213872832369943,"dim = 16
dim = 32
dim = 64"
METHODOLOGY,0.3179190751445087,"Figure 2: We train the model for 50 epochs. Left: AAE; Right: IVAAE (t = 4, block-wise). Then
we ﬁx the encoder and decoder to run the discriminator alone. We record the loss change every
iteration. The dimension of latent space is chosen at 16, 32, 64."
METHODOLOGY,0.3236994219653179,Under review as a conference paper at ICLR 2022
METHODOLOGY,0.32947976878612717,"0
1000
2000
3000
4000
5000
6000
training iterations 10−2 10−1 100 101 102 103"
METHODOLOGY,0.3352601156069364,"∇θL‖Eθ, D)"
METHODOLOGY,0.34104046242774566,Gradient of the Encoder
METHODOLOGY,0.3468208092485549,"AAE dim = 16
IVAAE dim = 16"
METHODOLOGY,0.35260115606936415,"Figure 3: We choose the latent dimension at 16 and run models for 50 epochs. Then we record
the change of encoder’s gradient with the encoder and decoder ﬁxed. Note that Ladv(E, D) =
−E[log(D(E(x)))] is used as the encoder loss."
RELATED WORK,0.3583815028901734,"3
RELATED WORK"
GENERATIVE MODELS,0.36416184971098264,"3.1
GENERATIVE MODELS"
GENERATIVE MODELS,0.3699421965317919,"Since the appearance of VAE and GAN model, large amount of attempts have been conducted
to improve their performance on generating data. Many of them put their attention on the latent
space. 2-stage VAE(Dai & Wipf, 2019) diagnoses the poor generate quality of VAE model and
attributes it to the mismatch of the distribution between the encoded q(z) and the prior p(z). They
handles the problem by imposing another VAE structure to learn the latent distribution, which
improves the model’s sample quality. RAE(Ghosh et al., 2019) view VAE from a deterministic
angle. It abandons the stochastic component and replace it with new proposed regularization term,
which provide an alternative to encode smooth and meaningful latent variables. Info VAE(Zhao
et al., 2019) enhance VAE model by introducing to ELBO a mutual information term between x
and z, which is a generalization of both VAE and AAE. It improves the quality of the variational
posterior and strengthen the bond between data space and latent features. Our method, different
from the above, dedicates to improve regularity of latent space by enhancing and stabilizing the
training process. It applies solid schemes and techniques while not deviating from the original target."
SVGD,0.37572254335260113,"3.2
SVGD"
SVGD,0.3815028901734104,"Stein variational gradient descent (SVGD) is proposed as a variational inference algorithm (Liu &
Wang, 2016). The method iteratively transports a set of particles to match the target distribution by
applying a form of functional gradient descent that minimized KL divergence among a reproducing
kernel Hilbert space (RKHS). By using the assignment as follow:"
SVGD,0.3872832369942196,"zl+1
i
←zl
i + ϵ m m
X"
SVGD,0.3930635838150289,"j=1
[k(zl
j, z)∇zl
jlog p(zl
j) + ∇zl
jk(zl
j, z)],
(2)"
SVGD,0.3988439306358382,"where the k represents the kernel of RKHS, m batch size, the particles zi gradually approximate
the target distribution p(z). We apply the method in the construction of bridge distribution of the
IVAAE model. We choose step size ϵ = 1, kernel function RBF k(z, z′) = exp(−1"
SVGD,0.4046242774566474,"h∥z −z′∥2)
with bandwidth h , and set the target distribution to the prior distribution p(z), which is standard"
SVGD,0.41040462427745666,Under review as a conference paper at ICLR 2022
SVGD,0.4161849710982659,"normal in our context. To calculate the intervention loss, we obtain the batched encoded data zi =
E(xi), i = 1...m. We plug it in the iterative function above to update it t times to get {zl}t
l=1.
Then we use {zl} as the input of discriminator to obtain the loss term and do the back propagation
all the way back. This is computationally practical because of the trivial form of ∇log p(z)."
EXPERIMENTS,0.42196531791907516,"4
EXPERIMENTS"
EXPERIMENTS,0.4277456647398844,"In this section we conduct experiments on multiple datasets including MNIST (LeCun, 1998), CI-
FAR10 (Krizhevsky & Hinton, 2010), CelebA (Liu et al., 2018). We use different patterns of bridge
distribution in IVAAE and compare the performance to those of the related baseline method with
similar architecture. By exhibiting multiple measures during the train and showing the generated
pictures, we empirically demonstrate that the performance of our methods is superior on training
stability and generating images quality. The measures we use include FID (Heusel et al., 2017), a
popular index to measure the generating variety and quality, and Frechet Latent Distance (FLD),
a similar form of distance we propose to judge the latent distribution. The bridge distributions we
employ include blockwise and radial patterns, corresponding method denoted by IVAAEbw and
IVAAErad, respectively. We choose t = 4 in all of our experiments. The competing approaches we
apply include WAE-GAN, for it generalizes the AAE algorithm, and WAE-MMD as well."
EXPERIMENTS,0.43352601156069365,"We use Pytorch to implement our models. In IVAAE and other baseline models we adopt similar
architecture in order to give persuasive conclusion. We use deterministic models only. For all
datasets and models, we set hyperparameters as follows: learning rate α = 0.001 for encoder
and decoder, α = 0.0005 for discriminator, t = 4, λ = 1, µ1 =
4
t+1, µ2 =
2
t+1, learning rate
decays by multiplying 0.5 after 30 epochs, further multiplying 0.2 after 50 epochs, and 0.1 after
100 epochs. We run all the codes for 300 epochs. Note that the intervention term in IVAAE
is down weighted by
c
t+1 because the derivatives of logrithmatic function becomes
2
t+1 greater
when striking balance at
1
t+1, instead of 1"
EXPERIMENTS,0.4393063583815029,2 in binary conditions. There’s no noise added in any layers.
EXPERIMENTS,0.44508670520231214,"We calculate FID by generating 20k latent variables from the prior and feeding them to the trained
decoder. We randomly choose 60k images, get the encoded z, and use the Frechet distance to
measure its difference with the standard Gaussian:"
EXPERIMENTS,0.4508670520231214,"FLD(z) = ∥µz∥2 + Tr(Σz −2
p"
EXPERIMENTS,0.45664739884393063,Σz + I)
EXPERIMENTS,0.4624277456647399,"where µz and Σz is the means and covariance matrix of z, respectively. We choose the model with
the lowest FID score and calculate its FLD score to measure the regularization performance of
latent variables. MNIST"
EXPERIMENTS,0.4682080924855491,"The MNIST dataset contains 60K images of 28 × 28 handwritten digits. Before we use them, we
resize the images to 64 × 64 pixels. Instead of going for the best model performance, we adopt the
same architecture as WAE (Tolstikhin et al., 2017) do to compare the results among those models
with same structure. We choose the latent size 16, batch size 64. The network structure is two
fully connected linear layers for encoder, decoder and discriminator. The only difference in WAE
and IVAAE’s model structure is the output size of discriminator. See Table 3 in appendix. We use
learning rate distinct with the ground settings: 0.0001 for decoder and discriminator, 0.00005 for
the encoder. We run each model for 300 epochs and learning rate is divided by 2 every 100 epochs."
EXPERIMENTS,0.47398843930635837,CelebA
EXPERIMENTS,0.4797687861271676,"CelebA dataset contains over 200k images of humans faces. We apply transform to crop 140 × 140
pixels from the central of images and then resize to 64 × 64 digits before we use them.
The
architecture we adopt is similar to what is recommended in WAE. The dimension of latent space is
64. We use convolutional network for encoder and decoder with 5 × 5 or 6 × 6 size of ﬁlters, and a
network composed of four fully connected linear layers of 512 elements for the discriminator. See
Table 4 in appendix. Batch size is set at 100."
EXPERIMENTS,0.48554913294797686,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.4913294797687861,CIFAR10
EXPERIMENTS,0.49710982658959535,"To implement methods on CIFAR10 we set dimension of latent space 128.
We reshape the
CIFAR10 images to 64 × 64 size. The network structure is completely the same as the network for
CelebA, except for the output size of encoder and input size for decoder and discriminator. The
latent dimension is relatively large considering batch size we choose as 128. We will note that
WAE-GAN fails to converge after 30-40 epochs while IVAAE is stable during the whole training 4."
EXPERIMENTS,0.5028901734104047,"Except for block-wise replacement and weighted Gaussian, we also use the SVGD(Liu & Wang,
2016) technique as a third method to construct bridge distribution. We obtain particle assembles
zl from encoded batched data z by iterative assignment (equation 2). A notable question is, once
the batch size is limited, the particle assemble zl is not approaching standard normal but with a
shrunk variance lower than 1. Therefore we need a high width h which could enlarge the repulsive
force between points. The discrepancy is also ampliﬁed when the data’s dimension goes higher and
batch size goes smaller. Therefore we implement the method only on MNIST dataset which needs
relatively low latent dimension."
EXPERIMENTS,0.5086705202312138,"We let t = 4 and choose RBF kernel. The step size ϵ is set to 1. We obtain {zl}t−1
l=1 for every two
iterations, while zt is sampled from standard Gaussian. SVGD degrades to a gradient descent of
MAP when width h = 0, while high h would cause the convergence to slowing down, so ﬁnally we
choose h = 100h0, h0 = med2"
EXPERIMENTS,0.5144508670520231,"logn as recommended (Liu & Wang, 2016), where med is the median
of the pairwise distance between the current points zl. Besides, we add 0.1 Gaussian noise on the
particles after every iteration."
EXPERIMENTS,0.5202312138728323,"Note that in SVGD process points update in a relative smooth way by adding deviation on former
points. It is similar to the radial substitution. Moreover, when using equation 2 to update a latent
point, all points in current batch are involved, therefore the tendency of one point towards Gaussian
distribution is directly affected by force from other points. Such interaction become stronger under a
great bandwidth h. Intuitively we hope this feature would improve the hidden layer’s regularity. To
measure such regularity to a certain extent, we utilize the trained encoder to do classiﬁcation work.
We ﬁx the parameters of encoder after training WAE-GAN, IVAAErad, IVAAEsvgd models, respec-
tively. Then a two-layer MLP with 1000 nodes each is connected to the encoder’s last hidden layer
to do supervised learning. We set batch size 60, learning rate 0.0001, and evaluate the classiﬁcation
error rate every 100 training iterations. The result is favorable to our argument as shown in Figure
5:"
EXPERIMENTS,0.5260115606936416,"20
40
60
80
100
120
140
training epochs 50 60 70 80 90 100"
EXPERIMENTS,0.5317919075144508,FID score MNIST
EXPERIMENTS,0.5375722543352601,"WAE-GAN
IVAAEbw
IVAAErad
IVAAEsvgd"
EXPERIMENTS,0.5433526011560693,"20
40
60
80
100
120
140
training epochs 50 100 150 200 250 300"
EXPERIMENTS,0.5491329479768786,FID score
EXPERIMENTS,0.5549132947976878,CIFAR10
EXPERIMENTS,0.5606936416184971,"WAE-GAN
IVAAEbw
IVAAErad"
EXPERIMENTS,0.5664739884393064,"Figure 4: FID curves for different methods calculated during the training. Note that WAE-GAN’s
training process is not stable on CIFAR10."
EXPERIMENTS,0.5722543352601156,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.5780346820809249,"0
5000
10000
15000
20000
25000
30000
training iterations 0.02 0.03 0.04 0.05 0.06 0.07"
CLASSIFICATION ERROR RATE,0.5838150289017341,"0.08
Classification Error Rate"
CLASSIFICATION ERROR RATE,0.5895953757225434,"WAE-GAN
IVAAErad
IVAAEsvgd"
CLASSIFICATION ERROR RATE,0.5953757225433526,"Figure 5: MNIST classiﬁcation error rate for methods WAE-GAN, IVAAErad, IVAAEsvgd. Param-
eters of the encoder is ﬁxed after pretrained."
CLASSIFICATION ERROR RATE,0.6011560693641619,"Table 1: FID score for different methods on multiple datasets. FID results are calculated every 10
epochs and we choose the minimum. Lower score means generated images are better. * means we
don’t go for the best performance across different structure, but for comparison purpose only."
CLASSIFICATION ERROR RATE,0.6069364161849711,"Methods
MNIST
CelebA
CIFAR10
WAE-MMD
67∗
36
87
WAE-GAN
53∗
33
109
IVAAEbw
49∗
34
117
IVAAErad
50∗
33
80
IVAAEsvgd
49∗
-
-"
CLASSIFICATION ERROR RATE,0.6127167630057804,"Table 2: FLD score for different methods on multiple datasets. We choose FLD when FID reaches
the best. Lower score means the encoded latent distribution is closer to the Guassian prior."
CLASSIFICATION ERROR RATE,0.6184971098265896,"Methods
MNIST
CelebA
CIFAR10
WAE-MMD
0.05
0.34
10.9
WAE-GAN
5.5
1.5
15.1
IVAAEbw
0.2
0.22
4.0
IVAAErad
0.07
0.18
11.8
IVAAEsvgd
5.7
-
-"
CLASSIFICATION ERROR RATE,0.6242774566473989,"From the result we could notice that WAE-GAN’s training stability and speed is lower than IVAAE
on dataset we study. It conforms to what have been discussed in previous section. When the latent
size reach higher as 128 (with batch size 128), WAE-GAN suffers from intense oscillation. Every
time the model converges to a certain degree, the adversarial loss decreases and reconstruction loss
increases sharply. IVAAE performs well under the situation we inspect, and the FID score and the
FLD [Figure 6] when FID reaches the lowest is overall better [Table 1, 2]. Curiously, in terms of
the performance, among patterns we choose there seems to be no pattern that are superior to the
other on every datasets. On CIFAR10 we notice that IVAAErad attain far lower FID score than
IVAAEbw, while the FLD score goes to the opposite. There seems to be a trade-off relation between
the generate quality and the regularity (how close to the standard normal) of latent variables. It is an
interesting phenomenon to explore in the future work."
CLASSIFICATION ERROR RATE,0.630057803468208,Under review as a conference paper at ICLR 2022
CLASSIFICATION ERROR RATE,0.6358381502890174,"0
10
20
30
40
50
60"
CLASSIFICATION ERROR RATE,0.6416184971098265,"0
10
20
30
40
50
60"
CLASSIFICATION ERROR RATE,0.6473988439306358,WAE-GAN 0.0 0.2 0.4 0.6 0.8 1.0
CLASSIFICATION ERROR RATE,0.653179190751445,"0
10
20
30
40
50
60"
CLASSIFICATION ERROR RATE,0.6589595375722543,"0
10
20
30
40
50
60"
CLASSIFICATION ERROR RATE,0.6647398843930635,IVAAErad 0.0 0.2 0.4 0.6 0.8 1.0
CLASSIFICATION ERROR RATE,0.6705202312138728,"Figure 6: Heat maps for covariance matrix of the encoded z for different models when converging.
We choose CelebA with the latent dimension as 64. The brightness represent absolute value of the
corresponding entry. The brighter the closer to 1. The darker the closer to 0."
CLASSIFICATION ERROR RATE,0.6763005780346821,"(a) WAE-GAN
(b) IVAAEbw
(c) IVAAErad"
CLASSIFICATION ERROR RATE,0.6820809248554913,"Figure 7:
CelebA images randomly generated.
Left:
WAE-GAN; Mid:
IVAAEbw; Right:
IVAAErad."
CONCLUSION,0.6878612716763006,"5
CONCLUSION"
CONCLUSION,0.6936416184971098,"In this paper we propose a generative model which possess robust and stable training property by
leveraging interventions on latent variables and classifying them. It creatively applies the multiple-
output structure to discriminate over two distributions. It is theoretically proved that the intervened
latent distributions can be regarded as certain bridge distributions which work as a guidance to pull
the encoded variables to the target distribution. By conducting a heuristic loss trace for discrimina-
tor, we demonstrate that multiple classifying have a signiﬁcant stabilizing effect on the adversarial
training in practice."
CONCLUSION,0.6994219653179191,"To structure different bridge distribution, we also apply the SVGD method to smoothly update the
latent variables. We successfully combine the method in our adversarial training, and to some extent
demonstrate the hidden space enjoys better regularity as well."
CONCLUSION,0.7052023121387283,"Besides, multiple experiments are conducted on real-world datasets. By using measures on both
images generating quality and latent regularity, we conclude a better performance of our models
compared to the baseline models. The data results coincide with our inference on the stability of
model training."
REFERENCES,0.7109826589595376,REFERENCES
REFERENCES,0.7167630057803468,"Martin Arjovsky and L´eon Bottou. Towards principled methods for training generative adversarial
networks. arXiv preprint arXiv:1701.04862, 2017."
REFERENCES,0.7225433526011561,"Bin Dai and David Wipf. Diagnosing and enhancing vae models. arXiv preprint arXiv:1903.05789,
2019."
REFERENCES,0.7283236994219653,Under review as a conference paper at ICLR 2022
REFERENCES,0.7341040462427746,"Partha Ghosh, Mehdi SM Sajjadi, Antonio Vergari, Michael Black, and Bernhard Sch¨olkopf. From
variational to deterministic autoencoders. arXiv preprint arXiv:1903.12436, 2019."
REFERENCES,0.7398843930635838,"Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv:1406.2661, 2014."
REFERENCES,0.7456647398843931,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. arXiv preprint
arXiv:1706.08500, 2017."
REFERENCES,0.7514450867052023,"Diederik P Kingma and Max Welling.
Auto-encoding variational bayes.
arXiv preprint
arXiv:1312.6114, 2013."
REFERENCES,0.7572254335260116,"Alex Krizhevsky and Geoff Hinton. Convolutional deep belief networks on cifar-10. Unpublished
manuscript, 40(7):1–9, 2010."
REFERENCES,0.7630057803468208,"Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998."
REFERENCES,0.7687861271676301,"Jiadong Liang, Liangyu Zhang, Cheng Zhang, and Zhihua Zhang. Intervention generative adversar-
ial networks. arXiv preprint arXiv:2008.03712, 2020."
REFERENCES,0.7745664739884393,"Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. arXiv preprint arXiv:1608.04471, 2016."
REFERENCES,0.7803468208092486,"Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Large-scale celebfaces attributes (celeba)
dataset. Retrieved August, 15(2018):11, 2018."
REFERENCES,0.7861271676300579,"Alireza
Makhzani
and
Jonathon
Shlens.
Adversarial
autoencoders.
arXiv
preprint
arXiv:1511.05644, 2015."
REFERENCES,0.791907514450867,"Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf.
Wasserstein auto-
encoders. arXiv preprint arXiv:1711.01558, 2017."
REFERENCES,0.7976878612716763,"Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Balancing learning and inference in
variational autoencoders. In Proceedings of the aaai conference on artiﬁcial intelligence, vol-
ume 33, pp. 5885–5892, 2019."
REFERENCES,0.8034682080924855,Under review as a conference paper at ICLR 2022
REFERENCES,0.8092485549132948,"A
APPENDIX"
REFERENCES,0.815028901734104,"A.1
PROOF OF THEOREM 2"
REFERENCES,0.8208092485549133,"LIV (E, D) = −Ek∼U[t]Ez∼pk[log(Dk(z))]"
REFERENCES,0.8265895953757225,"= −
1
t + 1 t
X k=0 Z"
REFERENCES,0.8323699421965318,"z
pk(z)logDk(z)dz =
Z"
REFERENCES,0.838150289017341,"z
−p(z) t
X"
REFERENCES,0.8439306358381503,"k=0
p(ek|z)logDk(z)dz"
REFERENCES,0.8497109826589595,Last equation holds because p(ek|z) = p(ek)pk(z)
REFERENCES,0.8554913294797688,"p(z)
=
pk(z)
(t+1)p(z).
Notice that − t
X"
REFERENCES,0.861271676300578,"k=0
p(ek|z)logDk(z) = − t
X"
REFERENCES,0.8670520231213873,"k=0
p(ek|z)log Dk(z)"
REFERENCES,0.8728323699421965,"p(ek|z) + H(p(·|z)) ≥−log t
X"
REFERENCES,0.8786127167630058,"k=0
p(ek|z) · Dk(z)"
REFERENCES,0.884393063583815,p(ek|z) + H(p(·|z))
REFERENCES,0.8901734104046243,=H(p(·|z))
REFERENCES,0.8959537572254336,"The equality holds when D∗
k(z) ∝p(ek|z) ∝pk(z), i.e. D∗
k(z) =
pk(z)
Pt
j=0 pj(z)."
REFERENCES,0.9017341040462428,"Plugging D∗(z) into the loss term, we obtain:"
REFERENCES,0.9075144508670521,"LIV (E, D) = −
1
t + 1 t
X k=0 Z"
REFERENCES,0.9132947976878613,"z
pk(z)logDk(z)dz"
REFERENCES,0.9190751445086706,"= −
1
t + 1 t
X k=0 Z"
REFERENCES,0.9248554913294798,"z
pk(z)log
pk(z)
Pt
j=0 pj(z)
dz"
REFERENCES,0.930635838150289,"= −
1
t + 1 t
X k=0 Z"
REFERENCES,0.9364161849710982,"z
pk(z)

logpk(z)"
REFERENCES,0.9421965317919075,"p(z) −log(t + 1)

dz"
REFERENCES,0.9479768786127167,"= −
1
t + 1 t
X"
REFERENCES,0.953757225433526,"k=0
KL(pk∥p) + log(t + 1)"
REFERENCES,0.9595375722543352,"So we have our theorem proved.
□"
REFERENCES,0.9653179190751445,"A.2
NETWORK STRUCTURE"
REFERENCES,0.9710982658959537,Table 3: The network architecture we use for MNIST. FC represents fully connected layer.
REFERENCES,0.976878612716763,"E
G
D
INPUT 64 × 64
INPUT z
INPUT z
FC(1024)
FC(16, 1024)
FC(16, 1024)
ReLU
ReLU
ReLU
FC(1024, 1024)
FC(1024, 1024)
FC(1024, 1024)
ReLU
ReLU
ReLU
FC(1024, 16)
FC(64 × 64), Tanh
FC(t + 1)"
REFERENCES,0.9826589595375722,Under review as a conference paper at ICLR 2022
REFERENCES,0.9884393063583815,"Table 4: The network architecture we use for CelebA and CIFAR10. CONV(C, K, S) represents
convolutional layer with C channels, K×K-size kernels, and S strides. DCONV represents decon-
volutional layer, parameter meaning similar with CONV. BN represents batch normalization layer."
REFERENCES,0.9942196531791907,"E
G
D
INPUT 64 × 64 × 3
INPUT z
INPUT z
CONV(C128, K5, S2)
FC(1024 × 8 × 8)
FC(512)
BN, ReLU
DCONV(C512, K5, S1)
ReLU
CONV(C256, K5, S2)
BN, ReLU
FC(512, 512)
BN, ReLU
DCONV(C256, K5, S1)
ReLU
CONV(C512, K5, S2)
BN, ReLU
FC(512, 512)
BN, ReLU
DCONV(C128, K5, S2)
ReLU
CONV(C1024, K5, S2)
BN, ReLU
FC(512, 512)
BN, ReLU
DCONV(C1, K4, S0)
ReLU
FC(C1)
Tanh
FC(t + 1)"
