Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0027247956403269754,"The word mover’s distance (WMD) is a fundamental technique for measuring
the similarity of two documents. As the crux of WMD, it can take advantage of
the underlying geometry of the word space by employing an optimal transport
formulation. The original study on WMD reported that WMD outperforms classical
baselines such as bag-of-words (BOW) and TF-IDF by signiﬁcant margins in
various datasets. In this paper, we point out that the evaluation in the original
study could be misleading. We re-evaluate the performances of WMD and the
classical baselines and ﬁnd that the classical baselines are competitive with WMD
if we employ an appropriate preprocessing, i.e., L1 normalization. In addition, We
introduce an analogy between WMD and L1-normalized BOW and ﬁnd that not
only the performance of WMD but also the distance values resemble those of BOW
in high dimensional spaces."
INTRODUCTION,0.005449591280653951,"1
INTRODUCTION Obama"
INTRODUCTION,0.008174386920980926,speaks media
INTRODUCTION,0.010899182561307902,Illinois
INTRODUCTION,0.013623978201634877,President
INTRODUCTION,0.01634877384196185,greets press
INTRODUCTION,0.01907356948228883,Chicago
INTRODUCTION,0.021798365122615803,"Obama speaks to the media in Illinois
vs The president greets the press in Chicago Obama"
INTRODUCTION,0.02452316076294278,speaks media
INTRODUCTION,0.027247956403269755,Illinois band gave
INTRODUCTION,0.02997275204359673,"concert
Japan"
INTRODUCTION,0.0326975476839237,"Obama speaks to the media in Illinois
vs The band gave a concert in Japan"
INTRODUCTION,0.035422343324250684,"Figure 1: Neither pair of texts
has common words. WMD
can choose a similar sen-
tence appropriately, whereas
the BOW distance cannot dis-
tinguish these cases."
INTRODUCTION,0.03814713896457766,"The optimal transport (OT) distance is an effective tool for comparing
probabilistic distributions. Applications of OT include image pro-
cessing (Ni et al., 2009; Rabin et al., 2011; De Goes et al., 2012),
natural language processing (NLP) (Kusner et al., 2015; Rolet et al.,
2016), biology (Schiebinger et al., 2019; Lozupone & Knight, 2005;
Evans & Matsen, 2012), and generative models (Arjovsky et al., 2017;
Salimans et al., 2018)."
INTRODUCTION,0.04087193460490463,"A prominent application of OT is the word mover’s distance (WMD)
(Kusner et al., 2015) for document comparison. WMD regards a doc-
ument as a probabilistic distribution of words, deﬁnes the underlying
word geometry using pre-trained word embeddings, and computes
the distance using the optimal transport distance between two word
distributions (i.e., documents). WMD is preferable because it takes
the underlying geometry into account. For example, bag-of-words
(BOW) will conclude that two documents are dissimilar if they have
no common words, whereas WMD will determine that they are similar
if the words are semantically similar (even if they are not exactly the
same), as illustrated in Figure 1."
INTRODUCTION,0.043596730245231606,"WMD has been widely used in NLP owing to this preferred property.
For example, Kusner et al. (2015) and others (Huang et al., 2016; Li
et al., 2019) used WMD for document classiﬁcation, Wu et al. (2018)
used WMD for computing document embeddings, Xu et al. (2018)
used WMD for topic modeling, Kilickaya et al. (2017) and others
(Clark et al., 2019; Zhao et al., 2019; 2020; Wang et al., 2020a; Gao
et al., 2020; Lu et al., 2019; Chen et al., 2020b) used WMD for evaluating text generation. Many
extensions have been proposed including supervised (Huang et al., 2016; Takezawa et al., 2021) and
fast (Le et al., 2019; Backurs et al., 2020; Genevay et al., 2016; Dong et al., 2020; Sato et al., 2020b)
variants. WMD is one of the fundamental tools used in NLP, and understanding the deep mechanism
of WMD is crucial for further applications."
INTRODUCTION,0.04632152588555858,"The most fundamental application of WMD is document classiﬁcation. The original study on WMD
(Kusner et al., 2015) conducted extensive experiments using kNN classiﬁers. Figure 2 shows the"
INTRODUCTION,0.04904632152588556,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.051771117166212535,"bbcsport
twitter
recipe
ohsumed
classic
reuters
amazon
20news
0 10 20 30 40 50 60 70"
INTRODUCTION,0.05449591280653951,"kNN classification error 21 22 4.6 44 33
29 59 53 43 61 63 44 36 35 2.8 14 29 3.5 28 42 7.4 58
54 27 BOW BOW BOW BOW BOW BOW BOW"
INTRODUCTION,0.05722070844686648,TF-IDF
INTRODUCTION,0.05994550408719346,TF-IDF
INTRODUCTION,0.06267029972752043,TF-IDF
INTRODUCTION,0.0653950953678474,TF-IDF
INTRODUCTION,0.0681198910081744,TF-IDF
INTRODUCTION,0.07084468664850137,TF-IDF
INTRODUCTION,0.07356948228882834,TF-IDF
INTRODUCTION,0.07629427792915532,TF-IDF WMD WMD WMD WMD
INTRODUCTION,0.07901907356948229,↓ better
INTRODUCTION,0.08174386920980926,"Figure 2: kNN classiﬁcation errors reported in the original WMD paper (Figure 3 in (Kusner et al.,
2015)). Lower is better. WMD outperformed the naive baselines by signiﬁcant margins."
INTRODUCTION,0.08446866485013624,"classiﬁcation errors reported in (Kusner et al., 2015). This ﬁgure clearly shows that WMD is superior
to classical baselines, BOW and TF-IDF1"
INTRODUCTION,0.08719346049046321,"Figure 2 is surprising in the following senses. First, WMD outperforms the classical baselines
by excessively large margins. BOW and TF-IDF have long been recognized as effective tools for
document classiﬁcation. Although it is reasonable for WMD to outperform them, the improvements
are surprisingly large. In particular, the performance is ten times better on the classic dataset and
ﬁve times better on the bbcsport dataset. Such results are excessively impressive. Second, although
TF-IDF is known to be more effective than BOW, it performs worse than BOW on the ohsumed,
reuters, and amazon datasets. In fact, the number of misclassiﬁcation doubles on the reuters datasets."
INTRODUCTION,0.08991825613079019,"In this paper, we point out the possibility that the evaluations conducted in the original WMD study
(Kusner et al., 2015) are misleading. Speciﬁcally, we found that the main improvements of WMD
were due to normalization. Using the same normalization, WMD is comparable to BOW and TF-IDF,
or WMD achieves improvements of only two to eight percent at the price of heavy computations. We
also conﬁrm that TF-IDF is more effective than raw BOW if we employ adequate normalization."
INTRODUCTION,0.09264305177111716,"To understand the mechanism of WMD, we introduce an analogy between WMD and L1-normalized
BOW. We experimentally ﬁnd that the distribution of the distances between matched words is not
Gaussian-like but two-modal in high dimensional spaces. We then ﬁnd that not only the performance
of WMD but also the distance values resemble those of BOW in high dimensional spaces."
INTRODUCTION,0.09536784741144415,The contributions of this paper are summarized as follows.
INTRODUCTION,0.09809264305177112,"• We point out that the performance of WMD is not as high as we previously believed. The
performance is comparable to classical baselines in document classiﬁcation with the same
normalization."
INTRODUCTION,0.1008174386920981,"• We introduce an analogy between WMD and L1-normalized BOW (Proposition 1) and ﬁnd
that WMD resembles BOW in high dimensional spaces (Figure 5)."
INTRODUCTION,0.10354223433242507,"• We point out several confusing aspects in the evaluations conducted in the original study on
WMD. We suspect that many readers and researchers are unaware of these issues. Clarifying
them is crucial for a solid evaluation and analysis in this ﬁeld"
INTRODUCTION,0.10626702997275204,"Reproducibility. We include our code in the supplementary materials. It contains a script to
download datasets and pre-computed results, algorithm implementations, and evaluation code."
RELATED WORK,0.10899182561307902,"2
RELATED WORK"
RELATED WORK,0.11171662125340599,"Word Mover’s Distance (WMD) and Optimal Transport (OT) in NLP. WMD (Kusner et al.,
2015) is one of the most thriving applications of OT. WMD can take the underlying word geometry
into account and inherit many elegant theoretical properties from OT. The success of WMD has
facilitated many applications of OT in NLP. EmbDist (Kobayashi et al., 2015) is a method concurrent"
RELATED WORK,0.11444141689373297,"1It should be noted that (Kusner et al., 2015) used many stronger baselines such as LSI and LDA. We focus
on BOW and TF-IDF because (i) BOW was used as the base performance (Figure 4 in (Kusner et al., 2015),
Figure 4 in (Yurochkin et al., 2019)), and (ii) BOW is a special case of WMD (Propositoin 1)."
RELATED WORK,0.11716621253405994,Under review as a conference paper at ICLR 2022
RELATED WORK,0.11989100817438691,"with WMD. It also regards a document as a distribution of word embeddings but uses greedy matching
instead of OT. Kumar et al. (2017) apply WMD to hidden representations of words instead of raw
word embeddings. Yurochkin et al. (2019) consider a document as a distribution of topics and
compute the document similarities using OT. They also use the OT distance for computing the ground
distance of the topics. Alvarez-Melis et al. (2018) proposed a structured OT and applied it to a
document comparison to take the positional consistency into account. Singh et al. (2020) consider a
word as a probabilistic distribution of surrounding words and compute the similarity of the words
using the OT distance of the distributions. Muzellec & Cuturi (2018) and others (Deudon, 2018;
Frogner et al., 2019; Sun et al., 2018) embed words or sentences into distributions instead of vectors
and compute the distance between embeddings using OT. Chen et al. (2019) and others (Li et al.,
2020; Chen et al., 2020a) regularize the text generation models based on the OT distance between the
generated texts and the ground truth texts to improve the generation. Nested Wasserstein (Zhang et al.,
2020a) compares the distributions of sequences and is successfully used in imitation learning for text
generation. Lei et al. (2019) use WMD to generate paraphrase texts for creating adversarial examples.
Zhang et al. (2020b) introduced partial OT to drop meaningless words. Michel et al. (2017) use a
Gromov Wasserstein-like distance instead of the standard OT to compare documents. Zhang et al.
(2016) and others (Zhang et al., 2017b;a; Grave et al., 2019; Dou & Neubig, 2021) use OT to align
word embeddings of different languages. Trapp et al. (2017) use WMD to compare compositional
documents by weighting each document. Kilickaya et al. (2017) and others (Clark et al., 2019; Zhao
et al., 2019; 2020; Wang et al., 2020a; Gao et al., 2020; Lu et al., 2019; Chen et al., 2020b) used
WMD for evaluating text generation. BERTScore (Zhang et al., 2020c) is a relevant method, but it
uses greedy matching instead of OT. To summarize, OT and WMD have been used in many NLP
tasks. It is important to understand the underlying mechanism of WMD for further advancements in
this ﬁeld."
RELATED WORK,0.1226158038147139,"Re-evaluation of Existing Methods. Back in 2009, Armstrong et al. (2009) found that, although
many studies have claimed statistically signiﬁcant improvements against the baselines, most have
employed excessively weak baselines, and the performance did not improve from the classical
baselines in the information retrieval domain. Dacrema et al. (2019) recently found that many of
the deep learning-based recommender systems are extremely difﬁcult to reproduce, and for the
methods whose results authors could reproduce, the performance was not as high as people had
believed, and the deep approaches were actually comparable to classical baselines with an appropriate
hyperparameter tuning. Their paper has had a large impact on the community and was awarded thr
best paper prize at RecSys 2019. Similar observations have also been made in sentence embeddings
(Arora et al., 2017; Shen et al., 2018), session-based recommendations (Ludewig et al., 2019), and
graph neural networks (Errica et al., 2020) as well. In general, science communication suffers
from publication and conﬁrmation biases. The importance of reproducing existing experiments by
third-party groups has been widely recognized in science (Lin, 2018; Sculley et al., 2018; Munafò
et al., 2017; Collins & Tabak, 2014; Goodman et al., 2016)."
BACKGROUNDS,0.12534059945504086,"3
BACKGROUNDS"
PROBLEM FORMULATION,0.12806539509536785,"3.1
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.1307901907356948,"In this paper, we consider document classiﬁcation tasks. Each document is represented by a bag-of-
word vector x ∈Rm, where m is the number of unique words in the dataset. The i-th component
of x represents the number of occurrences of the i-th word in the document. We focus on the kNN
classiﬁcation following the original paper. The kNN classiﬁcation gathers k samples of the smallest
distances (with respect to a certain distance) to a test sample from the training dataset and classiﬁes
the sample to the majority class of the gathered labels. The design of the distance function is crucial
for the performance of kNN classiﬁcation."
PROBLEM FORMULATION,0.1335149863760218,"3.2
WORD MOVER’S DISTANCE (WMD)"
PROBLEM FORMULATION,0.1362397820163488,"WMD provides an effective distance function utilizing pre-trained word embeddings. Let zi be the
embedding of the i-th word. To utilize OT, WMD ﬁrst regards a document as a discrete probabilistic
distribution of words by normalizing the bag-of-word vector:"
PROBLEM FORMULATION,0.13896457765667575,"nL1(x) = x/
X"
PROBLEM FORMULATION,0.14168937329700274,"i
xi.
(1)"
PROBLEM FORMULATION,0.1444141689373297,Under review as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.14713896457765668,"Table 1: Dataset statistics.
bbcsport
twitter
recipe
ohsumed
classic
reuters
amazon
20news"
PROBLEM FORMULATION,0.14986376021798364,"Number of documents
737
3108
4370
9152
7093
7674
8000
18821
Number of training documents
517
2176
3059
3999
4965
5485
5600
11293
Number of test documents
220
932
1311
5153
2128
2189
2400
7528
Size of the vocabulary
13243
6344
5708
31789
24277
22425
42063
29671
Unique words in a document
116.5
9.9
48.3
60.2
38.7
36.0
44.6
69.3
Number of classes
5
3
15
10
4
8
4
20
Split type
ﬁve-fold
ﬁve-fold
ﬁve-fold
one-fold
ﬁve-fold
one-fold
ﬁve-fold
one-fold"
PROBLEM FORMULATION,0.15258855585831063,"Duplicate pairs
15
976
48
1873
2588
143
159
59
Duplicate samples
30
474
66
3116
600
197
285
88"
PROBLEM FORMULATION,0.1553133514986376,"WMD deﬁnes the cost matrix C ∈Rm×Rm as the distance of the embeddings, i.e., Cij = ∥zi−zj∥2.
The distance between the two documents x and x′ is the optimal value of the following problem:"
PROBLEM FORMULATION,0.15803814713896458,"minimize
P ∈Rm×m
X"
PROBLEM FORMULATION,0.16076294277929154,"ij
CijPij
(2)"
PROBLEM FORMULATION,0.16348773841961853,"s.t.
Pij ≥0, P 1 = nL1(x), P ⊤1 = nL1(x′),
where P ⊤denotes the transpose of P , 1 ∈Rm is the vector of ones. Intuitively, Pij represents the
amount of word i that is transported to word j. WMD is deﬁned as the minimum total distance to
convert one document to another document. Let OT(x, x′, C) ∈R be the optimal value of eq. (2)."
EXPERIMENTAL SETUPS,0.16621253405994552,"3.3
EXPERIMENTAL SETUPS"
EXPERIMENTAL SETUPS,0.16893732970027248,"Datasets. We use the same datasets (Greene & Cunningham, 2006; Sanders, 2011; Joachims, 1998;
Sebastiani, 2002; Lang, 1995) as in the original paper (Kusner et al., 2015) and use the same train/test
splits as in the original paper. Three of the eight datasets have the standard train/test splits (e.g., based
on timestamps), and the other datasets do not. Thus, the original paper used ﬁve random splits for
such datasets. We refer to the former type as one-fold datasets and the latter as ﬁve-fold datasets.
Table 1 shows the statistics. Here, we point out the ﬁrst misleading point."
EXPERIMENTAL SETUPS,0.17166212534059946,Misleading Point 1 Many duplicate samples exist in the datasets.
EXPERIMENTAL SETUPS,0.17438692098092642,"The last two rows in Table 1 report the numbers of duplicate samples. Some of these samples cross
the train/test splits, and some of them have different labels despite having the same contents. This
causes problems in the evaluations. If the pairs have different labels, it is impossible to classify both
of them correctly. If the pairs have the same label, a kNN classiﬁer places more emphasis on this
class for no reason. We report this issue in more detail in Appendix A."
EXPERIMENTAL SETUPS,0.1771117166212534,"The datasets released by the WMD paper have been used in many studies (Huang et al., 2016;
Yurochkin et al., 2019; Le et al., 2019; Werner & Laber, 2020; Wang et al., 2020b; Wu et al., 2018;"
EXPERIMENTAL SETUPS,0.17983651226158037,"Skianis et al., 2020b; Mollaysa et al., 2017; Gupta et al., 2020; Skianis et al., 2020a) with the
same protocol. We suspect that many readers and authors were unaware of the duplicate samples,
which might have led to a misleading analysis. We release the indices of duplicate documents and a
preprocessing script to remove duplicate samples for the following studies. We believe that sharing
this fact within the community is considerably important for aiding in a solid evaluation and analysis."
EXPERIMENTAL SETUPS,0.18256130790190736,"In the following, we ﬁrst use the same dataset as the WMD paper to highlight the essential differences
with the original evaluation. We then evaluate using clean datasets to further corroborate the ﬁndings."
EXPERIMENTAL SETUPS,0.18528610354223432,"Embeddings. We use the same word embeddings as in the original paper (Kusner et al., 2015).
Namely, we use the 300-dimentional word2vec embeddings trained on the Google News corpus. We
found the second misleading point here."
EXPERIMENTAL SETUPS,0.1880108991825613,"Misleading Point 2 The ofﬁcial code of WMD2 normalized the embeddings by the L2 norm, al-
though this was not explicitly stated in the paper."
EXPERIMENTAL SETUPS,0.1907356948228883,"The hint of word normalization is not in the main logic but only in the preprocessed binary ﬁle in
the WMD’s repository. We accidentally found this when we were debugging the code. Note that the
Word Rotator’s Distance (Yokoi et al., 2020) proposed to use the angles between word embeddings"
EXPERIMENTAL SETUPS,0.19346049046321526,2https://github.com/mkusner/wmd
EXPERIMENTAL SETUPS,0.19618528610354224,Under review as a conference paper at ICLR 2022
EXPERIMENTAL SETUPS,0.1989100817438692,"bbcsport
twitter
recipe
ohsumed
classic
reuters
amazon
20news
0 10 20 30 40 50 60 70"
EXPERIMENTAL SETUPS,0.2016348773841962,kNN classification error
EXPERIMENTAL SETUPS,0.20435967302452315,3.9 2.8 5.1
EXPERIMENTAL SETUPS,0.20708446866485014,30.028.929.6
EXPERIMENTAL SETUPS,0.2098092643051771,"43.4
40.142.9
44.1 37.8 44.5"
EXPERIMENTAL SETUPS,0.2125340599455041,"4.1 3.3 2.9
5.7 5.5 4.0"
EXPERIMENTAL SETUPS,0.21525885558583105,10.4 8.0 7.4
EXPERIMENTAL SETUPS,0.21798365122615804,"29.1
25.926.8 BOW BOW BOW BOW"
EXPERIMENTAL SETUPS,0.22070844686648503,TF-IDF
EXPERIMENTAL SETUPS,0.22343324250681199,TF-IDF
EXPERIMENTAL SETUPS,0.22615803814713897,TF-IDF
EXPERIMENTAL SETUPS,0.22888283378746593,TF-IDF WMD WMD WMD WMD
EXPERIMENTAL SETUPS,0.23160762942779292,↓ better
EXPERIMENTAL SETUPS,0.23433242506811988,"Figure 3: kNN classiﬁcation errors in our re-evaluation. Lower is better. The shaded bars are the
performance without normalization. WMD is comparable to classical baselines with normalization."
EXPERIMENTAL SETUPS,0.23705722070844687,"for the cost matrix of WMD, but in fact, the original WMD already used angles for the cost matrix.
We suspect that most readers missed this point. In this paper, we follow the original evaluation and
normalize the embeddings using the L2 norm."
EXPERIMENTAL SETUPS,0.23978201634877383,"Preprocessing. For WMD, we use the same preprocessing as in the original paper (Kusner et al.,
2015), whereas for BOW and TF-IDF, we use a different preprocessing to clearly contrast the results.
WMD discards certain words because the word embeddings do not contain all words. In the original
study, the authors discard out-of-vocabulary words for WMD but maintain them for BOW and TF-IDF.
In this paper, we discard out-of-vocabulary words for BOW and TF-IDF as well and use the same
vocabulary for all of WMD, BOW, and TF-IDF. This setting is slightly advantageous for WMD. We
show that WMD is comparable to BOW and TF-IDF even under this setting. We include the results
with out-of-vocabulary words at the end of Section 4 for completeness."
EXPERIMENTAL SETUPS,0.24250681198910082,"Evaluation Protocol. WMD, BOW, and TF-IDF all have only one hyperparameter, i.e., size k of
the neighborhood in kNN classiﬁcation. We split the training set into an 80/20 train/validation set
uniformly and randomly and select the neighborhood size from {1, 2, · · · , 19} using the validation
data. During the process of our re-evaluations, we found that the kNN classiﬁers were much less
capable than we previously thought and that kNN classiﬁcation may underestimate the performances
of distance-based methods. Nevertheless, we adopt kNN evaluations in the main part to clearly con-
trast our results with the original evaluation. This is justiﬁed because we only use kNN classiﬁcation
for all methods. However, this fact can be a misleading point if other classiﬁers are used for other
methods, as in (Wu et al., 2018; Skianis et al., 2020b; Mollaysa et al., 2017; Gupta et al., 2020). We
report this issue in more detail in Appendix B."
EXPERIMENTAL SETUPS,0.2452316076294278,"Environment. We use a server cluster to compute WMD. Each node has two 2.4GHz Intel Xeon Gold
6148 CPUs. We use a Linux server with Intel Xeon E7-4830 v4 CPUs to evaluate the performances."
NORMALIZATION IS CRUCIAL,0.24795640326975477,"4
NORMALIZATION IS CRUCIAL"
NORMALIZATION IS CRUCIAL,0.2506811989100817,"Importance of Normalization. In the original paper, raw BOW and TF-IDF vectors are used for
the nearest neighbor classiﬁcation. However, this is problematic because the length of these vectors
varies based on the length of the documents. Even if two documents share the same topic, the BOW
vectors are distant if their lengths differ. We need to normalize these vectors to effectively use them
in the nearest neighbor classiﬁcation."
NORMALIZATION IS CRUCIAL,0.25340599455040874,Misleading Point 3 The original evaluation did not normalize the BOW and TF-IDF vectors.
NORMALIZATION IS CRUCIAL,0.2561307901907357,"To make direct comparisons to WMD, we employ L1 normalization (Eq. (1)) and the L1 distance for
BOW and TF-IDF vectors, i.e., dBOW/L1/L1(x, x′) = ∥nL1(x) −nL1(x′)∥1. With this normalization,
BOW is a special case of WMD that does not use the underlying geometry. Speciﬁcally, let Cunif be
the cost matrix of WMD when we use one-hot vectors as word embeddings, i.e., Cunif
ij
= 0 if i = j
and Cunif
ij
= 2 if i ̸= j. Then,"
NORMALIZATION IS CRUCIAL,0.25885558583106266,"Proposition 1. dBOW/L1/L1(x, x′) = OT(x, x′, Cunif)"
NORMALIZATION IS CRUCIAL,0.2615803814713896,Under review as a conference paper at ICLR 2022
NORMALIZATION IS CRUCIAL,0.26430517711171664,"Table 2: kNN classiﬁcation errors. Lower is better. Here, (x/y) uses x as the normalization and y
as the metric. The last column reports the average relative performance to the normalized BOW.
These values correspond to Figure 4 in the original paper, but we use BOW (L1/L1) as the base
performances, while the original paper used BOW (None/L2) as the base performances. Figure 4 in
(Yurochkin et al., 2019) also reports the relative performances, but it uses BOW (L1/L2) as the base
performances. The standard deviations are reported for ﬁve-fold datasets. The ﬁrst three rows are the
same as in Figure 3. For BOW and TF-IDF, a cell is highlighted with bold if the mean score is better
than that of WMD. For WMD, a cell is highlighted with bold if the mean score is better than those of
both BOW and TF-IDF. The fourth row reports the performances with WMD with TF-IDF weighting.
The following rows report the performance with different normalization and metrics."
NORMALIZATION IS CRUCIAL,0.2670299727520436,"bbcsport
twitter
recipe
ohsumed
classic
reuters
amazon
20news
rel."
NORMALIZATION IS CRUCIAL,0.26975476839237056,"BOW (L1/L1)
3.9 ± 1.1
30.0 ± 1.1
43.4 ± 0.8
44.1
4.1 ± 0.5
5.7
10.4 ± 0.5
29.1
1.000
TF-IDF (L1/L1)
2.8 ± 1.1
28.9 ± 0.8
40.1 ± 0.7
37.8
3.3 ± 0.4
5.5
8.0 ± 0.3
25.9
0.861
WMD
5.1 ± 1.2
29.6 ± 1.5
42.9 ± 0.8
44.5
2.9 ± 0.4
4.0
7.4 ± 0.5
26.8
0.917"
NORMALIZATION IS CRUCIAL,0.2724795640326976,"WMD-TF-IDF
3.3 ± 0.9
28.3 ± 2.3
39.9 ± 1.1
39.7
2.7 ± 0.3
4.0
6.6 ± 0.2
24.1
0.804"
NORMALIZATION IS CRUCIAL,0.27520435967302453,"BOW (None/L2)Kusner et al. (2015)
19.4 ± 3.0
34.2 ± 0.6
60.0 ± 2.3
61.6
35.0 ± 0.9
11.8
28.2 ± 1.0
57.7
3.024
BOW (None/L1)
25.4 ± 1.5
32.7 ± 1.6
65.8 ± 2.5
69.3
52.1 ± 0.5
14.2
31.4 ± 1.2
73.9
3.931
TF-IDF (None/L2)Kusner et al. (2015)
24.5 ± 1.3
38.2 ± 4.6
65.0 ± 1.9
65.3
38.8 ± 1.0
28.0
41.2 ± 3.2
60.0
3.867
TF-IDF (None/L1)
30.6 ± 1.3
37.8 ± 4.8
70.3 ± 1.3
70.6
52.6 ± 0.2
29.1
41.5 ± 4.9
74.6
4.602"
NORMALIZATION IS CRUCIAL,0.2779291553133515,"BOW (L1/L2)Yurochkin et al. (2019)
11.4 ± 3.6
37.0 ± 1.4
50.8 ± 1.1
56.7
17.3 ± 1.5
12.3
35.7 ± 1.3
46.5
2.253
BOW (L2/L1)
15.2 ± 1.5
33.3 ± 1.1
61.1 ± 1.1
65.7
51.1 ± 0.4
16.2
32.2 ± 1.3
77.6
3.622
BOW (L2/L2)Werner & Laber (2020)"
NORMALIZATION IS CRUCIAL,0.28065395095367845,"Wrzalik & Krechel (2019)
5.5 ± 0.7
31.0 ± 0.8
46.1 ± 0.6
46.2
6.3 ± 0.7
8.8
13.1 ± 0.5
33.2
1.254
TF-IDF (L1/L2)
25.5 ± 11.2
35.7 ± 1.4
54.2 ± 2.7
61.4
22.6 ± 4.2
24.7
41.9 ± 2.0
45.6
3.226
TF-IDF (L2/L1)
27.5 ± 7.2
33.4 ± 1.7
64.9 ± 3.8
69.7
52.0 ± 0.2
19.5
40.8 ± 6.6
78.3
4.245
TF-IDF (L2/L2)Yurochkin et al. (2019)"
NORMALIZATION IS CRUCIAL,0.28337874659400547,"Li et al. (2019)
4.0 ± 0.7
29.8 ± 1.5
43.7 ± 1.2
38.4
5.2 ± 0.3
10.5
11.1 ± 0.9
31.6
1.145"
NORMALIZATION IS CRUCIAL,0.28610354223433243,"The proof is in Appendix E. This proposition shows that the difference in the performances between
WMD and L1/L1 BOW indicates the beneﬁt of the underlying geometry."
NORMALIZATION IS CRUCIAL,0.2888283378746594,"Figure 3 shows the classiﬁcation errors with normalization. First, we can see that the errors of BOW
and TF-IDF drastically decrease. Even BOW performs better than WMD in bbcsport and ohsumed.
TF-IDF outperforms WMD in ﬁve out of eight datasets. Even in the other datasets where WMD
outperforms the baselines, the improvements are far less signiﬁcant than what was reported in the
original evaluation. We can also observe that TF-IDF always performs better than BOW in contrast
to the original evaluation. These results make more sense than what was reported in the WMD paper,
where BOW outperformed TF-IDF."
NORMALIZATION IS CRUCIAL,0.29155313351498635,"Comparison with other Normalization. Although normalized BOW and TF-IDF have been em-
ployed in other studies (Yurochkin et al., 2019; Li et al., 2019; Werner & Laber, 2020; Wrzalik &
Krechel, 2019), it was reported that normalized BOW is still far worse than WMD, which is incom-
patible with our observations above. We found out that this occurred because of the normalization
methods and metrics used to compare the vectors. For example, Yurochkin et al. (2019) used L1
normalization and the L2 metric for BOW, i.e., ∥nL1(x) −nL1(x′)∥2, and L2 normalization and
the L2 metric for TF-IDF, i.e., ∥nL2(x) −nL2(x′)∥2, where nL2(x) = x/∥x∥2. Note that the
L2/L2 scheme corresponds to the cosine similarity. We investigate the performance with different
normalization and metrics using the same protocol as in the previous experiments. Table 2 shows
the classiﬁcation errors. First, it is easy to see in the ﬁfth through eighth rows that the performances
degrade without normalization. In addition, even with normalization, the performances are still poor
if the normalization and metric use different norms. Although the L2/L2 scheme is better than these
schemes, the L1/L1 scheme is the best for all datasets. This result is natural if we adopt the stance that
a document is a distribution of words, and based on the analogy between the L1/L1 BOW and WMD,
i.e., Proposition 1. To summarize, the normalization method signiﬁcantly affects the performance.
BOW L1/L1 corresponds to “WMD without OT” (Proposition 1), and BOW None/L1 corresponds
“BOW L1/L1 without normalization”. The performances of these methods are"
NORMALIZATION IS CRUCIAL,0.29427792915531337,"3.931 (BOW None/L1)
introducing normalization
−−−−−−−−−−−−−→1.000 (BOW L1/L1)
introducing OT
−−−−−−−−→0.917 (WMD)"
NORMALIZATION IS CRUCIAL,0.2970027247956403,"This means that WMD owes its performance improvements from the naive method (i.e., BOW
None/L1) to normalization (by a factor of 3.9) rather than the OT formulation (by only a factor of 1.09).
This “ablation study” indicates that L1 normalization is the most signiﬁcant factor of improvement in
WMD. We stress that the improvements brought about by L1 normalization are obtained almost for"
NORMALIZATION IS CRUCIAL,0.2997275204359673,Under review as a conference paper at ICLR 2022
NORMALIZATION IS CRUCIAL,0.3024523160762943,Table 3: kNN classiﬁcation errors with clean data. Lower is better. The same notations as in Table 2.
NORMALIZATION IS CRUCIAL,0.30517711171662126,"bbcsport
twitter
recipe
ohsumed
classic
reuters
amazon
20news
rel."
NORMALIZATION IS CRUCIAL,0.3079019073569482,"BOW (L1/L1)
3.7 ± 1.0
30.6 ± 1.1
42.9 ± 0.6
39.7
4.2 ± 0.5
5.5
10.6 ± 0.6
29.2
1.000
TF-IDF (L1/L1)
2.3 ± 1.4
30.2 ± 0.7
40.0 ± 1.1
33.4
3.5 ± 0.2
5.9
8.0 ± 0.6
25.9
0.866
WMD
5.5 ± 1.2
30.6 ± 1.2
42.9 ± 0.9
40.6
3.4 ± 0.6
3.8
7.3 ± 0.4
26.9
0.952"
NORMALIZATION IS CRUCIAL,0.3106267029972752,"WMD-TF-IDF
4.1 ± 1.5
28.8 ± 1.6
40.2 ± 0.9
35.7
2.8 ± 0.3
4.3
6.6 ± 0.3
24.2
0.848"
NORMALIZATION IS CRUCIAL,0.3133514986376022,"Table 4: kNN classiﬁcation errors with all words and the original datasets. Lower is better. The last
column reports the average relative performance to the normalized BOW (the ﬁrst row in Table 2)."
NORMALIZATION IS CRUCIAL,0.31607629427792916,"bbcsport
twitter
ohsumed
classic
reuters
amazon
20news
rel."
NORMALIZATION IS CRUCIAL,0.3188010899182561,"BOW (L1/L1)
3.0 ± 0.8
29.5 ± 0.7
46.0
3.9 ± 0.4
6.1
12.3 ± 0.5
32.1
1.015
TF-IDF (L1/L1)
2.8 ± 0.8
29.4 ± 0.9
38.7
3.1 ± 0.4
6.8
7.9 ± 0.3
24.8
0.877"
NORMALIZATION IS CRUCIAL,0.3215258855585831,"Table 5: kNN classiﬁcation errors with all words and clean data. Lower is better. The last column
reports the average relative performance to the normalized BOW (the ﬁrst row in Table 3)."
NORMALIZATION IS CRUCIAL,0.3242506811989101,"bbcsport
twitter
ohsumed
classic
reuters
amazon
20news
rel."
NORMALIZATION IS CRUCIAL,0.32697547683923706,"BOW (L1/L1)
2.7 ± 0.6
31.0 ± 2.0
41.0
4.1 ± 0.4
6.3
12.3 ± 0.3
32.1
1.022
TF-IDF (L1/L1)
1.8 ± 0.8
30.3 ± 1.5
34.9
3.4 ± 0.5
6.4
8.1 ± 0.2
24.8
0.849"
NORMALIZATION IS CRUCIAL,0.329700272479564,"free, whereas the improvements by WMD are at the price of substantial computational costs. We
should also point out that many papers have not clariﬁed the normalization scheme. Because the
normalization method and metrics used to compare the vectors signiﬁcantly affect the performance,
we recommend clarifying them in the experimental setups."
NORMALIZATION IS CRUCIAL,0.33242506811989103,"To better illustrate the beneﬁt of WMD, we also use TF-IDF weighting for the marginal measures
of WMD. Speciﬁcally, let xtﬁdf ∈Rd denote a TF-IDF vector. We use nL1(xtﬁdf) instead of nL1(x)
in the marginal constraints in Eq. (2). The TF-IDF weighting makes WMD as robust to noise as
TF-IDF. The fourth row in Table 2 shows that WMD with TF-IDF weighting performs the best, with
approximately a six percent improvement from the normalized TF-IDF, which corresponds to the
WMD-TF-IDF with the uniform distance matrix Cunif. Investigating the (BOW, WMD) and (TF-IDF,
WMD-TF-IDF) pairs illustrates the beneﬁt of the optimal transport formulation. We can observe six
to eight percent improvements in the optimal transport formulation from the naive baselines. This is
in contrast to the sixty percent improvement claimed in the original study."
NORMALIZATION IS CRUCIAL,0.335149863760218,"Evaluation on Clean Data. We evaluate the methods using clean datasets without duplicate samples.
We adopt the same protocol as in the previous experiments except for the removal of duplicate
documents. Table 3 reports the classiﬁcation errors. Although the tendency is the same as that in
the previous experiments, the differences between (BOW, WMD) and (TF-IDF, WMD-TF-IDF)
become even narrower. Namely, an approximately ﬁve percent improvement in WMD and two
percent improvement in WMD-TF-IDF are found. The results for other normalization and metrics
are reported in Table 7 in the appendix."
NORMALIZATION IS CRUCIAL,0.33787465940054495,"Evaluation with Out of Vocabulary Words. We had hypothesized that the two to eight percent
improvements of WMD were due to the unnecessarily discarded vocabularies of BOW, which made
the evaluations slightly advantageous to WMD. We evaluate BOW and TF-IDF, including words not in
the word2vec vocabulary. We use all but the recipe dataset because the raw texts of the recipe dataset
have not been released by the authors. We include stopwords for the Twitter dataset and remove
them for the other datasets following the original paper. Tables 4 and 5 report the classiﬁcation errors.
Against expectations, these results show that the use of all words does not improve the performance.
We hypothesize that this is because the word2vec vocabulary implicitly helps the classiﬁcation by
removing noisy words."
NORMALIZATION IS CRUCIAL,0.3405994550408719,"Summary. We should emphasize that we have not concluded that the improvements brought by
WMD are spurious. Based on our careful evaluations, we conclude that the two to eight percent
improvements from the naive baselines reported in Tables 2 and 3 are genuine. These improvements
are less sensational than those reported in the original paper, i.e., sixty percent improvement. However,
we believe that our results indicate the true performance of WMD. These results indicate that if the
speed is important, WMD may not be worth trying, whereas if the performance is crucial at any cost,
WMD may be a worthwhile candidate over L1/L1 BOW."
NORMALIZATION IS CRUCIAL,0.34332425068119893,Under review as a conference paper at ICLR 2022
NORMALIZATION IS CRUCIAL,0.3460490463215259,"0.0
0.5
1.0
1.5
0.0 0.2"
NORMALIZATION IS CRUCIAL,0.34877384196185285,Frequency
NORMALIZATION IS CRUCIAL,0.35149863760217986,bbcsport
NORMALIZATION IS CRUCIAL,0.3542234332425068,"0.0
0.5
1.0
1.5
0.0 0.2"
TWITTER,0.3569482288828338,"0.4
twitter"
TWITTER,0.35967302452316074,"0.0
0.5
1.0
1.5
0.0 0.2"
RECIPE,0.36239782016348776,"0.4
recipe"
RECIPE,0.3651226158038147,"0.0
0.5
1.0
1.5
0.0 0.1"
OHSUMED,0.3678474114441417,"0.2
ohsumed"
OHSUMED,0.37057220708446864,"0.0
0.5
1.0
1.5
0.0 0.2"
OHSUMED,0.37329700272479566,Frequency
OHSUMED,0.3760217983651226,classic
OHSUMED,0.3787465940054496,"0.0
0.5
1.0
1.5
0.0 0.2"
REUTERS,0.3814713896457766,"0.4
reuters"
REUTERS,0.38419618528610355,"0.0
0.5
1.0
1.5
0.0 0.1"
AMAZON,0.3869209809264305,"0.2
amazon"
AMAZON,0.3896457765667575,"0.0
0.5
1.0
1.5
0.0 0.1"
AMAZON,0.3923705722070845,"0.2
20news"
AMAZON,0.39509536784741145,"0.0
0.5
1.0
1.5
0.0 0.2"
AMAZON,0.3978201634877384,Frequency
AMAZON,0.40054495912806537,bbcsport_5dim
AMAZON,0.4032697547683924,"0.0
0.5
1.0
1.5
0.0"
AMAZON,0.40599455040871935,"0.5
twitter_5dim"
AMAZON,0.4087193460490463,"0.0
0.5
1.0
1.5
0.0 0.2"
AMAZON,0.4114441416893733,recipe_5dim
AMAZON,0.4141689373297003,"0.0
0.5
1.0
1.5
0.0"
AMAZON,0.41689373297002724,"0.2
ohsumed_5dim"
AMAZON,0.4196185286103542,"0.0
0.5
1.0
1.5
Distance 0.0 0.2"
AMAZON,0.4223433242506812,Frequency
AMAZON,0.4250681198910082,classic_5dim
AMAZON,0.42779291553133514,"0.0
0.5
1.0
1.5
Distance 0.00 0.25"
AMAZON,0.4305177111716621,reuters_5dim
AMAZON,0.4332425068119891,"0.0
0.5
1.0
1.5
Distance 0.0"
AMAZON,0.4359673024523161,"0.2
amazon_5dim"
AMAZON,0.43869209809264303,"0.0
0.5
1.0
1.5
Distance 0.0"
AMAZON,0.44141689373297005,"0.2
20news_5dim"
AMAZON,0.444141689373297,"Figure 4: Histograms of distances between matched word embeddings. (Top) 300-dimensional
embeddings. (Bottom) 5-dimensional embeddings."
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.44686648501362397,"5
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES"
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.44959128065395093,"In this section, we experimentally show that not only the performance of WMD but also the distance
values of WMD themselves resemble those of L1/L1 BOW."
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.45231607629427795,"Travel Distance Distribution is Two-modal in High Dimensional Spaces. We had assumed that
the distribution of distances of matched word embeddings in WMD was a Gaussian-like one modal
distribution. However, against expectations, we found that the distribution was actually two-modal in
high dimensional spaces."
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.4550408719346049,"The top panels in Figure 4 show the histograms for matched words in nearest neighbor document
pairs. The x-axis represents the distances of matched words, and the y-axis represents the frequency.
All of the histograms have acute peaks at x = 0, where the source and target words exactly match.
Other pairs of matched words are at x ≈1 and are approximately equally distant. This occurs
intuitively because most pairs of embeddings are almost orthogonal in high dimensional spaces."
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.45776566757493187,"To contrast the results with low dimensional cases, we project the 300-dimensional word2vec to a
5-dimensional space using principal component analysis and compute the same histograms using
these low dimensional embeddings. Bottom panels in Figure 4 show the histograms. In contrast to
the high dimensional cases, these histograms are almost one modal. These results indicate that the
two modalities are a characteristic phenomenon in high dimensional spaces."
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.4604904632152589,"WMD resembles L1/L1 BOW in High Dimensional Spaces. The previous experiments show that
the travel distance distribution is two modal (zero or around one) in high dimensional spaces. In
an extreme case, if the distance is exactly zero or one, Proposition 1 shows that WMD coincides
with L1/L1 BOW. If not exact, we experimentally show that the distance values of WMD themselves
resemble those of L1/L1 BOW."
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.46321525885558584,"Figure 5 shows correlations between WMD and L1/L1 BOW distances and reports the Pearson’s
correlation coefﬁcients ρ. WMD and L1/L1 BOW are surprisingly similar to each other. To contrast
the results with low dimensional cases, we also report the scatter plots using 5-dimensional embed-
dings in the bottom of Figure 5. In contrast to the high dimensional cases, these scatter plots show
that WMD in low dimensional spaces is not similar to L1/L1 BOW. These results indicate that the
similarity to L1/L1 BOW is a characteristic phenomenon in high dimensional spaces."
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.4659400544959128,"For example, the distance between “Obama” and “President” is 1.174, and the distance between
“Obama” and “band” is 1.342 in 300-dimensional word2vec. The distance between “speaks” and
“greets” is 0.978, and the distance between “speaks” and “gave” is 1.309. Although “Obama” and
“President” seem much more semantically similar than “Obama” and “band”, the distances are not
much different in high dimensional spaces. In other words, WMD does not identify “Obama” with
“President” in high dimensional spaces. A simple calculation shows that
2WMD(“Obama greets”, “band greets”) = 1.342 + 0
< 2WMD(“Obama greets”, “President speaks”) = 1.174 + 0.978,
BOW(“Obama greets”, “band greets”) = 1
< BOW(“Obama greets”, “President speaks”) = 2."
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.46866485013623976,Under review as a conference paper at ICLR 2022
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.4713896457765668,"0.0
0.5
1.0
1.5
2.0
BOW (L1/L1) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 WMD"
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.47411444141689374,"bbcsport
ρ = 0.950"
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.4768392370572207,"0.0
0.5
1.0
1.5
2.0
BOW (L1/L1) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 WMD"
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.47956403269754766,"twitter
ρ = 0.886"
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.4822888283378747,"0.0
0.5
1.0
1.5
2.0
BOW (L1/L1) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 WMD"
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.48501362397820164,"recipe
ρ = 0.956"
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.4877384196185286,"0.5
1.0
1.5
2.0
BOW (L1/L1) 0.2 0.4 0.6 0.8 1.0 1.2 WMD"
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.4904632152588556,"ohsumed
ρ = 0.882"
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.49318801089918257,"0.0
0.5
1.0
1.5
2.0
BOW (L1/L1) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 WMD"
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.49591280653950953,"classic
ρ = 0.846"
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.4986376021798365,"0.0
0.5
1.0
1.5
2.0
BOW (L1/L1) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 WMD"
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.5013623978201635,"reuters
ρ = 0.979"
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.5040871934604905,"0.0
0.5
1.0
1.5
2.0
BOW (L1/L1) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 WMD"
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.5068119891008175,"amazon
ρ = 0.840"
WMD RESEMBLES BOW IN HIGH DIMENSIONAL SPACES,0.5095367847411444,"0.0
0.5
1.0
1.5
2.0
BOW (L1/L1) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 WMD"
NEWS,0.5122615803814714,"20news
ρ = 0.837"
NEWS,0.5149863760217984,"0.0
0.5
1.0
1.5
2.0
BOW (L1/L1) 0.0 0.1 0.2 0.3 0.4 0.5 WMD"
NEWS,0.5177111716621253,"bbcsport_5dim
ρ = 0.482"
NEWS,0.5204359673024523,"0.0
0.5
1.0
1.5
2.0
BOW (L1/L1) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 WMD"
NEWS,0.5231607629427792,"twitter_5dim
ρ = 0.391"
NEWS,0.5258855585831063,"0.0
0.5
1.0
1.5
2.0
BOW (L1/L1) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 WMD"
NEWS,0.5286103542234333,"recipe_5dim
ρ = 0.611"
NEWS,0.5313351498637602,"0.5
1.0
1.5
2.0
BOW (L1/L1) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 WMD"
NEWS,0.5340599455040872,"ohsumed_5dim
ρ = 0.333"
NEWS,0.5367847411444142,"0.0
0.5
1.0
1.5
2.0
BOW (L1/L1) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 WMD"
NEWS,0.5395095367847411,"classic_5dim
ρ = 0.299"
NEWS,0.5422343324250681,"0.0
0.5
1.0
1.5
2.0
BOW (L1/L1) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 WMD"
NEWS,0.5449591280653951,"reuters_5dim
ρ = 0.415"
NEWS,0.547683923705722,"0.0
0.5
1.0
1.5
2.0
BOW (L1/L1) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 WMD"
NEWS,0.5504087193460491,"amazon_5dim
ρ = 0.327"
NEWS,0.553133514986376,"0.0
0.5
1.0
1.5
2.0
BOW (L1/L1) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 WMD"
NEWS,0.555858310626703,"20news_5dim
ρ = 0.413"
NEWS,0.55858310626703,"Figure 5: Scatter plot of WMD and L1/L1 BOW. (Top) 300-dimensional embeddings. WMD is
mostly determined by L1/L1 BOW. (Bottom) 5-dimensional embeddings."
NEWS,0.5613079019073569,"This “almost equally distant” property may not be expected in the two-dimensional illustration (Figure
1). While such an illustration is helpful for understanding the mechanism of WMD, it does not reﬂect
the high dimensional nature of word embeddings. The characteristics of high dimensionality should
also be kept in mind to understand the behavior of WMD more precisely."
NEWS,0.5640326975476839,"It should be noted that 5-dimensional WMD performs worse than 300-dimensional WMD because
5-dimensional WMD loses much information on the word geometry. On the one hand, by increasing
the number of dimensions, the word embeddings become more discriminative, and the performance
of WMD increases. On the other hand, by increasing the number of dimensions, the word embeddings
become orthogonal, and WMD approaches L1/L1 BOW, i.e., a special case of WMD that completely
discriminates each word from the others."
CONCLUSION,0.5667574931880109,"6
CONCLUSION"
CONCLUSION,0.5694822888283378,"In this paper, we pointed out that the major improvement in WMD against classical baselines is
not from the inherent feature of WMD but mainly from the normalization. We re-evaluated the
performance of WMD and classical baselines and found that classical baselines are competitive
with WMD if we normalize the vectors. We also found that WMD resembled BOW much more
than we had thought in two-dimensional illustrations owing to the high dimensionality of the word
embeddings. In the process of our re-evaluation, we found several confusing aspects in the original
evaluations of the original paper on WMD. We pointed them out in this paper to help the following
researchers conduct solid evaluations."
CONCLUSION,0.5722070844686649,Under review as a conference paper at ICLR 2022
REFERENCES,0.5749318801089919,REFERENCES
REFERENCES,0.5776566757493188,"David Alvarez-Melis, Tommi S. Jaakkola, and Stefanie Jegelka. Structured optimal transport. In
Proceedings of the 21st International Conference on Artiﬁcial Intelligence and Statistics, AISTATS,
volume 84 of Proceedings of Machine Learning Research, pp. 1771–1780. PMLR, 2018."
REFERENCES,0.5803814713896458,"Martín Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks.
In Proceedings of the 34th International Conference on Machine Learning, ICML, pp. 214–223,
2017."
REFERENCES,0.5831062670299727,"Timothy G. Armstrong, Alistair Moffat, William Webber, and Justin Zobel. Improvements that
don’t add up: ad-hoc retrieval results since 1998. In Proceedings of the 18th ACM Conference on
Information and Knowledge Management, CIKM, pp. 601–610. ACM, 2009."
REFERENCES,0.5858310626702997,"Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence
embeddings. In 5th International Conference on Learning Representations, ICLR. OpenReview.net,
2017."
REFERENCES,0.5885558583106267,"Arturs Backurs, Yihe Dong, Piotr Indyk, Ilya P. Razenshteyn, and Tal Wagner. Scalable nearest
neighbor search for optimal transport. In Proceedings of the 37th International Conference on
Machine Learning, ICML, pp. 497–506, 2020."
REFERENCES,0.5912806539509536,"Liqun Chen, Yizhe Zhang, Ruiyi Zhang, Chenyang Tao, Zhe Gan, Haichao Zhang, Bai Li, Dinghan
Shen, Changyou Chen, and Lawrence Carin. Improving sequence-to-sequence learning via optimal
transport. In 7th International Conference on Learning Representations, ICLR. OpenReview.net,
2019."
REFERENCES,0.5940054495912807,"Liqun Chen, Ke Bai, Chenyang Tao, Yizhe Zhang, Guoyin Wang, Wenlin Wang, Ricardo Henao, and
Lawrence Carin. Sequence generation with optimal-transport-enhanced reinforcement learning. In
Proceedings of the 34th AAAI Conference on Artiﬁcial Intelligence, AAAI, pp. 7512–7520. AAAI
Press, 2020a."
REFERENCES,0.5967302452316077,"Yu Chen, Lingfei Wu, and Mohammed J. Zaki. Reinforcement learning based graph-to-sequence
model for natural question generation. In 8th International Conference on Learning Representa-
tions, ICLR. OpenReview.net, 2020b."
REFERENCES,0.5994550408719346,"Elizabeth Clark, Asli Celikyilmaz, and Noah A. Smith. Sentence mover’s similarity: Automatic
evaluation for multi-sentence texts. In Proceedings of the 57th Conference of the Association for
Computational Linguistics, ACL, pp. 2748–2760. Association for Computational Linguistics, 2019."
REFERENCES,0.6021798365122616,"Francis S Collins and Lawrence A Tabak. Policy: NIH plans to enhance reproducibility. Nature
News, 505(7485):612, 2014."
REFERENCES,0.6049046321525886,"Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in
Neural Information Processing Systems 26: Annual Conference on Neural Information Processing
Systems 2013, NeurIPS, pp. 2292–2300, 2013."
REFERENCES,0.6076294277929155,"Maurizio Ferrari Dacrema, Paolo Cremonesi, and Dietmar Jannach. Are we really making much
progress? A worrying analysis of recent neural recommendation approaches. In Proceedings of
the 13th ACM Conference on Recommender Systems, RecSys, pp. 101–109. ACM, 2019."
REFERENCES,0.6103542234332425,"Fernando De Goes, Katherine Breeden, Victor Ostromoukhov, and Mathieu Desbrun. Blue noise
through optimal transport. ACM Transactions on Graphics (TOG), 31(6):1–11, 2012."
REFERENCES,0.6130790190735694,"Michel Deudon.
Learning semantic similarity in a continuous space.
In Advances in Neural
Information Processing Systems 31: Annual Conference on Neural Information Processing Systems
2018, NeurIPS, pp. 994–1005, 2018."
REFERENCES,0.6158038147138964,"Soﬁen Dhouib, Ievgen Redko, Tanguy Kerdoncuff, Rémi Emonet, and Marc Sebban. A swiss army
knife for minimax optimal transport. In Proceedings of the 37th International Conference on
Machine Learning, ICML, pp. 2504–2513, 2020."
REFERENCES,0.6185286103542235,"Yihe Dong, Yu Gao, Richard Peng, Ilya P. Razenshteyn, and Saurabh Sawlani. A study of performance
of optimal transport. arXiv, abs/2005.01182, 2020."
REFERENCES,0.6212534059945504,Under review as a conference paper at ICLR 2022
REFERENCES,0.6239782016348774,"Zi-Yi Dou and Graham Neubig. Word alignment by ﬁne-tuning embeddings on parallel corpora. In
Proceedings of the 16th Conference of the European Chapter of the Association for Computational
Linguistics: Main Volume, EACL, pp. 2112–2128. Association for Computational Linguistics,
2021."
REFERENCES,0.6267029972752044,"Richard Mansﬁeld Dudley. The speed of mean glivenko-cantelli convergence. The Annals of
Mathematical Statistics, 40(1):40–50, 1969."
REFERENCES,0.6294277929155313,"Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. A fair comparison of graph neural
networks for graph classiﬁcation. In 8th International Conference on Learning Representations,
ICLR. OpenReview.net, 2020."
REFERENCES,0.6321525885558583,"Steven N Evans and Frederick A Matsen. The phylogenetic kantorovich–rubinstein metric for
environmental sequence samples. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 74(3):569–592, 2012."
REFERENCES,0.6348773841961853,"Charlie Frogner, Farzaneh Mirzazadeh, and Justin Solomon. Learning embeddings into entropic
wasserstein spaces. In 7th International Conference on Learning Representations, ICLR. OpenRe-
view.net, 2019."
REFERENCES,0.6376021798365122,"Yang Gao, Wei Zhao, and Steffen Eger. SUPERT: towards new frontiers in unsupervised evaluation
metrics for multi-document summarization. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics, ACL, pp. 1347–1354. Association for Computational
Linguistics, 2020."
REFERENCES,0.6403269754768393,"Aude Genevay, Marco Cuturi, Gabriel Peyré, and Francis R. Bach. Stochastic optimization for
large-scale optimal transport. In Advances in Neural Information Processing Systems 29: Annual
Conference on Neural Information Processing Systems 2016, NeurIPS, pp. 3432–3440, 2016."
REFERENCES,0.6430517711171662,"Aude Genevay, Gabriel Peyré, and Marco Cuturi. Learning generative models with sinkhorn di-
vergences. In Proceedings of the 23rd International Conference on Artiﬁcial Intelligence and
Statistics, AISTATS, volume 84 of Proceedings of Machine Learning Research, pp. 1608–1617.
PMLR, 2018."
REFERENCES,0.6457765667574932,"Aude Genevay, Lénaïc Chizat, Francis R. Bach, Marco Cuturi, and Gabriel Peyré. Sample complexity
of sinkhorn divergences. In Proceedings of the 22nd International Conference on Artiﬁcial
Intelligence and Statistics, AISTATS, volume 89 of Proceedings of Machine Learning Research, pp.
1574–1583. PMLR, 2019."
REFERENCES,0.6485013623978202,"Steven N Goodman, Daniele Fanelli, and John PA Ioannidis. What does research reproducibility
mean? Science translational medicine, 8(341):341ps12–341ps12, 2016."
REFERENCES,0.6512261580381471,"Edouard Grave, Armand Joulin, and Quentin Berthet. Unsupervised alignment of embeddings
with wasserstein procrustes. In Proceedings of the 22nd International Conference on Artiﬁcial
Intelligence and Statistics, AISTATS, volume 89 of Proceedings of Machine Learning Research, pp.
1880–1890. PMLR, 2019."
REFERENCES,0.6539509536784741,"Derek Greene and Padraig Cunningham. Practical solutions to the problem of diagonal dominance
in kernel document clustering. In Proceedings of the 23rd International Conference on Machine
Learning, ICML, pp. 377–384, 2006."
REFERENCES,0.6566757493188011,"Vivek Gupta, Ankit Saw, Pegah Nokhiz, Praneeth Netrapalli, Piyush Rai, and Partha P. Talukdar. P-
SIF: document embeddings using partition averaging. In Proceedings of the 34th AAAI Conference
on Artiﬁcial Intelligence, AAAI, pp. 7863–7870. AAAI Press, 2020."
REFERENCES,0.659400544959128,"Gao Huang, Chuan Guo, Matt J. Kusner, Yu Sun, Fei Sha, and Kilian Q. Weinberger. Supervised word
mover’s distance. In Advances in Neural Information Processing Systems 29: Annual Conference
on Neural Information Processing Systems 2016, NeurIPS, pp. 4862–4870, 2016."
REFERENCES,0.662125340599455,"Minhui Huang, Shiqian Ma, and Lifeng Lai. A riemannian block coordinate descent method for
computing the projection robust wasserstein distance. In Proceedings of the 38th International
Conference on Machine Learning, ICML, volume 139, pp. 4446–4455. PMLR, 2021."
REFERENCES,0.6648501362397821,Under review as a conference paper at ICLR 2022
REFERENCES,0.667574931880109,"Thorsten Joachims. Text categorization with support vector machines: Learning with many relevant
features. In Proceedings of the 10th European Conference on Machine Learning, ECML, volume
1398 of Lecture Notes in Computer Science, pp. 137–142. Springer, 1998."
REFERENCES,0.670299727520436,"Mert Kilickaya, Aykut Erdem, Nazli Ikizler-Cinbis, and Erkut Erdem. Re-evaluating automatic
metrics for image captioning. In Proceedings of the 15th Conference of the European Chapter of
the Association for Computational Linguistics, EACL, pp. 199–209. Association for Computational
Linguistics, 2017."
REFERENCES,0.6730245231607629,"Hayato Kobayashi, Masaki Noguchi, and Taichi Yatsuka. Summarization based on embedding
distributions. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language
Processing, EMNLP, pp. 1984–1989. The Association for Computational Linguistics, 2015."
REFERENCES,0.6757493188010899,"Soheil Kolouri, Yang Zou, and Gustavo K. Rohde.
Sliced wasserstein kernels for probability
distributions. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp.
5258–5267, 2016."
REFERENCES,0.6784741144414169,"Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo K. Rohde. Generalized
sliced wasserstein distances. In Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS, pp. 261–272, 2019."
REFERENCES,0.6811989100817438,"Sachin Kumar, Soumen Chakrabarti, and Shourya Roy. Earth mover’s distance pooling over siamese
lstms for automatic short answer grading. In Proceedings of the 26th International Joint Conference
on Artiﬁcial Intelligence, IJCAI, pp. 2046–2052. ijcai.org, 2017."
REFERENCES,0.6839237057220708,"Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger. From word embeddings to
document distances. In Proceedings of the 32nd International Conference on Machine Learning,
ICML, pp. 957–966, 2015."
REFERENCES,0.6866485013623979,"Ken Lang. NewsWeeder: Learning to ﬁlter netnews. In Proceedings of the 12th International
Conference on Machine Learning, ICML, pp. 331–339, 1995."
REFERENCES,0.6893732970027248,"Tam Le, Makoto Yamada, Kenji Fukumizu, and Marco Cuturi. Tree-sliced variants of wasserstein
distances. In Advances in Neural Information Processing Systems 32: Annual Conference on
Neural Information Processing Systems 2019, NeurIPS, pp. 12283–12294, 2019."
REFERENCES,0.6920980926430518,"Qi Lei, Lingfei Wu, Pin-Yu Chen, Alex Dimakis, Inderjit S. Dhillon, and Michael J. Witbrock.
Discrete adversarial attacks and submodular optimization with applications to text classiﬁcation.
In Proceedings of Machine Learning and Systems 2019, MLSys. mlsys.org, 2019."
REFERENCES,0.6948228882833788,"Changchun Li, Jihong Ouyang, and Ximing Li. Classifying extremely short texts by exploiting
semantic centroids in word mover’s distance space. In The World Wide Web Conference, WWW,
pp. 939–949. ACM, 2019."
REFERENCES,0.6975476839237057,"Jianqiao Li, Chunyuan Li, Guoyin Wang, Hao Fu, Yuh-Chen Lin, Liqun Chen, Yizhe Zhang,
Chenyang Tao, Ruiyi Zhang, Wenlin Wang, Dinghan Shen, Qian Yang, and Lawrence Carin.
Improving text generation with student-forcing optimal transport. In Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing, EMNLP, pp. 9144–9156.
Association for Computational Linguistics, 2020."
REFERENCES,0.7002724795640327,"Jimmy Lin. The neural hype and comparisons against weak baselines. SIGIR Forum, 52(2):40–51,
2018."
REFERENCES,0.7029972752043597,"Tianyi Lin, Chenyou Fan, Nhat Ho, Marco Cuturi, and Michael I. Jordan. Projection robust wasser-
stein distance and riemannian optimization. In Advances in Neural Information Processing Systems
33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS, 2020."
REFERENCES,0.7057220708446866,"Catherine Lozupone and Rob Knight. UniFrac: a new phylogenetic method for comparing microbial
communities. Appl. Environ. Microbiol., 71(12):8228–8235, 2005."
REFERENCES,0.7084468664850136,"Sidi Lu, Lantao Yu, Siyuan Feng, Yaoming Zhu, and Weinan Zhang. CoT: Cooperative training
for generative modeling of discrete data. In Proceedings of the 36th International Conference on
Machine Learning, ICML, pp. 4164–4172, 2019."
REFERENCES,0.7111716621253406,Under review as a conference paper at ICLR 2022
REFERENCES,0.7138964577656676,"Malte Ludewig, Noemi Mauro, Sara Latiﬁ, and Dietmar Jannach. Performance comparison of neural
and non-neural approaches to session-based recommendation. In Proceedings of the 13th ACM
Conference on Recommender Systems, RecSys, pp. 462–466. ACM, 2019."
REFERENCES,0.7166212534059946,"Paul Michel, Abhilasha Ravichander, and Shruti Rijhwani. Does the geometry of word embeddings
help document classiﬁcation? A case study on persistent homology-based representations. In
Proceedings of the 2nd Workshop on Representation Learning for NLP, Rep4NLP@ACL, pp.
235–240. Association for Computational Linguistics, 2017."
REFERENCES,0.7193460490463215,"Amina Mollaysa, Pablo Strasser, and Alexandros Kalousis. Regularising non-linear models using
feature side-information. In Proceedings of the 34th International Conference on Machine Learning,
ICML, pp. 2508–2517, 2017."
REFERENCES,0.7220708446866485,"Marcus R Munafò, Brian A Nosek, Dorothy VM Bishop, Katherine S Button, Christopher D
Chambers, Nathalie Percie Du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J Ware, and
John PA Ioannidis. A manifesto for reproducible science. Nature human behaviour, 1(1):1–9,
2017."
REFERENCES,0.7247956403269755,"Boris Muzellec and Marco Cuturi. Generalizing point embeddings using the wasserstein space
of elliptical distributions. In Advances in Neural Information Processing Systems 31: Annual
Conference on Neural Information Processing Systems 2018, NeurIPS, pp. 10258–10269, 2018."
REFERENCES,0.7275204359673024,"Kangyu Ni, Xavier Bresson, Tony F. Chan, and Selim Esedoglu. Local histogram based segmentation
using the wasserstein distance. International Journal of Computer Vision, 84(1):97–111, 2009."
REFERENCES,0.7302452316076294,"Giannis Nikolentzos, Polykarpos Meladianos, François Rousseau, Yannis Stavrakas, and Michalis
Vazirgiannis. Multivariate gaussian document representation from word embeddings for text
categorization. In Proceedings of the 15th Conference of the European Chapter of the Association
for Computational Linguistics, EACL, pp. 450–455. Association for Computational Linguistics,
2017."
REFERENCES,0.7329700272479565,"Giannis Nikolentzos, Antoine J.-P. Tixier, and Michalis Vazirgiannis. Message passing attention
networks for document understanding. In Proceedings of the 34th AAAI Conference on Artiﬁcial
Intelligence, AAAI, pp. 8544–8551. AAAI Press, 2020."
REFERENCES,0.7356948228882834,"François-Pierre Paty and Marco Cuturi. Subspace robust wasserstein distances. In Proceedings of the
36th International Conference on Machine Learning, ICML, pp. 5072–5081, 2019."
REFERENCES,0.7384196185286104,"Mathis Petrovich, Chao Liang, Ryoma Sato, Yanbin Liu, Yao-Hung Hubert Tsai, Linchao Zhu,
Yi Yang, Ruslan Salakhutdinov, and Makoto Yamada. Feature robust optimal transport for high-
dimensional data. arXiv, abs/2005.12123, 2020."
REFERENCES,0.7411444141689373,"Julien Rabin, Gabriel Peyré, Julie Delon, and Marc Bernot. Wasserstein barycenter and its application
to texture mixing. In Proceedings of Scale Space and Variational Methods in Computer Vision, pp.
435–446, 2011."
REFERENCES,0.7438692098092643,"Antoine Rolet, Marco Cuturi, and Gabriel Peyré. Fast dictionary learning with a smoothed wasserstein
loss. In Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics,
AISTATS, pp. 630–638, 2016."
REFERENCES,0.7465940054495913,"Tim Salimans, Han Zhang, Alec Radford, and Dimitris N. Metaxas. Improving gans using optimal
transport. In 6th International Conference on Learning Representations, ICLR, 2018."
REFERENCES,0.7493188010899182,"Niek J Sanders. Sanders-twitter sentiment corpus. Sanders Analytics LLC, 242, 2011."
REFERENCES,0.7520435967302452,"Filippo Santambrogio. Optimal transport for applied mathematicians. Birkhauser, 2015."
REFERENCES,0.7547683923705722,"Ryoma Sato, Marco Cuturi, Makoto Yamada, and Hisashi Kashima. Fast and robust comparison of
probability measures in heterogeneous spaces. arXiv, abs/2002.01615, 2020a."
REFERENCES,0.7574931880108992,"Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Fast unbalanced optimal transport on a tree. In
Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS, 2020b."
REFERENCES,0.7602179836512262,Under review as a conference paper at ICLR 2022
REFERENCES,0.7629427792915532,"Geoffrey Schiebinger, Jian Shu, Marcin Tabaka, Brian Cleary, Vidya Subramanian, Aryeh Solomon,
Joshua Gould, Siyan Liu, Stacie Lin, Peter Berube, et al. Optimal-transport analysis of single-cell
gene expression identiﬁes developmental trajectories in reprogramming. Cell, 176(4):928–943,
2019."
REFERENCES,0.7656675749318801,"D. Sculley, Jasper Snoek, Alexander B. Wiltschko, and Ali Rahimi. Winner’s curse? on pace,
progress, and empirical rigor. In 6th International Conference on Learning Representations, ICLR.
OpenReview.net, 2018."
REFERENCES,0.7683923705722071,"Fabrizio Sebastiani. Machine learning in automated text categorization. ACM Comput. Surv., 34(1):
1–47, 2002."
REFERENCES,0.771117166212534,"Dinghan Shen, Guoyin Wang, Wenlin Wang, Martin Renqiang Min, Qinliang Su, Yizhe Zhang,
Chunyuan Li, Ricardo Henao, and Lawrence Carin. Baseline needs more love: On simple word-
embedding-based models and associated pooling mechanisms. In Proceedings of the 56th Annual
Meeting of the Association for Computational Linguistics, ACL, pp. 440–450. Association for
Computational Linguistics, 2018."
REFERENCES,0.773841961852861,"Sidak Pal Singh, Andreas Hug, Aymeric Dieuleveut, and Martin Jaggi. Context mover’s distance
& barycenters: Optimal transport of contexts for building representations. In Proceedings of the
23rd International Conference on Artiﬁcial Intelligence and Statistics, AISTATS, volume 108, pp.
3437–3449. PMLR, 2020."
REFERENCES,0.776566757493188,"Konstantinos Skianis, Fragkiskos D. Malliaros, Nikolaos Tziortziotis, and Michalis Vazirgiannis.
Boosting tricks for word mover’s distance. In Artiﬁcial Neural Networks and Machine Learning,
ICANN, pp. 761–772. Springer, 2020a."
REFERENCES,0.779291553133515,"Konstantinos Skianis, Giannis Nikolentzos, Stratis Limnios, and Michalis Vazirgiannis. Rep the
set: Neural networks for learning set representations. In Proceedings of the 23rd International
Conference on Artiﬁcial Intelligence and Statistics, AISTATS, volume 108 of Proceedings of
Machine Learning Research, pp. 1410–1420. PMLR, 2020b."
REFERENCES,0.782016348773842,"Chi Sun, Hang Yan, Xipeng Qiu, and Xuanjing Huang. Gaussian word embedding with a wasserstein
distance loss. arXiv, abs/1808.07016, 2018."
REFERENCES,0.784741144414169,"Yuki Takezawa, Ryoma Sato, and Makoto Yamada. Supervised tree-wasserstein distance. In
Proceedings of the 38th International Conference on Machine Learning, ICML, volume 139, pp.
10086–10095, 2021."
REFERENCES,0.7874659400544959,"Martin Trapp, Marcin Skowron, and Dietmar Schabus. Retrieving compositional documents us-
ing position-sensitive word mover’s distance. In Proceedings of the ACM SIGIR International
Conference on Theory of Information Retrieval, ICTIR, pp. 233–236. ACM, 2017."
REFERENCES,0.7901907356948229,"Qingzhong Wang, Jia Wan, and Antoni B Chan. On diversity in image captioning: Metrics and
methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020a."
REFERENCES,0.7929155313351499,"Zihao Wang, Datong Zhou, Ming Yang, Yong Zhang, Chenglong Rao, and Hao Wu. Robust document
distance with wasserstein-ﬁsher-rao metric. In Proceedings of The 12th Asian Conference on
Machine Learning, ACML, volume 129 of Proceedings of Machine Learning Research, pp. 721–
736. PMLR, 2020b."
REFERENCES,0.7956403269754768,"Jonathan Weed, Francis Bach, et al. Sharp asymptotic and ﬁnite-sample rates of convergence of
empirical measures in wasserstein distance. Bernoulli, 25(4A):2620–2648, 2019."
REFERENCES,0.7983651226158038,"Matheus Werner and Eduardo Laber. Speeding up word mover’s distance and its variants via
properties of distances between embeddings. In Proceedings of the 24th European Conference on
Artiﬁcial Intelligence, ECAI, volume 325 of Frontiers in Artiﬁcial Intelligence and Applications,
pp. 2204–2211. IOS Press, 2020."
REFERENCES,0.8010899182561307,"Marco Wrzalik and Dirk Krechel. Balanced word clusters for interpretable document representation.
In Second International Workshop on Machine Learning, WML@ICDAR, pp. 103–109. IEEE,
2019."
REFERENCES,0.8038147138964578,Under review as a conference paper at ICLR 2022
REFERENCES,0.8065395095367848,"Lingfei Wu, Ian En-Hsu Yen, Kun Xu, Fangli Xu, Avinash Balakrishnan, Pin-Yu Chen, Pradeep
Ravikumar, and Michael J. Witbrock. Word mover’s embedding: From word2vec to document
embedding. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing, EMNLP, pp. 4524–4534. Association for Computational Linguistics, 2018."
REFERENCES,0.8092643051771117,"Hongteng Xu, Wenlin Wang, Wei Liu, and Lawrence Carin. Distilled wasserstein learning for word
embedding and topic modeling. In Advances in Neural Information Processing Systems 31: Annual
Conference on Neural Information Processing Systems 2018, NeurIPS, pp. 1723–1732, 2018."
REFERENCES,0.8119891008174387,"Sho Yokoi, Ryo Takahashi, Reina Akama, Jun Suzuki, and Kentaro Inui. Word rotator’s distance.
In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing,
EMNLP, pp. 2944–2960. Association for Computational Linguistics, 2020."
REFERENCES,0.8147138964577657,"Mikhail Yurochkin, Sebastian Claici, Edward Chien, Farzaneh Mirzazadeh, and Justin M. Solomon.
Hierarchical optimal transport for document representation. In Advances in Neural Information
Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019,
NeurIPS, pp. 1599–1609, 2019."
REFERENCES,0.8174386920980926,"Meng Zhang, Yang Liu, Huan-Bo Luan, Maosong Sun, Tatsuya Izuha, and Jie Hao. Building earth
mover’s distance on bilingual word embeddings for machine translation. In Proceedings of the
30th AAAI Conference on Artiﬁcial Intelligence, AAAI, pp. 2870–2876. AAAI Press, 2016."
REFERENCES,0.8201634877384196,"Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. Earth mover’s distance minimization for
unsupervised bilingual lexicon induction. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, EMNLP, pp. 1934–1945. Association for Computational
Linguistics, 2017a."
REFERENCES,0.8228882833787466,"Meng Zhang, Haoruo Peng, Yang Liu, Huan-Bo Luan, and Maosong Sun. Bilingual lexicon induction
from non-parallel data with minimal supervision. In Proceedings of the 31st AAAI Conference on
Artiﬁcial Intelligence, AAAI, pp. 3379–3385. AAAI Press, 2017b."
REFERENCES,0.8256130790190735,"Ruiyi Zhang, Changyou Chen, Zhe Gan, Zheng Wen, Wenlin Wang, and Lawrence Carin. Nested-
wasserstein self-imitation learning for sequence generation. In Proceedings of the 23rd Interna-
tional Conference on Artiﬁcial Intelligence and Statistics, AISTATS, volume 108 of Proceedings of
Machine Learning Research, pp. 422–433. PMLR, 2020a."
REFERENCES,0.8283378746594006,"Ruiyi Zhang, Changyou Chen, Xinyuan Zhang, Ke Bai, and Lawrence Carin. Semantic matching
via optimal partial transport. In Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: Findings, EMNLP, pp. 212–222. Association for Computational
Linguistics, 2020b."
REFERENCES,0.8310626702997275,"Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. BERTScore: Evalu-
ating text generation with BERT. In 8th International Conference on Learning Representations,
ICLR. OpenReview.net, 2020c."
REFERENCES,0.8337874659400545,"Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. Moverscore:
Text generation evaluating with contextualized embeddings and earth mover distance. In Proceed-
ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing, EMNLP-IJCNLP, pp. 563–578.
Association for Computational Linguistics, 2019."
REFERENCES,0.8365122615803815,"Wei Zhao, Goran Glavas, Maxime Peyrard, Yang Gao, Robert West, and Steffen Eger. On the
limitations of cross-lingual encoders as exposed by reference-free machine translation evaluation.
In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL,
pp. 1656–1671. Association for Computational Linguistics, 2020."
REFERENCES,0.8392370572207084,Under review as a conference paper at ICLR 2022
REFERENCES,0.8419618528610354,"Table 6: Weighted kNN classiﬁcation errors. Lower is better. The last column reports the average
relative performance to the normalized BOW with the standard kNN (the ﬁrst row in Table 2)."
REFERENCES,0.8446866485013624,"bbcsport
twitter
recipe
ohsumed
classic
reuters
amazon
20news
rel."
REFERENCES,0.8474114441416893,"BOW (L1/L1/wkNN)
3.3 ± 0.6
26.4 ± 1.6
41.1 ± 0.5
41.3
3.6 ± 0.4
4.7
10.3 ± 0.3
23.3
0.888
TF-IDF (L1/L1/wkNN)
2.0 ± 0.7
26.6 ± 1.5
38.3 ± 1.1
36.1
2.8 ± 0.4
5.8
8.2 ± 0.5
20.5
0.787
WMD (wkNN)
4.4 ± 1.2
26.2 ± 2.2
40.5 ± 1.0
41.1
2.7 ± 0.4
3.6
7.1 ± 0.5
21.7
0.823
WMD-TF-IDF (wkNN)
3.2 ± 1.0
25.6 ± 1.5
37.7 ± 0.9
37.0
2.4 ± 0.4
4.0
6.3 ± 0.4
19.6
0.743"
REFERENCES,0.8501362397820164,"A
DUPLICATE DOCUMENTS"
REFERENCES,0.8528610354223434,"We point out that there are many duplicate samples in the datasets used in the original study. The last
two rows in Table 2 report the numbers of duplicate samples. Speciﬁcally, “duplicate pair” reports
the number of pairs (s, t) such that s and t are the same, and “duplicate samples” reports the number
of samples s such that s has at least one samples t with the same content. Some of these pairs
cross the training and test splits, and some of them have different labels despite having the same
contents. This causes problems in the evaluations. If the pairs have different labels, it is impossible
to classify both of them correctly. If the pairs have the same label, a kNN classiﬁer places more
emphasis on this class for no reason. We found that duplication was caused by different reasons in
different datasets. For example, the ohsumed dataset was originally a multi-labeled dataset, and the
WMD paper duplicate samples for each label. We found that the original source dataset (Greene &
Cunningham, 2006) of bbcsport3 already contained duplicate samples, e.g., athletics/12.txt
and athletics/20.txt. We hypothesis this was originated from the data collection process."
REFERENCES,0.8555858310626703,"Although such duplication is not a problem if we intend to measure the performances in noisy
environments, the analysis could be misleading if we assume a clean environment. The datasets
released by the WMD paper have been used in many studies (Huang et al., 2016; Yurochkin et al.,
2019; Le et al., 2019; Werner & Laber, 2020; Wang et al., 2020b; Wu et al., 2018; Skianis et al.,
2020b; Mollaysa et al., 2017; Gupta et al., 2020; Skianis et al., 2020a) with the same protocol.
We suspect that many readers and authors were unaware of this fact, which might have led to a
misleading analysis. We release the indices of duplicate documents and a preprocessing script to
remove duplicate samples for the following studies. We believe that sharing this fact within the
community is considerably important for aiding in a solid evaluation and analysis."
REFERENCES,0.8583106267029973,"B
NEAREST NEIGHBOR CLASSIFICATION IS FAR FROM OPTIMAL"
REFERENCES,0.8610354223433242,"WMD is often used as a weak baseline in document classiﬁcation. In this section, we point out that
the poor performances observed in previous studies are not necessarily due to the problem we found
in the main part but rather due to the choices of classiﬁers. We found that many existing studies,
including the original paper on WMD, used kNN classiﬁcation for distance-based methods (Kusner
et al., 2015; Huang et al., 2016; Wang et al., 2020b). This is reasonable owing to its simplicity
regardless of suboptimal performance. However, we found that kNN classiﬁcation was much farther
from optimal than we had thought and caused some improper observations. The most prominent issue
appears in comparison with non-kNN classiﬁers. For example, WMD is used with a kNN classiﬁer
as a baseline, and other classiﬁers are adopted for the proposed methods in (Wu et al., 2018; Skianis
et al., 2020b; Mollaysa et al., 2017; Li et al., 2019; Gupta et al., 2020; Nikolentzos et al., 2020; 2017).
The improvement observed by such evaluations may not be due to the superiority of the proposed
methods against WMD, but due to the superiority of the classiﬁer against the kNN classiﬁer. To
mitigate this problem, we found that a weighted kNN classiﬁer (wkNN) with exponential weighting
was a good choice for distance-based methods. Speciﬁcally, wkNN ﬁrst gathers k nearest neighbor
samples and computes the distances {d1, · · · , dk} to these samples. It then deﬁnes the weight for
sample i as exp(−di/γ) and takes the weighted majority vote, where γ is a hyperparameter. We can
think of wkNN as a continuous variant of kNN, which uses a step function as the weight function.
As the crux of wkNN, the time and space complexities are the same as in kNN, and it involves no
training procedures. Thus, we can easily replace a kNN system with a wkNN system."
REFERENCES,0.8637602179836512,3http://mlg.ucd.ie/datasets/bbc.html
REFERENCES,0.8664850136239782,Under review as a conference paper at ICLR 2022
REFERENCES,0.8692098092643051,Table 7: kNN classiﬁcation errors with clean data. Lower is better. The same notations as in Table 2.
REFERENCES,0.8719346049046321,"bbcsport
twitter
recipe
ohsumed
classic
reuters
amazon
20news
rel."
REFERENCES,0.8746594005449592,"BOW (L1/L1)
3.7 ± 1.0
30.6 ± 1.1
42.9 ± 0.6
39.7
4.2 ± 0.5
5.5
10.6 ± 0.6
29.2
1.000
TF-IDF (L1/L1)
2.3 ± 1.4
30.2 ± 0.7
40.0 ± 1.1
33.4
3.5 ± 0.2
5.9
8.0 ± 0.6
25.9
0.866
WMD
5.5 ± 1.2
30.6 ± 1.2
42.9 ± 0.9
40.6
3.4 ± 0.6
3.8
7.3 ± 0.4
26.9
0.952"
REFERENCES,0.8773841961852861,"WMD-TF-IDF
4.1 ± 1.5
28.8 ± 1.6
40.2 ± 0.9
35.7
2.8 ± 0.3
4.3
6.6 ± 0.3
24.2
0.848"
REFERENCES,0.8801089918256131,"BOW (None/L2)
22.8 ± 1.6
34.2 ± 0.6
59.1 ± 0.9
60.7
36.9 ± 1.1
11.7
28.9 ± 1.1
58.0
3.227
BOW (None/L1)
25.3 ± 2.1
33.9 ± 0.8
64.1 ± 0.8
67.4
55.0 ± 0.5
14.0
32.4 ± 1.3
73.5
4.044
TF-IDF (None/L2)
25.8 ± 1.1
33.5 ± 0.6
65.6 ± 1.0
62.8
41.1 ± 1.1
28.3
43.4 ± 5.4
59.8
4.032
TF-IDF (None/L1)
32.7 ± 1.2
33.6 ± 0.6
70.6 ± 1.8
69.5
55.6 ± 0.2
29.1
42.5 ± 4.8
74.7
4.804"
REFERENCES,0.8828337874659401,"BOW (L1/L2)
11.8 ± 0.7
40.2 ± 2.0
51.4 ± 1.4
55.8
17.5 ± 1.7
12.9
36.8 ± 1.4
46.7
2.336
BOW (L2/L1)
15.0 ± 1.6
34.3 ± 0.9
59.9 ± 0.7
64.3
54.0 ± 0.4
16.2
32.7 ± 2.1
77.1
3.715
BOW (L2/L2)
5.3 ± 0.9
32.6 ± 0.7
45.4 ± 1.1
43.2
6.7 ± 0.7
8.1
13.3 ± 0.5
33.2
1.263
TF-IDF (L1/L2)
20.0 ± 7.4
39.3 ± 1.4
54.1 ± 2.4
55.2
24.8 ± 3.0
24.5
43.2 ± 2.0
45.9
3.168
TF-IDF (L2/L1)
28.7 ± 5.8
33.5 ± 1.0
65.7 ± 2.8
65.7
55.0 ± 0.2
19.9
39.0 ± 5.4
78.6
4.390
TF-IDF (L2/L2)
3.1 ± 1.3
31.5 ± 1.5
43.6 ± 0.8
33.2
5.2 ± 0.4
10.6
11.1 ± 0.6
31.6
1.127"
REFERENCES,0.885558583106267,"Table 8: kNN classiﬁcation errors with 5 dimensional embeddings. Lower is better. The ﬁrst two
rows are the same as Table 2 for reference."
REFERENCES,0.888283378746594,"bbcsport
twitter
recipe
ohsumed
classic
reuters
amazon
20news
rel."
REFERENCES,0.8910081743869209,"BOW (L1/L1)
3.9 ± 1.1
30.0 ± 1.1
43.4 ± 0.8
44.1
4.1 ± 0.5
5.7
10.4 ± 0.5
29.1
1.000
WMD
5.1 ± 1.2
29.6 ± 1.5
42.9 ± 0.8
44.5
2.9 ± 0.4
4.0
7.4 ± 0.5
26.8
0.917
WMD (5 dim)
18.5 ± 0.8
32.0 ± 1.0
51.8 ± 1.0
60.2
6.2 ± 0.3
6.9
15.9 ± 0.8
45.4
1.773"
REFERENCES,0.8937329700272479,"We conduct experiments to show the suboptimality of kNN. As the drawback of wkNN, it has two
hyperparameters k and γ. We ﬁnd that the k of wkNN plays a similar role as the maximum k of kNN
in the hyperparameter tuning. Thus, we ﬁx k of wkNN to 19 and tune only γ in the hyperparameter
tuning. Recall that the hyperparameter candidates were k ∈{1, · · · , 19} for kNN in the previous
experiments and original paper. We select γ from Γ = {0.005, 0.010, · · · , 0.095, 0.1} (|Γ| = 20)
in the hyperparameter tuning4. The remaining settings are the same as in the previous experiments.
Table 6 shows the classiﬁcation errors. We can see that wkNN performs much better than kNN. The
beneﬁt of wkNN (the ﬁrst row in Table 6 versus the ﬁrst row in Table 2) is more signiﬁcant than the
beneﬁt of WMD (the third versus the ﬁrst row in Table 2). This indicates that if a proposed method
uses non-kNN classiﬁers, ten percent improvements from the kNN baselines may be due to classiﬁers,
not to the proposed method. This also indicates that there is signiﬁcant room for improvement in the
classiﬁers before undergoing a design of better similarity measures."
REFERENCES,0.896457765667575,"It might be acceptable if all baselines and the proposed method use kNN because the relative
performances in Tables 2 and 6 do not change signiﬁcantly. However, it would be better to use more
capable classiﬁers because the results in a paper will be cited as baseline records in following papers,
in which other classiﬁers may be employed. In fact, such comparisons have been carried out in (Wu
et al., 2018; Skianis et al., 2020b; Mollaysa et al., 2017; Gupta et al., 2020). Besides, practitioners
may underestimate the performance of distance-based methods based on the reported results. We ﬁnd
wkNN is a better choice than kNN because it is as fast as kNN yet much more effective."
REFERENCES,0.8991825613079019,"C
COMPARISON WITH OTHER NORMALIZATION WITH CLEAN DATA"
REFERENCES,0.9019073569482289,"Table 7 shows the kNN classiﬁcation errors with clean data. The same tendency as Table 2 can be
observed."
REFERENCES,0.9046321525885559,"D
PERFORMANCE OF LOW DIMENSIONAL EMBEDDINGS"
REFERENCES,0.9073569482288828,"Table 8 shows the classiﬁcation errors of WMD with the 5-dimensional embeddings used in Section 5.
It shows that WMD with the low dimensional embeddings performs worse than with the original 300-
dimensional embeddings, although WMD performs differently from BOW with the low dimensional
embeddings (Figure 5). In general, an exact match of words indicates a strong similarity. It is"
REFERENCES,0.9100817438692098,"4The magnitude of this range is determined by the empirical distances between samples. This arbitrary
choice is a slight drawback of wkNN. We will investigate how to choose it automatically in future work."
REFERENCES,0.9128065395095368,Under review as a conference paper at ICLR 2022
REFERENCES,0.9155313351498637,"reasonable that counting common words can perform better than relying too much on the underlying
geometry."
REFERENCES,0.9182561307901907,"E
PROOF OF PROPOSITION 1"
REFERENCES,0.9209809264305178,"We prove the analogy between L1/L1 BOW and OT. Recall that
dBOW/L1/L1(x, x′) = ∥nL1(x) −nL1(x′)∥1,
and"
REFERENCES,0.9237057220708447,"Cunif
ij
=
0
(i = j)
2
(i ̸= j)."
REFERENCES,0.9264305177111717,"Proof of Proposition 1. Without loss of generality, we assume that the marginals x and x′ are already
L1 normalized, and P
i xi = 1 and P
i x′
i = 1 hold. Let P ∗∈Rm×m be the optimal solution of
OT(x, x′, Cunif). From the marginal constraints, P ∗
ii ≤min(xi, x′
i) holds. If P ∗
ii < min(xi, x′
i),
there exists j ̸= i and k ̸= i such that P ∗
ij > 0 and P ∗
ki > 0 from the marginal constraints. Then, let
Q ∈Rm×m be Qst ="
REFERENCES,0.9291553133514986,"





"
REFERENCES,0.9318801089918256,"




"
REFERENCES,0.9346049046321526,"P ∗
ii + min(P ∗
ij, P ∗
ki)
(s = i ∧t = i)
P ∗
ij −min(P ∗
ij, P ∗
ki)
(s = i ∧t = j)
P ∗
ki −min(P ∗
ij, P ∗
ki)
(s = k ∧t = i)
P ∗
kj + min(P ∗
ij, P ∗
ki)
(s = k ∧t = j)
P ∗
st
(otherwise).
Then, Q satisﬁes the constraints of OT and
 X"
REFERENCES,0.9373297002724795,"st
Cunif
st P ∗
st ! − X"
REFERENCES,0.9400544959128065,"st
Cunif
st Qst !"
REFERENCES,0.9427792915531336,"= Cunif
ii (P ∗
ii −(P ∗
ii + min(P ∗
ij, P ∗
ki))) + Cunif
ij (P ∗
ij −(P ∗
ij −min(P ∗
ij, P ∗
ki)))"
REFERENCES,0.9455040871934605,"+ Cunif
ki (P ∗
ki −(P ∗
ki −min(P ∗
ij, P ∗
ki))) + Cunif
kj (P ∗
kj −P ∗
kj + min(P ∗
ij, P ∗
ki)))"
REFERENCES,0.9482288828337875,"= (4 −Cunif
kj ) min(P ∗
ij, P ∗
ki)"
REFERENCES,0.9509536784741145,"> 0.
The ﬁrst equation holds from the deﬁnition of Q, and the second equation holds because Cunif
ii
= 0
and Cunif
ij
= Cunif
jk = 2, and the last inequality holds because Cunif
kl
≤2 and min(P ∗
ij, P ∗
ki) > 0. This
contradicts with the optimality of P ∗. Therefore,
P ∗
ii = min(xi, x′
i)
holds. Therefore,
X"
REFERENCES,0.9536784741144414,"st
Cunif
st P ∗
st =
X"
REFERENCES,0.9564032697547684,"st
2(11⊤−diag(1))stP ∗
st = 2
X"
REFERENCES,0.9591280653950953,"st
P ∗
st −2
X"
REFERENCES,0.9618528610354223,"s
P ∗
ss"
REFERENCES,0.9645776566757494,"= 2 −2
X"
REFERENCES,0.9673024523160763,"s
min(xs, x′
s) =
X"
REFERENCES,0.9700272479564033,"s
xs +
X"
REFERENCES,0.9727520435967303,"s
x′
s −
X"
REFERENCES,0.9754768392370572,"s
min(xs, x′
s) −
X"
REFERENCES,0.9782016348773842,"s
min(xs, x′
s) =
X"
REFERENCES,0.9809264305177112,"s
(xs −min(xs, x′
s)) + (x′
s −min(xs, x′
s)) =
X"
REFERENCES,0.9836512261580381,"s
|xs −x′
s|"
REFERENCES,0.9863760217983651,= ∥x −x′∥1.
REFERENCES,0.989100817438692,Under review as a conference paper at ICLR 2022
REFERENCES,0.9918256130790191,"F
ADDITIONAL RELATED WORK"
REFERENCES,0.9945504087193461,"We review additional related work, which we could not place in the main text because of the page
limit. First, it has already been known that OT behaves pathologically in high dimensional spaces,
mainly owing to the sample complexity. Dudley (1969) showed that the sample complexity of OT
grows exponentially with respect to the number of dimensions. Weed et al. (2019) alleviated the
bound using other senses of dimensionality, but the dependency is still exponential. Genevay et al.
(2019) showed that the sample complexity of the Sinkhorn Divergences (Genevay et al., 2018; Cuturi,
2013) matches that of MMD. The crucial difference between our work and theirs is that we shed
light on more practical sides, whereas their interests were on the theoretical side. We believe that our
analysis and explanation are insightful, especially for practitioners."
REFERENCES,0.997275204359673,"Secondly, high-dimensional problems have also been studied from computational aspects. Rabin
et al. (2011) and Kolouri et al. (2016) proposed to project embeddings to random one-dimensional
spaces to speedup computation because OT in a one-dimensional space can be solved efﬁciently
(Santambrogio, 2015). Paty & Cuturi (2019) and others (Kolouri et al., 2019; Dhouib et al., 2020;
Petrovich et al., 2020; Lin et al., 2020; Huang et al., 2021) proposed robust variants of OT with
low dimensional projections. Speciﬁcally, their methods adopt the worst distance with respect to
candidate projections. Although we tried in early experiments several projection methods, including
the principal component analysis as in Section 5 and Appendix D, tree slicing (Le et al., 2019), and
rank-based cost matrices instead of actual distance cost matrices (Sato et al., 2020a), we did not ﬁnd
performance improvements. However, exploring these approaches can be one of the promising future
directions with the curse of dimensionality in mind."
