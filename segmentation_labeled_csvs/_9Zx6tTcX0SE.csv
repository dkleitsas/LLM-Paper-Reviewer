Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0035460992907801418,"Humans excel at continually acquiring, consolidating, and retaining information
from an ever-changing environment, whereas artificial neural networks (ANNs)
exhibit catastrophic forgetting. There are considerable differences in the com-
plexity of synapses, the processing of information, and the learning mechanisms in
biological neural networks and their artificial counterpart, which may explain the
mismatch in performance. We consider a biologically plausible framework that
constitutes separate populations of exclusively excitatory and inhibitory neurons
which adhere to Dale’s principle, and the excitatory pyramidal neurons are aug-
mented with dendritic-like structures for context-dependent processing of stimuli.
We then conduct a comprehensive study on the role and interactions of different
mechanisms inspired by the brain including sparse non-overlapping representa-
tions, Hebbian learning, synaptic consolidation, and replay of past activations that
accompanied the learning event. Our study suggests that the employing of mul-
tiple complementary mechanisms in a biologically plausible architecture, similar
to the brain, may be effective in enabling continual learning in ANNs. 1"
ABSTRACT,0.0070921985815602835,TL;DR Keyword
INTRODUCTION,0.010638297872340425,"1
INTRODUCTION"
INTRODUCTION,0.014184397163120567,"The human brain excels at continually learning from a dynamically changing environment whereas
standard artificial neural networks (ANNs) are inherently designed for training from stationary i.i.d.
data. Sequential learning of tasks in continual learning (CL) violates this strong assumption, re-
sulting in catastrophic forgetting. Although ANNs are inspired by biological neurons (Fukushima,
1980), they omit numerous details of design principles and learning mechanisms in the brain. These
fundamental differences may account for the mismatch in performance and behavior."
INTRODUCTION,0.01773049645390071,"Biological neural networks are characterized by considerably more complex synapses and dynamic
context-dependent processing of information. Also, the individual neurons have a specific role.
Each presynaptic neuron has an exclusive excitatory or inhibitory impact on its postsynaptic part-
ners, as postulated by Dale’s principle (Strata et al., 1999). Furthermore, distal dendritic segments
in pyramidal neurons, which comprises the majority of excitatory cells in the neocortex, receive
additional context information and enable context-dependent processing of information. This, in
conjunction with inhibition, allows the network to learn task-specific patterns and avoid catastrophic
forgetting (Yang et al., 2014; Iyer et al., 2021; Barron et al., 2017). Furthermore, the replay of non-
overlapping and sparse neural activities of previous experiences in the neocortex and hippocampus
is considered to play a critical role in memory formation, consolidation, and retrieval (Walker &
Stickgold, 2004; McClelland et al., 1995). To protect information from erasure, the brain employs
synaptic consolidation in which plasticity rates are selectively reduced in proportion to strengthened
synapses (Cichon & Gan, 2015)."
INTRODUCTION,0.02127659574468085,"Thus, we study the role and interactions of different mechanisms inspired by the brain in a bio-
logically plausible framework in a CL setup. The underlying model constitutes separate popula-"
INTRODUCTION,0.024822695035460994,1We will make the code available upon acceptance.
INTRODUCTION,0.028368794326241134,Under review as a conference paper at ICLR 2023
INTRODUCTION,0.031914893617021274,Dropout
INTRODUCTION,0.03546099290780142,Context k-WTA Max
INTRODUCTION,0.03900709219858156,"Hebbian
Update"
INTRODUCTION,0.0425531914893617,Dendrite Segments  
INTRODUCTION,0.04609929078014184,inactive pyramidal cell
INTRODUCTION,0.04964539007092199,active pyramidal cell
INTRODUCTION,0.05319148936170213,inhibitory cell
INTRODUCTION,0.05673758865248227,fully connected (b) (a)
INTRODUCTION,0.06028368794326241,learnable weights
INTRODUCTION,0.06382978723404255,context vector
INTRODUCTION,0.0673758865248227,"Figure 1: Architecture of one hidden layer in the biologically plausible framework. Each layer con-
sists of separate populations of exclusively excitatory pyramidal cells and inhibitory neurons which
adheres to Dale’s principle. The shade indicates the strength of weights or activations where darker
shade indicating higher value. (a) The pyramidal cells are augmented with dendritic segments which
receive an additional context signal c and the dendrite segment whose weights are most aligned with
the context vector (bottom row) is selected to modulate the output activity of the feedforward neu-
rons for context-dependent processing of information. (b) The Hebbian update step further strength-
ens the association between the context and the winning dendritic segment with maximum absolute
value (indicated with darker shade for bottom row). Finally, Heterogeneous dropout keeps the ac-
tivation count of each pyramidal cell (indicated with the gray shade) and drops the neurons which
were most active for the previous task (darkest shade dropped) to enforce non-overlapping represen-
tations. The top-k remaining cells then project to the next layer (increased shade)."
INTRODUCTION,0.07092198581560284,"tions of exclusively excitatory and inhibitory neurons in each layer which adheres to Dale’s princi-
ple (Cornford et al., 2020) and the excitatory neurons (mimicking pyramidal cells) are augmented
with dendrite-like structures for context-dependent processing of information (Iyer et al., 2021).
Dendritic segments process an additional context signal encoding task information and subsequently
modulate the feedforward activity of the excitatory neuron (Figure 1). We then systematically study
the effect of controlling the overlap in representations, employing the “fire together, wire together""
learning paradigm and employing experience replay and synaptic consolidation. Our empirical study
shows that:"
INTRODUCTION,0.07446808510638298,"i. An ANN architecture equipped with context-dependent processing of information by den-
drites and separate populations of excitatory pyramidal and inhibitory neurons adhering to
Dale’s principle can learn effectively in CL setup.
ii. Enforcing different levels of activation sparsity in the hidden layers using k-winner-take-all
activations and employing a complementary dropout mechanism that encourages the model
to use a different set of active neurons for each task can effectively control the overlap in
representations, and hence reduce interference.
iii. Task similarities need to be considered when enforcing such constraints to allow for a balance
between forwarding transfer and interference.
iv. Mimicking the ubiquitous “fire together, wire together"" learning rule in the brain through a
Hebbian update step on the connection between context signal and dendritic segments, which
further strengthens context gating and facilitates the formation of task-specific subnetworks.
v. Synaptic consolidation by utilizing Synaptic Intelligence (Zenke et al., 2017) with importance
measures adjusted to take into account the discrepancy in the effect of weight changes in
excitatory and inhibitory neurons further reduces forgetting.
vi. Replaying the activations of previous tasks in a context-specific manner is critical for consol-
idating information across different tasks, especially for the challenging Class-IL setting."
INTRODUCTION,0.07801418439716312,Under review as a conference paper at ICLR 2023
INTRODUCTION,0.08156028368794327,"Our study suggests that employing multiple complementary mechanisms in a biologically plausible
architecture, similar to what is believed to exist in the brain, can be effective in enabling CL in
ANNs."
LIT REVIEW,0.0851063829787234,"2
BIOLOGICALLY PLAUSIBLE FRAMEWORK FOR CL"
LIT REVIEW,0.08865248226950355,We provide details of the biologically plausible framework within which we conduct our study.
LIT REVIEW,0.09219858156028368,"2.1
DALE’S PRINCIPLE"
LIT REVIEW,0.09574468085106383,"Biological neural networks differ from their artificial counterparts in the complexity of the synapses
and the role of individual units. In particular, the majority of neurons in the brain adhere to Dale’s
principle, which posits that presynaptic neurons can only have an exclusive excitatory or inhibitory
impact on their postsynaptic partners (Strata et al., 1999). Several studies show that the balanced
dynamics (Murphy & Miller, 2009; Van Vreeswijk & Sompolinsky, 1996) of excitatory and in-
hibitory populations provide functional advantages, including efficient predictive coding (Boerlin
et al., 2013) and pattern learning (Ingrosso & Abbott, 2019). Furthermore, inhibition is hypoth-
esized to play a role in alleviating catastrophic forgetting (Barron et al., 2017). Standard ANNs,
however, lack adherence to Dale’s principle, as neurons contain both positive and negative output
weights, and signs can change while learning."
LIT REVIEW,0.09929078014184398,"Cornford et al. (2020) incorporate Dale’s principle into ANNs (referred to as DANNs), which take
into account the distinct connectivity patterns of excitatory and inhibitory neurons (Tremblay et al.,
2016) and perform comparable to standard ANNs in the benchmark object recognition task. Each
layer l comprises of a separate population of excitatory, hl
e ∈Rne
+ , and inhibitory hl
i ∈Rni
+ neurons,
where ne ≫ni and synaptic weights are strictly non-negative. Similar to biological networks,
while both populations receive excitatory projections from the previous layer (hl−1
e
), only excitatory
neurons project between layers, whereas inhibitory neurons inhibit the activity of excitatory units of
the same layer. Cornford et al. (2020) characterized these properties by three sets of strictly positive
weights: excitatory connections between layers W l
ee ∈Rne×ne
+
, excitatory projection to inhibitory
units W l
ie ∈Rni×ne
+
, and inhibitory projections within the layer W l
ei ∈Rne×ni
+
. The output of the
excitatory units is impacted by the subtractive inhibition from the inhibitory units:"
LIT REVIEW,0.10283687943262411,"zl = (W l
ee −W l
eiW l
ie)hl−1
e
+ bl
(1)"
LIT REVIEW,0.10638297872340426,"where bl ∈Rne is the bias term. Figure 1 shows the interactions and connectivity between excitatory
pyramidal cells (triangle symbol) and inhibitory neurons (denoted by i)."
LIT REVIEW,0.1099290780141844,"We aim to employ DANNs as feedforward neurons to show that they can also learn in a challenging
CL setting and performance comparable to standard ANNs and provide a biologically plausible
framework for further studying the role of inhibition in alleviating catastrophic forgetting."
LIT REVIEW,0.11347517730496454,"2.2
ACTIVE DENDRITES"
LIT REVIEW,0.11702127659574468,"The brain employs specific structures and mechanisms for context-dependent processing and routing
of information. The prefrontal cortex, which plays an important role in cognitive control (Miller &
Cohen, 2001), receives sensory inputs as well as contextual information, which allows it to choose
the most relevant sensory features for the present task to guide actions (Mante et al., 2013; Fuster,
2015; Siegel et al., 2015; Zeng et al., 2019). Of particular interest are pyramidal cells, which rep-
resent the most populous members of the excitatory family of neurons in the brain (Bekkers, 2011).
The dendritic spines in pyramid cells exhibit highly nonlinear integrative properties which are con-
sidered important for learning task-specific patterns (Yang et al., 2014). Pyramidal cells integrate
a range of diverse inputs into multiple independent dendritic segments, whereby contextual inputs
in active dendrites can modulate a neuron’s response, making it more likely to fire. However, stan-
dard ANNs are based on a point neuron model (Lapique, 1907) which is an oversimplified model
of biological computations and lacks the sophisticated nonlinear and context-dependent behavior of
pyramidal cells."
LIT REVIEW,0.12056737588652482,"Iyer et al. (2021) model these integrative properties of dendrites by augmenting each neuron with
a set of dendritic segments. Multiple dendritic segments receive additional contextual information,"
LIT REVIEW,0.12411347517730496,Under review as a conference paper at ICLR 2023
LIT REVIEW,0.1276595744680851,"which is processed using a separate set of weights. The resultant dendritic output modulates the
feedforward activation which is computed by a linear weighted sum of the feedforward inputs. This
computation results in a neuron where the magnitude of the response to a given stimulus is highly
context-dependent. To enable task-specific processing of information, the prototype vector for task
τ is evaluated by taking the element-wise mean of the tasks samples, Dτ at the beginning of the task
and then subsequently provided as context during training."
LIT REVIEW,0.13120567375886524,"cτ =
1
|Dτ| X"
LIT REVIEW,0.1347517730496454,"x∈Dτ
x
(2)"
LIT REVIEW,0.13829787234042554,"During inference, the closest prototype vector to each test sample, x′, is selected as the context using
the Euclidean distance among all task prototypes, C, stored in memory."
LIT REVIEW,0.14184397163120568,"c′ = arg min
cτ
∥x′ −Cτ∥2
(3)"
LIT REVIEW,0.1453900709219858,"Following Iyer et al. (2021), we augment the excitatory units in each layer with dendritic segments
(Figure 1 (a)). The feedforward activity of excitatory units is modulated by the dendritic segments
which receive a context vector. Given the context vector, each dendritic segment j computes uT
j c,
given weight uj ∈Rd and the context vector c ∈Rd where d is the dimensions of the input image.
For excitatory neurons, the dendritic segment with the highest response to the context (maximum
absolute value with the sign retained) is selected to modulate output activity,"
LIT REVIEW,0.14893617021276595,"hl
e = k-WTA(zl × σ(uT
κ c)),
where κ = arg max
j
|uT
j c|
(4)"
LIT REVIEW,0.1524822695035461,"where σ is the sigmoid function (Han & Moraga, 1995) and k-WTA(.) is the k-Winner-Take-All
activation function (Ahmad & Scheinkman, 2019) which propagates only the top k neurons and
sets the rest to zero. This provides us with a biologically plausible framework where, similar to
biological networks, the feedforward neurons adheres to Dale’s principle and the excitatory neurons
mimic the integrative properties of active dendrties for context-dependent processing of stimulus."
IMPLEMENTATION/METHODS,0.15602836879432624,"3
CONTINUAL LEARNING SETTINGS"
IMPLEMENTATION/METHODS,0.1595744680851064,"To study the role of different components inspired by the brain in a biologically plausible NN for
CL and gauge their roles in the performance and characteristics of the model, we conduct all our
experiments under uniform settings. Implementation details and experimental setup are provided
in the Appendix. We evaluate the models on two CL scenarios. Domain incremental learning
(Domain-IL) refers to the CL scenario in which the classes remain the same in subsequent tasks but
the input distribution changes. We consider Rot-MNIST which involves classifying the 10 digits in
each task with each digit rotated by an angle between 0 and 180 degrees and Perm-MNIST which
applies a fixed random permutation to the pixels for each task. Importantly, there are different vari-
ants of Rot-MNIST with varying difficulties. We incrementally rotate the digits to a fixed degree,
i.e. {0, 8, ..., (N-1)*8} for task {τ1, τ2, .., τN} which is substantially more challenging than random
sampling rotations. Importantly, the Rot-MNIST dataset captures the notion of similarity in subse-
quent tasks where the similarity between two tasks is defined by the difference in their degree of
rotation, whereas each task in Perm-MNIST is independent. We also consider the challenging Class
incremental learning (Class-IL) scenario where new classes are added with each subsequent task
and the agent must learn to distinguish not only amongst the classes within the current task but also
across all learned tasks. Seq-MNIST divides the MNIST classification into 5 tasks with 2 classes in
each task."
IMPLEMENTATION/METHODS,0.16312056737588654,"4
EMPIRICAL EVALUATION"
IMPLEMENTATION/METHODS,0.16666666666666666,"To investigate the impact of the different components inspired by the brain, we use the aforemen-
tioned biologically plausible framework and study the effect on the performance and characteristics
of the model."
IMPLEMENTATION/METHODS,0.1702127659574468,Under review as a conference paper at ICLR 2023
IMPLEMENTATION/METHODS,0.17375886524822695,"Table 1: Effect of each component of the biologically plausible framework on different datasets with
varying number of tasks. We first show the effect of utilizing feedforward neurons adhering to Dale’s
principle in conjunction with Active Dendrites to form the framework within which we evaluate
the individual effect of brain-inspired mechanisms before combining them all together (along with
Heterogeneous Dropout) to forge Bio-ANN. For all the experiments, we set the percentage of active
neurons to 5. We provide the average task performance and 1 std of three runs."
IMPLEMENTATION/METHODS,0.1773049645390071,"Method
Rot-MNIST
Perm-MNIST
Seq-MNIST
5
10
20
5
10
20"
IMPLEMENTATION/METHODS,0.18085106382978725,"Active Dendrites
92.72±0.31
71.48±0.60
48.13±0.73
95.53±0.10
94.37±0.26
91.76±0.39
20.06±0.36
+ Dale’s Principle
92.28±0.27
70.78±0.23
48.79±0.27
95.77±0.33
95.06±0.29
92.40±0.38
19.81±0.03"
IMPLEMENTATION/METHODS,0.18439716312056736,"+ Hebbian Update
92.68±0.36
71.42±0.94
49.26±0.58
95.97±0.16
94.96±0.14
92.69±0.19
19.85±0.04
+ SC
93.40±0.86
75.87±1.35
64.78±3.43
96.67±0.23
96.36±0.10
95.61±0.10
20.26±0.56
+ ER
95.15±0.37
90.86±0.52
83.42±0.44
96.75±0.15
96.01±0.14
94.50±0.16
86.88±0.83
+ ER + CR
96.67±0.06
93.85±0.24
89.38±0.16
97.34±0.03
97.03±0.04
96.12±0.04
89.23±0.48"
IMPLEMENTATION/METHODS,0.1879432624113475,"Bio-ANN
96.82±0.14
94.64±0.23
91.32±0.26
97.33±0.04
97.07±0.05
96.51±0.03
89.26±0.42"
IMPLEMENTATION/METHODS,0.19148936170212766,"Table 2: Effect of different levels of sparsity in activations on the performance of the model.
Columns show the ratio of active neurons (k in k-WTA activation), and the rows provide the number
of tasks."
IMPLEMENTATION/METHODS,0.1950354609929078,"k
#Tasks"
IMPLEMENTATION/METHODS,0.19858156028368795,"Rot-MNIST
Perm-MNIST"
IMPLEMENTATION/METHODS,0.20212765957446807,"0.05
0.10
0.20
0.50
0.05
0.10
0.20
0.50"
IMPLEMENTATION/METHODS,0.20567375886524822,"5
92.28±0.27
92.26±0.31
92.79±0.44
92.26±0.65
95.77±0.33
96.32±0.20
90.29±6.07
74.51±13.55
10
70.78±0.23
71.95±1.54
73.32±0.69
71.61±0.76
95.06±0.29
93.45±0.92
72.68±12.83
41.33±6.72
20
48.79±0.27
47.96±1.84
48.65±0.91
47.71±0.91
92.40±0.38
84.28±1.35
63.84±3.45
20.80±0.99"
IMPLEMENTATION/METHODS,0.20921985815602837,"4.1
EFFECT OF INHIBITORY NEURONS"
IMPLEMENTATION/METHODS,0.2127659574468085,"We first study whether feedforward networks with separate populations of excitatory and inhibitory
units can work well in the CL setting. Importantly, we note that when learning a sequence of tasks
with inhibitory neurons, it is beneficial to take into account the disparities in the degree to which
updates to different parameters affect the layer’s output distribution (Cornford et al., 2020) and hence
forgetting. Specifically, as W l
ie and W l
ei affect the output distribution to a higher degree than W l
ee,
we reduce the learning rate for these weights after the first task (see Appendix)."
IMPLEMENTATION/METHODS,0.21631205673758866,"Table 1 shows that models with feedforward neurons adhering to Dale’s principle perform as well
as the standard neurons and can also further mitigate forgetting in some cases. Note that this gain
comes with considerably fewer parameters and context-dependent processing, as we keep the num-
ber of neurons in each layer the same, and only the excitatory neurons (∼90%) are augmented with
dendritic segments. For 20 tasks, Active Dendrite with Dale’s principle reduces the parameters from
∼70M to less than ∼64M parameters. We hypothesize that having separate populations within a
layer enables them to play a specialized role. In particular, inhibitory neurons can selectively inhibit
certain excitatory neurons based on stimulus, which can further facilitate the formation of task-
specific subnetworks and complement the context-dependent processing of information by dendritic
segments."
IMPLEMENTATION/METHODS,0.2198581560283688,"4.2
SPARSE ACTIVATIONS FACILITATE THE FORMATION OF SUBNETWORKS"
IMPLEMENTATION/METHODS,0.22340425531914893,"Neocortical circuits are characterized by high levels of sparsity in neural connectivity and activa-
tions (Barth & Poulet, 2012; Graham & Field, 2006). This is in stark contrast to the dense and
highly entangled connectivity in standard ANNs. Particularly for CL, sparsity provides several ad-
vantages: sparse non-overlapping representations can reduce interference between tasks (Abbasi
et al., 2022; Iyer et al., 2021; Aljundi et al., 2018), can lead to the natural emergence of task-specific
modules (Hadsell et al., 2020), and sparse connectivity can further ensure fewer task-specific pa-
rameters (Mallya et al., 2018)."
IMPLEMENTATION/METHODS,0.22695035460992907,Under review as a conference paper at ICLR 2023
IMPLEMENTATION/METHODS,0.23049645390070922,"1
3
5
7
9
11
13
15
17
19
21
23
25"
IMPLEMENTATION/METHODS,0.23404255319148937,Task 1
IMPLEMENTATION/METHODS,0.2375886524822695,Task 2
IMPLEMENTATION/METHODS,0.24113475177304963,Task 3
IMPLEMENTATION/METHODS,0.24468085106382978,Task 4
IMPLEMENTATION/METHODS,0.24822695035460993,Task 5
IMPLEMENTATION/METHODS,0.25177304964539005,w/o Hetrogeneous Dropout
IMPLEMENTATION/METHODS,0.2553191489361702,"1
3
5
7
9
11
13
15
17
19
21
23
25"
IMPLEMENTATION/METHODS,0.25886524822695034,w/ Hetrogeneous Dropout 0 1 2 3 4 5
IMPLEMENTATION/METHODS,0.2624113475177305,Activation Count (×103)
IMPLEMENTATION/METHODS,0.26595744680851063,"Figure 2: Total activation counts for the test set of each task (y-axis) for a random set of 25 units in
the second hidden layer of the model. Heterogeneous dropout reduces the overlap in activations and
facilitates the formation of task-specific subnetworks."
IMPLEMENTATION/METHODS,0.2695035460992908,"Table 3: Effect of Heterogeneous dropout with increasing ρ values on different datasets with varying
number of tasks."
IMPLEMENTATION/METHODS,0.2730496453900709,"Dataset
# Tasks
w/o
Dropout parameter (ρ)"
IMPLEMENTATION/METHODS,0.2765957446808511,"Dropout
0.1
0.3
0.5
0.7
1.0"
IMPLEMENTATION/METHODS,0.2801418439716312,Rot-MNIST
IMPLEMENTATION/METHODS,0.28368794326241137,"5
92.28±0.20
91.79±0.53
92.53±0.11
92.74±0.38
93.19±0.32
93.42±0.25
10
70.78±0.23
71.53±1.07
72.38±1.44
73.63±1.00
74.20±0.78
75.50±0.74
20
48.79±0.27
48.57±0.90
48.91±0.65
49.84±0.59
51.03±0.31
51.11±0.76"
IMPLEMENTATION/METHODS,0.2872340425531915,Perm-MNIST
IMPLEMENTATION/METHODS,0.2907801418439716,"5
95.77±0.33
95.70±0.29
95.97±0.44
96.40±0.28
96.58±0.17
96.48±0.26
10
95.06±0.29
95.23±0.04
95.65±0.20
95.54±0.26
95.74±0.22
95.94±0.24
20
92.40±0.38
92.83±0.42
93.20±0.32
92.82±0.06
93.09±0.47
91.77±0.30"
IMPLEMENTATION/METHODS,0.29432624113475175,"We study the effect of different levels of activation sparsity by varying the ratio of active neurons
in k-winner-take-all (k-WTA) activations (Ahmad & Scheinkman, 2019). Each hidden layer of our
model has a constant sparsity in its connections (randomly 50% of weights are set to 0 at initial-
ization) and propagates only the top-k activations (in Figure 1, k-WTA layer). Table 2 shows that
sparsity plays a critical role in enabling CL in DNNs. Sparsity in activations effectively reduces
interference by reducing the overlap in representations. Interestingly, the stark difference in the
effect of different levels of sparse activations on Rot-MNIST and Perm-MNIST highlights the im-
portance of considering task similarity in the design of CL methods. As the tasks in Perm-MNIST
are independent of each other, having fewer active neurons (5%) enables the network to learn non-
overlapping representations for each task while the high task similarity in Rot-MNIST can benefit
from overlapping representation, which allows for the reusability of features across the tasks. The
number of tasks the agent has to learn also has an effect on the optimal sparsity level. In Appendix,
we show that having different levels of sparsity in different layers can further improve performance.
As the earlier layers learn general features, having a higher ratio of active neurons can enable higher
resuability and forward transfer. For the later layers, a smaller ratio of active neurons can reduce
interference between task-specific features."
IMPLEMENTATION/METHODS,0.2978723404255319,"4.3
HETEROGENEOUS DROPOUT FOR NON-OVERLAPPING ACTIVATIONS AND
SUBNETWORKS"
IMPLEMENTATION/METHODS,0.30141843971631205,"The information in the brain is encoded by the strong activation of a relatively small set of neurons,
forming sparse coding. A different subset of neurons is utilized to represent different types of
stimuli Graham & Field (2006). Furthermore, there is evidence of non-overlapping representations
in the brain. To miimic this, we employ Heterogeneous dropout (Abbasi et al., 2022) which in
conjunction with context-dependent processing of information can effectively reduce the overlap
of representations which leads to less interference between the tasks and thereby less forgetting.
During training, we track the frequency of activations for each neuron in a layer for a given task,
and in the subsequent tasks, the probability of a neuron being dropped is inversely proportional to
its activation counts. This encourages the model to learn the new task by utilizing neurons that have
been less active for previous tasks. Figure 1 shows that neurons that have been more active (darker"
IMPLEMENTATION/METHODS,0.3049645390070922,Under review as a conference paper at ICLR 2023
IMPLEMENTATION/METHODS,0.30851063829787234,"0.0
0.1
0.3
0.5
0.7
1.0
5.0
Heterogeneous Dropout ( ) 8.0 8.5 9.0 9.5 10.0 10.5 11.0"
IMPLEMENTATION/METHODS,0.3120567375886525,Symmetric KL-Div (1e-3)
IMPLEMENTATION/METHODS,0.31560283687943264,Overlap in Representations
IMPLEMENTATION/METHODS,0.3191489361702128,"Figure 3: Effect of dropout ρ on the
overlap between the distributions of
layer two activation counts for each
task in Perm-MNIST with 2 tasks.
Higher ρ reduces the overlap."
IMPLEMENTATION/METHODS,0.32269503546099293,"1
2
3
4
5
Dendrite Segments"
IMPLEMENTATION/METHODS,0.3262411347517731,Task 1
IMPLEMENTATION/METHODS,0.32978723404255317,Task 2
IMPLEMENTATION/METHODS,0.3333333333333333,Task 3
IMPLEMENTATION/METHODS,0.33687943262411346,Task 4
IMPLEMENTATION/METHODS,0.3404255319148936,Task 5
IMPLEMENTATION/METHODS,0.34397163120567376,Context Vector
IMPLEMENTATION/METHODS,0.3475177304964539,"-0.71
1.38
0.33
0.38
0.50"
IMPLEMENTATION/METHODS,0.35106382978723405,"0.79
1.08
-0.55
0.24
-2.15"
IMPLEMENTATION/METHODS,0.3546099290780142,"-0.91
0.37
-2.34
0.70
0.33"
IMPLEMENTATION/METHODS,0.35815602836879434,"-0.33
0.66
-1.53
-0.52
2.49"
IMPLEMENTATION/METHODS,0.3617021276595745,"-0.72
0.13
-0.83
0.34
-1.13"
IMPLEMENTATION/METHODS,0.36524822695035464,w/o Hebbian Update
IMPLEMENTATION/METHODS,0.36879432624113473,"1
2
3
4
5
Dendrite Segments"
IMPLEMENTATION/METHODS,0.3723404255319149,"-0.71
3.55
0.21
0.38
0.26"
IMPLEMENTATION/METHODS,0.375886524822695,"0.79
1.13
-0.56
0.24
-4.97"
IMPLEMENTATION/METHODS,0.37943262411347517,"-0.91
0.42
-5.75
0.70
0.47"
IMPLEMENTATION/METHODS,0.3829787234042553,"-0.33
0.59
-1.64
-0.52
5.77"
IMPLEMENTATION/METHODS,0.38652482269503546,"-0.72
0.10
-0.75
0.34
-2.75"
IMPLEMENTATION/METHODS,0.3900709219858156,w/ Hebbian Update
IMPLEMENTATION/METHODS,0.39361702127659576,"Figure 4: Dendritic segment activations of a unit in layer 1
for the context vectors of each task for a model trained on
Perm-MNIST with 5 tasks. Hebbian update strengthens the
association between the context and the dendritic segments,
increasing the magnitude of the modulating signal."
IMPLEMENTATION/METHODS,0.3971631205673759,"shade) are more likely to be dropped before k-WTA is applied. Specifically, let [al
t]j denote the
activation counter for neuron j in layer l after learning t tasks. For learning task t+1, the probability
of this neuron being retained is given by:"
IMPLEMENTATION/METHODS,0.40070921985815605,"[pl
t+1]j = exp(
−[al
t]j
maxj [al
t]j
ρ)
(5)"
IMPLEMENTATION/METHODS,0.40425531914893614,"where ρ controls the strength of enforcement of non-overlapping representations with larger values
leading to less overlap. This provides us with an efficient mechanism for controlling the degree of
overlap between the representations of different tasks, and hence the degree of forward transfer and
interference based on the task similarities."
IMPLEMENTATION/METHODS,0.4078014184397163,"Table 3 shows that employing Heterogeneous dropout can further improve the performance of the
model. We also analyze the effect of the ρ parameter on the activation counts and the overlap in
the representations. Figure 2 shows that Heterogeneous dropout can facilitate the formation of task-
specific subnetworks and Figure 3 shows the symmetric KL-divergence between the distribution of
activation counts on the test set of Task 1 and Task 2 on the model trained with different ρ values
on Perm-MNIST with two tasks. As we increase the ρ parameter, the activations in each layer
become increasingly dissimilar. Similar to the sparsity in activations, in the Appendix, we show that
having different dropout ρ for each layer (with lower ρ for earlier layers to encourage resuability and
higher ρ for later layers to reduce interference between task representations) can further improve the
performance. Heterogeneous dropout provides a simple mechanism for balancing the reusability
and interference of features depending on the similarity of tasks."
IMPLEMENTATION/METHODS,0.41134751773049644,"4.4
HEBBIAN LEARNING STRENGTHENS CONTEXT GATING"
IMPLEMENTATION/METHODS,0.4148936170212766,"For a biologically plausible ANN, it is important not only to incorporate the design elements of
biological neurons but also the learning mechanisms it employs. Lifetime plasticity in the brain
generally follows the Hebbian principle: a neuron that consistently contributes in making another
neuron fire will build a stronger connection to that neuron (Hebb, 2005)."
IMPLEMENTATION/METHODS,0.41843971631205673,"Therefore, we follow the approach in Flesch et al. (2022) to complement error-based learning with
Hebbian update to strengthen the connections between contextual information and dendritic seg-
ments (Figure 1(b)). Each supervised parameter update with backpropagation is followed by a
Hebbian update step on the dendritic segments to strengthen the connections between the context
input and the corresponding dendritic segment which is activated. To constrain the parameters, we
use Oja’s rule which adds weight decay to Hebbian learning (Oja, 1982),"
IMPLEMENTATION/METHODS,0.4219858156028369,"uκ ←uκ + ηhd(c −duκ)
(6)"
IMPLEMENTATION/METHODS,0.425531914893617,"where ηh is the learning rate, κ is the index of the winning dendrite with weight uκ and modulating
signal d = uT
κ c for the context signal c. Figure 4 shows that the Hebbian update step increases the"
IMPLEMENTATION/METHODS,0.42907801418439717,Under review as a conference paper at ICLR 2023
IMPLEMENTATION/METHODS,0.4326241134751773,"magnitude of the modulating signal from the dendrites on the feedforward activity, which can further
strengthen context-dependent gating and facilitate the formation of task-specific subnetworks. Table
1 shows that this results in a consistent improvement in performance."
IMPLEMENTATION/METHODS,0.43617021276595747,"4.5
SYNAPTIC CONSOLIDATION FURTHER MITIGATES FORGETTING"
IMPLEMENTATION/METHODS,0.4397163120567376,"In addition to their integrative properties, dendrites also play a key role in retaining information and
providing protection against erasure (Cichon & Gan, 2015; Yang et al., 2009). The new spines that
are formed on different sets of dendritic branches in response to learning different tasks are protected
from being eliminated through mediation of synaptic plasticity and structural changes that persist
when learning a new task (Yang et al., 2009)."
IMPLEMENTATION/METHODS,0.4432624113475177,"We employ synaptic consolidation by incorporating Synaptic Intelligence (Zenke et al., 2017) (de-
tails in Appendix) which maintains an importance estimate of each synapse in an online manner
during training and subsequently reduces the plasticity of synapses which are considered important
for learned tasks. Notably, we adjust the importance estimate to account for the disparity in the
degree to which updates to different parameters affect the output of the layer due to the inhibitory
interneuron architecture in the DANN layers (Cornford et al., 2020). The importance estimate of the
excitatory connections to the inhibitory units and the intra-layer inhibitory connections are upscaled
to further penalize changes to these weights. Table 1 shows that employing Synaptic Intelligence
(+SC) in this manner further mitigates forgetting. Particularly for Rot-MNIST with 20 tasks, it
provides considerable performance improvement."
IMPLEMENTATION/METHODS,0.44680851063829785,"4.6
EXPERIENCE REPLAY IS ESSENTIAL FOR ENABLING CL IN CHALLENGING SCENARIOS"
IMPLEMENTATION/METHODS,0.450354609929078,"Replay of past neural activation patterns in the brain is considered to play a critical role in memory
formation, consolidation, and retrieval (Walker & Stickgold, 2004; McClelland et al., 1995). The
replay mechanism in the hippocampus (Kumaran et al., 2016) has inspired a series of rehearsal-based
approaches (Li & Hoiem, 2017; Chaudhry et al., 2018; Lopez-Paz & Ranzato, 2017; Arani et al.,
2022) which have proven to be effective in challenging continual learning scenarios (Farquhar &
Gal, 2018; Hadsell et al., 2020). Therefore, to replay samples from previous tasks, we utilize a small
episodic memory buffer that is maintained through Reservoir sampling (Vitter, 1985). It attempts
to approximately match the distribution of the incoming stream by assigning equal probability to
each new sample to be represented in the buffer. During training, samples from the current task,
(xb, yb) ∼Dτ, are interleaved with memory buffer samples, (xm, ym) ∼M to approximate the
joint distribution of tasks seen so far. Furthermore, to mimic the replay of the activation patterns that
accompanied the learning event in the brain, we also save the output logits, zm, across the training
trajectory and enforce a consistency loss when replaying the samples from the episodic memory.
Concretely, the loss is given by:"
IMPLEMENTATION/METHODS,0.45390070921985815,"L = Lcls(f(xb; θ), yb) + αLcls(f(xm; θ), ym) + β(f(xm; θ) −zm)2
(7)"
IMPLEMENTATION/METHODS,0.4574468085106383,"where f(.; θ) is the model parameterized by θ, Lcls is the standard cross-entropy loss, and α and β
controls the strength of interleaved training and consistency constraint, respectively."
IMPLEMENTATION/METHODS,0.46099290780141844,"Table 1 shows that experience replay (+ER) complements the context-dependent processing of in-
formation and enables the model to learn the joint distribution well in varying challenging settings.
Particularly, the failure of the model to avoid forgetting in the Class-IL setting (Seq-MNIST) without
experience replay suggests that context-dependent processing of information alone does not suffice.
and experience replay might be essential. Adding consistency regularization (+CR) further improves
performance as the model receives additional relational information about the structural similarity
of classes, which facilitates the consolidation of information."
IMPLEMENTATION/METHODS,0.4645390070921986,"4.7
COMBINING THE INDIVIDUAL COMPONENTS"
IMPLEMENTATION/METHODS,0.46808510638297873,"Having shown the individual effect of each of the brain-inspired components in the biologically
plausible framework, here we look at their combined effect. The resultant model is referred to as
Bio-ANN. Table 1 shows that the different components complement each other and consistently
improve the performance of the model. The empirical results suggest that employing multiple com-
plementary components and learning mechanisms, similar to the brain, can be an effective approach
to enable continual learning in ANNs."
IMPLEMENTATION/METHODS,0.4716312056737589,Under review as a conference paper at ICLR 2023
CONCLUSION/DISCUSSION,0.475177304964539,"5
DISCUSSION"
CONCLUSION/DISCUSSION,0.4787234042553192,"We conducted a study on the effect of different brain-inspired mechanisms under a biologically plau-
sible framework in the CL setting. The underlying model incorporates several key components of the
design principles and learning mechanisms in the brain: each layer constitutes separate populations
of exclusively excitatory and inhibitory units, which adheres to Dale’s principle, and the excita-
tory pyramidal neurons are augmented with dendritic segments for context-dependent processing
of information. We first showed that equipped with the integrative properties of dendrites, feedfor-
ward network adhering to the Dale’s principle not only performs as well as standard ANNs but also
provides gains. Then we studied the individual role of different components. We showed that con-
trolling the sparsity in activations using k-WTA activations and Heterogeneous dropout mechanism
that encourages the model to use a different set of neurons for each task is an effective approach
for maintaining a balance between reusability of features and interference, which is critical for en-
abling CL. We further showed that complementing the error-based learning with the “fire together,
wire together"" learning paradigm can further strengthen the association between the context sig-
nal and dendritic segments which process them and facilitate context-dependent gating. To further
mitigate forgetting, we incorporated synaptic consolidation in conjunction with experience replay
and showed their effectiveness in challenging CL settings. Finally, the combined effect of these
components suggests that, similar to the brain, employing multiple complementary mechanisms in
a biologically plausible architecture is an effective approach to enable CL in ANN. It also provides
a framework for further study of the role of inhibition in mitigating catastrophic forgetting."
CONCLUSION/DISCUSSION,0.48226950354609927,"However, there are several limitations and potential avenues for future research. In particular, as
dendritic segments provide an effective mechanism for studying the effect of encoding different in-
formation in the context signal, it provides an interesting research avenue as to what information is
useful for the sequential learning of tasks and the effect of different context signals. Neuroscience
studies suggest that multiple brain regions are involved in processing a stimulus, and while there
is evidence that active dendritic segments receive contextual information that is different from the
input received by the proximal segments, it is unclear what information is encoded in the contex-
tual information and how it is extracted. Here, we used the context signal as in (Iyer et al., 2021),
which aims to encode the identity of the task by taking the average input image of all samples in
the task. Although this approach empirically works well in the Perm-MNIST setting, it is impor-
tant to consider its utility and limitations under different CL settings. Given the specific design of
Perm-MNIST, binary centered digits, and the independent nature of the permutations in each task,
the average input image can provide a good approximation of the applied permutation and hence
efficiently encode the task identity. However, this is not straightforward for Rot-MNIST where the
task similarities are higher and even more challenging for natural images where averaging the in-
put image does not provide a meaningful signal. More importantly, it does not seem biologically
plausible to encode task information alone as the context signal and ignore the similarity of classes
occurring in different tasks. For instance, it seems more reasonable to process slight rotations of the
same digits similarly (as in Rot-MNIST) rather than processing them through different subnetworks.
Ideally, we would want the context signal for different rotations of a digit to be highly similar. It is,
however, quite challenging to design context signals that can capture a wide range of complexities
in sequential learning of tasks. Furthermore, instead of hand engineering the context signal to bias
learning towards certain types of task, an effective approach for learning the context signal in an
end-to-end training framework is an interesting direction for future search."
CONCLUSION/DISCUSSION,0.4858156028368794,"Overall, our study presents a compelling case for incorporating the design principles and learning
machinery of the brain into ANNs and provides credence to the argument that distilling the details of
the learning machinery of the brain can bring us closer to human intelligence (Hassabis et al., 2017;
Hayes et al., 2021). Furthermore, deep learning is increasingly being used in neuroscience research
to model and analyze brain data (Richards et al., 2019). The utility of the model for such research
depends on two critical aspects: the performance of the model and how close the architecture is to
the brain (Cornford et al., 2020; Schrimpf et al., 2020). The biologically plausible framework in our
study incorporates several design components and learning mechanisms of the brain and performs
well in a (continual learning) task that is closer to human learning. Therefore, we believe that
this work can also be useful for the neuroscience community to evaluate and guide computational
neuroscience. Studying the properties of ANNs with higher similarity to the brain may provide
insight into the mechanisms of brain functions. We believe that the fields of artificial intelligence
and neuroscience are intricately intertwined and progress in one can drive the other as well."
CONCLUSION/DISCUSSION,0.48936170212765956,Under review as a conference paper at ICLR 2023
REFERENCES,0.4929078014184397,REFERENCES
REFERENCES,0.49645390070921985,"Ali Abbasi, Parsa Nooralinejad, Vladimir Braverman, Hamed Pirsiavash, and Soheil Kolouri. Spar-
sity and heterogeneous dropout for continual learning in the null space of neural activations. arXiv
preprint arXiv:2203.06514, 2022."
REFERENCES,0.5,"Subutai Ahmad and Luiz Scheinkman. How can we be so dense? the benefits of using highly sparse
representations. arXiv preprint arXiv:1903.11257, 2019."
REFERENCES,0.5035460992907801,"Rahaf Aljundi, Marcus Rohrbach, and Tinne Tuytelaars. Selfless sequential learning. arXiv preprint
arXiv:1806.05421, 2018."
REFERENCES,0.5070921985815603,"Elahe Arani, Fahad Sarfraz, and Bahram Zonooz. Learning fast, learning slow: A general con-
tinual learning method based on complementary learning system.
In International Confer-
ence on Learning Representations, 2022. URL https://openreview.net/forum?id=
uxxFrDwrE7Y."
REFERENCES,0.5106382978723404,"Helen C Barron, Tim P Vogels, Timothy E Behrens, and Mani Ramaswami. Inhibitory engrams in
perception and memory. Proceedings of the National Academy of Sciences, 114(26):6666–6674,
2017."
REFERENCES,0.5141843971631206,"Alison L Barth and James FA Poulet. Experimental evidence for sparse firing in the neocortex.
Trends in neurosciences, 35(6):345–355, 2012."
REFERENCES,0.5177304964539007,"John M Bekkers. Pyramidal neurons. Current biology, 21(24):R975, 2011."
REFERENCES,0.5212765957446809,"Marcus K Benna and Stefano Fusi. Computational principles of synaptic memory consolidation.
Nature neuroscience, 19(12):1697–1706, 2016."
REFERENCES,0.524822695035461,"Martin Boerlin, Christian K Machens, and Sophie Denève. Predictive coding of dynamical variables
in balanced spiking networks. PLoS computational biology, 9(11):e1003258, 2013."
REFERENCES,0.5283687943262412,"Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient
lifelong learning with a-gem. arXiv preprint arXiv:1812.00420, 2018."
REFERENCES,0.5319148936170213,"Joseph Cichon and Wen-Biao Gan. Branch-specific dendritic ca 2+ spikes cause persistent synaptic
plasticity. Nature, 520(7546):180–185, 2015."
REFERENCES,0.5354609929078015,"Jonathan Cornford, Damjan Kalajdzievski, Marco Leite, Amélie Lamarquette, Dimitri Michael
Kullmann, and Blake Aaron Richards. Learning to live with dale’s principle: Anns with sep-
arate excitatory and inhibitory units. In International Conference on Learning Representations,
2020."
REFERENCES,0.5390070921985816,"Sebastian Farquhar and Yarin Gal. Towards robust evaluations of continual learning. arXiv preprint
arXiv:1805.09733, 2018."
REFERENCES,0.5425531914893617,"Timo Flesch, David G Nagy, Andrew Saxe, and Christopher Summerfield. Modelling continual
learning in humans with hebbian context gating and exponentially decaying task signals. arXiv
preprint arXiv:2203.11560, 2022."
REFERENCES,0.5460992907801419,"Peter Foldiak. Sparse coding in the primate cortex. The handbook of brain theory and neural
networks, 2003."
REFERENCES,0.549645390070922,"Kunihiko Fukushima. A self-organizing neural network model for a mechanism of pattern recogni-
tion unaffected by shift in position. Biol. Cybern., 36:193–202, 1980."
REFERENCES,0.5531914893617021,"Joaquin Fuster. The prefrontal cortex. Academic press, 2015."
REFERENCES,0.5567375886524822,"Daniel J Graham and David J Field. Sparse coding in the neocortex. Evolution of nervous systems,
3:181–187, 2006."
REFERENCES,0.5602836879432624,"Raia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu. Embracing change: Continual
learning in deep neural networks. Trends in cognitive sciences, 24(12):1028–1040, 2020."
REFERENCES,0.5638297872340425,Under review as a conference paper at ICLR 2023
REFERENCES,0.5673758865248227,"Jun Han and Claudio Moraga. The influence of the sigmoid function parameters on the speed of
backpropagation learning. In International workshop on artificial neural networks, pp. 195–201.
Springer, 1995."
REFERENCES,0.5709219858156028,"Demis Hassabis,
Dharshan Kumaran,
Christopher Summerfield,
and Matthew Botvinick.
Neuroscience-inspired artificial intelligence. Neuron, 95(2):245–258, 2017."
REFERENCES,0.574468085106383,"Tyler L Hayes, Giri P Krishnan, Maxim Bazhenov, Hava T Siegelmann, Terrence J Sejnowski,
and Christopher Kanan. Replay in deep learning: Current approaches and missing biological
elements. Neural Computation, 33(11):2908–2950, 2021."
REFERENCES,0.5780141843971631,"Donald Olding Hebb. The organization of behavior: A neuropsychological theory. Psychology
Press, 2005."
REFERENCES,0.5815602836879432,"Carl Holmgren, Tibor Harkany, Björn Svennenfors, and Yuri Zilberter. Pyramidal cell communi-
cation within local networks in layer 2/3 of rat neocortex. The Journal of physiology, 551(1):
139–153, 2003."
REFERENCES,0.5851063829787234,"Kevin Lee Hunter, Lawrence Spracklen, and Subutai Ahmad. Two sparsities are better than one:
Unlocking the performance benefits of sparse-sparse networks. arXiv preprint arXiv:2112.13896,
2021."
REFERENCES,0.5886524822695035,"Alessandro Ingrosso and LF Abbott. Training dynamically balanced excitatory-inhibitory networks.
PloS one, 14(8):e0220547, 2019."
REFERENCES,0.5921985815602837,"Abhiram Iyer, Karan Grewal, Akash Velu, Lucas Oliveira Souza, Jeremy Forest, and Subutai Ah-
mad. Avoiding catastrophe: Active dendrites enable multi-task learning in dynamic environments.
arXiv preprint arXiv:2201.00042, 2021."
REFERENCES,0.5957446808510638,"James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the national academy of sciences,
114(13):3521–3526, 2017."
REFERENCES,0.599290780141844,"Dhireesha Kudithipudi, Mario Aguilar-Simon, Jonathan Babb, Maxim Bazhenov, Douglas Black-
iston, Josh Bongard, Andrew P Brna, Suraj Chakravarthi Raja, Nick Cheney, Jeff Clune, et al.
Biological underpinnings for lifelong learning machines. Nature Machine Intelligence, 4(3):196–
210, 2022."
REFERENCES,0.6028368794326241,"Dharshan Kumaran, Demis Hassabis, and James L McClelland. What learning systems do intelligent
agents need? complementary learning systems theory updated. Trends in cognitive sciences, 20
(7):512–534, 2016."
REFERENCES,0.6063829787234043,"Louis Lapique. Recherches quantitatives sur l’excitation electrique des nerfs traitee comme une
polarization. Journal of Physiology and Pathololgy, 9:620–635, 1907."
REFERENCES,0.6099290780141844,"Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis
and machine intelligence, 40(12):2935–2947, 2017."
REFERENCES,0.6134751773049646,"David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In
Advances in neural information processing systems, pp. 6467–6476, 2017."
REFERENCES,0.6170212765957447,"Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multi-
ple tasks by learning to mask weights. In Proceedings of the European Conference on Computer
Vision (ECCV), pp. 67–82, 2018."
REFERENCES,0.6205673758865248,"Valerio Mante, David Sussillo, Krishna V Shenoy, and William T Newsome. Context-dependent
computation by recurrent dynamics in prefrontal cortex. nature, 503(7474):78–84, 2013."
REFERENCES,0.624113475177305,"James L McClelland, Bruce L McNaughton, and Randall C O’Reilly. Why there are complementary
learning systems in the hippocampus and neocortex: insights from the successes and failures of
connectionist models of learning and memory. Psychological review, 102(3):419, 1995."
REFERENCES,0.6276595744680851,Under review as a conference paper at ICLR 2023
REFERENCES,0.6312056737588653,"Earl K Miller and Jonathan D Cohen. An integrative theory of prefrontal cortex function. Annual
review of neuroscience, 24(1):167–202, 2001."
REFERENCES,0.6347517730496454,"Brendan K Murphy and Kenneth D Miller. Balanced amplification: a new mechanism of selective
amplification of neural activity patterns. Neuron, 61(4):635–648, 2009."
REFERENCES,0.6382978723404256,"Erkki Oja. Simplified neuron model as a principal component analyzer. Journal of mathematical
biology, 15(3):267–273, 1982."
REFERENCES,0.6418439716312057,"Blake A Richards, Timothy P Lillicrap, Philippe Beaudoin, Yoshua Bengio, Rafal Bogacz, Amelia
Christensen, Claudia Clopath, Rui Ponte Costa, Archy de Berker, Surya Ganguli, et al. A deep
learning framework for neuroscience. Nature neuroscience, 22(11):1761–1770, 2019."
REFERENCES,0.6453900709219859,"Martin Schrimpf, Jonas Kubilius, Ha Hong, Najib J Majaj, Rishi Rajalingham, Elias B Issa, Ko-
hitij Kar, Pouya Bashivan, Jonathan Prescott-Roy, Franziska Geiger, et al. Brain-score: Which
artificial neural network for object recognition is most brain-like? BioRxiv, pp. 407007, 2020."
REFERENCES,0.648936170212766,"Markus Siegel, Timothy J Buschman, and Earl K Miller. Cortical information flow during flexible
sensorimotor decisions. Science, 348(6241):1352–1355, 2015."
REFERENCES,0.6524822695035462,"Piergiorgio Strata, Robin Harvey, et al. Dale’s principle. Brain research bulletin, 50(5):349–350,
1999."
REFERENCES,0.6560283687943262,"Robin Tremblay, Soohyun Lee, and Bernardo Rudy. Gabaergic interneurons in the neocortex: from
cellular properties to circuits. Neuron, 91(2):260–292, 2016."
REFERENCES,0.6595744680851063,"Carl Van Vreeswijk and Haim Sompolinsky. Chaos in neuronal networks with balanced excitatory
and inhibitory activity. Science, 274(5293):1724–1726, 1996."
REFERENCES,0.6631205673758865,"Jeffrey S Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical Software
(TOMS), 11(1):37–57, 1985."
REFERENCES,0.6666666666666666,"Matthew P Walker and Robert Stickgold. Sleep-dependent learning and memory consolidation.
Neuron, 44(1):121–133, 2004."
REFERENCES,0.6702127659574468,"Guang Yang, Feng Pan, and Wen-Biao Gan. Stably maintained dendritic spines are associated with
lifelong memories. Nature, 462(7275):920–924, 2009."
REFERENCES,0.6737588652482269,"Guang Yang, Cora Sau Wan Lai, Joseph Cichon, Lei Ma, Wei Li, and Wen-Biao Gan. Sleep pro-
motes branch-specific formation of dendritic spines after learning. Science, 344(6188):1173–
1178, 2014."
REFERENCES,0.6773049645390071,"Guanxiong Zeng, Yang Chen, Bo Cui, and Shan Yu. Continual learning of context-dependent pro-
cessing in neural networks. Nature Machine Intelligence, 1(8):364–372, 2019."
REFERENCES,0.6808510638297872,"Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.
In International Conference on Machine Learning, pp. 3987–3995. PMLR, 2017."
OTHER,0.6843971631205674,Under review as a conference paper at ICLR 2023
OTHER,0.6879432624113475,"A
APPENDIX"
OTHER,0.6914893617021277,"A.1
RELATED WORK - BIOLOGICAL INSPIRED AI"
OTHER,0.6950354609929078,"The human brain has long been a source of inspiration for ANNs design (Hassabis et al., 2017;
Kudithipudi et al., 2022). However, we have failed take full advantage of our enhanced understand-
ing of the brain and there are fundamental differences between the design principles and learning
mechanisms employed in the brain and ANNs. These differences may account for the huge gap in
performance and behavior."
OTHER,0.6985815602836879,"From an architecture design perspective, standard ANNs are predominantly based on the point neu-
ron model (Lapique, 1907) which is an outdated and oversimplified model of biological computa-
tions which lacks the sophisticated and context dependent processing in the brain. Furthermore, the
neurons in standard ANNs lack adherence to the Dale’s principle (Strata et al., 1999) to which the
majority of neurons in the brain adhere to. Unlike the brain where presynaptic neurons have an ex-
clusively excitatory or inhibitory impact on their postsynaptic partners, neurons in standard ANNs
contain both positive and negative output weights, and signs can change while learning. These con-
stitute as two of the major fundamental differences in the underlying design principle of ANNs and
the brain."
OTHER,0.7021276595744681,"Two recent studies attempt to address this gap. Cornford et al. (2020) incorporated Dale’s principle
into ANNs (DANNs) in a more biologically plausible manner and show that with certain initializa-
tion and regularization considerations, DANNs can perform comparable to standard ANNs in object
recognition task, which earlier attempts failed to do so. Our study extends DANNs to the more chal-
lenging CL setting and show that accounting for the discrepancy in the effect of weight changes in
excitatory and inhibitory neurons can further reduce forgetting in CL. Iyer et al. (2021) propose an
alternative to the point neuron model and provide an algorithmic abstraction of pyramidal neurons
in the neocortex. Each neuron is augmented with dendritic segments which receive an additional
context signal and the output of the dendrite segment modulates the activity of the neuron, allowing
context dependent processing of information. Our study builds upon their work and provides a bio-
logically plausible architecture characterized with both adherence to dale’s principle and the context
dependent processing of pyramidal neurons. This provides us with a framework to study the role
of brain inspired mechanisms and allow for the studying the role of inhibition in the challenging
continual learning setting which is more closer to human learning."
OTHER,0.7056737588652482,"From learning perspective, several approaches have been inspired by the brain, particularly for
CL (Kudithipudi et al., 2022).The replay mechanism in the hippocampus (Kumaran et al., 2016)
has inspired a series of rehearsal-based approaches (Hayes et al., 2021; Hadsell et al., 2020) which
have proven to be effective in challenging continual learning scenarios (Farquhar & Gal, 2018). An-
other popular approach for continual learning, regularization-based approaches Zenke et al. (2017);
Kirkpatrick et al. (2017), have been inspired by neurobiological models which suggest that CL in
the neocortex relies on a process of task-specific synaptic consolidation which involves rendering a
proportion of synapses less plastic and therefore stable over long timescales (Benna & Fusi, 2016;
Yang et al., 2009). While both these approaches are inspired by the brain, researches have mostly
discounted the fact that the brain employs both of them in conjunction to consolidate information
rather than in isolation. Therefore, research in both of these methods have been orthogonal. Further-
more they have been applied on top of standard ANNs which are not representative of the complex-
ities of the neuron in the brain. Our study employs replay and synaptic consolidation together in a
more biologically plausible architecture and show that they complement each other to improcve the
performance."
OTHER,0.7092198581560284,"Additionally, our frameworks employs several techniques to mimic the characteristics of the acti-
vations in the brain. As the yyramidal neurons in the neocortex have highly sparse connectivity
to each other (Hunter et al., 2021; Holmgren et al., 2003) and only a small percentage (<2%) of
neurons are active for a given stimuli neurons (Barth & Poulet, 2012), we apply k-winner-take-all
(k-WTA) activations (Ahmad & Scheinkman, 2019) to mimic activation sparsity. Several studies
have shown the benefits of sparsity in CL (Abbasi et al., 2022; Mallya et al., 2018; Aljundi et al.,
2018), they do not consider that the brain not only utilizes sparsity, it does so in an efficient man-
ner to encode information. The information in the brain is encoded by the strong activation of a
relatively small set of neurons, forming sparse coding. A different subset of neurons is utilized to
represent different types of stimuli (Foldiak, 2003) and semantically similar stimuli activate similar"
OTHER,0.7127659574468085,Under review as a conference paper at ICLR 2023
OTHER,0.7163120567375887,"set of neurons.Heterogeneous dropout (Abbasi et al., 2022) coupled with k-WTA activations aim to
mimic these characteristics by encouraging the new task to utilize a new set of neurons for learning.
Finally we argue that that it is important not only to incorporate the design elements of biological
neurons but also the learning mechanisms it employs. Lifetime plasticity in the brain generally fol-
lows the Hebbian principle (Hebb, 2005). Therefore, we follow the approach in Flesch et al. (2022)
to complement error-based learning with Hebbian update to strengthen the connections between
contextual information and dendritic segments and show that it strengthens context gating."
OTHER,0.7198581560283688,"Our study provides a biologically plausible framework with the the underlying architecture with
the context dependent processing of information and adherence to dale’s principle. Additionally it
employs the learning mechanisms (experience replay, synaptic consolidation and hebbian update)
and characteristics (sparse non-overlapping activations and task specific subnetworks) of the brain.
To the best of our knowledge, we are the first to provide a comprehensive study of the integration of
different brain-inspired mechanisms in a biologically plausible architecture in a CL setting."
OTHER,0.723404255319149,"A.2
ADDITIONAL RESULTS"
OTHER,0.7269503546099291,"Additionally, we conducted experiments on Fashion-MNIST which is more challenging than the
MNIST datasets. We considered both the the Class-IL (Seq-FMNIST) and the Domain-IL (Rot-
FMNIST) setting. Seq-FMNIST divides the classification into 5 tasks with 2 classes each while
Rot-FMNIST involves classifying the 10 classes in each task with the samples rotated by i.e. {0, 8,
..., (N-1)*8} for task {τ1, τ2, .., τN}."
OTHER,0.7304964539007093,"For brevity, we refer to Active Dendrites + Dale’s principle as ActiveDANN. To show the effect
of different components better (ActiveDANN without ER fail on the class-IL setting), we consider
ActiveDann + ER as the baseline upon which we add the other components. Empirical results in
Table A.2 show that the findings on the MNIST settings also translate to Fashion-MNIST and each
component leads to performance improvement."
OTHER,0.7340425531914894,"Table 4:
Effect of each component of the biologically plausible framework on different Seq-
FMNIST and Rot-FMNIST. For all experiments, we use a memory budget of 500 samples. HD
refers to heterogeneous dropout. We provide the average task performance and 1 std of 5 runs."
OTHER,0.7375886524822695,"Method
Seq-FMNIST
Rot-FMNIST"
OTHER,0.7411347517730497,"Joint
94.33±0.51
98.15±0.09
SGD
19.83±0.04
51.89±0.27
ActiveDANN + ER
77.56±0.27
80.99±0.53
+ Hebb
78.02±0.38
82.16±0.26
+ SC
78.05±0.61
82.55±0.37
+ HD
78.74±0.38
83.97±0.46
Bio-ANN
79.28±0.42
89.22±0.21"
OTHER,0.7446808510638298,"A.3
EXPERIMENTAL SETUP"
OTHER,0.74822695035461,"To study the role of the different components inspired by the brain in a biologically plausible NN
for CL and gauge their roles in the performance and characteristics of the model, we conduct all
our experiments under uniform settings. Unless otherwise stated, we use a multi-layer perception
(MLP) with two hidden layers with 2048 units and k-WTA activations. Each neuron is augmented
with N dendritic segments where N corresponds to the number of tasks and the dimensions match
the dimensions of the context vector which corresponds to the input image size (784 for all MNIST
based settings). The model is trained using an SGD optimizer with 0.3 learning rate and a batch
size of 128 for 3 epochs on each task. We set the weight sparsity to 0 and set the percentage
of active neurons to 5%. For our experiments involving Dale’s principle, we maintain the same
number of total units in each layer divided into 1844 excitatory and 204 inhibitory units. Only
the excitatory units are augmented with dendritic segments. Importantly, we use the initialization
strategy and corrections for the SGD update as posited in Cornford et al. (2020) to account for the
disparities in the degree to which updates to different parameters affect the layer output distribution.
The inhibitory unit parameters updates are scaled down relative to excitatory parameter update."
OTHER,0.75177304964539,Under review as a conference paper at ICLR 2023
OTHER,0.7553191489361702,"Concretely, the gradient updates to Wie were scaled by √ne
−1 and Wei by d−1, where ne are the
number of excitatory neurons in the layer and d is the input dimension to the layer. Furthermore,
to select the hyperparameters for different settings we use a small validation set. Note, as the goal
was not to achieve the best possible accuracy, rather to show the effect of each component, we did
not conduct an extensive hyperparameter search. Table A.3 provides the selected hyperparameters
for the effect of individual components experiments in Table 1 and Table A.3 provides the selected
hyperparameter for Bio-ANN experiments. We report the mean accuracy over all tasks and 1 std
over three different random seeds."
OTHER,0.7588652482269503,"Table 5: The selected hyperparamneters for the experiments showing the individual effect of each
component (Table 1). The base learning rate for all the experiments is 0.3 and the individual com-
ponents use the same learning rates for Wie and Wei as (+ Dale’s Principle). For + SC experiments,
we use λWie=10 and λWei=10. For ER experiments, we use a memory budget of 500 samples."
OTHER,0.7624113475177305,"Dataset
#Tasks
+ Dale’s Principle
+ Hebbian
+ SC
+ ER
+ ER + CR
Update"
OTHER,0.7659574468085106,Rot-MNIST
OTHER,0.7695035460992907,"5
ηWie=3e-2, ηWei=3e-3
nh=3e-10
λ=0.25
α=1, β=0
α=1, β=0.50
10
ηWie=3e-2, ηWei=3e-3
nh=3e-08
λ=0.25
α=1, β=0
α=1, β=0.50
20
ηWie=3e-3, ηWei=3e-4
nh=3e-10
λ=1.00
α=1, β=0
α=1, β=0.50"
OTHER,0.7730496453900709,Perm-MNIST
OTHER,0.776595744680851,"5
ηWie=3e-2, ηWei=3e-2
nh=3e-09
λ=0.10
α=1, β=0
α=1, β=0.50
10
ηWie=3e-2, ηWei=3e-2
nh=3e-06
λ=0.25
α=1, β=0
α=1, β=0.50
20
ηWie=3e-2, ηWei=3e-3
nh=3e-09
λ=0.10
α=1, β=0
α=1, β=0.50"
OTHER,0.7801418439716312,"Seq-MNIST
5
ηWie=3e-2, ηWei=3e-3
nh=3e-07
λ=0.25
α=1, β=0
α=1, β=0.25"
OTHER,0.7836879432624113,"Table 6: The selected hyperparamneters for Bio-ANN experiments in Table 1. We use the same
learning rate for each setting as + Dale’s Principle (Table A.3)."
OTHER,0.7872340425531915,"Dataset
#Tasks
nh
λ
ρ
α
β"
OTHER,0.7907801418439716,Rot-MNIST
OTHER,0.7943262411347518,"5
3e-8
0.25
0.1
1
0.5
10
3e-8
0.1
0.3
1
0.5
20
3e-8
0.1
0.3
1
0.5"
OTHER,0.7978723404255319,Perm-MNIST
OTHER,0.8014184397163121,"5
3e-6
0.1
0.1
1
0.5
10
3e-8
0.1
0.3
1
0.5
20
3e-8
0.1
0.3
1
0.5"
OTHER,0.8049645390070922,"Seq-MNIST
5
3e-6
0.1
0.1
1
0.25"
OTHER,0.8085106382978723,"A.4
EFFECT OF ADJUSTING FOR THE INHIBITORY WEIGHTS"
OTHER,0.8120567375886525,"The inhibitory interneuron architecture of DANN layers introduces disparities in the degree to which
updates to different parameters affect the layer’s output distribution e.g. if a single element of Wie is
updated, this has an effect on each element of the layer’s output. An inhibitory weight update of δ to
Wie changes the model distribution approximately ne times more than an excitatory weight update
of δ to Wee (Cornford et al., 2020). The effect of these disparities would be even more pronounced
in CL setting as large changes to output distribution when learning a new task can cause more
forgetting of previous tasks. To account for these, we further reduce the learning rate of Wie and
Wei after learning the first task. Table A.4 shows that accounting for the higher effect of inhibitory
neurons can further improve the performance of the model in majority of the settings. It would
be interesting to explore better approaches to account for the aforementioned disparities which are
tailored for CL and consider the effect on forgetting."
OTHER,0.8156028368794326,"A.5
EFFECT OF ADJUSTING FOR SCALING THE SI IMPORTANCE ESTIMATE FOR INHIBITORY
WEIGHTS"
OTHER,0.8191489361702128,"Similar to adjusting the learning rate of inhibitory weights, we check whether scaling up the impor-
tance estimate of inhibitory neurons can further improve the effectiveness of synaptic consolidation"
OTHER,0.8226950354609929,Under review as a conference paper at ICLR 2023
OTHER,0.8262411347517731,"Table 7: Effect of adjusting the learning rates of Wie and Wei at the end of first task on different
datasets with varying number of tasks."
OTHER,0.8297872340425532,"ηW ie
ηW ei
Rot-MNIST
Perm-MNIST"
OTHER,0.8333333333333334,"5
10
20
5
10
20"
OTHER,0.8368794326241135,"3e-1
3e-1
92.12±0.34
70.86±0.44
46.30±1.03
95.78±0.19
94.73±0.36
92.67±0.61
3e-2
3e-2
92.23±0.53
70.23±1.12
47.53±1.79
95.77±0.33
95.06±0.29
91.63±0.39
3e-2
3e-3
92.28±0.27
70.78±0.23
47.32±1.43
95.68±0.14
94.96±0.49
92.40±0.38
3e-3
3e-3
92.34±0.51
71.27±1.69
47.81±1.10
95.70±0.29
94.44±0.70
92.02±0.19
3e-3
3e-4
92.03±0.09
70.79±1.75
48.79±0.27
95.90±0.13
94.19±0.47
91.04±0.34"
OTHER,0.8404255319148937,"in reducing forgetting. Table A.5 shows that scaling the importance estimate in accordance with the
degree to which the inhibitory weights affect the output distribution and hence forgetting further im-
proves the performance in majority of cases, especially for lower number of tasks. This suggest that
regularization methods designed specifically for networks with inhibitory neurons is a promising
research direction."
OTHER,0.8439716312056738,"Table 8: Effect of scaling the importance estimate for Wie and Wei to reduce the parameter shift in
the inhibitory weights on Rot-MNIST and Perm-MNIST datasets with varying number of tasks."
OTHER,0.8475177304964538,"λ
λWie
λWei
Rot-MNIST
Perm-MNIST"
OTHER,0.851063829787234,"5
10
20
5
10
20"
OTHER,0.8546099290780141,"0.1
1
1
92.92±0.22
75.53±1.46
52.16±1.17
96.59±0.17
96.20±0.13
95.69±0.10
10
10
92.77±0.27
74.70±1.05
51.94±1.05
96.57±0.34
96.18±0.18
95.64±0.15
10
100
92.50±0.65
74.67±1.27
52.55±1.12
96.67±0.23
96.26±0.26
95.61±0.10"
OTHER,0.8581560283687943,"0.25
1
1
92.77±0.69
76.30±0.77
55.05±2.47
96.62±0.28
96.07±0.02
95.36±0.24
10
10
93.54±0.79
75.44±0.81
54.93±2.35
96.54±0.22
96.23±0.16
95.03±0.13
10
100
93.40±0.86
75.87±1.35
55.06±1.77
96.65±0.25
96.36±0.10
95.18±0.29"
OTHER,0.8617021276595744,"0.5
1
1
93.23±0.71
75.24±0.62
60.24±2.02
96.22±0.37
95.97±0.06
92.94±0.85
10
10
93.13±0.24
75.95±0.38
59.35±1.53
96.24±0.51
95.81±0.10
92.86±0.87
10
100
92.85±0.42
75.85±1.18
58.94±2.15
96.36±0.42
96.02±0.27
93.48±0.57"
OTHER,0.8652482269503546,"w/o SC
92.28±0.27
70.78±0.23
48.79±0.27
95.77±0.33
95.06±0.29
92.40±0.38"
OTHER,0.8687943262411347,"A.6
LAYERWISE HETEROGENEOUS DROPOUT AND TASK SIMILARITY"
OTHER,0.8723404255319149,"For an effective CL agent, it is important to maintain a balance between the forward transfer and
interference between the tasks. As the earlier layers learn general features, a higher portion of
the features can be reused to learn the new task which can facilitate forward transfer whereas the
later layers learn more task specific features which can cause interference. Heterogeneous dropout
provides us with an efficient mechanism for controlling the degree of overlap between the activations
and hence the features of each layer. Here, we investigate whether having different levels of sparsity
(controlled with the ρ parameter) in different layers can further improve performance. As the earlier
layers learn general features, having higher overlap (smaller ρ) between the set of active neurons
can enable higher resuability and forward transfer. For the later layers, lesser overlap between the
activations (higher ρ) can reduce the interference between task-specific features."
OTHER,0.875886524822695,"To study the effect of heterogeneous dropout in relation with task similarity, we vary the incremental
rotation, θinc, in each subsequent task for Rot-MNIST setting with 5 tasks. The rotation for task
τ is given by (τ −1)θinc. Table A.6 shows the performance of the model for different layerwise
ρ values. Generally, heterogeneous dropout consistently improves the performance of the model,
especially when the task similarity is low. For θinc = 32, it provides ∼25% improvement. As the
task similarity reduces (θinc increases), higher values of ρ are more effective. Furthermore, we see
that having different ρ values for each layer can provide additional gains in performance."
OTHER,0.8794326241134752,Under review as a conference paper at ICLR 2023
OTHER,0.8829787234042553,"Table 9: Effect of layerwise dropout ρ on Rot-MNIST with 5 tasks with varying degrees of incre-
mental rotation (θinc) in each subsequent task. Row 0 shows (ρl1, ρl2) the ρ values for the first and
second hidden layer respectively."
OTHER,0.8865248226950354,"ρl1, ρl2
Task Similarity (θinc)"
OTHER,0.8900709219858156,"2
4
8
16
24
32"
OTHER,0.8936170212765957,"0.1, 0.1
97.60±0.12
96.74±0.16
91.79±0.53
74.99±1.16
63.33±1.15
57.39±2.36
0.1, 0.5
97.77±0.08
97.02±0.11
92.39±0.39
75.56±1.46
64.18±1.79
57.05±2.13
0.5, 0.5
97.88±0.12
97.22±0.11
92.74±0.38
76.73±1.01
64.18±1.42
58.35±0.73
0.5, 1.0
97.88±0.04
97.25±0.11
92.87±0.39
76.87±0.39
64.84±0.65
59.40±2.31
1.0, 1.0
97.89±0.09
97.19±0.09
93.42±0.25
77.48±0.94
66.33±1.62
61.35±1.90
1.0, 2.0
97.68±0.09
97.00±0.23
93.46±0.78
79.07±0.67
68.20±2.34
63.08±0.86
2.0, 2.0
97.42±0.17
97.00±0.11
93.53±0.53
80.03±0.62
69.99±1.97
65.74±1.21
2.0, 5.0
97.39±0.03
96.54±0.15
92.95±0.01
80.55±0.89
73.74±0.21
69.46±2.66
5.0, 5.0
96.86±0.11
96.12±0.08
92.33±0.18
79.53±0.42
72.47±1.23
70.77±2.11"
OTHER,0.8971631205673759,"w/o Dropout
97.72±0.29
96.93±0.40
92.31±0.56
75.67±1.40
63.68±1.36
56.49±2.86"
OTHER,0.900709219858156,"Table 10: Effect of different levels of sparsity in activations (ratio of active neurons, (kl1, kl2) in 1st
and 2nd hidden layer respectively) and connections (ratio of zero weights, SW ) on Rot-MNIST and
Perm-MNNIST with increasing number of tasks. The best performance across the different sparsity
levels for each task is in bold."
OTHER,0.9042553191489362,"#Tasks
SW
Activation Sparsity (kl1, kl2)"
OTHER,0.9078014184397163,"(0.05, 0.05)
(0.1, 0.05)
(0.1, 0.1)
(0.2, 0.1)
(0.2, 0.2)
(0.5, 0.2)
(0.5, 0.5)"
OTHER,0.9113475177304965,Rot-MNIST 5
OTHER,0.9148936170212766,"0.00
92.28±0.27
92.63±0.46
92.26±0.31
92.25±0.71
92.79±0.44
92.51±0.50
92.26±0.65
0.25
91.22±0.58
91.50±0.36
92.33±0.26
91.75±0.30
92.60±0.19
91.23±1.63
91.66±0.46
0.50
90.78±0.11
91.15±0.34
91.25±0.14
91.25±0.92
90.67±0.73
90.75±0.86
90.41±1.48 10"
OTHER,0.9184397163120568,"0.00
70.78±0.23
71.16±0.51
71.95±1.54
72.22±0.63
73.32±0.69
71.89±0.62
71.61±0.76
0.25
70.23±0.76
71.42±0.98
71.84±0.72
73.22±1.34
72.58±0.64
72.23±0.71
71.23±0.84
0.50
69.61±0.50
70.59±0.62
70.94±0.71
72.05±0.34
72.25±1.40
71.37±0.65
70.85±0.67 20"
OTHER,0.9219858156028369,"0.00
48.79±0.27
48.01±0.58
47.96±1.84
48.33±1.23
48.65±0.91
48.19±0.14
47.71±0.91
0.25
47.72±0.83
48.61±0.30
48.41±0.84
48.53±1.77
48.30±0.87
48.29±1.59
47.11±0.44
0.50
46.20±0.26
47.15±1.37
48.02±1.10
48.17±1.42
48.30±1.41
47.66±1.53
47.73±0.51"
OTHER,0.925531914893617,Perm-MNIST 5
OTHER,0.9290780141843972,"0.00
95.77±0.33
95.55±0.27
96.43±0.10
95.85±0.29
90.29±6.07
88.18±8.86
74.51±13.55
0.25
95.45±0.25
95.14±0.27
95.65±0.22
95.75±0.32
93.73±1.29
87.49±2.33
75.97±5.61
0.50
93.95±0.65
94.19±0.41
94.90±0.22
94.22±1.19
94.05±0.81
91.02±3.71
83.94±9.58 10"
OTHER,0.9326241134751773,"0.00
95.06±0.29
94.08±0.95
94.38±0.73
89.54±2.27
78.91±5.26
76.44±7.93
35.86±2.04
0.25
94.51±0.12
93.52±0.01
93.62±0.64
88.58±2.17
84.94±3.58
69.32±6.24
59.53±9.57
0.50
92.12±0.62
90.31±2.01
92.29±0.73
88.48±0.22
82.57±2.40
74.42±2.84
69.31±5.66 20"
OTHER,0.9361702127659575,"0.00
92.40±0.38
89.41±1.73
84.28±1.35
73.29±2.60
63.84±3.45
58.85±5.00
20.80±0.99
0.25
90.75±0.63
89.22±0.94
84.29±2.17
75.82±6.68
67.26±1.22
63.96±1.19
41.60±2.64
0.50
87.31±0.82
84.90±1.99
83.72±0.63
69.62±5.98
66.27±2.74
64.42±2.33
51.92±3.79"
OTHER,0.9397163120567376,"A.7
EFFECT OF SPARSITY"
OTHER,0.9432624113475178,"To further study the effect of different levels of sparsity in activations and connections, we vary
the number of weights set randomly to zero at initialization (SW ∈{0, 0.25, 0.50}) and the ratio
of active neurons (kl1, kl2) in each hidden layer. Table A.7 shows that sparsity in activation plays
a critical role in enabling CL in ANNs. Interestingly, sparsity in connections play a considerable
role in Perm-MNIST with higher levels of active neurons (≥0.2). Furthermore, exploring finer
differences in activation sparsity of different layers may further improve the performance. Similar
to heterogeneous dropout, we show the effect of activation sparsity in relation to the task similarity
in Table A.7. Similar tasks (lower θinc) benefits from higher number of active neurons which can
increase the forward transfer whereas dissimilar tasks (higher θinc) performs better with higher
activation sparsity which can reduce the overlap in representations."
OTHER,0.9468085106382979,Under review as a conference paper at ICLR 2023
OTHER,0.950354609929078,"Table 11: Effect of different levels of activation sparsity on Rot-MNIST with 5 tasks with varying
degrees of incremental rotation (θinc) in each subsequent task. Row 0 shows (kl1, kl2) the ratio of
active neurons in the 1st and 2nd hidden layer, respectively."
OTHER,0.9539007092198581,"(kl1, kl2)
Task Similarity (θinc)"
OTHER,0.9574468085106383,"2
4
8
16
24
32"
OTHER,0.9609929078014184,"0.05, 0.05
97.54±0.06
96.56±0.14
91.95±0.54
75.13±0.83
63.14±0.52
57.01±0.89
0.1, 0.05
97.57±0.32
96.84±0.18
92.49±0.59
76.15±1.28
64.03±1.12
58.56±1.17
0.1, 0.1
97.81±0.08
96.84±0.30
92.28±0.31
76.80±1.20
64.91±1.17
58.62±1.70
0.2, 0.1
97.44±0.74
96.88±0.45
92.47±0.69
75.79±1.26
64.38±1.58
58.34±1.54
0.2, 0.2
97.88±0.11
97.27±0.15
92.79±0.50
76.22±1.46
64.30±1.38
57.22±1.06
0.5, 0.2
97.67±0.64
96.92±0.78
92.61±0.65
75.66±0.95
63.86±0.61
56.11±1.20
0.5, 0.5
97.67±0.55
97.03±0.53
92.29±0.58
74.55±1.01
62.37±0.62
53.13±3.62"
OTHER,0.9645390070921985,Under review as a conference paper at ICLR 2023
OTHER,0.9680851063829787,Algorithm 1 Bio-ANN: A biologically plausible framework for CL.
OTHER,0.9716312056737588,"Input: Data stream D; Learning rates η, ηWie, ηWei; Hebbian learning rate ηh; Heterogeneous
dropout ρ; Synaptic consolidation weights λ, λWie, λWei, γ; Experience replay weights α, β
Initialize:
Model weights θ, Reference weights θc = {}, Task prototypes Cτ = {}
Heterogeneous dropout: Overall activation counts Aτ = 0, Keep probabilities Pτ = 1
Memory buffer M ←−{}
Synaptic Intelligence: ω = 0, Ω= 0
▷Sample task from data stream
1: for Dτ ∈{D1, D2, .., DT } do
▷Task context
2:
Evaluate context vector (Eq. 2):
cτ =
1
|Dτ |
P"
OTHER,0.975177304964539,"x∈Dτ x
3:
Update the set of prototypes:
Cτ ←−{Cτ, cτ}
▷Train on task τ
4:
while Training do
5:
Sample data: (xb, yb) ∼Dτ and (xm, ym, zm) ∼M
▷Task specific loss
6:
Get the model output and activation counts on the current task batch:
zb, ab = F(xb, cτ; θ, Pτ)
# Apply Heterogeneous dropout
7:
Calculate task loss:
Lτ = Lcls(zb, yb)
8:
Update overall activation counts:
Aτ ←−UpdateActivationCounts(at)
▷Experience replay
9:
Infer context for buffer samples (Eq. 3):
cm = arg min
cτ
∥x′ −Cτ∥2"
OTHER,0.9787234042553191,"10:
Get model output on buffer samples:
z = F(xm, cm; θ)
# Disable Heterogeneous dropout
11:
Calculate replay loss:
Ler = αLcls(z, ym) + β(z −zm)2
▷Synaptic regularization
12:
Calculate SI loss:
Lsc = Ωadj(θ −θc)2"
OTHER,0.9822695035460993,"13:
Calculate overall loss and clip the gradient between 0 and 1:
L = Lτ + Ler + Lsc
∇θL = Clip(∇θL, 0, 1)
▷Update Models
14:
SGD update: θ = UpdateModel(∇θL, η, ηWie, ηWei)
15:
Hebbian update on dendritic segments:
U = HebbianStep({cτ, cm}, U)
16:
17:
Update small omega:
ω = ω + η∇2
θL)
▷Update SI parameter
18:
M ←−Reservoir(M, (xb, yb, zb))
▷Update memory buffer (Algorithm 2)"
OTHER,0.9858156028368794,"19:
▷Task Boundary
20:
Update keep Probabilities (Eq 5):
Pτ = exp(
−Aτ
max Aτ ρ)
21:
Update SI Omega and reference weights and reset small omega:
Ω= Ω+
ω
(θ−θc)2+γ
ω = 0
θc = θ
22:
Scale up importance for inhibitory weights
Ωadj = ScaleUpInhib(Ω, λWie, λWei)
return θ"
OTHER,0.9893617021276596,Under review as a conference paper at ICLR 2023
OTHER,0.9929078014184397,Algorithm 2 Reservoir Sampling
OTHER,0.9964539007092199,"Input: Memory Buffer M, Buffer Size B, Number of examples seen so far N, data points
(x, y, z)
1: if B > N then
▷Check if memory is full
2:
M[N] ←−(x, y, z)
3: else
▷Select a sample to replace
4:
n = SampleRandomInteger(min=0, max=N)
5:
if n < B then
6:
M[n] ←−(x, y, z)
return M"
