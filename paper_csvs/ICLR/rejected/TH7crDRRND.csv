Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003067484662576687,"The method of Random Fourier Feature (RFF) has been popular for large-scale
learning, which generates non-linear random features of the data. It has also been
used to construct binary codes via stochastic quantization for efﬁcient information
retrieval. In this paper, we revisit binary hashing from RFF, and propose Sign-
RFF, a new and simple strategy to extract RFF-based binary codes. We show the
locality-sensitivity of SignRFF, and propose a new measure called ranking efﬁ-
ciency to theoretically compare different Locality-Sensitive Hashing (LSH) meth-
ods, which provides a new systematic and uniﬁed framework for LSH comparison.
It suggests that SignRFF should be preferred in the high similarity region. Experi-
ments are conducted to show that SignRFF is consistently better than the previous
RFF-based method, and also outperforms other data-dependent and deep learn-
ing based hashing methods with sufﬁcient number of hash bits. Moreover, the
proposed ranking efﬁciency aligns well with the empirical search performance."
INTRODUCTION,0.006134969325153374,"1
INTRODUCTION"
INTRODUCTION,0.009202453987730062,"Developing efﬁcient machine learning algorithms in large-scale problems has been an important
research topic to deal with massive data. In this paper, we focus on efﬁcient retrieval/search methods,
speciﬁcally, by designing similarity-preserving binary representations of the data. That is, for each
data vector x ∈Rd, we hash it into a length-b binary 0/1 vector h(x) ∈{0, 1}b, where the geometry
of the data should be well preserved in the Hamming space. Searching with binary codes has been
widely applied in many applications, such as large-scale image retrieval (Weiss et al., 2008; Gong &
Lazebnik, 2011; Kulis & Darrell, 2009; He et al., 2013; Lin et al., 2015; Liu et al., 2016; 2017; Song
et al., 2018; Yu et al., 2018; Yan et al., 2021). The beneﬁts are two-fold. Firstly, it may largely reduce
the memory cost for storing massive datasets, especially with high-dimensional data. Secondly, it
can signiﬁcantly speedup the retrieval process. For instance, the binary codes can be used to build
hash tables (e.g., Indyk & Motwani (1998)) for sub-linear time approximate nearest neighbor (ANN)
search. Moreover, in the Hamming space, we can also apply exhaustive search, which is much faster
than computing the Euclidean distances, plus the technique of multi-index hashing (Norouzi et al.,
2012) can further accelerate the exact Hamming search by orders of magnitude."
INTRODUCTION,0.012269938650306749,"In general, binary hashing methods can be categorized into supervised and unsupervised approaches.
In this work, we focus on the unsupervised setting. Locality-Sensitive Hashing (LSH) (Indyk & Mot-
wani, 1998) is one of the early hashing methods leading to binary embedding. The LSH targeting
at cosine similarity (Charikar, 2002), also known as SimHash, generates binary hash codes using
random hyperplanes by taking the sign of data-independent random Gaussian projections. For the
cosine similarity, LSH has strict theoretical guarantee on the approximation error and search efﬁ-
ciency, but typically requires relatively long codes to achieve good performance. Hence, many works
have focused on learning data-dependent short binary hash codes, through different objective func-
tions. Examples include Iterative Quantization (ITQ) (Gong & Lazebnik, 2011), Spectral Hashing
(SpecH) (Weiss et al., 2008) and Binary Reconstruction Embedding (BRE) (Kulis & Darrell, 2009).
Recently, some unsupervised deep learning based methods have been proposed in the computer vi-
sion community, many of which are, to some extent, “task-speciﬁc” for cross-modal/video/image
retrieval, implemented based on some deep models like the autoencoder and VGG-16 (Liu et al.,
2016; Do et al., 2017; Chen et al., 2018; Li et al., 2019; Yang et al., 2019; Hansen et al., 2020; Liu
et al., 2020; Qiu et al., 2021). By taking the advantage of the complicated model structures (e.g.,
CNN layers), these deep methods show promising performance in many image retrieval tasks."
INTRODUCTION,0.015337423312883436,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.018404907975460124,"Compared with the data-dependent methods (including deep methods), LSH has three advantages:"
INTRODUCTION,0.02147239263803681,"• Although the data-dependent procedures can provide improved performance with fewer
binary codes, a known undesirable effect of many of these mechanisms is the performance
degradation when we increase the code length b, as reported in prior literature (Raginsky
& Lazebnik, 2009; Joly & Buisson, 2011). In our experiments, we will show that this may
also be an issue for deep learning based methods. On the other hand, the performance (i.e.,
search accuracy) of the data-independent LSH would keep boosting with larger b. In many
scenarios, it is often the case that only using short codes (e.g., ≤128 bits) cannot achieve
a desirable level of search accuracy for practical purposes. In these cases, one may need to
use longer codes anyway, where LSH could be more favorable.
• LSH is very simple to implement (only with random projections), while data-dependent
methods require additional optimization/training which might be costly. Moreover, large
deep learning models would typically need longer inference time, which might be infeasible
in practice where the query speed is important.
• It is difﬁcult to characterize the properties of the data-dependent methods theoretically,
while LSH enjoys rigorous guarantees on the retrieval/approximation performance."
INTRODUCTION,0.024539877300613498,"Therefore, LSH is still a popular hashing method with great research interest and many practical
applications (Shrivastava & Li, 2014; Qi et al., 2017; Driemel & Silvestri, 2017; Chen et al., 2019;
Zandieh et al., 2020; Lei et al., 2020; Daghaghi et al., 2021)."
LOCALITY-SENSITIVE HASHING FROM RANDOM FOURIER FEATURES,0.027607361963190184,"1.1
LOCALITY-SENSITIVE HASHING FROM RANDOM FOURIER FEATURES"
LOCALITY-SENSITIVE HASHING FROM RANDOM FOURIER FEATURES,0.03067484662576687,"Kernel methods have gained great success in many machine learning tasks (Schölkopf & Smola,
2002; Zhu & Hastie, 2001; Avron et al., 2017; Sun et al., 2018). However, standard kernel methods
require the n × n kernel matrix, which becomes computationally expensive on large-scale datasets
with massive data points. To this end, Rahimi & Recht (2007) proposes the method of random
Fourier feature (RFF), which deﬁnes a feature map that approximates shift-invariant kernels by the
linear inner products. As such, the RFF preserves the “non-linear locality structure” of the data.
This leads to numerous applications in large-scale learning where one trains linear models on RFF
to approximate training non-linear kernel machines (Yang et al., 2012; Chwialkowski et al., 2015;
Sutherland & Schneider, 2015; Avron et al., 2017; Sun et al., 2018; Tompkins & Ramos, 2018)."
LOCALITY-SENSITIVE HASHING FROM RANDOM FOURIER FEATURES,0.03374233128834356,"Given the popularity of RFF, one natural step is to apply it to search problems. Raginsky & Lazebnik
(2009) proposes to construct binary codes from the RFF using stochastic binary quantization. We
call this method “SQ-RFF”, whose convergence (as b →∞) and concentration can be theoretically
characterized. The author showed that SQ-RFF can achieve competitive search accuracy with sufﬁ-
cient b, e.g., b ≥512. Since RFF itself is a widely used tool for large-scale kernel learning, SQ-RFF
could be a convenient/useful tool in practical scenarios where RFF has been generated for training
kernel machines, and the data scientist wants to further use it for efﬁcient near neighbor retrieval."
OUR CONTRIBUTIONS,0.03680981595092025,"1.2
OUR CONTRIBUTIONS"
OUR CONTRIBUTIONS,0.03987730061349693,"Given the beneﬁts of LSH and RFF, we revisit hashing methods for non-linear kernels, and improve
the prior RFF based hashing method. Our approach is named as “SignRFF”. Speciﬁcally,"
OUR CONTRIBUTIONS,0.04294478527607362,"• We theoretically compare several linear and non-linear LSH methods in terms of a novel
measure called the ranking efﬁciency, which is deﬁned based on the probability of retriev-
ing a wrong/reversed similarity ranking. Under this uniﬁed metric, the proposed SignRFF
is uniformly better than SQ-RFF. Moreover, the ranking efﬁciency also indicates that typi-
cally one should prefer SignRFF over linear LSH when the near neighbors are close to each
other, which is the ﬁrst systematic comparison of linear vs. non-linear LSH in literature.
• Empirically, we conduct near neighbor search experiments on benchmark image datasets
to compare the proposed SignRFF with other popular hashing methods, which veriﬁes
the superiority of SignRFF over SQ-RFF. We validate that the ranking efﬁciency metric
aligns well with the empirical results. Moreover, SignRFF outperforms various competing
methods (including deep methods) with moderately large number of hash bits, indicating
its advantage in applications to achieve a high search recall/precision."
OUR CONTRIBUTIONS,0.046012269938650305,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.049079754601226995,"2
PRELIMINARIES"
PRELIMINARIES,0.05214723926380368,"2.1
LOCALITY-SENSITIVE HASHING (LSH)"
PRELIMINARIES,0.05521472392638037,"In large-scale information retrieval, exhaustive search of the exact nearest neighbors is usually too
expensive. A common relaxation in this setting is the Approximate Nearest Neighbor (ANN) search,
where we return the “good” neighbors of a query with high probability. In this paper, we will
consider the search problem with data points in Rd. X denotes the database consisting of n data
points, and q is a query point. x, y are two data points with ρ = cos(x, y)."
PRELIMINARIES,0.05828220858895705,"Deﬁnition 2.1 ( ˜S-neighbor). For a similarity measure S : Rd × Rd 7→R, the ˜S-neighbor set of q
is deﬁned as {x ∈X : S(x, q) > ˜S}."
PRELIMINARIES,0.06134969325153374,"Deﬁnition 2.2 ((c, ˜S)-ANN). Assume δ > 0 is a parameter. An algorithm A is a (c, ˜S)-ANN method
provided the following: with probability at least 1 −δ, for 0 < c < 1, if there exists an ˜S-neighbor
of q in X, A returns a c ˜S-neighbor of q."
PRELIMINARIES,0.06441717791411043,One popular method satisfying Deﬁnition 2.2 is the Locality-Sensitive Hashing (LSH).
PRELIMINARIES,0.06748466257668712,"Deﬁnition 2.3 (Indyk & Motwani (1998)). A family of hash functions H is called ( ˜S, c ˜S, p1, p2)-
locality-sensitive for similarity measure S and 0 < c < 1, if for ∀x, y ∈Rd and hash function
h uniformly chosen from H, it holds that: 1) If S(x, y) ≥˜S, then P(h(x) = h(y)) ≥p1; 2) If
S(x, y) ≤c ˜S, then P(h(x) = h(y)) ≤p2, with p2 < p1."
PRELIMINARIES,0.0705521472392638,"A key intuition of LSH is that, similar data points with have higher chance of hash collision in
the Hamming space. In this paper, we will speciﬁcally consider the LSH for the cosine similar-
ity (Charikar, 2002). For a data vector x ∈Rd, the LSH binary code is given by"
PRELIMINARIES,0.0736196319018405,"LSH:
hLSH(x) = sign(wT x),
(1)"
PRELIMINARIES,0.07668711656441718,"where wT is a standard Gaussian random vector. We use b i.i.d. w1, ..., wb to generate b LSH binary
codes. The collision probability between the codes of two data points x, y is"
PRELIMINARIES,0.07975460122699386,P(hLSH(x) = hLSH(y)) = 1 −cos−1(ρ)
PRELIMINARIES,0.08282208588957055,"π
,
(2)"
PRELIMINARIES,0.08588957055214724,"where ρ = cos(x, y) is their cosine similarity. Note that P(hLSH(x) = hLSH(y)) is increasing in
ρ, which, By Deﬁnition 2.3, is the key to ensure the locality sensitivity."
PRELIMINARIES,0.08895705521472393,"2.2
KERNELIZED LOCALITY-SENSITIVE HASHING (KLSH)"
PRELIMINARIES,0.09202453987730061,"In this paper, we will consider the popular Gaussian kernel function deﬁned for x, y ∈Rd as"
PRELIMINARIES,0.0950920245398773,"k(x, y) = exp
−γ2∥x −y∥2 2 
,"
PRELIMINARIES,0.09815950920245399,"where γ is a hyper-parameter. Let Ψ : Rd 7→F be the feature map with the kernel induced
feature space F. To incorporate non-linearity in LSH, Kulis & Grauman (2009) proposes Kernelized
Locality-Sensitive Hashing (KLSH), by applying LSH (1) in the kernel induced feature space F,
i.e., h(x) = sign(wT Ψ(x)). However, as in many cases (e.g., for the Gaussian kernel) the map Ψ
cannot be explicitly identiﬁed, constructing the random Gaussian projection vector w needs some
careful design. KLSH approximates the random Gaussian distribution using a sufﬁcient number of
data points, by the Central Limit Theorem (CLT) in the Reproducing Kernel Hilbert Space. One
ﬁrst samples m data points from X to form a kernel matrix K, then uniformly picks t points from
[1, ..., m] at random to approximate the Gaussian distribution. The hash code has the form"
PRELIMINARIES,0.10122699386503067,"KLSH:
hKLSH(x) = sign( m
X"
PRELIMINARIES,0.10429447852760736,"i=1
w(i)k(x, xi)),
(3)"
PRELIMINARIES,0.10736196319018405,"where w = K−1/2et, and et ∈{0, 1}m has ones in the entries with indices of the t selected
points. We see that the codes h1, ..., hb are actually dependent, and the quality of using CLT to
approximate Gaussian distribution is not guaranteed, especially in high-dimensional kernel feature
space. Jiang et al. (2015) re-formulates KLSH as applying LSH on the kernel principle components"
PRELIMINARIES,0.11042944785276074,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.11349693251533742,"in the kernel induced feature space, resolving a theoretical concern of KLSH. Notably, since KLSH
uses a pool of data samples to approximate the Gaussian distribution, the hash codes are in fact
dependent in practical implementation. It is observed that the performance of KLSH also drops as
b increases (Joly & Buisson, 2011), similar to the behavior of many data-dependent methods. We
provide more detailed explanation in Appendix C."
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.1165644171779141,"3
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.1196319018404908,"Rahimi & Recht (2007) proposes random Fourier feature (RFF) to alleviate the computational bottle-
neck of standard kernel methods. For a data vector x, the RFF for the Gaussian kernel is deﬁned as
RFF:
F(x) = cos(wT x + τ),
(4)"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.12269938650306748,"where w ∼N(0, γ2Id) and τ ∼Unif(0, 2π) where Id is the identity matrix. It holds that
E[F(x)F(y)] = k(x, y)/2.
That is, the non-linear kernel similarity can be preserved in expectation by the linear inner product
of RFF. To obtain good approximation, we generate b independent RFFs for each data point using
i.i.d. w1, ..., wb and τ1, ..., τb, which can be subsequently used for various learning tasks."
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.12576687116564417,"From now on, we will assume the data vectors are normalized to have unit l2 norm: 1) this is a
standard pre-processing step in practice; and 2) the theoretical guarantees of LSH (1) is only valid
for the cosine similarity (i.e., normalized data). Nevertheless, our analysis can be easily extended
to the general unnormalized case. With normalized data, the Gaussian kernel can be written as a
function of ρ = cos(x, y):
k(ρ) ≡k(x, y) = e−γ2(1−ρ)."
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.12883435582822086,"To extend RFF to efﬁcient search problems, Raginsky & Lazebnik (2009) designs a mapping
[0, 1] 7→{0, 1} for each RFF. For x ∈Rd, the code is produced by stochastic quantization (SQ):"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.13190184049079753,"hSQ(x) = sign(F(x) + ξ) = sign(cos(wT x + τ) + ξ),
(5)
where ξ is a random perturbation from Unif(−1, 1), with two possible choices on the dependency.
In Raginsky & Lazebnik (2009), the same ξ is used for all data points. Another option in litera-
ture is to use i.i.d. ξ for each data point, such that the binary codes admit E[h(x)h(y)] = k(x, y).
For example, it is used in Zhang et al. (2019); Li & Li (2021) for large-scale low-precision train-
ing. We also tested this strategy in our experiments. The search performance is however very poor,
because more variation is introduced. On the other hand, we actually do not need the exactly es-
timation/recovery of the kernel values in search problems. Hence, in this paper we will consider
the baseline using the same ξ for all data samples as in Raginsky & Lazebnik (2009). The collision
probability of the so-called “SQ-RFF” is given as follows.
Theorem 3.1 (Raginsky & Lazebnik (2009)). For the SQ-RFF (5), for ∀x, y ∈Rd, it holds that"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.13496932515337423,"PSQ(ρ) := P(hSQ(x) = hSQ(y)) = 1 −8 π2 ∞
X s=1"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.13803680981595093,1 −e−γ2s2(1−ρ)
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.1411042944785276,"4s2 −1
.
(6)"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.1441717791411043,"This method is locality-sensitive w.r.t. ρ because of the fact that (6) is an increasing function of ρ.
Hence, it is also locality-sensitive w.r.t. the kernel k(ρ) by the monotonicity of k(ρ)."
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.147239263803681,"Proposition 3.2. The SQ-RFF (5) is (˜k, c˜k, PSQ(ρ1), PSQ(ρ2))-locality sensitive w.r.t. similarity
measure k(·), where ρ1 = log(˜k)/γ2 + 1 and ρ2 = log(c˜k)/γ2 + 1."
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.15030674846625766,"In the kernel approximation problem, stochastic rounding introduces higher variance due to the noise
ξ, which hurts the kernel estimation accuracy (Li & Li, 2021). Later, we will see that a similar effect
also presents in search problems. To this end, we propose our simple strategy—directly taking
the sign of the RFF, i.e., applying deterministic quantization. Formally, the so-called “SignRFF”
approach constructs the binary codes for x by"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.15337423312883436,"SignRFF:
hsign(x) = sign(F(x)) = sign(cos(wT x + τ)).
(7)
Operationally, SignRFF is extremely convenient. At the ﬁrst glance, it may appear a bit surprising
that this simple scheme has not been studied in literature. We believe one of the reasons is that the
theoretical correctness of SignRFF is hard to justify. Indeed, with some recent progress on RFF, we
can show that SignRFF is locality-sensitive."
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.15644171779141106,Under review as a conference paper at ICLR 2022
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.15950920245398773,"Lemma 3.3 (Li & Li (2021)). For two normalized data points x, y with cosine ρ, let F(·) be the
RFF deﬁned as (4). The joint distribution of zx = F(x) and zy = F(y) is"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.16257668711656442,"f(zx, zy|ρ) = ∞
P k=−∞"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.1656441717791411,"h
φσ(a∗
x −a∗
y + 2kπ) + φσ(a∗
x + a∗
y + 2kπ)
i π
p"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.1687116564417178,"1 −z2x
q"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.17177914110429449,"1 −z2y
,
(8)"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.17484662576687116,"where a∗
x = cos−1(zx), a∗
y = cos−1(zy), and φσ(·) is the p.d.f. of N(0, σ2) with σ =
p"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.17791411042944785,"2(1 −ρ)γ.
Furthermore, E[sign(F(x))sign(F(y))] is an increasing function of ρ."
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.18098159509202455,"Proposition 3.4. The SignRFF (7) is (˜k, c˜k, Psign(ρ1), Psign(ρ2))-locality sensitive w.r.t. to k(·),
with ρ1 = log(˜k)/γ2 + 1 and ρ2 = log(c˜k)/γ2 + 1, with collision probability"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.18404907975460122,"Psign(ρ) := P(hsign(x) = hsign(y)) = 2
Z 1 0 Z 1"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.18711656441717792,"0
f(zx, zy|ρ)dzxdzy,
(9)"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.1901840490797546,"where f(zx, zy|ρ) is the density function (8)."
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.19325153374233128,"Proof. By Deﬁnition 2.3 and the monotonicity of k(ρ), it sufﬁces to show that the collision
probability, Psign(ρ), is increasing in ρ.
This immediately follows from Lemma 3.3 that
E[sign(F(x))sign(F(y))] = Psign(ρ) −(1 −Psign(ρ)) = 2Psign(ρ) −1 is increasing in ρ."
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.19631901840490798,"4
THEORETICAL COMPARISON OF LSH METHODS: A NEW SCHEME"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.19938650306748465,"In our theoretical analysis, we assume for KLSH (Kulis & Grauman, 2009) that the Gaussian pro-
jection in the kernel induced feature space F is truly random. That is, KLSH “ideally” performs
LSH in the kernel space: hKLSH(x) = sign(wT Ψ(x)) where w is a random Gaussian vector with
proper dimensionality. Since Ψ(x)T Ψ(y) = k(x, y), applying (2) in F we obtain"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.20245398773006135,PKLSH(ρ) := P(hKLSH(x) = hKLSH(y)) = 1 −cos−1(e−γ2(1−ρ))
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.20552147239263804,"π
.
(10)"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.2085889570552147,"Proposition 4.1. KLSH is (˜k, c˜k, 1 −cos−1(˜k)"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.2116564417177914,"π
, 1 −cos−1(c˜k)"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.2147239263803681,"π
)-locality sensitive w.r.t. k(·)."
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.21779141104294478,"Recall Deﬁnition 2.3 of the (˜k, c˜k, p1, p2)-LSH. It is known (Indyk & Motwani, 1998) that one
can construct an LSH data structure with the worst case query time O(nR), where R := log p1"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.22085889570552147,"log p2 is
called the LSH efﬁciency, which has been used in literature to theoretically compare different LSH
methods, e.g. SimHash vs. MinHash (Shrivastava & Li, 2014). However, we should note that the
LSH efﬁciency is only based on a worst case analytical bound. Therefore, it may not well explain
or predict the practical search performance (See Appendix A.1 for related results)."
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.22392638036809817,"We propose a novel measure for evaluating the search accuracy of LSH schemes, namely, the ranking
efﬁciency, that can better compare LSHs in practice. The deﬁnition is motivated by the observation
that regardless of the concrete approach (e.g., building hash tables or exhaustive search), the nearest
neighbor retrieval, to a large extent, essentially boils down to estimating the Euclidean (cosine)
similarity rankings in the Hamming space. Suppose x and y are two points in the database, and q is
a query with ρx = cos(q, x), ρy = cos(q, y). Assume x is the true nearest neighbor of q, implying
ρx > ρy. By the property of LSH, we know that the hash collision probability px > py. For an LSH
hash function h, deﬁne the corresponding collision probability estimators as"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.22699386503067484,"ˆpx = 1 b b
X"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.23006134969325154,"i=1
1{hi(x) = hi(q)},
ˆpy = 1 b b
X"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.2331288343558282,"i=1
1{hi(y) = hi(q)}.
(11)"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.2361963190184049,"Now the problem becomes comparing ˆpx and ˆpy to estimate the true ranking of px and py. We
consider the event of obtaining a wrong similarity comparison from our estimation, i.e. ˆpx ≤ˆpy.
Obviously, a higher probability implies worse search accuracy, as we are more likely to retrieve the
wrong nearest neighbor y. Denote
Ex = E[1{h(x) = h(q)}],
Ey = E[1{h(y) = h(q)}],"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.2392638036809816,"Cov(1{h(x) = h(q)}, 1{h(y) = h(q)}) = Σ =

Vx
Vxy
Vxy
Vy 
."
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.24233128834355827,Under review as a conference paper at ICLR 2022
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.24539877300613497,"By the CLT, we know that as b →∞,

ˆpx
ˆpy"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.24846625766871167,"
∼N(

Ex
Ey"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.25153374233128833,"
, Σ/b). This approximation would be"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.254601226993865,"good with a sufﬁciently large number of b, e.g., b ≥30. In this regime, we can compute"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.25766871165644173,P(ˆpx ≤ˆpy) = P(ˆpx −ˆpy ≤0) = 1 −Φ √
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.2607361963190184,"b(Ex −Ey)
p"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.26380368098159507,Vx + Vy −2Vxy !
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.2668711656441718,",
(12)"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.26993865030674846,"where Φ(·) is the c.d.f. of standard normal distribution. For all the LSH methods studied in this
paper, the Ex and Ey are simply the collision probabilities, and Vx = Ex(1−Ex), Vy = Ey(1−Ey)
from the binomial distribution. Yet, the covariance Vxy is difﬁcult to be analytically computed. For
simplicity, we assume that this term has same relative inﬂuence on the estimators of all approaches,
thus dropping it in the formal deﬁnition of the ranking efﬁciency, which is given as below.
Deﬁnition 4.1 ((ρ, c)-Ranking Efﬁciency). For a LSH method, let the hash collision probability at
cosine ρ and cρ be E and Ec, respectively. Let V = E(1−E), Vc = Ec(1−Ec). The (ρ, c)-ranking
efﬁciency is deﬁned as RE =
E−Ec
√V +Vc ."
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.27300613496932513,"Remark 4.1. The RE can also be deﬁned in terms of the non-linear kernel k(·). We use cosine ρ
here to address both linear and non-linear LSH for generality. In most retrieval applications, we
may concern more about high c (e.g., c = 0.95) which corresponds to similar data points."
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.27607361963190186,"In general, higher RE implies smaller probability of the reversed estimated ranking in (12), which
is more favorable. In Figure 1, we provide the comparison of ranking efﬁciency at different ρ, which
covers the cases in our empirical study (Section 5). We observe some consistent patterns:"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.2791411042944785,"• Non-linear LSH. Firstly, compared with the other RFF-based approach SQ-RFF, the pro-
posed SignRFF is uniformly more efﬁcient at all ρ. In other words, in search tasks, the
stochastic quantization of SQ-RFF introduces more “variation” that lowers the ranking ef-
ﬁciency. Compared with KLSH, SignRFF is more efﬁcient when γ is large (e.g., γ > 2) in
all plots, while KLSH tends to be more efﬁcient with small γ. In general, SignRFF is the
best in terms of ranking efﬁciency.
• When should we prefer non-linear LSH? The ranking efﬁciency gives a novel theoretical
guidance on the appropriate choice between linear and non-linear LSH methods. We see
that, when the target ρ is high (e.g., ρ > 0.8), with proper tuning, kernel methods (Sign-
RFF and KLSH) can be more efﬁcient than linear LSH. However, if the target ρ is small
(e.g., ρ < 0.7), linear LSH might be more favorable. In other words, SignRFF could be
better than LSH on datasets where the near neighbors are close to each other with high
similarity/short distance, which is usually the case for, e.g., image data."
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.2822085889570552,"0
1
2
3
4
5
0 2 4 6 8"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.2852760736196319,ranking efficiency 10-3
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.2883435582822086,"= 0.3 c = 0.95
LSH
SignRFF
KLSH
SQ-RFF"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.29141104294478526,"0
1
2
3
4
5
0 0.5 1 1.5"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.294478527607362,ranking efficiency 10-2
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.29754601226993865,"= 0.5 c = 0.95
LSH
SignRFF
KLSH
SQ-RFF"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.3006134969325153,"0
1
2
3
4
5
0 1 2"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.30368098159509205,ranking efficiency 10-2
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.3067484662576687,= 0.7 c = 0.95
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.3098159509202454,"LSH
SignRFF
KLSH
SQ-RFF"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.3128834355828221,"0
1
2
3
4
5
0 1 2 3 4"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.3159509202453988,ranking efficiency 10-2
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.31901840490797545,= 0.8 c = 0.95
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.3220858895705521,"LSH
SignRFF
KLSH
SQ-RFF"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.32515337423312884,"0
1
2
3
4
5
0 2 4 6 8"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.3282208588957055,ranking efficiency 10-2
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.3312883435582822,= 0.9 c = 0.95
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.3343558282208589,"LSH
SignRFF
KLSH
SQ-RFF"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.3374233128834356,"0
1
2
3
4
5
0 0.5 1 1.5"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.34049079754601225,ranking efficiency 10-1
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.34355828220858897,= 0.95 c = 0.95
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.34662576687116564,"LSH
SignRFF
KLSH
SQ-RFF"
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.3496932515337423,"Figure 1: Ranking efﬁciency (Deﬁnition 4.1) of different LSH methods with various ρ, c = 0.95."
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.35276073619631904,Under review as a conference paper at ICLR 2022
LOCALITY-SENSITIVE BINARY CODES FROM RANDOM FEATURES,0.3558282208588957,"The ranking efﬁciency measures the search accuracy for given target ρ value. In practice, a dataset
contains many different pairs (with different ρ). Our experiments on real datasets in the next section
show that the performance is largely consistent with the prediction based on the “average ρ” value."
EXPERIMENTS,0.3588957055214724,"5
EXPERIMENTS"
EXPERIMENTS,0.3619631901840491,"In this section, we conduct image retrieval experiments on benchmark datasets to demonstrate the
effectiveness of our approach and justify the theoretical comparison of ranking efﬁciency."
EXPERIMENTS,0.36503067484662577,"5.1
DATASETS, METHODS AND EVALUATION"
EXPERIMENTS,0.36809815950920244,"100
101
102
103"
EXPERIMENTS,0.37116564417177916,top similarity 0.6 0.7 0.8 0.9 1 MNIST CIFAR SIFT
EXPERIMENTS,0.37423312883435583,CIFAR-VGG
EXPERIMENTS,0.3773006134969325,Figure 2: Average ρ to N-th neighbor.
EXPERIMENTS,0.3803680981595092,"Datasets.
We use three popular benchmark datasets
in image retrieval literature. The SIFT1M dataset (Jé-
gou et al., 2011) contains 1M 128-dimensional SIFT
image features, and 1000 query samples. The MNIST
dataset (LeCun, 1998) contains 60000 hand-written dig-
its. We randomly choose 1000 samples from the test set as
the query points. The CIFAR dataset (Krizhevsky, 2009)
contains 50000 natural images. We use the gray-scale im-
ages in our experiments, and randomly select 1000 test
samples as the queries. In addition, when comparing our
methods with VGG-16 (Simonyan & Zisserman, 2015)
based deep methods, following prior literature (e.g.,Yang
et al. (2019); Qiu et al. (2021)), we use the 4096-d fea-
tures from the last fully connected layer of the pre-trained
VGG-16 network as the input data for shallow methods for fairness. This dataset is called “CIFAR-
VGG”. On all datasets, the data points are normalized to have unit norm. In Figure 2, we report the
average cosine similarity between queries to their N-nearest neighbor, from N = 1 to 1000."
EXPERIMENTS,0.3834355828220859,"Methods. We compare the following unsupervised hashing methods: 1) LSH (Charikar, 2002); 2)
Iterative Qauntization (ITQ) (Gong & Lazebnik, 2011); 3) Spectral Hashing (SpecH) (Weiss et al.,
2008); 4) Binary Reconstruction Embedding (BRE) with 1000 random samples for model training
as suggested by Kulis & Darrell (2009); 5) KLSH with m = 500 random samples for formulating
the kernel matrix and t = 50 samples for the CLT Gaussian approximation, which should be more
accurate than (300, 30) as recommended in Kulis & Grauman (2009); 6) SQ-RFF (Raginsky &
Lazebnik, 2009); 7) Our proposed SignRFF. For kernel LSH methods 5) - 7), we tune the parameter
γ over 0.1 ∼5 and report the best γ with highest average recall over multiple b = 32 ∼1024. For
each tested method, we generate binary codes and perform exhaustive search in the Hamming space."
EXPERIMENTS,0.38650306748466257,"Evaluation. For each query point, the ground truth nearest neighbors are set by ranking the top N =
100 smallest Euclidean distance (top-100 largest cosine similarity). After running each algorithm,
the Hamming ranking returns R estimated nearest neighbors to each query. We report the average
recall over 1000 queries. Note that, recall@N is equivalent to precision@N in our setting. See
Appendix A.2 for more detailed explanation and additional results on the retrieval precision."
RESULTS,0.3895705521472393,"5.2
RESULTS"
RESULTS,0.39263803680981596,"In Figure 3, we report the recall@N (precision@N) against the number of binary codes b:"
RESULTS,0.39570552147239263,"• On SIFT and MNIST, data-dependent methods (ITQ, SpecH, BRE) perform well with low
bits, but the recall does not improve much after b > 100 ∼200. Yet, this low bit region
does not help much in practice, since the recall level is too low (e.g. < 0.3 on SIFT and
CIFAR) to be satisfactory for real-world tasks. As we mentioned, this is a known limitation
of these methods. When b ≥256, LSH-type methods start to dominate, as expected. On
CIFAR, SignRFF and KLSH outperform the data-dependent methods even with low bits.
• On all three datasets, SignRFF is substantially better than SQ-RFF with all b. Due to
dependence, KLSH has higher recall than SignRFF when b is small (e.g., ≤256), but is
beaten by SignRFF with more bits. The gap is signiﬁcant and consistent when b is as large
as 512, where SignRFF achieves the highest recall on all three datasets."
RESULTS,0.3987730061349693,Under review as a conference paper at ICLR 2022
RESULTS,0.401840490797546,"0
500
1000
bits 0 0.2 0.4 0.6"
RESULTS,0.4049079754601227,recall@100 SIFT
RESULTS,0.40797546012269936,"LSH
ITQ
SpecH
BRE
SQ-RFF
KLSH
SignRFF"
RESULTS,0.4110429447852761,"0
500
1000
bits 0 0.2 0.4 0.6"
RESULTS,0.41411042944785276,recall@100 CIFAR
RESULTS,0.4171779141104294,"LSH
ITQ
SpecH
BRE
SQ-RFF
KLSH
SignRFF"
RESULTS,0.42024539877300615,"0
500
1000
bits 0 0.2 0.4 0.6 0.8"
RESULTS,0.4233128834355828,recall@100 MNIST
RESULTS,0.4263803680981595,"LSH
ITQ
SpecH
BRE
SQ-RFF
KLSH
SignRFF"
RESULTS,0.4294478527607362,"Figure 3: Recall vs. b. “recall@100” is the recall evaluated on the top-100 retrieved neighbors. Note
that in our case, recall@100 is equivalent to precision@100."
RESULTS,0.4325153374233129,"0
500
1000
# of retrieved neighbors 0 0.2 0.4 0.6 0.8 1"
RESULTS,0.43558282208588955,recall SIFT
RESULTS,0.4386503067484663,"LSH
ITQ
SpecH
BRE
SQ-RFF
KLSH
SignRFF"
RESULTS,0.44171779141104295,"0
500
1000
# of retrieved neighbors 0 0.2 0.4 0.6 0.8 1"
RESULTS,0.4447852760736196,recall CIFAR
RESULTS,0.44785276073619634,"LSH
ITQ
SpecH
BRE
SQ-RFF
KLSH
SignRFF"
RESULTS,0.450920245398773,"0
500
1000
# of retrieved neighbors 0.2 0.4 0.6 0.8 1"
RESULTS,0.4539877300613497,recall MNIST
RESULTS,0.4570552147239264,"LSH
ITQ
SpecH
BRE
SQ-RFF
KLSH
SignRFF"
RESULTS,0.4601226993865031,"0
500
1000
# of retrieved neighbors 0 0.2 0.4 0.6 0.8 1"
RESULTS,0.46319018404907975,recall SIFT
RESULTS,0.4662576687116564,"LSH
ITQ
SpecH
BRE
SQ-RFF
KLSH
SignRFF"
RESULTS,0.46932515337423314,"0
500
1000
# of retrieved neighbors 0 0.2 0.4 0.6 0.8 1"
RESULTS,0.4723926380368098,recall CIFAR
RESULTS,0.4754601226993865,"LSH
ITQ
SpecH
BRE
SQ-RFF
KLSH
SignRFF"
RESULTS,0.4785276073619632,"0
500
1000
# of retrieved neighbors 0.2 0.4 0.6 0.8 1"
RESULTS,0.4815950920245399,recall MNIST
RESULTS,0.48466257668711654,"LSH
ITQ
SpecH
BRE
SQ-RFF
KLSH
SignRFF"
RESULTS,0.48773006134969327,Figure 4: Recall vs. number of retrieved neighbors. 1st row: b = 512. 2nd row: b = 1024.
RESULTS,0.49079754601226994,"In Figure 4, we present the recall versus the number of retrieved neighbors, with b = 512 and
b = 1024. SignRFF performs the best among all competing methods on SIFT and CIFAR. On
MNIST, SignRFF performs similarly to LSH and better than the other methods with sufﬁcient b."
RESULTS,0.4938650306748466,"Ranking efﬁciency. In Figure 5, we additionally provide the recall against the Gaussian parameter
γ, to justify the effect of ranking efﬁciency discussed in Section 4. In Figure 1, the theoretical
comparison suggests that KLSH would perform better with small γ, while SignRFF is more efﬁcient
larger γ, which matches the empirical evidence (e.g., 1 vs. 2.5 on SIFT). Moreover, as shown
in Figure 2, the average ρ between each query and its neighbors is around 0.8, 0.9 and 0.95 for
MNIST, SIFT and CIFAR, respectively. Figure 1 predicts that compared with SQ-RFF (blue), LSH
(green) would perform better on MNIST, similarly on SIFT, and worse on CIFAR, which aligns with"
RESULTS,0.49693251533742333,"0.5
1
1.5
2
2.5
3
0.2 0.3 0.4 0.5"
RESULTS,0.5,recall@100 SIFT
RESULTS,0.5030674846625767,"LSH
SQ-RFF
KLSH
SignRFF"
RESULTS,0.5061349693251533,"0
1
2
3
4
5
0.1 0.2 0.3 0.4 0.5"
RESULTS,0.50920245398773,recall@100
RESULTS,0.5122699386503068,"LSH
SQ-RFF
KLSH
SignRFF CIFAR"
RESULTS,0.5153374233128835,"0.5
1
1.5
2
2.5
3
0.3 0.4 0.5 0.6 0.7"
RESULTS,0.5184049079754601,recall@100 MNIST
RESULTS,0.5214723926380368,"LSH
SQ-RFF
KLSH
SignRFF"
RESULTS,0.5245398773006135,"Figure 5: Recall@100 (precision@100) vs. γ, with b = 512."
RESULTS,0.5276073619631901,Under review as a conference paper at ICLR 2022
RESULTS,0.5306748466257669,"Figure 3 and Figure 4. Therefore, although the behavior of KLSH slightly deviates from the theory
due to the dependence among hash codes, our experiments in general verify the effectiveness of the
ranking efﬁciency as an informative measure to compare different LSH methods."
COMPARISON WITH DEEP HASHING,0.5337423312883436,"5.3
COMPARISON WITH DEEP HASHING"
COMPARISON WITH DEEP HASHING,0.5368098159509203,"We provide additional experiments on the CIFAR-VGG dataset with a recent unsupervised deep
hashing method for image retrieval, the Contrastive Information Bottleneck (CIB) (Qiu et al., 2021),
which uses VGG-16 pre-trained model as the backbone. We apply the same training setting as
in Qiu et al. (2021): 60 epochs, 64 mini-batch size and Adam optimizer with 0.001 learning rate.
We note that, CIB (as well as many other deep methods) is not designed to ﬁnd the most similar data
points. Instead, by using techniques like cropping and rotation in training CNN, these methods aim
at robustly ﬁnding data points with same label as the query, which does not necessarily imply high
similarity with the query when represented by the fc7 layer of VGG-16. Hence, to favor CIB, in this
experiment we expand the range of true neighbors of a query to the top-1000 similar points in the
database. See Appendix B for examples of the retrieved images and more discussion."
COMPARISON WITH DEEP HASHING,0.5398773006134969,"• From Figure 6, we see that CIB performs the best with 32 and 64 bits than LSH methods,
which illustrates the beneﬁt of deep hashing with short codes. However, the recall is only
0.3∼0.4, and does not improve with more bits, since simply increasing the size of last layer
in the deep net typically would not elevate the model performance signiﬁcantly. When
b ≥256, LSH, SignRFF and KLSH provide much higher recall than CIB."
COMPARISON WITH DEEP HASHING,0.5429447852760736,"• Among LSHs, KLSH performs the best on this task. SignRFF again consistently improves
SQ-RFF, and matches the recall of KLSH with b = 1024. In addition, linear LSH slightly
outperforms SignRFF, which is consistent with the theoretical comparison in Figure 1 as
expected (the target ρ is ∼0.7 for this task, where LSH is more efﬁcient than SignRFF)."
COMPARISON WITH DEEP HASHING,0.5460122699386503,"0
500
1000
bits 0 0.2 0.4 0.6 0.8"
COMPARISON WITH DEEP HASHING,0.549079754601227,recall@1000
COMPARISON WITH DEEP HASHING,0.5521472392638037,CIFAR-VGG
COMPARISON WITH DEEP HASHING,0.5552147239263804,"CIB
LSH
SQ-RFF
KLSH
SignRFF"
COMPARISON WITH DEEP HASHING,0.558282208588957,"0
500
1000
# of retrieved neighbors 0 0.2 0.4 0.6 0.8"
COMPARISON WITH DEEP HASHING,0.5613496932515337,recall
COMPARISON WITH DEEP HASHING,0.5644171779141104,CIFAR-VGG
COMPARISON WITH DEEP HASHING,0.5674846625766872,"CIB
LSH
SQ-RFF
KLSH
SignRFF"
COMPARISON WITH DEEP HASHING,0.5705521472392638,"0
500
1000
# of retrieved neighbors 0 0.2 0.4 0.6 0.8"
COMPARISON WITH DEEP HASHING,0.5736196319018405,recall
COMPARISON WITH DEEP HASHING,0.5766871165644172,CIFAR-VGG
COMPARISON WITH DEEP HASHING,0.5797546012269938,"CIB
LSH
SQ-RFF
KLSH
SignRFF"
COMPARISON WITH DEEP HASHING,0.5828220858895705,"Figure 6: Left panel: Recall@1000 (precision@1000) vs. b. Mid & Right panel: Recall vs. #
retrieved points, b = 512, 1024."
CONCLUSION,0.5858895705521472,"6
CONCLUSION"
CONCLUSION,0.588957055214724,"Random Fourier Feature (RFF) is a popular method in large-scale kernel learning, which has also
been used for constructing binary codes for fast non-linear neighbor search (Raginsky & Lazebnik,
2009). In this work, we revisit RFF-based binary hashing, and propose a new strategy named Sign-
RFF. We analyze its locality-sensitive property, and provide a new uniﬁed theoretical framework to
compare different LSH methods, based on a measure called ranking efﬁciency. The metric implies
that, we should prefer SignRFF over the linear LSH in the high similarity region when data points
are close to each other. Empirically, we demonstrate that SignRFF is signiﬁcantly better than the
previous RFF-based approach, SQ-RFF, and outperforms data-dependent and deep hashing meth-
ods when the number of hash bits is moderately large. Moreover, the theoretical comparisons of the
proposed ranking efﬁciency is consistent with the empirical results, suggesting that it could serve as
a good theoretical tool for comparing the search performance of different LSH methods in practice."
CONCLUSION,0.5920245398773006,Under review as a conference paper at ICLR 2022
REFERENCES,0.5950920245398773,REFERENCES
REFERENCES,0.598159509202454,"Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, and Amir
Zandieh. Random fourier features for kernel ridge regression: Approximation bounds and sta-
tistical guarantees. In Proceedings of the 34th International Conference on Machine Learning
(ICML), pp. 253–262, Sydney, Australia, 2017."
REFERENCES,0.6012269938650306,"Moses S. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings on
34th Annual ACM Symposium on Theory of Computing (STOC), pp. 380–388, Montreal, Canada,
2002."
REFERENCES,0.6042944785276073,"Lin Chen, Hossein Esfandiari, Gang Fu, and Vahab S. Mirrokni. Locality-sensitive hashing for f-
divergences: Mutual information loss and beyond. In Advances in Neural Information Processing
Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
December 8-14, 2019, Vancouver, BC, Canada, pp. 10044–10054, 2019."
REFERENCES,0.6073619631901841,"Zhixiang Chen, Xin Yuan, Jiwen Lu, Qi Tian, and Jie Zhou. Deep hashing via discrepancy mini-
mization. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018,
Salt Lake City, UT, USA, June 18-22, 2018, pp. 6838–6847. Computer Vision Foundation / IEEE
Computer Society, 2018."
REFERENCES,0.6104294478527608,"Kacper Chwialkowski, Aaditya Ramdas, Dino Sejdinovic, and Arthur Gretton. Fast two-sample
testing with analytic representations of probability measures. In Advances in Neural Information
Processing Systems (NIPS), pp. 1981–1989, Montreal, Canada, 2015."
REFERENCES,0.6134969325153374,"Shabnam Daghaghi, Tharun Medini, Nicholas Meisburger, Beidi Chen, Mengnan Zhao, and An-
shumali Shrivastava. A tale of two efﬁcient and informative negative sampling distributions. In
Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July
2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 2319–2329.
PMLR, 2021."
REFERENCES,0.6165644171779141,"Thanh-Toan Do, Dang-Khoa Le Tan, Trung T. Pham, and Ngai-Man Cheung. Simultaneous feature
aggregating and hashing for large-scale image search. In 2017 IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 4217–
4226. IEEE Computer Society, 2017."
REFERENCES,0.6196319018404908,"Anne Driemel and Francesco Silvestri.
Locality-sensitive hashing of curves.
arXiv preprint
arXiv:1703.04040, 2017."
REFERENCES,0.6226993865030674,"Yunchao Gong and Svetlana Lazebnik. Iterative quantization: A procrustean approach to learning
binary codes. In The 24th IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2011, Colorado Springs, CO, USA, 20-25 June 2011, pp. 817–824. IEEE Computer Society, 2011."
REFERENCES,0.6257668711656442,"Casper Hansen, Christian Hansen, Jakob Grue Simonsen, Stephen Alstrup, and Christina Lioma.
Unsupervised semantic hashing with pairwise reconstruction. In Proceedings of the 43rd Inter-
national ACM SIGIR conference on research and development in Information Retrieval, SIGIR
2020, Virtual Event, China, July 25-30, 2020, pp. 2009–2012. ACM, 2020."
REFERENCES,0.6288343558282209,"Kaiming He, Fang Wen, and Jian Sun.
K-means hashing: An afﬁnity-preserving quantization
method for learning binary compact codes. In 2013 IEEE Conference on Computer Vision and
Pattern Recognition, Portland, OR, USA, June 23-28, 2013, pp. 2938–2945. IEEE Computer So-
ciety, 2013."
REFERENCES,0.6319018404907976,"Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing the curse
of dimensionality. In Proceedings of the Thirtieth Annual ACM Symposium on the Theory of
Computing (STOC), pp. 604–613, Dallas, TX, 1998."
REFERENCES,0.6349693251533742,"Hervé Jégou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor
search. IEEE Trans. Pattern Anal. Mach. Intell., 33(1):117–128, 2011."
REFERENCES,0.6380368098159509,"Ke Jiang, Qichao Que, and Brian Kulis. Revisiting kernelized locality-sensitive hashing for im-
proved large-scale image retrieval. In IEEE Conference on Computer Vision and Pattern Recog-
nition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pp. 4933–4941. IEEE Computer Society,
2015."
REFERENCES,0.6411042944785276,Under review as a conference paper at ICLR 2022
REFERENCES,0.6441717791411042,"Alexis Joly and Olivier Buisson. Random maximum margin hashing. In The 24th IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2011, Colorado Springs, CO, USA, 20-25
June 2011, pp. 873–880. IEEE Computer Society, 2011."
REFERENCES,0.647239263803681,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, Depart-
ment of Computer Science, University of Toronto, 2009."
REFERENCES,0.6503067484662577,"Brian Kulis and Trevor Darrell. Learning to hash with binary reconstructive embeddings. In Ad-
vances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Infor-
mation Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009, Vancouver,
British Columbia, Canada, pp. 1042–1050. Curran Associates, Inc., 2009."
REFERENCES,0.6533742331288344,"Brian Kulis and Kristen Grauman. Kernelized locality-sensitive hashing for scalable image search.
In ICCV, pp. 2130–2137, 2009."
REFERENCES,0.656441717791411,"Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998."
REFERENCES,0.6595092024539877,"Yifan Lei, Qiang Huang, Mohan S. Kankanhalli, and Anthony K. H. Tung. Locality-sensitive hash-
ing scheme based on longest circular co-substring. In Proceedings of the 2020 International
Conference on Management of Data (SIGMOD), pp. 2589–2599, Online conference [Portland,
OR, USA], 2020."
REFERENCES,0.6625766871165644,"Shuyan Li, Zhixiang Chen, Jiwen Lu, Xiu Li, and Jie Zhou. Neighborhood preserving hashing for
scalable video retrieval. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV
2019, Seoul, Korea (South), October 27 - November 2, 2019, pp. 8211–8220. IEEE, 2019."
REFERENCES,0.6656441717791411,"Xiaoyun Li and Ping Li. Quantization algorithms for random fourier features. In Proceedings of
the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual
Event, volume 139 of Proceedings of Machine Learning Research, pp. 6369–6380. PMLR, 2021."
REFERENCES,0.6687116564417178,"Kevin Lin, Huei-Fang Yang, Jen-Hao Hsiao, and Chu-Song Chen. Deep learning of binary hash
codes for fast image retrieval. In 2015 IEEE Conference on Computer Vision and Pattern Recog-
nition Workshops, CVPR Workshops 2015, Boston, MA, USA, June 7-12, 2015, pp. 27–35. IEEE
Computer Society, 2015."
REFERENCES,0.6717791411042945,"Haomiao Liu, Ruiping Wang, Shiguang Shan, and Xilin Chen. Deep supervised hashing for fast
image retrieval. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 2064–2072. IEEE Computer Society, 2016."
REFERENCES,0.6748466257668712,"Hong Liu, Rongrong Ji, Yongjian Wu, Feiyue Huang, and Baochang Zhang. Cross-modality binary
code learning via fusion similarity hashing. In 2017 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 6345–6353. IEEE
Computer Society, 2017."
REFERENCES,0.6779141104294478,"Song Liu, Shengsheng Qian, Yang Guan, Jiawei Zhan, and Long Ying. Joint-modal distribution-
based similarity hashing for large-scale unsupervised deep cross-modal retrieval. In Proceedings
of the 43rd International ACM SIGIR conference on research and development in Information
Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, pp. 1379–1388. ACM, 2020."
REFERENCES,0.6809815950920245,"Mohammad Norouzi, Ali Punjani, and David J. Fleet. Fast search in hamming space with multi-
index hashing. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, Provi-
dence, RI, USA, June 16-21, 2012, pp. 3108–3115. IEEE Computer Society, 2012."
REFERENCES,0.6840490797546013,"Lianyong Qi, Xuyun Zhang, Wanchun Dou, and Qiang Ni. A distributed locality-sensitive hashing-
based approach for cloud service recommendation from multi-source data. IEEE J. Sel. Areas
Commun., 35(11):2616–2624, 2017."
REFERENCES,0.6871165644171779,"Zexuan Qiu, Qinliang Su, Zijing Ou, Jianxing Yu, and Changyou Chen. Unsupervised hashing with
contrastive information bottleneck. In Proceedings of the Thirtieth International Joint Conference
on Artiﬁcial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pp.
959–965. ijcai.org, 2021."
REFERENCES,0.6901840490797546,Under review as a conference paper at ICLR 2022
REFERENCES,0.6932515337423313,"Maxim Raginsky and Svetlana Lazebnik. Locality-sensitive binary codes from shift-invariant ker-
nels. In Advances in Neural Information Processing Systems (NIPS), pp. 1509–1517, Vancouver,
Canada, 2009."
REFERENCES,0.696319018404908,"Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
Neural Information Processing Systems (NIPS), pp. 1177–1184, Vancouver, Canada, 2007."
REFERENCES,0.6993865030674846,"Bernhard Schölkopf and Alexander J. Smola. Learning with Kernels. The MIT Press, Cambridge,
MA, 2002."
REFERENCES,0.7024539877300614,"Anshumali Shrivastava and Ping Li. In defense of minhash over simhash. In Proceedings of the
Seventeenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pp. 886–
894, Reykjavik, Iceland, 2014."
REFERENCES,0.7055214723926381,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale im-
age recognition. In 3rd International Conference on Learning Representations, ICLR 2015, San
Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015."
REFERENCES,0.7085889570552147,"Jingkuan Song, Tao He, Lianli Gao, Xing Xu, Alan Hanjalic, and Heng Tao Shen. Binary generative
adversarial networks for image retrieval. In Proceedings of the Thirty-Second AAAI Conference on
Artiﬁcial Intelligence, (AAAI-18), the 30th innovative Applications of Artiﬁcial Intelligence (IAAI-
18), and the 8th AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-18),
New Orleans, Louisiana, USA, February 2-7, 2018, pp. 394–401. AAAI Press, 2018."
REFERENCES,0.7116564417177914,"Yitong Sun, Anna C. Gilbert, and Ambuj Tewari. But how does it work in theory? linear SVM
with random features. In Advances in Neural Information Processing Systems (NeurIPS), pp.
3383–3392, Montréal, Canada, 2018."
REFERENCES,0.7147239263803681,"Danica J. Sutherland and Jeff G. Schneider. On the error of random fourier features. In Proceed-
ings of the Thirty-First Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 862–871,
Amsterdam, The Netherlands, 2015."
REFERENCES,0.7177914110429447,"Anthony Tompkins and Fabio Ramos. Fourier feature approximations for periodic kernels in time-
series modelling. In Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence
(AAAI), pp. 4155–4162, New Orleans, LA, 2018."
REFERENCES,0.7208588957055214,"Yair Weiss, Antonio Torralba, and Robert Fergus. Spectral hashing. In NIPS, 2008."
REFERENCES,0.7239263803680982,"Chenggang Yan, Biao Gong, Yuxuan Wei, and Yue Gao. Deep multi-view enhancement hashing for
image retrieval. IEEE Trans. Pattern Anal. Mach. Intell., 43(4):1445–1451, 2021."
REFERENCES,0.7269938650306749,"Erkun Yang, Tongliang Liu, Cheng Deng, Wei Liu, and Dacheng Tao. Distillhash: Unsupervised
deep hashing by distilling data pairs. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 2946–2955, Long Beach, CA, 2019."
REFERENCES,0.7300613496932515,"Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-Hua Zhou. Nyström method
vs random fourier features: A theoretical and empirical comparison.
In Advances in Neural
Information Processing Systems (NIPS), pp. 485–493, Lake Tahoe, NV, 2012."
REFERENCES,0.7331288343558282,"Laihang Yu, Lin Feng, Huibing Wang, Li Li, Yang Liu, and Shenglan Liu. Multi-trend binary
code descriptor: a novel local texture feature descriptor for image retrieval. Signal Image Video
Process., 12(2):247–254, 2018."
REFERENCES,0.7361963190184049,"Amir Zandieh, Navid Nouri, Ameya Velingker, Michael Kapralov, and Ilya P. Razenshteyn. Scaling
up kernel ridge regression via locality sensitive hashing. In Proceedings of th 23rd International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pp. 4088–4097, Online [Palermo,
Sicily, Italy], 2020."
REFERENCES,0.7392638036809815,"Jian Zhang, Avner May, Tri Dao, and Christopher Ré. Low-precision random fourier features for
memory-constrained kernel approximation. In Proceedings of the 22nd International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS), pp. 1264–1274, Naha, Okinawa, Japan, 2019."
REFERENCES,0.7423312883435583,"Ji Zhu and Trevor Hastie. Kernel logistic regression and the import vector machine. In NIPS, pp.
1081–1088, Vancouver, BC, Canada, 2001."
REFERENCES,0.745398773006135,Under review as a conference paper at ICLR 2022
REFERENCES,0.7484662576687117,"A
MORE ANALYTICAL FIGURES AND EXPERIMENTS"
REFERENCES,0.7515337423312883,"A.1
COMPARISON OF LSH EFFICIENCY"
REFERENCES,0.754601226993865,"0.8
0.9
1
0.2 0.4 0.6 0.8 1"
REFERENCES,0.7576687116564417,LSH Efficiency
REFERENCES,0.7607361963190185,= 0.5 c = 0.95
REFERENCES,0.7638036809815951,"LSH
SignRFF
KLSH
SQ-RFF"
REFERENCES,0.7668711656441718,"0.4
0.6
0.8
1
0.4 0.6 0.8 1"
REFERENCES,0.7699386503067485,LSH Efficiency
REFERENCES,0.7730061349693251,= 1 c = 0.95
REFERENCES,0.7760736196319018,"LSH
SignRFF
KLSH
SQ-RFF"
REFERENCES,0.7791411042944786,"0
0.5
1
0.6 0.7 0.8 0.9 1"
REFERENCES,0.7822085889570553,LSH Efficiency
REFERENCES,0.7852760736196319,= 2 c = 0.95
REFERENCES,0.7883435582822086,"LSH
SignRFF
KLSH
SQ-RFF"
REFERENCES,0.7914110429447853,"0
0.2
0.4
0.6
0.8
0.9 0.92 0.94 0.96 0.98 1"
REFERENCES,0.7944785276073619,LSH Efficiency
REFERENCES,0.7975460122699386,= 5 c = 0.95
REFERENCES,0.8006134969325154,"LSH
SignRFF
KLSH
SQ-RFF"
REFERENCES,0.803680981595092,"Figure 7: LSH efﬁciency of different LSH methods with various γ, c = 0.95. The x-axis ˜k is the
Gaussian kernel value in alignment with Deﬁnition 2.3, Proposition 3.2, 3.2 and 4.1."
REFERENCES,0.8067484662576687,"Recall Deﬁnition 2.3 of the (˜k, c˜k, p1, p2)-LSH. It is known (Indyk & Motwani, 1998) that one can
construct an LSH data structure with the worst case query time O(nR), where"
REFERENCES,0.8098159509202454,"R := log p1/ log p2
is called the LSH efﬁciency, which has been used in literature to theoretically compare different
LSH methods, e.g. SimHash vs. MinHash (Shrivastava & Li, 2014). However, we found that in our
case, the LSH efﬁciency does not provide much informative comparison of different LSH methods of
interest. In Figure 7, we provide the LSH efﬁciency at multiple γ. Firstly, we see that the differences
among the curves are very small. Basically, the ﬁgures tells us that SignRFF is always better than
KLSH and SQ-RFF, but do not provide the comparison of SignRFF and KLSH regarding γ, as the
ranking efﬁciency does. Also, the ﬁgures seem to suggest that SignRFF could (roughly) be better
than LSH with large γ and large ρ, but it does not give a good threshold at around ρ = 0.7 (validated
by the experiments) as suggested by the ranking efﬁciency. Thus, LSH efﬁciency is insufﬁcient to
well predict the practical search performance."
REFERENCES,0.8128834355828221,"A.2
MORE EXPERIMENTS ON SEARCH PRECISION"
REFERENCES,0.8159509202453987,Deﬁne N as the number of ground truth neighbors. The search recall and precision are deﬁned as
REFERENCES,0.8190184049079755,"recall@R = # true neighbors in R retrieved points N
,"
REFERENCES,0.8220858895705522,"precision@R = # true neighbors in R retrieved points R
."
REFERENCES,0.8251533742331288,"Consequently, by deﬁnition, recall@N is equivalent to precision@N. Therefore, in our experiments,
i.e., Figure 3, Figure 5 and Figure 6, the curves are also “precision@N”. In Figure 8, we report
more results on the search precision@R. As we can see, on SIFT, CIFAR and MNIST, the SignRFF
method again performs the best as long as b ≥256. The comparison is fairly consistent with the
recall (precision) curves presented in Section 5."
REFERENCES,0.8282208588957055,Under review as a conference paper at ICLR 2022
REFERENCES,0.8312883435582822,"0
500
1000
bits 0 0.2 0.4 0.6 0.8 1"
REFERENCES,0.8343558282208589,precision@10 SIFT
REFERENCES,0.8374233128834356,"LSH
ITQ
SpecH
BRE
SQ-RFF
KLSH
SignRFF"
REFERENCES,0.8404907975460123,"0
500
1000
bits 0 0.2 0.4 0.6 0.8 1"
REFERENCES,0.843558282208589,precision@10 CIFAR
REFERENCES,0.8466257668711656,"LSH
ITQ
SpecH
BRE
SQ-RFF
KLSH
SignRFF"
REFERENCES,0.8496932515337423,"0
500
1000
bits 0 0.2 0.4 0.6 0.8 1"
REFERENCES,0.852760736196319,precision@10 MNIST
REFERENCES,0.8558282208588958,"LSH
ITQ
SpecH
BRE
SQ-RFF
KLSH
SignRFF"
REFERENCES,0.8588957055214724,"0
50
100
# of retrieved neighbors 0.2 0.4 0.6 0.8 1"
REFERENCES,0.8619631901840491,precision
REFERENCES,0.8650306748466258,"SIFT
LSH
ITQ
SpecH
BRE
SQ-RFF
KLSH
SignRFF"
REFERENCES,0.8680981595092024,"0
50
100
# of retrieved neighbors 0 0.2 0.4 0.6 0.8"
REFERENCES,0.8711656441717791,precision CIFAR
REFERENCES,0.8742331288343558,"LSH
ITQ
SpecH
BRE
SQ-RFF
KLSH
SignRFF"
REFERENCES,0.8773006134969326,"0
50
100
# of retrieved neighbors 0.4 0.6 0.8 1"
REFERENCES,0.8803680981595092,precision MNIST
REFERENCES,0.8834355828220859,"LSH
ITQ
SpecH
BRE
SQ-RFF
KLSH
SignRFF"
REFERENCES,0.8865030674846626,"Figure 8: 1st row: Precision@10 against b. 2nd row: Precision vs. # of retrieved points, b = 512."
REFERENCES,0.8895705521472392,"A.3
DATA PROCESSING TIME"
REFERENCES,0.8926380368098159,"Another beneﬁt of SignRFF is its simplicity in implementation. This can be reﬂected in the data
processing time. Note that, the query time consists of two parts: the processing time (to generate
query binary codes), and the search time (to compare the query codes to the database). In our
experiments, since we adopt the standard Hamming ranking method, the search time to return a
ﬁxed number of retrieved points are the same for all algorithms. Hence, we plot the comparison of
processing time (for 1000 queries), as shown in Figure 9. We observe that SignRFF and LSH are the
most efﬁcient methods. KLSH is notably slower than SignRFF. The two data-dependent methods,
SpecH and BRE, are signiﬁcantly slower than SignRFF. This reveals a potential advantage of the
simplicity of SignRFF in practical retrieval systems."
REFERENCES,0.8957055214723927,"0
500
1000
bits 0 0.02 0.04 0.06 0.08"
REFERENCES,0.8987730061349694,processing time
REFERENCES,0.901840490797546,"LSH
SpecH
BRE
SQ-RFF
KLSH
SignRFF"
REFERENCES,0.9049079754601227,Figure 9: Data processing time (1000 queries).
REFERENCES,0.9079754601226994,Under review as a conference paper at ICLR 2022
REFERENCES,0.911042944785276,"B
MORE IMAGE RETRIEVAL RESULTS ON CIFAR-VGG"
REFERENCES,0.9141104294478528,"In Section 5.3, we compare the locality-sensitive hashing methods with a deep learning based
method, the Contrastive Information Bottleneck (CIB) (Qiu et al., 2021). CIB is built upon the
pre-trained VGG-16 CNN model, whose objective is to generate binary representations such that
images from the same class are close. Thus, same as many other deep hashing methods, the objec-
tive of CIB is slightly different from our setting (we ﬁnd the most similar data points, without any
label information), as being in the same class does not necessarily implies high similarity. In fact,
in the empirical evaluation of many of these papers (e.g., Yang et al. (2019); Qiu et al. (2021)), the
true neighbors are simply set as those data points with same label as the query. In our setting, we
strictly follow the ranking to ﬁnd the true neighbors with highest similarities. To better illustrate the
difference, we present top-10 retrieval result of two queries (automobile and cat) in Figure 10. For
SignRFF and KLSH, the retrieved images mostly have high similarity with the query, but may in-
clude some data points from other classes (e.g., truck vs. automobile, dog vs. cat). On the contrary,
the retrieved images of CIB clearly have lower similarity with the query, but contain fewer other
classes. This is largely because the VGG-16 is pre-trained by classiﬁcation. Hence, in this exper-
iment we set the true neighbors as the top-1000 similar data points, where CIB could be evaluated
properly since most images from the same class would, at least, have not too low similarity."
REFERENCES,0.9171779141104295,"0.84346
0.83695
0.83491
0.83276
0.82833
0.82549
0.82006
0.8187
0.81866
0.81854"
REFERENCES,0.9202453987730062,"0.83695
0.78657
0.83491
0.75924
0.81388
0.84346
0.78874
0.78314
0.81096
0.79105"
REFERENCES,0.9233128834355828,"0.84346
0.82549
0.83276
0.81854
0.79838
0.83695
0.82006
0.7392
0.78659
0.81144"
REFERENCES,0.9263803680981595,"0.7116
0.78082
0.66964
0.67989
0.78262
0.65833
0.78874
0.75491
0.72452
0.708"
REFERENCES,0.9294478527607362,"0.8453
0.73363
0.72542
0.7122
0.71098
0.71
0.70822
0.70533
0.70361
0.69812"
REFERENCES,0.9325153374233128,"0.8453
0.70533
0.73363
0.69618
0.65724
0.67699
0.56063
0.65057
0.68594
0.67611"
REFERENCES,0.9355828220858896,"0.8453
0.70533
0.6608
0.6599
0.69812
0.63937
0.73363
0.71098
0.6686
0.65584"
REFERENCES,0.9386503067484663,"0.63228
0.58711
0.58725
0.62174
0.64851
0.62651
0.56138
0.65125
0.58358
0.60518"
REFERENCES,0.941717791411043,"Figure 10: CIFAR Top-10 retrieved images (right) for two query points (left) with b = 512: Auto-
mobile and Cat. 1st row: true nearest neighbors in terms of cosine similarity of the features extract
from the last fc layer of VGG-16. 2nd row: SignRFF. 3rd row: KLSH. 4th row: CIB. The number
on each retrieved image is the cosine similarity to the VGG-feature of the query."
REFERENCES,0.9447852760736196,Under review as a conference paper at ICLR 2022
REFERENCES,0.9478527607361963,"C
IMPACT OF THE DEPENDENCE IN KLSH"
REFERENCES,0.950920245398773,"In practical implementation, the hash codes of KLSH are dependent. We give an intuitive explana-
tion on how this dependence affect the search performance."
REFERENCES,0.9539877300613497,"Firstly, let us review the reason why data-independent methods with independent codes (LSH, SQ-
RFF, SignRFF) can boost search accuracy with increasing b. Essentially, same as the intuition of
the rank efﬁciency, comparing Hamming distance is equivalent to searching for the data points with
highest estimated hash collision probability using b codes. Let x, y be two database points and q
be the query, with ρx > ρy. This means the hash collision probability px > py. We estimate the
probability by averaging the collision indicators:"
REFERENCES,0.9570552147239264,"ˆpx = 1 b b
X"
REFERENCES,0.9601226993865031,"i=1
1{hi(x) = hi(q)},
ˆpy = 1 b b
X"
REFERENCES,0.9631901840490797,"i=1
1{hi(y) = hi(q)}.
(13)"
REFERENCES,0.9662576687116564,"For data-dependent methods, with sufﬁcient b, (12) gives the probability of wrong raking (i.e., ˆpx <
ˆpy). We see that it strictly decreases with larger b."
REFERENCES,0.9693251533742331,"Recall the steps of KLSH. We ﬁrst sample m data points from database X, denoted as ˜
X, to form
a kernel matrix K. Then we uniformly pick t points, denoted as ˜
X′, from [1, ..., m] at random to
approximate the Gaussian distribution. After some algebra, the hash code has the form"
REFERENCES,0.9723926380368099,"KLSH:
h(x) = sign( m
X"
REFERENCES,0.9754601226993865,"i=1
w(i)k(x, xi)),
(14)"
REFERENCES,0.9785276073619632,"where w = K−1/2et, and et ∈{0, 1}m has ones in the entries with indices of the t selected points.
To generate multiple codes, we use the same pool of points ˜
X, but with different choice of et, i.e.,
˜
X′ to approximate the Gaussian distribution."
REFERENCES,0.9815950920245399,"Boosted performance with small b. For two hash codes hk and hk−1 of x, the k(x, xi) terms in
(14) are the same. The w’s are dependent since the points used for Gaussian approximation (i.e. ˜
X′)
may overlap. Thus, the conditional hash collision probability is usually larger than the unconditional
one,"
REFERENCES,0.9846625766871165,"P(hk(x) = hk(y)|hk−1(x) = hk−1(y)) > P(hk(x) = hk(y)),"
REFERENCES,0.9877300613496932,"and it may increase as k get larger (intuitively, by dependence, more previous collisions implies
higher chance of collision later on). Similarly,"
REFERENCES,0.99079754601227,P(hk(x) ̸= hk(y)|hk−1(x) ̸= hk−1(y)) > P(hk(x) ̸= hk(y)).
REFERENCES,0.9938650306748467,"Thus, at the beginning, the more similar (dissimilar) the data pair is, the more the estimator ˆp is
upward (downward) biased. In other words, similar points would have further increased hash col-
lision probability, while dissimilar points would have even lower chance of collision. This is the
main reason that the empirical performance of KLSH is higher than theoretical prediction (where
we assume independence) with short binary codes."
REFERENCES,0.9969325153374233,"Slow improvement with large b. However, this is not the whole story. As more bits are used, there
is less and less marginal information left. That is, the later generated codes would be more and more
dependent on the previous ones. In an extreme case, at the point when all
 m
t

combinations of the
t points in ˜
X′ have been used, the hash codes produced afterwards would all be the same as some
previously generated ones, which would hardly improve the distance estimation anymore. This is
why for KLSH, the recall curve becomes ﬂat as b increases."
