Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0013736263736263737,"We introduce a new approach to the mixture of experts model that consists in
1"
ABSTRACT,0.0027472527472527475,"imposing local differential privacy on the gating mechanism. This is theoretically
2"
ABSTRACT,0.004120879120879121,"justified by statistical learning theory. Notably, we provide generalization bounds
3"
ABSTRACT,0.005494505494505495,"specifically tailored for mixtures of experts, leveraging the one-out-of-n gating
4"
ABSTRACT,0.006868131868131868,"mechanism rather than the more common n-out-of-n mechanism. Moreover,
5"
ABSTRACT,0.008241758241758242,"through experiments, we show that our approach improves the generalization
6"
ABSTRACT,0.009615384615384616,"ability of mixtures of experts.
7"
INTRODUCTION,0.01098901098901099,"1
Introduction
8"
INTRODUCTION,0.012362637362637362,"Mixtures of experts, initially introduced by Jacobs et al. [1991], have found widespread use in
9"
INTRODUCTION,0.013736263736263736,"modeling sequential data, including applications in classification, regression, pattern recognition and
10"
INTRODUCTION,0.01510989010989011,"feature selection tasks (Städler et al. [2010] and Khalili and Lin [2013]). One of the fundamental
11"
INTRODUCTION,0.016483516483516484,"motivations behind mixtures of experts is their ability to break down complex problems into more
12"
INTRODUCTION,0.017857142857142856,"manageable sub-problems, potentially simplifying the overall task. The structure of these models is
13"
INTRODUCTION,0.019230769230769232,"well suited to capturing unobservable heterogeneity in the data generation process, dealing with this
14"
INTRODUCTION,0.020604395604395604,"problem by splitting the data into homogeneous subsets (with the gating network) and associating
15"
INTRODUCTION,0.02197802197802198,"each subset with an expert. This intuitive architecture has led to significant interest in mixture of
16"
INTRODUCTION,0.023351648351648352,"experts models, resulting in a wealth of research (Yuksel et al. [2012]), ranging from simple mixtures
17"
INTRODUCTION,0.024725274725274724,"of experts ( Jacobs et al. [1991], Jordan and Jacobs [1993]) to sparsely gated models (Shazeer et al.
18"
INTRODUCTION,0.0260989010989011,"[2017]). Moreover, this architecture has inspired the development of various other models, such as
19"
INTRODUCTION,0.027472527472527472,"switch transformers (Fedus et al. [2022]). However, despite the considerable attention mixtures of
20"
INTRODUCTION,0.028846153846153848,"experts have received, advancements in their theoretical analysis have been relatively limited. Azran
21"
INTRODUCTION,0.03021978021978022,"and Meir [2004] proved data-dependent risk bounds for mixtures of experts (with the n-out-of-n
22"
INTRODUCTION,0.03159340659340659,"gating mechanism) using Rademacher complexity, but they exhibit a dependence on the complexity
23"
INTRODUCTION,0.03296703296703297,"of the class of gating networks and the sum of the complexities of the expert classes, which reflects
24"
INTRODUCTION,0.034340659340659344,"the complex structure of mixtures of experts but unfortunately leads to potentially large bounds. We
25"
INTRODUCTION,0.03571428571428571,"are not aware of other work proving generalization bounds specifically tailored to mixtures of experts.
26"
INTRODUCTION,0.03708791208791209,"To make theoretical progress, we utilize a well-known privacy-preserving technique called Local
27"
INTRODUCTION,0.038461538461538464,"Differential Privacy (LDP). It was initially introduced by Dwork [2006] and has since been widely
28"
INTRODUCTION,0.03983516483516483,"used to preserve privacy for individual data points as in Kasiviswanathan et al. [2010]. This is
29"
INTRODUCTION,0.04120879120879121,"achieved by introducing stochasticity in algorithm outputs to control their dependence on specific
30"
INTRODUCTION,0.042582417582417584,"inputs. This stochasticity is generally quantified by a positive real number ϵ. In this case, we write
31"
INTRODUCTION,0.04395604395604396,"ϵ-LDP instead of just LDP. The parameter ϵ quantifies the level of privacy protection in the local
32"
INTRODUCTION,0.04532967032967033,"differential privacy mechanism. A smaller value indicates stronger privacy protection, which requires
33"
INTRODUCTION,0.046703296703296704,"the addition of more noise.
34"
INTRODUCTION,0.04807692307692308,"In this work, we exploit this noise for regularization in our models by imposing the ϵ-LDP condition
35"
INTRODUCTION,0.04945054945054945,"on their gating networks. This method allows us to leverage the numerous benefits of the most
36"
INTRODUCTION,0.050824175824175824,"complex architectures, such as neural networks, without compromising theoretical guarantees on risk.
37"
INTRODUCTION,0.0521978021978022,"By relying on LDP, we offer tight theoretical guarantees on the risk of mixtures of experts models,
38"
INTRODUCTION,0.05357142857142857,"provided with the one-out-of-n gating mechanism. Unlike the very few existing guarantees, these
39"
INTRODUCTION,0.054945054945054944,"bounds depend only logarithmically on the number of experts we have, and the complexity of the
40"
INTRODUCTION,0.05631868131868132,"gating network only appears in our bounds through the parameter ϵ of the LDP condition.
41"
PRELIMINARIES,0.057692307692307696,"2
Preliminaries
42"
PRELIMINARIES,0.059065934065934064,"Let X be the instance space, Y the label space, and Y′ the output space (which can be different
43"
PRELIMINARIES,0.06043956043956044,"from Y). As is usual in supervised learning, we assume that data (x, y) ∈X × Y are generated
44"
PRELIMINARIES,0.061813186813186816,"independently from an unknown probability distribution D. We consider a training set of m examples
45"
PRELIMINARIES,0.06318681318681318,"S = ((x1, y1), . . . , (xm, ym)) ∼Dm and a bounded loss function ℓ: Y′ × Y →[0, 1].
46"
MIXTURES OF EXPERTS,0.06456043956043957,"2.1
Mixtures of experts
47"
MIXTURES OF EXPERTS,0.06593406593406594,"We consider classes Hi of experts hi : X →Y′ for i = 1, . . . , n. Let G be a set of gating functions
48"
MIXTURES OF EXPERTS,0.0673076923076923,"g: X →[0, 1]n such that, given any x ∈X, we have that Pn
i=1 gi(x) = 1, where gi(x) is the
49"
MIXTURES OF EXPERTS,0.06868131868131869,"i-th component of g(x). This means that each gating function defines a probability distribution on
50"
MIXTURES OF EXPERTS,0.07005494505494506,"[n] = {1, . . . , n} for each x ∈X, where gi(x) is the probability of i.
51"
MIXTURES OF EXPERTS,0.07142857142857142,"In this work, a mixture of experts consists of n experts, h = (h1, . . . , hn) ∈H1 × · · · × Hn, a
52"
MIXTURES OF EXPERTS,0.07280219780219781,"gating function g ∈G and a gating mechanism that combines the outputs of the experts and the
53"
MIXTURES OF EXPERTS,0.07417582417582418,"output of the gating function to produce the final output. Our models use the stochastic one-out-of-n
54"
MIXTURES OF EXPERTS,0.07554945054945054,"gating mechanism, as described in Jacobs et al. [1991]. It is defined as follows: to make a prediction
55"
MIXTURES OF EXPERTS,0.07692307692307693,"with (g, h) ∈G × Qn
i=1 Hi given an instance x, draw i ∼g(x) and output hi(x). This stochastic
56"
MIXTURES OF EXPERTS,0.0782967032967033,"predictor has risk and empirical risk defined by, respectively,
57"
MIXTURES OF EXPERTS,0.07967032967032966,"R(g, h) =
E
(x,y)∼D
E
i∼g(x) ℓ(hi(x), y),
and
RS(g, h) = 1 m m
X"
MIXTURES OF EXPERTS,0.08104395604395605,"j=1
E
i∼g(xj) ℓ(hi(xj), yj)."
MIXTURES OF EXPERTS,0.08241758241758242,"The preference for the one-out-of-n gating mechanism over the n-out-of-n mechanism in mixtures
58"
MIXTURES OF EXPERTS,0.08379120879120878,"of experts is justified by its ability to induce sparsity and noise, enhancing computational efficiency
59"
MIXTURES OF EXPERTS,0.08516483516483517,"and robustness to overfitting. This sparsity also offers scalability benefits, particularly in large-scale
60"
MIXTURES OF EXPERTS,0.08653846153846154,"applications, where activating all experts for each input can lead to increased computational and
61"
MIXTURES OF EXPERTS,0.08791208791208792,"memory requirements as explained in Shazeer et al. [2017] and Jacobs et al. [1991]. Moreover, the
62"
MIXTURES OF EXPERTS,0.08928571428571429,"one-out-of-n mechanism is more amenable to certain kinds of theoretical analysis, including ours.
63"
LOCAL DIFFERENTIAL PRIVACY,0.09065934065934066,"2.2
Local Differential Privacy
64"
LOCAL DIFFERENTIAL PRIVACY,0.09203296703296704,"Definition 2.1. Let I be a finite set, consider a mechanism that produces an output i ∈I, given an
65"
LOCAL DIFFERENTIAL PRIVACY,0.09340659340659341,"input x ∈X, with probability P(i|x), and let ϵ be a nonnegative real number. Then, the mechanism
66"
LOCAL DIFFERENTIAL PRIVACY,0.09478021978021978,"satisfies the ϵ-Local Differential Privacy (ϵ-LDP) property if and only if
67"
LOCAL DIFFERENTIAL PRIVACY,0.09615384615384616,"P(i|x) ≤eϵ P(i|x′)
for all x, x′ ∈X and all i ∈I."
LOCAL DIFFERENTIAL PRIVACY,0.09752747252747253,"Unless stated otherwise, we assume that each g ∈G satisfies ϵ-LDP, for some fixed nonnegative real
number ϵ. Since we can interpret g as a random mechanism that, given x ∈X, selects i ∈[n] with
probability gi(x), the condition of ϵ-LDP amounts to the following:
gi(x) ≤eϵgi(x′)
for all x, x′ ∈X and all i ∈[n]."
LOCAL DIFFERENTIAL PRIVACY,0.0989010989010989,"Since ϵ-LDP is an important condition for all of our theoretical results, we provide a practical way
68"
LOCAL DIFFERENTIAL PRIVACY,0.10027472527472528,"of obtaining gating functions satisfying ϵ-LDP from an arbitrary set F of bounded functions, in the
69"
LOCAL DIFFERENTIAL PRIVACY,0.10164835164835165,"form of the following theorem.
70"
LOCAL DIFFERENTIAL PRIVACY,0.10302197802197802,"Theorem 2.2. Let b > 0 and β ≥0 be real numbers, and suppose that F is a set of functions
71"
LOCAL DIFFERENTIAL PRIVACY,0.1043956043956044,"f : X →[−b, b]n. Let G be the set of functions g : X →[0, 1]n defined by
72"
LOCAL DIFFERENTIAL PRIVACY,0.10576923076923077,"gi(x) =
exp(βfi(x) + ci)
Pn
k=1 exp(βfk(x) + ck),
where f = (f1, . . . , fn) ∈F and (c1, . . . , cn) ∈Rn."
LOCAL DIFFERENTIAL PRIVACY,0.10714285714285714,"Then, each g ∈G satisfies 4βb-LDP.
73"
LOCAL DIFFERENTIAL PRIVACY,0.10851648351648352,"Proof. The proof is obtained by performing simple calculations, bounding the ratio gi(x)/gi(x′), for
74"
LOCAL DIFFERENTIAL PRIVACY,0.10989010989010989,"all x, x′ ∈X and all i ∈[n]. The detailed proof is given in Appendix A.
75"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.11126373626373626,"3
PAC-Bayesian bounds for mixtures of experts
76"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.11263736263736264,"To apply the PAC-Bayes theory, we need to add a level of stochasticity to our predictors: instead of
training experts hi, we train probability measures Qi on each expert set Hi. For convenience, we
write Q = Q1 ⊗· · · ⊗Qn. Now, putting everything together, a mixture of experts (g, Q) makes
predictions as follows: given x ∈X, draw i ∼g(x), then draw h ∼Qi, and finally output h(x).
Such a predictor has risk and empirical risk defined by, respectively,"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.11401098901098901,"R(g, Q) =
E
h∼Q R(g, h)
and
RS(g, Q) =
E
h∼Q RS(g, h)."
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.11538461538461539,"Notice that, though probability distributions have replaced the individual experts, there is no need to
77"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.11675824175824176,"define a probability distribution on the gating functions to get a PAC-Bayesian bound. Training a
78"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.11813186813186813,"single gating function will do, and, remarkably, Lemma 3.1 below shows that it can be obtained from
79"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.11950549450549451,"a very complicated function, such as a neural network, provided we impose ϵ-LDP (for example, with
80"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.12087912087912088,"Theorem 2.2).
81"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.12225274725274725,"Finally, let us recall the notion of Kullback-Leibler (KL) divergence. Given probability distributions
82"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.12362637362637363,"Qi and Pi on Hi, it is defined by
83"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.125,"KL(Qi ∥Pi) = 
"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.12637362637362637,"
E
h∼Qi ln dQi"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.12774725274725274,"dPi
(h)
if Qi ≪Pi"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.12912087912087913,"∞
otherwise,"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.1304945054945055,"where dQi/dPi is a Radon-Nikodym derivative.
84"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.13186813186813187,"Lemma 3.1. We consider mixtures of experts as defined in section 2.1 and provided with the one-
85"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.13324175824175824,"out-of-n routing mechanism. Let ∆: R2 →R be a convex function that is decreasing in its first
86"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.1346153846153846,"argument and increasing in its second argument, and let ϵ be a nonnegative real number. Then, for
87"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.13598901098901098,"any g ∈G that satisfies the ϵ-LDP property, for any Q = Q1 ⊗· · · ⊗Qn on H1 × · · · × Hn, and for
88"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.13736263736263737,"any x′ ∈X:
89"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.13873626373626374,"∆
 
eϵRS(g, Q), e−ϵR(g, Q)

≤
E
i∼g(x′) ∆
 
RS(Qi), R(Qi)
"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.1401098901098901,"where R(Qi) = Ex∼D Eh∼Qi ℓ(h(x), y) and RS(Qi) = 1"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.14148351648351648,"m
Pm
j=1 Eh∼Qi ℓ(h(xj), yj).
90"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.14285714285714285,"Proof. Since the gating function satisfies ϵ-LDP, we have that e−ϵgi(x′) ≤gi(x) ≤eϵgi(x′) for
91"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.14423076923076922,"all x, x′ ∈X and all i ∈[n]. It follows that eϵRS(g, Q) ≥Ei∼g(x′) RS(Qi) and e−ϵR(g, Q) ≤
92"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.14560439560439561,"Ei∼g(x′) R(Qi). Given that ∆is decreasing in its first argument and increasing in its second argument,
93"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.14697802197802198,"we find that
94"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.14835164835164835,"∆
 
eϵRS(g, Q), e−ϵR(g, Q)

≤∆

E
i∼g(x′) RS(Qi),
E
i∼g(x′) R(Qi)
"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.14972527472527472,"Since ∆is a convex function, we can apply Jensen’s inequality to the expression on the right-hand
95"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.1510989010989011,"side, yielding the desired result.
96"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.15247252747252749,"Different choices of function ∆will allow us to obtain different PAC-Bayes bounds:
97"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.15384615384615385,"• Let ∆(u, v) = v −u. This is compatible with typical PAC-Bayes bounds on the difference
98"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.15521978021978022,"between the true and empirical risks.
99"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.1565934065934066,"• Given λ > 1/2, let ∆be defined by ∆(u, v) = v −
2λ
2λ−1u. This choice is compatible with
100"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.15796703296703296,"a Catoni-type bound, as we will see below.
101"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.15934065934065933,"• Let ∆be defined by ∆(u, v) = kl(u∥v) = u ln u"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.16071428571428573,v + (1 −u) ln 1−u
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.1620879120879121,"1−v . This choice is
102"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.16346153846153846,"compatible with a Langford-Seeger-type bound. However, note that the function ∆defined
103"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.16483516483516483,"here does not quite obey the hypotheses of lemma 3.1. Indeed, it is only defined for (u, v) ∈
104"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.1662087912087912,"[0, 1]2, and only has the right monotonicity properties on the set {(u, v) ∈[0, 1]2 | u ≤v}.
105"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.16758241758241757,"We can remedy those defects through small adjustments to the proof.
106"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.16895604395604397,"We prove a generalization bound of Catoni-type as an illustration of the machinery just described.
107"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.17032967032967034,"Theorem 3.2 (Theorem 2 in McAllester [2013]). Let δ ∈(0, 1) and λ > 1/2. Fix i ∈[n], and let Pi
108"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.1717032967032967,"be a probability measure on Hi (chosen without seeing the training data). Then, with probability at
109"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.17307692307692307,"least 1 −δ over the draws of S, for all probability measures Qi on Hi, we have that
110"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.17445054945054944,"R(Qi) ≤
2λ
2λ −1"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.17582417582417584,"
RS(Qi) + λ m"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.1771978021978022,"
KL(Qi ∥Pi) + ln 1 δ 
."
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.17857142857142858,"Theorem 3.3. Let δ ∈(0, 1), ϵ ≥0, and λ > 1/2. For each i ∈[n], let Pi be a probability measure
111"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.17994505494505494,"on Hi (chosen without seeing the training data). Then, with probability at least 1 −δ over the draws
112"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.1813186813186813,"of S, for all probability measures Q = Q1 ⊗· · · ⊗Qn on H, all g ∈G that satisfy ϵ-LDP, and all
113"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.18269230769230768,"x′ ∈X, we have that
114"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.18406593406593408,"R(g, Q) ≤
2λeϵ 2λ −1"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.18543956043956045,"
eϵRS(g, Q) + λ m"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.18681318681318682,"
E
i∼g(x′) KL(Qi ∥Pi) + ln n δ 
."
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.18818681318681318,"Proof. By n applications of Theorem 3.2, we have that, for each i ∈[n], with probability at least
115"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.18956043956043955,"1 −δ/n, for all Qi,
116"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.19093406593406592,"R(Qi) ≤
2λ
2λ −1"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.19230769230769232,"
RS(Qi) + λ m"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.1936813186813187,"
KL(Qi ∥Pi) + ln n δ 
."
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.19505494505494506,"We can make all these inequalities (for each i ∈[n]) hold simultaneously with a union bound. Now,
117"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.19642857142857142,"applying Lemma 3.1 with ∆(u, v) = v −
2λ
2λ−1u, we find that, with probability at least 1 −δ, for all
118"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.1978021978021978,"Q, all g ∈G and all x′ ∈X, we have that
119"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.19917582417582416,"e−ϵR(g, Q) −2λeϵ"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.20054945054945056,"2λ −1RS(g, Q) ≤
E
i∼g(x′)"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.20192307692307693,"
R(Qi) −
2λ
2λ −1RS(Qi)
 ≤
2λ2"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.2032967032967033,(2λ −1)m
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.20467032967032966,"
E
i∼g(x′) KL(Qi ∥Pi) + ln n δ 
."
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.20604395604395603,"We also give a bound of Langford-Seeger type, since they are generally recognized as among the
120"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.20741758241758243,"tightest PAC-Bayes bounds available, and to prove the flexibility of our approach.
121"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.2087912087912088,"Theorem 3.4. Let δ ∈(0, 1), ϵ ≥0, and m ≥8. For each i ∈[n], let Pi be a probability measure
122"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.21016483516483517,"on Hi (chosen without seeing the training data). Then, with probability at least 1 −δ over the draws
123"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.21153846153846154,"of S, for all probability measures Q = Q1 ⊗· · · ⊗Qn on H, all g ∈G that satisfy ϵ-LDP, and all
124"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.2129120879120879,"x′ ∈X, we have that, either R(g, Q) < e2ϵRS(g, Q), or
125"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.21428571428571427,"kl(eϵRS(g, Q)∥e−ϵR(g, Q)) ≤1 m"
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.21565934065934067,"
E
i∼g(x′) KL(Qi ∥Pi) + ln 2n√m δ 
."
PAC-BAYESIAN BOUNDS FOR MIXTURES OF EXPERTS,0.21703296703296704,"Proof. The proof, which is similar to that of Theorem 3.3, is available in Appendix A.
126"
COMPARISON WITH OTHER BOUNDS,0.2184065934065934,"3.1
Comparison with other bounds
127"
COMPARISON WITH OTHER BOUNDS,0.21978021978021978,"Very few generalizations bound tailored specifically to mixtures of experts appear in the literature,
128"
COMPARISON WITH OTHER BOUNDS,0.22115384615384615,"and those we could find do not apply to mixtures of experts with the one-out-of-n gating mechanism.
129"
COMPARISON WITH OTHER BOUNDS,0.22252747252747251,"We can, however, compare our bounds to those obtained by naively applying generic PAC-Bayes
130"
COMPARISON WITH OTHER BOUNDS,0.2239010989010989,"generalization bounds to mixtures of experts. In this case, we need to consider classifiers of the form
131"
COMPARISON WITH OTHER BOUNDS,0.22527472527472528,"(QG, Q), where QG is a probability measure on G, and Q = Q1 ⊗· · · ⊗Qn is a probability measure
132"
COMPARISON WITH OTHER BOUNDS,0.22664835164835165,"on H1 × · · · × Hn as before. Then, note that
133"
COMPARISON WITH OTHER BOUNDS,0.22802197802197802,"KL(QG ⊗Q1 ⊗· · · ⊗Qn ∥PG ⊗P1 ⊗· · · ⊗Pn) = KL(QG ∥PG) + n
X"
COMPARISON WITH OTHER BOUNDS,0.22939560439560439,"i=1
KL(Qi ∥Pi)."
COMPARISON WITH OTHER BOUNDS,0.23076923076923078,"This means that a generic PAC-Bayes bound applied to mixtures of experts will depend on the sum of
134"
COMPARISON WITH OTHER BOUNDS,0.23214285714285715,"the KL divergences corresponding to the gating functions and each of the experts. Obviously, this
135"
COMPARISON WITH OTHER BOUNDS,0.23351648351648352,"sum could be very large. By imposing ϵ-LDP to the gating functions as in our approach, we can
136"
COMPARISON WITH OTHER BOUNDS,0.2348901098901099,"eliminate the stochasticity associated to the gating functions, and rid our bounds of the (potentially
137"
COMPARISON WITH OTHER BOUNDS,0.23626373626373626,"very large) KL(QG ∥PG) term. Instead, it is ϵ-LDP which controls our gating functions to ensure
138"
COMPARISON WITH OTHER BOUNDS,0.23763736263736263,"generalization. Furthermore, our bounds replace the sum of the KL divergences of the experts by
139"
COMPARISON WITH OTHER BOUNDS,0.23901098901098902,"a g(x′)-weighted average, which means we can have many more experts with almost no penalty
140"
COMPARISON WITH OTHER BOUNDS,0.2403846153846154,"from the theoretical point of view. Indeed, our bounds only depend on the number n of experts
141"
COMPARISON WITH OTHER BOUNDS,0.24175824175824176,"logarithmically, through the use of the union bound.
142"
RADEMACHER BOUNDS FOR MIXTURES OF EXPERTS,0.24313186813186813,"4
Rademacher bounds for mixtures of experts
143"
RADEMACHER BOUNDS FOR MIXTURES OF EXPERTS,0.2445054945054945,"Let us start with a slight modification of Lemma 3.1.
144"
RADEMACHER BOUNDS FOR MIXTURES OF EXPERTS,0.24587912087912087,"Lemma 4.1. We consider mixtures of experts as defined in section 2.1 and provided with the one-
145"
RADEMACHER BOUNDS FOR MIXTURES OF EXPERTS,0.24725274725274726,"out-of-n routing mechanism. Let ∆: R2 →R be a convex function that is decreasing in its first
146"
RADEMACHER BOUNDS FOR MIXTURES OF EXPERTS,0.24862637362637363,"argument and increasing in its second argument, and let ϵ be a nonnegative real number. Then, for
147"
RADEMACHER BOUNDS FOR MIXTURES OF EXPERTS,0.25,"any g ∈G that satisfies the ϵ-LDP property, for any h ∈H , and for any x′ ∈X:
148"
RADEMACHER BOUNDS FOR MIXTURES OF EXPERTS,0.25137362637362637,"∆
 
eϵRS(g, h), e−ϵR(g, h)

≤
E
i∼g(x′) ∆
 
RS(hi), R(hi)
"
RADEMACHER BOUNDS FOR MIXTURES OF EXPERTS,0.25274725274725274,"where R(hi) = Ex∼D ℓ(hi(x), y) and RS(hi) = 1"
RADEMACHER BOUNDS FOR MIXTURES OF EXPERTS,0.2541208791208791,"m
Pm
j=1 ℓ(hi(xj), yj).
149"
RADEMACHER BOUNDS FOR MIXTURES OF EXPERTS,0.2554945054945055,"Proof. The proof is similar to that of Lemma 3.1 and is provided in Appendix A.
150"
RADEMACHER BOUNDS FOR MIXTURES OF EXPERTS,0.25686813186813184,"Let us now recall the following definition.
151"
RADEMACHER BOUNDS FOR MIXTURES OF EXPERTS,0.25824175824175827,"Definition 4.2 (Rademacher complexity). Given a space H of predictors, a loss function ℓ, and a
152"
RADEMACHER BOUNDS FOR MIXTURES OF EXPERTS,0.25961538461538464,"data generating distribution D, the Rademacher complexity R(ℓ◦H) is defined by
153"
RADEMACHER BOUNDS FOR MIXTURES OF EXPERTS,0.260989010989011,"R(ℓ◦H) =
E
S∼Dm E
σσσ sup
h∈H"
M,0.2623626373626374,"1
m m
X"
M,0.26373626373626374,"j=1
σjℓ(h(xj), yj),"
M,0.2651098901098901,"where σσσ = (σ1, . . . , σm) is distributed uniformly on {−1, 1}m.
154"
M,0.2664835164835165,"Our main theorem will make use of the following well-known risk bound.
155"
M,0.26785714285714285,"Theorem 4.3 (Basic Rademacher risk bound). Given a [0, 1]-valued loss function ℓ, with probability
156"
M,0.2692307692307692,"at least 1 −δ, for all h ∈H, we have that
157"
M,0.2706043956043956,R(h) ≤RS(h) + 2R(ℓ◦H) + r
M,0.27197802197802196,"2 ln(2/δ) m
."
M,0.2733516483516483,"Theorem 4.4. Let δ ∈(0, 1) and ϵ ≥0. Given a [0, 1]-valued loss function ℓ, then, with probability
158"
M,0.27472527472527475,"at least 1 −δ over the draws of S, for all h ∈H1 × · · · × Hn, for all g ∈G that satisfy ϵ-LDP, and
159"
M,0.2760989010989011,"all x′ ∈X, we have that
160"
M,0.2774725274725275,"R(g, h) ≤eϵ

eϵRS(g, h) + 2
E
i∼g(x′) R(ℓ◦Hi) + r"
M,0.27884615384615385,"2 ln(2n/δ) m 
."
M,0.2802197802197802,"Proof. By n applications of Theorem 4.3, we have that, for each i ∈[n], with probability at least
161"
M,0.2815934065934066,"1 −δ/n, for all hi ∈Hi,
162"
M,0.28296703296703296,R(hi) ≤RS(hi) + 2R(ℓ◦Hi) + r
M,0.28434065934065933,"2 ln(2n/δ) m
."
M,0.2857142857142857,"We can make all these inequalities (for each i ∈[n]) hold simultaneously with a union bound. Now,
163"
M,0.28708791208791207,"applying Lemma 4.1 with ∆(u, v) = v −u, we find that, with probability at least 1 −δ, for all
164"
M,0.28846153846153844,"h ∈H1 × · · · × Hn, all g ∈G and all x′ ∈X, we have that
165"
M,0.28983516483516486,"e−ϵR(g, h) −eϵRS(g, h) ≤
E
i∼g(x′)"
M,0.29120879120879123," 
R(hi) −RS(hi)
"
M,0.2925824175824176,"≤
E
i∼g(x′)"
M,0.29395604395604397,"
2R(ℓ◦Hi) + r"
M,0.29532967032967034,"2 ln(2n/δ) m 
."
M,0.2967032967032967,"Note, that the risk bound of Theorem 4.4 depends only on the average Rademacher complexity of
166"
M,0.2980769230769231,"the classes of experts instead of the sum of their Rademacher complexities. Note also that, as in the
167"
M,0.29945054945054944,"previous section, the complexity of G does not affect the risk bound. Finally, the risk bound does not
168"
M,0.3008241758241758,"hold uniformly for all values of ϵ. However, by the union bound, the theorem holds for any fixed set
169"
M,0.3021978021978022,"{ϵ1, . . . , ϵk} if we replace δ by δ/k. Consequently, this suggests a learning algorithm that minimizes
170"
M,0.30357142857142855,"RS(g, h) for ϵ ∈{ϵ1, . . . , ϵk}.
171"
M,0.30494505494505497,"Also note that Lemma 4.1 allows us to obtain risk bounds for mixtures of experts as long as we
172"
M,0.30631868131868134,"have bounds on ∆
 
RS(hi), R(hi)

which hold with high probability, whether they are based on
173"
M,0.3076923076923077,"Rademacher complexity, margins, VC dimension, or algorithmic stability.
174"
THE NEED TO USE ADAPTIVE EXPERTS,0.3090659340659341,"4.1
The need to use adaptive experts
175"
THE NEED TO USE ADAPTIVE EXPERTS,0.31043956043956045,"Following these theoretical results, we may be tempted to use a gating network satisfying ϵ-LDP to
176"
THE NEED TO USE ADAPTIVE EXPERTS,0.3118131868131868,"accomplish a learning task all by itself using non-adaptive experts, that is, experts hi each taking
177"
THE NEED TO USE ADAPTIVE EXPERTS,0.3131868131868132,"a constant value, no matter the input: hi(x) = i for all x ∈X. In that case, each Rademacher
178"
THE NEED TO USE ADAPTIVE EXPERTS,0.31456043956043955,"complexity R(ℓ◦Hi) is zero and we can show that Theorem 4.4 can become vacuous under reasonable
179"
THE NEED TO USE ADAPTIVE EXPERTS,0.3159340659340659,"circumstances.
180"
THE NEED TO USE ADAPTIVE EXPERTS,0.3173076923076923,"Consider, for example, the binary classification case with the 0-1 loss. In that case, we have two
181"
THE NEED TO USE ADAPTIVE EXPERTS,0.31868131868131866,"experts h+1 and h−1 such that h+1(x) = +1 and h−1(x) = −1 for all x ∈X, and a gating network
182"
THE NEED TO USE ADAPTIVE EXPERTS,0.32005494505494503,"g = (g+1, g−1). Then, the following holds:
183"
THE NEED TO USE ADAPTIVE EXPERTS,0.32142857142857145,"RS(g, h) = 1 m m
X"
THE NEED TO USE ADAPTIVE EXPERTS,0.3228021978021978,"j=1
E
i∼g(xj) ℓ0-1(hi(xj), yj) = 1 m m
X"
THE NEED TO USE ADAPTIVE EXPERTS,0.3241758241758242,"j=1
E
i∼g(xj) 1(hi(xj) ̸= yj) ≥1 m m
X j=1 X"
THE NEED TO USE ADAPTIVE EXPERTS,0.32554945054945056,"i∈I
e−ϵ max
x′∈X gi(x′)1(hi(xj) ̸= yj),
with I = {+1, −1}"
THE NEED TO USE ADAPTIVE EXPERTS,0.3269230769230769,"= e−ϵ 1 m m
X"
THE NEED TO USE ADAPTIVE EXPERTS,0.3282967032967033,"j=1
max
x′∈X g−yj(x′)."
THE NEED TO USE ADAPTIVE EXPERTS,0.32967032967032966,"Under the assumption that the classes are balanced, meaning that the (marginal) probability of a
184"
THE NEED TO USE ADAPTIVE EXPERTS,0.33104395604395603,"positive label is equal to the (marginal) probability of a negative label, we have the following:
185"
THE NEED TO USE ADAPTIVE EXPERTS,0.3324175824175824,"lim
m→∞
1
m m
X"
THE NEED TO USE ADAPTIVE EXPERTS,0.33379120879120877,"j=1
max
x′∈X g−yj(x′) = 1 2"
THE NEED TO USE ADAPTIVE EXPERTS,0.33516483516483514,"
max
x′∈X g−1(x′) + max
x′∈X g+1(x′)
 ≥1"
MAX,0.33653846153846156,"2 max
x′∈X
 
g−1(x′) + g+1(x′)

= 1 2."
MAX,0.33791208791208793,"It follows that, in the limit m →∞, the risk bound of Theorem 4.4 for any g has a value of at least
186"
MAX,0.3392857142857143,"eϵ/2 ≥1/2. Consequently, the risk bound becomes large or even vacuous in this regime, highlighting
187"
MAX,0.34065934065934067,"the importance of having adaptive experts of finite complexity that can drive the empirical risk to
188"
MAX,0.34203296703296704,"zero when they are selected by the gating network.
189"
EXPERIMENTS AND RESULTS,0.3434065934065934,"5
Experiments and results
190"
EXPERIMENTS AND RESULTS,0.3447802197802198,"In what follows, we consider mixtures of n linear experts in binary classification tasks. Let X = Rd
191"
EXPERIMENTS AND RESULTS,0.34615384615384615,"for some positive integer d. Let S be a training set of m examples. Each expert, denoted by hi, where
192"
EXPERIMENTS AND RESULTS,0.3475274725274725,"i ranges from 1 to n, is characterized by a weight vector wi. Given an input x ∈X, the output of the
193"
EXPERIMENTS AND RESULTS,0.3489010989010989,"expert hi is given by hi(x) = wi · x. We use the probit loss function ℓ= Φ, which can be seen as a
194"
EXPERIMENTS AND RESULTS,0.35027472527472525,"smooth surrogate to the 0-1 loss function, when it is used with an argument of the form ywi·x"
EXPERIMENTS AND RESULTS,0.3516483516483517,"∥x∥. In this
195"
EXPERIMENTS AND RESULTS,0.35302197802197804,"case, R(g, Q) and RS(g, Q) are given by:
196"
EXPERIMENTS AND RESULTS,0.3543956043956044,"R(g, Q) =
E
(x,y)∼D
E
i∼g(x) Φ
ywi · x ∥x∥ "
EXPERIMENTS AND RESULTS,0.3557692307692308,"and
197"
EXPERIMENTS AND RESULTS,0.35714285714285715,"RS(g, Q) = 1 m m
X j=1 n
X"
EXPERIMENTS AND RESULTS,0.3585164835164835,"i=1
gi(xj)Φ
yjwi · xj ∥xj∥"
EXPERIMENTS AND RESULTS,0.3598901098901099,"
,
(1)"
EXPERIMENTS AND RESULTS,0.36126373626373626,"where Φ(x) =
1
√"
EXPERIMENTS AND RESULTS,0.3626373626373626,"2π
R +∞
x
e−t2/2 dt provides the probability that a standard normal random variable
198"
EXPERIMENTS AND RESULTS,0.364010989010989,"is greater than a given value x.
199"
EXPERIMENTS AND RESULTS,0.36538461538461536,"To illustrate the regularizing effect of the LDP condition, we carried out several experiments, on
200"
EXPERIMENTS AND RESULTS,0.36675824175824173,"different datasets, by minimizing the empirical risk as defined in Equation 1. For all experiments, our
201"
EXPERIMENTS AND RESULTS,0.36813186813186816,"models consist of mixtures of n = 100 linear experts and a gating network. The gating network is a
202"
EXPERIMENTS AND RESULTS,0.3695054945054945,"neural network having 2 hidden layers. It is parameterized by weights W1 ∈R64×d, where d is the
203"
EXPERIMENTS AND RESULTS,0.3708791208791209,"dimension of input vectors, W2 ∈R64×64, and W3 ∈Rn×64, and biases b1 ∈R64, b2 ∈R64 and
204"
EXPERIMENTS AND RESULTS,0.37225274725274726,"b3 ∈Rn. Given an input x ∈Rd, the output of the gating network g(x) = (g1(x), . . . , gn(x)) is
205"
EXPERIMENTS AND RESULTS,0.37362637362637363,"computed as follows: first, we compute f0(x) = tanh(W2 ReLU(W1x + b1) + b2). Then, when
206"
EXPERIMENTS AND RESULTS,0.375,"we want the ϵ-LDP condition to be satisfied, we ensure that the outputs are between −ϵ/4 and ϵ/4:
207"
EXPERIMENTS AND RESULTS,0.37637362637362637,f(x) =
EXPERIMENTS AND RESULTS,0.37774725274725274,"(
ϵW3f0(x)
4∥f0(x)∥∥W3∥F
if the gating network must satisfy ϵ-LDP
W3f0(x)
otherwise."
EXPERIMENTS AND RESULTS,0.3791208791208791,"Note that tanh is the hyperbolic tangent activation function, ReLU the Rectified Linear Unit function,
208"
EXPERIMENTS AND RESULTS,0.3804945054945055,"∥W3∥F the Frobenius norm of the matrix W3, and ∥f0(x)∥the euclidean norm of the vector f0(x).
209"
EXPERIMENTS AND RESULTS,0.38186813186813184,"Indeed, if we let Wi
3 denote the i-th row of W3, then the i-th component of W3f0(x) is
210"
EXPERIMENTS AND RESULTS,0.38324175824175827,"Wi
3 · f0(x) ≤∥Wi
3∥∥f0(x)∥≤∥W3∥F ∥f0(x)∥,"
EXPERIMENTS AND RESULTS,0.38461538461538464,"by the Cauchy-Schwarz inequality and the definition of the Frobenius norm. The reason we use
211"
EXPERIMENTS AND RESULTS,0.385989010989011,"the Frobenius norm instead of directly using ∥Wi
3∥is to preserve the proportions between the
212"
EXPERIMENTS AND RESULTS,0.3873626373626374,"components of W3f0(x) when setting up ϵ-LDP.
213"
EXPERIMENTS AND RESULTS,0.38873626373626374,"The final output of the gating network is given by
214"
EXPERIMENTS AND RESULTS,0.3901098901098901,"gi(x) =
exp(fi(x) + (b3)i)
Pn
k=1 exp(fk(x) + (b3)k)
for all
i ∈[n]."
EXPERIMENTS AND RESULTS,0.3914835164835165,"In our experiments, we ran the Stochastic Gradient Descent algorithm 10 times with a learning rate
215"
EXPERIMENTS AND RESULTS,0.39285714285714285,"fixed to 0.1. In each experiment, we trained the model for 1000 epochs, except for the MNIST
216"
EXPERIMENTS AND RESULTS,0.3942307692307692,"dataset, where the training duration was shortened to 300 epochs due to dataset size. We allocated
217"
EXPERIMENTS AND RESULTS,0.3956043956043956,"approximately 75% of the data to the training set and the remaining 25% to the test set. At the
218"
EXPERIMENTS AND RESULTS,0.39697802197802196,"outset of each experiment, the weights of our neural networks were reinitialized to ensure a fresh
219"
EXPERIMENTS AND RESULTS,0.3983516483516483,"starting point. After each training run, we computed both the training and test loss values to evaluate
220"
EXPERIMENTS AND RESULTS,0.39972527472527475,"the model’s performance. We first ran the training without imposing any constraints on the gating
221"
EXPERIMENTS AND RESULTS,0.4010989010989011,"network, except for the architecture. Then, we ran several experiments with a gating mechanism
222"
EXPERIMENTS AND RESULTS,0.4024725274725275,"satisfying ϵ-LDP, with ϵ ∈{0.5, 2, 4, 5, 10}. A summary of the results is shown in Table 1. One can
223"
EXPERIMENTS AND RESULTS,0.40384615384615385,"observe that regularization with ϵ-LDP improves results in practice, and this regularization is even
224"
EXPERIMENTS AND RESULTS,0.4052197802197802,"more evident when the models employing a gating network not satisfying LDP overfit heavily, as in
225"
EXPERIMENTS AND RESULTS,0.4065934065934066,"the Breast Cancer and Heart experiments. The regularization effect is slightly less pronounced on
226"
EXPERIMENTS AND RESULTS,0.40796703296703296,"MNIST, where the overfitting is not as severe as with the previous datasets. We can also observe the
227"
EXPERIMENTS AND RESULTS,0.40934065934065933,"importance of choosing the right hyperparameter ϵ. Indeed, if the value is too small, the output of the
228"
EXPERIMENTS AND RESULTS,0.4107142857142857,"gating network becomes insufficiently dependent on the input x. In this case, the experts have to do
229"
EXPERIMENTS AND RESULTS,0.41208791208791207,"all the work, and the gating network does not allow them to specialize in well-defined subsets of the
230"
EXPERIMENTS AND RESULTS,0.41346153846153844,"instance space. This makes our model closer to a weighted sum of linear classifiers and significantly
231"
EXPERIMENTS AND RESULTS,0.41483516483516486,"reduces its performance. Conversely, if ϵ is overly large, our model tends towards a situation where
232"
EXPERIMENTS AND RESULTS,0.41620879120879123,"the LDP condition does not hold, making it prone to overfitting.
233"
EXPERIMENTS AND RESULTS,0.4175824175824176,"Note that our experiments are executed on GPUs in order to parallelize computations and take
234"
EXPERIMENTS AND RESULTS,0.41895604395604397,"advantage of the sparsity of our model, but they can also be performed without GPUs. The duration
235"
EXPERIMENTS AND RESULTS,0.42032967032967034,"of experiments can range from a few minutes for small datasets such as Breast Cancer to around 3
236"
EXPERIMENTS AND RESULTS,0.4217032967032967,"hours for large datasets like MNIST.
237"
CONCLUSION,0.4230769230769231,"6
Conclusion
238"
CONCLUSION,0.42445054945054944,"In this work, we introduce a new way to regularize mixtures of experts. We provide both theoretical
239"
CONCLUSION,0.4258241758241758,"and algorithmic contributions in this regard. Our approach offers a significant advantage in that
240"
CONCLUSION,0.4271978021978022,"it allows us to harness the remarkable performances of neural networks by using them as gating
241"
CONCLUSION,0.42857142857142855,"networks, without being constrained by their architecture or their complexity from the theoretical
242"
CONCLUSION,0.42994505494505497,"point of view. By imposing LDP, we obtain nonvacuous bounds on the mixture of experts’ risk. Our
243"
CONCLUSION,0.43131868131868134,"bounds can become significantly tighter than those presented in section 3.1 and those presented in
244"
CONCLUSION,0.4326923076923077,"2If N denotes the number of runs, Rk denotes the training or test empirical risk during the k-th run, and ¯R"
CONCLUSION,0.4340659340659341,"denotes the average, then standard deviation is given by
q"
"N
PN",0.43543956043956045,"1
N
PN
k=1(Rk −¯R)2."
"N
PN",0.4368131868131868,"Table 1: Experiment results for mixtures of 100 linear models applied to binary classification tasks:
Ads, Breast Cancer [Zwitter and Soklic, 1988], Heart [Janosi et al., 1988] and MNIST [Deng, 2012].
The objective is to minimize the empirical risk as defined in Equation 1. We report the mean training
loss (RS) and mean test loss (RT ), averaged over ten runs, along with their associated standard
deviations.2"
"N
PN",0.4381868131868132,"MoE with a gating network satisfying ϵ-LDP
Dataset
Risk
No LDP
ϵ = 0.5
ϵ = 2
ϵ = 4
ϵ = 5
ϵ = 10
Ads
RS
0.02425
0.13854
0.01829
0.05288
0.06459
0.02811
±
0.00499
0.00261
0.00216
0.05543
0.05821
0.03648
RT
0.03822
0.13051
0.03206
0.06693
0.07757
0.04384
±
0.00696
0.01138
0.00564
0.05276
0.05822
0.03501
Breast
RS
0.00780
0.04520
0.01252
0.01062
0.01089
0.01207
Cancer
±
0.00347
0.00426
0.00182
0.00286
0.00193
0.00181
RT
0.03617
0.04930
0.03238
0.03297
0.02942
0.02604
±
0.01505
0.01244
0.01349
0.01379
0.00948
0.01277
Heart
RS
0.00001
0.03524
0.00015
0.00010
0.00009
0.00013
±
0.00000
0.00487
0.00002
0.00001
0.00001
0.00006
RT
0.00029
0.03962
0.00026
0.00026
0.00032
0.00032
±
0.00065
0.01013
0.00014
0.00033
0.00030
0.00032
MNIST
RS
0.00525
0.00558
0.00529
0.00504
0.00536
0.00523
0 vs 8
±
0.00029
0.00059
0.00044
0.00031
0.00031
0.00032
RT
0.00844
0.00869
0.00815
0.00864
0.00769
0.00802
±
0.00103
0.00109
0.00131
0.00165
0.00144
0.00067
MNIST
RS
0.00287
0.00330
0.00289
0.00285
0.00298
0.00286
1 vs 7
±
0.00024
0.00033
0.00028
0.00025
0.00023
0.00013
RT
0.00501
0.00485
0.00501
0.00518
0.00450
0.00526
±
0.00042
0.00101
0.00093
0.00098
0.00101
0.00066
MNIST
RS
0.01419
0.01509
0.01388
0.01396
0.01440
0.01154
5 vs 6
±
0.00046
0.00057
0.00038
0.00051
0.00056
0.00336
RT
0.02195
0.02131
0.02206
0.02236
0.02072
0.01852
±
0.00111
0.00160
0.00185
0.00269
0.00229
0.00518"
"N
PN",0.43956043956043955,"Azran and Meir [2004], especially in cases where the empirical risk is close to zero and ϵ < ln n.
245"
"N
PN",0.4409340659340659,"However, as the empirical risk is multiplied by eϵ, the bounds can become loose when ϵ is large and
246"
"N
PN",0.4423076923076923,"the empirical risk is significant.
247"
"N
PN",0.44368131868131866,"Even though the ϵ-LDP condition is easy to set up, a challenge arises in striking a balance between
248"
"N
PN",0.44505494505494503,"the parameter ϵ and the KL divergence or the Rademacher complexity of our experts. Our method
249"
"N
PN",0.44642857142857145,"introduces an extra hyperparameter ϵ to optimize but does not provide theoretical guidance on
250"
"N
PN",0.4478021978021978,"configuring it. This forces us to navigate a trade-off between the value of ϵ, which measures the
251"
"N
PN",0.4491758241758242,"extent to which the output of the gating network can depend on a given x ∈X, and the complexity
252"
"N
PN",0.45054945054945056,"of our experts, which reflects how well our model captures the data distribution. Finding the right
253"
"N
PN",0.4519230769230769,"balance requires empirical testing and careful consideration and can open up new avenues of study in
254"
"N
PN",0.4532967032967033,"the future.
255"
REFERENCES,0.45467032967032966,"References
256"
REFERENCES,0.45604395604395603,"Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures
257"
REFERENCES,0.4574175824175824,"of local experts. Neural Computation, 3(1):79–87, 1991. doi: 10.1162/neco.1991.3.1.79.
258"
REFERENCES,0.45879120879120877,"Nicolas Städler, Peter Bühlmann, and Sara van de Geer. l1-penalization for mixture regression models
259"
REFERENCES,0.46016483516483514,"(with discussion). TEST, 19(2):209–285, 2010. doi: 10.1007/s11749-010-0197-z.
260"
REFERENCES,0.46153846153846156,"Abbas Khalili and Shili Lin. Regularization in finite mixture of regression models with diverging
261"
REFERENCES,0.46291208791208793,"number of parameters. Biometrics, 69, 04 2013. doi: 10.1111/biom.12020.
262"
REFERENCES,0.4642857142857143,"Seniha Yuksel, Joseph Wilson, and Paul Gader. Twenty years of mixture of experts, 08 2012.
263"
REFERENCES,0.46565934065934067,"M.I. Jordan and R.A. Jacobs. Hierarchical mixtures of experts and the em algorithm. In Proceedings
264"
REFERENCES,0.46703296703296704,"of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan), volume 2,
265"
REFERENCES,0.4684065934065934,"pages 1339–1344 vol.2, 1993. doi: 10.1109/IJCNN.1993.716791.
266"
REFERENCES,0.4697802197802198,"Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and
267"
REFERENCES,0.47115384615384615,"Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017.
268"
REFERENCES,0.4725274725274725,"URL https://arxiv.org/abs/1701.06538.
269"
REFERENCES,0.4739010989010989,"William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
270"
REFERENCES,0.47527472527472525,"models with simple and efficient sparsity, 2022.
271"
REFERENCES,0.4766483516483517,"Arik Azran and Ron Meir. Data dependent risk bounds for hierarchical mixture of experts classifiers.
272"
REFERENCES,0.47802197802197804,"In John Shawe-Taylor and Yoram Singer, editors, Learning Theory, pages 427–441, Berlin,
273"
REFERENCES,0.4793956043956044,"Heidelberg, 2004. Springer Berlin Heidelberg. ISBN 978-3-540-27819-1.
274"
REFERENCES,0.4807692307692308,"Cynthia Dwork. Differential privacy. In Michele Bugliesi, Bart Preneel, Vladimiro Sassone, and Ingo
275"
REFERENCES,0.48214285714285715,"Wegener, editors, Automata, Languages and Programming, pages 1–12, Berlin, Heidelberg, 2006.
276"
REFERENCES,0.4835164835164835,"Springer Berlin Heidelberg. ISBN 978-3-540-35908-1.
277"
REFERENCES,0.4848901098901099,"Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam
278"
REFERENCES,0.48626373626373626,"Smith. What can we learn privately?, 2010.
279"
REFERENCES,0.4876373626373626,"David McAllester. A PAC-bayesian tutorial with a dropout bound, 2013.
280"
REFERENCES,0.489010989010989,"Matjaz Zwitter and Milan Soklic. Breast Cancer. UCI Machine Learning Repository, 1988. DOI:
281"
REFERENCES,0.49038461538461536,"https://doi.org/10.24432/C51P4M.
282"
REFERENCES,0.49175824175824173,"Andras Janosi, William Steinbrunn, Matthias Pfisterer, and Robert Detrano. Heart Disease. UCI
283"
REFERENCES,0.49313186813186816,"Machine Learning Repository, 1988. DOI: https://doi.org/10.24432/C52P4X.
284"
REFERENCES,0.4945054945054945,"Li Deng. The MNIST database of handwritten digit images for machine learning research. IEEE
285"
REFERENCES,0.4958791208791209,"Signal Processing Magazine, 29(6):141–142, 2012.
286"
REFERENCES,0.49725274725274726,"Michael D. Perlman.
Jensen’s inequality for a convex vector-valued function on an infinite-
287"
REFERENCES,0.49862637362637363,"dimensional space. Journal of Multivariate Analysis, 4(1):52–65, 1974. ISSN 0047-259X. doi:
288"
REFERENCES,0.5,"https://doi.org/10.1016/0047-259X(74)90005-0.
URL https://www.sciencedirect.com/
289"
REFERENCES,0.5013736263736264,"science/article/pii/0047259X74900050.
290"
REFERENCES,0.5027472527472527,"Andreas Maurer. A note on the PAC bayesian theorem, 2004.
291"
REFERENCES,0.5041208791208791,"A
Proofs and auxiliary results
292"
REFERENCES,0.5054945054945055,"Proof of theorem 2.2. Given x ∈X, let Z(x) = Pn
i=1 exp(βfi(x) + ci), for convenience.
293"
REFERENCES,0.5068681318681318,"For all x, x′ ∈X and all i ∈[n], we have that
294"
REFERENCES,0.5082417582417582,"gi(x)
gi(x′) = exp
 
β(fi(x) −fi(x′))

1
Z(x) n
X"
REFERENCES,0.5096153846153846,"k=1
exp(βfk(x′) + ck)"
REFERENCES,0.510989010989011,"= exp
 
β(fi(x) −fi(x′))

1
Z(x) n
X"
REFERENCES,0.5123626373626373,"k=1
exp(βfk(x) + ck) exp
 
β(fk(x′) −fk(x))
"
REFERENCES,0.5137362637362637,"≤
max
i∈[n]; x1,x2∈X exp
 
2β(fi(x1) −fi(x2))

1
Z(x) n
X"
REFERENCES,0.5151098901098901,"k=1
exp(βfk(x) + ck)"
REFERENCES,0.5164835164835165,"≤exp(4βb).
Theorem A.1 (Jensen’s inequality, proposition 1.1 in Perlman [1974]). Let Ωbe a probability space,
295"
REFERENCES,0.5178571428571429,"let A be a convex subset of Rk, let X : Ω→A be an integrable vector-valued random variable, and
296"
REFERENCES,0.5192307692307693,"let ϕ : A →R be a convex function. Then, EX ∈A, and ϕ(EX) ≤E ϕ(X) (in particular, the
297"
REFERENCES,0.5206043956043956,"right-hand side of this inequality exists, though it may be infinite).
298"
REFERENCES,0.521978021978022,"Theorem A.2 (Theorem 5 in Maurer [2004]). Let δ ∈(0, 1) and m ≥8. Fix i ∈[n], and let Pi be a
299"
REFERENCES,0.5233516483516484,"probability measure on Hi (chosen without seeing the training data). Then, with probability at least
300"
REFERENCES,0.5247252747252747,"1 −δ over the draws of S, for all probability measures Qi on Hi, we have that
301"
REFERENCES,0.5260989010989011,kl(RS(Qi)∥R(Qi)) ≤1 m
REFERENCES,0.5274725274725275,"
KL(Qi ∥Pi) + ln 2√m δ 
."
REFERENCES,0.5288461538461539,"Proof of theorem 3.4. As remarked earlier, the function (u, v) −→kl(u∥v) : [0, 1]2 →R does not
302"
REFERENCES,0.5302197802197802,"exactly satisfy the hypotheses of lemma 3.1, but it is convex. Moreover, on {(u, v) ∈[0, 1]2 |u ≤v},
303"
REFERENCES,0.5315934065934066,"it is decreasing in its first argument and increasing in its second argument. Also note that, assuming
304"
REFERENCES,0.532967032967033,"that R(g, Q) ≥e2ϵRS(g, Q), then we also have the following inequalities:
305"
REFERENCES,0.5343406593406593,"0 ≤
E
i∼g(x′) RS(Qi) ≤eϵRS(g, Q) ≤e−ϵR(g, Q) ≤
E
i∼g(x′) R(Qi) ≤1."
REFERENCES,0.5357142857142857,"It follows that
306"
REFERENCES,0.5370879120879121,"kl(eϵRS(g, Q)∥e−ϵR(g, Q)) ≤kl

E
i∼g(x′) RS(Qi)
 e−ϵR(g, Q)
"
REFERENCES,0.5384615384615384,"≤kl

E
i∼g(x′) RS(Qi)

E
i∼g(x′) R(Qi)

,"
REFERENCES,0.5398351648351648,"and therefore
307"
REFERENCES,0.5412087912087912,"kl(eϵRS(g, Q)∥e−ϵR(g, Q)) ≤
E
i∼g(x′) kl
 
RS(Qi)∥R(Qi)
"
REFERENCES,0.5425824175824175,"by Jensen’s inequality. Now, by theorem A.2, for a fixed i, with probability at least 1 −δ/n, we have
308"
REFERENCES,0.5439560439560439,"that
309"
REFERENCES,0.5453296703296703,kl(RS(Qi)∥R(Qi)) ≤1 m
REFERENCES,0.5467032967032966,"
KL(Qi ∥Pi) + ln 2n√m δ 
."
REFERENCES,0.5480769230769231,"We can make the above inequality hold for all i ∈[n] simultaneously with the union bound. Then,
310"
REFERENCES,0.5494505494505495,"with probability at least 1 −δ, for all (g, Q), given that R(g, Q) ≥e2ϵRS(g, Q), we have that
311"
REFERENCES,0.5508241758241759,"kl(eϵRS(g, Q)∥e−ϵR(g, Q)) ≤1 m"
REFERENCES,0.5521978021978022,"
E
i∼g(x′) KL(Qi ∥Pi) + ln 2n√m δ 
."
REFERENCES,0.5535714285714286,"Proof of Lemma 4.1. Since the gating function satisfies ϵ-LDP, we have that e−ϵgi(x′) ≤gi(x) ≤
312"
REFERENCES,0.554945054945055,"eϵgi(x′) for all x, x′ ∈X and all i ∈[n]. It follows that eϵRS(g, h) ≥Ei∼g(x′) RS(hi) and
313"
REFERENCES,0.5563186813186813,"e−ϵR(g, h) ≤Ei∼g(x′) R(hi). Given that ∆is decreasing in its first argument and increasing in its
314"
REFERENCES,0.5576923076923077,"second argument, we find that
315"
REFERENCES,0.5590659340659341,"∆
 
eϵRS(g, h), e−ϵR(g, h)

≤∆

E
i∼g(x′) RS(hi),
E
i∼g(x′) R(hi)
"
REFERENCES,0.5604395604395604,"Since ∆is a convex function, we can apply Jensen’s inequality to the expression on the right-hand
316"
REFERENCES,0.5618131868131868,"side, yielding the desired result.
317"
REFERENCES,0.5631868131868132,"NeurIPS Paper Checklist
318"
CLAIMS,0.5645604395604396,"1. Claims
319"
CLAIMS,0.5659340659340659,"Question: Do the main claims made in the abstract and introduction accurately reflect the
320"
CLAIMS,0.5673076923076923,"paper’s contributions and scope?
321"
CLAIMS,0.5686813186813187,"Answer: [Yes]
322"
CLAIMS,0.570054945054945,"Justification: Our claims are supported by theorems, which we prove, and by experiments.
323"
CLAIMS,0.5714285714285714,"Guidelines:
324"
CLAIMS,0.5728021978021978,"• The answer NA means that the abstract and introduction do not include the claims
325"
CLAIMS,0.5741758241758241,"made in the paper.
326"
CLAIMS,0.5755494505494505,"• The abstract and/or introduction should clearly state the claims made, including the
327"
CLAIMS,0.5769230769230769,"contributions made in the paper and important assumptions and limitations. A No or
328"
CLAIMS,0.5782967032967034,"NA answer to this question will not be perceived well by the reviewers.
329"
CLAIMS,0.5796703296703297,"• The claims made should match theoretical and experimental results, and reflect how
330"
CLAIMS,0.5810439560439561,"much the results can be expected to generalize to other settings.
331"
CLAIMS,0.5824175824175825,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
332"
CLAIMS,0.5837912087912088,"are not attained by the paper.
333"
LIMITATIONS,0.5851648351648352,"2. Limitations
334"
LIMITATIONS,0.5865384615384616,"Question: Does the paper discuss the limitations of the work performed by the authors?
335"
LIMITATIONS,0.5879120879120879,"Answer: [Yes]
336"
LIMITATIONS,0.5892857142857143,"Justification: We discuss the limitations of the work in the conclusion section.
337"
LIMITATIONS,0.5906593406593407,"Guidelines:
338"
LIMITATIONS,0.592032967032967,"• The answer NA means that the paper has no limitation while the answer No means that
339"
LIMITATIONS,0.5934065934065934,"the paper has limitations, but those are not discussed in the paper.
340"
LIMITATIONS,0.5947802197802198,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
341"
LIMITATIONS,0.5961538461538461,"• The paper should point out any strong assumptions and how robust the results are to
342"
LIMITATIONS,0.5975274725274725,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
343"
LIMITATIONS,0.5989010989010989,"model well-specification, asymptotic approximations only holding locally). The authors
344"
LIMITATIONS,0.6002747252747253,"should reflect on how these assumptions might be violated in practice and what the
345"
LIMITATIONS,0.6016483516483516,"implications would be.
346"
LIMITATIONS,0.603021978021978,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
347"
LIMITATIONS,0.6043956043956044,"only tested on a few datasets or with a few runs. In general, empirical results often
348"
LIMITATIONS,0.6057692307692307,"depend on implicit assumptions, which should be articulated.
349"
LIMITATIONS,0.6071428571428571,"• The authors should reflect on the factors that influence the performance of the approach.
350"
LIMITATIONS,0.6085164835164835,"For example, a facial recognition algorithm may perform poorly when image resolution
351"
LIMITATIONS,0.6098901098901099,"is low or images are taken in low lighting. Or a speech-to-text system might not be
352"
LIMITATIONS,0.6112637362637363,"used reliably to provide closed captions for online lectures because it fails to handle
353"
LIMITATIONS,0.6126373626373627,"technical jargon.
354"
LIMITATIONS,0.614010989010989,"• The authors should discuss the computational efficiency of the proposed algorithms
355"
LIMITATIONS,0.6153846153846154,"and how they scale with dataset size.
356"
LIMITATIONS,0.6167582417582418,"• If applicable, the authors should discuss possible limitations of their approach to
357"
LIMITATIONS,0.6181318681318682,"address problems of privacy and fairness.
358"
LIMITATIONS,0.6195054945054945,"• While the authors might fear that complete honesty about limitations might be used by
359"
LIMITATIONS,0.6208791208791209,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
360"
LIMITATIONS,0.6222527472527473,"limitations that aren’t acknowledged in the paper. The authors should use their best
361"
LIMITATIONS,0.6236263736263736,"judgment and recognize that individual actions in favor of transparency play an impor-
362"
LIMITATIONS,0.625,"tant role in developing norms that preserve the integrity of the community. Reviewers
363"
LIMITATIONS,0.6263736263736264,"will be specifically instructed to not penalize honesty concerning limitations.
364"
THEORY ASSUMPTIONS AND PROOFS,0.6277472527472527,"3. Theory Assumptions and Proofs
365"
THEORY ASSUMPTIONS AND PROOFS,0.6291208791208791,"Question: For each theoretical result, does the paper provide the full set of assumptions and
366"
THEORY ASSUMPTIONS AND PROOFS,0.6304945054945055,"a complete (and correct) proof?
367"
THEORY ASSUMPTIONS AND PROOFS,0.6318681318681318,"Answer: [Yes]
368"
THEORY ASSUMPTIONS AND PROOFS,0.6332417582417582,"Justification: All theoretical results are proved, either in the main paper or in the appendix.
369"
THEORY ASSUMPTIONS AND PROOFS,0.6346153846153846,"Moreover, all assumptions necessary for our theorems to hold are mentioned in their
370"
THEORY ASSUMPTIONS AND PROOFS,0.635989010989011,"statement.
371"
THEORY ASSUMPTIONS AND PROOFS,0.6373626373626373,"Guidelines:
372"
THEORY ASSUMPTIONS AND PROOFS,0.6387362637362637,"• The answer NA means that the paper does not include theoretical results.
373"
THEORY ASSUMPTIONS AND PROOFS,0.6401098901098901,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
374"
THEORY ASSUMPTIONS AND PROOFS,0.6414835164835165,"referenced.
375"
THEORY ASSUMPTIONS AND PROOFS,0.6428571428571429,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
376"
THEORY ASSUMPTIONS AND PROOFS,0.6442307692307693,"• The proofs can either appear in the main paper or the supplemental material, but if
377"
THEORY ASSUMPTIONS AND PROOFS,0.6456043956043956,"they appear in the supplemental material, the authors are encouraged to provide a short
378"
THEORY ASSUMPTIONS AND PROOFS,0.646978021978022,"proof sketch to provide intuition.
379"
THEORY ASSUMPTIONS AND PROOFS,0.6483516483516484,"• Inversely, any informal proof provided in the core of the paper should be complemented
380"
THEORY ASSUMPTIONS AND PROOFS,0.6497252747252747,"by formal proofs provided in appendix or supplemental material.
381"
THEORY ASSUMPTIONS AND PROOFS,0.6510989010989011,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
382"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6524725274725275,"4. Experimental Result Reproducibility
383"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6538461538461539,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
384"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6552197802197802,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
385"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6565934065934066,"of the paper (regardless of whether the code and data are provided or not)?
386"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.657967032967033,"Answer: [Yes]
387"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6593406593406593,"Justification: All datasets used are freely available, and we describe the architecture of the
388"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6607142857142857,"model and the hyperparameters of our experiments in detail. We also provide the code used
389"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6620879120879121,"to run our experiments as supplemental material.
390"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6634615384615384,"Guidelines:
391"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6648351648351648,"• The answer NA means that the paper does not include experiments.
392"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6662087912087912,"• If the paper includes experiments, a No answer to this question will not be perceived
393"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6675824175824175,"well by the reviewers: Making the paper reproducible is important, regardless of
394"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6689560439560439,"whether the code and data are provided or not.
395"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6703296703296703,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
396"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6717032967032966,"to make their results reproducible or verifiable.
397"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6730769230769231,"• Depending on the contribution, reproducibility can be accomplished in various ways.
398"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6744505494505495,"For example, if the contribution is a novel architecture, describing the architecture fully
399"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6758241758241759,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
400"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6771978021978022,"be necessary to either make it possible for others to replicate the model with the same
401"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6785714285714286,"dataset, or provide access to the model. In general. releasing code and data is often
402"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.679945054945055,"one good way to accomplish this, but reproducibility can also be provided via detailed
403"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6813186813186813,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
404"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6826923076923077,"of a large language model), releasing of a model checkpoint, or other means that are
405"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6840659340659341,"appropriate to the research performed.
406"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6854395604395604,"• While NeurIPS does not require releasing code, the conference does require all submis-
407"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6868131868131868,"sions to provide some reasonable avenue for reproducibility, which may depend on the
408"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6881868131868132,"nature of the contribution. For example
409"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6895604395604396,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
410"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6909340659340659,"to reproduce that algorithm.
411"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6923076923076923,"(b) If the contribution is primarily a new model architecture, the paper should describe
412"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6936813186813187,"the architecture clearly and fully.
413"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.695054945054945,"(c) If the contribution is a new model (e.g., a large language model), then there should
414"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6964285714285714,"either be a way to access this model for reproducing the results or a way to reproduce
415"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6978021978021978,"the model (e.g., with an open-source dataset or instructions for how to construct
416"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6991758241758241,"the dataset).
417"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7005494505494505,"(d) We recognize that reproducibility may be tricky in some cases, in which case
418"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7019230769230769,"authors are welcome to describe the particular way they provide for reproducibility.
419"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7032967032967034,"In the case of closed-source models, it may be that access to the model is limited in
420"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7046703296703297,"some way (e.g., to registered users), but it should be possible for other researchers
421"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7060439560439561,"to have some path to reproducing or verifying the results.
422"
OPEN ACCESS TO DATA AND CODE,0.7074175824175825,"5. Open access to data and code
423"
OPEN ACCESS TO DATA AND CODE,0.7087912087912088,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
424"
OPEN ACCESS TO DATA AND CODE,0.7101648351648352,"tions to faithfully reproduce the main experimental results, as described in supplemental
425"
OPEN ACCESS TO DATA AND CODE,0.7115384615384616,"material?
426"
OPEN ACCESS TO DATA AND CODE,0.7129120879120879,"Answer: [Yes]
427"
OPEN ACCESS TO DATA AND CODE,0.7142857142857143,"Justification: All datasets used in our experiments are freely available. The code used to run
428"
OPEN ACCESS TO DATA AND CODE,0.7156593406593407,"our experiments is provided as supplemental material.
429"
OPEN ACCESS TO DATA AND CODE,0.717032967032967,"Guidelines:
430"
OPEN ACCESS TO DATA AND CODE,0.7184065934065934,"• The answer NA means that paper does not include experiments requiring code.
431"
OPEN ACCESS TO DATA AND CODE,0.7197802197802198,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
432"
OPEN ACCESS TO DATA AND CODE,0.7211538461538461,"public/guides/CodeSubmissionPolicy) for more details.
433"
OPEN ACCESS TO DATA AND CODE,0.7225274725274725,"• While we encourage the release of code and data, we understand that this might not be
434"
OPEN ACCESS TO DATA AND CODE,0.7239010989010989,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
435"
OPEN ACCESS TO DATA AND CODE,0.7252747252747253,"including code, unless this is central to the contribution (e.g., for a new open-source
436"
OPEN ACCESS TO DATA AND CODE,0.7266483516483516,"benchmark).
437"
OPEN ACCESS TO DATA AND CODE,0.728021978021978,"• The instructions should contain the exact command and environment needed to run to
438"
OPEN ACCESS TO DATA AND CODE,0.7293956043956044,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
439"
OPEN ACCESS TO DATA AND CODE,0.7307692307692307,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
440"
OPEN ACCESS TO DATA AND CODE,0.7321428571428571,"• The authors should provide instructions on data access and preparation, including how
441"
OPEN ACCESS TO DATA AND CODE,0.7335164835164835,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
442"
OPEN ACCESS TO DATA AND CODE,0.7348901098901099,"• The authors should provide scripts to reproduce all experimental results for the new
443"
OPEN ACCESS TO DATA AND CODE,0.7362637362637363,"proposed method and baselines. If only a subset of experiments are reproducible, they
444"
OPEN ACCESS TO DATA AND CODE,0.7376373626373627,"should state which ones are omitted from the script and why.
445"
OPEN ACCESS TO DATA AND CODE,0.739010989010989,"• At submission time, to preserve anonymity, the authors should release anonymized
446"
OPEN ACCESS TO DATA AND CODE,0.7403846153846154,"versions (if applicable).
447"
OPEN ACCESS TO DATA AND CODE,0.7417582417582418,"• Providing as much information as possible in supplemental material (appended to the
448"
OPEN ACCESS TO DATA AND CODE,0.7431318681318682,"paper) is recommended, but including URLs to data and code is permitted.
449"
OPEN ACCESS TO DATA AND CODE,0.7445054945054945,"6. Experimental Setting/Details
450"
OPEN ACCESS TO DATA AND CODE,0.7458791208791209,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
451"
OPEN ACCESS TO DATA AND CODE,0.7472527472527473,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
452"
OPEN ACCESS TO DATA AND CODE,0.7486263736263736,"results?
453"
OPEN ACCESS TO DATA AND CODE,0.75,"Answer: [Yes]
454"
OPEN ACCESS TO DATA AND CODE,0.7513736263736264,"Justification: Experimental details are provided in section 5.
455"
OPEN ACCESS TO DATA AND CODE,0.7527472527472527,"Guidelines:
456"
OPEN ACCESS TO DATA AND CODE,0.7541208791208791,"• The answer NA means that the paper does not include experiments.
457"
OPEN ACCESS TO DATA AND CODE,0.7554945054945055,"• The experimental setting should be presented in the core of the paper to a level of detail
458"
OPEN ACCESS TO DATA AND CODE,0.7568681318681318,"that is necessary to appreciate the results and make sense of them.
459"
OPEN ACCESS TO DATA AND CODE,0.7582417582417582,"• The full details can be provided either with the code, in appendix, or as supplemental
460"
OPEN ACCESS TO DATA AND CODE,0.7596153846153846,"material.
461"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.760989010989011,"7. Experiment Statistical Significance
462"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7623626373626373,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
463"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7637362637362637,"information about the statistical significance of the experiments?
464"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7651098901098901,"Answer: [Yes]
465"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7664835164835165,"Justification: In experiments, we had multiple runs and reported averages along with standard
466"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7678571428571429,"deviations.
467"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7692307692307693,"Guidelines:
468"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7706043956043956,"• The answer NA means that the paper does not include experiments.
469"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.771978021978022,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
470"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7733516483516484,"dence intervals, or statistical significance tests, at least for the experiments that support
471"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7747252747252747,"the main claims of the paper.
472"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7760989010989011,"• The factors of variability that the error bars are capturing should be clearly stated (for
473"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7774725274725275,"example, train/test split, initialization, random drawing of some parameter, or overall
474"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7788461538461539,"run with given experimental conditions).
475"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7802197802197802,"• The method for calculating the error bars should be explained (closed form formula,
476"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7815934065934066,"call to a library function, bootstrap, etc.)
477"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.782967032967033,"• The assumptions made should be given (e.g., Normally distributed errors).
478"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7843406593406593,"• It should be clear whether the error bar is the standard deviation or the standard error
479"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7857142857142857,"of the mean.
480"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7870879120879121,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
481"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7884615384615384,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
482"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7898351648351648,"of Normality of errors is not verified.
483"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7912087912087912,"• For asymmetric distributions, the authors should be careful not to show in tables or
484"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7925824175824175,"figures symmetric error bars that would yield results that are out of range (e.g. negative
485"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7939560439560439,"error rates).
486"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7953296703296703,"• If error bars are reported in tables or plots, The authors should explain in the text how
487"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7967032967032966,"they were calculated and reference the corresponding figures or tables in the text.
488"
EXPERIMENTS COMPUTE RESOURCES,0.7980769230769231,"8. Experiments Compute Resources
489"
EXPERIMENTS COMPUTE RESOURCES,0.7994505494505495,"Question: For each experiment, does the paper provide sufficient information on the com-
490"
EXPERIMENTS COMPUTE RESOURCES,0.8008241758241759,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
491"
EXPERIMENTS COMPUTE RESOURCES,0.8021978021978022,"the experiments?
492"
EXPERIMENTS COMPUTE RESOURCES,0.8035714285714286,"Answer: [Yes]
493"
EXPERIMENTS COMPUTE RESOURCES,0.804945054945055,"Justification: We provide information on the type of compute workers (GPU) and time of
494"
EXPERIMENTS COMPUTE RESOURCES,0.8063186813186813,"execution in section 5.
495"
EXPERIMENTS COMPUTE RESOURCES,0.8076923076923077,"Guidelines:
496"
EXPERIMENTS COMPUTE RESOURCES,0.8090659340659341,"• The answer NA means that the paper does not include experiments.
497"
EXPERIMENTS COMPUTE RESOURCES,0.8104395604395604,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
498"
EXPERIMENTS COMPUTE RESOURCES,0.8118131868131868,"or cloud provider, including relevant memory and storage.
499"
EXPERIMENTS COMPUTE RESOURCES,0.8131868131868132,"• The paper should provide the amount of compute required for each of the individual
500"
EXPERIMENTS COMPUTE RESOURCES,0.8145604395604396,"experimental runs as well as estimate the total compute.
501"
EXPERIMENTS COMPUTE RESOURCES,0.8159340659340659,"• The paper should disclose whether the full research project required more compute
502"
EXPERIMENTS COMPUTE RESOURCES,0.8173076923076923,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
503"
EXPERIMENTS COMPUTE RESOURCES,0.8186813186813187,"didn’t make it into the paper).
504"
CODE OF ETHICS,0.820054945054945,"9. Code Of Ethics
505"
CODE OF ETHICS,0.8214285714285714,"Question: Does the research conducted in the paper conform, in every respect, with the
506"
CODE OF ETHICS,0.8228021978021978,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
507"
CODE OF ETHICS,0.8241758241758241,"Answer: [Yes]
508"
CODE OF ETHICS,0.8255494505494505,"Justification: In this work, we provide theoretical results that do not have any direct negative
509"
CODE OF ETHICS,0.8269230769230769,"impact on society and do not pose any obvious ethical problem. We have ensured to cite our
510"
CODE OF ETHICS,0.8282967032967034,"sources and adhere to the NeurIPS Code of Ethics.
511"
CODE OF ETHICS,0.8296703296703297,"Guidelines:
512"
CODE OF ETHICS,0.8310439560439561,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
513"
CODE OF ETHICS,0.8324175824175825,"• If the authors answer No, they should explain the special circumstances that require a
514"
CODE OF ETHICS,0.8337912087912088,"deviation from the Code of Ethics.
515"
CODE OF ETHICS,0.8351648351648352,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
516"
CODE OF ETHICS,0.8365384615384616,"eration due to laws or regulations in their jurisdiction).
517"
BROADER IMPACTS,0.8379120879120879,"10. Broader Impacts
518"
BROADER IMPACTS,0.8392857142857143,"Question: Does the paper discuss both potential positive societal impacts and negative
519"
BROADER IMPACTS,0.8406593406593407,"societal impacts of the work performed?
520"
BROADER IMPACTS,0.842032967032967,"Answer: [NA]
521"
BROADER IMPACTS,0.8434065934065934,"Justification: Our paper has a theoretical orientation and no clear negative impacts.
522"
BROADER IMPACTS,0.8447802197802198,"Guidelines:
523"
BROADER IMPACTS,0.8461538461538461,"• The answer NA means that there is no societal impact of the work performed.
524"
BROADER IMPACTS,0.8475274725274725,"• If the authors answer NA or No, they should explain why their work has no societal
525"
BROADER IMPACTS,0.8489010989010989,"impact or why the paper does not address societal impact.
526"
BROADER IMPACTS,0.8502747252747253,"• Examples of negative societal impacts include potential malicious or unintended uses
527"
BROADER IMPACTS,0.8516483516483516,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
528"
BROADER IMPACTS,0.853021978021978,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
529"
BROADER IMPACTS,0.8543956043956044,"groups), privacy considerations, and security considerations.
530"
BROADER IMPACTS,0.8557692307692307,"• The conference expects that many papers will be foundational research and not tied
531"
BROADER IMPACTS,0.8571428571428571,"to particular applications, let alone deployments. However, if there is a direct path to
532"
BROADER IMPACTS,0.8585164835164835,"any negative applications, the authors should point it out. For example, it is legitimate
533"
BROADER IMPACTS,0.8598901098901099,"to point out that an improvement in the quality of generative models could be used to
534"
BROADER IMPACTS,0.8612637362637363,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
535"
BROADER IMPACTS,0.8626373626373627,"that a generic algorithm for optimizing neural networks could enable people to train
536"
BROADER IMPACTS,0.864010989010989,"models that generate Deepfakes faster.
537"
BROADER IMPACTS,0.8653846153846154,"• The authors should consider possible harms that could arise when the technology is
538"
BROADER IMPACTS,0.8667582417582418,"being used as intended and functioning correctly, harms that could arise when the
539"
BROADER IMPACTS,0.8681318681318682,"technology is being used as intended but gives incorrect results, and harms following
540"
BROADER IMPACTS,0.8695054945054945,"from (intentional or unintentional) misuse of the technology.
541"
BROADER IMPACTS,0.8708791208791209,"• If there are negative societal impacts, the authors could also discuss possible mitigation
542"
BROADER IMPACTS,0.8722527472527473,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
543"
BROADER IMPACTS,0.8736263736263736,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
544"
BROADER IMPACTS,0.875,"feedback over time, improving the efficiency and accessibility of ML).
545"
SAFEGUARDS,0.8763736263736264,"11. Safeguards
546"
SAFEGUARDS,0.8777472527472527,"Question: Does the paper describe safeguards that have been put in place for responsible
547"
SAFEGUARDS,0.8791208791208791,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
548"
SAFEGUARDS,0.8804945054945055,"image generators, or scraped datasets)?
549"
SAFEGUARDS,0.8818681318681318,"Answer: [NA]
550"
SAFEGUARDS,0.8832417582417582,"Justification: Our models are trained on freely available datasets as a way to prove the
551"
SAFEGUARDS,0.8846153846153846,"efficacy of our approach, but they pose no risk as such.
552"
SAFEGUARDS,0.885989010989011,"Guidelines:
553"
SAFEGUARDS,0.8873626373626373,"• The answer NA means that the paper poses no such risks.
554"
SAFEGUARDS,0.8887362637362637,"• Released models that have a high risk for misuse or dual-use should be released with
555"
SAFEGUARDS,0.8901098901098901,"necessary safeguards to allow for controlled use of the model, for example by requiring
556"
SAFEGUARDS,0.8914835164835165,"that users adhere to usage guidelines or restrictions to access the model or implementing
557"
SAFEGUARDS,0.8928571428571429,"safety filters.
558"
SAFEGUARDS,0.8942307692307693,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
559"
SAFEGUARDS,0.8956043956043956,"should describe how they avoided releasing unsafe images.
560"
SAFEGUARDS,0.896978021978022,"• We recognize that providing effective safeguards is challenging, and many papers do
561"
SAFEGUARDS,0.8983516483516484,"not require this, but we encourage authors to take this into account and make a best
562"
SAFEGUARDS,0.8997252747252747,"faith effort.
563"
LICENSES FOR EXISTING ASSETS,0.9010989010989011,"12. Licenses for existing assets
564"
LICENSES FOR EXISTING ASSETS,0.9024725274725275,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
565"
LICENSES FOR EXISTING ASSETS,0.9038461538461539,"the paper, properly credited and are the license and terms of use explicitly mentioned and
566"
LICENSES FOR EXISTING ASSETS,0.9052197802197802,"properly respected?
567"
LICENSES FOR EXISTING ASSETS,0.9065934065934066,"Answer: [Yes]
568"
LICENSES FOR EXISTING ASSETS,0.907967032967033,"Justification: We use openly accessible datasets and ensure to cite them properly when
569"
LICENSES FOR EXISTING ASSETS,0.9093406593406593,"necessary.
570"
LICENSES FOR EXISTING ASSETS,0.9107142857142857,"Guidelines:
571"
LICENSES FOR EXISTING ASSETS,0.9120879120879121,"• The answer NA means that the paper does not use existing assets.
572"
LICENSES FOR EXISTING ASSETS,0.9134615384615384,"• The authors should cite the original paper that produced the code package or dataset.
573"
LICENSES FOR EXISTING ASSETS,0.9148351648351648,"• The authors should state which version of the asset is used and, if possible, include a
574"
LICENSES FOR EXISTING ASSETS,0.9162087912087912,"URL.
575"
LICENSES FOR EXISTING ASSETS,0.9175824175824175,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
576"
LICENSES FOR EXISTING ASSETS,0.9189560439560439,"• For scraped data from a particular source (e.g., website), the copyright and terms of
577"
LICENSES FOR EXISTING ASSETS,0.9203296703296703,"service of that source should be provided.
578"
LICENSES FOR EXISTING ASSETS,0.9217032967032966,"• If assets are released, the license, copyright information, and terms of use in the
579"
LICENSES FOR EXISTING ASSETS,0.9230769230769231,"package should be provided. For popular datasets, paperswithcode.com/datasets
580"
LICENSES FOR EXISTING ASSETS,0.9244505494505495,"has curated licenses for some datasets. Their licensing guide can help determine the
581"
LICENSES FOR EXISTING ASSETS,0.9258241758241759,"license of a dataset.
582"
LICENSES FOR EXISTING ASSETS,0.9271978021978022,"• For existing datasets that are re-packaged, both the original license and the license of
583"
LICENSES FOR EXISTING ASSETS,0.9285714285714286,"the derived asset (if it has changed) should be provided.
584"
LICENSES FOR EXISTING ASSETS,0.929945054945055,"• If this information is not available online, the authors are encouraged to reach out to
585"
LICENSES FOR EXISTING ASSETS,0.9313186813186813,"the asset’s creators.
586"
NEW ASSETS,0.9326923076923077,"13. New Assets
587"
NEW ASSETS,0.9340659340659341,"Question: Are new assets introduced in the paper well documented and is the documentation
588"
NEW ASSETS,0.9354395604395604,"provided alongside the assets?
589"
NEW ASSETS,0.9368131868131868,"Answer: [Yes]
590"
NEW ASSETS,0.9381868131868132,"Justification: The code is provided as supplemental material and the details are given in the
591"
NEW ASSETS,0.9395604395604396,"README file.
592"
NEW ASSETS,0.9409340659340659,"Guidelines:
593"
NEW ASSETS,0.9423076923076923,"• The answer NA means that the paper does not release new assets.
594"
NEW ASSETS,0.9436813186813187,"• Researchers should communicate the details of the dataset/code/model as part of their
595"
NEW ASSETS,0.945054945054945,"submissions via structured templates. This includes details about training, license,
596"
NEW ASSETS,0.9464285714285714,"limitations, etc.
597"
NEW ASSETS,0.9478021978021978,"• The paper should discuss whether and how consent was obtained from people whose
598"
NEW ASSETS,0.9491758241758241,"asset is used.
599"
NEW ASSETS,0.9505494505494505,"• At submission time, remember to anonymize your assets (if applicable). You can either
600"
NEW ASSETS,0.9519230769230769,"create an anonymized URL or include an anonymized zip file.
601"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9532967032967034,"14. Crowdsourcing and Research with Human Subjects
602"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9546703296703297,"Question: For crowdsourcing experiments and research with human subjects, does the paper
603"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9560439560439561,"include the full text of instructions given to participants and screenshots, if applicable, as
604"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9574175824175825,"well as details about compensation (if any)?
605"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9587912087912088,"Answer: [NA]
606"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9601648351648352,"Justification: The paper does not involve research with human subjects nor crowdsourcing.
607"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9615384615384616,"Guidelines:
608"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9629120879120879,"• The answer NA means that the paper does not involve crowdsourcing nor research with
609"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9642857142857143,"human subjects.
610"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9656593406593407,"• Including this information in the supplemental material is fine, but if the main contribu-
611"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.967032967032967,"tion of the paper involves human subjects, then as much detail as possible should be
612"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9684065934065934,"included in the main paper.
613"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9697802197802198,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
614"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9711538461538461,"or other labor should be paid at least the minimum wage in the country of the data
615"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9725274725274725,"collector.
616"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9739010989010989,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
617"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9752747252747253,"Subjects
618"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9766483516483516,"Question: Does the paper describe potential risks incurred by study participants, whether
619"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.978021978021978,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
620"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9793956043956044,"approvals (or an equivalent approval/review based on the requirements of your country or
621"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9807692307692307,"institution) were obtained?
622"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9821428571428571,"Answer: [NA]
623"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9835164835164835,"Justification: The paper does not involve research with human subjects.
624"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848901098901099,"Guidelines:
625"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9862637362637363,"• The answer NA means that the paper does not involve crowdsourcing nor research with
626"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9876373626373627,"human subjects.
627"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.989010989010989,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
628"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9903846153846154,"may be required for any human subjects research. If you obtained IRB approval, you
629"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9917582417582418,"should clearly state this in the paper.
630"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9931318681318682,"• We recognize that the procedures for this may vary significantly between institutions
631"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9945054945054945,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
632"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9958791208791209,"guidelines for their institution.
633"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9972527472527473,"• For initial submissions, do not include any information that would break anonymity (if
634"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9986263736263736,"applicable), such as the institution conducting the review.
635"
