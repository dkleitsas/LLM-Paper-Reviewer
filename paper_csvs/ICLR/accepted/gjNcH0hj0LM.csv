Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0029069767441860465,"Time-series data are ubiquitous these days, but lack of the labels in time-series
data is regarded as a hurdle for its broad applicability. Meanwhile, active learn-
ing has been successfully adopted to reduce the labeling efforts in various tasks.
Thus, this paper addresses an important issue, time-series active learning. Inspired
by the temporal coherence in time-series data, where consecutive data points tend
to have the same label, our label propagation framework, called TCLP, automat-
ically assigns a queried label to the data points within an accurately estimated
time-series segment, thereby signiï¬cantly boosting the impact of an individual
query. Compared with traditional time-series active learning, TCLP is shown to
improve the classiï¬cation accuracy by up to 7.1 times when only 0.8% of data
points in the entire time series are queried for their labels."
INTRODUCTION,0.005813953488372093,"1
INTRODUCTION"
INTRODUCTION,0.00872093023255814,"A time series is a sequence of data points at successive timestamps. Supervised learning (e.g., clas-
siï¬cation) with a time series requires the label of every data point, but unfortunately labels are often
missing and hard to obtain due to lack of domain-speciï¬c knowledge (Shen et al., 2018; Malhotra
et al., 2019; Li et al., 2020). It is worse for a time series collected for an extended length of time, as
manually labeling so many data points is labor-intensive and time-consuming (Perslev et al., 2019;
Tonekaboni et al., 2021). Active learning (Settles, 2009), a method that iteratively selects the most
informative data point and queries a user for its label, can mitigate the high labeling cost. However,
most active learning methods are not geared for time-series data, as they assume that data points are
independent of one other (Sener & Savarese, 2018; Yoo & Kweon, 2019; Ash et al., 2020), which is
obviously not true in time-series data."
INTRODUCTION,0.011627906976744186,"Time-series data typically has the characteristic of temporal coherence; that is, temporally consecu-
tive data points tend to have the same label (Wang et al., 2020; Barrow et al., 2020; Ishikawa et al.,
2021). Let us refer to a sub-sequence of temporally coherent data points as a segment. For exam-
ple, in motion-sensing time-series data, a segment consists of data points with the same motion
status (e.g., walking, running). This temporal coherence of a segment can be exploited in time-series
active learning. Speciï¬cally, when the label of a certain data point is obtained from a user, the same
label can be propagated to other data points in the same segment. One challenge here is that the seg-
ment length is not known but needs to be estimated. If it is too short, unnecessarily frequent queries
are issued; if too long, data points on the fringe of the segment are labeled incorrectly, consequently
damaging the learning performance (e.g., classiï¬cation accuracy). Thus, accurate estimation of the
segments is important to enable the label propagation to achieve the maximum learning performance
with the minimum number of queries."
INTRODUCTION,0.014534883720930232,"This paper addresses the label propagation segment estimation problem in time-series active learn-
ing through a novel framework called Temporal Coherence-based Label Propagation (TCLP). Fig-
ure 1 illustrates the overall workï¬‚ow in the time-series active learning centered on TCLP. TCLP
receives the class probabilities (i.e., softmax output) for the label of each data point from a classiï¬er
model and estimates the extent of the segment to propagate the label. This estimation is challenging"
INTRODUCTION,0.01744186046511628,âˆ—Corresponding author.
INTRODUCTION,0.020348837209302327,Published as a conference paper at ICLR 2022
INTRODUCTION,0.023255813953488372,"Classifier output
Query1
Label
Propagation"
INTRODUCTION,0.02616279069767442,(TCLP)
INTRODUCTION,0.029069767441860465,Unlabeled
INTRODUCTION,0.03197674418604651,"ğœƒğœƒ2 = ğœƒğœƒ1 âˆ’ğœ‚ğœ‚âˆ‡ğœƒğœƒâ„“
â‹¯
â‹¯"
INTRODUCTION,0.03488372093023256,"Classifier ğ‘“ğ‘“È‰ ; ğœƒğœƒ1
Classifier ğ‘“ğ‘“È‰ ; ğœƒğœƒ2"
INTRODUCTION,0.0377906976744186,Query2
INTRODUCTION,0.040697674418604654,"A few labels added
Labels propagated"
INTRODUCTION,0.0436046511627907,"Figure 1: Overall workï¬‚ow of time-series active learning centered on TCLP. A data point that is
queried for a label is determined by a query selection strategy. The label obtained is then propagated
to adjacent data points guided by the TCLP framework."
INTRODUCTION,0.046511627906976744,"in a real time series, as the classiï¬er model output is uncertain and the time-series segments are
unknown. TCLP meets this challenge by taking advantage of the temporal coherence via a quadratic
plateau model (Moltisanti et al., 2019), by ï¬tting it to the classiï¬er model output to smooth out the
ï¬‚uctuations of class probabilities across consecutive data points."
INTRODUCTION,0.04941860465116279,"To the best of our knowledge, TCLP is the ï¬rst that performs label propagation for time-series
active learning. The previous work closest to ours is pseudo-labeling in single-timestamp supervised
learning, where labels are known for at least one data point in each segment (Moltisanti et al., 2019;
Ma et al., 2020; Li et al., 2021). The approximate location and true class of a segment must be known
in their work, which is often impractical in the real world. Moreover, the known labels are relatively
dense in single-timestamp supervised learning, but they are very sparse in active learningâ€”typically,
no more than 5% of segments in our experiments. Thus, ï¬nding the boundaries between segments
is more challenging in active learning than in single-timestamp supervised learning. To cope with
the sparsity of labeled data points, TCLP performs sparsity-aware label propagation by exploiting
temperature scaling (Guo et al., 2017) and plateau regularization."
INTRODUCTION,0.05232558139534884,Contributions of this paper are summarized as follows:
INTRODUCTION,0.055232558139534885,"â€¢ It proposes a novel time-series active learning framework equipped with a sparsity-aware label
propagation within an accurately estimated segment.
â€¢ It veriï¬es the merit of TCLP through extensive experiments. The classiï¬cation accuracy is im-
proved by up to 7.1 times with TCLP compared to without label propagation. Moreover, TCLP
works with any query selection strategy including core-set sampling (Sener & Savarese, 2018)
and BADGE (Ash et al., 2020), boosting the effect of individual labeling."
RELATED WORK,0.05813953488372093,"2
RELATED WORK"
ACTIVE LEARNING,0.061046511627906974,"2.1
ACTIVE LEARNING"
ACTIVE LEARNING,0.06395348837209303,"Active learning is a special case of machine learning that â€˜activelyâ€™ queries a user for the labels of
data points, to the effect of using fewer labels to achieve the same learning performance. Recent
studies have focused on developing such query strategies for machine learning based on deep neural
networks (Settles, 2009; Ren et al., 2020). These approaches exploit prediction probabilities (Beluch
et al., 2018), embeddings (Sener & Savarese, 2018), gradients (Ash et al., 2020), and losses (Yoo
& Kweon, 2019) from deep neural networks to estimate the impact of each unlabeled data point
if it were to be labeled. However, these methods are not suitable for time-series data, because they
assume that data points are independent."
ACTIVE LEARNING,0.06686046511627906,"Several methods have been developed for time-series or sequence data, but most of them are applica-
ble to only segmented time-series data under the assumption that a time series is already divided into
labeled and unlabeled segments. Treating these segments as independent and identically distributed,
these methods simply apply existing active learning frameworks to the segments. For example, He
et al. (2015) select unlabeled segments that are far from labeled segments to maximize diversity;
Peng et al. (2017) select unlabeled segments with distinctive patterns to maximize diversity; and
Zhang et al. (2017) select unlabeled segments with high gradients to consider uncertainty for sen-
tence classiï¬cation. In addition, new neural network architectures or measures have been developed
for sequence-data applications such as named entity recognition (Shen et al., 2018), video action
recognition (Wang et al., 2018), and speech recognition (Malhotra et al., 2019). None of these meth-
ods is applicable to our problem, which handles unsegmented time-series data."
PSEUDO-LABELING,0.06976744186046512,"2.2
PSEUDO-LABELING
Pseudo-labeling has been actively studied for label-deï¬cient learning environments, such as semi-
supervised learning, to exploit unlabeled data points in training a classiï¬er (Lee et al., 2013). In"
PSEUDO-LABELING,0.07267441860465117,Published as a conference paper at ICLR 2022
PSEUDO-LABELING,0.0755813953488372,"general, a pseudo-label is given to an unlabeled data point based on the predictions from a classiï¬er
trained with labeled data points. Conï¬dence-based methods create a pseudo-label if it is conï¬dently
predicted by a classiï¬er (Lee et al., 2013). Consistency-based methods create a pseudo-label if it is
consistently predicted for the original and augmented data points (Sajjadi et al., 2016; Rizve et al.,
2021). Graph-based methods propagate pseudo-labels from labeled data points (nodes) to unlabeled
data points based on a similarity graph constructed from the features of all data points (Shi et al.,
2018; Liu et al., 2019; Wagner et al., 2018). However, these methods are not designed for time-series
data, and therefore are not directly applicable to our problem."
PSEUDO-LABELING,0.07848837209302326,"Coherence-based methods are developed for single-timestamp supervised learning for unsegmented
time-series data; they assume that at least one data point in each segment is given a true class label
through weak-supervision. Ma et al. (2020) propose probability thresholding propagation (PTP),
which propagates known labels bidirectionally unless the predicted class probability for each data
point is decreased by more than a threshold. Deldari et al. (2021) propose embedding similarity
propagation (ESP), which propagates known labels bidirectionally unless the embedding of each
data point changes rapidly. Recently, Moltisanti et al. (2019) adopt a plateau model that represents
class probabilities across consecutive data points, where a plateau model is constructed for each
labeled data point and ï¬tted to the classiï¬er output; a known label is propagated as long as the value
of a plateau model is higher than a threshold. While this work shares the idea of using a plateau
model with our work, using the plateau model as it is for active learning results in performance
degradation owing to the difference in the density of known labels, as will be shown in Section 4."
PSEUDO-LABELING,0.08139534883720931,"3
TCLP: TEMPORAL COHERENCE-BASED LABEL PROPAGATION"
PRELIMINARIES AND PROBLEM SETTING,0.08430232558139535,"3.1
PRELIMINARIES AND PROBLEM SETTING"
PRELIMINARIES AND PROBLEM SETTING,0.0872093023255814,"Active learning: Let D = {(xt, yt), t âˆˆT } be a time series where T is the index set of timestamps;
xt is a multi-dimensional data point at timestamp t, and yt is one of the class labels if xt is labeled
or null otherwise. Let DL âŠ†D be a labeled set, i.e., a set of labeled data points, and DU âŠ†D
be an unlabeled set, i.e., a set of unlabeled data points, where DU âˆªDL = D. At each round of
active learning, b data points are selected from DU by a query selection strategy, such as entropy
sampling (Wang & Shang, 2014), core-set selection (Sener & Savarese, 2018), and BADGE (Ash
et al., 2020), and their ground-truth labels are obtained from a user; these newly-labeled b data
points are then removed from DU and added to DL. After DL is updated, a classiï¬er model fÎ¸ is
re-trained using the updated labeled set."
PRELIMINARIES AND PROBLEM SETTING,0.09011627906976744,"Label propagation: Given a data point at timestamp tq and its label, (xtq, ytq), obtained from a
user in response to a query, TCLP assigns the label ytq to nearby data points in the timestamp range
[ts : te] (ts â‰¤tq â‰¤te) estimated according to its temporal coherence property criteria. We call
the sub-sequence of data points in [ts : te] an estimated segment at tq. There are two properties: (i)
accuracy, which indicates that as many data points in the segment as possible should have the same
ground-truth label ytq; and (ii) coverage, which indicates that the length of the segment (te âˆ’ts)
should be as long as possible. More formally, we estimate the segment for tq by"
PRELIMINARIES AND PROBLEM SETTING,0.09302325581395349,"ts, te = arg min
tâ€²
s,tâ€²
e"
PRELIMINARIES AND PROBLEM SETTING,0.09593023255813954,"1
tâ€²e âˆ’tâ€²s"
PRELIMINARIES AND PROBLEM SETTING,0.09883720930232558,"tâ€²
e
X"
PRELIMINARIES AND PROBLEM SETTING,0.10174418604651163,"t=tâ€²
s"
PRELIMINARIES AND PROBLEM SETTING,0.10465116279069768," 
1 âˆ’fÎ¸(xt)[ytq]

,
(1)"
PRELIMINARIES AND PROBLEM SETTING,0.10755813953488372,"where tâ€²
s â‰¤tq â‰¤tâ€²
e holds, fÎ¸(xt) is the softmax output vector of the classiï¬er model at timestamp t,
and fÎ¸(xt)[ytq] is the estimated probability of the label ytq. In Equation (1), the accuracy is achieved
by minimizing the sum of errors in the numerator, and the coverage is achieved by maximizing the
candidate segment length in the denominator. Note that estimated probabilities are used to calculate
the errors, since the true probabilities are not known."
PRELIMINARIES AND PROBLEM SETTING,0.11046511627906977,"Once segment estimation is done, all data points in the estimated segment (i.e., in [ts : te]) are re-
moved from DU and added to DL with the label ytq, thus doing coherence-based label propagation.
At each round of active learning, the segment estimation repeats for each of the b queried data points;
as a result, the size of DL is increased by the total length of the estimated segments. Besides, we
allow the data points in [ts : te], except tq, to be queried again in subsequent rounds so that the
propagated labels can be reï¬ned subsequently."
PRELIMINARIES AND PROBLEM SETTING,0.11337209302325581,Published as a conference paper at ICLR 2022
PLATEAU MODEL FOR SEGMENT ESTIMATION,0.11627906976744186,"3.2
PLATEAU MODEL FOR SEGMENT ESTIMATION"
PLATEAU MODEL FOR SEGMENT ESTIMATION,0.11918604651162791,"An adequately trained classiï¬er model fÎ¸ returns higher probabilities of the (unknown) true labels
for data points inside a segment and lower priorities for data points outside the segment. Besides,
the output probabilities of the model are not constant within the segment because of noise in the
time-series data. Thus, one natural approach to ï¬nding a segment is to ï¬t a plateau model to the
output of the classiï¬er model and make a plateau of probability 1 into an estimated segment."
PLATEAU MODEL AND ITS FITTING,0.12209302325581395,"3.2.1
PLATEAU MODEL AND ITS FITTING 2ğ‘¤ğ‘¤"
PLATEAU MODEL AND ITS FITTING,0.125,"Estim. prob.
Plateau Fit Prob."
PLATEAU MODEL AND ITS FITTING,0.12790697674418605,"Timestamp
ğ‘ğ‘
2ğ‘¤ğ‘¤ 1"
PLATEAU MODEL AND ITS FITTING,0.1308139534883721,True prob. Prob. ğ‘ğ‘ 1
PLATEAU MODEL AND ITS FITTING,0.13372093023255813,"Timestamp ğ‘ ğ‘ 
ğ‘ ğ‘ "
PLATEAU MODEL AND ITS FITTING,0.13662790697674418,"(a) Initial.
(b) Optimal.
Figure 2: Plateau model and its ï¬tting."
PLATEAU MODEL AND ITS FITTING,0.13953488372093023,"Among many functions with a plateau-shaped value, we
use the function introduced by Moltisanti et al. (2019):"
PLATEAU MODEL AND ITS FITTING,0.14244186046511628,"h(t; c, w, s) =
1
(es(tâˆ’câˆ’w) + 1)(es(âˆ’t+câˆ’w) + 1), (2)"
PLATEAU MODEL AND ITS FITTING,0.14534883720930233,"where c, w, and s are trainable parameters for the model
h. As shown in Figure 2, c and w respectively represent
the center and half-width of the plateau, and s indicates the steepness of the side slopes."
PLATEAU MODEL AND ITS FITTING,0.14825581395348839,"The ï¬tting of the plateau model at timestamp tq is illustrated in Figure 2. At the beginning, as in
Figure 2a, c is set to tq, and w and s are set to initial values and updated by the following optimization"
PLATEAU MODEL AND ITS FITTING,0.1511627906976744,"c, w, s = arg min
câ€²,wâ€²,sâ€²
1
2wâ€²"
PLATEAU MODEL AND ITS FITTING,0.15406976744186046,"câ€²+wâ€²
X"
PLATEAU MODEL AND ITS FITTING,0.1569767441860465,"t=câ€²âˆ’wâ€²
|h(t; câ€², wâ€², sâ€²) âˆ’fÎ¸(xt)[ytq]|,
(3)"
PLATEAU MODEL AND ITS FITTING,0.15988372093023256,"where h(t; câ€², wâ€², sâ€²) = 1 for t âˆˆ[câ€²âˆ’wâ€² :câ€²+wâ€²]. Letting Ïµ(t) be |h(t; câ€², wâ€², sâ€²)âˆ’fÎ¸(xt)[ytq]|/2wâ€²
in Equation (3) and E be P"
PLATEAU MODEL AND ITS FITTING,0.16279069767441862,"t Ïµ(t), the optimal values of the three parameters are obtained by repeat-
ing a gradient update step,"
PLATEAU MODEL AND ITS FITTING,0.16569767441860464,"c = câ€² âˆ’Î·âˆ‡câ€²E, w = wâ€² âˆ’Î·âˆ‡wâ€²E, and s = sâ€² âˆ’Î·âˆ‡sâ€²E, where Î· is the learning rate.
(4)"
PLATEAU MODEL AND ITS FITTING,0.1686046511627907,"Estimated prob.
Plateau
True class Prob. Query t"
PLATEAU MODEL AND ITS FITTING,0.17151162790697674,(a) Plateau models at the initial round. Prob. t
PLATEAU MODEL AND ITS FITTING,0.1744186046511628,(b) Plateau models after the ï¬nal round.
PLATEAU MODEL AND ITS FITTING,0.17732558139534885,Figure 3: Updated plateau models.
PLATEAU MODEL AND ITS FITTING,0.18023255813953487,"After a model is ï¬tted through enough rounds,
as shown in Figure 2b, the plateau is located at
the center of a true segment and its width covers
most of the true segment. [câˆ’w : c+w], indi-
cated by the red line in Figure 2b, is determined
as the estimated segment for the plateau model
at tq. Overall, as shown in Figures 3a and 3b, as
active learning progresses, more data points are
queried, estimated probabilities becomes more
accurate, and plateau models are better ï¬tted
to the estimated probabilities. Eventually, the
plateau models accurately represent the true segments in the time series."
SPARSITY-AWARE LABEL PROPAGATION,0.18313953488372092,"3.2.2
SPARSITY-AWARE LABEL PROPAGATION"
SPARSITY-AWARE LABEL PROPAGATION,0.18604651162790697,"In active learning, known labels are typically very sparseâ€”initially no more than 5% of the segments
and growing slowly as more labels are obtained for queried data points. Our experience indicates
that simply optimizing the plateau model as explained in Section 3.2.1 tends to generate plateaus
much longer than true segments. See Section 4.4 for the poor performance of the typical (sparsity-
unaware) plateau model. On the other hand, in single-timestamp supervised learning, where the
plateau model has been successfully employed (Moltisanti et al., 2019), at least one label should
exist for every segment; thus, the boundaries of segments can be easily recognized while making
the plateaus not overlap between segments. Thus, to overcome the lack of potential boundaries for
segment estimation, we extend the procedure of the plateau model ï¬tting in three ways."
SPARSITY-AWARE LABEL PROPAGATION,0.18895348837209303,"Calibrating the classiï¬er output: Because modern neural network models have millions of learning
parameters, the distribution of predicted probabilities in such models is often highly skewed to either
1 or 0, which means that the model is overconï¬dent (Guo et al., 2017; MÂ¨uller et al., 2019). That is, the
output of the classiï¬er model fÎ¸ could be too high even outside the true segment, thereby making the"
SPARSITY-AWARE LABEL PROPAGATION,0.19186046511627908,Published as a conference paper at ICLR 2022
SPARSITY-AWARE LABEL PROPAGATION,0.19476744186046513,"plateau wider than the true segment. We address this issue by employing temperature scaling (Guo
et al., 2017) to calibrate the classiï¬er output (i.e., softmax probabilities). Temperature scaling is
widely used to mitigate the overconï¬dence issue, because it reduces the differences of the softmax
probabilities while keeping their order. Speciï¬cally, temperature scaling divides the logits (inputs to
the softmax layer) zt by a learned scale parameter T, i.e., as fÎ¸(xt) = softmax(zt/T)."
SPARSITY-AWARE LABEL PROPAGATION,0.19767441860465115,"Regularizing the width of a plateau: To prevent a plateau from rapidly growing wider than a true
segment, we constrain the amount of updates on the parameter w of the plateau model. Speciï¬cally,
w cannot be increased more than twice its current value in a single round of active learning. Accord-
ingly, the gradient update of w in Equation (4) is slightly modiï¬ed to w = min(2wâ€², wâ€²âˆ’Î·âˆ‡wâ€²Ïµ(t))."
SPARSITY-AWARE LABEL PROPAGATION,0.2005813953488372,"Balancing the class skewness: This issue of class label imbalance can become more severe by
label propagation from sporadic queries; although the propagated labels are correct, the number
of such propagated labels can vary across different classes. To reduce the effect of this potential
skewness, we re-weight the loss â„“(Ë†yt, yt) at each timestamp in training the classiï¬er model fÎ¸, where
Ë†yt = arg maxk fÎ¸(xt)[k] and yt is the propagated ground-truth label (Johnson & Khoshgoftaar,
2019). The loss is adjusted by the inverse ratio of the number of the timestamps of a given class over
that of the most infrequent class. That is, if we let Nk be the number of the timestamps assigned with
the class k, the parameter of the classiï¬er model is updated as follows: Î¸ = Î¸ âˆ’Î» mini Ni"
SPARSITY-AWARE LABEL PROPAGATION,0.20348837209302326,"Nyt
âˆ‡â„“(Ë†yt, yt),
where Î» is another learning rate for training a classiï¬er model fÎ¸."
THEORETICAL ANALYSIS,0.2063953488372093,"3.3
THEORETICAL ANALYSIS"
THEORETICAL ANALYSIS,0.20930232558139536,"We show that our plateau-based segment estimation is expected to produce a segment closer to the
true segment than a simple threshold-based segment estimation. For ease of analysis, we consider
a single segment whose true length is L, the query data point at tq is located at the center of the
true segment with k = ytq known. In addition, we assume that the estimated class probabilities
fÎ¸(xt)[k](1 â‰¤t â‰¤L) are conditionally independent at different timestamps (Graves et al., 2006;
Chung et al., 2015)."
THEORETICAL ANALYSIS,0.21220930232558138,"Threshold-based segment estimation: A simple and straightforward way to estimate the segment
is to expand its width bidirectionally as long as the estimated probability at each timestamp is higher
than or equal to a threshold Î´. The probability that the length of a segment reaches l is"
THEORETICAL ANALYSIS,0.21511627906976744,"Pr(teâˆ’ts =l) = l Â· zlâˆ’1(1 âˆ’z)2,
(5)"
THEORETICAL ANALYSIS,0.2180232558139535,"where z = Pr(fÎ¸(xt)[k] â‰¥Î´). Here, the l multiplied to zlâˆ’1(1 âˆ’z)2 in Equation (5) is the number
of alignments possible for a segment containing the center tq. As a result, the expected length is"
THEORETICAL ANALYSIS,0.22093023255813954,"EfÎ¸(xt)[teâˆ’ts] = L
X"
THEORETICAL ANALYSIS,0.2238372093023256,"l=1
l Â· Pr(teâˆ’ts =l) = L
X"
THEORETICAL ANALYSIS,0.22674418604651161,"l=1
l2zlâˆ’1(1 âˆ’z)2"
THEORETICAL ANALYSIS,0.22965116279069767,= 1 + z âˆ’(L + 1)2zL + (2L2 + 2L âˆ’1)zL+1 âˆ’L2zL+2
THEORETICAL ANALYSIS,0.23255813953488372,"1 âˆ’z
. (6)"
THEORETICAL ANALYSIS,0.23546511627906977,"Plateau-based segment estimation: Let us ï¬x c to tq and s to âˆ(90â—¦steepness), and denote the
plateau model simply as h(t; w). Then, h(t; w) = 1 if câˆ’w â‰¤t â‰¤c+w, and h(t; w) = 0 otherwise.
In addition, for simplicity, let us ï¬x the denominator 2wâ€² in Equation (3) to L. Then, the inside of
the argmin operator in Equation (3) becomes"
THEORETICAL ANALYSIS,0.23837209302325582,Ïµ(l) = 1
THEORETICAL ANALYSIS,0.24127906976744187,"L
 
(L âˆ’l) Â· |0 âˆ’fÎ¸(xt)[k]| + l Â· |1 âˆ’fÎ¸(xt)[k]|

,
(7)"
THEORETICAL ANALYSIS,0.2441860465116279,"where l (= 2w) denotes the length of the segment estimated with the plateau model. Ïµ(l) is a linear
function of l, where its slope is (1 âˆ’2fÎ¸(xt)[k]) and 1 â‰¤l â‰¤L. Thus, Equation (7) evaluates to the
minimum at either l = 1 when the slope is positive or l = L when the slope is negative. Letting z be
Pr(fÎ¸(xt)[k]â‰¥0.5), the probabilities of l = 1 and l = L are 1âˆ’z and z, respectively. In conclusion,
the expected length of the estimated segment is"
THEORETICAL ANALYSIS,0.24709302325581395,"EfÎ¸(xt)[te âˆ’ts] = 1 Â· Pr(l = 1) + L Â· Pr(l = L) = 1 âˆ’z + Lz.
(8)"
THEORETICAL ANALYSIS,0.25,"Comparison and discussion: As L increases, Equation (6) converges toward 1+z"
THEORETICAL ANALYSIS,0.25290697674418605,"1âˆ’z and is not affected
by L, whereas Equation (8), which equals z(L âˆ’1) + 1, increases linearly with L. Therefore, when"
THEORETICAL ANALYSIS,0.2558139534883721,Published as a conference paper at ICLR 2022
THEORETICAL ANALYSIS,0.25872093023255816,"a true segment is sufï¬ciently long and z is in the typical range (e.g., less than 0.9), the plateau-
based segmentation (Equation (8)) is expected to produce a longer (i.e., closer to L) segment than
the threshold-based segmentation (Equation (6))."
OVERALL ACTIVE LEARNING PROCEDURE WITH TCLP,0.2616279069767442,"3.4
OVERALL ACTIVE LEARNING PROCEDURE WITH TCLP"
OVERALL ACTIVE LEARNING PROCEDURE WITH TCLP,0.26453488372093026,Algorithm 1 Time-series active learning with TCLP
OVERALL ACTIVE LEARNING PROCEDURE WITH TCLP,0.26744186046511625,"Input: Timestamp feature xt, initially labeled set DL, unlabeled set DU, query strategy Q,"
OVERALL ACTIVE LEARNING PROCEDURE WITH TCLP,0.2703488372093023,"number of rounds R, query size b, initial classiï¬er fÎ¸0, classiï¬er loss â„“, learning rate Î».
Output: Final classiï¬er fÎ¸R+1."
OVERALL ACTIVE LEARNING PROCEDURE WITH TCLP,0.27325581395348836,"1: H0 â†Initialized plateau models for DL;
2: Î¸1 = Î¸0 âˆ’Î» mini Ni"
OVERALL ACTIVE LEARNING PROCEDURE WITH TCLP,0.2761627906976744,"Nyt
âˆ‡â„“(Ë†yt, yt) for each (xt, yt) âˆˆDL;"
OVERALL ACTIVE LEARNING PROCEDURE WITH TCLP,0.27906976744186046,"3: for r = 1, . . . , R do
4:
Â¯yt = TEMPERATURESCALING(fÎ¸r(xt)); Hr = Ã˜;
// see Section 3.2.2
5:
for h in Hrâˆ’1 do
6:
hâ€² â†ï¬t h on Â¯yt; Hr â†Hr
S hâ€²;
// see Section 3.2.1
7:
{tq}b
q=1 â†queried timestamps acquired by Q from DU;
8:
Hr â†Hr
S {plateau model htq}b
q=1;
9:
Hr â†ADJUST(Hr);
// see Appendix C
10:
DL â†LABELPROPAGATION(Hr);
// see Section 3.2.2
11:
Î¸r+1 = Î¸r âˆ’Î» mini Ni"
OVERALL ACTIVE LEARNING PROCEDURE WITH TCLP,0.2819767441860465,"Nyt
âˆ‡â„“(Ë†yt, yt) for each (xt, yt) âˆˆDL;"
OVERALL ACTIVE LEARNING PROCEDURE WITH TCLP,0.28488372093023256,12: return fÎ¸R+1;
OVERALL ACTIVE LEARNING PROCEDURE WITH TCLP,0.2877906976744186,"Algorithm 1 summarizes how TCLP works in time-series active learning. First, plateau models are
initialized with the initially labeled set DL and stored in the set H0 (Line 1); and then using DL,
TCLP trains the classiï¬er model fÎ¸0 (Line 2). Then, at each active learning round r, TCLP ï¬rst per-
forms calibration by inferring the data points with the classiï¬er model fÎ¸r and scaling the softmax
output, and then initializes a new set of plateau models Hr (Line 4). Next, each plateau model in
Hrâˆ’1 from the previous round is ï¬tted to the scaled output, and then the updated plateau model
is added to Hr (Line 6). The new plateau models are then initialized from queried timestamp la-
bels (Line 7) and added to Hr (Line 8). Then, any overlapping plateaus in Hr are adjustedâ€”either
merged into one or reduced to avoid the overlapâ€”as needed (Line 9). Finally, the queried labels are
propagated following the plateau models in Hr (Line 10), and the classiï¬er model fÎ¸r is re-trained
with the augmented labeled set DL (Line 11). The complexity analysis of TCLP is presented in
Appendix A."
EVALUATION,0.29069767441860467,"4
EVALUATION"
EVALUATION,0.2936046511627907,We conduct experiments with various active learning settings to test the following hypotheses.
EVALUATION,0.29651162790697677,"â€¢ TCLP accelerates active learning methods faster than other label propagation methods can.
â€¢ TCLP achieves both high accuracy and wide coverage in segment estimation.
â€¢ TCLP overcomes the label sparsity by the extensions discussed in Section 3.2.2."
EXPERIMENT SETTING,0.29941860465116277,"4.1
EXPERIMENT SETTING"
EXPERIMENT SETTING,0.3023255813953488,Table 1: Summary of datasets and conï¬gurations.
EXPERIMENT SETTING,0.30523255813953487,"Timestamps
Length
#class
Dim
b
R
w0
s0
50salads
288798
289
19
2048
200
15
15
0.5"
EXPERIMENT SETTING,0.3081395348837209,"GTEA
31225
34
11
2048
200
15
5
0.5"
EXPERIMENT SETTING,0.311046511627907,"mHealth
343195
2933
12
23
200
15
15
0.5"
EXPERIMENT SETTING,0.313953488372093,"HAPT
815614
967
6
6
200
15
15
0.5"
EXPERIMENT SETTING,0.3168604651162791,"Datasets: The four benchmark datasets sum-
marized in Table 1 are used. 50Salads contains
videos at 30 frames per second that capture 25
people preparing a salad (Stein & McKenna,
2013), and GTEA contains 15 frame videos of
four people (Fathi et al., 2011). For these two
video datasets, we extract I3D features of 2,048
dimensional vectors at each timestamp follow-
ing the previous literature (Farha & Gall, 2019). mHealth contains 50Hz sensor time-series record-
ings of human movement, measured by 3D accelerometers, 3D gyroscopes, 3D magnetometers,
and electrocardiograms (Banos et al., 2014); we extract labeled regions from the raw data and stitch"
EXPERIMENT SETTING,0.31976744186046513,Published as a conference paper at ICLR 2022
EXPERIMENT SETTING,0.3226744186046512,"NOP
PTP
ESP
TCLP"
EXPERIMENT SETTING,0.32558139534883723,"0
0.05
0.01 0.17 0.33 0.5 F1@25"
EXPERIMENT SETTING,0.32848837209302323,"0
0.05
0.1 0.23 0.45 0.67"
EXPERIMENT SETTING,0.3313953488372093,"0
0.004
0.008 0.12 0.23 0.35"
EXPERIMENT SETTING,0.33430232558139533,"0
0.002
0.004 0.33 0.55 0.76"
EXPERIMENT SETTING,0.3372093023255814,"0
0.05
0.01
Queried data ratio 0.26 0.43 0.6"
EXPERIMENT SETTING,0.34011627906976744,TS accuracy
EXPERIMENT SETTING,0.3430232558139535,"0
0.05
0.1
Queried data ratio 0.25 0.44 0.63"
EXPERIMENT SETTING,0.34593023255813954,"0
0.004
0.008
Queried data ratio 0.36 0.53 0.69"
EXPERIMENT SETTING,0.3488372093023256,"0
0.002
0.004
Queried data ratio 0.62 0.75 0.89"
EXPERIMENT SETTING,0.35174418604651164,"(a) 50salads.
(b) GTEA.
(c) mHealth.
(d) HAPT."
EXPERIMENT SETTING,0.3546511627906977,"Figure 4: Classiï¬cation accuracy measured at each (1stâ€“15th) round of active learning. The accuracy
value is an average over all query selection methods. Detailed results are in Appendix B."
EXPERIMENT SETTING,0.35755813953488375,"them in chronological order to make a time series. HAPT represents 50Hz sensor time-series record-
ings of human actions in laboratory setting, measured by 3D accelerometers and multiple 3D gyro-
scopes (Anguita et al., 2013)."
EXPERIMENT SETTING,0.36046511627906974,"Query selection methods: To evaluate the efï¬cacy of TCLP, we combine it with six different query
selection methods. CONF (Wang & Shang, 2014) selects b timestamps exhibiting the lowest con-
ï¬dence in the modelâ€™s prediction, where the conï¬dence is evaluated by using the largest predicted
class probability; MARG (Settles, 2012) is similar to CONF, but it deï¬nes the conï¬dence as the
difference between the ï¬rst- and second-largest predicted class probabilities; ENTROPY (Wang &
Shang, 2014) selects the top b timestamps exhibiting the largest entropy for their predicted class
probabilities; CS (Sener & Savarese, 2018) chooses the top b most representative timestamps in em-
bedding space; BADGE (Ash et al., 2020) computes gradients from fÎ¸ at each timestamp t and
queries b timestamps found by k-MEANS++ to consider uncertainty and diversity; UTILITY is our
simple selection strategy that selects b timestamps randomly from the timestamps not covered by
the current set of plateau models, to increase the utility of the plateau models."
EXPERIMENT SETTING,0.3633720930232558,"Compared label propagation methods: For a thorough comparison, we compare TCLP with three
available label propagation approachesâ€”NOP, PTP, and ESP. For this purpose, each of TCLP and
the three approaches is combined with each of the aforementioned six query selection methods. NOP
is the baseline without using any label propagation. As explained in Section 2.2, PTP propagates
labels based on the predicted class probabilities with a certain threshold (Î´ = 0.8), while ESP
leverages cosine similarity between embeddings for label propagation. PTP and ESP are modiï¬ed
to work in an active learning setting as done by Deldari et al. (2021)."
EXPERIMENT SETTING,0.36627906976744184,"TCLP implementation details: We use the multi-stage temporal convolutional network (MS-
TCN) (Farha & Gall, 2019) as the classiï¬er fÎ¸ for time-series data. We use exactly the same training
conï¬guration suggested in the original work (Farha & Gall, 2019). Regarding active learning hy-
perparameters, the number of queried data points per round (b) and the number of active learning
rounds (R) are summarized in Table 1. For TCLP, we use the initial parameters for plateau mod-
els (w0 and s0) in Table 1 and temperature scaling with T = 2. Our experience indicates that any
value of w0 smaller than 20% of the mean segment length is adequate enough. Accuracy met-
rics: Timestamp accuracy and segmental F1 score are measured at each round by ï¬ve-fold cross
validation; they represent the prediction accuracy at the granularity of timestamp and segment, re-
spectively. The former is deï¬ned as the proportion of the timestamps with correct prediction. The
latter is deï¬ned as the F1 score of segments with an overlapping threshold on the intersection over
union (IoU) (Farha & Gall, 2019); that is, a prediction is classiï¬ed as correct if the IoU between pre-
dicted and true segments is larger than the threshold. F1@25, with the threshold 25%, is commonly
used in the literature (Lea et al., 2017; Farha & Gall, 2019); the trends with the thresholds 10% and
50% are similar."
EXPERIMENT SETTING,0.3691860465116279,Published as a conference paper at ICLR 2022
EXPERIMENT SETTING,0.37209302325581395,Table 2: Classiï¬cation accuracy measured after the ï¬nal (15th) round (the best results in bold).
EXPERIMENT SETTING,0.375,"Dataset
Query
F1@25
Timestamp Accuracy
NOP
PTP
ESP
TCLP
NOP
PTP
ESP
TCLP"
SALADS,0.37790697674418605,50salads
SALADS,0.3808139534883721,"CONF
0.191Â±0.015
0.204Â±0.015
0.280Â±0.017
0.433Â±0.010
0.505Â±0.017
0.451Â±0.032
0.462Â±0.031
0.559Â±0.010
ENTROPY
0.133Â±0.004
0.193Â±0.011
0.263Â±0.020
0.368Â±0.031
0.432Â±0.019
0.416Â±0.019
0.455Â±0.024
0.496Â±0.027
MARG
0.287Â±0.021
0.359Â±0.018
0.436Â±0.031
0.600Â±0.028
0.616Â±0.033
0.615Â±0.015
0.637Â±0.031
0.697Â±0.020
CS
0.322Â±0.021
0.426Â±0.018
0.480Â±0.022
0.559Â±0.021
0.595Â±0.021
0.602Â±0.017
0.632Â±0.023
0.657Â±0.024
BADGE
0.197Â±0.014
0.317Â±0.032
0.377Â±0.019
0.471Â±0.030
0.514Â±0.023
0.529Â±0.024
0.567Â±0.018
0.600Â±0.025
UTILITY
0.372Â±0.017
0.482Â±0.012
0.511Â±0.028
0.595Â±0.022
0.625Â±0.028
0.642Â±0.022
0.659Â±0.026
0.672Â±0.018
AVERAGE
0.250Â±0.034
0.330Â±0.043
0.391Â±0.038
0.504Â±0.035
0.548Â±0.028
0.543Â±0.035
0.569Â±0.034
0.614Â±0.028 GTEA"
SALADS,0.38372093023255816,"CONF
0.386Â±0.119
0.297Â±0.123
0.443Â±0.108
0.690Â±0.020
0.409Â±0.107
0.321Â±0.115
0.443Â±0.082
0.654Â±0.011
ENTROPY
0.355Â±0.119
0.575Â±0.032
0.422Â±0.128
0.613Â±0.028
0.364Â±0.114
0.565Â±0.028
0.456Â±0.104
0.590Â±0.021
MARG
0.248Â±0.072
0.491Â±0.122
0.582Â±0.035
0.727Â±0.024
0.320Â±0.057
0.469Â±0.099
0.545Â±0.026
0.659Â±0.015
CS
0.656Â±0.031
0.512Â±0.125
0.610Â±0.057
0.666Â±0.011
0.609Â±0.026
0.520Â±0.102
0.591Â±0.035
0.630Â±0.007
BADGE
0.718Â±0.041
0.723Â±0.033
0.710Â±0.029
0.703Â±0.028
0.705Â±0.020
0.682Â±0.010
0.692Â±0.017
0.663Â±0.015
UTILITY
0.661Â±0.025
0.671Â±0.030
0.700Â±0.014
0.656Â±0.025
0.608Â±0.018
0.608Â±0.033
0.646Â±0.019
0.644Â±0.019
AVERAGE
0.504Â±0.074
0.545Â±0.056
0.578Â±0.046
0.676Â±0.015
0.502Â±0.059
0.528Â±0.047
0.562Â±0.038
0.640Â±0.010"
SALADS,0.3866279069767442,mHealth
SALADS,0.38953488372093026,"CONF
0.016Â±0.005
0.015Â±0.002
0.029Â±0.006
0.228Â±0.064
0.294Â±0.016
0.445Â±0.044
0.398Â±0.063
0.685Â±0.051
ENTROPY
0.008Â±0.004
0.011Â±0.001
0.023Â±0.005
0.074Â±0.030
0.175Â±0.047
0.363Â±0.032
0.413Â±0.050
0.560Â±0.054
MARG
0.018Â±0.005
0.065Â±0.017
0.078Â±0.027
0.363Â±0.082
0.305Â±0.011
0.485Â±0.042
0.485Â±0.030
0.693Â±0.079
CS
0.055Â±0.024
0.289Â±0.085
0.187Â±0.049
0.594Â±0.094
0.590Â±0.034
0.680Â±0.032
0.577Â±0.040
0.817Â±0.009
BADGE
0.131Â±0.042
0.145Â±0.041
0.129Â±0.024
0.296Â±0.054
0.404Â±0.056
0.578Â±0.026
0.545Â±0.049
0.665Â±0.047
UTILITY
0.076Â±0.006
0.462Â±0.103
0.432Â±0.089
0.538Â±0.113
0.752Â±0.051
0.831Â±0.028
0.901Â±0.015
0.789Â±0.065
AVERAGE
0.051Â±0.018
0.164Â±0.067
0.146Â±0.057
0.349Â±0.072
0.420Â±0.080
0.564Â±0.064
0.553Â±0.069
0.702Â±0.034 HAPT"
SALADS,0.39244186046511625,"CONF
0.324Â±0.114
0.289Â±0.084
0.459Â±0.050
0.656Â±0.034
0.611Â±0.107
0.592Â±0.083
0.793Â±0.026
0.857Â±0.019
ENTROPY
0.332Â±0.055
0.368Â±0.082
0.384Â±0.044
0.653Â±0.047
0.646Â±0.067
0.755Â±0.024
0.763Â±0.044
0.836Â±0.011
MARG
0.276Â±0.043
0.667Â±0.042
0.790Â±0.020
0.805Â±0.022
0.556Â±0.031
0.799Â±0.010
0.809Â±0.025
0.889Â±0.031
CS
0.739Â±0.033
0.816Â±0.010
0.843Â±0.026
0.835Â±0.024
0.880Â±0.005
0.898Â±0.018
0.926Â±0.007
0.909Â±0.014
BADGE
0.784Â±0.029
0.719Â±0.044
0.813Â±0.019
0.860Â±0.018
0.859Â±0.032
0.840Â±0.016
0.858Â±0.014
0.886Â±0.035
UTILITY
0.846Â±0.012
0.867Â±0.018
0.811Â±0.026
0.849Â±0.023
0.936Â±0.006
0.937Â±0.008
0.916Â±0.010
0.934Â±0.004
AVERAGE
0.550Â±0.099
0.621Â±0.089
0.684Â±0.076
0.776Â±0.036
0.748Â±0.060
0.803Â±0.046
0.844Â±0.025
0.885Â±0.013"
OVERALL ACTIVE LEARNING PERFORMANCE,0.3953488372093023,"4.2
OVERALL ACTIVE LEARNING PERFORMANCE"
OVERALL ACTIVE LEARNING PERFORMANCE,0.39825581395348836,"Figure 4 shows the F1@25 and timestamp accuracy at each round of varying the queried data ra-
tio (= the number of queried data points / the total number of data points), where the accuracy values
are averaged over the six query selection methods. 15 rounds are conducted for each dataset. TCLP
performs the best among all label propagation approaches, with the accuracy improving much faster
with a smaller number of queries than in the other approaches; this performance is attributed to the
larger number of correctly propagated labels in TCLP, as will be shown in Section 4.3. Interestingly,
the accuracy gain is higher in F1@25 than in timestamp accuracy; this difference makes sense be-
cause F1@25 measures the accuracy at the granularity of segment and therefore reï¬‚ects temporal
coherence better than the granularity of timestamp. Appendix D shows more details."
OVERALL ACTIVE LEARNING PERFORMANCE,0.4011627906976744,"Table 2 shows the F1@25 and timestamp accuracy measured after the ï¬nal (15th) round, i.e., at
the last queried data ratio in Figure 4, for each of the six query selection methods. TCLP performs
best here as well in almost all combinations of datasets, query selection methods, and accuracy
metrics. Speciï¬cally, TCLP outperforms the compared label propagation approaches (NOP, PTP,
and ESP) for all query selection methods except only a few cases. This result conï¬rms that TCLP
maintains its performance advantage regardless of the query selection method. Interestingly, TCLPâ€™s
performance gain is most outstanding for the mHealth dataset and least outstanding for the GTEA
dataset. The reason lies in the length of the segments. As shown in Table 1, mHealthâ€™s segments are
the longest (2,933 on average) and GTEAâ€™s segments are the shortest (34 on average). Longer seg-
ments certainly allow more temporal coherence to be exploited in label propagation, thus resulting
in higher performance. For instance, using UTILITY on the mHealth dataset, TCLP outperforms
NOP by 7.1 times, PTP by 1.2 times, and ESP by 1.2 times in F1@25 while, on the GTEA dataset,
TCLP outperforms them less signiï¬cantly."
LABEL PROPAGATION PERFORMANCE,0.40406976744186046,"4.3
LABEL PROPAGATION PERFORMANCE"
LABEL PROPAGATION PERFORMANCE,0.4069767441860465,"Table 3 shows the correct label propagation ratio (= the number of correctly propagated labels / the
total number of data points) to verify how many labels are correctly propagated with each label prop-
agation approach. Overall, fully taking advantage of the temporal coherence based on the plateau
model, TCLP adds far more correct labels than PTP and ESP. Speciï¬cally, using UTILITY on the
mHealth dataset, the correct propagation ratio of TCLP is higher than that of PTP by 5.0 times and
that of ESP by 3.7 times. It is impressive that querying only 0.8% of data points results in up to 33%
of data points correctly labeled. Appendix E shows more details."
LABEL PROPAGATION PERFORMANCE,0.40988372093023256,Published as a conference paper at ICLR 2022
LABEL PROPAGATION PERFORMANCE,0.4127906976744186,Table 3: Correct label propagation ratio after the ï¬nal (15th) round (the best results in bold).
LABEL PROPAGATION PERFORMANCE,0.41569767441860467,"Dataset
Query
Correct Propagation Ratio
Dataset
Query
Correct Propagation Ratio
PTP
ESP
TCLP
PTP
ESP
TCLP"
SALADS,0.4186046511627907,50salads
SALADS,0.42151162790697677,"CONF
0.032Â±0.001
0.054Â±0.001
0.129Â±0.005"
SALADS,0.42441860465116277,mHealth
SALADS,0.4273255813953488,"CONF
0.027Â±0.001
0.038Â±0.002
0.109Â±0.017
ENTROPY
0.026Â±0.000
0.042Â±0.001
0.100Â±0.005
ENTROPY
0.024Â±0.000
0.031Â±0.001
0.066Â±0.009
MARG
0.071Â±0.000
0.151Â±0.004
0.368Â±0.006
MARG
0.054Â±0.001
0.083Â±0.004
0.204Â±0.027
CS
0.076Â±0.000
0.151Â±0.001
0.306Â±0.003
CS
0.061Â±0.000
0.081Â±0.001
0.210Â±0.018
BADGE
0.054Â±0.001
0.094Â±0.004
0.193Â±0.013
BADGE
0.060Â±0.001
0.087Â±0.002
0.201Â±0.020
UTILITY
0.081Â±0.000
0.170Â±0.002
0.352Â±0.003
UTILITY
0.065Â±0.000
0.089Â±0.001
0.325Â±0.017
AVERAGE
0.057Â±0.009
0.110Â±0.020
0.241Â±0.043
AVERAGE
0.049Â±0.007
0.068Â±0.010
0.186Â±0.034 GTEA"
SALADS,0.43023255813953487,"CONF
0.270Â±0.008
0.252Â±0.008
0.498Â±0.007 HAPT"
SALADS,0.4331395348837209,"CONF
0.011Â±0.001
0.019Â±0.001
0.050Â±0.005
ENTROPY
0.226Â±0.009
0.220Â±0.010
0.403Â±0.018
ENTROPY
0.009Â±0.000
0.014Â±0.000
0.033Â±0.002
MARG
0.423Â±0.008
0.380Â±0.002
0.404Â±0.009
MARG
0.022Â±0.001
0.047Â±0.002
0.148Â±0.007
CS
0.437Â±0.014
0.398Â±0.014
0.371Â±0.014
CS
0.026Â±0.000
0.048Â±0.002
0.146Â±0.004
BADGE
0.348Â±0.014
0.305Â±0.013
0.404Â±0.008
BADGE
0.026Â±0.000
0.053Â±0.001
0.160Â±0.008
UTILITY
0.516Â±0.002
0.453Â±0.001
0.424Â±0.006
UTILITY
0.028Â±0.000
0.058Â±0.003
0.257Â±0.004
AVERAGE
0.370Â±0.041
0.335Â±0.034
0.418Â±0.016
AVERAGE
0.020Â±0.003
0.040Â±0.007
0.132Â±0.031"
SALADS,0.436046511627907,"Table 4: Classiï¬cation timestamp accuracy after the ï¬nal (15th) round with and without plateau
width regularization and temperature scaling (the best results in bold)."
SALADS,0.438953488372093,"Dataset
Width Reg.
No
Yes
Temp. Scal.
No (T = 1)
Yes (T = 2)
T = 0.5
T = 0.75
T = 1
T = 1.5
T = 1.75
T = 2
T = 2.25
T = 2.5
T = 2.75"
SALADS,0.4418604651162791,50salads
SALADS,0.44476744186046513,"CONF
0.441Â±0.015
0.487Â±0.030
0.465Â±0.044
0.489Â±0.026
0.480Â±0.035
0.519Â±0.019
0.559Â±0.020
0.559Â±0.010
0.535Â±0.020
0.508Â±0.023
0.460Â±0.045
ENTROPY
0.431Â±0.042
0.455Â±0.040
0.430Â±0.044
0.410Â±0.027
0.442Â±0.013
0.462Â±0.029
0.452Â±0.009
0.496Â±0.027
0.479Â±0.015
0.462Â±0.018
0.482Â±0.041
MARG
0.655Â±0.033
0.671Â±0.027
0.668Â±0.033
0.667Â±0.016
0.691Â±0.025
0.671Â±0.024
0.682Â±0.018
0.697Â±0.020
0.664Â±0.028
0.658Â±0.013
0.599Â±0.030
CS
0.611Â±0.028
0.618Â±0.026
0.592Â±0.040
0.616Â±0.027
0.624Â±0.034
0.610Â±0.020
0.627Â±0.029
0.657Â±0.024
0.656Â±0.018
0.633Â±0.025
0.626Â±0.015
BADGE
0.595Â±0.018
0.599Â±0.020
0.575Â±0.021
0.594Â±0.037
0.623Â±0.023
0.631Â±0.017
0.634Â±0.014
0.600Â±0.025
0.575Â±0.018
0.546Â±0.028
0.566Â±0.010
UTILITY
0.671Â±0.017
0.670Â±0.016
0.662Â±0.024
0.644Â±0.036
0.662Â±0.022
0.652Â±0.037
0.651Â±0.024
0.672Â±0.018
0.661Â±0.024
0.661Â±0.025
0.667Â±0.028
AVERAGE
0.567Â±0.039
0.583Â±0.034
0.565Â±0.037
0.570Â±0.037
0.587Â±0.038
0.591Â±0.031
0.601Â±0.031
0.614Â±0.028
0.595Â±0.029
0.578Â±0.032
0.566Â±0.030 GTEA"
SALADS,0.4476744186046512,"CONF
0.539Â±0.072
0.575Â±0.052
0.614Â±0.025
0.587Â±0.020
0.603Â±0.023
0.607Â±0.010
0.575Â±0.011
0.654Â±0.011
0.553Â±0.082
0.638Â±0.027
0.477Â±0.087
ENTROPY
0.553Â±0.016
0.578Â±0.020
0.606Â±0.016
0.574Â±0.018
0.596Â±0.018
0.614Â±0.019
0.592Â±0.022
0.590Â±0.021
0.582Â±0.026
0.551Â±0.022
0.593Â±0.010
MARG
0.605Â±0.014
0.646Â±0.023
0.609Â±0.015
0.636Â±0.014
0.632Â±0.010
0.636Â±0.014
0.672Â±0.009
0.659Â±0.015
0.682Â±0.021
0.669Â±0.009
0.657Â±0.032
CS
0.596Â±0.012
0.606Â±0.013
0.601Â±0.021
0.586Â±0.012
0.484Â±0.112
0.598Â±0.021
0.597Â±0.019
0.630Â±0.007
0.616Â±0.020
0.629Â±0.015
0.673Â±0.011
BADGE
0.645Â±0.015
0.644Â±0.011
0.616Â±0.012
0.635Â±0.018
0.620Â±0.010
0.641Â±0.016
0.658Â±0.015
0.663Â±0.015
0.676Â±0.019
0.687Â±0.017
0.677Â±0.007
UTILITY
0.589Â±0.043
0.608Â±0.031
0.605Â±0.011
0.529Â±0.081
0.616Â±0.016
0.595Â±0.017
0.627Â±0.017
0.644Â±0.019
0.638Â±0.014
0.659Â±0.015
0.667Â±0.013
AVERAGE
0.588Â±0.014
0.609Â±0.012
0.608Â±0.002
0.591Â±0.015
0.592Â±0.020
0.615Â±0.007
0.620Â±0.014
0.640Â±0.010
0.624Â±0.019
0.639Â±0.018
0.624Â±0.029"
EFFECTS OF SPARSITY-AWARE LABEL PROPAGATION TECHNIQUES IN TCLP,0.45058139534883723,"4.4
EFFECTS OF SPARSITY-AWARE LABEL PROPAGATION TECHNIQUES IN TCLP"
EFFECTS OF SPARSITY-AWARE LABEL PROPAGATION TECHNIQUES IN TCLP,0.45348837209302323,"Table 4 shows the timestamp accuracy achieved by TCLP with and without two techniques em-
ployed to handle sparsity of labelsâ€”temperature scaling for classiï¬er output calibration and plateau
width regularization in Section 3.2.2. The class skewness balancing is employed by default to assure
stable performance. In addition, the temperature scale factor T is varied for the temperature scal-
ing technique. Compared with enabling both width regularization and temperature scaling (T = 2),
removing width regularization only, temperature scaling only, and both degrades the timestamp ac-
curacy by 5.0%, 4.3%, and 7.6%, respectively, in the 50salads dataset and 4.8%. 4.2%, and 8.1%,
respectively, in the GTEA dataset. Clearly, both techniques are helpful for the label propagation,
with the width regularization showing higher effect than the temperature scaling. Note that when
T > 2, the label propagation length is suppressed, thereby causing deï¬ciency in labels needed to
train a classiï¬er; when T < 1, the label propagation length may exceed the true segment length,
thereby including wrong labels when training the classiï¬er. The break point is affected by the av-
erage length of segments in each dataset, where it occurs at a higher value of T in a dataset with
shorter segments: the average segment length is 34 in the GTEA dataset whereas it is 289 in the
50salads dataset."
CONCLUSION,0.4563953488372093,"5
CONCLUSION"
CONCLUSION,0.45930232558139533,"In this paper, we present a novel label propagation framework for time-series active learning, TCLP,
that fully takes advantage of the temporal coherence inherent in time-series data. The temporal co-
herence is modeled by the quadratic plateau model, which plays a key role in segment estimation.
Furthermore, the sparsity of known labels is relieved using temperature scaling and plateau reg-
ularization. Thanks to accurate and effective label propagation, TCLP enables us to improve the
performance of time-series supervised learning with much smaller labeling effort. Extensive experi-
ments with various datasets show that TCLP improves the classiï¬cation accuracy by up to 7.1 times
when only 0.8% of data points are queried for their labels. Future work includes developing a query
selection strategy that maximizes the merit of label propagation and utilizing a constraint on plateau
model ï¬tting based on similarity among plateau models with the same class. Overall, we expect that
our work will contribute greatly to various applications whose labeling cost is expensive."
CONCLUSION,0.4622093023255814,Published as a conference paper at ICLR 2022
CONCLUSION,0.46511627906976744,ACKNOWLEDGEMENT
CONCLUSION,0.4680232558139535,"This work was partly supported by Samsung Electronics Co., Ltd. (IO201211-08051-01) through the
Strategic Collaboration Academic Program and Institute of Information & Communications Tech-
nology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2020-0-
00862, DB4DL: High-Usability and Performance In-Memory Distributed DBMS for Deep Learn-
ing)."
CONCLUSION,0.47093023255813954,CODE OF ETHICS
CONCLUSION,0.4738372093023256,"In this paper, we introduce a novel time-series active learning framework that propagates the queried
labels to the neighboring data points. Our work reduces excessive effort and time spent on annotation
for running deep learning system. We thereby contribute the society to run deep learning systems
efï¬ciently for personal, industrial, and social purposes. All datasets used in this paper are available
in public websites and have been already anonymized, using random numeric identiï¬ers to indicate
different subjects to preserve privacy; in addition, these datasets have been extensively cited for the
studies in human activity recognition and video segmentation."
CONCLUSION,0.47674418604651164,REPRODUCIBILITY
CONCLUSION,0.4796511627906977,"We try our best in the main text to enhance the reproducibility of our work. Section 3.3 explains
the details on the procedures for the theoretical analysis. Section 4.1 clariï¬es the active learning set-
ting, datasets, evaluation metrics, and conï¬gurations for training the classiï¬er as well as the hyper-
parameters of our proposed method. The source code is uploaded on https://github.com/
kaist-dmlab/TCLP, which contains the source code for active learning (main algorithm) and
data preprocessing as well as detailed instructions. We download all datasets used in this paper from
public websites, as speciï¬ed in the corresponding references."
REFERENCES,0.48255813953488375,REFERENCES
REFERENCES,0.48546511627906974,"Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge Luis Reyes-Ortiz,
et al.
A public domain dataset for human activity recognition using smartphones.
In
Proceedings of the 2013 International European Symposium on Artiï¬cial Neural Net-
works, Computational Intelligence and Machine Learning, pp.
3, 2013.
URL http:
//archive.ics.uci.edu/ml/datasets/smartphone-based+recognition+
of+human+activities+and+postural+transitions."
REFERENCES,0.4883720930232558,"Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep
batch active learning by diverse, uncertain gradient lower bounds. In Proceedings of the 2020
International Conference on Learning Representations, 2020."
REFERENCES,0.49127906976744184,"Oresti Banos, Rafael Garcia, Juan A Holgado-Terriza, Miguel Damas, Hector Pomares, Ignacio
Rojas, Alejandro Saez, and Claudia Villalonga.
mHealthDroid: A novel framework for agile
development of mobile health applications. In Proceedings of the 2014 International Workshop
on Ambient Assisted Living, pp. 91â€“98, 2014. URL http://archive.ics.uci.edu/ml/
datasets/mhealth+dataset."
REFERENCES,0.4941860465116279,"Joe Barrow, Rajiv Jain, Vlad Morariu, Varun Manjunatha, Douglas W Oard, and Philip Resnik. A
joint model for document segmentation and segment labeling. In Proceedings of the 2020 Annual
Meeting of the Association for Computational Linguistics, pp. 313â€“322, 2020."
REFERENCES,0.49709302325581395,"William H Beluch, Tim Genewein, Andreas NÂ¨urnberger, and Jan M KÂ¨ohler. The power of ensem-
bles for active learning in image classiï¬cation. In Proceedings of the 2018 IEEE Conference on
Computer Vision and Pattern Recognition, pp. 9368â€“9377, 2018."
REFERENCES,0.5,"Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Ben-
gio. A recurrent latent variable model for sequential data. In Advances in Neural Information
Processing Systems, 2015."
REFERENCES,0.502906976744186,Published as a conference paper at ICLR 2022
REFERENCES,0.5058139534883721,"Shohreh Deldari, Daniel V Smith, Hao Xue, and Flora D Salim. Time series change point detection
with self-supervised contrastive predictive coding. In Proceedings of the 2021 Web Conference,
pp. 3124â€“3135, 2021."
REFERENCES,0.5087209302325582,"Yazan Abu Farha and Jurgen Gall. MS-TCN: Multi-stage temporal convolutional network for action
segmentation. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 3575â€“3584, 2019."
REFERENCES,0.5116279069767442,"Alireza Fathi, Xiaofeng Ren, and James M Rehg. Learning to recognize objects in egocentric ac-
tivities.
In Proceedings of the 2011 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 3281â€“3288, 2011. URL http://cbs.ic.gatech.edu/fpv/."
REFERENCES,0.5145348837209303,"Alex Graves, Santiago FernÂ´andez, Faustino Gomez, and JÂ¨urgen Schmidhuber. Connectionist tem-
poral classiï¬cation: labelling unsegmented sequence data with recurrent neural networks. In Pro-
ceedings of the 23rd international conference on Machine learning, pp. 369â€“376, 2006."
REFERENCES,0.5174418604651163,"Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.
On calibration of modern neural
networks. In Proceedings of the 2017 International Conference on Machine Learning, pp. 1321â€“
1330, 2017."
REFERENCES,0.5203488372093024,"Guoliang He, Yong Duan, Yifei Li, Tieyun Qian, Jinrong He, and Xiangyang Jia. Active learning
for multivariate time series classiï¬cation with positive unlabeled data. In Proceedings of the 2015
IEEE International Conference on Tools with Artiï¬cial Intelligence, pp. 178â€“185, 2015."
REFERENCES,0.5232558139534884,"Yuchi Ishikawa, Seito Kasai, Yoshimitsu Aoki, and Hirokatsu Kataoka.
Alleviating over-
segmentation errors by detecting action boundaries. In Proceedings of the 2021 IEEE/CVF Winter
Conference on Applications of Computer Vision, pp. 2322â€“2331, 2021."
REFERENCES,0.5261627906976745,"Justin M Johnson and Taghi M Khoshgoftaar. Survey on deep learning with class imbalance. Journal
of Big Data, 6(1):1â€“54, 2019."
REFERENCES,0.5290697674418605,"Colin Lea, Michael D. Flynn, Rene Vidal, Austin Reiter, and Gregory D. Hager. Temporal con-
volutional networks for action segmentation and detection. In Proceedings of the 2017 IEEE
Conference on Computer Vision and Pattern Recognition, July 2017."
REFERENCES,0.5319767441860465,"Dong-Hyun Lee et al. Pseudo-label: The simple and efï¬cient semi-supervised learning method
for deep neural networks. In Proceedings of the 2013 International Conference on Learning
Representations Workshop on Challenges in Representation Learning, volume 3, pp. 896, 2013."
REFERENCES,0.5348837209302325,"Shi-Jie Li, Yazan AbuFarha, Yun Liu, Ming-Ming Cheng, and Juergen Gall. MS-TCN++: Multi-
stage temporal convolutional network for action segmentation. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2020."
REFERENCES,0.5377906976744186,"Zhe Li, Yazan Abu Farha, and Jurgen Gall. Temporal action segmentation from timestamp su-
pervision. In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 8365â€“8374, 2021."
REFERENCES,0.5406976744186046,"Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sungju Hwang, and Yi Yang.
Learning to propagate labels: transductie propagation network for few-shot learning. In Proceed-
ings of the 2019 International Conference on Learning Representations, 2019."
REFERENCES,0.5436046511627907,"Fan Ma, Linchao Zhu, Yi Yang, Shengxin Zha, Gourab Kundu, Matt Feiszli, and Zheng Shou. SF-
Net: Single-frame supervision for temporal action localization. In Proceedings of the 2020 Euro-
pean Conference on Computer Vision, pp. 420â€“437, 2020."
REFERENCES,0.5465116279069767,"Karan Malhotra, Shubham Bansal, and Sriram Ganapathy. Active learning methods for low resource
end-to-end speech recognition. In Proceedings of the 2019 Conference of the International Speech
Communication Association, pp. 2215â€“2219, 2019."
REFERENCES,0.5494186046511628,"Davide Moltisanti, Sanja Fidler, and Dima Damen. Action recognition from single timestamp su-
pervision in untrimmed videos. In Proceedings of the 2019 IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 9915â€“9924, 2019."
REFERENCES,0.5523255813953488,Published as a conference paper at ICLR 2022
REFERENCES,0.5552325581395349,"Rafael MÂ¨uller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? In
Proceedings of the 33rd Conference on Neural Information Processing Systems, pp. 4696â€“4705,
2019."
REFERENCES,0.5581395348837209,"Fengchao Peng, Qiong Luo, and Lionel M Ni. ACTS: an active learning method for time series
classiï¬cation. In Proceedings of the 2017 IEEE International Conference on Data Engineering,
pp. 175â€“178, 2017."
REFERENCES,0.561046511627907,"Mathias Perslev, Michael Hejselbak Jensen, Sune Darkner, Poul JÃ¸rgen Jennum, and Christian Igel.
U-time: A fully convolutional network for time series segmentation applied to sleep staging. In
Proceedings of the 2019 International Conference on Neural Information Processing Systems, pp.
4415â€“4426, 2019."
REFERENCES,0.563953488372093,"Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen, and Xin
Wang. A survey of deep active learning. arXiv preprint arXiv:2009.00236, 2020."
REFERENCES,0.5668604651162791,"Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat, and Mubarak Shah. In defense of pseudo-
labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning. In
Proceedings of the 2021 International Conference on Learning Representations, 2021."
REFERENCES,0.5697674418604651,"Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transforma-
tions and perturbations for deep semi-supervised learning. Proceedings of the 2016 International
Conference on Neural Information Processing Systems, 29:1163â€“1171, 2016."
REFERENCES,0.5726744186046512,"Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. In Proceedings of the 2018 International Conference on Learning Representations,
2018."
REFERENCES,0.5755813953488372,"Burr Settles. Active learning literature survey. Technical Report 1648, University of Wisconsin-
Madison, Department of Computer Sciences, 2009."
REFERENCES,0.5784883720930233,"Burr Settles. Active Learning. Morgan and Claypool Publishers, 2012. ISBN 1608457257."
REFERENCES,0.5813953488372093,"Yanyao Shen, Hyokun Yun, Zachary C. Lipton, Yakov Kronrod, and Animashree Anandkumar. Deep
active learning for named entity recognition. In Proceedings of the 2018 International Conference
on Learning Representations, 2018."
REFERENCES,0.5843023255813954,"Weiwei Shi, Yihong Gong, Chris Ding, Zhiheng MaXiaoyu Tao, and Nanning Zheng. Transductive
semi-supervised deep learning using min-max features. In Proceedings of the 2018 European
Conference on Computer Vision, pp. 299â€“315, 2018."
REFERENCES,0.5872093023255814,"Sebastian Stein and Stephen J McKenna. Combining embedded accelerometers with computer vi-
sion for recognizing food preparation activities. In Proceedings of the 2013 ACM International
Joint Conference on Pervasive and Ubiquitous Computing, pp. 729â€“738, 2013. URL https:
//cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/."
REFERENCES,0.5901162790697675,"Sana Tonekaboni, Danny Eytan, and Anna Goldenberg. Unsupervised representation learning for
time series with temporal neighborhood coding. In Proceedings of the 2021 International Con-
ference on Learning Representations, 2021."
REFERENCES,0.5930232558139535,"Tal Wagner, Sudipto Guha, Shiva Kasiviswanathan, and Nina Mishra. Semi-supervised learning on
data streams via temporal label propagation. In Proceedings of the 2018 International Conference
on Machine Learning, pp. 5095â€“5104, 2018."
REFERENCES,0.5959302325581395,"Dan Wang and Yi Shang. A new active labeling method for deep learning. In Proceedings of the
2014 International Joint Conference on Neural Networks, pp. 112â€“119, 2014."
REFERENCES,0.5988372093023255,"Hanmo Wang, Xiaojun Chang, Lei Shi, Yi Yang, and Yi-Dong Shen. Uncertainty sampling for action
recognition via maximizing expected average precision. In Proceedings of the 2018 International
Joint Conference on Artiï¬cial Intelligence, pp. 964â€“970, 2018."
REFERENCES,0.6017441860465116,"Zhenzhi Wang, Ziteng Gao, Limin Wang, Zhifeng Li, and Gangshan Wu. Boundary-aware cascade
networks for temporal action segmentation. In Proceedings of the 2020 European Conference on
Computer Vision, pp. 34â€“51, 2020."
REFERENCES,0.6046511627906976,Published as a conference paper at ICLR 2022
REFERENCES,0.6075581395348837,"Donggeun Yoo and In So Kweon. Learning loss for active learning. In Proceedings of the 2019
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 93â€“102, 2019."
REFERENCES,0.6104651162790697,"Ye Zhang, Matthew Lease, and Byron Wallace. Active discriminative text representation learning.
In Proceedings of the 2017 AAAI Conference on Artiï¬cial Intelligence, 2017."
REFERENCES,0.6133720930232558,Published as a conference paper at ICLR 2022
REFERENCES,0.6162790697674418,"A
COMPUTATIONAL COMPLEXITY OF TCLP"
REFERENCES,0.6191860465116279,"In each round of active learning, let M be the number of plateau models, V be the average length
of sub-sequences of predicted probability fÎ¸(xt)[k], and S be the number of training steps for eval-
uating Equation (4). Then, considering constant computational complexity for calculating the loss
and gradient at each timestamp in the sub-sequences, we derive the computational complexity of
plateau model ï¬tting per round to be O(MV S). Here, M can be reduced by merging two overlap-
ping plateau models with the same class. This complexity of ï¬tting the plateau models is negligible
compared with the complexity of training the classiï¬er. For instance, according to the experiment
for the 50salads dataset conducted using Intel Xeon Gold 6226R and Nvidia RTX3080, ï¬tting the
plateau models took only about 1 to 2 minutes, whereas training the classiï¬er took about half an
hour per active learning round."
REFERENCES,0.622093023255814,"B
DETAILED FIGURE WITH STANDARD ERROR AND SUPERVISED ACCURACY"
REFERENCES,0.625,"NOP
PTP
ESP
TCLP
MAX"
REFERENCES,0.627906976744186,"0
0.05
0.01 0.22 0.43 0.64 F1@25"
REFERENCES,0.6308139534883721,"0
0.05
0.1 0.22 0.44 0.65"
REFERENCES,0.6337209302325582,"0
0.004
0.008 0.27 0.53 0.8"
REFERENCES,0.6366279069767442,"0
0.002
0.004 0.39 0.65 0.92"
REFERENCES,0.6395348837209303,"0
0.05
0.01
Queried data ratio 0.3 0.52 0.73"
REFERENCES,0.6424418604651163,TS accuracy
REFERENCES,0.6453488372093024,"0
0.05
0.1
Queried data ratio 0.28 0.5 0.71"
REFERENCES,0.6482558139534884,"0
0.004
0.008
Queried data ratio 0.43 0.67 0.9"
REFERENCES,0.6511627906976745,"0
0.002
0.004
Queried data ratio 0.64 0.79 0.95"
REFERENCES,0.6540697674418605,"(a) 50salads.
(b) GTEA.
(c) mHealth.
(d) HAPT."
REFERENCES,0.6569767441860465,"Figure 5: Classiï¬cation accuracy with standard error measured at each (1stâ€“15th) round of active
learning. The accuracy value is an average over all query selection methods. The black line labeled
MAX at the top indicates the maximum classiï¬cation accuracy."
REFERENCES,0.6598837209302325,"Figure 5 enriches Figure 4 with standard error, indicated by the shadow around a line, and fully-
supervised classiï¬cation accuracy, indicated by the horizontal line labeled MAX. At the last queried
data ratio (i.e., after 15 active learning rounds) of each ï¬gure, in the 50salads dataset where only
1% of data points are queried, TCLP achieves 85% of the timestamp accuracy of fully-supervised
classiï¬cation. Similarly, in the HAPT dataset where only 0.4% of data points are queried, TCLP
achieves 92% of the timestamp accuracy of fully-supervised classiï¬cation. Overall, these results
show that TCLP achieves the performance very close to fully-supervised classiï¬cation using a very
small proportion of query data points."
REFERENCES,0.6627906976744186,"C
ADJUSTMENT OF OVERLAPPING PLATEAUS IN TCLP"
REFERENCES,0.6656976744186046,"The process of adjusting overlapping updated plateaus through either merge or reduction is as fol-
lows. Consider two plateaus hkl(cl, wl, sl) (on the left) and hkr(cr, wr, sr) (on he right), where
(clâˆ’wl <crâˆ’wr) âˆ§(cl+wl >crâˆ’wr) âˆ§(cl+wl <cr+wr) holds. If the classes assigned to these
two plateaus are the same, i.e., kl = kr, then they are merged to become one plateau whose width
covers the segment merged from the two plateausâ€™ segments. Hence, the half-width wâ€² and center
câ€² of the new plateau are wâ€² = (cr + wr âˆ’cl + wl)/2 and câ€² = cl âˆ’wl + wâ€², respectively. If, on
the other hand, different classes are assigned to the two plateaus, i.e., kl Ì¸= kr, their half-widths are
reduced to remove any overlap between them. As a result, separating the two plateaus at the mid-
point m(= (cl + cr)/2) between their centers, after the reduction the left plateau has the half-width
wâ€²
l = (m âˆ’(cl âˆ’wl))/2 and the center at câ€²
l = cl âˆ’wl + wâ€²
l, and the right plateau has the half-width
wâ€²
r = (cr + wr âˆ’m)/2 and the center câ€²
r = cr + wr âˆ’wâ€²
r. Note that the labels of queried data
points should not change as a result of this reduction. If the right plateau covers multiple queried
data points whose timestamps are smaller than cr, the timestamp of the leftmost queried data point
becomes a new cr. The same is applicable to the left plateau, except the change of the direction."
REFERENCES,0.6686046511627907,Published as a conference paper at ICLR 2022
REFERENCES,0.6715116279069767,"D
EFFICACY OF TCLP FOR EACH QUERY SELECTION METHOD"
REFERENCES,0.6744186046511628,"Figures 6â€“11 show the efï¬cacy of each label propagation (LP) approach combined with each query
selection method. The performance plot shown in Figure 4 is the average of these results over the
six query selection methods. For all query selection methods, TCLP is shown to be effective in
improving active learning performance compared to the other label propagation approaches."
REFERENCES,0.6773255813953488,"NOP
PTP
ESP
TCLP"
REFERENCES,0.6802325581395349,"0
0.005
0.01 0.15 0.29 0.44 F1@25"
REFERENCES,0.6831395348837209,"0
0.05
0.1 0.23 0.46 0.68"
REFERENCES,0.686046511627907,"0
0.004
0.008 0.08 0.15 0.23"
REFERENCES,0.688953488372093,"0
0.002
0.004 0.26 0.46 0.65"
REFERENCES,0.6918604651162791,"0
0.005
0.01
Queried data ratio 0.23 0.39 0.56"
REFERENCES,0.6947674418604651,TS accuracy
REFERENCES,0.6976744186046512,"0
0.05
0.1
Queried data ratio 0.25 0.45 0.65"
REFERENCES,0.7005813953488372,"0
0.004
0.008
Queried data ratio 0.37 0.53 0.69"
REFERENCES,0.7034883720930233,"0
0.002
0.004
Queried data ratio 0.53 0.69 0.86"
REFERENCES,0.7063953488372093,"(a) 50salads.
(b) GTEA.
(c) mHealth.
(d) HAPT."
REFERENCES,0.7093023255813954,Figure 6: Efï¬cacy of the four LP approaches with CONF query selection.
REFERENCES,0.7122093023255814,"0
0.005
0.01 0.13 0.25 0.37 F1@25"
REFERENCES,0.7151162790697675,"0
0.05
0.1 0.2 0.4 0.6"
REFERENCES,0.7180232558139535,"0
0.004
0.008 0.02 0.05 0.07"
REFERENCES,0.7209302325581395,"0
0.002
0.004 0.26 0.48 0.69"
REFERENCES,0.7238372093023255,"0
0.005
0.01
Queried data ratio 0.19 0.34 0.5"
REFERENCES,0.7267441860465116,TS accuracy
REFERENCES,0.7296511627906976,"0
0.05
0.1
Queried data ratio 0.22 0.4 0.58"
REFERENCES,0.7325581395348837,"0
0.004
0.008
Queried data ratio 0.29 0.43 0.58"
REFERENCES,0.7354651162790697,"0
0.002
0.004
Queried data ratio 0.49 0.67 0.84"
REFERENCES,0.7383720930232558,"(a) 50salads.
(b) GTEA.
(c) mHealth.
(d) HAPT."
REFERENCES,0.7412790697674418,Figure 7: Efï¬cacy of the four LP approaches with ENTROPY query selection.
REFERENCES,0.7441860465116279,"0
0.005
0.01 0.21 0.41 0.6 F1@25"
REFERENCES,0.747093023255814,"0
0.05
0.1 0.24 0.48 0.72"
REFERENCES,0.75,"0
0.004
0.008 0.16 0.32 0.48"
REFERENCES,0.752906976744186,"0
0.002
0.004 0.31 0.57 0.82"
REFERENCES,0.7558139534883721,"0
0.005
0.01
Queried data ratio 0.29 0.49 0.69"
REFERENCES,0.7587209302325582,TS accuracy
REFERENCES,0.7616279069767442,"0
0.05
0.1
Queried data ratio 0.25 0.46 0.66"
REFERENCES,0.7645348837209303,"0
0.004
0.008
Queried data ratio 0.39 0.57 0.74"
REFERENCES,0.7674418604651163,"0
0.002
0.004
Queried data ratio 0.62 0.76 0.91"
REFERENCES,0.7703488372093024,"(a) 50salads.
(b) GTEA.
(c) mHealth.
(d) HAPT."
REFERENCES,0.7732558139534884,Figure 8: Efï¬cacy of the four LP approaches with MARG query selection.
REFERENCES,0.7761627906976745,Published as a conference paper at ICLR 2022
REFERENCES,0.7790697674418605,"NOP
PTP
ESP
TCLP"
REFERENCES,0.7819767441860465,"0
0.005
0.01 0.19 0.37 0.55 F1@25"
REFERENCES,0.7848837209302325,"0
0.05
0.1 0.22 0.44 0.66"
REFERENCES,0.7877906976744186,"0
0.004
0.008 0.19 0.39 0.58"
REFERENCES,0.7906976744186046,"0
0.002
0.004 0.32 0.58 0.83"
REFERENCES,0.7936046511627907,"0
0.005
0.01
Queried data ratio 0.26 0.46 0.65"
REFERENCES,0.7965116279069767,TS accuracy
REFERENCES,0.7994186046511628,"0
0.05
0.1
Queried data ratio 0.25 0.44 0.62"
REFERENCES,0.8023255813953488,"0
0.004
0.008
Queried data ratio 0.4 0.61 0.81"
REFERENCES,0.8052325581395349,"0
0.002
0.004
Queried data ratio 0.55 0.74 0.92"
REFERENCES,0.8081395348837209,"(a) 50salads.
(b) GTEA.
(c) mHealth.
(d) HAPT."
REFERENCES,0.811046511627907,Figure 9: Efï¬cacy of the four LP approaches with CS query selection.
REFERENCES,0.813953488372093,"0
0.005
0.01 0.16 0.31 0.47 F1@25"
REFERENCES,0.8168604651162791,"0
0.05
0.1 0.24 0.48 0.71"
REFERENCES,0.8197674418604651,"0
0.004
0.008 0.1 0.2 0.31"
REFERENCES,0.8226744186046512,"0
0.002
0.004 0.33 0.59 0.85"
REFERENCES,0.8255813953488372,"0
0.005
0.01
Queried data ratio 0.27 0.43 0.6"
REFERENCES,0.8284883720930233,TS accuracy
REFERENCES,0.8313953488372093,"0
0.05
0.1
Queried data ratio 0.27 0.49 0.7"
REFERENCES,0.8343023255813954,"0
0.004
0.008
Queried data ratio 0.33 0.5 0.66"
REFERENCES,0.8372093023255814,"0
0.002
0.004
Queried data ratio 0.64 0.78 0.93"
REFERENCES,0.8401162790697675,"(a) 50salads.
(b) GTEA.
(c) mHealth.
(d) HAPT."
REFERENCES,0.8430232558139535,Figure 10: Efï¬cacy of the four LP approaches with BADGE query selection.
REFERENCES,0.8459302325581395,"0
0.005
0.01 0.21 0.4 0.59 F1@25"
REFERENCES,0.8488372093023255,"0
0.05
0.1 0.23 0.46 0.69"
REFERENCES,0.8517441860465116,"0
0.004
0.008 0.21 0.42 0.63"
REFERENCES,0.8546511627906976,"0
0.002
0.004 0.46 0.66 0.86"
REFERENCES,0.8575581395348837,"0
0.005
0.01
Queried data ratio 0.31 0.49 0.67"
REFERENCES,0.8604651162790697,TS accuracy
REFERENCES,0.8633720930232558,"0
0.05
0.1
Queried data ratio 0.25 0.45 0.64"
REFERENCES,0.8662790697674418,"0
0.004
0.008
Queried data ratio 0.41 0.66 0.9"
REFERENCES,0.8691860465116279,"0
0.002
0.004
Queried data ratio 0.81 0.88 0.94"
REFERENCES,0.872093023255814,"(a) 50salads.
(b) GTEA.
(c) mHealth.
(d) HAPT."
REFERENCES,0.875,Figure 11: Efï¬cacy of the four LP approaches with UTILITY query selection.
REFERENCES,0.877906976744186,Published as a conference paper at ICLR 2022
REFERENCES,0.8808139534883721,"E
EVALUATION OF PROPAGATED LABELS"
REFERENCES,0.8837209302325582,"Figures 12â€“17 show the propagation quality of each label propagation approach, again demonstrat-
ing the superiority of TCLP. These plots provide the overall trends including the ï¬nal round results
reported in Table 3. The correct propagation ratio (CPR), i.e., the number of correctly propagated
labels / the total number of data points, is measured at each round using a speciï¬c query selection
method on each dataset. The CPR of TCLP is shown to be higher than those of the other label
propagation approaches throughout the entire period (i.e., from the 1st through the 15th round)."
REFERENCES,0.8866279069767442,"NOP
PTP
ESP
TCLP"
REFERENCES,0.8895348837209303,"0
0.005
0.01
Queried data ratio 0.04 0.09 0.13 CPR"
REFERENCES,0.8924418604651163,"0
0.05
0.1
Queried data ratio 0.17 0.33 0.5"
REFERENCES,0.8953488372093024,"0
0.004
0.008
Queried data ratio 0.04 0.07 0.11"
REFERENCES,0.8982558139534884,"0
0.004
0.008
Queried data ratio 0.02 0.03 0.05"
REFERENCES,0.9011627906976745,"(a) 50salads.
(b) GTEA.
(c) mHealth.
(d) HAPT."
REFERENCES,0.9040697674418605,Figure 12: CPR of the four LP approaches with CONF query selection.
REFERENCES,0.9069767441860465,"0
0.005
0.01
Queried data ratio 0.03 0.07 0.1 CPR"
REFERENCES,0.9098837209302325,"0
0.05
0.1
Queried data ratio 0.14 0.27 0.4"
REFERENCES,0.9127906976744186,"0
0.004
0.008
Queried data ratio 0.02 0.05 0.07"
REFERENCES,0.9156976744186046,"0
0.004
0.008
Queried data ratio 0.01 0.02 0.03"
REFERENCES,0.9186046511627907,"(a) 50salads.
(b) GTEA.
(c) mHealth.
(d) HAPT."
REFERENCES,0.9215116279069767,Figure 13: CPR of the four LP approaches with ENTROPY query selection.
REFERENCES,0.9244186046511628,"0
0.005
0.01
Queried data ratio 0.12 0.24 0.37 CPR"
REFERENCES,0.9273255813953488,"0
0.05
0.1
Queried data ratio 0.15 0.28 0.42"
REFERENCES,0.9302325581395349,"0
0.004
0.008
Queried data ratio 0.07 0.13 0.2"
REFERENCES,0.9331395348837209,"0
0.004
0.008
Queried data ratio 0.05 0.10 0.15"
REFERENCES,0.936046511627907,"(a) 50salads.
(b) GTEA.
(c) mHealth.
(d) HAPT."
REFERENCES,0.938953488372093,Figure 14: CPR of the four LP approaches with MARG query selection.
REFERENCES,0.9418604651162791,"0
0.005
0.01
Queried data ratio 0.10 0.20 0.31 CPR"
REFERENCES,0.9447674418604651,"0
0.05
0.1
Queried data ratio 0.15 0.29 0.44"
REFERENCES,0.9476744186046512,"0
0.004
0.008
Queried data ratio 0.07 0.14 0.21"
REFERENCES,0.9505813953488372,"0
0.004
0.008
Queried data ratio 0.05 0.10 0.15"
REFERENCES,0.9534883720930233,"(a) 50salads.
(b) GTEA.
(c) mHealth.
(d) HAPT."
REFERENCES,0.9563953488372093,Figure 15: CPR of the four LP approaches with CS query selection.
REFERENCES,0.9593023255813954,Published as a conference paper at ICLR 2022
REFERENCES,0.9622093023255814,"NOP
PTP
ESP
TCLP"
REFERENCES,0.9651162790697675,"0
0.005
0.01
Queried data ratio 0.06 0.13 0.19 CPR"
REFERENCES,0.9680232558139535,"0
0.05
0.1
Queried data ratio 0.14 0.27 0.4"
REFERENCES,0.9709302325581395,"0
0.004
0.008
Queried data ratio 0.07 0.13 0.2"
REFERENCES,0.9738372093023255,"0
0.004
0.008
Queried data ratio 0.05 0.11 0.16"
REFERENCES,0.9767441860465116,"(a) 50salads.
(b) GTEA.
(c) mHealth.
(d) HAPT."
REFERENCES,0.9796511627906976,Figure 16: CPR of the four LP approaches with BADGE query selection.
REFERENCES,0.9825581395348837,"0
0.005
0.01
Queried data ratio 0.12 0.23 0.35 CPR"
REFERENCES,0.9854651162790697,"0
0.05
0.1
Queried data ratio 0.18 0.35 0.51"
REFERENCES,0.9883720930232558,"0
0.004
0.008
Queried data ratio 0.11 0.22 0.33"
REFERENCES,0.9912790697674418,"0
0.004
0.008
Queried data ratio 0.09 0.17 0.26"
REFERENCES,0.9941860465116279,"(a) 50salads.
(b) GTEA.
(c) mHealth.
(d) HAPT."
REFERENCES,0.997093023255814,Figure 17: CPR of the four LP approaches with UTILITY query selection.
