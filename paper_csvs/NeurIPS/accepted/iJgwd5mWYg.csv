Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0015060240963855422,"We address a generalization of the bandit with knapsacks problem, where a learner
aims to maximize rewards while satisfying an arbitrary set of long-term constraints.
Our goal is to design best-of-both-worlds algorithms that perform optimally under
both stochastic and adversarial constraints. Previous works address this problem
via primal-dual methods, and require some stringent assumptions, namely the
Slater’s condition, and in adversarial settings, they either assume knowledge of
a lower bound on the Slater’s parameter, or impose strong requirements on the
primal and dual regret minimizers such as requiring weak adaptivity. We propose
an alternative and more natural approach based on optimistic estimations of the
constraints. Surprisingly, we show that estimating the constraints with an UCB-
like approach guarantees optimal performances. Our algorithm consists of two
main components: (i) a regret minimizer working on moving strategy sets and
(ii) an estimate of the feasible set as an optimistic weighted empirical mean of
previous samples. The key challenge in this approach is designing adaptive weights
that meet the different requirements for stochastic and adversarial constraints.
Our algorithm is signiﬁcantly simpler than previous approaches, and has a cleaner
analysis. Moreover, ours is the ﬁrst best-of-both-worlds algorithm providing bounds
logarithmic in the number of constraints. Additionally, in stochastic settings, it
provides eO(
√"
ABSTRACT,0.0030120481927710845,T) regret without Slater’s condition.
INTRODUCTION,0.004518072289156626,"1
Introduction"
INTRODUCTION,0.006024096385542169,"We address the problem faced by a decision maker who aims to maximize its cumulative reward over
a time horizon T, while satisfying an arbitrary set of m long-term constraints. At each round t, the
learner selects an action at from a ﬁnite set of K actions, and then observes a reward ft(at) and
some costs gt(at) ∈[−1, 1]m. The goal is to design best-of-both-worlds algorithms for this problem
that perform optimally under both stochastic and adversarial constraints. We always assume rewards
are generated adversarially. This is because the real complexity of the problem is captured by the
nature of the constraints, so that transitioning from adversarial to stochastic rewards under the same
type of constraints does not affect our results."
INTRODUCTION,0.007530120481927711,"The ﬁrst works on bandits with constraints focus on budget constraints, a.k.a bandit with knapsack
(BwK) [7] study the settings in which both rewards and constraints are i.i.d. and propose an UCB-
based approach, combined with primal-dual method. Agrawal and Devanur [2] provide an UCB-like
approach for more general rewards and costs. Immorlica et al. [21], Kesselheim and Singla [22]"
INTRODUCTION,0.009036144578313253,"analyse settings with adversarial constraints and rewards, providing a primal-dual algorithm to tackle
the problem. Castiglioni et al. [15] show that a similar primal-dual approach provides best-of-both-
worlds guarantees. Many subsequent works extend the setting to more general constraints, mostly
employing primal-dual methods [16, 17, 28, 11, 13, 10, 18]. Primal-dual methods have been the
only effective method that provides best-of-both-worlds guarantees for bandits with constraints
[16, 17, 11, 13, 10]. However, such methods require assumptions that are particularly stringent in
settings beyond knapsack constraints. First, they require the existence of a strictly feasible solution
(i.e., Slater’s condition) to avoid a regret of order O(T
3/4) [16, 28]. While this assumption always
holds in bandits with knapsack setting (where “doing nothing” incurs in a negative cost equal to
the per-round budget), this assumption is far more stringent with general constraints. Moreover,
some works require the knowledge of a lower bound on the Slater’s parameter [16, 28]. Subsequent
works circumvent this assumption at the expense of strong requirements on the primal and dual regret
minimizers [17, 11, 13, 1]. In particular, such approaches require weakly-adaptive primal and dual
regret minimizers. The challenge of applying such primal-dual algorithms to bandit beyond knapsack
constraint is reﬂected in regret bounds that exhibit non-optimal dependencies on some parameters. For
instance, a polynomial (instead of logarithmic) dependence on the number of constraints [17, 11, 13].
For further pointers to the literature, we refer to Appendix A."
OUR CONTRIBUTION,0.010542168674698794,"1.1
Our contribution"
OUR CONTRIBUTION,0.012048192771084338,"We propose an alternative and insightful approach to design best-of-both-worlds algorithms for
bandit with long-term constraints. Our method relies on optimistic estimations of the constraints
through a weighted empirical mean of past samples. Surprisingly, we demonstrate that using a
UCB-based approach to estimate the constraints ensures optimal performance under both stochastic
and adversarial constraints. Our algorithm differs signiﬁcantly from previous UCB-based approaches.
For instance, it guarantees no-regret even with adversarial rewards and stochastic constraints, unlike
previous works [2, 7, 23]. Moreover, it is the ﬁrst UCB-like approach that provides an optimal
competitive ratio of 1 + 1/ρ with adversarial constraints, where ρ is the unknown Slater’s parameter."
OUR CONTRIBUTION,0.01355421686746988,"Our algorithm consists of two simple components. The ﬁrst is an adversarial regret minimizer
working on moving strategy sets. In particular, at each round, the regret minimizer chooses a strategy
in the current optimistic estimation of the feasible set, and is required to achieve no-regret with
respect to any strategy in the intersection of all feasibility set estimations. The second component is
a tool for estimating the feasible set using an optimistic weighted mean of previous samples. The
key challenge in this approach is designing adaptive weights that meet the different requirements
for stochastic and adversarial constraints. Intuitively, in stochastic settings, we aim to converge to
the (unweighted) empirical mean of the observed constraints. Conversely, in adversarial settings, we
should assign larger weights to recent samples to address time-dependent constraints."
OUR CONTRIBUTION,0.015060240963855422,"Not only is our algorithm signiﬁcantly simpler than previous approaches, with a clean and insightful
analysis, but it also provides better theoretical performance than primal-dual methods. Indeed, it is
the ﬁrst best-of-both-worlds algorithm to provide bounds logarithmic in the number of constraints.
Moreover, in stochastic settings, it is the ﬁrst algorithm to provide eO(
√"
OUR CONTRIBUTION,0.016566265060240965,"T) regret without requiring
Slater’s condition. Finally, it guarantees that the expected violation in the current round converges to
zero, making our algorithm “converge"" to strategies that are feasible in expectation. This provides a
more stable and consistent control on the violations."
MODEL AND PRELIMINARIES,0.018072289156626505,"2
Model and Preliminaries"
MODEL AND PRELIMINARIES,0.01957831325301205,"We address the problem faced by an agent aiming at maximizing its cumulative reward over a time
horizon T, while satisfying JmK long-term constraints.1 The agent has a set JKK of available actions
and, at each round t ∈JTK, selects at ∈JKK. The agent then observes the corresponding reward
ft(at) ∈[0, 1] and a cost g(i)
t (at) ∈[−1, 1], for each constraint i ∈JmK. We deﬁne the cumulative
violation of the ith constraint as"
MODEL AND PRELIMINARIES,0.02108433734939759,"V (i)
T
:= P"
MODEL AND PRELIMINARIES,0.022590361445783132,"t∈JT K g(i)
t (at),"
MODEL AND PRELIMINARIES,0.024096385542168676,"1For any N ∈N, we use JNK to denote the set {1, . . . , N}."
MODEL AND PRELIMINARIES,0.025602409638554216,"while VT := maxi∈JmK V (i)
T
is the maximum violation across all constraints. At a high level, we"
MODEL AND PRELIMINARIES,0.02710843373493976,"want to minimize the regret while keeping the violation of each constraint V (i)
T
sublinear in T."
MODEL AND PRELIMINARIES,0.0286144578313253,"The focus of this paper is on handling both stochastic and adversarial constraints. Conversely, we
always assume the rewards to be generated up-front by an adversary; we do not treat explicitly the
situation where the rewards are generated i.i.d. because our guarantees are already tight for the harder
case of adversarial rewards.2 In the stochastic setting, we assume that gt = {g(i)
t }i∈JmK is drawn
i.i.d. from a ﬁxed but unknown distribution G, and we let ¯g(i)(a) = Eg∼G[g(i)(a)] be the expected
cost of action a for the ith constraint. On the other hand, in the adversarial setting {gt}t∈JT K is an
arbitrary sequence of cost functions."
MODEL AND PRELIMINARIES,0.030120481927710843,"Let ∆K to be the set of discrete probability distributions over the set JKK. Then, at round t ∈JTK,
given a randomized strategy xt ∈∆K, the expected learner reward is P
a∈JKK ft(a)xt(a) = ⟨xt, ft⟩."
MODEL AND PRELIMINARIES,0.03162650602409638,"Similarly, ⟨xt, g(i)
t ⟩denotes the expected cost of the ith constraint. Finally, we use nt(a) to denote
the number of times arm a was played up to time t, i.e., nt(a) = Pt
τ=1 I(aτ = a)."
MODEL AND PRELIMINARIES,0.03313253012048193,"We want to design algorithms which achieve good performances in both the adversarial and the
stochastic setting. As it is customary in the literature, we compare our learning algorithm with
different benchmarks according to the setting."
MODEL AND PRELIMINARIES,0.03463855421686747,"Stochastic Benchmark
In the stochastic setting, the constraints g(i)
t
are i.i.d. samples with mean
¯g(i) and thus we consider as benchmark the best ﬁxed randomized strategy that satisﬁes the constraints
in expectation, which is a standard choice in bandits with constraints [11, 28, 16]. Formally, in the
stochastic setting, we can deﬁne the feasible sets X ⋆
i and X ⋆as follows:"
MODEL AND PRELIMINARIES,0.03614457831325301,"X ⋆
i :=
n
x ∈∆K : ⟨x, ¯g(i)⟩≤0
o
and
X ⋆:= ∩i∈JmKX ⋆
i ."
MODEL AND PRELIMINARIES,0.03765060240963856,"Then, we can deﬁne the stochastic baseline as:"
MODEL AND PRELIMINARIES,0.0391566265060241,"OPTS := max
x∈X ⋆
X"
MODEL AND PRELIMINARIES,0.04066265060240964,"t∈JT K
⟨x, ft⟩."
MODEL AND PRELIMINARIES,0.04216867469879518,"We naturally assume the existence of safe mixed strategies, i.e., that X ⋆̸= ∅. This is equivalent to
assume the existence of a randomized strategy x∅such that ⟨x∅, ¯g(i)⟩≤0 for all i. Notice that this
is a weaker assumption than the one commonly assumed by best-of-both-worlds algorithms in which
⟨x∅, ¯g(i)⟩≤−ρ, where ρ is a strictly positive constant (see, e.g., [16, 11, 10])."
MODEL AND PRELIMINARIES,0.043674698795180725,"Adversarial Benchmark
In the adversarial setting, {gt}t∈JT K is an arbitrary sequence of con-
straints. We consider as benchmark the best unconstrained strategy:"
MODEL AND PRELIMINARIES,0.045180722891566265,"OPTA := max
x∈∆K X"
MODEL AND PRELIMINARIES,0.046686746987951805,"t∈JT K
⟨x, ft⟩."
MODEL AND PRELIMINARIES,0.04819277108433735,"While this baseline has already been used [e.g., 11, 13]), other works on adversarial bandit with
constraints employ weaker baselines [e.g., 21, 16]. For instance, Castiglioni et al. [16] consider the
best ﬁxed strategy which is feasible on average. However, we show that, despite using a stronger
baseline, we obtain a competitive ratio that is optimal even for the weaker baselines commonly
adopted in the literature [16, 10, 9]."
BEST-OF-BOTH-WORLDS GUARANTEES,0.04969879518072289,"2.1
Best-Of-Both-Worlds Guarantees"
BEST-OF-BOTH-WORLDS GUARANTEES,0.05120481927710843,"Our goal is to design learning algorithms that exhibit optimal guarantees both in the stochastic
and adversarial settings. In the stochastic setting, we are interested in minimizing the regret RT
w.r.t. OPTS:
RT = OPTS −P"
BEST-OF-BOTH-WORLDS GUARANTEES,0.05271084337349398,"t∈JT K ft(at),"
BEST-OF-BOTH-WORLDS GUARANTEES,0.05421686746987952,"2Indeed, when the constraints are stochastic, we obtain the state-of-the-art eO(
√"
BEST-OF-BOTH-WORLDS GUARANTEES,0.05572289156626506,"T) regret even with adversar-
ial rewards."
BEST-OF-BOTH-WORLDS GUARANTEES,0.0572289156626506,Algorithm 1
BEST-OF-BOTH-WORLDS GUARANTEES,0.058734939759036146,"Require: bonuses bt(a), weights w(i)
t,a and parameter β
1: Initialize regret minimizer R with β
2: for each step t = 1, . . . , T do
3:
Estimation:
4:
ˆg(i)
t (a) ←P"
BEST-OF-BOTH-WORLDS GUARANTEES,0.060240963855421686,"τ∈Tt−1,a w(i)
t,a(τ)g(i)
τ (a) for all a ∈JKK and i ∈JmK"
BEST-OF-BOTH-WORLDS GUARANTEES,0.061746987951807226,"5:
b
X (i)
t
←{x ∈∆K : ⟨x, ˆg(i)
t
−bt⟩≤0}"
BEST-OF-BOTH-WORLDS GUARANTEES,0.06325301204819277,"6:
b
Xt ←∩i∈JmK b
X (i)
t
7:
Regret minimization:
8:
Get prediction from R on set b
Xt: xt ←R( b
Xt)
9:
Sample at ∼xt and receive {g(i)
t (at)}i∈JmK and ft(at)"
BEST-OF-BOTH-WORLDS GUARANTEES,0.06475903614457831,"and speciﬁcally we require both RT and VT to be in eO(
√"
BEST-OF-BOTH-WORLDS GUARANTEES,0.06626506024096386,"T) with high probability. This clearly
matches the standard Ω(
√"
BEST-OF-BOTH-WORLDS GUARANTEES,0.0677710843373494,T) lower bound that holds even without constraints [5].
BEST-OF-BOTH-WORLDS GUARANTEES,0.06927710843373494,"In the (harder) adversarial setting, we pose the less ambitious goal of achieving a constant competitive
ratio with respect to OPTA, or equivalently sublinear α-regret with constant α. Formally, given an
α < 1, we deﬁne the α-regret as:"
BEST-OF-BOTH-WORLDS GUARANTEES,0.07078313253012049,α-RT = α · OPTA −P
BEST-OF-BOTH-WORLDS GUARANTEES,0.07228915662650602,t∈JT K ft(at).
BEST-OF-BOTH-WORLDS GUARANTEES,0.07379518072289157,"As it is customary in the literature [15], the competitive ratio α obtained by our algorithms depends
on the following Slater’s parameter ρ:"
BEST-OF-BOTH-WORLDS GUARANTEES,0.07530120481927711,"ρ = −inf
a∈JKK
max
t∈JT K,i∈JmK g(i)
t (a).
(1)"
BEST-OF-BOTH-WORLDS GUARANTEES,0.07680722891566265,"The parameter ρ is related to the existence of strictly-feasible actions, and only depends on the
constraints. Our deﬁnition is slightly stronger than the one in most previous works where the inf is
over randomized strategies. To guarantee the existence of a feasible strategy we assume that ρ ≥0.
Then, our goal is to guarantee that both VT and the α-regret, with α = ρ/ρ+1, belong to eO(
√"
BEST-OF-BOTH-WORLDS GUARANTEES,0.0783132530120482,"T) with
high probability. Note that this matches the lower bound of Bernasconi et al. [11]."
OUR APPROACH,0.07981927710843373,"3
Our Approach"
OUR APPROACH,0.08132530120481928,"In this section, we present the main components of our algorithm, while the following sections will
describe the speciﬁc components in details. We refer to Algorithm 1 for the pseudocode. At each
step t, the algorithm works in two phases: i) it estimates the feasible set, and ii) it plays a strategy in
the estimated set. Each phase requires a speciﬁc ingredient:"
OUR APPROACH,0.08283132530120482,"i) An estimator ˆg(i)
t
of the costs functions g(i)
t
that is used together with the optimistic bonus
bt to deﬁne the estimation of the feasible set deﬁned as b
Xt := ∩i∈JmK b
X (i)
t
. In the stochastic
case, we would like b
Xt ⊇X ⋆, while in the adversarial case our goal is to maintain a
sequence of sets that always contains a version of the action set X, properly scaled around
a∅(see Equation (2) for a formal deﬁnition)."
OUR APPROACH,0.08433734939759036,"ii) A regret minimizer R for adversarial linear reward function that, at each round, takes in
input a convex set of feasible strategies b
Xt ⊆∆K, and then selects a strategy xt ∈b
Xt. We
require the regret minimizer to achieve ˜O(
√"
OUR APPROACH,0.0858433734939759,"KT) regret with respect to any x ∈∩t∈JT K b
Xt;"
OUR APPROACH,0.08734939759036145,"In the following we deﬁne the two phases more in details. Let Tt,a := {τ ≤t : at = a} be the set
of rounds in which the algorithm plays action a. Then, at each round t, Algorithm 1 computes the
estimate
ˆg(i)
t (a) =
X"
OUR APPROACH,0.08885542168674698,"τ∈Tt−1,a
w(i)
t,a(τ)gτ(a)
∀a ∈JKK and i ∈JmK"
OUR APPROACH,0.09036144578313253,"Algorithm 2 No Regret on Moving Sets
Require: Parameter β > 0"
OUR APPROACH,0.09186746987951808,"1: Set γ = β/2
2: for each step t = 1, . . . , T do
3:
Receive b
Xt
4:
ˆxt(a) ←xt−1(a)eβ( ˆ
ft−1(a)−1) for all a ∈JKK
5:
xt ←Π b
Xt(ˆxt) := arg minx∈b
Xt B(x||ˆxt)
6:
Sample at ∼xt
7:
Observe ft(at) and set ˆft(a) ←1 for all a ̸= at and ˆft(at) ←1 −1−ft(at)"
OUR APPROACH,0.09337349397590361,xt(at)+γ
OUR APPROACH,0.09487951807228916,"as the weighted mean of available past observations {g(i)
τ (a)}τ∈Jt−1K for each actions a ∈JKK and
constraint i ∈JmK, for some weights w(i)
t,a.3 Then, the estimates together with the optimistic bonus
{bt(a)}a∈JKK are used to deﬁne the moving sets b
Xt, which are fed to the regret minimizer R which
in turn selects a point xt ∈b
Xt."
OUR APPROACH,0.0963855421686747,"One crucial property that is required for the execution of the regret minimizer R is that all the sets b
Xt
are non-empty (as otherwise the regret minimizer has no feasible strategies). To simplify exposition,
in the following sections we assume that the clean event C :=
n
b
Xt ̸= {∅} ∀t ∈JTK
o
holds. In
Corollary 6.3, we prove that this event holds with high probability in the stochastic setting, while in
Theorem 5.2 we argue that it holds deterministically in the adversarial one."
OUR APPROACH,0.09789156626506024,"In Algorithm 1 we left unspeciﬁed two crucial parts of our approach. The ﬁrst is how to build the
regret minimizer R, and the second concerns how to actually generate the sets b
Xt, i.e., the weights
w(i)
t,a and the bonus bt(a). We delve into these details in Section 4 and Section 5, respectively."
NO-REGRET ON MOVING SETS,0.09939759036144578,"4
No-regret on moving sets"
NO-REGRET ON MOVING SETS,0.10090361445783133,"We describe the regret minimizer R that exhibits no-regret with respect to any x ∈∩t∈JT K b
Xt.
We achieve this via a simple modiﬁcation to the EXP-IX algorithm of Neu [25] that provides
high probability results for multi-armed bandits via implicit exploration. More speciﬁcally, our
algorithm maintains a randomized strategy xt ∈∆K which is updated using the biased reward
estimate ˆft(a) as in Neu [25] and then projected onto b
Xt according to the negative entropy Bregman
divergence B(x||y) = P"
NO-REGRET ON MOVING SETS,0.10240963855421686,"a∈JKK [x(a) log (x(a)/y(a)) −x(a) + y(a)]. We refer to Algorithm 2 for the
pseudocode, and present here the main result of the Section.
Theorem 4.1. Let xt be selected accordingly to Algorithm 2 run with arbitrary sequence of convex"
NO-REGRET ON MOVING SETS,0.10391566265060241,"sets b
Xt ⊆∆K with γ = β"
NO-REGRET ON MOVING SETS,0.10542168674698796,"2 and β =
q"
NO-REGRET ON MOVING SETS,0.10692771084337349,log(K/δ1)
NO-REGRET ON MOVING SETS,0.10843373493975904,"KT
. Then, with probability at least 1 −δ1 it holds that
X"
NO-REGRET ON MOVING SETS,0.10993975903614457,"t∈JT K
⟨ft, x⟩−ft(at) ≤4
p"
NO-REGRET ON MOVING SETS,0.11144578313253012,"KT log(K/δ1),
∀x ∈
\"
NO-REGRET ON MOVING SETS,0.11295180722891567,"t∈JT K
b
Xt."
NO-REGRET ON MOVING SETS,0.1144578313253012,"This result establishes no-regret in the case of moving sets, taking as benchmark the optimal strategy
in the intersection of all sets. To exploit this result in Algorithm 1, we have to make sure that in both
the stochastic and adversarial setting the intersection of the sets b
Xt contains “good” strategies. In the
stochastic setting, we show that with high probability it includes X ⋆, while, in the adversarial setting,
it includes a strategy with utility ρ/1+ρ · OPTA."
"HOW TO BUILD THE SETS B
XT",0.11596385542168675,"5
How to build the sets b
Xt"
"HOW TO BUILD THE SETS B
XT",0.11746987951807229,"In this section, we show how to design estimations b
Xt of the feasible sets that, surprisingly, are
effective both in stochastic and adversarial settings. Indeed, the main challenge is to design sets b
Xt"
"HOW TO BUILD THE SETS B
XT",0.11897590361445783,"3If a given action has been played at least once, we require P"
"HOW TO BUILD THE SETS B
XT",0.12048192771084337,"τ∈Tt−1,a w(i)
t,a(τ) = 1, i.e., that ˆg(i)
t (a) is
actually a weighted mean. Otherwise, the estimation is simply set to 0."
"HOW TO BUILD THE SETS B
XT",0.12198795180722892,"that accommodate the different requirements of the two settings. First, in Section 5.1, we discuss
how to set the optimistic bonuses bt and then in Section 5.2 we focus on how to set the weights w(i)
t,a."
HOW TO SET THE OPTIMISTIC BONUS,0.12349397590361445,"5.1
How to set the optimistic bonus"
HOW TO SET THE OPTIMISTIC BONUS,0.125,"The optimistic bonuses have the main purpose of balancing the estimation error in the stochastic
setting. As the following lemma show, we simply need that |ˆg(i)
t (a) −¯g(i)(a)| ≤bt(a) with high
probability. Indeed, this is sufﬁcient to show that X ⋆⊆∩t∈JT K b
Xt in the stochastic setting."
HOW TO SET THE OPTIMISTIC BONUS,0.12650602409638553,"Theorem 5.1. Consider the stochastic setting. Given any δ > 0, let bt(a) be such that with probability
at least 1 −δ it holds:"
HOW TO SET THE OPTIMISTIC BONUS,0.1280120481927711,"|ˆg(i)
t (a) −¯g(i)(a)| ≤bt(a)
∀t ∈JTK, i ∈JmK, a ∈JKK."
HOW TO SET THE OPTIMISTIC BONUS,0.12951807228915663,"Then, it holds X ⋆⊆∩t∈JT K b
Xt with probability at least 1 −δ."
HOW TO SET THE OPTIMISTIC BONUS,0.13102409638554216,"Even tough it is crucial in the stochastic setting, it turns out that in the adversarial setting the optimistic
bonus bt is not really needed. Indeed, as we will show in the following, we are interested in obtaining
no-regret with respect to the set X ⋆
∅which is obtained via interpolation of points in X and the strictly
feasible actions a∅. Let x∅be such that x∅(a∅) = 1 and x∅(a) = 0 for all a ̸= a∅. Formally:"
HOW TO SET THE OPTIMISTIC BONUS,0.13253012048192772,"X ⋆
∅:=
1
1 + ρ{x∅} +
ρ
1 + ρX,
(2)"
HOW TO SET THE OPTIMISTIC BONUS,0.13403614457831325,"where A + B is the Minkowski sum between sets and αA indicates the set that contains each element
of A multiplied by α.4 The following theorem proves that X ⋆
∅⊆b
Xt for all t."
HOW TO SET THE OPTIMISTIC BONUS,0.1355421686746988,"Theorem 5.2. In the adversarial setting, it holds X ⋆
∅⊆b
Xt for all t ∈JTK."
HOW TO SET THE OPTIMISTIC BONUS,0.13704819277108435,"Notice that having no-regret with respect to the set X ⋆
∅is not sufﬁcient to achieve no-regret in the
adversarial setting. Nonetheless, we will show that this is sufﬁcient to guarantee no-α-regret, for
α = ρ/1+ρ with respect to any strategy x ∈∆K."
HOW TO SET THE WEIGHTS,0.13855421686746988,"5.2
How to set the weights"
HOW TO SET THE WEIGHTS,0.14006024096385541,"We focus on the design of estimators ˆg(i)
t
that are good approximations of the real functions g(i)
t .
Algorithm 1 computes the estimators ˆg(i)
t
by using a weighted mean of all past observations:"
HOW TO SET THE WEIGHTS,0.14156626506024098,"ˆg(i)
t (a) = P"
HOW TO SET THE WEIGHTS,0.1430722891566265,"τ∈Tt−1,a w(i)
t,a(τ)g(i)
τ (a)
∀t ∈JTK, a ∈JKK, i ∈JMK."
HOW TO SET THE WEIGHTS,0.14457831325301204,"However, to simplify the exposition, we use the following equivalence between online gradient"
HOW TO SET THE WEIGHTS,0.1460843373493976,"descent (OGD) on quadratic losses ˆg(i)
t (at) 7→1"
HOW TO SET THE WEIGHTS,0.14759036144578314,"2

g(i)
t (at) −ˆg(i)
t (at)
2
and weighted means. In"
HOW TO SET THE WEIGHTS,0.14909638554216867,"particular this equivalence is realized by observing that such loss has gradient g(i)
t (at) −ˆg(i)
t (at).
Lemma 5.3. Given any sequence {yt}t∈JT K such that y1 = 0 and any sequence of learning rates
{ηt}t∈JT K such that η1 = 1, let {ˆyt}t∈JT K be the estimator updated as:"
HOW TO SET THE WEIGHTS,0.15060240963855423,ˆyt+1 = ˆyt + ηt(yt −ˆyt).
HOW TO SET THE WEIGHTS,0.15210843373493976,"Then, it holds that ˆyt = Pt−1
τ=1 yτwt(τ) where wt(τ) = ητ
Qt−1
k=τ+1(1 −ηk).
Moreover,
Pt−1
τ=1 wt(τ) = 1 for any t ≥2."
HOW TO SET THE WEIGHTS,0.1536144578313253,"Clearly, in the OGD interpretation of our update, we only update ˆg(i)
t (a) only when at = a, and thus
we only need to deﬁne learning rates for action a for the times t in which at = a. Based on this
observation, we are going to update ˆg(i)
t (a) as
(
ˆg(i)
t+1(at) = ˆgi
t(at) + η(i)
t (at)

g(i)
t (at) −ˆgi
t(at)
"
HOW TO SET THE WEIGHTS,0.15512048192771086,"ˆg(i)
t+1(a) = ˆg(i)
t (a)
∀a ̸= at."
HOW TO SET THE WEIGHTS,0.1566265060240964,"4Formally Minkowski sum between sets A + B is deﬁned as A + B := {a + b : a ∈A, b ∈B}."
HOW TO SET THE WEIGHTS,0.15813253012048192,"Thus, given an action a ∈JKK and a time t ∈JTK the corresponding weights {w(i)
t,a(τ)}τ∈Tt−1,a are:"
HOW TO SET THE WEIGHTS,0.15963855421686746,"w(i)
t,a(τ) = η(i)
τ (a)
Y"
HOW TO SET THE WEIGHTS,0.16114457831325302,"k∈Tt−1,a:k>τ
(1 −η(i)
k (a))
∀τ ∈Tt−1,a"
HOW TO SET THE WEIGHTS,0.16265060240963855,"We now proceed to give two notable examples on how to instantiate the learning rates and recover
commonly used estimators such as the empirical mean and the exponentially weighted mean.5"
HOW TO SET THE WEIGHTS,0.16415662650602408,"Proposition 5.4. If η(i)
t (at) =
1
nt(at) for each τ ∈Tt−1,a, then w(i)
t,a(τ) =
1
nt−1(a) and we recover"
HOW TO SET THE WEIGHTS,0.16566265060240964,"the empirical mean estimator for ˆg(i)
t (a) =
1
nt−1(a)
P"
HOW TO SET THE WEIGHTS,0.16716867469879518,"τ∈Tt−1,a g(i)
τ (a)."
HOW TO SET THE WEIGHTS,0.1686746987951807,"Proposition 5.5. If η(i)
t (at) = η then"
HOW TO SET THE WEIGHTS,0.17018072289156627,"w(i)
t,a(τ) = η(1 −η)|{k∈Tt−1,a:k>τ}|"
HOW TO SET THE WEIGHTS,0.1716867469879518,"for each τ ∈Tt−1,a and we recover an exponentially weighted average estimator for ˆg(i)
t (a)."
HOW TO SET THE WEIGHTS,0.17319277108433734,"As it will turns out, these are the two extreme cases that we want to interpolate between. Indeed,
the empirical mean estimator is particularly effective in the stochastic case but ineffective in the
adversarial case, while the converse happens with the exponentially weighted estimator."
HOW TO SET THE WEIGHTS,0.1746987951807229,"Now, we show that the OGD interpretation is particularly useful to bounds the violations suffered by
the algorithm. First, we deﬁne the violations in an interval [t1, t2] := {t ∈JTK : t1 ≤t ≤t2} as:"
HOW TO SET THE WEIGHTS,0.17620481927710843,"V (i)
[t1,t2] = t2
X"
HOW TO SET THE WEIGHTS,0.17771084337349397,"t=t1
g(i)
t (at)."
HOW TO SET THE WEIGHTS,0.17921686746987953,"Then, in the following lemma we show that the violations in the interval are related to the variation of
the estimates ˆg(i)
t (a).
Theorem 5.6. Given an interval [t1, t2] ⊆JTK, an i ∈JmK, and a δ > 0, with probability at least
1 −δ it holds:"
HOW TO SET THE WEIGHTS,0.18072289156626506,"V (i)
[t1,t2] ≤
X a∈JKK X"
HOW TO SET THE WEIGHTS,0.1822289156626506,"τ∈Tt2,a∩[t1,t2] 1"
HOW TO SET THE WEIGHTS,0.18373493975903615,"η(i)
τ (a)"
HOW TO SET THE WEIGHTS,0.1852409638554217,"
ˆg(i)
τ+1(a) −ˆg(i)
τ (a)

+ t2
X"
HOW TO SET THE WEIGHTS,0.18674698795180722,"τ=t1
⟨xτ, bτ⟩+ 4
p"
HOW TO SET THE WEIGHTS,0.18825301204819278,(t2 −t1) log(1/δ).
HOW TO SET THE WEIGHTS,0.1897590361445783,"By a simple telescoping argument, we have the following corollary, which holds whenever the
learning rates are non-increasing within a time interval. Let ℓ(a, [t1, t2]) be the last rounds in the
interval [t1, t2] in which action a is played.
Corollary 5.7. Given an interval [t1, t2] ⊆JTK, a i ∈JmK, and a δ > 0, assume that for any
a ∈JKK it holds η(i)
τ (a) ≥η(i)
τ ′ (a) ∀τ < τ ′ ∈Tt2,a ∩[t1, t2]. Then, with probability at least 1 −δ it
holds:"
HOW TO SET THE WEIGHTS,0.19126506024096385,"V (i)
[t1,t2] ≤
X a∈JKK 2"
HOW TO SET THE WEIGHTS,0.1927710843373494,"η(i)
ℓ(a,[t1,t2])(a)
+ t2
X"
HOW TO SET THE WEIGHTS,0.19427710843373494,"τ=t1
⟨xτ, bτ⟩+ 4
p"
HOW TO SET THE WEIGHTS,0.19578313253012047,(t2 −t1) log(1/δ).
HOW TO SET THE WEIGHTS,0.19728915662650603,"Corollary 5.7 shows how to bound the violation as a function of the learning rates η(i)
t
and the bonus
terms bτ. The following lemma shows how to bound the second term of the violations depending on
the structure of the bonus terms.
Lemma 5.8. Given a c > 0, an α ∈(0, 1), a t ∈JTK, and a δ > 0, let bt(a) =
c
nt(a)α for all
a ∈JKK. Then, with probability at least 1 −δ, it holds: t
X"
HOW TO SET THE WEIGHTS,0.19879518072289157,"τ=1
⟨xτ, bτ⟩≤
c
1 −αKαt1−α + 4
p"
HOW TO SET THE WEIGHTS,0.2003012048192771,t log(1/δ).
HOW TO SET THE WEIGHTS,0.20180722891566266,"In this section, we saw how the choice of the learning rates of the estimator affects the estimators. In
the following section, we will see how to adaptively set those learning rates to handle both stochastic
and adversarial settings."
HOW TO SET THE WEIGHTS,0.2033132530120482,"5The proof of the ﬁrst proposition can be found in Appendix D, while the proof of the second is straightforward
and thus it is omitted."
ADAPTIVE LEARNING RATES,0.20481927710843373,"6
Adaptive learning rates"
ADAPTIVE LEARNING RATES,0.2063253012048193,"The previous section highlights the main difﬁculties of obtaining best-of-both-world algorithms: we
need to set the weights w(i)
t,a (or equivalently - by Lemma 5.3 - the learning rates η(i)
t (at)) and the
optimistic bonuses bt so that they meet, at the same time, the requirements needed by the stochastic
and the adversarial settings."
ADAPTIVE LEARNING RATES,0.20783132530120482,"We start presenting two possible choices and show that they fail either in the stochastic or the
adversarial setting. Then, we show how adaptive learning rates combine the strengths of both
approaches. The ﬁrst, natural, choice of setting the learning rate is to use an exponentially weighted
estimator, i.e., choose η(i)
t (at) = 1/
√"
ADAPTIVE LEARNING RATES,0.20933734939759036,"T. With this choice, we can apply a weighted version of Azuma-
Hoeffding inequality and ﬁnd that |ˆg(i)
t (a) −¯g(i)(a)| ∈eO
 
nt(a)−1/4
, with high probability. Thus,
as discussed in Section 5.1, we would need to deﬁne bt(a) ∈eO
 
nt(a)−1/4
, which, by Corollary 5.7
and Lemma 5.8 would imply a suboptimal ˜O(T
3/4) rate for the violations."
ADAPTIVE LEARNING RATES,0.21084337349397592,"The second option is to set η(i)
t (at) = 1/nt(at). In the stochastic setting, we have an optimal rate
of concentration of the terms |ˆg(i)
t (a) −¯g(i)(a)| ∈eO
 
nt(a)−1/2
as, by Proposition 5.4, this is
equivalent to compute the empirical mean. However, this second option fails disastrously in the
adversarial setting as highlighted in Corollary 5.7, where the ﬁrst component of the violations
becomes linear in T. Intuitively, a learning rate of order 1/nt(a) makes the update of the estimates too
slow when the underlying constraints change, as it does happen in the adversarial setting."
ADAPTIVE LEARNING RATES,0.21234939759036145,"This trade-off forces us to employ adaptive learning rates. Our idea is to use learning rates of
the order 1/nt(a) with an adaptive multiplicative term that depends on the current violation of the
constraint. Formally, we use learning rates:"
ADAPTIVE LEARNING RATES,0.21385542168674698,"η(i)
t (at) :=
1
nt(a)"
ADAPTIVE LEARNING RATES,0.21536144578313254,"
1 + Γ(i)
t

,"
ADAPTIVE LEARNING RATES,0.21686746987951808,"where Γ(i)
t
is a bonus term deﬁned as"
ADAPTIVE LEARNING RATES,0.2183734939759036,"Γ(i)
t
:=
h
V (i)
t−1 −21
p"
ADAPTIVE LEARNING RATES,0.21987951807228914,"Kt log(1/δ2)
i21√"
ADAPTIVE LEARNING RATES,0.2213855421686747,"Kt log(1/δ2) 0
,"
ADAPTIVE LEARNING RATES,0.22289156626506024,"and [x]b
a := min(max(x, a), b) is the clipping of x between a and b. Moreover, we set the exploration
bonus as"
ADAPTIVE LEARNING RATES,0.22439759036144577,bt(a) = s
ADAPTIVE LEARNING RATES,0.22590361445783133,2 log(2/δ2)
ADAPTIVE LEARNING RATES,0.22740963855421686,nt−1(a) .
ADAPTIVE LEARNING RATES,0.2289156626506024,"The following theorem shows that such approach guarantees eO(
√"
ADAPTIVE LEARNING RATES,0.23042168674698796,"KT) violations in both adversarial
and stochastic settings.
Theorem 6.1. Both in the stochastic and the adversarial setting, with probability at least 1−2mT 2δ2
it holds that
Vt ≤53
p"
ADAPTIVE LEARNING RATES,0.2319277108433735,"Kt log(2/δ2)
∀t ∈JTK."
ADAPTIVE LEARNING RATES,0.23343373493975902,"The previous theorem shows that this choice of learning rates is sufﬁcient to guarantee optimal
bounds on the violations. However, to achieve this result we are setting bt(a) ∈eO(nt(a)−1/2). As we
showed in theorem 5.1, this requires a concentration on the estimates |ˆg(i)
t (a) −¯g(i)
t (a)| of the same
magnitude (in the stochastic setting). This is crucially needed to ensure that the regret minimizer R
provides the desired guarantees and that the event C deﬁned in Section 3 actually holds with high
probability.
Lemma 6.2. In the stochastic setting, with probability at least 1 −5mKTδ2, it holds that:"
ADAPTIVE LEARNING RATES,0.23493975903614459,"|ˆg(i)
t (a) −¯g(i)
t (a)| ≤bt(a)
∀a ∈JKK, t ∈JTK, i ∈JmK"
ADAPTIVE LEARNING RATES,0.23644578313253012,"The proof of the previous result relies on the fact that in the stochastic case the bonus Γ(i)
t
does not
“kick in” ensuring that η(i)
t (a) = 1/nt(a). Thus, ˆg(i)
t
is the empirical average of past observations. The
previous result, together with Theorem 5.1 proves the following corollary."
ADAPTIVE LEARNING RATES,0.23795180722891565,"Corollary 6.3. In the stochastic setting, with probability at least 1−5mKTδ2, it holds that X ⋆∈b
Xt
for all t ∈JTK."
ADAPTIVE LEARNING RATES,0.2394578313253012,"This proves that the clean event C holds with high probability, as promised in Section 3."
PUTTING EVERYTHING TOGETHER,0.24096385542168675,"7
Putting everything together"
PUTTING EVERYTHING TOGETHER,0.24246987951807228,"Now, we have everything in place to easily prove the our main theorems. First, we deﬁne the
parameters δ1 = δ1(ϵ) and δ2 = δ2(ϵ) in order to guarantee that our theorems hold with probability
at least 1 −ϵ. In particular, we set δ1(ϵ) = ϵ/2, where we recall that δ1 is the parameter used to set
β and γ in Algorithm 2, and δ2(ϵ) = ϵ/(14mKT 2), where δ2 is used to set the optimistic bonus and
learning rate of Algorithm 1."
PUTTING EVERYTHING TOGETHER,0.24397590361445784,"In the stochastic setting, the violation guarantees directly follow from Theorem 6.1, while the regret
guarantee follows by combining Theorem 4.1 and Corollary 6.3. Formally:"
PUTTING EVERYTHING TOGETHER,0.24548192771084337,"Theorem 7.1. In the stochastic setting, for any ϵ > 0 Algorithm 1 guarantees that with probability at
least 1 −ϵ:"
PUTTING EVERYTHING TOGETHER,0.2469879518072289,"RT ≤4
p"
PUTTING EVERYTHING TOGETHER,0.24849397590361447,"KT log(2K/ϵ)
and
Vt ≤53
p"
PUTTING EVERYTHING TOGETHER,0.25,"Kt log(28mKT 2/ϵ)
∀t ∈JTK."
PUTTING EVERYTHING TOGETHER,0.25150602409638556,"Now, we turn to the adversarial setting. Theorem 6.1 guarantee eO(
√"
PUTTING EVERYTHING TOGETHER,0.25301204819277107,"T) violations even with
adversarial constraint, while the regret guarantees follows by combining Theorem 5.2 and Theorem 4.1"
PUTTING EVERYTHING TOGETHER,0.2545180722891566,"Theorem 7.2. In the adversarial setting, for any ϵ > 0 Algorithm 1 guarantees that with probability
at least 1 −ϵ:"
PUTTING EVERYTHING TOGETHER,0.2560240963855422,"α-RT ≤4
p"
PUTTING EVERYTHING TOGETHER,0.2575301204819277,"KT log(2K/ϵ)
and
Vt ≤53
p"
PUTTING EVERYTHING TOGETHER,0.25903614457831325,"Kt log(28mKT 2/ϵ)
∀t ∈JTK,"
PUTTING EVERYTHING TOGETHER,0.2605421686746988,where α = ρ/(1+ρ).
PUTTING EVERYTHING TOGETHER,0.2620481927710843,"Note that in both settings, the regret upper bound is of order eO(
√"
PUTTING EVERYTHING TOGETHER,0.2635542168674699,"KT) and it is independent from
the number of constraints m, while the violations are of order eO(
p"
PUTTING EVERYTHING TOGETHER,0.26506024096385544,"KT log(m)) and depend only
logarithmically on m. This is in contrast to the other best-of-both-world algorithms for bandits with
long term constraints, based on primal-dual methods, in which both the regret and the violations
depends polynomially in m."
PUTTING EVERYTHING TOGETHER,0.26656626506024095,"Another interesting characteristic of our methodology is that we guarantee an anytime bound on the
constraint violation. Indeed, this matches the guarantees provided by the most recent primal-dual
methods [11, 1] that, however, require weakly-adaptive underlying regret minimizers."
CONVERGENCE RATE IN THE STOCHASTIC SETTING,0.2680722891566265,"7.1
Convergence rate in the stochastic setting"
CONVERGENCE RATE IN THE STOCHASTIC SETTING,0.26957831325301207,"To conclude, we point to a nice byproduct of our analysis. In the stochastic setting, we can easily
prove a sort of “convergence rate” of xt to the set X ⋆. Formally, we can prove that positive
violations are bounded by eO(√Kt log m) as long as we consider expected violations. Let us deﬁne
x+ := max(x, 0) and"
CONVERGENCE RATE IN THE STOCHASTIC SETTING,0.2710843373493976,"V+
t := max
i∈JmK t
X τ=1"
CONVERGENCE RATE IN THE STOCHASTIC SETTING,0.27259036144578314,"h
⟨xτ, ¯g(i)
τ ⟩
i+
."
CONVERGENCE RATE IN THE STOCHASTIC SETTING,0.2740963855421687,"Then, we can state the following theorem:"
CONVERGENCE RATE IN THE STOCHASTIC SETTING,0.2756024096385542,"Theorem 7.3. Algorithm 1, in the stochastic setting, guarantees that with probability at least 1 −ϵ,
it holds that:
V+
t ≤16
p"
CONVERGENCE RATE IN THE STOCHASTIC SETTING,0.27710843373493976,"Kt log(28mKT 2/ϵ)
∀t ∈JTK."
CONVERGENCE RATE IN THE STOCHASTIC SETTING,0.2786144578313253,"Intuitively, our result shows that our algorithm plays only a sublinear number of times “far” from the
set X ⋆, or that our algorithm plays a linear number of times “close” to the set X ⋆. This is a much
stronger result then just guaranteeing that VT is sublinear, as in that case it might be a linear number
of times the algorithm plays “far” from X ⋆as long as it plays strictly inside of X ⋆often enough."
CONVERGENCE RATE IN THE STOCHASTIC SETTING,0.28012048192771083,Acknowledgments
CONVERGENCE RATE IN THE STOCHASTIC SETTING,0.2816265060240964,"MB, MC, AC, FF are partially supported by the FAIR (Future Artiﬁcial Intelligence Research) project
PE0000013, funded by the NextGenerationEU program within the PNRR-PE-AI scheme (M4C2,
investment 1.3, line on Artiﬁcial Intelligence). FF is also partially supported by ERC Advanced Grant
788893 AMDROMA “Algorithmic and Mechanism Design Research in Online Markets”, and PNRR
MUR project IR0000013-SoBigData.it. MC is also partially supported by the EU Horizon project
ELIAS (European Lighthouse of AI for Sustainability, No. 101120237). AC is partially supported by
MUR - PRIN 2022 project 2022R45NBB funded by the NextGenerationEU program."
REFERENCES,0.28313253012048195,References
REFERENCES,0.28463855421686746,"[1] Gagan Aggarwal, Giannis Fikioris, and Mingfei Zhao. No-regret algorithms in non-truthful
auctions with budget and ROI constraints. arXiv preprint, abs/2404.09832, 2024."
REFERENCES,0.286144578313253,"[2] Shipra Agrawal and Nikhil R. Devanur. Bandits with concave rewards and convex knapsacks.
In EC, pages 989–1006. ACM, 2014."
REFERENCES,0.2876506024096386,"[3] Shipra Agrawal and Nikhil R Devanur. Bandits with global convex constraints and objective.
Operations Research, 67(5):1486–1502, 2019."
REFERENCES,0.2891566265060241,"[4] Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both
stochastic and adversarial bandits. In COLT, volume 49 of JMLR Workshop and Conference
Proceedings, pages 116–120. JMLR.org, 2016."
REFERENCES,0.29066265060240964,"[5] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic
multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002."
REFERENCES,0.2921686746987952,"[6] Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knap-
sacks. In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, FOCS
2013, pages 207–216. IEEE, 2013."
REFERENCES,0.2936746987951807,"[7] Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knap-
sacks. J. ACM, 65(3), 2018."
REFERENCES,0.29518072289156627,"[8] Santiago Balseiro, Christian Kroer, and Rachitesh Kumar. Online resource allocation under
horizon uncertainty. SIGMETRICS Perform. Eval. Rev., 51(1):63–64, 2023."
REFERENCES,0.29668674698795183,"[9] Santiago R Balseiro and Yonatan Gur. Learning in repeated auctions with budgets: Regret
minimization and equilibrium. Management Science, 65(9):3952–3968, 2019."
REFERENCES,0.29819277108433734,"[10] Santiago R Balseiro, Haihao Lu, and Vahab Mirrokni. The best of many worlds: Dual mirror
descent for online allocation problems. Operations Research, 2022."
REFERENCES,0.2996987951807229,"[11] Martino Bernasconi, Matteo Castiglioni, and Andrea Celli. No-regret is not enough! bandits
with general constraints through adaptive regret minimization. arXiv preprint, abs/2405.06575,
2024."
REFERENCES,0.30120481927710846,"[12] Martino Bernasconi, Matteo Castiglioni, Andrea Celli, and Federico Fusco. No-regret learning
in bilateral trade via global budget balance. In STOC. ACM, 2024."
REFERENCES,0.30271084337349397,"[13] Martino Bernasconi, Matteo Castiglioni, Andrea Celli, and Federico Fusco. Bandits with
replenishable knapsacks: the best of both worlds. In International Conference on Learning
Representations (ICLR), 2024."
REFERENCES,0.3042168674698795,"[14] Sébastien Bubeck and Aleksandrs Slivkins. The best of both worlds: Stochastic and adversarial
bandits. In COLT, volume 23 of JMLR Proceedings, pages 42.1–42.23. JMLR.org, 2012."
REFERENCES,0.3057228915662651,"[15] Matteo Castiglioni, Andrea Celli, and Christian Kroer. Online learning with knapsacks: the best
of both worlds. In International Conference on Machine Learning, pages 2767–2783. PMLR,
2022."
REFERENCES,0.3072289156626506,"[16] Matteo Castiglioni, Andrea Celli, Alberto Marchesi, Giulia Romano, and Nicola Gatti. A
unifying framework for online optimization with long-term constraints. In Advances in Neural
Information Processing Systems, volume 35, pages 33589–33602, 2022."
REFERENCES,0.30873493975903615,"[17] Matteo Castiglioni, Andrea Celli, and Christian Kroer. Online learning under budget and ROI
constraints via weak adaptivity. In ICML. OpenReview.net, 2024."
REFERENCES,0.3102409638554217,"[18] Giannis Fikioris and Éva Tardos. Approximately stationary bandits with knapsacks. In Proceed-
ings of Thirty Sixth Conference on Learning Theory, volume 195, pages 3758–3782, 12–15 Jul
2023.
[19] Elad Hazan et al. Introduction to online convex optimization, volume 2. Now Publishers, Inc.,
2016.
[20] Nicole Immorlica, Karthik Abinav Sankararaman, Robert Schapire, and Aleksandrs Slivkins.
Adversarial bandits with knapsacks. In 60th IEEE Annual Symposium on Foundations of
Computer Science, FOCS 2019, pages 202–219. IEEE Computer Society, 2019.
[21] Nicole Immorlica, Karthik Sankararaman, Robert Schapire, and Aleksandrs Slivkins. Adversar-
ial bandits with knapsacks. J. ACM, 69(6), 2022. ISSN 0004-5411.
[22] Thomas Kesselheim and Sahil Singla. Online learning with vector costs and bandits with
knapsacks. In Conference on Learning Theory, pages 2286–2305. PMLR, 2020.
[23] Raunak Kumar and Robert Kleinberg. Non-monotonic resource utilization in the bandits with
knapsacks problem. In Advances in Neural Information Processing Systems (NeurIPS), 2022.
[24] Shang Liu, Jiashuo Jiang, and Xiaocheng Li. Non-stationary bandits with knapsacks. Advances
in Neural Information Processing Systems, 35:16522–16532, 2022.
[25] Gergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic
bandits. Advances in Neural Information Processing Systems, 28, 2015.
[26] Yevgeny Seldin and Gábor Lugosi. An improved parametrization and analysis of the EXP3++
algorithm for stochastic and adversarial bandits. In COLT, volume 65 of Proceedings of Machine
Learning Research, pages 1743–1759. PMLR, 2017.
[27] Yevgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and
adversarial bandits. In ICML, volume 32 of JMLR Workshop and Conference Proceedings,
pages 1287–1295. JMLR.org, 2014.
[28] Aleksandrs Slivkins, Karthik Abinav Sankararaman, and Dylan J Foster. Contextual bandits
with packing and covering constraints: A modular lagrangian approach via regression. In The
Thirty Sixth Annual Conference on Learning Theory, pages 4633–4656. PMLR, 2023.
[29] Chen-Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. In COLT,
volume 75 of Proceedings of Machine Learning Research, pages 1263–1291. PMLR, 2018.
[30] Julian Zimmert and Yevgeny Seldin. Tsallis-inf: An optimal algorithm for stochastic and
adversarial bandits. J. Mach. Learn. Res., 22:28:1–28:49, 2021."
REFERENCES,0.3117469879518072,"A
Further Related Works"
REFERENCES,0.3132530120481928,"Best-of-Both-Worlds. A long line of work has investigated Best-of-Both-Worlds algorithms for
bandits without constraints. These algorithms aim to achieve an instance-dependent logarithmic
regret bound in stochastic environments, while also ensuring the worst-case Θ(
√"
REFERENCES,0.3147590361445783,"T) regret bound that
characterizes the adversarial settings [14, 4, 27, 26, 29, 30]. Although our focus is on the generation
model of the constraints, our motivation in this paper is afﬁne: retaining the best of the stochastic
(sublinear regret with respect to the optimal dynamic policy) and adversarial world (tight competitive
ratio with respect to the adversarial benchamrk). Furthermore, our idea of setting an adaptive learning
rate that forces the learning algorithm to interpolate between an adversarial and a stochastic routine is
reminescent of some of the techniques adopted in, e.g., Bubeck and Slivkins [14]."
REFERENCES,0.31626506024096385,"Bandits with Knapsacks. The (stochastic) BwK problem, where the rewards ft as well as the gi
t
are drawn i.i.d. from a non-negative distribution (so that the budget available for each resource
can only decrease over time) is formally introduced and solved in Badanidiyuru et al. [6] (see also
its journal version [7]). Agrawal and Devanur [2] studies a more general stochastic setting, which
subsumes knapsack and exhibit optimal guarantees via optimism in the face of uncertainty [see also
3]. Moving to the adversarial BwK problem (which corresponds to our model when the gi
t are all
non-negative), an optimal solution is proposed in Immorlica et al. [20] [see also 21]; there, the authors
propose the LagrangeBwK framework, which has a natural interpretation: arms can be thought of
as primal variables, and resources as dual variables. The framework works by setting up a repeated
two-player zero-sum game between a primal and a dual player, and by showing convergence to a
Nash equilibrium of the expected Lagrangian game. Differently from the stochastic version, the
adversarial BwK does not admit no-regret algorithms, but Θ(log T) competitive ratio. In a subsequent
work, [22] provides a new analysis obtaining a O(log m log T) competitive ratio, which is optimal
both in the time horizon T and in the number of resources m (and improves on the O(m log T) of
Immorlica et al. [20, 21]). In the special case in which budgets are Ω(T), Castiglioni et al. [15]
further improves the competitive ratio to 1/ρ where ρ is the per-iteration budget."
REFERENCES,0.3177710843373494,"More general constraints. Castiglioni et al. [15] studies a setting with general constraints, and
show how to adapt the LagrangeBwK framework to obtain best-of-both-worlds guarantees when
Slater’s parameter is known a priori. Similar guarantees are also provided, in the stochastic setting,
by Slivkins et al. [28], which then extend the results to the contextual model. Finally, Castiglioni et al.
[17] introduces the use of weakly adaptive regret minimizers within the LagrangeBwK framework,
and provides guarantees in the speciﬁc case of one budget constraint and one return-on-investments
constraint."
REFERENCES,0.3192771084337349,"Other related works. In an effort to bridge the results for adversarial and stochastic BwK, Fikioris
and Tardos [18] investigates a data generation model that interpolate between the fully stochastic and
the fully adversarial setting, depending on the magnitude of ﬂuctuations in expected rewards and
resources consumption across rounds. A similar effort is undertaken in Liu et al. [24], that study a
non-stationary setting and provide no-regret guarantees against the best dynamic policy through a
UCB-based algorithm. A recent line of work also investigates the natural situation where resources
can be replenished in certain rounds (as also captured in our model) [23, 13, 12]. Finally, a related
line of works is the one on online allocation problems with ﬁxed per-iteration budget, where the input
pair of reward and costs is observed before the learner makes a decision [10, 8]."
REFERENCES,0.3207831325301205,"B
Proofs omitted from Section 4"
REFERENCES,0.32228915662650603,Theorem 4.1. Let xt be selected accordingly to Algorithm 2 run with arbitrary sequence of convex
REFERENCES,0.32379518072289154,"sets b
Xt ⊆∆K with γ = β"
REFERENCES,0.3253012048192771,"2 and β =
q"
REFERENCES,0.32680722891566266,log(K/δ1)
REFERENCES,0.32831325301204817,"KT
. Then, with probability at least 1 −δ1 it holds that X"
REFERENCES,0.32981927710843373,"t∈JT K
⟨ft, x⟩−ft(at) ≤4
p"
REFERENCES,0.3313253012048193,"KT log(K/δ1),
∀x ∈
\"
REFERENCES,0.3328313253012048,"t∈JT K
b
Xt."
REFERENCES,0.33433734939759036,"Proof. Let us deﬁne the negative entropy for a vector x ∈RK
≥0 as:"
REFERENCES,0.3358433734939759,"Ψ(x) :=
X"
REFERENCES,0.3373493975903614,"a∈JKK
x(a) (log(x(a)) −1)"
REFERENCES,0.338855421686747,and the Bregman divergence using Ψ can be written as
REFERENCES,0.34036144578313254,"B(x||y) := Ψ(x) −Ψ(y) −⟨∇Ψ(y), x −y⟩."
REFERENCES,0.34186746987951805,For the Bregman divergence it holds the following:
REFERENCES,0.3433734939759036,"Claim B.1 ([19]). For any z1, z2, and z3, it holds:"
REFERENCES,0.34487951807228917,"B(z1||z2) + B(z2||z3) −B(z1||z3) = ⟨z1 −z2, ∇Ψ(z3) −∇Ψ(z2)⟩."
REFERENCES,0.3463855421686747,"Moreover, given z, deﬁne z′ = arg min¯z∈K B(¯z||z). Then:"
REFERENCES,0.34789156626506024,"B(˜z||z′) ≤B(z′||z) + B(˜z||z′) ≤B(˜z||z)
∀˜z ∈K."
REFERENCES,0.3493975903614458,"At this point, is more convenient to work with losses rather then rewards. Deﬁne ℓt(a) := 1 −ft(a)
and ˆℓt(a) := 1 −ˆft(a). Note that:"
REFERENCES,0.3509036144578313,ˆℓt(a) = 1 −ˆft(a) =
REFERENCES,0.35240963855421686,"(
0
if a ̸= at
1−ft(a)
xt(a)+γ
if a = at."
REFERENCES,0.3539156626506024,"Then, it is easy to verify that ∇Ψ(x) = log(x) in which log(x) has to be interpreted to be applied
entry-wise. Simple calculations also show that βˆℓt = log(xt) −log(ˆxt+1). Thus, we can apply
Claim B.1 with z1 = x, z2 = xt and z3 = ˆxt+1 and this gives us the following:"
REFERENCES,0.35542168674698793,"β⟨xt −x, ˆℓt⟩= B(x||xt) + B(xt||ˆxt+1) −B(x||ˆxt+1).
(3)"
REFERENCES,0.3569277108433735,"Moreover using the second part of Claim B.1 in which z = ˆx, z′ = xt, ˜z = x, and K = b
Xt, we can
conclude that B(x||xt) ≤B(x||ˆxt). Notice that here we use x ∈b
Xt for each t. Then, we have the
following chain of inequalities: β
X"
REFERENCES,0.35843373493975905,"t∈JT K
⟨xt −x, ˆℓt⟩=
X"
REFERENCES,0.35993975903614456,"t∈JT K
B(x||xt) + B(xt||ˆxt+1) −B(x||ˆxt+1)
(By Equation (3))"
REFERENCES,0.3614457831325301,= B(x||x1) −B(x||ˆxT +1) +
REFERENCES,0.3629518072289157,"T −1
X"
REFERENCES,0.3644578313253012,"t=2
(B(x||xt) −B(x||ˆxt)) +
X"
REFERENCES,0.36596385542168675,"t∈JT K
B(xt||ˆxt+1)"
REFERENCES,0.3674698795180723,"≤B(x||x1) +
X"
REFERENCES,0.3689759036144578,"t∈JT K
B(xt||ˆxt+1) (B is non-negative and B(·||xt) ≤B(·||ˆxt))"
REFERENCES,0.3704819277108434,"= B(x||x1) +
X"
REFERENCES,0.37198795180722893,"t∈JT −1K
B(xt||ˆxt+1)"
REFERENCES,0.37349397590361444,"Combining the two we can ﬁnd that: β
X"
REFERENCES,0.375,"t∈JT K
⟨xt −x, ˆℓt⟩≤
X"
REFERENCES,0.37650602409638556,"t∈JT K
[B(x||ˆxt) + B(xt||ˆxt+1) −B(x||ˆxt+1)]
(4)"
REFERENCES,0.37801204819277107,"≤B(x||x1) +
X"
REFERENCES,0.3795180722891566,"t∈JT K
B(xt||ˆxt+1)
(5)"
REFERENCES,0.3810240963855422,"Now we analyze the term B(xt||ˆxt+1).
B(xt||ˆxt+1) ≤B(xt||ˆxt+1) + B(ˆxt+1||xt)
= ⟨xt −ˆxt+1, ∇Ψ(xt) −∇Ψ(ˆxt+1)⟩
(Deﬁnition of B(·||·))"
REFERENCES,0.3825301204819277,"= β⟨xt −ˆxt+1, ˆℓt⟩
(∇Ψ(x) = log(x) and βˆℓt = log(xt) −log(ˆxt+1)) = β
X"
REFERENCES,0.38403614457831325,"a∈JKK
xt(a)(1 −e−βˆℓt(a))ˆℓt(a) ≤β2 X"
REFERENCES,0.3855421686746988,"a∈JKK
xt(a)ˆℓt(a)2
(1 −e−x ≤x) ≤β2 X a∈JKK"
REFERENCES,0.3870481927710843,"1 −ft(a)
xt(a) + γ xt(a)ˆℓt(a) ≤β2 X a∈JKK"
REFERENCES,0.3885542168674699,"ˆℓt(a),"
REFERENCES,0.39006024096385544,"where, in the last inequality, we use that xt(a)/(xt(a)+γ) is at most 1. Thus, by choosing x1(a) = 1/K
for all a, we have that B(x||x1) ≤log(K) and thus:
X"
REFERENCES,0.39156626506024095,"t∈JT K
⟨xt −x, ˆℓt⟩≤log(K)"
REFERENCES,0.3930722891566265,"β
+ β
X"
REFERENCES,0.39457831325301207,t∈JT K X a∈JKK
REFERENCES,0.3960843373493976,"ˆℓt(a)
(6)"
REFERENCES,0.39759036144578314,"Form [25, Corollary 1] we know that with probability at least 1 −δ1 we have:
X"
REFERENCES,0.3990963855421687,t∈JT K
REFERENCES,0.4006024096385542,ˆℓt(a) −(1 −ft(a)) ≤log(K/δ1)
REFERENCES,0.40210843373493976,"2γ
∀a ∈JKK.
(7)"
REFERENCES,0.4036144578313253,"Moreover, it is easy to verify that:"
REFERENCES,0.40512048192771083,"1 −ft(at) =
X"
REFERENCES,0.4066265060240964,"a∈JKK
I(at = a)(1 −ft(a))xt(a) + γ"
REFERENCES,0.40813253012048195,"xt(a) + γ =
X a∈JKK"
REFERENCES,0.40963855421686746,"ˆℓt(a)xt(a) + γ
X a∈JKK"
REFERENCES,0.411144578313253,ℓt(a)I(at = a)
REFERENCES,0.4126506024096386,xt(a) + γ
REFERENCES,0.4141566265060241,"= ⟨xt, ˆℓt⟩+ γ
X a∈JKK"
REFERENCES,0.41566265060240964,"ˆℓt(a)
(8)"
REFERENCES,0.4171686746987952,"The regret is with probability at least 1 −δ1:
X"
REFERENCES,0.4186746987951807,"t∈JT K
[⟨x, ft⟩−ft(at)] =
X"
REFERENCES,0.42018072289156627,"t∈JT K
[(1 −ft(at)) −(1 −⟨x, ft⟩)] =
X"
REFERENCES,0.42168674698795183,"t∈JT K
[(1 −ft(at)) −⟨x, ˆℓt⟩] +
X"
REFERENCES,0.42319277108433734,"t∈JT K
[⟨x, ˆℓt⟩−(1 −⟨x, ft⟩)] ≤
X"
REFERENCES,0.4246987951807229,"t∈JT K
⟨xt −x, ˆℓt⟩+
X"
REFERENCES,0.42620481927710846,"t∈JT K
[⟨x, ˆℓt⟩−(1 −⟨x, ft⟩)] + γ
X"
REFERENCES,0.42771084337349397,t∈JT K X a∈JKK
REFERENCES,0.4292168674698795,"ˆℓt(a)
(Equation (8))"
REFERENCES,0.4307228915662651,≤log(K)
REFERENCES,0.4322289156626506,"β
+ log(K/δ1)"
REFERENCES,0.43373493975903615,"2γ
+ (γ + β)
X"
REFERENCES,0.4352409638554217,t∈JT K X a∈JKK
REFERENCES,0.4367469879518072,"ˆℓt(a)
(Equation (6) and Equation (7))"
REFERENCES,0.4382530120481928,≤log(K)
REFERENCES,0.4397590361445783,"β
+ log(K/δ1)"
REFERENCES,0.44126506024096385,"2γ
+ (γ + β)  X"
REFERENCES,0.4427710843373494,t∈JT K X
REFERENCES,0.4442771084337349,"a∈JKK
(1 −ft(a)) + K log(K/δ1) 2γ  "
REFERENCES,0.4457831325301205,≤log(K)
REFERENCES,0.44728915662650603,"β
+ log(K/δ1)"
REFERENCES,0.44879518072289154,"2γ
+ (γ + β)KT + (γ + β)K log(K/δ1) 2γ"
REFERENCES,0.4503012048192771,= log(K)
REFERENCES,0.45180722891566266,"β
+ log(K/δ1)"
REFERENCES,0.45331325301204817,"β
+ 2βKT + 2Klog(K/δ1)"
REFERENCES,0.45481927710843373,"where in the last inequality we used that β = 2γ. By taking β =
q"
REFERENCES,0.4563253012048193,log(K/δ1)
REFERENCES,0.4578313253012048,"KT
we obtain, that with
probability at least 1 −δ1:
X"
REFERENCES,0.45933734939759036,"t∈JT K
[⟨x, ft⟩−ft(at)] ≤4
p"
REFERENCES,0.4608433734939759,"KT log(K/δ1),"
REFERENCES,0.4623493975903614,as desired.
REFERENCES,0.463855421686747,"C
Proofs omitted from Section 5.1: How to set the optimistic bonus"
REFERENCES,0.46536144578313254,"Theorem 5.1. Consider the stochastic setting. Given any δ > 0, let bt(a) be such that with probability
at least 1 −δ it holds:"
REFERENCES,0.46686746987951805,"|ˆg(i)
t (a) −¯g(i)(a)| ≤bt(a)
∀t ∈JTK, i ∈JmK, a ∈JKK."
REFERENCES,0.4683734939759036,"Then, it holds X ⋆⊆∩t∈JT K b
Xt with probability at least 1 −δ."
REFERENCES,0.46987951807228917,"Proof. In the following, we assume that the condition in the statement of the theorem holds. Hence,
our result with hold with probability 1 −δ as promised. Let x ∈X ⋆
i . Consider a t ∈JTK and an
i ∈JmK. Then, consider the following inequalities:"
REFERENCES,0.4713855421686747,"⟨x, ˆg(i)
t ⟩= ⟨x, ˆg(i)
t
−¯g(i)⟩+ ⟨x, ¯g(i)⟩"
REFERENCES,0.47289156626506024,"≤⟨x, ˆg(i)
t
−¯g(i)⟩
(x ∈X ⋆
i ) =
X"
REFERENCES,0.4743975903614458,"a∈JKK
x(a)(ˆg(i)
t (a) −¯g(i)(a))"
REFERENCES,0.4759036144578313,"≤⟨x, bt⟩."
REFERENCES,0.47740963855421686,"Thus, ⟨x, ˆg(i)
t
−bt⟩≤0 which, by deﬁnition, proves that x ∈b
X (i)
t
. This concludes the proof."
REFERENCES,0.4789156626506024,"Theorem 5.2. In the adversarial setting, it holds X ⋆
∅⊆b
Xt for all t ∈JTK."
REFERENCES,0.48042168674698793,"Proof. In the adversarial setting, by Equation (1) we have that"
REFERENCES,0.4819277108433735,"g(i)
t (a∅) ≤−ρ,"
REFERENCES,0.48343373493975905,"for all t ∈JTK and constraint i ∈JmK. Moreover, for each t ∈JTK, i ∈JmK, and a ∈JKK, it holds"
REFERENCES,0.48493975903614456,"ˆg(i)
t (a) =
X"
REFERENCES,0.4864457831325301,"τ∈Tt−1,a
w(i)
t,a(τ) g(i)
τ (a) and P"
REFERENCES,0.4879518072289157,"τ∈Tt−1,a w(i)
t,a(τ) = 1. Then, for all t ∈JTK and constraint i ∈JmK, ˆg(i)
t (a∅) ≤−ρ and"
REFERENCES,0.4894578313253012,"ˆg(i)
t (a) ≤1 for each a ̸= a∅. 6 Thus, we can consider the following inequalities for any ˜x ∈X ⋆
∅:"
REFERENCES,0.49096385542168675,"⟨˜x, ˆg(i)
t ⟩=
1
1 + ρ ˆg(i)
t (a∅) +
ρ
1 + ρ⟨x, ˆg(i)
t ⟩"
REFERENCES,0.4924698795180723,"≤
1
1 + ρ(−ρ) +
ρ
1 + ρ
≤0,"
REFERENCES,0.4939759036144578,"thus proving that ˜x ∈b
Xt."
REFERENCES,0.4954819277108434,"6Notice that these inequalities hold only for action played at least one time. Otherwise, similar inequalities
continue to be true thanks to the optimistic bonus bt."
REFERENCES,0.49698795180722893,"D
Proofs omitted from Section 5.2: How to set the weights"
REFERENCES,0.49849397590361444,"Lemma 5.3. Given any sequence {yt}t∈JT K such that y1 = 0 and any sequence of learning rates
{ηt}t∈JT K such that η1 = 1, let {ˆyt}t∈JT K be the estimator updated as:"
REFERENCES,0.5,ˆyt+1 = ˆyt + ηt(yt −ˆyt).
REFERENCES,0.5015060240963856,"Then, it holds that ˆyt = Pt−1
τ=1 yτwt(τ) where wt(τ) = ητ
Qt−1
k=τ+1(1 −ηk).
Moreover,
Pt−1
τ=1 wt(τ) = 1 for any t ≥2."
REFERENCES,0.5030120481927711,"Proof. The ﬁrst part of the statement is trivial as it can be easily checked that: ˆyt = t−1
X"
REFERENCES,0.5045180722891566,"τ=1
yτ  ητ t−1
Y"
REFERENCES,0.5060240963855421,"k=τ+1
(1 −ηk) ! ."
REFERENCES,0.5075301204819277,"Then, we prove the second part of the lemma by induction on t. The base case holds trivially as
w2(1) = η1 = 1. Moreover, assuming Pt−2
τ=1 wt
τ = 1, it holds: t−1
X"
REFERENCES,0.5090361445783133,"τ=1
wt(τ) = t−2
X"
REFERENCES,0.5105421686746988,"τ=1
wt−1(τ)(1 −ηt−1) + wt(t −1) = (1 −ηt−1) + ηt−1 = 1,"
REFERENCES,0.5120481927710844,where in the second-to-last equality we use the inductive hypothesis. This concludes the proof.
REFERENCES,0.5135542168674698,"Proposition 5.4. If η(i)
t (at) =
1
nt(at) for each τ ∈Tt−1,a, then w(i)
t,a(τ) =
1
nt−1(a) and we recover"
REFERENCES,0.5150602409638554,"the empirical mean estimator for ˆg(i)
t (a) =
1
nt−1(a)
P"
REFERENCES,0.516566265060241,"τ∈Tt−1,a g(i)
τ (a)."
REFERENCES,0.5180722891566265,"Proof. Consider an a ∈JKK, an i ∈JmK, and a t ∈JTK. Then, by applying Lemma 5.3 to the set of
rounds Tt−1,a we have that:"
REFERENCES,0.5195783132530121,"w(i)
t,a(τ) =
1
nτ(a) Y"
REFERENCES,0.5210843373493976,"k∈Tt−1,a:k>τ"
REFERENCES,0.5225903614457831,"
1 −
1
nk(a)"
REFERENCES,0.5240963855421686,"
∀τ ∈Tt−1,a."
REFERENCES,0.5256024096385542,"Now, we show that
Y"
REFERENCES,0.5271084337349398,"k∈Tt−1,a:k>τ"
REFERENCES,0.5286144578313253,"
1 −
1
nk(a) 
=
Y"
REFERENCES,0.5301204819277109,"k∈Tt−1,a:k>τ"
REFERENCES,0.5316265060240963,nk(a) −1 nk(a) =
REFERENCES,0.5331325301204819,"nt−1(a)
Y"
REFERENCES,0.5346385542168675,j=nτ (a)+1 j −1 j
REFERENCES,0.536144578313253,"=
nτ(a)
nt−1(a),"
REFERENCES,0.5376506024096386,"and thus w(i)
t,a(τ) =
1
nt−1(a), as desired."
REFERENCES,0.5391566265060241,"Theorem 5.6. Given an interval [t1, t2] ⊆JTK, an i ∈JmK, and a δ > 0, with probability at least
1 −δ it holds:"
REFERENCES,0.5406626506024096,"V (i)
[t1,t2] ≤
X a∈JKK X"
REFERENCES,0.5421686746987951,"τ∈Tt2,a∩[t1,t2] 1"
REFERENCES,0.5436746987951807,"η(i)
τ (a)"
REFERENCES,0.5451807228915663,"
ˆg(i)
τ+1(a) −ˆg(i)
τ (a)

+ t2
X"
REFERENCES,0.5466867469879518,"τ=t1
⟨xτ, bτ⟩+ 4
p"
REFERENCES,0.5481927710843374,(t2 −t1) log(1/δ).
REFERENCES,0.5496987951807228,"Proof. First, applying Lemma G.1, we have that with probability 1 −δ it holds: t2
X"
REFERENCES,0.5512048192771084,"τ=t1
⟨g(i)
τ , xτ⟩≥ t2
X"
REFERENCES,0.552710843373494,"τ=t1
g(i)
τ (aτ) −4
p"
REFERENCES,0.5542168674698795,"(t2 −t1) log(1/δ)
(9)"
REFERENCES,0.5557228915662651,Consider the following chain of inequalities:
REFERENCES,0.5572289156626506,"V (i)
[t1,t2] = t2
X"
REFERENCES,0.5587349397590361,"τ=t1
g(i)
τ (aτ) ≤ t2
X"
REFERENCES,0.5602409638554217,"τ=t1
g(i)
τ (aτ) − t2
X"
REFERENCES,0.5617469879518072,"τ=t1
⟨ˆg(i)
τ , xτ⟩+ t2
X"
REFERENCES,0.5632530120481928,"τ=t1
⟨xτ, bτ⟩
(xτ ∈b
Xτ) ≤ t2
X τ=t1"
REFERENCES,0.5647590361445783,"
g(i)
τ (aτ) −ˆg(i)
τ (aτ)

+ t2
X"
REFERENCES,0.5662650602409639,"τ=t1
⟨xτ, bτ⟩+ 4
p"
REFERENCES,0.5677710843373494,"(t2 −t1) log(1/δ)
(Equation (9)) =
X a∈JKK X"
REFERENCES,0.5692771084337349,"τ∈Tt2,a∩[t1,t2]
(g(i)
τ (a) −ˆg(i)
τ (a)) + t2
X"
REFERENCES,0.5707831325301205,"τ=t1
⟨xτ, bτ⟩+ 4
p"
REFERENCES,0.572289156626506,"(t2 −t1) log(1/δ) =
X a∈JKK X"
REFERENCES,0.5737951807228916,"τ∈Tt2,a∩[t1,t2]"
REFERENCES,0.5753012048192772,"ˆg(i)
τ+1(a) −ˆg(i)
τ (a)"
REFERENCES,0.5768072289156626,"η(i)
τ (a)
+ t2
X"
REFERENCES,0.5783132530120482,"τ=t1
⟨xτ, bτ⟩+ 4
p"
REFERENCES,0.5798192771084337,"(t2 −t1) log(1/δ),"
REFERENCES,0.5813253012048193,where the last equality follows by the deﬁnition of the update:
REFERENCES,0.5828313253012049,"ˆg(i)
τ+1(a) =

1 −η(i)
τ (a)

ˆg(i)
τ (a) + η(i)
τ (a)g(i)
τ (a)
for a = aτ."
REFERENCES,0.5843373493975904,This concludes the proof.
REFERENCES,0.5858433734939759,"Corollary 5.7. Given an interval [t1, t2] ⊆JTK, a i ∈JmK, and a δ > 0, assume that for any
a ∈JKK it holds η(i)
τ (a) ≥η(i)
τ ′ (a) ∀τ < τ ′ ∈Tt2,a ∩[t1, t2]. Then, with probability at least 1 −δ it
holds:"
REFERENCES,0.5873493975903614,"V (i)
[t1,t2] ≤
X a∈JKK 2"
REFERENCES,0.588855421686747,"η(i)
ℓ(a,[t1,t2])(a)
+ t2
X"
REFERENCES,0.5903614457831325,"τ=t1
⟨xτ, bτ⟩+ 4
p"
REFERENCES,0.5918674698795181,(t2 −t1) log(1/δ).
REFERENCES,0.5933734939759037,"Proof. We assume that Theorem 5.6 holds, and hence our statement holds with probability 1 −δ.
Then, to prove the statement it is sufﬁcient to show that
X a∈JKK X"
REFERENCES,0.5948795180722891,"τ∈Tt2,a∩[t1,t2] 1"
REFERENCES,0.5963855421686747,"η(i)
τ (a)"
REFERENCES,0.5978915662650602,"
ˆg(i)
τ+1(a) −ˆg(i)
τ (a)

≤
X a∈JKK X"
REFERENCES,0.5993975903614458,"τ∈Tt2,a∩[t1,t2] 1"
REFERENCES,0.6009036144578314,"η(i)
t2 (a)
."
REFERENCES,0.6024096385542169,"Fix any a ∈JKK, and let k = |Tt2,a ∩[t1, t2]| be the number of times action a is played in the
interval [t1, t2]. Moreover, let τ(j) be the rounds in which action a is played the j-th time in the
interval [t1, t2]. Then:
X"
REFERENCES,0.6039156626506024,"τ∈Tt2,a∩[t1,t2] 1"
REFERENCES,0.6054216867469879,"η(i)
τ (a)"
REFERENCES,0.6069277108433735,"
ˆg(i)
τ+1(a) −ˆg(i)
τ (a)
 =
X"
REFERENCES,0.608433734939759,j∈Jk−1K 1
REFERENCES,0.6099397590361446,"η(i)
τ(j)(a)"
REFERENCES,0.6114457831325302,"
ˆg(i)
τ(j+1)(a) −ˆg(i)
τ(j)(a)

+
1"
REFERENCES,0.6129518072289156,"η(i)
τ(k)(a)"
REFERENCES,0.6144578313253012,"
ˆg(i)
τ(k)+1(a) −ˆg(i)
τ(k)(a)
 ≤
X"
REFERENCES,0.6159638554216867,"j∈Jk−1K  
1"
REFERENCES,0.6174698795180723,"η(i)
τ(j+1)(a)
ˆg(i)
τ(j+1)(a) −
1"
REFERENCES,0.6189759036144579,"η(i)
τ(j)(a)
ˆg(i)
τ(j)(a)  +
1"
REFERENCES,0.6204819277108434,"η(i)
τ(k)(a)"
REFERENCES,0.6219879518072289,"
ˆg(i)
τ(k)+1(a) −ˆg(i)
τ(k)(a)
 =
1"
REFERENCES,0.6234939759036144,"η(i)
τ(k)(a)
ˆg(i)
τ(k)+1(a) −
1"
REFERENCES,0.625,"η(i)
τ(1)(a)
ˆg(i)
τ(1)(a) ≤
2"
REFERENCES,0.6265060240963856,"η(i)
τ(k)(a) =
2"
REFERENCES,0.6280120481927711,"η(i)
ℓ(a,[t1,t2])(a)"
REFERENCES,0.6295180722891566,Summing over all the actions we obtain the desired inequality.
REFERENCES,0.6310240963855421,"Lemma 5.8. Given a c > 0, an α ∈(0, 1), a t ∈JTK, and a δ > 0, let bt(a) =
c
nt(a)α for all
a ∈JKK. Then, with probability at least 1 −δ, it holds: t
X"
REFERENCES,0.6325301204819277,"τ=1
⟨xτ, bτ⟩≤
c
1 −αKαt1−α + 4
p"
REFERENCES,0.6340361445783133,t log(1/δ).
REFERENCES,0.6355421686746988,"Proof. Consider the following inequalities: t
X"
REFERENCES,0.6370481927710844,"τ=1
bτ(aτ) = c
X a∈JKK X τ∈JtK"
REFERENCES,0.6385542168674698,"1
nτ(a)α I(aτ = a) = c
X a∈JKK"
REFERENCES,0.6400602409638554,"nt(a)
X k=1 1
kα"
REFERENCES,0.641566265060241,"≤
c
1 −α X"
REFERENCES,0.6430722891566265,"a∈JKK
nt(a)1−α
(PN
k=1 k−α ≤
R N
0 x−αdx)"
REFERENCES,0.6445783132530121,"≤
c
1 −αKαt1−α
(Jensen’s inequality)"
REFERENCES,0.6460843373493976,The proof is concluded by using Lemma G.1.
REFERENCES,0.6475903614457831,"E
Proofs omitted from Section 6"
REFERENCES,0.6490963855421686,"Theorem 6.1. Both in the stochastic and the adversarial setting, with probability at least 1−2mT 2δ2
it holds that"
REFERENCES,0.6506024096385542,"Vt ≤53
p"
REFERENCES,0.6521084337349398,"Kt log(2/δ2)
∀t ∈JTK."
REFERENCES,0.6536144578313253,"Proof. We prove that given an i ∈JmK, it holds:"
REFERENCES,0.6551204819277109,"V (i)
t
≤53
p"
REFERENCES,0.6566265060240963,"Kt log(2/δ2)
∀t ∈JTK"
REFERENCES,0.6581325301204819,"with probability 1 −2T 2δ2. Then, a union bound over i completes the proof."
REFERENCES,0.6596385542168675,"Given an i ∈JmK, we ﬁrst assume some high-probability events. In particular, we assume that
Corollary 5.7 with δ = δ2 holds for any interval, and that Lemma 5.8 with δ = δ2 holds for all t ∈JTK.
This happens with probability at least 1−2T 2δ2. We consider two cases. If V (i)
t
≤53
p"
REFERENCES,0.661144578313253,"KT log(2/δ2)
for all t ∈JTK, then the statement it is trivially satisﬁed. Otherwise, there exists an a time ¯t for
which V (i)
¯t
≥53
p"
REFERENCES,0.6626506024096386,"Kt log(1/δ2). Clearly, this implies that there exists a t < ¯t such that V (i)
t
≥
42
p"
REFERENCES,0.6641566265060241,"Kt log(2/δ2) for all t ∈[t, ¯t] and V (i)"
REFERENCES,0.6656626506024096,"t−1 ≤42
p"
REFERENCES,0.6671686746987951,"Kt log(1/δ2). Since V (i)
t
≥42
p"
REFERENCES,0.6686746987951807,"Kt log(1/δ2) for
all t ∈[t, ¯t] we have that:"
REFERENCES,0.6701807228915663,"V (i)
t
−21
p"
REFERENCES,0.6716867469879518,"Kt log(1/δ2) ≥42
p"
REFERENCES,0.6731927710843374,"Kt log(1/δ2) −21
p"
REFERENCES,0.6746987951807228,"Kt log(1/δ2) ≥21
p"
REFERENCES,0.6762048192771084,Kt log(1/δ2)
REFERENCES,0.677710843373494,"and thus Γ(i)
t
= 21
p"
REFERENCES,0.6792168674698795,"Kt log(1/δ2) for all t ∈[t, ¯t]. Hence, on the interval t ∈[t, ¯t] we known that the
learning rate can be lower bounded by a non-increasing function of time as"
REFERENCES,0.6807228915662651,"η(i)
t (at) = 1 + 21
p"
REFERENCES,0.6822289156626506,"Kt log(1/δ2)
nt(at)
≥21 s"
REFERENCES,0.6837349397590361,K log(1/δ2)
REFERENCES,0.6852409638554217,"nt(at)
."
REFERENCES,0.6867469879518072,This let us use Corollary 5.7 (that we assumed to hold) to show that:
REFERENCES,0.6882530120481928,"V (i)
[t,¯t] ≤
2"
P,0.6897590361445783,"21
p"
P,0.6912650602409639,K log(1/δ2) X a∈JKK p
P,0.6927710843373494,"n¯t(a) + ¯t
X τ=t"
P,0.6942771084337349,"⟨xτ, bτ⟩+ 4
p"
P,0.6957831325301205,"t log(1/δ2) ≤
2
√ K¯t"
P,0.697289156626506,"21
p"
P,0.6987951807228916,"K log(1/δ2)
+ ¯t
X τ=t"
P,0.7003012048192772,"⟨xτ, bτ⟩+ 4
p"
P,0.7018072289156626,"t log(1/δ2)
(Jensen’s inequality) ≤
2
√ K¯t"
P,0.7033132530120482,"21
p"
P,0.7048192771084337,"K log(1/δ2)
+ 2
p"
P,0.7063253012048193,"2Kt log(2/δ2) + 8
p"
P,0.7078313253012049,"t log(1/δ2)
(Lemma 5.8)"
P,0.7093373493975904,"≤(1/10 + 10)
p"
P,0.7108433734939759,Kt log(2/δ2).
P,0.7123493975903614,"Now, V (i)
¯t
≤Vt + V (i)
[t,¯t] ≤(42 + 1/10 + 10)
p"
P,0.713855421686747,"Kt log(2/δ) < 53
p"
P,0.7153614457831325,"Kt log(2/δ). We thus reached a
contradiction and there is no such a ¯t. The union bound on all i ∈JmK concludes the proof."
P,0.7168674698795181,"Lemma 6.2. In the stochastic setting, with probability at least 1 −5mKTδ2, it holds that:"
P,0.7183734939759037,"|ˆg(i)
t (a) −¯g(i)
t (a)| ≤bt(a)
∀a ∈JKK, t ∈JTK, i ∈JmK"
P,0.7198795180722891,"Proof. First, we show some concentration inequalities that will be useful in the following. By an
Hoeffding’s inequality and an union bound with probability at least 1 −mKTδ2, it holds:"
P,0.7213855421686747,"1
nt−1(a) X"
P,0.7228915662650602,"τ∈Tt−1,a
g(i)
τ (a) −¯g(i)a ≤ s"
P,0.7243975903614458,2 log(2/δ2)
P,0.7259036144578314,"nt(a)
∀t ∈JTK, k ∈JKK, i ∈JmK.
(10)"
P,0.7274096385542169,"Moreover, by Lemma Lemma G.1 and an union bound, with probability at least 1 −mTδ2, it holds:"
P,0.7289156626506024,"V (i)
t
≤ t−1
X"
P,0.7304216867469879,"τ=1
⟨xτ, g(i)
τ ⟩+ 4
p"
P,0.7319277108433735,"t log(1/δ2)
∀t ∈JTK, i ∈JmK
(11)"
P,0.733433734939759,"Similarly, by Lemma Lemma G.1 and an union bound, with probability at least 1 −mTδ2, it holds: t
X"
P,0.7349397590361446,"τ=1
⟨xτ, ¯g(i)
τ ⟩≤ t
X"
P,0.7364457831325302,"τ=1
¯g(i)(aτ) + 4
p"
P,0.7379518072289156,"t log(1/δ2)
∀t ∈JTK, i ∈JmK
(12)"
P,0.7394578313253012,"By Lemma G.2 and an union bound, with probability at least 1 −mTδ2 t
X"
P,0.7409638554216867,"τ=1
⟨xτ, g(i)
τ ⟩≤ t
X"
P,0.7424698795180723,"τ=1
⟨xτ, ¯g(i)⟩+ 4
p"
P,0.7439759036144579,"t log(1/δ2)
∀t ∈JTK, i ∈JmK
(13)"
P,0.7454819277108434,"Finally, by Lemma 5.8 and an union bound, with probability 1 −Tδ2, it holds: t
X"
P,0.7469879518072289,"τ=1
⟨xτ, bτ⟩≤2
p"
P,0.7484939759036144,"2Kt log(2/δ2) + 4
p"
P,0.75,"t log(1/δ2)
∀t ∈JTK
(14)"
P,0.7515060240963856,"In the following, we will assume the the previous events hold, and hence our result holds with
probability at least 1 −5mKTδ2."
P,0.7530120481927711,"First, we show that V i
t ≤21
p"
P,0.7545180722891566,"Kt log(2/δ2) for each t and i. Our proof works by induction on
t. Clearly, the inequality holds for t = 1. Now, assume that it holds for all τ ≤t −1. By the
deﬁnition of Γ(i)
τ , the induction assumption implies that η(i)
τ (a) =
1
nτ (a) for all a ∈JKK, i ∈JmK
and τ ≤t −1. Then, thanks to Proposition 5.4 we have that:"
P,0.7560240963855421,"ˆg(i)
τ (a) =
1
nτ−1(a) X"
P,0.7575301204819277,"ˆt∈Tτ−1,a
g(i)
ˆt (a)
∀τ ≤t −1.
(15)"
P,0.7590361445783133,"Hence, by Equation (10), it holds that:"
P,0.7605421686746988,"ˆg(i)
τ (a) −¯g(i)(a)
 ≤ s"
P,0.7620481927710844,2 log(2/δ2)
P,0.7635542168674698,"nτ(a)
∀τ ≤t −1."
P,0.7650602409638554,"and thus that b
X (i)
τ
̸= {∅} for all τ ≤t −1. Assuming that the events above holds, consider now the
following inequalities:"
P,0.766566265060241,"V (i)
t
= V (i)
t−1 + g(i)
t (at) ≤ t−1
X"
P,0.7680722891566265,"τ=1
⟨xτ, g(i)
τ ⟩+ g(i)
t (at) + 4
p"
P,0.7695783132530121,"t log(1/δ2)
(Equation (11)) ≤ t−1
X"
P,0.7710843373493976,"τ=1
⟨xτ, g(i)
τ
−ˆg(i)
τ ⟩+ t−1
X"
P,0.7725903614457831,"τ=1
⟨xτ, bτ⟩+ g(i)
t (at) + 4
p"
P,0.7740963855421686,"t log(1/δ2)
(xτ ∈b
X (i)
τ ) ≤ t−1
X"
P,0.7756024096385542,"τ=1
⟨xτ, g(i)
τ
−ˆg(i)
τ ⟩+ 2
p"
P,0.7771084337349398,"2Kt log(2/δ2) + g(i)
t (at) + 8
p"
P,0.7786144578313253,"t log(1/δ2)
(Equation (14)) ≤ t−1
X"
P,0.7801204819277109,"τ=1
⟨xτ, g(i)
τ
−ˆg(i)
τ ⟩+ 2
p"
P,0.7816265060240963,"2Kt log(2/δ2) + 1 + 8
p"
P,0.7831325301204819,"t log(1/δ2)
(g(i)
t (a) ≤1) ≤ t−1
X"
P,0.7846385542168675,"τ=1
⟨xτ, ¯g(i) −ˆg(i)
τ ⟩+ 2
p"
P,0.786144578313253,"2Kt log(2/δ2) + 1 + 12
p"
P,0.7876506024096386,"t log(1/δ2)
(Equation (13)) ≤ t−1
X"
P,0.7891566265060241,"τ=1
(¯g(i)(aτ) −ˆg(i)
τ (aτ)) + 2
p"
P,0.7906626506024096,"2Kt log(2/δ2) + 1 + 12
p"
P,0.7921686746987951,"t log(1/δ2)
(Equation (12)) =
X a∈JKK t−1
X"
P,0.7936746987951807,"τ=1
(¯g(i)(a) −ˆg(i)
τ (a))I(aτ = a) + 2
p"
P,0.7951807228915663,"2Kt log(2/δ2) + 1 + 12
p"
P,0.7966867469879518,"t log(1/δ2) ≤
p"
P,0.7981927710843374,"2 log(2/δ2)
X a∈JKK t−1
X τ=1"
P,0.7996987951807228,"1
p"
P,0.8012048192771084,"nτ(a)
I(aτ = a) + 2
p"
P,0.802710843373494,"2Kt log(2/δ2) + 1 + 12
p"
P,0.8042168674698795,"t log(1/δ2) ≤2
p"
P,0.8057228915662651,"2Kt log(2/δ2) + 2
p"
P,0.8072289156626506,"2Kt log(2/δ2) + 1 + 12
p"
P,0.8087349397590361,t log(1/δ2)
P,0.8102409638554217,"and thus V (i)
t
≤21
p"
P,0.8117469879518072,Kt log(2/δ2).
P,0.8132530120481928,"Thus Γ(i)
t
= 0 and ˆg(i)
t (a) is the empirical mean of past observations. This concludes the induction
step, showing that V i
t ≤21
p"
P,0.8147590361445783,"Kt log(1/δ2) for all t ∈JTK, and Γ(i)
t
= 0 for all t ∈JTK and i ∈JmK."
P,0.8162650602409639,"Now, we proved that with probability 1 −3mKTδ2, all Γ(i)
t
= 0, and hence by Equation (10) we
have that:
ˆg(i)
t (a) −¯g(i)(a)
 ≤ s"
P,0.8177710843373494,2 log(2/δ2)
P,0.8192771084337349,"nt(a)
∀i ∈JmK, t ∈JTK, a ∈JKK"
P,0.8207831325301205,as desired.
P,0.822289156626506,"F
Proofs omitted from Section 7"
P,0.8237951807228916,"Theorem 7.1. In the stochastic setting, for any ϵ > 0 Algorithm 1 guarantees that with probability at
least 1 −ϵ:"
P,0.8253012048192772,"RT ≤4
p"
P,0.8268072289156626,"KT log(2K/ϵ)
and
Vt ≤53
p"
P,0.8283132530120482,"Kt log(28mKT 2/ϵ)
∀t ∈JTK."
P,0.8298192771084337,"Proof. To prove the upper bound on the regret, we simply have to combine Corollary 6.3 with
Theorem 4.1. By Corollary 6.3 which probability at least 1 −5mKTδ, it holds X ⋆⊆∩t∈JT K b
Xt."
P,0.8313253012048193,"Moreover, by Theorem 4.1, we have that for each x ∈X ⋆with probability at least 1 −δ1:
X"
P,0.8328313253012049,"t∈JT K
⟨ft, x⟩−ft(at) ≤4
p"
P,0.8343373493975904,KT log(K/δ1).
P,0.8358433734939759,"Let x⋆= arg maxx∈X ⋆P
t∈JT K⟨x, ft⟩. Then, by union bound we have that with probability at least
1 −5mKTδ2 −δ1 it holds:
X"
P,0.8373493975903614,"t∈JT K
⟨ft, x⋆⟩−ft(at) ≤4
p"
P,0.838855421686747,"KT log(K/δ1),"
P,0.8403614457831325,"proving the bound on the regret. The bound on the violations holds with probability at least 1 −
2mT 2δ2 by Theorem 6.1, and guarantees:"
P,0.8418674698795181,"Vt ≤53
p"
P,0.8433734939759037,Kt log(2/δ2).
P,0.8448795180722891,"By an union bounds on all events, the guarantees hold with probability at least 1 −7mKT 2δ2 −δ1.
Thus by taking δ1 = ϵ/2 and δ2 = ϵ/(14mKT 2) we obtain the desired result."
P,0.8463855421686747,"Theorem 7.2. In the adversarial setting, for any ϵ > 0 Algorithm 1 guarantees that with probability
at least 1 −ϵ:"
P,0.8478915662650602,"α-RT ≤4
p"
P,0.8493975903614458,"KT log(2K/ϵ)
and
Vt ≤53
p"
P,0.8509036144578314,"Kt log(28mKT 2/ϵ)
∀t ∈JTK,"
P,0.8524096385542169,where α = ρ/(1+ρ).
P,0.8539156626506024,"Proof. Combining Theorem 5.2 and Theorem 4.1 readily proves that with probability at least 1 −δ1
we have that for all ˜x ∈X ⋆
∅⊆b
Xt, we have:
X"
P,0.8554216867469879,"t∈JT K
⟨ft, ˜x⟩−ft(at) ≤4
p"
P,0.8569277108433735,KT log(K/δ1).
P,0.858433734939759,"Let x⋆= arg maxx∈∆K
P
t∈JT K⟨x, ft⟩. Then, observe that ¯x =
1
1+ρx∅+
ρ
1+ρx∗∈X ∅, where
x∅(a∅) = 1 and x∅(a) = 0 for each a ̸= a∅. Then, we have that: X"
P,0.8599397590361446,"t∈JT K
⟨¯x, ft⟩=
X"
P,0.8614457831325302,t∈JT K
P,0.8629518072289156,"1
1 + ρx∅+
ρ
1 + ρx⋆, ft"
P,0.8644578313253012,"≥
ρ
1 + ρ X"
P,0.8659638554216867,"t∈JT K
⟨x⋆, ft⟩."
P,0.8674698795180723,"since ft(a∅) ≥0. This proves that with probability at least 1 −δ1:

ρ
1+ρ

-RT ≤4
p"
P,0.8689759036144579,KT log(K/δ1).
P,0.8704819277108434,"Similarly to the proof of Theorem 7.1, we can prove that the bound on the violations holds with
probability at least 1 −2mT 2δ2 by Theorem 6.1, and give:"
P,0.8719879518072289,"Vt ≤53
p"
P,0.8734939759036144,Kt log(2/δ2).
P,0.875,"Overall these events hold with probability at least 1 −2mT 2δ2 −δ1. By deﬁning δ1 = ϵ/2 and
δ2 = ϵ/(14mKT 2) we have that the desired results hold with probability at least 1 −ϵ."
P,0.8765060240963856,"Theorem 7.3. Algorithm 1, in the stochastic setting, guarantees that with probability at least 1 −ϵ,
it holds that:
V+
t ≤16
p"
P,0.8780120481927711,"Kt log(28mKT 2/ϵ)
∀t ∈JTK."
P,0.8795180722891566,Proof. Deﬁne for each i ∈JmK and t ∈JTK
P,0.8810240963855421,"Vi,+
t
:= t
X τ=1"
P,0.8825301204819277,"h
⟨xτ, ¯g(i)
τ ⟩
i+
."
P,0.8840361445783133,"Then, given an i and a t consider the following chain of inequalities:"
P,0.8855421686746988,"Vi,+
t
= t
X τ=1"
P,0.8870481927710844,"h
⟨xτ, ¯g(i)
τ ⟩
i+ = t
X τ=1"
P,0.8885542168674698,"h
⟨xτ, ¯g(i)
τ
−ˆg(i)
τ
+ ˆg(i)
τ ⟩
i+ = t
X τ=1"
P,0.8900602409638554,"h
⟨xτ, ¯g(i)
τ
−ˆg(i)
τ ⟩+ ⟨xτ, ˆg(i)
τ ⟩
i+ ≤ t
X τ=1"
P,0.891566265060241,"h
⟨xτ, ¯g(i)
τ
−ˆg(i)
τ ⟩+ ⟨xτ, bτ⟩
i+
(xτ ∈b
Xτ) ≤ t
X τ=1"
P,0.8930722891566265,"h
⟨xτ, ¯g(i)
τ
−ˆg(i)
τ ⟩
i+
+ ⟨xτ, bτ⟩]+ ≤2 t
X"
P,0.8945783132530121,"τ=1
⟨xτ, bτ⟩+
(Lemma 6.2)"
P,0.8960843373493976,where last inequality both hold with probability 1 −5mKTδ2 jointly for each i and t.
P,0.8975903614457831,"Since bt =
q"
P,0.8990963855421686,2 log(2/δ2)
P,0.9006024096385542,"nt−1(a)
we can apply Lemma 5.8 and an union bound on all t to ﬁnd that with
probability at least 1 −Tδ2 −5mKTδ2:"
P,0.9021084337349398,"Vi,+
t
≤4
p"
P,0.9036144578313253,"2Kt log(2/δ2) + 8
p"
P,0.9051204819277109,"t log(1/δ2)
∀i ∈JmK, t ∈JTK.
Thus, we can conclude that:"
P,0.9066265060240963,"V+
t ≤16
p"
P,0.9081325301204819,"Kt log(2/δ2)
∀i ∈JmK, t ∈JTK
with probability at least 1 −6mKTδ2. Recalling that δ2 = ϵ/(14mKT 2) we obtain the result."
P,0.9096385542168675,"G
Further technical lemmas"
P,0.911144578313253,"Lemma G.1. For any sequence of function rt : JKK →[−1, 1] which is t −1 predictable and any
sequence of randomized strategy xt ∈∆K, it holds that with probability at least 1 −δ: X"
P,0.9126506024096386,"t∈JT K
⟨xt, rt⟩−
X"
P,0.9141566265060241,"t∈JT K
rt(at) ≤4
p"
P,0.9156626506024096,T log(1/δ).
P,0.9171686746987951,Proof. By deﬁnition Ea∼xt[rt(a)] = P
P,0.9186746987951807,"a∈JKK rt(a)xt(a) = ⟨xt, rt⟩. Thus the sequence Xt =
Pt
τ=1[rτ(aτ)−⟨xτ, rτ⟩] is a martingale and |Xt −Xt−1| ≤2. Thus we can apply Azuma inequality
and ﬁnd that with probability at least 1 −δ: X"
P,0.9201807228915663,"t∈JT K
⟨xt, rt⟩−
X"
P,0.9216867469879518,"t∈JT K
rt(at) ≤4
p"
P,0.9231927710843374,T log(1/δ).
P,0.9246987951807228,"Lemma G.2. For any sequence of randomized strategy xt ∈∆K and any function ¯r(a) such that
rt(a) are sampled from a distribution with mean ¯r(a), i.e., E[rt(a)] = ¯r(a) and P(|rt(a)| ≤1) = 1,
it holds that with probability at least 1 −δ: X"
P,0.9262048192771084,"t∈JT K
⟨xt, rt⟩−
X"
P,0.927710843373494,"t∈JT K
⟨xt, ¯r⟩ ≤4
p"
P,0.9292168674698795,T log(1/δ).
P,0.9307228915662651,"Proof. This holds by simple application of Azuma’s inequality, similarly to the proof of Lemma G.1."
P,0.9322289156626506,NeurIPS Paper Checklist
CLAIMS,0.9337349397590361,1. Claims
CLAIMS,0.9352409638554217,"Question: Do the main claims made in the abstract and introduction accurately reﬂect the
paper’s contributions and scope?"
CLAIMS,0.9367469879518072,Answer: [Yes]
CLAIMS,0.9382530120481928,Justiﬁcation: We included the main contributions and scope in the abstract
CLAIMS,0.9397590361445783,Guidelines:
CLAIMS,0.9412650602409639,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reﬂect how
much the results can be expected to generalize to other settings.
• It is ﬁne to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.9427710843373494,2. Limitations
LIMITATIONS,0.9442771084337349,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.9457831325301205,Answer: [Yes]
LIMITATIONS,0.947289156626506,"Justiﬁcation: Yes, the paper discusses limitations."
LIMITATIONS,0.9487951807228916,Guidelines:
LIMITATIONS,0.9503012048192772,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-speciﬁcation, asymptotic approximations only holding locally). The authors
should reﬂect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reﬂect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reﬂect on the factors that inﬂuence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efﬁciency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be speciﬁcally instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.9518072289156626,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9533132530120482,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.9548192771084337,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.9563253012048193,"Justiﬁcation: Assumptions are explicitly discussed and all the proofs are provided.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9578313253012049,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
THEORY ASSUMPTIONS AND PROOFS,0.9593373493975904,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justiﬁcation: The paper is theoretical and we do not have any experimental results.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9608433734939759,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or veriﬁable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might sufﬁce, or if the contribution is a speciﬁc model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
THEORY ASSUMPTIONS AND PROOFS,0.9623493975903614,"Question: Does the paper provide open access to the data and code, with sufﬁcient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
THEORY ASSUMPTIONS AND PROOFS,0.963855421686747,"Answer: [NA]
Justiﬁcation: The paper is theoretical and we do not have any experimental results.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9653614457831325,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
THEORY ASSUMPTIONS AND PROOFS,0.9668674698795181,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justiﬁcation: The paper is theoretical and we do not have any experimental results.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9683734939759037,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Signiﬁcance"
THEORY ASSUMPTIONS AND PROOFS,0.9698795180722891,"Question: Does the paper report error bars suitably and correctly deﬁned or other appropriate
information about the statistical signiﬁcance of the experiments?
Answer: [NA]
Justiﬁcation: The paper is theoretical and we do not have any experimental results.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9713855421686747,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, conﬁ-
dence intervals, or statistical signiﬁcance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean."
THEORY ASSUMPTIONS AND PROOFS,0.9728915662650602,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not veriﬁed.
• For asymmetric distributions, the authors should be careful not to show in tables or
ﬁgures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding ﬁgures or tables in the text.
8. Experiments Compute Resources"
THEORY ASSUMPTIONS AND PROOFS,0.9743975903614458,"Question: For each experiment, does the paper provide sufﬁcient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justiﬁcation: The paper is theoretical and we do not have any experimental results.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9759036144578314,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
THEORY ASSUMPTIONS AND PROOFS,0.9774096385542169,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justiﬁcation: The paper conforms.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9789156626506024,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
THEORY ASSUMPTIONS AND PROOFS,0.9804216867469879,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justiﬁcation: The paper is theoretical without any immediate societal impacts.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9819277108433735,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake proﬁles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact speciﬁc
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to"
THEORY ASSUMPTIONS AND PROOFS,0.983433734939759,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efﬁciency and accessibility of ML).
11. Safeguards"
THEORY ASSUMPTIONS AND PROOFS,0.9849397590361446,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justiﬁcation: No data has been used in this paper.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9864457831325302,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety ﬁlters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
THEORY ASSUMPTIONS AND PROOFS,0.9879518072289156,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justiﬁcation: No data has been used in this paper.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9894578313253012,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
THEORY ASSUMPTIONS AND PROOFS,0.9909638554216867,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
THEORY ASSUMPTIONS AND PROOFS,0.9924698795180723,"Answer: [NA]
Justiﬁcation: No new assets have been introduced in this paper.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9939759036144579,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip ﬁle.
14. Crowdsourcing and Research with Human Subjects"
THEORY ASSUMPTIONS AND PROOFS,0.9954819277108434,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justiﬁcation: No crowdsourcing have been used in this paper.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9969879518072289,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is ﬁne, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justiﬁcation: No crowdsourcing have been used in this paper.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9984939759036144,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary signiﬁcantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
