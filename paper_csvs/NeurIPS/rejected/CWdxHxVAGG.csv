Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0021598272138228943,"The optimal prediction strategy for out-of-distribution (OOD) setups is a funda-
1"
ABSTRACT,0.004319654427645789,"mental question in machine learning. In this paper, we address this question and
2"
ABSTRACT,0.0064794816414686825,"present several contributions. We propose three reject option models for OOD
3"
ABSTRACT,0.008639308855291577,"setups: the Cost-based model, the Bounded TPR-FPR model, and the Bounded
4"
ABSTRACT,0.01079913606911447,"Precision-Recall model. These models extend the standard reject option models
5"
ABSTRACT,0.012958963282937365,"used in non-OOD setups and define the notion of an optimal OOD selective classi-
6"
ABSTRACT,0.01511879049676026,"fier. We establish that all the proposed models, despite their different formulations,
7"
ABSTRACT,0.017278617710583154,"share a common class of optimal strategies. Motivated by the optimal strategy, we
8"
ABSTRACT,0.019438444924406047,"introduce double-score OOD methods that leverage uncertainty scores from two
9"
ABSTRACT,0.02159827213822894,"chosen OOD detectors: one focused on OOD/ID discrimination and the other on
10"
ABSTRACT,0.023758099352051837,"misclassification detection. The experimental results consistently demonstrate the
11"
ABSTRACT,0.02591792656587473,"superior performance of this simple strategy compared to state-of-the-art methods.
12"
ABSTRACT,0.028077753779697623,"Additionally, we propose novel evaluation metrics derived from the definition of
13"
ABSTRACT,0.03023758099352052,"the optimal strategy under the proposed OOD rejection models. These new metrics
14"
ABSTRACT,0.032397408207343416,"provide a comprehensive and reliable assessment of OOD methods without the
15"
ABSTRACT,0.03455723542116631,"deficiencies observed in existing evaluation approaches.
16"
INTRODUCTION,0.0367170626349892,"1
Introduction
17"
INTRODUCTION,0.038876889848812095,"Most methods for learning predictors from data are based on the closed-world assumption, i.e., the
18"
INTRODUCTION,0.04103671706263499,"training and the test samples are generated i.i.d. from the same distribution, so-called in-distribution
19"
INTRODUCTION,0.04319654427645788,"(ID). However, in real-world applications, ID test samples can be contaminated by samples from
20"
INTRODUCTION,0.04535637149028078,"another distribution, the so-called Out-of-Distribution (OOD), which is not represented in training
21"
INTRODUCTION,0.047516198704103674,"examples. A trustworthy prediction model should detect OOD samples and reject to predict them,
22"
INTRODUCTION,0.04967602591792657,"while simultaneously minimizing the prediction error on accepted ID samples.
23"
INTRODUCTION,0.05183585313174946,"In recent years, the development of deep learning models for handling OOD data has emerged as a
24"
INTRODUCTION,0.05399568034557235,"critical challenge in the field of machine learning, leading to an explosion of research papers dedicated
25"
INTRODUCTION,0.056155507559395246,"to developing effective OOD detection methods (OODD) [10, 11, 4, 3, 12, 8, 1, 17, 16, 19, 20].
26"
INTRODUCTION,0.058315334773218146,"Existing methods use various principles to learn a classifier of ID samples and a selective function
27"
INTRODUCTION,0.06047516198704104,"that accepts the input for prediction or rejects it to predict. We further denote the pair of ID classifier
28"
INTRODUCTION,0.06263498920086392,"and the selective function as OOD selective classifier, borrowing terminology from the non-OOD
29"
INTRODUCTION,0.06479481641468683,"setup [7]. There is an agreement that a good OOD selective classifier should reject OOD samples
30"
INTRODUCTION,0.06695464362850972,"and simultaneously achieve high classification accuracy on ID samples that are accepted [22]. To
31"
INTRODUCTION,0.06911447084233262,"our knowledge, there is surprisingly no formal definition of an optimal OOD selective classifier.
32"
INTRODUCTION,0.07127429805615551,"Consequently, there is also no consensus on how to evaluate the OODD methods. The commonly used
33"
INTRODUCTION,0.0734341252699784,"metrics [21] evaluate only one aspect of the OOD selective classifier, either the accuracy of the ID
34"
INTRODUCTION,0.0755939524838013,"classifier or the performance of the selective function as an OOD/ID discriminator. Such evaluation
35"
INTRODUCTION,0.07775377969762419,"is inconclusive and usually inconsistent; e.g., the two most commonly used metrics, AUROC and
36"
INTRODUCTION,0.07991360691144708,"OSCR, often lead to a completely reversed ranking of evaluated methods (see Sec. 3.4).
37"
INTRODUCTION,0.08207343412526998,"In this paper, we ask the following question: What would be the optimal prediction strategy for
38"
INTRODUCTION,0.08423326133909287,"the OOD setup in the ideal case when ID and OOD distributions were known? To this end, we
39"
INTRODUCTION,0.08639308855291576,"offer the contributions: (i) We propose three reject option models for the OOD setup: Cost-based
40"
INTRODUCTION,0.08855291576673865,"model, bounded TPR-FPR model, and Bounded Precision-Recall model. These models extend the
41"
INTRODUCTION,0.09071274298056156,"standard rejection models used in the non-OOD setup [2, 15] and define the notion of an optimal OOD
42"
INTRODUCTION,0.09287257019438445,"classifier. (ii) We establish that all the proposed models, despite their different formulations, share
43"
INTRODUCTION,0.09503239740820735,"a common class of optimal strategies. The optimal OOD selective classifier combines a Bayes ID
44"
INTRODUCTION,0.09719222462203024,"classifier with a selective function based on a linear combination of the conditional risk and likelihood
45"
INTRODUCTION,0.09935205183585313,"ratio of the OOD and ID samples. This selective function enables a trade-off between distinguishing
46"
INTRODUCTION,0.10151187904967603,"ID from OOD samples and detecting misclassifications. (iii) Motivated by the optimal strategy,
47"
INTRODUCTION,0.10367170626349892,"we introduce double-score OOD methods that leverage uncertainty scores from two chosen OOD
48"
INTRODUCTION,0.10583153347732181,"detectors: one focused on OOD/ID discrimination and the other on misclassification detection. We
49"
INTRODUCTION,0.1079913606911447,"show experimentally that this simple strategy consistently outperforms the state-of-the-art. (iv) We
50"
INTRODUCTION,0.1101511879049676,"review existing metrics for evaluation of OODD methods and show that they provide incomplete
51"
INTRODUCTION,0.11231101511879049,"view, if used separately, or inconsistent view of the evaluated methods, if used together. We propose
52"
INTRODUCTION,0.11447084233261338,"novel evaluation metrics derived from the definition of optimal strategy under the proposed OOD
53"
INTRODUCTION,0.11663066954643629,"rejection models. These new metrics provide a comprehensive and reliable assessment of OODD
54"
INTRODUCTION,0.11879049676025918,"methods without the deficiencies observed in existing approaches.
55"
REJECT OPTION MODELS FOR OOD SETUP,0.12095032397408208,"2
Reject option models for OOD setup
56"
REJECT OPTION MODELS FOR OOD SETUP,0.12311015118790497,"The terminology of ID and OOD samples comes from the setups when the training set contains only
57"
REJECT OPTION MODELS FOR OOD SETUP,0.12526997840172785,"ID samples, while the test set contains a mixture of ID and OOD samples. In this paper, we analyze
58"
REJECT OPTION MODELS FOR OOD SETUP,0.12742980561555076,"which prediction strategies are optimal on the test samples, but we do not address the problem of
59"
REJECT OPTION MODELS FOR OOD SETUP,0.12958963282937366,"learning such strategy. We follow the OOD setup from [5]. Let X be a set of observable inputs (or
60"
REJECT OPTION MODELS FOR OOD SETUP,0.13174946004319654,"features), and Y a finite set of labels that can be assigned to in-distribution (ID) inputs. ID samples
61"
REJECT OPTION MODELS FOR OOD SETUP,0.13390928725701945,"(x, y) ∈X × Y are generated from a joint distribution pI : X × Y →R+. Out-of-distribution (OOD)
62"
REJECT OPTION MODELS FOR OOD SETUP,0.13606911447084233,"samples x are generated from a distribution pO : X →R+. ID and OOD samples share the same
63"
REJECT OPTION MODELS FOR OOD SETUP,0.13822894168466524,"input space X. Let ∅be a special label to mark the OOD sample. Let ¯Y = Y ∪{∅} be an extended
64"
REJECT OPTION MODELS FOR OOD SETUP,0.14038876889848811,"set of labels. In the testing stage the samples (x, ¯y) ∈X × ¯Y are generated from the joint distribution
65"
REJECT OPTION MODELS FOR OOD SETUP,0.14254859611231102,"p: X × ¯Y →R+ defined as a mixture of ID and OOD:
66"
REJECT OPTION MODELS FOR OOD SETUP,0.1447084233261339,"p(x, ¯y) =

pO(x) π
if ¯y = ∅
pI(x, ¯y) (1 −π)
if ¯y ∈Y
,
(1)"
REJECT OPTION MODELS FOR OOD SETUP,0.1468682505399568,"where π ∈[0, 1) is the probability of observing the OOD sample. Our OOD setup subsumes the
67"
REJECT OPTION MODELS FOR OOD SETUP,0.1490280777537797,"standard non-OOD setup as a special case when π = 0, and the reject option models that will be
68"
REJECT OPTION MODELS FOR OOD SETUP,0.1511879049676026,"introduced below will become for π = 0 the known reject option models for the non-OOD setup.
69"
REJECT OPTION MODELS FOR OOD SETUP,0.15334773218142547,"Our goal is to design OOD selective classifier q: X →D, where D = Y ∪{reject}, which either
70"
REJECT OPTION MODELS FOR OOD SETUP,0.15550755939524838,"predicts a label, q(x) ∈Y, or it rejects the prediction, q(x) = reject, when (i) input x ∈X prevents
71"
REJECT OPTION MODELS FOR OOD SETUP,0.15766738660907129,"accurate prediction of y ∈Y because it is noisy, or (ii) comes from OOD. We represent the selective
72"
REJECT OPTION MODELS FOR OOD SETUP,0.15982721382289417,"classifier by the ID classifier h: X →Y, and a stochastic selective function c: X →[0, 1] that
73"
REJECT OPTION MODELS FOR OOD SETUP,0.16198704103671707,"outputs a probability that the input is accepted [7], i.e.,
74"
REJECT OPTION MODELS FOR OOD SETUP,0.16414686825053995,"q(x) = (h, c)(x) =

h(x)
with probability c(x)
reject
with probability 1 −c(x)
.
(2)"
REJECT OPTION MODELS FOR OOD SETUP,0.16630669546436286,"In the following sections, we propose three reject option models that define the notion of the optimal
75"
REJECT OPTION MODELS FOR OOD SETUP,0.16846652267818574,"OOD selective classifier of the form (2) applied to samples generated by (1).
76"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.17062634989200864,"2.1
Cost-based rejection model for OOD setup
77"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.17278617710583152,"A classical approach to define an optimal classifier is to formulate it as a loss minimization problem.
78"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.17494600431965443,"This requires defining a loss ¯ℓ: ¯Y × D →R+ for each combination of the label ¯y ∈¯Y = Y ∪{∅}
79"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.1771058315334773,"and the output of the classifier q(x) ∈D = Y ∪{reject}. Let ℓ: Y × Y →R+ be some application-
80"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.17926565874730022,"specific loss on ID samples, e.g., 0/1-loss or MAE. Furthermore, we need to define the loss for the
81"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.18142548596112312,"case where the input is OOD sample ¯y = ∅or the classifier rejects q(x) = reject. Let ε1 ∈R+ be
82"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.183585313174946,"the loss for rejecting the ID sample, ε2 ∈R+ loss for prediction on the OOD sample, and ε3 ∈R+
83"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.1857451403887689,"loss for correctly rejecting the OOD sample. ℓ, ε1, ε2 and ε3 can be arbitrary, but we assume that
84"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.1879049676025918,"ε2 > ε3. The loss ¯ℓis then:
85"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.1900647948164147,"¯ℓ(¯y, q) = 

 
"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.19222462203023757,"ℓ(¯y, q)
if
¯y ∈Y ∧q ∈Y
ε1
if
¯y ∈Y ∧q = reject
ε2
if
¯y = ∅∧q ∈Y
ε3
if
¯y = ∅∧q = reject (3)"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.19438444924406048,"Having the loss ¯ℓ, we can define the optimal OOD selective classifier as a minimizer of the expected
86"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.19654427645788336,"risk R(h, c) = Ex,y∼p(x,¯y)¯ℓ(¯y, (h, c)(x)).
87"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.19870410367170627,"Definition 1 (Cost-based OOD model) An optimal OOD selective classifier (hC, cC) is a solution to
88"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.20086393088552915,"the minimization problem minh,c R(h, c) where we assume that both minimizers exist.
89"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.20302375809935205,"An optimal solution of the cost-based OOD model requires three components: The Bayes ID classifier
90"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.20518358531317496,"hB(x) ∈Argmin
y′∈Y X"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.20734341252699784,"y∈Y
pI(y | x)ℓ(y, y′) ,
(4)"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.20950323974082075,"its conditional risk rB(x) = P
y∈Y pI(y | x)ℓ(y, hB(x)), and the likelihood ratio of the OOD and
91"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.21166306695464362,"ID inputs, g(x) = pO(x)"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.21382289416846653,"pI(x) , which we defined to be g(x) = ∞for pI(x) = 0.
92"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.2159827213822894,"Theorem 1 An optimal selective classifier (hC, cC) under the cost-based OOD model is composed
93"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.21814254859611232,"of the Bayes classifier (4), hC = hB, and the selective function
94"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.2203023758099352,cC(x) =
COST-BASED REJECTION MODEL FOR OOD SETUP,0.2224622030237581,"( 1
if
sC(x) < ε1
τ
if
sC(x) = ε1
0
if
sC(x) > ε1
using the score
sC(x) = rB(x) + (ε2 −ε3)
π
1 −π g(x) (5)"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.22462203023758098,"where τ is an arbitrary number in [0, 1], and ε1, ε2, ε3 are losses defining the extended loss (3).
95"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.2267818574514039,"Note that τ can be arbitrary and therefore a deterministic selective function cC(x) = [[sC(x) ≤ε1]] is
96"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.22894168466522677,"also optimal. An optimal selective function accepts inputs based on the score sC(x), which is a linear
97"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.23110151187904968,"combination of two functions, conditional risk rB(x) and the likelihood ratio g(x) = pO(x)/pI(x).
98"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.23326133909287258,"Relation to cost-based model for Non-OOD setup
For π = 0, the cost-based OOD model reduces
99"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.23542116630669546,"to the standard cost-based model of the reject option classifier in a non-OOD setup [2]. In the
100"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.23758099352051837,"non-OOD setup, we do not need to specify the losses ε2 and ε3 and the risk R(h, c) simplifies
101"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.23974082073434125,"to R′(h, c) = Ex,y∼pI(x,y)

ℓ(y, h(x)) c(x) + ε1 (1 −c(x))

. The well-known optimal solution
102"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.24190064794816415,"is composed of the Bayes classifier hB(x) as in the OOD case; however, the selection function
103"
COST-BASED REJECTION MODEL FOR OOD SETUP,0.24406047516198703,"c′
C(x) = [[r(x) ≤A]] accepts the input solely based on the conditional risk r(x).
104"
BOUNDED TPR-FPR REJECTION MODEL,0.24622030237580994,"2.2
Bounded TPR-FPR rejection model
105"
BOUNDED TPR-FPR REJECTION MODEL,0.24838012958963282,"The cost-based OOD model requires the classification loss ℓfor ID samples and defining the costs ε1,
106"
BOUNDED TPR-FPR REJECTION MODEL,0.2505399568034557,"ε2, ε3 which is difficult in practice because the physical units of ℓand ε1, ε2, ε3 are often different.
107"
BOUNDED TPR-FPR REJECTION MODEL,0.2526997840172786,"In this section, we propose an alternative approach which requires only the classification loss ℓwhile
108"
BOUNDED TPR-FPR REJECTION MODEL,0.2548596112311015,"costs ε1, ε2, ε3 are replaced by constraints on the performance of the selective function.
109"
BOUNDED TPR-FPR REJECTION MODEL,0.2570194384449244,"The selective function c: X →[0, 1] can be seen as a discriminator of OOD/ID samples. Let
110"
BOUNDED TPR-FPR REJECTION MODEL,0.2591792656587473,"us consider ID and OOD samples as positive and negative classes, respectively. We introduce
111"
BOUNDED TPR-FPR REJECTION MODEL,0.2613390928725702,"three metrics to measure the performance of the OOD selective classifier (h, c). We measure the
112"
BOUNDED TPR-FPR REJECTION MODEL,0.2634989200863931,"performance of selective function by the True Positive Rate (TPR) and the False Positive Rate (FPR).
113"
BOUNDED TPR-FPR REJECTION MODEL,0.265658747300216,"The TPR is defined as the probability that ID sample is accepted by the selective function c, i.e.,
114"
BOUNDED TPR-FPR REJECTION MODEL,0.2678185745140389,"ϕ(c) =
Z"
BOUNDED TPR-FPR REJECTION MODEL,0.26997840172786175,"X
p(x | ¯y ̸= ∅) c(x) dx =
Z"
BOUNDED TPR-FPR REJECTION MODEL,0.27213822894168466,"X
pI(x) c(x) dx .
(6)"
BOUNDED TPR-FPR REJECTION MODEL,0.27429805615550756,"The FPR is defined as the probability that OOD sample is accepted by the selective function c, i.e.,
115"
BOUNDED TPR-FPR REJECTION MODEL,0.27645788336933047,"ρ(c) =
Z"
BOUNDED TPR-FPR REJECTION MODEL,0.2786177105831533,"X
p(x | ¯y = ∅) c(x) dx =
Z"
BOUNDED TPR-FPR REJECTION MODEL,0.28077753779697623,"X
pO(x) c(x) dx .
(7)"
BOUNDED TPR-FPR REJECTION MODEL,0.28293736501079914,"The second identity in (6) and (7) is obtained after substituting the definition of p(x, ¯y) from (1).
116"
BOUNDED TPR-FPR REJECTION MODEL,0.28509719222462204,"Lastly, we characterize the performance of the ID classifier h: X →Y by the selective risk
117"
BOUNDED TPR-FPR REJECTION MODEL,0.28725701943844495,"RS(h, c) = R X
P"
BOUNDED TPR-FPR REJECTION MODEL,0.2894168466522678,"y∈Y pI(x, y) ℓ(h(x), y) c(x) dx"
BOUNDED TPR-FPR REJECTION MODEL,0.2915766738660907,"ϕ(c)
defined for non-zero ϕ(c), i.e., the expected loss of the classifier h calculated on the ID samples
118"
BOUNDED TPR-FPR REJECTION MODEL,0.2937365010799136,"accepted by the selective function c.
119"
BOUNDED TPR-FPR REJECTION MODEL,0.2958963282937365,"Definition 2 (Bounded TPR-FPR model) Let ϕmin ∈[0, 1] be the minimal acceptable TPR and
120"
BOUNDED TPR-FPR REJECTION MODEL,0.2980561555075594,"ρmax ∈[0, 1] maximal acceptable FPR. An optimal OOD selective classifier (hT , cT ) under the
121"
BOUNDED TPR-FPR REJECTION MODEL,0.3002159827213823,"bounded TPR-FPR model is a solution of the problem
122"
BOUNDED TPR-FPR REJECTION MODEL,0.3023758099352052,"min
h∈YX ,c∈[0,1]X RS(h, c)
s.t.
ϕ(c) ≥ϕmin
and
ρ(c) ≤ρmax ,
(8)"
BOUNDED TPR-FPR REJECTION MODEL,0.3045356371490281,"where we assume that both minimizers exist.
123"
BOUNDED TPR-FPR REJECTION MODEL,0.30669546436285094,"Theorem 2 Let (h, c) be an optimal solution to (8). Then (hB, c), where hB is the Bayes ID
124"
BOUNDED TPR-FPR REJECTION MODEL,0.30885529157667385,"classifier (4), is also optimal to (8).
125"
BOUNDED TPR-FPR REJECTION MODEL,0.31101511879049676,"According to Theorem 2, the Bayes ID classifier hB is an optimal solution to (8) that defines the
126"
BOUNDED TPR-FPR REJECTION MODEL,0.31317494600431967,"bounded TPR-FPR model. This is not surprising, but it is a practically useful result, because it allows
127"
BOUNDED TPR-FPR REJECTION MODEL,0.31533477321814257,"one to solve (8) in two consecutive steps: First, set hT to the Bayes ID classifier hB. Second, when
128"
BOUNDED TPR-FPR REJECTION MODEL,0.3174946004319654,"hT is fixed, the optimal selection function cT is obtained by solving (8) only w.r.t. c which boils
129"
BOUNDED TPR-FPR REJECTION MODEL,0.31965442764578833,"down to:
130"
BOUNDED TPR-FPR REJECTION MODEL,0.32181425485961124,"Problem 1 (Bounded TPR-FPR model for known h(x)) Given ID classifier h: X →Y, the opti-
131"
BOUNDED TPR-FPR REJECTION MODEL,0.32397408207343414,"mal selective function c∗: X →[0, 1] is a solution to
132"
BOUNDED TPR-FPR REJECTION MODEL,0.326133909287257,"min
c∈[0,1]X RS(h, c)
s.t.
ϕ(c) ≥ϕmin ,
and
ρ(c) ≤ρmax ."
BOUNDED TPR-FPR REJECTION MODEL,0.3282937365010799,"Problem 1 is meaningful even if h is not the Bayes ID classifier hB. We can search for an optimal
133"
BOUNDED TPR-FPR REJECTION MODEL,0.3304535637149028,"selective function c∗(x) for any fixed h, which in practice is usually our best approximation of hB
134"
BOUNDED TPR-FPR REJECTION MODEL,0.3326133909287257,"learned from the data.
135"
BOUNDED TPR-FPR REJECTION MODEL,0.3347732181425486,Theorem 3 Let h: X →Y be ID classifier and r: X →R its conditional risk r(x) = P
BOUNDED TPR-FPR REJECTION MODEL,0.3369330453563715,"y∈Y pI(y |
136"
BOUNDED TPR-FPR REJECTION MODEL,0.3390928725701944,"x)ℓ(y, h(x)). Let g(x) = pI(x)/pI(x) be the likelihood ratio of ID and OOD samples. Then, the set
137"
BOUNDED TPR-FPR REJECTION MODEL,0.3412526997840173,"of optimal solutions of Problem 1 contains the selective classifier
138"
BOUNDED TPR-FPR REJECTION MODEL,0.3434125269978402,c∗(x) =
BOUNDED TPR-FPR REJECTION MODEL,0.34557235421166305,"(
0
if
s(x) > λ
τ(x)
if
s(x) = λ
1
if
s(x) < λ
using score
s(x) = r(x) + µ g(x)
(9)"
BOUNDED TPR-FPR REJECTION MODEL,0.34773218142548595,"where decision threshold λ ∈R, and multiplier µ ∈R are constants and τ : X →[0, 1] is a function
139"
BOUNDED TPR-FPR REJECTION MODEL,0.34989200863930886,"implicitly defined by the problem parameters.
140"
BOUNDED TPR-FPR REJECTION MODEL,0.35205183585313177,"The optimal c∗(x) is based on the score composed of a linear combination of r(x) and g(x) as in the
141"
BOUNDED TPR-FPR REJECTION MODEL,0.3542116630669546,"case of the cost-based model (5). Unlike the cost-based model, the acceptance probability τ(x) for
142"
BOUNDED TPR-FPR REJECTION MODEL,0.3563714902807775,"boundary inputs Xs(x)=λ = {x ∈X | s(x) = λ} cannot be arbitrary, in general. However, if X is
143"
BOUNDED TPR-FPR REJECTION MODEL,0.35853131749460043,"continuous, the set Xs(x)=λ has probability measure zero, up to some pathological cases, and τ(x)
144"
BOUNDED TPR-FPR REJECTION MODEL,0.36069114470842334,"can be arbitrary, i.e., the deterministic c∗(x) = [[s(x) ≤λ]] is optimal. If X is finite, the value of
145"
BOUNDED TPR-FPR REJECTION MODEL,0.36285097192224625,"τ(x) can be found by linear programming. The linear program and more details on the form of τ(x)
146"
BOUNDED TPR-FPR REJECTION MODEL,0.3650107991360691,"are in the Appendix.
147"
BOUNDED TPR-FPR REJECTION MODEL,0.367170626349892,"Relation to Bounded-Abstention model for the non-OOD setup
For π = 0, the bounded TPR-
148"
BOUNDED TPR-FPR REJECTION MODEL,0.3693304535637149,"FPR model reduces to the bounded-abstention option model for non-OOD setup [15]. Namely,
149"
BOUNDED TPR-FPR REJECTION MODEL,0.3714902807775378,"ρ(c) ≤ρmax can be removed because there are no OOD samples, and (8) becomes the bounded-
150"
BOUNDED TPR-FPR REJECTION MODEL,0.37365010799136067,"abstention model: minh,c RS(h, c), s.t. ϕ(c) ≥ϕmin, which seeks the selective classifier with
151"
BOUNDED TPR-FPR REJECTION MODEL,0.3758099352051836,"guaranteed TPR and minimal selective risk. In the non-OOD setup, TPR is called coverage. An
152"
BOUNDED TPR-FPR REJECTION MODEL,0.3779697624190065,"optimal solution of the bounded abstention model [6], is composed of the Bayes ID classifier hB, and
153"
BOUNDED TPR-FPR REJECTION MODEL,0.3801295896328294,"the same optimal selective function as the TPR-FPR model (9), however, with µ = 0 and τ(x) = τ,
154"
BOUNDED TPR-FPR REJECTION MODEL,0.38228941684665224,"∀x ∈X, i.e., the score depends only on r(x) and an identical randomization is applied in all edge
155"
BOUNDED TPR-FPR REJECTION MODEL,0.38444924406047515,"cases [6]. Therefore, r(x) is the optimal score to detect misclassified ID samples in non-OOD setup
156"
BOUNDED TPR-FPR REJECTION MODEL,0.38660907127429806,"as it allows to achieve the minimal selective risk RS for any fixed coverage (TPR,ϕ).
157"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.38876889848812096,"2.3
Bounded Precision-Recall rejection model
158"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.39092872570194387,"The optimal selective classifier under the bounded TPR-FPR model does not depend on the prior of
159"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.3930885529157667,"the OOD samples π, which is useful, e.g., when π is unknown in the testing stage. In the case π is
160"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.3952483801295896,"known, it might be more suitable to constrain the precision rather than the FPR, while the constraint
161"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.39740820734341253,"on TPR remains the same. In the context of precision, we denote ϕ(c) as recall instead of TPR. The
162"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.39956803455723544,"precision κ(c) is defined as the portion of samples accepted by c(x) that are actual ID samples, i.e.,
163"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.4017278617710583,"κ(c) = (1 −π)
R"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.4038876889848812,"X p(x | ¯y ̸= ∅) c(x) dx
R"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.4060475161987041,"X p(x) c(x) dx
=
(1 −π) ϕ(c)
ρ(c) π + ϕ(c) (1 −π) ."
BOUNDED PRECISION-RECALL REJECTION MODEL,0.408207343412527,"Definition 3 (Bounded Precision-Recall model) Let κmin ∈[0, 1] be a minimal acceptable pre-
164"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.4103671706263499,"cision and ϕmin ∈[0, 1] minimal acceptable recall (a.k.a. TPR). An optimal selective classifier
165"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.41252699784017277,"(hP , cP ) under the bounded Precision-Recall model is a solution of the problem
166"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.4146868250539957,"min
h∈YX ,c∈[0,1]X RS(h, c)
s.t.
ϕ(c) ≥ϕmin
and
κ(c) ≥κmin
(10)"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.4168466522678186,"where we assume that both minimizers exist.
167"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.4190064794816415,"Theorem 4 Let (h, c) be an optimal solution to (10). Then (hB, c), where hB is the Bayes ID
168"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.42116630669546434,"classifier (4), is also optimal to (10).
169"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.42332613390928725,"Theorem 4 ensures that the Bayes ID classifier is an optimal solution to (10). After fixing hP = hB,
170"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.42548596112311016,"the search for an optimal selective function c leads to:
171"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.42764578833693306,"Problem 2 (Bounded Prec-Recall model for known h(x)) Given ID classifier h: X →Y, the
172"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.4298056155507559,"optimal selective function c∗: X →[0, 1] is a solution to
173"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.4319654427645788,"min
c∈[0,1]X RS(h, c)
s.t.
ϕ(c) ≥ϕmin
and
κ(c) ≥κmin ."
BOUNDED PRECISION-RECALL REJECTION MODEL,0.43412526997840173,Theorem 5 Let h: X →Y be ID classifier and r: X →R its conditional risk r(x) = P
BOUNDED PRECISION-RECALL REJECTION MODEL,0.43628509719222464,"y∈Y pI(y |
174"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.43844492440604754,"x)ℓ(y, h(x)). Let g(x) = pO(x)/pI(x) be the likelihood ratio of OOD and ID samples. Then, the
175"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.4406047516198704,"set of optimal solutions of Problem 2 contains the selective function
176"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.4427645788336933,c∗(x) =
BOUNDED PRECISION-RECALL REJECTION MODEL,0.4449244060475162,"(
0
if
s(x) > λ
τ(x)
if
s(x) = λ
1
if
s(x) < λ
using the score
s(x) = r(x) + µ g(x)
(11)"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.4470842332613391,"where detection threhold λ ∈R, and multiplier µ ∈R are constants and τ : X →[0, 1] is a function
177"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.44924406047516197,"implicitly defined by the problem parameters.
178"
SUMMARY,0.4514038876889849,"2.4
Summary
179"
SUMMARY,0.4535637149028078,"We proposed three rejection models for OOD setup which define the notion of optimal OOD selective
180"
SUMMARY,0.4557235421166307,"classifier: Cost-based model, Bounded TRP-FPR model, and Bounded Precision-Recall model. We
181"
SUMMARY,0.45788336933045354,"established that all three models, despite different formulation, share the class of optimal prediction
182"
SUMMARY,0.46004319654427644,"strategies. Namely, the optimal OOD selective classifier (h∗, c∗) is composed of the Bayes ID
183"
SUMMARY,0.46220302375809935,"classifier (4), h∗= hB, and the selective function
184"
SUMMARY,0.46436285097192226,c∗(x) =
SUMMARY,0.46652267818574517,"(
0
if
s(x) > λ
τ(x)
if
s(x) = λ
1
if
s(x) < λ
where
s(x) = r(x) + µ g(x)
(12)"
SUMMARY,0.468682505399568,"where λ, µ, and τ(x) are specific for the used rejection model. However, in all cases, the optimal
185"
SUMMARY,0.4708423326133909,"uncertainty score s(x) for accepting the inputs is based on a linear combination of the conditional
186"
SUMMARY,0.47300215982721383,"risk r(x) of the ID classifier h∗and the OOD/ID likelihood ratio g(x) = pO(x)/pI(x). On the other
187"
SUMMARY,0.47516198704103674,"hand, from the optimal solution of the well-known Neyman-Person problem [14], it follows that the
188"
SUMMARY,0.4773218142548596,"likelihood ratio g(x) is the optimal score of OOD/ID discrimination. Our results thus show that the
189"
SUMMARY,0.4794816414686825,"optimal OOD selective function needs to trade-off the ability to detect the misclassification of ID
190"
SUMMARY,0.4816414686825054,"samples and the ability to distinguish ID from OOD samples.
191"
SUMMARY,0.4838012958963283,"Single-score vs. double-score OODD methods
The existing OODD methods, which we further
192"
SUMMARY,0.48596112311015116,"call single-score methods, produce a classifier h: X →Y and an uncertainty score s: X →R. The
193"
SUMMARY,0.48812095032397407,"score s(x) is used to construct a selective function c(x) = [[s(x) ≤λ]] where λ ∈R is a decision
194"
SUMMARY,0.490280777537797,"threshold chosen in post-hoc evaluation. Hence, the existing methods effectively produce a set of
195"
SUMMARY,0.4924406047516199,"selective classifiers Q = {(h, c) | c(x) = [[s(x) ≤λ]] , λ ∈R}. In contrast to existing methods, we
196"
SUMMARY,0.4946004319654428,"established that the optimal selective function is always based on a linear combination of two scores:
197"
SUMMARY,0.49676025917926564,"conditional risk r(x) and likelihood ratio g(x). Therefore, we propose the double-score method,
198"
SUMMARY,0.49892008639308855,"which in addition to a classifier h(x), produces two scores, sr : X →R and sg : X →R, and uses
199"
SUMMARY,0.5010799136069114,"their combination s(x) = sr(x) + µ sg(x) to accept inputs. Formally, the double-score method
200"
SUMMARY,0.5032397408207343,"produces a set of selective classifiers Q = {(h, c) | c(x) = [[sr(x) + µ sg(x) ≤λ]] , µ ∈R , λ ∈R}.
201"
SUMMARY,0.5053995680345572,"The double-score strategy can be used to leverage uncertainty scores from two chosen OODD
202"
SUMMARY,0.5075593952483801,"methods: one focused on OOD/ID discrimination and the other on misclassification detection.
203"
POST-HOC TUNING AND EVALUATION METRICS,0.509719222462203,"3
Post-hoc tuning and evaluation metrics
204"
POST-HOC TUNING AND EVALUATION METRICS,0.5118790496760259,"Let T = ((xi, ¯yi) ∈X × ¯Y | i = 1, . . . , n) be a set of validation examples i.i.d. drawn from a
205"
POST-HOC TUNING AND EVALUATION METRICS,0.5140388768898488,"distribution p(x, ¯y). Given a set of selective classifiers Q, trained by the single-score or double-score
206"
POST-HOC TUNING AND EVALUATION METRICS,0.5161987041036717,"OODD method, the goal of the post-hoc tuning is to use T to select the best selective classifier
207"
POST-HOC TUNING AND EVALUATION METRICS,0.5183585313174947,"(hn, cn) ∈Q and estimate its performance on unseen samples generated from the same p(x, ¯y). This
208"
POST-HOC TUNING AND EVALUATION METRICS,0.5205183585313174,"task requires a notion of an optimal selective classifier which we defined by the proposed rejection
209"
POST-HOC TUNING AND EVALUATION METRICS,0.5226781857451404,"models. In Sec 3.2 and Sec 3.3, we propose the post-hoc tuning and evaluation metrics for the
210"
POST-HOC TUNING AND EVALUATION METRICS,0.5248380129589633,"Bounded TPR-FPR and Bounded Precision-Recall models, respectively. In Sec 3.4 we review the
211"
POST-HOC TUNING AND EVALUATION METRICS,0.5269978401727862,"existing evaluation metrics for OODD methods and point out their deficiencies. We will exemplify
212"
POST-HOC TUNING AND EVALUATION METRICS,0.5291576673866091,"the proposed metrics on synthetic data and OODD methods described in Sec 3.1.
213"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.531317494600432,"3.1
Synthetic data and exemplar single-score and double-score OODD methods
214"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.5334773218142549,"Let us consider a simple 1-D setup. The input space is X = R and there are three ID labels
215"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.5356371490280778,"Y = {1, 2, 3}. ID samples are generated from pI(x, 1) = 0.3N(x; −1, 1), pI(x, 2) = 0.3N(x; 1, 1),
216"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.5377969762419006,"pI(x, 3) = 0.4N(x; 3, 1), where N(x; µ, σ) is normal distribution with mean µ and variance σ.
217"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.5399568034557235,"OOD is the normal distribution pO(x) = N(x; 3, 0.2), and the OOD prior π = 0.25. We use
218"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.5421166306695464,"0/1-loss ℓ(y, y′) = [[y ̸= y′]], i.e., RS is the classification error on accepted inputs. The known ID
219"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.5442764578833693,"and OOD alows us to evaluate the Bayes ID classifier hB(x) by (4), its conditional risk rB(x) =
220"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.5464362850971922,"miny′∈Y
P"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.5485961123110151,"y∈Y pI(y | x)ℓ(y, y′) and the OOD/ID likelihood ratio g(x) = pO(x)/pI(x).
221"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.550755939524838,"We consider 3 exemplar single-score OODD methods A, B, C. The methods produce the same optimal
222"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.5529157667386609,"classifier h∗(x) and the selective functions c(x) = [[rB(x) + µ g(x) ≤λ]] with a different setting of
223"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.5550755939524838,"µ. I.e., the method k ∈{A, B, C} produces the set of selective classifiers Qk = {(h∗(x), c(x)) |
224"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.5572354211663066,"c(x) = [[rB(x) + µk g(x) ≤λ]] , λ ∈R}, where the constant µk is defined as follows:
225"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.5593952483801296,"• Method A(∞): µ = ∞, s(x) = g(x). This corresponds to the optimal OOD/ID discriminator.
226"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.5615550755939525,"• Method B(0.2): µ = 0.2, s(x) = rB(x) + 0.2g(x). Combination of method A and C.
227"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.5637149028077754,"• Method C(0): µ = 0, s(x) = rB(x). This corresponds to the optimal misclassification detector.
228"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.5658747300215983,"We also consider a double-score method, Method D(R), which outputs the same optimal classifier
229"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.5680345572354212,"h∗(x), and scores sr(x) = r(x) and sg(x) = g(x). I.e., Method D(R) produces the set of selective
230"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.5701943844492441,"classifiers QD = {(h∗(x), c(x)) | c(x) = [[r(x) + µ g(x) ≤λ]] , µ ∈R, λ ∈R}. Note that we have
231"
SYNTHETIC DATA AND EXEMPLAR SINGLE-SCORE AND DOUBLE-SCORE OODD METHODS,0.572354211663067,"shown that QD contains an optimal selective classifier regardless of the reject option model used.
232"
BOUNDED TPR-FPR REJECTION MODEL,0.5745140388768899,"3.2
Bounded TPR-FPR rejection model
233"
BOUNDED TPR-FPR REJECTION MODEL,0.5766738660907127,"The bounded TPR-FPR model is defined using the selective risk RS(h, c), TPR ϕ(c) and FPR ρ(c)
234"
BOUNDED TPR-FPR REJECTION MODEL,0.5788336933045356,"the value of which can be estimated from the validation set T as follows:
235"
BOUNDED TPR-FPR REJECTION MODEL,0.5809935205183585,"RS
n(h, c) = P"
BOUNDED TPR-FPR REJECTION MODEL,0.5831533477321814,"i∈II ℓ(yi, h(xi)) c(xi)
P"
BOUNDED TPR-FPR REJECTION MODEL,0.5853131749460043,"i∈II c(xi)
,
ϕn(h, c) =
1
|II| X"
BOUNDED TPR-FPR REJECTION MODEL,0.5874730021598272,"i∈II
c(xi) ,
ρn(h, c) =
1
|IO| X"
BOUNDED TPR-FPR REJECTION MODEL,0.5896328293736501,"i∈IO
c(xi)"
BOUNDED TPR-FPR REJECTION MODEL,0.591792656587473,"where II = {i ∈{1, . . . , n} | ¯yi ̸= ∅} and IO = {i ∈{1, . . . , n} | ¯yi = ∅} are indices of ID and
236"
BOUNDED TPR-FPR REJECTION MODEL,0.593952483801296,"OOD samples in T , respectively.
237"
BOUNDED TPR-FPR REJECTION MODEL,0.5961123110151187,"Proposed metrics
TPR-FPR model
Prec-Recall model
↓Selective risk at
↓Selective risk at
↑Existing metrics
Method
TPR(0.7),FPR(0.2)
Prec(0.9),Recall(0.7)
AUROC
AUPR
OSCR
A(∞)
0.157
0.157
0.88
0.96
0.82
B(0.2)
0.143
0.143
0.86
0.95
0.83
C(0)
unable
unable
0.76
0.92
0.86
D(R) proposed
0.133
0.129
0.88
0.96
0.86
Table 1: Evalution of the examplar single-score methods A, B, C and the proposed double-score
method D on synthetic data using the proposed metrics and the existing ones. The selective risk
correponds to the classification error on accepted ID samples."
BOUNDED TPR-FPR REJECTION MODEL,0.5982721382289417,"Given the target TPR ϕmin ∈(0, 1] and FPR ρmax ∈(0, 1], the best selective classifier (hn, cn) out
238"
BOUNDED TPR-FPR REJECTION MODEL,0.6004319654427646,"of Q is found by solving:
239"
BOUNDED TPR-FPR REJECTION MODEL,0.6025917926565875,"(hn, cn) ∈Argmin
(h,c)∈Q
RS
n(h, c)
s.t.
ϕn(h, c) ≥ϕmin ,
and
ρn(h, c) ≤ρmax .
(13)"
BOUNDED TPR-FPR REJECTION MODEL,0.6047516198704104,"Proposed evaluation metric
If problem (13) is feasible, RS
n(hn, cn) is reported as the performance
240"
BOUNDED TPR-FPR REJECTION MODEL,0.6069114470842333,"estimator of OODD method producing Q. Otherwise, the method is marked as unable to achieve
241"
BOUNDED TPR-FPR REJECTION MODEL,0.6090712742980562,"the target TPR and FPR. Tab. 1 shows the selective risk for the methods A-D at the target TPR
242"
BOUNDED TPR-FPR REJECTION MODEL,0.6112311015118791,"ϕmin = 0.7 and FPR ρmax = 0.2. The minimal RS
n is achieved by method D(R), followed by B(0.2)
243"
BOUNDED TPR-FPR REJECTION MODEL,0.6133909287257019,"and A(∞), while C(0) is unable to achieve the target TPR and FPR. One can visualize RS
n in a range
244"
BOUNDED TPR-FPR REJECTION MODEL,0.6155507559395248,"of operating points while bounding only ρmax or ϕmin. E.g., by fixing ρmax we can plot RS
n as
245"
BOUNDED TPR-FPR REJECTION MODEL,0.6177105831533477,"a function of attainable values of ϕn by which we obtain the Risk-Coverage curve, known from
246"
BOUNDED TPR-FPR REJECTION MODEL,0.6198704103671706,"non-OOD setup, at ρmax. Recall that TPR is coverage. See Appendix for Risk-Coverage curve at
247"
BOUNDED TPR-FPR REJECTION MODEL,0.6220302375809935,"ρmax for methods A-D.
248"
BOUNDED TPR-FPR REJECTION MODEL,0.6241900647948164,"ROC curve
The problem (13) can be infeasible. To choose a feasible target on ϕmin and ρmax, it
249"
BOUNDED TPR-FPR REJECTION MODEL,0.6263498920086393,"is advantageous to plot the ROC curve, i.e., values of TPR and FPR attainable by the classifiers in Q.
250"
BOUNDED TPR-FPR REJECTION MODEL,0.6285097192224622,"For single-score methods, the ROC curve is a set of points obtained by varying the decision threshold:
251"
BOUNDED TPR-FPR REJECTION MODEL,0.6306695464362851,"ROC(Q) = {(ϕn(h, c), ρn(h, c)) | c(x) = [[s(x) ≤λ]] , λ ∈R}. In case of double-score methods,
252"
BOUNDED TPR-FPR REJECTION MODEL,0.6328293736501079,"we vary ρmax ∈[0, 1] and for each ρmax we choose the maximal feasible ϕn. I.e., ROC curve
253"
BOUNDED TPR-FPR REJECTION MODEL,0.6349892008639308,"is ROC(Q) = {(ϕ, ρmax) | ϕ = max(h,c)∈Q ϕn(h, c) s.t. ρn(h, c) ≤ρmax ,
ρmax ∈[0, 1]}.
254"
BOUNDED TPR-FPR REJECTION MODEL,0.6371490280777538,"See Appendix for ROC curve of the methods A-D. In Tab. 1 we report the Area Under ROC curve
255"
BOUNDED TPR-FPR REJECTION MODEL,0.6393088552915767,"(AUROC) which is a commonly used summary of the entire ROC curve. The highest AUROC
256"
BOUNDED TPR-FPR REJECTION MODEL,0.6414686825053996,"achieved Methods A(∞) and E(R). Recall that Method A(∞) uses the optimal ID/OOD discriminator
257"
BOUNDED TPR-FPR REJECTION MODEL,0.6436285097192225,"and the proposed Method E(R) subsumes A(∞).
258"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6457883369330454,"3.3
Bounded Precision-Recall rejection model
259"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6479481641468683,"Let κn(c) = (1 −π) ϕn(c)/((1 −π)ϕn(c) + πρn(c)) be the sample precision of the selective
260"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6501079913606912,"function c. Given the target recall ϕmin ∈(0, 1] and precision κmin ∈(0, 1], the best selective
261"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.652267818574514,"classifier (hn, cn) out of Q is found by solving
262"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6544276457883369,"(hn, cn) ∈Argmin
(h,c)∈Q
RS
n(h, c)
s.t.
ϕn(h, c) ≥ϕmin ,
κn(h, c) ≥κmin .
(14)"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6565874730021598,"Proposed evaluation metric
If problem (14) is feasible, RS
n(hn, cn) is reported as the performance
263"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6587473002159827,"estimator of OODD method which produced Q. Otherwise, the method is marked as unable to achieve
264"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6609071274298056,"the target Precison/Recall. Tab. 1 shows the selective risk for the methods A-D at the Precision
265"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6630669546436285,"κmin = 0.9 and recall ϕmax = 0.7. The minimal RS
n is achieved by the proposed method D(R),
266"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6652267818574514,"followed by B(0.2) and A(∞), while method C(0) is unable to achieve the target Precision/Recall.
267"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6673866090712743,"Note that single-score methods A-C achieve the same RS
n under both TPR-FPR and Prec-Recall
268"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6695464362850972,"models while the results for double-score method D(R) differ. The reason is that both models share
269"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.67170626349892,"the same constraint ϕn ≥0.7 (TPR is Recall) which is active, while the other two constraints are not
270"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.673866090712743,"active because RS
n is a monotonic function w.r.t. the value of the decision threshold.
271"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6760259179265659,"Precision-Recall (PR) curve
To choose feasible bounds on κmin and ϕmin before solving (14),
272"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6781857451403888,"one can plot the PR curve, i.e., the values of precision and recall attainable by the classifiers in
273"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6803455723542117,"Q. For single-score methods, the PR curve is a set of points obtained by varying the decision
274"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6825053995680346,"threshold: PR(Q) = {(κn(h, c), ϕn(h, c)) | c(x) = [[s(x) ≤λ]] , λ ∈R}. In case of double-
275"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6846652267818575,"score methods, we vary ϕmin ∈[0, 1] and for each ϕmin we choose the maximal feasible κn, i.e.,
276"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6868250539956804,"PR(Q) = {(κ, ϕmin) | κ = max(h,c)∈Q κn(h, c) s.t. ϕn(h, c) ≥ϕmin ,
ϕmin ∈[0, 1]}. See
277"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6889848812095032,"Appendix for PR curve of the methods A-D. We compute the Area Under the PR curve and report it
278"
BOUNDED PRECISION-RECALL REJECTION MODEL,0.6911447084233261,"for Methods A-D in Tab. 1. Rankings of the methods w.r.t AUPR and AUROC are the same.
279"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.693304535637149,"3.4
Shortcomings of existing evaluation metrics
280"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.6954643628509719,"The most commonly used metrics to evaluate OODD methods are the AUROC and AUPR [10, 13,
281"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.6976241900647948,"3, 12, 1, 16]. Both metrics measure the ability of the selective function c(x) to distinguish ID from
282"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.6997840172786177,"OOD samples. AUROC and AUPR are often the only metrics reported although they completely
283"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7019438444924406,"ignore the performance of the ID classifier. Our synthetic example shows that high AUROC/AUPR
284"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7041036717062635,"is not a precursor of a good OOD selective classifier. E.g., Method A(∞), using optimal OOD/ID
285"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7062634989200864,"discriminator, attains the highest (best) AUROC and AUPR (see Tab. 1), however, at the same time
286"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7084233261339092,"Method A(∞) achieves the highest (worst) RS
n under both rejection models, and it is also the worst
287"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7105831533477321,"misclassification detector according to the OSCR score defined below.
288"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.712742980561555,"The performance of the ID classifier h(x) is usually evaluated by the ID classification accuracy
289"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.714902807775378,"(a.k.a. closed set accuracy) [13, 3] and by the OSCR score [4, 8, 1]. The ID accuracy measures
290"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7170626349892009,"the performance of h(x) assuming all inputs are accepted, i.e., c(x) = 1, ∀x ∈X, hence it says
291"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7192224622030238,"nothing about the performance on the actually accepted samples like RS
n. E.g., Methods A-D in our
292"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7213822894168467,"synthetic example use the same classifier h(x) and hence have the same ID accuracy, however, they
293"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7235421166306696,"perform quite differently in terms of the other more relevant metrics, like RS
n or OSCR. The OSCR
294"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7257019438444925,"score is defined as the area under CCR versus FPR curve [21], where the CCR stands for the correct
295"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7278617710583153,"classification rate on the accepted ID samples; in case of 0/1-loss CCR = 1 −RS
n. The CCR-FPR
296"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7300215982721382,"curve evaluates the performance of the ID classifier on the accepted samples, but it ignores the ability
297"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7321814254859611,"of c(x) to discriminate OOD and ID samples as it does not depend on TPR. E.g., Method D(0), using
298"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.734341252699784,"the optimal misclassification detector, achieves the highest (best) OSCR score; however, at the same
299"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7365010799136069,"time, it has the lowest (worst) AUROC and AUPR.
300"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7386609071274298,"Other, less frequently used metrics involve: F1-score, FPR@TPRx, TNR@TPRx, CCR@FPRx [10, 8,
301"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7408207343412527,"1, 21, 16]. All these metrics are derived from either ROC, PR or CCR-FPR curve, and hence they
302"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7429805615550756,"suffer with the same conceptual problems as AUROC, AUPR and OSCR, respectively.
303"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7451403887688985,"We argue that the existing metrics evaluate only one aspect of the OOD selective classifier, namely,
304"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7473002159827213,"either the ability to disciminate ID from OOD samples, or the performance of ID classifier on the
305"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7494600431965442,"accepted (or on possibly all) ID samples. We show that in principle there can be methods that are best
306"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7516198704103672,"OOD/ID discriminators but the worst misclassification detectors and vice versa. Therefore, using
307"
SHORTCOMINGS OF EXISTING EVALUATION METRICS,0.7537796976241901,"individual metrics can (and often does) provide inconsistent ranking of the evaluated methods.
308"
SUMMARY,0.755939524838013,"3.5
Summary
309"
SUMMARY,0.7580993520518359,"We propose novel evaluation metrics derived from the definition of the optimal strategy under the
310"
SUMMARY,0.7602591792656588,"proposed OOD rejection models. The proposed metrics simultaneously evaluate the classification
311"
SUMMARY,0.7624190064794817,"performance on the accepted ID samples and they guarantee the perfomance of the OOD/ID discrimi-
312"
SUMMARY,0.7645788336933045,"nator, either via constraints in TPR-FPR or Precision-Recall pair. Advantages of the proposed metrics
313"
SUMMARY,0.7667386609071274,"come at a price. Namely, we need to specify feasible target TPR and FPR, or Precision and Recall,
314"
SUMMARY,0.7688984881209503,"depending on the model used. However, feasible values of TPR-FPR and Prec-Recall pairs can be
315"
SUMMARY,0.7710583153347732,"easily read out of the ROC and PR curve, respectively. We argue that setting these extra parameters is
316"
SUMMARY,0.7732181425485961,"better than using the existing metrics that provide incomplete, if used separately, or inconsistent, if
317"
SUMMARY,0.775377969762419,"used in combination, view of the evaluated methods.
318"
SUMMARY,0.7775377969762419,"Another issue is solving the problems (13) and (14) to compute the proposed evaluation metrics and
319"
SUMMARY,0.7796976241900648,"figures. Fortunately, both problems lead to optimization w.r.t one or two varibales in case of the
320"
SUMMARY,0.7818574514038877,"single-score and double-score methods, respectively. A simple and efficient algorithm to solve the
321"
SUMMARY,0.7840172786177105,"problems in O(n log n) time is provided in Appendix.
322"
SUMMARY,0.7861771058315334,"OOD: notmnist
OOD: fashionmnist
OOD: cifar10
↓S. risk at
↓S. risk at
↓S. risk at
TPR(0.80)
TPR(0.80)
TPR(0.80)
Method
FPR(0.08)
↑AUROC
↑OSCR
FPR(0.10)
↑AUROC
↑OSCR
FPR(0.29)
↑AUROC
↑OSCR"
SUMMARY,0.7883369330453563,ID: mnist
SUMMARY,0.7904967602591793,"MSP [10]
0.00014
0.936
0.996
0.00013
0.956
0.994
0.00013
0.989
0.991
MLS [9]
0.00139
0.941
0.993
0.00139
0.972
0.991
0.00139
0.993
0.990
ODIN [11]
0.00069
0.942
0.993
0.00069
0.970
0.991
0.00069
0.993
0.990
REACT [17]
0.00637
0.962
0.991
0.00637
0.985
0.990
0.00637
0.992
0.989
KNN [19]
0.00041
0.976
0.991
0.00041
0.947
0.993
0.00041
0.976
0.991
VIM [20]
0.00193
0.983
0.990
0.00194
0.926
0.993
0.00194
0.860
0.995
KNN+MSP
0.00000
0.976
0.996
0.00000
0.962
0.994
0.00000
0.991
0.991
VIM+MSP
0.00014
0.987
0.996
0.00013
0.976
0.994
0.00013
0.992
0.995"
SUMMARY,0.7926565874730022,"OOD: cifar100
OOD: tiny imagenet
OOD: mnist
↓S. risk at
↓S. risk at
↓S. risk at
TPR(0.80)
TPR(0.80)
TPR(0.80)
Method
FPR(0.21)
↑AUROC
↑OSCR
FPR(0.19)
↑AUROC
↑OSCR
FPR(0.19)
↑AUROC
↑OSCR"
SUMMARY,0.7948164146868251,ID: cifar10
SUMMARY,0.796976241900648,"MSP [10]
0.00676
0.871
0.977
0.00676
0.887
0.976
0.00676
0.899
0.976
MLS [9]
0.00984
0.861
0.973
0.00984
0.885
0.971
0.00984
0.905
0.971
ODIN [11]
0.01000
0.851
0.975
0.01000
0.864
0.974
0.00995
0.915
0.969
REACT [17]
0.00856
0.864
0.973
0.00856
0.888
0.971
0.00856
0.883
0.972
KNN [19]
0.00665
0.896
0.974
0.00665
0.914
0.972
0.00665
0.916
0.973
VIM [20]
0.01232
0.872
0.972
0.01232
0.888
0.971
0.01236
0.873
0.974
KNN+MSP
0.00652
0.896
0.977
0.00652
0.914
0.976
0.00652
0.916
0.976
VIM+MSP
0.00676
0.879
0.977
0.00676
0.894
0.976
0.00676
0.900
0.976
Table 2: Evaluation of existing single-score methods MSP, MLS, ODIN, REACT, KNN and two
instances of the proposed double-score strategy: KNN+MSP and VIM+MSP. We use MNIST (top
table) and CIFAR10 (bottom table) as ID, and three different datasets as OOD. We report the standard
AUROC and OSCR, and the proposed selective risk at target TPR and FPR, where the selective risk
corresponds to the classification error on accepted ID samples. Best results are in bold."
EXPERIMENTS,0.7991360691144709,"4
Experiments
323"
EXPERIMENTS,0.8012958963282938,"In this section, we evaluate single-score OODD methods and the proposed double-score strategy,
324"
EXPERIMENTS,0.8034557235421166,"using the existing and the proposed evaluation metrics. We use MSP [10], MLS [9], ODIN [11] as
325"
EXPERIMENTS,0.8056155507559395,"baselines and REACT [17], KNN [19], VIM [20] as repesentatives of recent single-score approaches.
326"
EXPERIMENTS,0.8077753779697624,"We evaluate two instances of the double-score strategy. First, we combine the scores of MSP [10]
327"
EXPERIMENTS,0.8099352051835853,"and KNN [18] and, second, scores of MSP and VIM [20]. MSP score is asymptotically the best
328"
EXPERIMENTS,0.8120950323974082,"misclassification detector, while KNN and VIM are two best OOD/ID discriminators according
329"
EXPERIMENTS,0.8142548596112311,"to their AUROC. We always use the ID classifier of the MSP method. The evaluation data and
330"
EXPERIMENTS,0.816414686825054,"implementations of OODD methods are taken from OpenOOD benchmark [21]. Because the datasets
331"
EXPERIMENTS,0.8185745140388769,"have unrealistically high portion of OOD samples, e.g., π > 0.5, we use metrics that do not depend
332"
EXPERIMENTS,0.8207343412526998,"on π. Namely, AUROC and OSCR as the most frequently used metrics, and the proposed selective
333"
EXPERIMENTS,0.8228941684665226,"risk at TPR and FPR. We use 0/1-loss, hence the reported selective risk is the classification error on
334"
EXPERIMENTS,0.8250539956803455,"accepted ID samples with guranteed TPR and FPR. In all experiments we fix the target TPR to 0.8
335"
EXPERIMENTS,0.8272138228941684,"while FPR is set for each database to the highest FPR attained by all compared methods.
336"
EXPERIMENTS,0.8293736501079914,"Results are presented in Tab. 2. It is seen that the single-score methods with the highest AUROC and
337"
EXPERIMENTS,0.8315334773218143,"OSCR are always different, which prevents us to create a single conclusive ranking of the evaluated
338"
EXPERIMENTS,0.8336933045356372,"approaches. MSP is almost consistently the best misclassification detector according to OSCR. The
339"
EXPERIMENTS,0.8358531317494601,"best OOD/ID discriminator is, according to AUROC, one of the recent methods: REACT, KNN, or
340"
EXPERIMENTS,0.838012958963283,"VIM. The proposed double-score strategy, KNN+MSP and VIM+MSP, consistently outperforms the
341"
EXPERIMENTS,0.8401727861771058,"other approaches in all metrics.
342"
CONCLUSIONS,0.8423326133909287,"5
Conclusions
343"
CONCLUSIONS,0.8444924406047516,"This paper introduces novel reject option models which define the notion of the optimal prediction
344"
CONCLUSIONS,0.8466522678185745,"strategy for OOD setups. We prove that all models, despite their different formulations, share the
345"
CONCLUSIONS,0.8488120950323974,"same class of optimal prediction strategies. The main insight is that the optimal prediction strategy
346"
CONCLUSIONS,0.8509719222462203,"must trade-off the ability to detect misclassified examples and to distinguish ID from OOD samples.
347"
CONCLUSIONS,0.8531317494600432,"This is in contrast to existing OOD methods that output a single uncertainty score. We propose a
348"
CONCLUSIONS,0.8552915766738661,"simple and effective double-score strategy that allows us to boost performance of two existing OOD
349"
CONCLUSIONS,0.857451403887689,"methods by combining their uncertainty scores. Finally, we suggest improved evaluation metrics
350"
CONCLUSIONS,0.8596112311015118,"for assessing OOD methods that simultaneously evaluate all aspects of the OOD methods and are
351"
CONCLUSIONS,0.8617710583153347,"directly related to the optimal OOD strategy under the proposed reject option models.
352"
REFERENCES,0.8639308855291576,"References
353"
REFERENCES,0.8660907127429806,"[1] Guangyao Chen, Peixi Peng, Xiangqian Wang, and Yonghong Tian. Adversarial reciprocal points learning
354"
REFERENCES,0.8682505399568035,"for open set recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11):8065–
355"
REFERENCES,0.8704103671706264,"8081, 2022.
356"
REFERENCES,0.8725701943844493,"[2] C. Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on Information Theory,
357"
REFERENCES,0.8747300215982722,"16(1):41–46, 1970.
358"
REFERENCES,0.8768898488120951,"[3] Terrance DeVries and Graham W Taylor. Learning confidence for out-of-distribution detection in neural
359"
REFERENCES,0.8790496760259179,"networks. arXiv preprint arXiv:1802.04865, 2018.
360"
REFERENCES,0.8812095032397408,"[4] Akshay Raj Dhamija, Manuel Günther, and Terrance Boult. Reducing network agnostophobia. In S. Bengio,
361"
REFERENCES,0.8833693304535637,"H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural
362"
REFERENCES,0.8855291576673866,"Information Processing Systems, volume 31. Curran Associates, Inc., 2018.
363"
REFERENCES,0.8876889848812095,"[5] Zhen Fang, Yixuan Li, Jie Lu, Jiahua Dong, Bo Han, and Feng Liu. Is out-of-distribution detection
364"
REFERENCES,0.8898488120950324,"learnable? In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in
365"
REFERENCES,0.8920086393088553,"Neural Information Processing Systems, volume 35, pages 37199–37213. Curran Associates, Inc., 2022.
366"
REFERENCES,0.8941684665226782,"[6] Vojtech Franc, Daniel Prusa, and Vaclav Voracek. Optimal strategies for reject option classifiers. Journal
367"
REFERENCES,0.896328293736501,"of Machine Learning Research, 24(11):1–49, 2023.
368"
REFERENCES,0.8984881209503239,"[7] Y. Geifman and R. El-Yaniv. Selective classification for deep neural networks. In Advances in Neural
369"
REFERENCES,0.9006479481641468,"Information Processing Systems 30, pages 4878–4887, 2017.
370"
REFERENCES,0.9028077753779697,"[8] Federica Granese, Marco Romanelli, Daniele Gorla, Catuscia Palamidessi, and Pablo Piantanida. Doctor:
371"
REFERENCES,0.9049676025917927,"A simple method for detecting misclassification errors. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S.
372"
REFERENCES,0.9071274298056156,"Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34,
373"
REFERENCES,0.9092872570194385,"pages 5669–5681. Curran Associates, Inc., 2021.
374"
REFERENCES,0.9114470842332614,"[9] Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou, Joseph Kwon, Mohammadreza Mostajabi,
375"
REFERENCES,0.9136069114470843,"Jacob Steinhardt, and Dawn Song. Scaling out-of-distribution detection for real-world settings. In Kamalika
376"
REFERENCES,0.9157667386609071,"Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings
377"
REFERENCES,0.91792656587473,"of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine
378"
REFERENCES,0.9200863930885529,"Learning Research, pages 8759–8773. PMLR, 17–23 Jul 2022.
379"
REFERENCES,0.9222462203023758,"[10] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples
380"
REFERENCES,0.9244060475161987,"in neural networks. In Proceedings of International Conference on Learning Representations, 2017.
381"
REFERENCES,0.9265658747300216,"[11] Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image detection in
382"
REFERENCES,0.9287257019438445,"neural networks. In International Conference on Learning Representations, 2018.
383"
REFERENCES,0.9308855291576674,"[12] Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. In S. Bengio,
384"
REFERENCES,0.9330453563714903,"H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural
385"
REFERENCES,0.9352051835853131,"Information Processing Systems, volume 31. Curran Associates, Inc., 2018.
386"
REFERENCES,0.937365010799136,"[13] Lawrence Neal, Matthew Olson, Xiaoli Fern, Weng-Keen Wong, and Fuxin Li. Open set learning with
387"
REFERENCES,0.9395248380129589,"counterfactual images. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors,
388"
REFERENCES,0.9416846652267818,"Computer Vision – ECCV 2018, pages 620–635, Cham, 2018. Springer International Publishing.
389"
REFERENCES,0.9438444924406048,"[14] Jerzy Neyman and Egon Person. On the use and interpretation of certain test criteria for purpose of
390"
REFERENCES,0.9460043196544277,"statistical inference. Biometrica, pages 175–240, 1928.
391"
REFERENCES,0.9481641468682506,"[15] T. Pietraszek. Optimizing abstaining classifiers using ROC analysis. In Proceedings of the 22nd Interna-
392"
REFERENCES,0.9503239740820735,"tional Conference on Machine Learning, page 665–672, 2005.
393"
REFERENCES,0.9524838012958964,"[16] Yue Song, Nicu Sebe, and Wei Wang. Rankfeat: Rank-1 feature removal for out-of-distribution detection.
394"
REFERENCES,0.9546436285097192,"In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural
395"
REFERENCES,0.9568034557235421,"Information Processing Systems, volume 35, pages 17885–17898. Curran Associates, Inc., 2022.
396"
REFERENCES,0.958963282937365,"[17] Yiyou Sun, Chuan Guo, and Yixuan Li. React: Out-of-distribution detection with rectified activations. In
397"
REFERENCES,0.9611231101511879,"A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information
398"
REFERENCES,0.9632829373650108,"Processing Systems, 2021.
399"
REFERENCES,0.9654427645788337,"[18] Yiyou Sun, Chuan Guo, and Yixuan Li. React: Out-of-distribution detection with rectified activations.
400"
REFERENCES,0.9676025917926566,"In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in
401"
REFERENCES,0.9697624190064795,"Neural Information Processing Systems, volume 34, pages 144–157. Curran Associates, Inc., 2021.
402"
REFERENCES,0.9719222462203023,"[19] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest
403"
REFERENCES,0.9740820734341252,"neighbors. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan
404"
REFERENCES,0.9762419006479481,"Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of
405"
REFERENCES,0.978401727861771,"Proceedings of Machine Learning Research, pages 20827–20840. PMLR, 17–23 Jul 2022.
406"
REFERENCES,0.980561555075594,"[20] Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtual-logit
407"
REFERENCES,0.9827213822894169,"matching. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
408"
REFERENCES,0.9848812095032398,"4911–4920, 2022.
409"
REFERENCES,0.9870410367170627,"[21] Jingkang Yang, Pengyun Wang, Dejian Zou, Zitang Zhou, Kunyuan Ding, Wenxuan Peng, Haoqi Wang,
410"
REFERENCES,0.9892008639308856,"Guangyao Chen, Bo Li, Yiyou Sun, Xuefeng Du, Kaiyang Zhou, Wayne Zhang, Dan Hendrycks, Yixuan
411"
REFERENCES,0.9913606911447084,"Li, and Ziwei Liu. Openood: Benchmarking generalized out-of-distribution detection. In Conference on
412"
REFERENCES,0.9935205183585313,"Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmar, 2022.
413"
REFERENCES,0.9956803455723542,"[22] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A
414"
REFERENCES,0.9978401727861771,"survey, 2022.
415"
