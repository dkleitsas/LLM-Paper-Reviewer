Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0014245014245014246,"Game-theoretic formulations in machine learning have recently risen in promi-
nence, whereby entire modeling paradigms are best captured as zero-sum games.
Despite their popularity, however, their dynamics are still poorly understood. This
lack of theory is often substantiated with painful empirical observations of volatile
training dynamics and even divergence. Such results highlight the need to develop
an appropriate theory with convergence guarantees that are powerful enough to in-
form practice. This paper studies the generalized Gradient Descent-Ascent (GDA)
ﬂow in a large class of non-convex non-concave Zero-Sum games dubbed Hid-
den Convex-Concave games, a class of games that includes GANs. We focus on
two speciﬁc geometries: a novel geometry induced by the hidden convex-concave
structure that we call the hidden mapping geometry and the Fisher information
geometry. For the hidden mapping geometry, we prove global convergence under
mild assumptions. In the case of Fisher information geometry, we provide a com-
plete picture of the dynamics in an interesting special setting of team competition
via invariant function analysis."
INTRODUCTION,0.002849002849002849,"1
INTRODUCTION"
INTRODUCTION,0.004273504273504274,"Min-max optimization has found extensive applications in modern Machine Learning (ML) and
Deep Learning.
Popular application settings include Generative Adversarial Networks (GANs)
(Goodfellow et al., 2014), adversarial training (Madry et al., 2018), and multi-agent reinforcement
learning (Silver et al., 2017). In all of these cases, a pair of networks is typically trained towards
ﬁnding an approximate equilibrium of a highly non-convex non-concave problem. However, such
settings go beyond classical and well-known results in game theory for which equilibria only exist
in more restrictive settings, i.e., convex-concave games (Sion et al., 1958). Unfortunately, there is
no parallel guarantee if the payoff is not convex-concave, and all these applications are indeed based
upon non-convex non-concave games. Even worse, many negative results occur when dealing with
such payoffs: global or local minimax may not exist (Jin et al., 2020), and even if they exist, there is,
in general, no “reasonable algorithm”1 that can globally converge to any meaningful notion of local
optimum (Letcher, 2021; Hsieh et al., 2020). To overcome these negative results, we analyze a spe-
ciﬁc class of non-convex non-concave games called Hidden Convex-Concave (HCC) games (Flokas
et al., 2020; Gidel et al., 2021; Flokas et al., 2021) that include many Machine Learning applications
such as GANs (Goodfellow et al., 2014), Adversarial Example Games (AEG) (Bose et al., 2020), or
Minimax Estimation of Conditional Moment (Dikkala et al., 2020)."
INTRODUCTION,0.005698005698005698,"Our contributions. In the setting of HCC games, our analysis tackles unique challenges not occur-
ring in the standard convex-concave games setting. We propose a new type of dynamics, dubbed
Natural Hidden Gradient dynamics (NHG), and we prove its convergence to stationary points of the
HCC game. Critically, our convergence results are global, i.e., we do not make any assumptions
about initial conditions, e.g., that the initial points belong to some local neighborhood. This novel
algorithm is inspired by the generalization of gradient ﬂows in different Banach spaces than the stan-
dard Euclidean one. Arguably, the most well-known non-Euclidean gradient is the natural gradient"
INTRODUCTION,0.007122507122507123,"∗Emails of contact:{andjela.mladenovic,gidelgau}@mila.quebec,
iosif sakos@mymail.sutd.edu.sg, georgios@sutd.edu.sg
†Canada CIFAR AI Chair
1see (Letcher, 2021, Deﬁnition 5) for a deﬁnition of “reasonable algorithm”"
INTRODUCTION,0.008547008547008548,Published as a conference paper at ICLR 2022
INTRODUCTION,0.009971509971509971,"ﬂow induced by the Fisher information matrix, which enjoys deep connections to the Replicator
Dynamics (RD). Thus, a natural second question emerges: Can we have similar convergence guar-
antees for RD? In that regard, we provide a complete picture of the resulting dynamics in a setting of
team competition via invariant function analysis. Speciﬁcally, we reduce the ﬂow of the dynamics to
a competition between two generalized gradients, one for each team. We show that the behavior of
the dynamics in this setting is constrained by a maximal number of invariant functions, which, com-
bined with the actual geometry of the available strategies, shows that the system can either converge
or cycle. These two results showcase, both, the importance of adapting the “algorithmic geometry”
to the application and the data at hand, as well as the unexplored effects of feasibility constraints on
the complexity of well-known game dynamics."
RELATED WORK,0.011396011396011397,"2
RELATED WORK"
RELATED WORK,0.01282051282051282,"Despite the popularity of deep learning applications involving non-convex non-concave games, our
understanding of their optimization dynamics and the nature of their solution concepts are still pre-
liminary. However, this space has already witnessed a few important early works that focus on
identifying new solution concepts. These solution concepts—which are also broadly applicable in
general min-max games—include (local/differential) Nash equilibria (Adolphs et al., 2019; Mazum-
dar & Ratliff, 2019), (local/differential) Stackelberg equilibria (Fiez et al., 2020; Wang et al., 2020),
local minmax (Daskalakis & Panageas, 2018), local robust points (Zhang et al., 2020), and approxi-
mate minimax theorems (Jin et al., 2020; Gidel et al., 2021). Numerous solutions concepts such as
cycles (Vlatakis-Gkaragkounis et al., 2019), chaotic behavior (Cheung & Piliouras, 2019; Cheung &
Tao, 2021), and computational issues (Daskalakis et al., 2021) indicate that solving min-max games,
in general, might involve challenging and complex behavior."
RELATED WORK,0.014245014245014245,"Many algorithms have been proposed to solve restricted classes of non-convex non-concave games
such as Polyak-Łojasiewicz games (Nouiehed et al., 2019; Yang et al., 2020), nonconvex-concave
games (Lin et al., 2020; Ostrovskii et al., 2021; Yang et al., 2020; Kong & Monteiro, 2021), as well as
classes of games inspired by variational inequalities (Mertikopoulos et al., 2019; Diakonikolas et al.,
2021; Lee & Kim, 2021). However, even though they are signiﬁcant advances in the understanding
of non-convex non-concave game dynamics, such classes of games may not encompass games where
the players are parameterized neural networks such as GANs."
RELATED WORK,0.01566951566951567,"While convergence in GANs has been a topic of exploration (Kodali et al., 2017; Heusel et al.,
2017; Mescheder et al., 2018; Gemp & Mahadevan, 2018; Li et al., 2018; Hsieh et al., 2019; Cao &
Guo, 2020), its hidden convex-concave structure has not been exploited before, and there were no
theoretical global convergence guarantees in a general setting up to this date. In particular, Hsieh
et al. (2019) use a lifting trick and proceed in solving a relaxation of the problem in the distribution
space, while in our work we work entirely in the parameter space. With this respect, we consider
Flokas et al. (2021), and Gemp & Mahadevan (2018) as the closest related works. On the one hand,
in Gemp & Mahadevan (2018), the authors propose crossing-the-curl, a second-order technique, and
provide global convergence guarantees for the Wasserstein Linear Quadratic GAN (W-LQGAN).
While the W-LQGANs is a class of non-convex non-concave GANs it remains far from the GAN
formulation used in practice where the discriminator and the generator are neural networks. On
the other hand, the idea of Hidden Convex-Concave (HCC) games was ﬁrst proposed by Flokas
et al. (2021) and Gidel et al. (2021). While Flokas et al. (2021) study the Gradient Descent-Ascent
(GDA) dynamics in the HCC setting, their work relies on hard-to-verify safety conditions. Our
global convergence results without hard-to-test assumptions on initialization are ﬁrst of their kind,
to the best of knowledge."
PRELIMINARIES,0.017094017094017096,"3
PRELIMINARIES"
PRELIMINARIES,0.018518518518518517,"Many game-theoretic applications in machine learning often involve a speciﬁc structure where the
models’ payoff is a convex-concave function (e.g., minimizing Jensen-Shannon-Divergence when
training GANs). To model this speciﬁc structure, we propose the following deﬁnition of Hidden
Convex-Concave (HCC) games where intuitively, the game’s payoff is a convex-concave function
whose actions are parametrized by non-convex mappings."
PRELIMINARIES,0.019943019943019943,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.021367521367521368,"Deﬁnition 1 (Hidden Convex-Concave game). A Hidden Convex-Concave game comprises a collec-
tion of payoff (Lx,x′)x,x′∈Rd, a distribution p, and two parametrized mappings, F : RM ×Rd →R,
and G : RN × Rd′ →R, such that the minimax game of interest is"
PRELIMINARIES,0.022792022792022793,"min
θ∈RM max
φ∈RN Ψ(θ, φ)
where
Ψ(θ, φ) = L(Fθ, Gφ) := E(x,x′)∼p[Lx,x′(Fθ(x), Gφ(x′))] . (1)"
PRELIMINARIES,0.024216524216524215,"In this setting, while we do not expect Ψ(θ, φ) to be convex-concave, we assume the function L :
F×G →R to be convex-concave where F and G are, respectively, convex subsets of {F : Rd →R}
and {G : Rp →R}.2 We extended Flokas et al. (2021)’s deﬁnition to now be able to include most
minimax machine learning applications such as GANs or AEG."
PRELIMINARIES,0.02564102564102564,"Example 1 (Hidden Matching Pennies (HMP) games). Let us consider p, q ∈[0, 1] the probabilities
of picking HEADS for the ﬁrst and the second player, respectively, in a Matching Pennies game with
payoff matrix A ∈R2×2, where Ai,j = 1 if i = j; −1, otherwise. Then the payoff of this game is
deﬁned via"
PRELIMINARIES,0.027065527065527065,"Lx,x′(p, q) := (1 −2p)(1 −2q)
if (p, q) ∈[0, 1]2
and
0 otherwise .
(2)"
PRELIMINARIES,0.02849002849002849,"Now, let us consider any mappings F : RM ×Rd →[0, 1] and G : RN ×Rd →[0, 1], such that their
output does not depend on the d-dimensional input, i.e., Fθ(x) = f(θ), and Gφ(x) = g(φ), ∀x ∈
Rd. The payoff Ψ(θ, φ) = L(Fθ, Gφ) := E[Lx,x′(Fθ(x), Gφ(x′))] = (1 −2f(θ))(1 −2g(φ))
deﬁnes a HCC game."
PRELIMINARIES,0.029914529914529916,"In this example, the two agents play the typical bilinear game of Matching Pennies. However, they
do not act on it directly (i.e., choose randomized actions to apply, e.g., the probability of playing
HEADS). Instead, they choose the input parameters θ, and φ, which are fed into functions f, and g,
respectively, whose outputs deﬁne the probability of playing HEADS for each agent."
PRELIMINARIES,0.03133903133903134,"Example 2 (GANs). A Generative Adversarial Network (GAN) is a minimax game where the ﬁrst
player, i.e., the generator, aims at learning a distribution pθ similar to a reference data distribution
pdata. In practice, the reference data distribution is taken to be the empirical data distribution.
Conversely, the second player, usually called the discriminator or critic, Dφ, tries to distinguish the
distributions of pθ and pdata. The payoff, Ψ, of this game is deﬁned as:"
PRELIMINARIES,0.03276353276353276,"Ψ(θ, φ) = Ex∼pdata[log Dφ(x)] + Ex∼pθ[log(1 −Dφ(x))] .
(3)"
PRELIMINARIES,0.03418803418803419,"Assuming that pdata and pθ have a density with respect to Lebesgue measure,3 and that the
support of pθ is included in the support of pdata, we can consider the distribution p such
that p(x, x′) = pdata(x) if x = x′, and 0 otherwise, and set Lx,x′(p′, D) := log D +
p′"
PRELIMINARIES,0.03561253561253561,"pdata(x′) log(1 −D). Thus Lx,x′ is convex-concave for any x, x′ ∈Rd. We have that Ψ(θ, φ) =
E(x,x′)∼p[Lx,x′(pθ(x), Dφ(x′))], which is a HCC game."
PRELIMINARIES,0.037037037037037035,"A GAN formulates the generative modeling task as ﬁnding a Nash equilibrium of a minimax game.
The generator of a GAN is deﬁned as a function that aims to produce realistic data samples by
transforming samples drawn from a ﬁxed noise distribution, e.g., N(0, Id). Here, we notice that the
GAN payoff is convex (actually linear) as a function of the density of the generated distribution. An
alternative formulation of a GAN is a Wasserstein GAN (WGAN) (Arjovsky et al., 2017). It turns
out that this GAN is also a HCC game."
PRELIMINARIES,0.038461538461538464,"Example 3 (WGANs). A Wasserstein GAN is a constrained minimax game, where the second player
Dφ is a 1-Lipchitz function and where the payoff is"
PRELIMINARIES,0.039886039886039885,"Ψ(θ, φ) = Ex∼pdata[Dφ(x)] −Ex∼pθ[Dφ(x)] .
(4)"
PRELIMINARIES,0.04131054131054131,"Similarly, as in Example 2, the WGAN payoff can be shown to be a HCC game."
PRELIMINARIES,0.042735042735042736,"As the last class of examples of HCC games, we present the Adversarial Example Games
(AEG) (Bose et al., 2020)."
PRELIMINARIES,0.04415954415954416,"2One can always assume F and G to be convex sets by considering their convex hulls.
3We make this assumption for simplicity. We can consider Radon–Nikodym derivatives for the general case."
PRELIMINARIES,0.045584045584045586,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.04700854700854701,"Example 4 (AEG). An Adversarial Example Game is a minimax game between a generator Gθ and
a classiﬁer f. Given samples (x, y) ∼pdata, the generator Gθ aims at ﬁnding adversarial examples
x′ such that ∥x −x′∥∞≤ϵ and that x′ is not classiﬁed as y by f, thus generating a distribution
pθ. Overall, the payoff of this game is"
PRELIMINARIES,0.04843304843304843,"Ψ(θ, φ) = −E(x′,y)∼pθ[ℓ(fφ(x′), y)] for (x′, y) ∼pθ ⇐⇒x′ = Gθ(x), (x, y) ∼pdata,
(5)"
PRELIMINARIES,0.04985754985754986,"and where ℓis the cross-entropy loss and Gθ is such that ∥x −Gθ(x)∥≤ϵ, ∀x. By using a similar
construction as in Example 2, we can show that (5) is a HCC game with respect to pθ and fφ."
PRELIMINARIES,0.05128205128205128,"In this work, we make the assumption that the minimax problem induced by L admits a solution:
Assumption 1. The HCC game deﬁned by (1) admits a Nash equilibrium, (F ∗, G∗), i.e.,"
PRELIMINARIES,0.05270655270655271,"min
F ∈F max
G∈G L(F, G) = max
G∈G min
F ∈F L(F, G) = L(F ∗, G∗) .
(6)"
PRELIMINARIES,0.05413105413105413,"Such an assumption is relatively mild since it holds when the set F is compact (by deﬁnition, it is
convex) (Sion et al., 1958). Note that this solution may not be achievable, i.e., we do not assume
that there exists θ∗and φ∗such that (Fθ∗, Gφ∗) = (F ∗, G∗). Such sufﬁcient conditions for the
existence of a Nash equilibrium of L : F×G →R are discussed in detail in Gidel et al. (2021, Prop.
1), e.g., Assumption 1 holds if the parameters θ and φ are bounded. From a high-level perspective,
this assumption is analogous to the existence of a global solution non-convex optimization. We use
(F ∗, G∗) as a target to build a Lyapunov function of the natural hidden gradient ﬂow."
NATURAL GRADIENT FLOW,0.05555555555555555,"3.1
NATURAL GRADIENT FLOW"
NATURAL GRADIENT FLOW,0.05698005698005698,"In this section, we present the notion of a natural gradient ﬂow (Amari, 1985; 1998). Let us consider
a function f : S →R and a class of symmetric positive deﬁnite matrices (Pθ)θ∈S ≻0 that we
will refer to as metric tensors. The natural gradient ﬂow is the ﬂow given by the steepest descent
direction (Ollivier et al., 2017) with respect to the geometry induced by the matrices Pθ,
˙θ = −P −1
θ ∇f(θ) .
(7)"
NATURAL GRADIENT FLOW,0.0584045584045584,"When Pθ = I, we consider the canonical Euclidean geometry and recover the standard gradient
ﬂow. One celebrated example of a natural gradient in machine learning is the natural gradient
induced by the Fisher information matrix (Amari, 1998; Martens, 2020).
Example 5 (The natural gradient ﬂow of the Fisher information matrix). Let us consider P(X) the
space of probability distributions on a set X ⊆R with the metric induced by the Kullback–Leibler
(KL) divergence. Then the natural gradient ﬂow of the Fisher information matrix is given by
˙θ = −F −1
θ ∇f(θ)
where
Fθ := −Ex∼pθ[∇2
θ log pθ(x)], pθ ∈P(X) .
(8)"
NATURAL GRADIENT FLOW,0.05982905982905983,"Moreover, if n := |X| = dim S is ﬁnite, and if pθ = θ, the ﬂow (8) is the natural gradient ﬂow of
the Shahshahani metric (Shahshahani, 1979) induced by the metric tensors"
NATURAL GRADIENT FLOW,0.06125356125356125,Sθ := diag( 1
NATURAL GRADIENT FLOW,0.06267806267806268,"θ1 , . . . , 1"
NATURAL GRADIENT FLOW,0.0641025641025641,"θn ) .
(9)"
NATURAL GRADIENT FLOW,0.06552706552706553,"Recently, alternative natural gradient formulations have been developed using the Wasserstein dis-
tance (Li & Mont´ufar, 2018; Arbel et al., 2020)."
CONNECTIONS BETWEEN REPLICATOR DYNAMICS AND GRADIENT FLOWS,0.06695156695156695,"3.2
CONNECTIONS BETWEEN REPLICATOR DYNAMICS AND GRADIENT FLOWS"
CONNECTIONS BETWEEN REPLICATOR DYNAMICS AND GRADIENT FLOWS,0.06837606837606838,"The Replicator Dynamics (RD) are standard dynamics used in Evolutionary Game Theory and learn-
ing in games. It is, arguably, the most widely used model of evolutionary selection with multiple
applications in economics, biology, and other ﬁelds. Interestingly, RD enjoys a close connection
to gradient ﬂows (see Sigmund (1984); Hofbauer et al. (1998); Harper (2009); Mertikopoulos &
Sandholm (2018)). Speciﬁcally, in the case of a symmetric and linear ﬁtness landscape, the gradi-
ent induced by the Shahshahani metric of the mean ﬁtness is a special case of RD. Such a land-
scape can be formally represented by a Potential game. A Potential game is a n-player game
G = (n, S := [m1] × . . . × [mn], u : S →Rn), with payoff function u, characterized by the
existence of a potential function Φ : S →R that satisﬁes"
CONNECTIONS BETWEEN REPLICATOR DYNAMICS AND GRADIENT FLOWS,0.0698005698005698,"Φ(si, s−i) −Φ(s′
i, s−i) = ui(si, s−i) −ui(s′
i, s−i)
∀i ∈[n] .
(10)"
CONNECTIONS BETWEEN REPLICATOR DYNAMICS AND GRADIENT FLOWS,0.07122507122507123,Published as a conference paper at ICLR 2022
CONNECTIONS BETWEEN REPLICATOR DYNAMICS AND GRADIENT FLOWS,0.07264957264957266,"The connection between potential games and the gradient ﬂows induced by the Shahshahani metric
can be formalized with a generalization of Hofbauer et al. (1998)’s lemma.
Proposition 1. The Replicator Dynamics of a potential game G with potential function Φ is an
(extended) Shahshahani gradient in int(∆m1 × . . .×∆mn) having potential Ψ(θ) := Esi∼θi
i∈[n]
[Φ(s)]."
CONNECTIONS BETWEEN REPLICATOR DYNAMICS AND GRADIENT FLOWS,0.07407407407407407,"It follows that, for potential games, RD is a gradient ﬂow with respect to a very speciﬁc geometry.
In the next section, we consider a geometry induced by the HCC structure, and which we leverage
to obtain a new natural gradient ﬂow that we call Natural Hidden Gradient ﬂow (NGH)."
THE NATURAL HIDDEN GRADIENT FLOW FOR HCC GAMES,0.0754985754985755,"3.3
THE NATURAL HIDDEN GRADIENT FLOW FOR HCC GAMES"
THE NATURAL HIDDEN GRADIENT FLOW FOR HCC GAMES,0.07692307692307693,"In HCC games, we assume Ψ(θ, φ) = L(Fθ, Gφ) (see Deﬁnition 1), and, therefore, since the
entities that characterize a solution to the minimax problem are the mappings Fθ and Gφ instead of
their parametrizations, θ and φ, respectively, a natural geometry for consideration is the one deﬁned
by the L2 distance in the space of F × G. Formally, let θ and θ′ be two parameterizations for F
(similarly, for G). The L2 distance between the mappings are:"
THE NATURAL HIDDEN GRADIENT FLOW FOR HCC GAMES,0.07834757834757834,"∥Fθ′ −Fθ∥2 := Ex∼px(Fθ′(x) −Fθ(x))2
and
∥Gφ −Gφ′∥2 := Ex∼px′(Gφ′(x′) −Gφ(x′))2."
THE NATURAL HIDDEN GRADIENT FLOW FOR HCC GAMES,0.07977207977207977,"where px and px′ are the marginal of px,x′. We can then derive the metric tensors of this geometry.
Proposition 2 (Metric tensors of the model space). Under mild regularity assumptions, we have
that, for any θ ∈RM,"
THE NATURAL HIDDEN GRADIENT FLOW FOR HCC GAMES,0.0811965811965812,"∥Fθ+δθ −Fθ∥2 = ⟨δθ, Aθδθ⟩+ o(∥δθ∥2)
where
Aθ := Ex∼px[∇θFθ(x)∇θFθ(x)⊺] . (11)"
THE NATURAL HIDDEN GRADIENT FLOW FOR HCC GAMES,0.08262108262108261,"Consequently, Aθ deﬁnes a metric on the parameter space in which the “distance” between two
values, θ, and θ′, corresponds to the L2 distance between Fθ and Fθ′. We can, thus, construct the
corresponding natural gradient ﬂow.
Proposition 3 (Natural Hidden Gradient dynamics). The ﬂow induced by the geometry (11) is"
THE NATURAL HIDDEN GRADIENT FLOW FOR HCC GAMES,0.08404558404558404,"˙θ = −A†
θE(x,x′)∼p[∇θLx,x′(Fθ(x), Gφ(x′))]
˙φ = B†
φE(x,x′)∼p[∇φLx,x′(Fθ(x), Gφ(x′))]
(D1)"
THE NATURAL HIDDEN GRADIENT FLOW FOR HCC GAMES,0.08547008547008547,"where Aθ := Ex∼px[∇θFθ(x)∇θFθ(x)⊺] and Bφ := Ex′∼px′[∇φGφ(x)∇φGφ(x)⊺] and C†
denotes the pseudo-inverse of a matrix C."
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.0868945868945869,"4
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW"
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.08831908831908832,"In this section, we propose a new type of dynamics, dubbed Natural Hidden Gradient dynamics
(NHG), and in the following two theorems, we prove their convergence in HCC games and GANs.
At the heart of our analysis lies the construction of a proper Lyapunov function that measures the
distance from the game’s equilibrium point. By proving that the Lyapunov function is proper, i.e.,
monotonic, we will prove our proposed dynamics are approaching a Nash Equilibrium."
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.08974358974358974,"4.1
WARM-UP: A SINGLE DATAPOINT"
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.09116809116809117,"In this section, as a warm-up, we will consider the single datapoint case. In this case, (1) is,"
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.09259259259259259,"Ψ(θ, φ) = L(Fθ(x), Gφ(x′))
(12)"
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.09401709401709402,"where L : R × R →R is convex-concave (see Example 1). In this case, since the mapping Fθ and
Gφ are evaluated at a single point, one can simplify the notation and consider Fθ(x) = f(θ) and
Gφ(x′) = g(φ) where f : RM →R and g : RN →R are real-valued mappings that do not depend
on an input x or x′. This situation is already non-trivial since, as illustrated in Example 1, it can
correspond to a non-convex non-concave parametrization of the Matching Pennies game. In order
to solve this game we consider the Natural gradient of the metric deﬁned in Proposition 2.
Proposition 4. In the uni-dimensional case, the Natural Hidden Gradient ﬂow D1 takes the form of"
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.09544159544159544,"˙θ = −∇θL(f(θ),g(φ))"
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.09686609686609686,"∥∇θf(θ)∥2
and
˙φ = ∇φL(f(θ),g(φ))"
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.09829059829059829,"∥∇φg(φ)∥2
.
(D2)"
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.09971509971509972,Published as a conference paper at ICLR 2022
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.10113960113960115,"Using Proposition 4, it is relatively straightforward to show that the distance to the optimum is a
Lyapunov function for the Natural Hidden Gradient ﬂow.
Theorem 1. Let Ψ be the payoff of an HCC game (12) and consider the dynamics (D2). Then,"
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.10256410256410256,"V (θ, φ) := 1"
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.10398860398860399,2(f(θ) −f ∗)2 + 1
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.10541310541310542,"2(g(φ) −g∗)2
(13)"
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.10683760683760683,"is a Lyapunov function, i.e., it is positive, non-increasing, and null if and only if it evaluated at a
game solution. Moreover, if L is strictly convex-concave, we have that V is decreasing and that any
limit point (θ, φ) satisﬁes ∇θL(f(θ), g(φ)) = ∇φL(f(θ), g(φ)) = 0."
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.10826210826210826,"In order to see this, one can compute the time derivative of V . After some elementary computations,
it follows ˙V (θ, φ) = −(f(θ) −f ∗) ∂L(f,g(φ))"
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.10968660968660969,"∂f

f=f(θ) + (g(φ) −g∗) ∂L(f(θ),g)"
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.1111111111111111,"∂g

g=g(φ), and, thus,"
CONVERGENCE OF THE NATURAL HIDDEN GRADIENT FLOW,0.11253561253561253,"by the convex-concavity of L, we have that ˙V ≤0. However, generalizing this theorem to HCC
games with non-constant mappings (with respect to x) is non-trivial since we drastically used the
simplicity of the mappings (Proposition 4) to simplify the expression of the ﬂow. In the next section,
we propose to extend our convergence analysis to ﬁnite sum HCC games."
THE GENERAL FINITE SUM CASE,0.11396011396011396,"4.2
THE GENERAL FINITE SUM CASE"
THE GENERAL FINITE SUM CASE,0.11538461538461539,"In this section, we consider a ﬁnite-sum version of the HCC games as they appear in Deﬁnition 1.
In this case, the payoff is deﬁned as"
THE GENERAL FINITE SUM CASE,0.1168091168091168,"Ψ(θ, φ) :=
1
nm
X"
THE GENERAL FINITE SUM CASE,0.11823361823361823,"(i,j)∈[m]×[b]
Li,j(Fθ(xi), Gφ(x′
j)) =: L((Fθ(xi))i∈[m], (Gφ(x′
j))j∈[n]) ,
(14)"
THE GENERAL FINITE SUM CASE,0.11965811965811966,"where the function L : Rm × Rn →R is assumed to be convex-concave. We note Li,j := Lxi,x′
j
for compactness. Let us recall that we assumed the existence of a Nash equilibrium (F ∗, G∗) for the
minimax problem (Assumption 1). In this situation, the Natural gradient deﬁned in Proposition 3
has the following form:"
THE GENERAL FINITE SUM CASE,0.12108262108262108,"˙θ = −A†
θ
nm
X"
THE GENERAL FINITE SUM CASE,0.1225071225071225,"(i,j)∈[m]×[n]
∇θLi,j(Fθ(xi), Gφ(x′
j)), ˙φ =
B†
φ
nm
X"
THE GENERAL FINITE SUM CASE,0.12393162393162394,"(i,j)∈[m]×[n]
∇φLi,j(Fθ(xi), Gφ(x′
j)) ,
(D3)"
THE GENERAL FINITE SUM CASE,0.12535612535612536,where Aθ := 1
THE GENERAL FINITE SUM CASE,0.1267806267806268,"m
Pm
i=1 ∇θFθ(xi)∇θFθ(xi)⊺and Bφ := 1"
THE GENERAL FINITE SUM CASE,0.1282051282051282,"n
Pn
j=1 ∇φGφ(x′
j)∇φGφ(x′
j)⊺."
THE GENERAL FINITE SUM CASE,0.12962962962962962,"We will generalize the Lyapunov function considered in the uni-dimensional case (13). The idea is
to consider the L2 distance between (Fθ(xi), Gφ(x′
j))i,j and (F ∗(xi), G∗(x′
j))i,j as our Lyapunov
function V . However, in order to prove our result, we will need the following technical assumption.
Assumption 2. For any θ
∈
RM and φ
∈
RN, we have that the families of vectors
(∇θFθ(xi))i∈[m] and (∇φGφ(x′
j))j∈[n] are linearly independent."
THE GENERAL FINITE SUM CASE,0.13105413105413105,"When the models are overparametrized, e.g., M > m and N > n, this assumption is relatively mild
since it can be insured by a small perturbation of the considered vectors. In practice, it suggests
regularizing the matrices Aθ and Bφ by adding ϵ · Id which is a standard way to stabilize methods
requiring matrix inversions.
Theorem 2. Let Ψ be the payoff of a ﬁnite-sum HCC game given by (14) and consider the game
dynamics in (D3). Under Assumption 1, Assumption 2, we have that the quantity"
THE GENERAL FINITE SUM CASE,0.13247863247863248,"V (θ, φ) := 1"
N,0.1339031339031339,"2n n
X"
N,0.13532763532763534,"i=1
(Fθ(xi) −F ∗(xi))2 + 1"
M,0.13675213675213677,"2m m
X"
M,0.13817663817663817,"j=1
(Gφ(x′
j) −G∗(x′
j))2
(15)"
M,0.1396011396011396,"is a Lyapunov function, i.e., is positive, non-increasing and null if and only if evaluated at a game
solution. Moreover, if L is strictly convex-concave, V is decreasing as long as (θ, φ) ̸= (θ∗, φ∗)
and if L is a µ-strongly convex-concave function we have that V is decreasing exponentially as
V (θ, φ) = V (θ0, φ0) exp(−µt)."
M,0.14102564102564102,"We showed that, in the overparametrized regime, if we assume not to encounter any singular matrices
Aθ and Bφ along the trajectory, then, preconditioning low dimensional gradient of θ and φ can
behave like doing gradient update on F and G to leverage the hidden-convex-cave structure of"
M,0.14245014245014245,Published as a conference paper at ICLR 2022
M,0.14387464387464388,"the the payoff. We do not know how to recover the gradients updates on F and G in the non-
overparametrized regime. It is a great open question that we consider outside of the scope of this
paper as we focus on understanding convergence in minimax games for deep learning models (that
are over-parametrized). The proofs of Theorem 1 and Theorem 2 are in §A.3."
M,0.1452991452991453,"Regarding the practicability of the method described in (D3), efﬁcient approximations of precondi-
tioning, such as the K-FAC algorithm, were proposed (Martens, 2020; Li & Mont´ufar, 2018) and
used to train large models on Imagenet and CIFAR (Martens et al., 2021; Arbel et al., 2020)."
CHARACTERIZATIONS OF REPLICATOR DYNAMICS IN HCC GAMES,0.1467236467236467,"5
CHARACTERIZATIONS OF REPLICATOR DYNAMICS IN HCC GAMES"
CHARACTERIZATIONS OF REPLICATOR DYNAMICS IN HCC GAMES,0.14814814814814814,"In this section, we consider a speciﬁc instance of HMP games (cf. Example 1), dubbed 2-Team HMP
games or the XOR-XOR games. Although the possibility of cycling orbits for RD in such games was
established before (Piliouras & Schulman, 2018), in this section, we show stronger results. Speciﬁ-
cally, as we prove in subsection 5.2, by enforcing restrictions to the game’s parameters, it is possible
to affect the game’s outcome, e.g., we may deviate from the well-known cyclic behavior in the un-
restricted setting, and moreover, enforce divergence away from the game’s original equilibrium and
convergence to novel ﬁxed points. We completely characterize the geometry of possible limit cycles
in such a restricted setting by exploiting intuitions developed via the connection between RD and the
Shahshahani information geometry. Speciﬁcally, we show the dynamics are controlled by invariant
functions, which correspond to weighted sums of the cross-entropy of the current mixed strategies
of opposing members relative to the uniform, equilibrium strategies (19). For the complete proofs
of this section, we refer the interested reader to §A.4."
CHARACTERIZATIONS OF REPLICATOR DYNAMICS IN HCC GAMES,0.14957264957264957,"5.1
2-TEAM HIDDEN MATCHING PENNIES GAMES"
CHARACTERIZATIONS OF REPLICATOR DYNAMICS IN HCC GAMES,0.150997150997151,"We introduce the 2-Team HMP game, G = (n := n1 + n2, S := {0, 1}n, u : S →Rn), between
n1 + n2 members divided into two teams. The ﬁrst team, team 1, consists of n1 members, while
the second team, team 2, consists of n2 members. The payoff function u for a strategy proﬁle
s := (s1, s2) ∈S is given by"
CHARACTERIZATIONS OF REPLICATOR DYNAMICS IN HCC GAMES,0.15242165242165243,"uk,i(s) := (−1)k−1"
CHARACTERIZATIONS OF REPLICATOR DYNAMICS IN HCC GAMES,0.15384615384615385,"nk
(1 −2 · 1XOR(s1)=XOR(s2)), k ∈[2], i ∈[nk] .
(16)"
CHARACTERIZATIONS OF REPLICATOR DYNAMICS IN HCC GAMES,0.15527065527065528,"where XOR(sk) = 1 if |{sk,i | sk,i = 1}| is odd, and 0, otherwise. Given a mixed-strategy proﬁle,
(θ, 1 −θ), θ := (θk)k∈[2] ∈[0, 1]n, the expected payoff of the i-th member of team k is given by"
CHARACTERIZATIONS OF REPLICATOR DYNAMICS IN HCC GAMES,0.15669515669515668,"Ψk,i(θ) := Esk,i∼Ber(θk,i)
k∈[2], i∈[nk]
[uk,i(s)] = (−1)k−1"
CHARACTERIZATIONS OF REPLICATOR DYNAMICS IN HCC GAMES,0.1581196581196581,"nk
(1 −2f(θ1))(1 −2g(θ2))
(17)"
CHARACTERIZATIONS OF REPLICATOR DYNAMICS IN HCC GAMES,0.15954415954415954,"where f(θ1) := Es1∼Ber(θ1)[XOR(s1)] and g(θ2) := Es2∼Ber(θ2)[XOR(s2)]. Notice that, since
each member of a given team aims at maximizing the same payoff, G is a Hidden Matching Pennies
game (Example 1) with hidden mappings f(θ1), and g(θ2),"
CHARACTERIZATIONS OF REPLICATOR DYNAMICS IN HCC GAMES,0.16096866096866097,"min
θ2 max
θ1 Ψ(θ1, θ2) := n1
X"
CHARACTERIZATIONS OF REPLICATOR DYNAMICS IN HCC GAMES,0.1623931623931624,"i=1
Ψ1,i(θ) = (1 −2f(θ1))(1 −2g(θ2)) .
(18)"
CHARACTERIZATIONS OF REPLICATOR DYNAMICS IN HCC GAMES,0.16381766381766383,"It is not difﬁcult to prove that the RD exhibit cyclic behavior in this setting. The following theorem
provides a ﬁne-grained characterization of the dynamics of the HCC game (18) where we show that
the trajectories lie on the intersection of level sets of invariant functions.
Theorem 3. Consider the Replicator Dynamics of G. Given any interior initial condition, the
resulting orbit is a cycle that satisﬁes the following n1 + n2 −1 independent invariant functions:"
CHARACTERIZATIONS OF REPLICATOR DYNAMICS IN HCC GAMES,0.16524216524216523,"Vi1,i2(θ) ="
X,0.16666666666666666,"2
X"
X,0.16809116809116809,"k=1
nk[log(θk,ik) + log(1 −θk,ik)], ik ∈[nk], k ∈[2] .
(19)"
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.16951566951566951,"5.2
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING"
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.17094017094017094,"Next, we introduce restrictions in the range of each member’s strategies in G such that θk,i ∈Sk,i :=
[αk,i, βk,i] ⊆[0, 1], ∀k ∈[2], i ∈[nk]. These restrictions reﬂect in RD as halts, i.e., ˙θk,i = 0, every"
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.17236467236467237,Published as a conference paper at ICLR 2022
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.1737891737891738,"time the strategy of the i-th member of the k-th team exceeds those bounds. To ease our notation, we
let Sk := {i ∈[nk] | 1"
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.1752136752136752,"2 ∈Sk,i}, k ∈[2] denote all the members of the k-th team for which ( 1 2, 1"
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.17663817663817663,"2)
is an allowed mixed strategy, and, to simplify this part of the analysis, we also make the following
mild assumption on the initialization of the dynamics:"
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.17806267806267806,"Assumption 3. For all k ∈[2] and i ∈[nk], θk,i(0) ∈Sk,i ⊂(0, 1)."
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.1794871794871795,"A signiﬁcant observation is that for any mixed-strategy proﬁle (θ, 1 −θ), θ := (θ1, θ2) ∈[0, 1]n,
if there exists some ik ∈[nk], ∀k ∈[2] such that θik = 1"
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.18091168091168092,"2, then θ is an equilibrium point; in fact,
it is not difﬁcult to see that these are the only interior equilibrium points of G, which implies that
an interior equilibrium point is reachable if and only if Sk ̸= ∅, ∀k ∈[2]. In our ﬁrst result, we
prove that if the Nash Equilibrium of the unrestricted case is not reachable by both teams due to the
constraints, then the dynamics converges to a point which only depends on those restrictions."
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.18233618233618235,"Theorem 4. Under Assumption 3, if Sk = ∅, ∀k ∈[2], the restricted RD of G converge to"
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.18376068376068377,"θ∗
1,i =

α1,i,
if β1,i < 1"
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.18518518518518517,"2
β1,i,
otherwise
i ∈[n1]"
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.1866096866096866,"θ∗
2,i =

β2,i,
if β2,i < 1"
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.18803418803418803,"2
α2,i,
otherwise
i ∈[n2]
|
{z
}
if |S| is even"
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.18945868945868946,"or
θ∗
1,i =

β1,i,
if β1,i < 1"
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.1908831908831909,"2
α1,i,
otherwise
i ∈[n1]"
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.19230769230769232,"θ∗
2,i =

α2,i,
if β2,i < 1"
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.19373219373219372,"2
β2,i,
otherwise
i ∈[n2]
|
{z
}
if |S| is odd (20)"
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.19515669515669515,"where S := {(k, i) | k ∈[2], i ∈[nk], αk,i > 1 2}."
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.19658119658119658,"The key idea behind this result is that at any given time t ≥0, the direction of a strategy θk,i(t) only
depends on whether |S|, the total number of members who can access the uniform strategy, is odd or
even. Thus, we can decouple the evolution of the dynamics of each of the members and analyze its
behavior separately. If an equilibrium point of the unrestricted setting is reachable by both teams,
i.e., Sk ̸= ∅, ∀k ∈[2], then the dynamics converge to an invariant set whose degrees of freedom
depend on the number of members who have access to the uniform strategy, ( 1 2, 1 2)."
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.198005698005698,"Theorem 5. Under Assumption 3, if Sk ̸= ∅, ∀k ∈[2], then the restricted RD of G converge to an
invariant set deﬁned by the |S1|+|S2|−1 independent invariant functions, Vi1,i2(θ), ik ∈Sk, ∀k ∈
[2], where Vi1,i2(θ) is given as in (19)."
REPLICATOR DYNAMICS OF HIDDEN MATCHING PENNIES IN A RESTRICTED SETTING,0.19943019943019943,"One can prove this result by partitioning the time based on the set of members that have halted. The
analysis of Vi1,i2(θ) in each time-partition is almost trivial, and the result follows by a continuity
argument on Vi1,i2(θ). These results depict how the behavior of RD depends on the parameter space
of the game. Notably, we show that if no equilibrium point is feasible, the RD converge to a point
described entirely by the strategy space restrictions. On the other hand, if an equilibrium point is
feasible, we prove the existence of a maximal number of invariant functions, with close connections
to the KL divergence. The latter does not merely show that the RD cycle in this setting, but that they
actually converge to an invariant set with speciﬁc degrees of freedom, and which we characterize.
For parameterizations that visualize the behavior of RD in such restricted settings, see §B.1."
EXPERIMENTAL RESULTS,0.20085470085470086,"6
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.2022792022792023,"Toy multi-dimensional case. As a ﬁrst experiment, we consider the HCC objective"
EXPERIMENTAL RESULTS,0.2037037037037037,"Ψ(θ, φ) = L(Fθ, Gφ) := F ⊺
θ MGφ + λ"
EXPERIMENTAL RESULTS,0.20512820512820512,2 (∥Fθ −1
EXPERIMENTAL RESULTS,0.20655270655270655,3∥2 −∥Gφ −1
EXPERIMENTAL RESULTS,0.20797720797720798,"3∥2) ,"
EXPERIMENTAL RESULTS,0.2094017094017094,"where M is the payoff matrix of the Rock-Paper-Scissors game (see e.g. Gidel et al. (2021)). We
note Fθ(xi) = [Fθ]i, i ∈[3], and 1"
EXPERIMENTAL RESULTS,0.21082621082621084,"3, i.e., the uniform distribution, is the game’s equilibrium. The
mappings F and G are 2-layer MLP with 130 parameters and GELU non-linearities (Hendrycks &
Gimpel, 2016). In Figure 1, we depict a comparison between the performance of GDA and NHG
dynamics on the task of solving minθ maxφ Ψ(θ, φ). We remark, the NHG dynamics converges
smoothly (Figure 1 (Right)), compared to GDA (Figure 1 (Center)), which fail to converge. The
value of the Lyapunov function of the game described is depicted in (Figure 1 (Left))."
EXPERIMENTAL RESULTS,0.21225071225071226,"GANs. For our second experiment, we implement the NHG dynamics to train a GAN. We consider
a synthetic experiment to learn a sine wave sampled uniformly from 0 to π with 1024 observations."
EXPERIMENTAL RESULTS,0.21367521367521367,Published as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.2150997150997151,"0
500
1000
1500
2000
Number of Iterations 10−3 10−2 10−1"
EXPERIMENTAL RESULTS,0.21652421652421652,Lyapunov value
EXPERIMENTAL RESULTS,0.21794871794871795,"Natural Gradient
Standard Gradient
0.0
0.1
0.2
0.3
0.4 0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.0 0.1 0.2 0.3 0.4 0.5"
EXPERIMENTAL RESULTS,0.21937321937321938,"p
q
Solution"
EXPERIMENTAL RESULTS,0.2207977207977208,"0.0
0.1
0.2
0.3
0.0 0.1 0.2 0.3"
EXPERIMENTAL RESULTS,0.2222222222222222,"0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35"
EXPERIMENTAL RESULTS,0.22364672364672364,"p
q
Solution"
EXPERIMENTAL RESULTS,0.22507122507122507,"Figure 1:
A comparison of GDA (Center) and NHG dynamics (Right) in the task of solving
minθ maxφ Ψ(θ, φ) where p = f(θ), and q = g(φ). In NHG, the Lyapunov function (Left) is
monotonically decreasing, as opposed to GDA."
EXPERIMENTAL RESULTS,0.2264957264957265,"Regarding the GAN, we consider a Flow-GAN architecture where our generator, G, is a Real NVP
consisting of 8 coupling layers (Dinh et al., 2017; 2014), and the discriminator is a 4-layer MLP
with 256-128-64-1 output features. We adapt K-FAC (Martens & Grosse, 2015) as an approximator
to compute NHG, with a learning rate of 10−4, selected using the standard parameter optimizer
package Optuna (Akiba et al., 2019). We used gradient clipping with gradient clip to a maximum
norm of 1 on, both the discriminator and the generator, to stabilize the optimization."
EXPERIMENTAL RESULTS,0.22792022792022792,"As observed in (Figure 2 (Center)), by iteration 200, the GAN optimized using K-FAC learns the
true sinusoid almost perfectly and converges to a better Wasserstein-1 score than the conventional
GDA (Figure 2 (Right)). The experiments reveal that our approach provides good performance and
convergence guarantees. However, we observe instabilities during training, and lack of convergence
in certain instances, which stay in line with the empirical observations regarding the difﬁculty of
GAN training, and the instability of matrix inversions close to singularities. We remark that this ex-
periment goes slightly beyond the theoretical results: while in theory, we assume a natural gradient
ﬂow on a “full-batch”, our experiments are based on discrete stochastic updates, and an approxima-
tion of the pseudo-inverses of the matrices (see (D1)) using K-FAC. Thus, this experiment acts as a
proof-of-concept rather than a large-scale comparison between NHG and GDA. The details of both
experiments can be found within the source-code ﬁles included with this work."
EXPERIMENTAL RESULTS,0.22934472934472935,"0
1
2
3
4
5
6 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00"
EXPERIMENTAL RESULTS,0.23076923076923078,"0
1
2
3
4
5
6 1.0 0.5 0.0 0.5 1.0"
EXPERIMENTAL RESULTS,0.23219373219373218,"0
25
50
75
100
125
150
175
200
Number of Iterations 0.0 0.5 1.0 1.5 2.0 2.5 3.0"
EXPERIMENTAL RESULTS,0.2336182336182336,Wasserstein-1
EXPERIMENTAL RESULTS,0.23504273504273504,"Natural Gradient
Standard Gradient"
EXPERIMENTAL RESULTS,0.23646723646723647,"Figure 2: A plot of the real sinusoid sampled on the interval from 0 to 2π (Left). GAN generated
samples for NHG dynamics (Center). The performance of GDA vs. NHG dynamics on GANs as
measured by Wasserstein-1 distance (Right)."
DISCUSSION,0.2378917378917379,"7
DISCUSSION"
DISCUSSION,0.23931623931623933,"We proposed a novel version of Gradient Descent Ascent dynamics in Hidden Convex-Concave
games, a subset of non-convex non-concave games. In this class of games, which includes GANs, the
utility is convex-concave in the function space. Still, training happens in the parameter space, where
the mappings between the input and output are non-convex non-concave functions. We explored
the dynamics of gradient ﬂows induced by different Banach spaces (e.g., the Fischer information
geometry) that led to the discovery of Lyapunov functions suited to these geometries. Our analysis
of the convergence of our proposed type of dynamics, Natural Hidden Gradient (NHG) dynamics,
uses ideas from Game Theory and Dynamical Systems. We proved global convergence guarantees
for NHG in HCC games to local stationary points via Lyapunov function analysis in the ﬁnite-sum
case. To the best of our knowledge, such a non-local convergence result in HCC games and GAN-
like settings, is one of the ﬁrst of its kind. We also show promising experimental results on practical
GANs using NHG and standard gradient approximation techniques such as KFAC. We are aware that
the current formulation of NHG may be challenging to scale up to large neural networks because
of the tensor pseudo-inversion step that is part of the dynamics. Investigating experimentally novel
versions of Gradient Descent Ascent dynamics with a richer set of experiments and developing
techniques to scale our results for larger neural networks is a natural direction for our future work."
DISCUSSION,0.24074074074074073,Published as a conference paper at ICLR 2022
DISCUSSION,0.24216524216524216,STATEMENT OF REPRODUCIBILITY
DISCUSSION,0.24358974358974358,"We made sure to provide sufﬁcient details to ensure the reproducibility of our results. The complete
proofs of the theoretical results can be found in §A, and all the assumptions have been stated and
are referenced in each statement. We provide details regarding the experimental results, such as
code language, required libraries, and parametrization to execute and reproduce the experiments in
section 6 and §B. We also include the source code ﬁles and the necessary input ﬁles in the supple-
mentary material that accompanies this work."
DISCUSSION,0.245014245014245,ACKNOWLEDGEMENTS
DISCUSSION,0.24643874643874644,The authors would like to acknowledge Joey Bose for his help on the GANs experiments.
DISCUSSION,0.24786324786324787,"This research/project is supported in part by the National Research Foundation, Singapore under
its AI Singapore Program (AISG Award No: AISG2-RP-2020-016), NRF 2018 Fellowship NRF-
NRFF2018-07, NRF2019-NRF-ANR095 ALIAS grant, grant PIE-SGP-AI-2020-01, AME Pro-
grammatic Fund (Grant No. A20H6b0151) from the Agency for Science, Technology and Research
(A*STAR) and Provost’s Chair Professorship grant RGEPPV2101. This work is supported by the
Canada CIFAR AI Chair Program and an IVADO grant."
REFERENCES,0.2492877492877493,REFERENCES
REFERENCES,0.25071225071225073,"Leonard Adolphs, Hadi Daneshmand, Aurelien Lucchi, and Thomas Hofmann. Local saddle point
optimization: A curvature exploitation approach. In The 22nd International Conference on Arti-
ﬁcial Intelligence and Statistics. PMLR, 2019."
REFERENCES,0.25213675213675213,"Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna:
A next-generation hyperparameter optimization framework. In Proceedings of the 25rd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, 2019."
REFERENCES,0.2535612535612536,"Shun-ichi Amari. Differential-geometrical methods in statistics. Lecture Notes on Statistics, 28,
1985."
REFERENCES,0.254985754985755,"Shun-Ichi Amari. Natural gradient works efﬁciently in learning. Neural computation, 10(2), 1998."
REFERENCES,0.2564102564102564,"M Arbel, A Gretton, W Li, and G Montufar. Kernelized wasserstein natural gradient. In Interna-
tional Conference on Learning Representations, 2020."
REFERENCES,0.25783475783475784,"Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning. PMLR, 2017."
REFERENCES,0.25925925925925924,"Avishek Joey Bose, Gauthier Gidel, Hugo Berrard, Andre Cianﬂone, Pascal Vincent, Simon
Lacoste-Julien, and William L Hamilton. Adversarial example games. In NeurIPS, 2020."
REFERENCES,0.2606837606837607,"Haoyang Cao and Xin Guo. Approximation and convergence of GANs training: an SDE approach.
arXiv preprint arXiv:2006.02047, 2020."
REFERENCES,0.2621082621082621,"Yun Kuen Cheung and Georgios Piliouras. Vortices instead of equilibria in minmax optimization:
Chaos and butterﬂy effects of online learning in zero-sum games. In Conference on Learning
Theory. PMLR, 2019."
REFERENCES,0.26353276353276356,"Yun Kuen Cheung and Yixin Tao. Chaos of learning beyond zero-sum and coordination via game
decompositions. In ICLR, 2021."
REFERENCES,0.26495726495726496,"Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in
min-max optimization. In NeurIPS, 2018."
REFERENCES,0.26638176638176636,"Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of constrained
min-max optimization. STOC, 2021."
REFERENCES,0.2678062678062678,"Jelena Diakonikolas, Constantinos Daskalakis, and Michael Jordan. Efﬁcient methods for structured
nonconvex-nonconcave min-max optimization. In ICML, 2021."
REFERENCES,0.2692307692307692,Published as a conference paper at ICLR 2022
REFERENCES,0.2706552706552707,"Nishanth Dikkala, Greg Lewis, Lester Mackey, and Vasilis Syrgkanis. Minimax estimation of con-
ditional moment models. In NeurIPS, 2020."
REFERENCES,0.2720797720797721,"Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components esti-
mation. arXiv preprint arXiv:1410.8516, 2014."
REFERENCES,0.27350427350427353,"Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In
ICLR, 2017."
REFERENCES,0.27492877492877493,"Tanner Fiez, Benjamin Chasnov, and Lillian Ratliff.
Implicit learning dynamics in stackelberg
games: Equilibria characterization, convergence analysis, and empirical study. In International
Conference on Machine Learning. PMLR, 2020."
REFERENCES,0.27635327635327633,"Lampros Flokas, Emmanouil-Vasileios Vlatakis-Gkaragkounis, and Georgios Piliouras. Poincar´e
recurrence, cycles and spurious equilibria in gradient-descent-ascent for non-convex non-concave
zero-sum games. In NeurIPS, 2020."
REFERENCES,0.2777777777777778,"Lampros Flokas, Emmanouil-Vasileios Vlatakis-Gkaragkounis, and Georgios Piliouras. Solving
min-max optimization with hidden structure via gradient descent ascent. In NeurIPS, 2021."
REFERENCES,0.2792022792022792,"Ian Gemp and Sridhar Mahadevan. Global convergence to the equilibrium of GANs using variational
inequalities. arXiv preprint arXiv:1808.01531, 2018."
REFERENCES,0.28062678062678065,"Gauthier Gidel, David Balduzzi, Wojciech Marian Czarnecki, Marta Garnelo, and Yoram Bachrach.
Minimax theorem for latent games or: How i learned to stop worrying about mixed-nash and love
neural nets. AISTATS, 2021."
REFERENCES,0.28205128205128205,"I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, and Y Ben-
gio. Generative adversarial nets. In NeurIPS, 2014."
REFERENCES,0.28347578347578345,"Marc Harper. Information geometry and evolutionary game theory. CoRR, abs/0911.1383, 2009.
URL http://arxiv.org/abs/0911.1383."
REFERENCES,0.2849002849002849,"Dan Hendrycks and Kevin Gimpel.
Gaussian error linear units (GELUs).
arXiv preprint
arXiv:1606.08415, 2016."
REFERENCES,0.2863247863247863,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 30,
2017."
REFERENCES,0.28774928774928776,"Josef Hofbauer, Karl Sigmund, et al. Evolutionary games and population dynamics. Cambridge
university press, 1998."
REFERENCES,0.28917378917378916,"Ya-Ping Hsieh, Chen Liu, and Volkan Cevher. Finding mixed Nash equilibria of generative adver-
sarial networks. In ICML, 2019."
REFERENCES,0.2905982905982906,"Ya-Ping Hsieh, Panayotis Mertikopoulos, and Volkan Cevher. The limits of min-max optimization
algorithms: convergence to spurious non-critical sets. CoRR, 2020."
REFERENCES,0.292022792022792,"Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvex-nonconcave
minimax optimization? In International Conference on Machine Learning. PMLR, 2020."
REFERENCES,0.2934472934472934,"Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of
GANs. arXiv preprint arXiv:1705.07215, 2017."
REFERENCES,0.2948717948717949,"Weiwei Kong and Renato DC Monteiro. An accelerated inexact proximal point method for solving
nonconvex-concave min-max problems. SIAM Journal on Optimization, 2021."
REFERENCES,0.2962962962962963,"Sucheol Lee and Donghwan Kim. Fast extra gradient methods for smooth structured nonconvex-
nonconcave minimax problems. In NeurIPS, 2021."
REFERENCES,0.29772079772079774,"Alistair Letcher. On the impossibility of global convergence in multi-loss optimization. In ICLR,
2021."
REFERENCES,0.29914529914529914,Published as a conference paper at ICLR 2022
REFERENCES,0.3005698005698006,"Jerry Li, Aleksander Madry, John Peebles, and Ludwig Schmidt. On the limitations of ﬁrst-order
approximation in gan dynamics. In International Conference on Machine Learning. PMLR, 2018."
REFERENCES,0.301994301994302,"Wuchen Li and Guido Mont´ufar. Natural gradient via optimal transport. Information Geometry, 1
(2):181–214, 2018."
REFERENCES,0.3034188034188034,"Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave mini-
max problems. In ICML, 2020."
REFERENCES,0.30484330484330485,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR, 2018."
REFERENCES,0.30626780626780625,"James Martens. New insights and perspectives on the natural gradient method. Journal of Machine
Learning Research, 2020."
REFERENCES,0.3076923076923077,"James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, 2015."
REFERENCES,0.3091168091168091,"James Martens, Andy Ballard, Guillaume Desjardins, Grzegorz Swirszcz, Valentin Dalibard, Jascha
Sohl-Dickstein, and Samuel S Schoenholz. Rapid training of deep neural networks without skip
connections or normalization layers using deep kernel shaping. arXiv preprint arXiv:2110.01765,
2021."
REFERENCES,0.31054131054131057,"Eric Mazumdar and Lillian J Ratliff. Local nash equilibria are isolated, strict local nash equilibria in
‘almost all’zero-sum continuous games. In 2019 IEEE 58th Conference on Decision and Control
(CDC). IEEE, 2019."
REFERENCES,0.31196581196581197,"Panayotis Mertikopoulos and William H Sandholm. Riemannian game dynamics. Journal of Eco-
nomic Theory, 2018."
REFERENCES,0.31339031339031337,"Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chan-
drasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going
the extra(-gradient) mile. In ICLR, 2019."
REFERENCES,0.3148148148148148,"Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do
actually converge? In International conference on machine learning. PMLR, 2018."
REFERENCES,0.3162393162393162,"Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D Lee, and Meisam Razaviyayn. Solving
a class of non-convex min-max games using iterative ﬁrst order methods. NeurIPS, 2019."
REFERENCES,0.3176638176638177,"Yann Ollivier, Ludovic Arnold, Anne Auger, and Nikolaus Hansen. Information-geometric opti-
mization algorithms: A unifying picture via invariance principles. Journal of Machine Learning
Research, 18(18), 2017."
REFERENCES,0.3190883190883191,"Dmitrii M Ostrovskii, Andrew Lowy, and Meisam Razaviyayn. Efﬁcient search of ﬁrst-order nash
equilibria in nonconvex-concave smooth min-max problems. SIAM Journal on Optimization,
2021."
REFERENCES,0.32051282051282054,"Georgios Piliouras and Leonard J Schulman. Learning dynamics and the co-evolution of competing
sexual species. In 9th Innovations in Theoretical Computer Science Conference (ITCS 2018).
Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2018."
REFERENCES,0.32193732193732194,"Siavash Shahshahani. A new mathematical framework for the study of linkage and selection. Amer-
ican Mathematical Soc., 1979."
REFERENCES,0.32336182336182334,"Karl Sigmund.
The maximum principle for replicator equations.
Iiasa working paper, IIASA,
Laxenburg, Austria, 1984."
REFERENCES,0.3247863247863248,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676), 2017."
REFERENCES,0.3262108262108262,"Maurice Sion et al. On general minimax theorems. Paciﬁc Journal of mathematics, 8(1), 1958."
REFERENCES,0.32763532763532766,Published as a conference paper at ICLR 2022
REFERENCES,0.32905982905982906,"Emmanouil-Vasileios Vlatakis-Gkaragkounis, Lampros Flokas, and Georgios Piliouras. Poincar´e
recurrence, cycles and spurious equilibria in gradient-descent-ascent for non-convex non-concave
zero-sum games. In NeurIPS 32: Annual Conference on Neural Information Processing Systems
2019, 2019."
REFERENCES,0.33048433048433046,"Yuanhao Wang, Guodong Zhang, and Jimmy Ba. On solving minimax optimization locally: A
follow-the-ridge approach. In International Conference on Learning Representations, 2020."
REFERENCES,0.3319088319088319,"Junchi Yang, Negar Kiyavash, and Niao He. Global convergence and variance reduction for a class
of nonconvex-nonconcave minimax problems. In NeurIPS, 2020."
REFERENCES,0.3333333333333333,"Guojun Zhang, Pascal Poupart, and Yaoliang Yu. Optimality and stability in non-convex smooth
games. arXiv preprint arXiv:2002.11875, 2020."
REFERENCES,0.33475783475783477,"A
OMITTED PROOFS"
REFERENCES,0.33618233618233617,"A.1
OMITTED PROOFS OF SECTION 3.1"
REFERENCES,0.33760683760683763,"Proposition A.1. Let us consider P(X) the space of probability distributions on a set X ⊆R with
the metric induced by the Kullback–Leibler (KL) divergence. If n := |X| = dim S, and pθ := θ ∈
P(X), then the natural gradient ﬂow of the Fisher information matrix in (8) is the natural gradient
ﬂow of the Shahshahani metric induced by the metric tensors in (9)."
REFERENCES,0.33903133903133903,"Proof. All we need to show is that Fθ = Sθ, ∀θ ∈S. Simply, note that ∀i, j ∈[n]:"
REFERENCES,0.34045584045584043,"(Fθ)i,j = −Ex∼pθ"
REFERENCES,0.3418803418803419,∂2 log pθ(x)
REFERENCES,0.3433048433048433,∂θi∂θj
REFERENCES,0.34472934472934474,"
= −
X x∈X"
REFERENCES,0.34615384615384615,∂2 log pθ(x)
REFERENCES,0.3475783475783476,"∂θi∂θj
pθ(x) = −
X x∈X"
REFERENCES,0.349002849002849,∂2 log θx
REFERENCES,0.3504273504273504,"∂θi∂θj
θx =
X x∈X"
REFERENCES,0.35185185185185186,"δx,iδx,j"
REFERENCES,0.35327635327635326,"θx
= δi,j"
REFERENCES,0.3547008547008547,"θi
= (Sθ)i,j ."
REFERENCES,0.3561253561253561,"A.2
OMITTED PROOFS OF SECTION 3.2"
REFERENCES,0.3575498575498576,"Let G = (n, S := [m1] × . . . × [mn], u : S →Rn) be a n-player (exact) potential game with payoff
function u and potential Φ : S →R, and let M := ∆m1 × . . . × ∆mn be the mixed-strategy space
of G. We are going to consider the RD of G in int M = int ∆m1 × . . . × int ∆mn as given in the
following proposition.
Proposition A.2. The Replicator Dynamics of G in int M are given by the following dynamical
system of equations:"
REFERENCES,0.358974358974359,"˙θi,j := θi,j(Esk∼θk
k∈[n]
[ui(sj, s−i)] −Esk∼θk
k∈[n]
[ui(s)]) = θi,j"
REFERENCES,0.3603988603988604,∂Ψ(θ)
REFERENCES,0.36182336182336183,"∂θi,j
−Ψ(θ)

(D4)"
REFERENCES,0.36324786324786323,"where Ψ(θ) := Esi∼θi
i∈[n]
[Φ(s)] is the the expected potential function of the game."
REFERENCES,0.3646723646723647,Proof. First note that ∂Ψ(θ)
REFERENCES,0.3660968660968661,"∂θi,j
=
X s Y"
REFERENCES,0.36752136752136755,"k̸=i
θk,skΦ(s)∂θi,si"
REFERENCES,0.36894586894586895,"∂θi,j
=
X s Y"
REFERENCES,0.37037037037037035,"k̸=i
θk,skΦ(s)δj,si =
X s−i Y"
REFERENCES,0.3717948717948718,"k̸=i
θk,skΦ(j, s−i) = Esi∼θi
i∈[n]
[Φ(j, s−i)] ."
REFERENCES,0.3732193732193732,Published as a conference paper at ICLR 2022
REFERENCES,0.37464387464387466,From which follows that
REFERENCES,0.37606837606837606,"˙θi,j = θi,j(Esk∼θk
k∈[n]
[ui(j, s−i) −ui(s)]) = θi,j(Esk∼θk
k∈[n]
[Φ(j, s−i) −Φ(s)])"
REFERENCES,0.37749287749287747,"= θi,j"
REFERENCES,0.3789173789173789,∂Ψ(θ)
REFERENCES,0.3803418803418803,"∂θi,j
−Ψ(θ)

."
REFERENCES,0.3817663817663818,"Next, let us consider a point θ ∈int M, and note that the following proposition holds:
Proposition A.3. For any θ ∈int M = int ∆m1 × . . . × int ∆mn, the tangent plane of int M at θ
is"
REFERENCES,0.3831908831908832,"Tθ(int M) = {µ ∈Rm1 × . . . × Rmn |
X"
REFERENCES,0.38461538461538464,"j
µi,j = 0} .
(21)"
REFERENCES,0.38603988603988604,"Proof. We ﬁrst note that int M is an open-subset of the surface D := {µ ∈Rm1 × . . . × Rmn |
Ui(µ) = 1}, where Ui(µ) = P"
REFERENCES,0.38746438746438744,"j µi,j = 1; hence, Tθ(int M) = Tθ D, up to isomorphism. Let
r(t) be a smooth curve in D with r(0) = θ. Then, by deﬁnition, Ui(r(t)) = 1 for all i, and, by
differentiating with respect to t, we have"
REFERENCES,0.3888888888888889,∇Ui(r(t)) · d
REFERENCES,0.3903133903133903,dtr(t) = d
REFERENCES,0.39173789173789175,dtUi(r(t)) = d
REFERENCES,0.39316239316239315,dt1 = 0 .
REFERENCES,0.3945868945868946,"However, d"
REFERENCES,0.396011396011396,"dtr(t) is tangent to D for all t; hence, d"
REFERENCES,0.3974358974358974,"dtr(t)

t=0 ∈Tθ D. It follows that ∇Ui(r(0)) ="
REFERENCES,0.39886039886039887,"∇Ui(θ) is normal to Tθ D for all i, and ,thus, we have"
REFERENCES,0.40028490028490027,"Tθ D ⊆{µ ∈Rm1 × . . . × Rmn | ⟨∇Ui(θ), µ⟩= 0} = {µ ∈Rm1 × . . . × Rmn |
X"
REFERENCES,0.4017094017094017,"j
µi,j = 0} ."
REFERENCES,0.4031339031339031,"Finally, notice that dim{µ ∈Rm1 × . . . × Rmn | P"
REFERENCES,0.4045584045584046,"j µi,j = 0} = dim D, which implies the
proposition."
REFERENCES,0.405982905982906,"Proposition 1. The Replicator Dynamics of a potential game G with potential function Φ is an
(extended) Shahshahani gradient in int(∆m1 × . . .×∆mn) having potential Ψ(θ) := Esi∼θi
i∈[n]
[Φ(s)]."
REFERENCES,0.4074074074074074,Proof. Let θ ∈int M := int ∆m1 × . . . × int ∆mn. Then for every ξ ∈Tθ(int M) we have:
REFERENCES,0.40883190883190884,"⟨˙θ, ξ⟩θ =
X 1"
REFERENCES,0.41025641025641024,"θi,j
˙θi,jξi,j =
X ∂Ψ(θ)"
REFERENCES,0.4116809116809117,"∂θi,j
−Ψ(θ)

ξi,j =
X ∂Ψ(θ)"
REFERENCES,0.4131054131054131,"∂θi,j
ξi,j −Ψ(θ)
X
ξi,j"
REFERENCES,0.41452991452991456,"=
X ∂Ψ(θ)"
REFERENCES,0.41595441595441596,"∂θi,j
ξi,j = ∇Ψ(θ) · ξ ."
REFERENCES,0.41737891737891736,"Hence, by deﬁnition, we have that ∇Ψ(θ) = ˙θ, where the Del operator is deﬁned with respect to
the Shahshahani metric, which implies the lemma."
REFERENCES,0.4188034188034188,"Now, let us consider a different class of games, which we deﬁne as 2-Team Zero-Sum games."
REFERENCES,0.4202279202279202,Published as a conference paper at ICLR 2022
REFERENCES,0.42165242165242167,"Deﬁnition 2. A 2-Team Zero-Sum game, G = (m := m1+m2, S := [n1]×. . .×[nm], u : S →Rm),
is a game between m players with strategy-space S whose utility function, u, satisﬁes the following:"
REFERENCES,0.4230769230769231,ui(s) =
REFERENCES,0.42450142450142453,"(
1
m1 Φ(s),
if i ∈[m1] −1"
REFERENCES,0.42592592592592593,"m2 Φ(s),
otherwise
∀s ∈S
(22)"
REFERENCES,0.42735042735042733,for some function Φ : S →R.
REFERENCES,0.4287749287749288,"We are going to, collectively, refer to the ﬁrst m1 players of a 2-Team Zero-Sum game, G = (m :=
m1 + m2, S := [n1] × . . . × [nm], u : S →Rm), as team 1 and to the rest of them as team 2.
Notice that if we consider each team as a single player, then the two teams are playing a 2-Player
Zero-Sum game, G′ = (2, S′ := S1 × S2, u′ : S′ →R2) where S1 = [n1] × . . . × [nm1], S2 =
[nm1+1 × . . . × nm], and the utility function u′
k is the total utility of the corresponding team k, i.e.
∀s ∈S we have"
REFERENCES,0.4301994301994302,"u′
1(s) = m1
X i=1"
REFERENCES,0.43162393162393164,"1
m1
Φ(s) = Φ(s)
and
u′
2(s) = − m
X"
REFERENCES,0.43304843304843305,i=m1+1
REFERENCES,0.43447293447293445,"1
m2
Φ(s) = −Φ(s) .
(23)"
REFERENCES,0.4358974358974359,"Note that, by deﬁnition, the 2-Team Hidden Matching Pennies game (see section 5) is a a 2-Team
Zero-Sum game. Let us consider the Replicator Dynamics of G given by the following dynamical
system of equations:"
REFERENCES,0.4373219373219373,"˙θi,j := θi,j(Esk∼θk[ui(j, s−i)] −Esk∼θk[ui(s)]) = 

 
"
REFERENCES,0.43874643874643876,"1
m1
θi,jEsk∼θk[Φ(j, s−i) −Φ(s)],
if i ∈[m1] −1"
REFERENCES,0.44017094017094016,"m2
θi,jEsk∼θk[Φ(j, s−i) −Φ(s)],
otherwise (D5)"
REFERENCES,0.4415954415954416,"where, as before, we can rewrite (D5) in terms of Ψ(x) := ESk∼xk[Φ(S)] as"
REFERENCES,0.443019943019943,"˙θi,j ="
REFERENCES,0.4444444444444444,"


 

"
REFERENCES,0.4458689458689459,"1
m1
θi,j(∂Ψ(θ)"
REFERENCES,0.4472934472934473,"∂θi,j
−Ψ(θ)),
if i ∈[m1] −1"
REFERENCES,0.44871794871794873,"m2
θi,j(∂Ψ(θ)"
REFERENCES,0.45014245014245013,"∂θi,j
−Ψ(θ)),
otherwise .
(D6)"
REFERENCES,0.4515669515669516,"Notice that the Replicator equations of each team are similar to the Replicator equations of a Poten-
tial game. In fact, it’s not difﬁcult to prove the following lemma."
REFERENCES,0.452991452991453,"Lemma 1. Let G = (m := m1 + m2, S := [n1] × . . . × [nm], u : S →Rm) be a 2-Team
Zero-Sum game. If we assume the strategies of team 2 to be time-invariant, then the Replicator
Dynamics of G, given by (D5) (or (D6), equivalently), is a (m1-scaled) Shahshahani gradient in
int M := int ∆n1 × . . .×int ∆nm with potential Ψ′
1(θ) = Ψ◦Π1(θ) where Ψ(θ) := Esi∼θi[Φ(s)],
and Π1 : int M →int M is the natural projection"
REFERENCES,0.4544159544159544,"Πk,i,j(θ) =
θi,j,
if i in team k
0,
otherwise .
(24)"
REFERENCES,0.45584045584045585,"Likewise, if we assume the strategies of team 1 to be time-invariant, then the Replicator Dynamics
of G is a (m2-scaled) Shahshahani gradient in int M with potential Ψ′
2 = −Ψ ◦Π2(θ)."
REFERENCES,0.45726495726495725,"Proof. This proof is similar to the proof of Proposition 1, but we need to perform the correct pro-
jection before applying the deﬁnition of a gradient ﬂow. Let θ ∈int M. Without any loss of the
generality, let us assume that the strategies of team 2 are time-invariant, i.e., ˙θi,j = 0 for all i ≥m1.
Then, for every ξ ∈Tθ(int M) we have"
REFERENCES,0.4586894586894587,Published as a conference paper at ICLR 2022
REFERENCES,0.4601139601139601,"⟨˙θ, ξ⟩θ =
X m1"
REFERENCES,0.46153846153846156,"θi,j
˙θi,jξi,j = m1
X i=1 X j"
REFERENCES,0.46296296296296297,"m1
eθi,j"
REFERENCES,0.46438746438746437,"1
m1
θi,j"
REFERENCES,0.4658119658119658,∂Ψ(θ)
REFERENCES,0.4672364672364672,"∂θi,j
−Ψ(θ)

ξi,j = m1
X i=1 X j ∂Ψ(θ)"
REFERENCES,0.4686609686609687,"∂θi,j
ξi,j −Ψ(θ) m1
X i=1 X"
REFERENCES,0.4700854700854701,"j
ξij = m1
X i=1 X j ∂Ψ(θ)"
REFERENCES,0.47150997150997154,"∂θi,j
ξi,j"
REFERENCES,0.47293447293447294,"=
X ∂Ψ(Π1(θ))"
REFERENCES,0.47435897435897434,"∂θi,j
ξi,j = ∇Ψ′
1(θ) · ξ ."
REFERENCES,0.4757834757834758,"Hence, by deﬁnition, we have that dΨ′
1(θ) = ˙θ♭, which implies the theorem."
REFERENCES,0.4772079772079772,"Proposition 2 (Metric tensors of the model space). Under mild regularity assumptions, we have
that, for any θ ∈RM,"
REFERENCES,0.47863247863247865,"∥Fθ+δθ −Fθ∥2 = ⟨δθ, Aθδθ⟩+ o(∥δθ∥2)
where
Aθ := Ex∼px[∇θFθ(x)∇θFθ(x)⊺] . (11)"
REFERENCES,0.48005698005698005,Proof. The assumption on F we need to prove this results are the following:
REFERENCES,0.48148148148148145,"• θ 7→Fθ(x) is almost surely differentiable, i.e., almost surely (in x),"
REFERENCES,0.4829059829059829,"Fθ+δθ(x) = Fθ(x) + ⟨∇Fθ(x), δθ⟩+ ∥δθ∥f(x, θ) , ∀θ ∈RM,
(25)"
REFERENCES,0.4843304843304843,"where f(x, δθ) →δθ→0 0 almost surely in x."
REFERENCES,0.48575498575498577,"• For small enough δθ ∈RM, The remainder of the Taylor expansion of Fθ has a ﬁnite
variance, i.e.,
Ex∼px[f(x, δθ)2] < +∞.
(26)"
REFERENCES,0.48717948717948717,"A consequence of these two assumption is that (by dominated convergence theorem and Jensen’s
inequality)
Ex∼px[|f(x, δθ)|] →δθ→0 0
and
Ex∼px[f(x, δθ)2] →δθ→0 0 .
(27)"
REFERENCES,0.4886039886039886,"Let us now prove the desired property. We start by doing a Taylor expansion of θ 7→Fθ(x),"
REFERENCES,0.49002849002849,"Fθ+δθ(x) = Fθ(x) + ⟨∇Fθ(x), δθ⟩+ ∥δθ∥f(x, θ) ."
REFERENCES,0.49145299145299143,Then we have that
REFERENCES,0.4928774928774929,"∥Fθ+δθ −Fθ∥2 = Ex∼px[(⟨∇Fθ(x), δθ⟩+ f(x, θ)∥δθ∥)2]"
REFERENCES,0.4943019943019943,= Ex∼px[(∇Fθ(x)⊺δθ)2] + o(δθ)2
REFERENCES,0.49572649572649574,= Ex∼px[δθ⊺∇Fθ(x)∇Fθ(x)⊺δθ] + o(δθ)2
REFERENCES,0.49715099715099714,= δθ⊺Ex∼px[∇Fθ(x)∇Fθ(x)⊺]δθ + o(δθ)2;
REFERENCES,0.4985754985754986,which concludes the proof.
REFERENCES,0.5,Proposition 3 (Natural Hidden Gradient dynamics). The ﬂow induced by the geometry (11) is
REFERENCES,0.5014245014245015,"˙θ = −A†
θE(x,x′)∼p[∇θLx,x′(Fθ(x), Gφ(x′))]
˙φ = B†
φE(x,x′)∼p[∇φLx,x′(Fθ(x), Gφ(x′))]
(D7)"
REFERENCES,0.5028490028490028,"where Aθ := Ex∼px[∇θFθ(x)∇θFθ(x)⊺] and Bφ := Ex′∼px′[∇φGφ(x)∇φGφ(x)⊺] and C†
denotes the pseudo-inverse of a matrix C."
REFERENCES,0.5042735042735043,Published as a conference paper at ICLR 2022
REFERENCES,0.5056980056980057,"Proof. The proposition directly follows from Proposition 2 applied to the deﬁnition of (7). However,
in order to convey more intuition we could use the deﬁnition that the natural gradient ﬂow of the
objective function f induced by the distance d if"
REFERENCES,0.5071225071225072,"˙θ = arg min
δθ
lim
λ→0
1
λf(θ + λδθ) +
1
2λ2 d2(Fθ+λδθ, Fθ) ."
REFERENCES,0.5085470085470085,"By using the L2 distance and noting that when λ →0 we have f(θ+λδθ) = f(θ)+λ∇f(θ)⊺δθ+
o(λ) and
1
2λ2 ∥Fθ+λδθ −Fθ∥2 = 1"
REFERENCES,0.50997150997151,"2⟨δθ, Aθδθ⟩+ o(1) we have that the RHS of the equation above
is"
REFERENCES,0.5113960113960114,"arg min
δθ
∇f(θ)⊺δθ + 1"
REFERENCES,0.5128205128205128,"2⟨δθ, Aθδθ⟩;"
REFERENCES,0.5142450142450142,"which is minimized for δθ = −A†
θ∇f(θ)."
REFERENCES,0.5156695156695157,"A.3
OMITTED PROOFS OF SECTION 4"
REFERENCES,0.5170940170940171,"Proposition 4. In the uni-dimensional case, the Natural Hidden Gradient ﬂow D1 takes the form of"
REFERENCES,0.5185185185185185,"˙θ = −∇θL(f(θ),g(φ))"
REFERENCES,0.51994301994302,"∥∇θf(θ)∥2
and
˙φ = ∇φL(f(θ),g(φ))"
REFERENCES,0.5213675213675214,"∥∇φg(φ)∥2
.
(D8)"
REFERENCES,0.5227920227920227,"Proof. For the uni-dimensional case, Aθ = ∇f(θ)∇f(θ)⊺is a rank-1 matrix that projects any
vector in the direction of ∇f(θ) and scales it by ∥∇f(θ)∥2. Thus, by deﬁnition of the pseudo-
inverse, we have that for any vector u ∈Rd:"
REFERENCES,0.5242165242165242,"A†
θu = ∇f(θ)⟨u, ∇f(θ)⟩"
REFERENCES,0.5256410256410257,"∥∇f(θ)∥4
."
REFERENCES,0.5270655270655271,"Finally, notice that"
REFERENCES,0.5284900284900285,"∇θL(f(θ), g(φ)) = ∇f(θ)∂(f, g(φ)) ∂f"
REFERENCES,0.5299145299145299,"f=f(θ) ,"
REFERENCES,0.5313390313390314,which leads to the stated proposition.
REFERENCES,0.5327635327635327,"Theorem 1. Let Ψ be the payoff of an HCC game (12) and consider the dynamics (D2). Then,"
REFERENCES,0.5341880341880342,"V (θ, φ) := 1"
REFERENCES,0.5356125356125356,2(f(θ) −f ∗)2 + 1
REFERENCES,0.5370370370370371,"2(g(φ) −g∗)2
(13)"
REFERENCES,0.5384615384615384,"is a Lyapunov function, i.e., it is positive, non-increasing, and null if and only if it evaluated at a
game solution. Moreover, if L is strictly convex-concave, we have that V is decreasing and that any
limit point (θ, φ) satisﬁes ∇θL(f(θ), g(φ)) = ∇φL(f(θ), g(φ)) = 0."
REFERENCES,0.5398860398860399,"Proof. By taking the time derivative of V (θ, φ), we get"
REFERENCES,0.5413105413105413,"˙V (θ, φ) = −(∇f(θ)(f(θ) −f ∗))⊺
∇f(θ)
∥∇f(θ)∥2
∂L(f, g(φ)) ∂f"
REFERENCES,0.5427350427350427,f=f(θ)
REFERENCES,0.5441595441595442,"+ (∇g(φ)(g(φ) −g∗))⊺
∇g(φ)
∥∇g(φ)∥2
∂L(f(θ), g) ∂g"
REFERENCES,0.5455840455840456,g=g(φ)
REFERENCES,0.5470085470085471,"= −(f(θ) −f ∗)∂L(f, g(φ) ∂f"
REFERENCES,0.5484330484330484,"f=f(θ) + (g(φ) −g∗)∂L(f(θ), g) ∂g"
REFERENCES,0.5498575498575499,g=g(φ) ≤0
REFERENCES,0.5512820512820513,Published as a conference paper at ICLR 2022
REFERENCES,0.5527065527065527,"where the last inequality holds because L is convex-concave. Speciﬁcally, let L : RM × RN →R
be any differentiable convex-concave function with saddle point (f ∗, g∗), then we have, respectively"
REFERENCES,0.5541310541310541,"−⟨f −f ∗, ∇fL(f, g)⟩≤L(f ∗, g) −L(f, g)
and
⟨g −g∗, ∇gL(f, g)⟩≤L(f, g) −L(f, g∗) ."
REFERENCES,0.5555555555555556,"By adding the two inequalities, we get"
REFERENCES,0.5569800569800569,"−⟨f −f ∗, ∇fL(f, g)⟩+ ⟨g −g∗, ∇gL(f, g)⟩≤L(f ∗, g) −L(f, g∗) ≤0
(28)"
REFERENCES,0.5584045584045584,"where the last inequality holds because (f ∗, g∗) is a saddle point. Hence, by deﬁnition, L is a
Lyapunov function."
REFERENCES,0.5598290598290598,"Finally, when L is strictly convex-concave, if there exists a limit point of (θ, φ) that is not a point
where ∇θL(f(θ), g(φ)) = 0 and ∇φL(f(θ), g(φ)) = 0 then by strict convex-concavity we get
that V should decrease by a “signiﬁcant enough amount” to create a contradiction with the fact that
V does converge."
REFERENCES,0.5612535612535613,"Next, before proving the general case of Theorem 1, we’ll have to prove the following lemma:"
REFERENCES,0.5626780626780626,"Lemma 2. Let (ui)i∈[n] be a linearly independent family of vectors on Rd. Then, the matrix A := n
X"
REFERENCES,0.5641025641025641,"i=1
uiu⊺
i .
(29)"
REFERENCES,0.5655270655270656,"is the Gram matrix of the family (ui)i∈[n] and and we have that ⟨ui, A†uj⟩= δi,j, ∀i, j ∈[n]."
REFERENCES,0.5669515669515669,"Proof. Let us introduce the matrix P := [u1, . . . , un]⊺; we can easily verify that A = P ⊺A. Let
P = UDV ⊺be the SVD decomposition of P and, thus, A† = V (D⊺D)†V ⊺where D⊺D is a
diagonal matrix. Then, we have"
REFERENCES,0.5683760683760684,"⟨ui, A†uk⟩= ⟨P ⊺e(i), A†P ⊺e(j)⟩= ⟨V D⊺U ⊺e(i), V (D⊺D)†V ⊺V D⊺U ⊺e(j)⟩"
REFERENCES,0.5698005698005698,"= ⟨e(i), UD(D⊺D)†D⊺U ⊺e(j)⟩."
REFERENCES,0.5712250712250713,"To conclude this lemma, we just need to notice that the matrix D is a matrix with non-zero entries
on the diagonal and, since we assumed that the vectors in (ui)i∈[n] are linearly independent, we
have that Di,i > 0, ∀i ∈[n]. Thus, by a direct computation, we get that"
REFERENCES,0.5726495726495726,D(D⊺D)†D⊺= In
REFERENCES,0.5740740740740741,"which leads to ⟨ui, A†ui⟩= δi,j."
REFERENCES,0.5754985754985755,"Theorem 2. Let Ψ be the payoff of a ﬁnite-sum HCC game given by (14) and consider the game
dynamics in (D3). Under Assumption 1, Assumption 2, we have that the quantity"
REFERENCES,0.5769230769230769,"V (θ, φ) := 1"
N,0.5783475783475783,"2n n
X"
N,0.5797720797720798,"i=1
(Fθ(xi) −F ∗(xi))2 + 1"
M,0.5811965811965812,"2m m
X"
M,0.5826210826210826,"j=1
(Gφ(x′
j) −G∗(x′
j))2
(15)"
M,0.584045584045584,"is a Lyapunov function, i.e., is positive, non-increasing and null if and only if evaluated at a game
solution. Moreover, if L is strictly convex-concave, V is decreasing as long as (θ, φ) ̸= (θ∗, φ∗)
and if L is a µ-strongly convex-concave function we have that V is decreasing exponentially as
V (θ, φ) = V (θ0, φ0) exp(−µt)."
M,0.5854700854700855,Published as a conference paper at ICLR 2022
M,0.5868945868945868,"Proof. Similarly as the proof of Theorem 1, we consider the time derivative of V (θ, φ). For com-
pactness, we are going to simplify the notation slightly by setting Fi := Fθ(xi), Gj := Gφ(x′
j),
F ∗
i := F ∗(xi), G∗
j := G∗(x′
j), and Li,j := Lxi,x′
j."
M,0.5883190883190883,"˙V (θ, φ) = 1 n n
X"
M,0.5897435897435898,"i=1
(Fi −F ∗
i )⟨∇θFi, ˙θ⟩+ 1 m m
X"
M,0.5911680911680912,"j=1
(Gj −G∗
j)⟨∇φGj, ˙φ⟩"
M,0.5925925925925926,"= −
1
n2m n
X"
M,0.594017094017094,"i=1
(Fi −F ∗
i )⟨∇θFi, A†
θ
X"
M,0.5954415954415955,"(j,k)∈[m]×[n]
∇θLk,j(Fk, Gj)⟩"
M,0.5968660968660968,"+
1
nm2 m
X"
M,0.5982905982905983,"j=1
(Gj −G∗
j)⟨∇φGj, B†
φ
X"
M,0.5997150997150997,"(k,i)∈[m]×[n]
∇φLi,k(Fi, Gk)⟩"
M,0.6011396011396012,"= −
1
n2m n
X"
M,0.6025641025641025,"i=1
(Fi −F ∗
i )
X"
M,0.603988603988604,"(j,k)∈[m]×[n]
⟨∇θFi, A†
θ∇θFk⟩∂Lk,j(F, Gj) ∂F F =Fk"
M,0.6054131054131054,"+
1
nm2 m
X"
M,0.6068376068376068,"j=1
(Gj −G∗
j)
X"
M,0.6082621082621082,"(k,i)∈[m]×[n]
⟨∇φGj, B†
φ∇φGk⟩∂Li,k(Fi, G) ∂G"
M,0.6096866096866097,"G=Gk
."
M,0.6111111111111112,"From Lemma 2 we have that ⟨∇θFi, A†
θ∇θFk⟩= n · δi,k, ∀i ∈[n], k ∈[m], and that
⟨∇φGj, B†
φ∇φFk⟩= m · δj,k. Hence, it follows"
M,0.6125356125356125,"˙V (θ, φ) = −1 nm X"
M,0.613960113960114,"i∈[n]
j∈[m]"
M,0.6153846153846154,"(Fi −F ∗
i )∂Li,j(F, Gj) ∂F"
M,0.6168091168091168,"F =Fi +
1
nm X"
M,0.6182336182336182,"i∈[n]
j∈[m]"
M,0.6196581196581197,"(Gj −G∗
j)∂Li,j(Fi, G) ∂G G=Gj"
M,0.6210826210826211,"= −⟨∂F L(F, G), F −F ∗⟩+ ⟨∂GL(F, G), G −G∗⟩"
M,0.6225071225071225,"where [∂F L(F, G)](i,j) := ∂Li,j(F,Gj)"
M,0.6239316239316239,"∂F

F =Fi
, ∂GL(F, G)](i,j) := ∂Li,j(Fi,G)"
M,0.6253561253561254,"∂G

G=Gj
, and F, G, F ∗,"
M,0.6267806267806267,"and G∗are indexed by (i, j), as well (while repeating vector elements as necessarily). Thus, using
the same reasoning as in Theorem 1, it follows that if L is convex-concave we have that V is non-
increasing, and, if L is strictly convex-concave, ˙V (θ, φ) < 0 whenever (θ, φ) ̸= (θ∗, φ∗). Finally,
if L is µ-strongly convex-concave, we have by deﬁnition that"
M,0.6282051282051282,"−⟨∂F L(F, G), F −F ∗⟩+ ⟨∂GL(F, G), G −G∗⟩"
M,0.6296296296296297,"≥−µ(∥F −F ∗∥2 + ∥G −G∗∥2) = −µV (θ, φ)"
M,0.6310541310541311,"Thus we conclude that V (θ, φ) ≤V (θ0, φ0) exp(−µt)."
M,0.6324786324786325,"A.4
OMITTED PROOFS OF SECTION 5"
M,0.6339031339031339,"In order to prove (17), and (D9) we are, ﬁrst, going to prove the following useful lemma:
Lemma 3. Let hθ(i) := Exj∼Ber(θj)[XOR(x1, . . . , xi)] where θ ∈[0, 1]n, and i ≤n. Then the
following equality holds:"
M,0.6353276353276354,1 −2hθ(i) = iY
M,0.6367521367521367,"j=1
(1 −2θj) .
(30)"
M,0.6381766381766382,"Proof. The easiest way to prove this relationship is by induction on n ∈N. For n = 1, we only
need to verify that (30) holds for i = 1. Indeed, we have,"
M,0.6396011396011396,Published as a conference paper at ICLR 2022
M,0.6410256410256411,1 −2hθ(1) = 1 −2Ex∼Ber(θ1)[XOR(x1)] = 1 −2Ex∼Ber(θ1)[x1] = 1 −2θ1 .
M,0.6424501424501424,"Next, let us assume that Lemma 3 holds for some n = n′ ∈N. We are going to prove that Lemma 3
also holds for n = n′ + 1, and this comes down in proving that (30) holds for i = n′ + 1:"
M,0.6438746438746439,"1 −2hθ(n′ + 1) = 1 −2E[XOR(x1, . . . , xn′+1)] = 1 −2E[E[XOR(x1, . . . , xn′+1) | xn′+1]]
= 1 −2(θn′+1E[XOR(x1, . . . , xn′, 1)] + (1 −θn′+1)E[XOR(x1, . . . , xn′, 0)])
= 1 −2(θn′+1E[1 −XOR(x1, . . . , xn′)] + (1 −θn′+1)E[XOR(x1, . . . , xn′)])"
M,0.6452991452991453,= 1 −2(θn′+1(1 −hθ(n′)]) + (1 −θn′+1)hθ(n′)) = (1 −2θn′+1)(1 −2hθ(n′))
M,0.6467236467236467,"= (1 −2θn′+1) n′
Y"
M,0.6481481481481481,"j=1
(1 −2θj) ="
M,0.6495726495726496,"n′+1
Y"
M,0.6509971509971509,"j=1
(1 −2θj) ."
M,0.6524216524216524,"And we that, the proof by induction is complete."
M,0.6538461538461539,"Proposition A.4. Let G = (n := n1+n2, S := {0, 1}n, u : S →Rn) be a 2-Team Hidden Matching
Pennies game with payoff function given by (16). Then, the expected payoff of the i-th member of
team k is given by (17)."
M,0.6552706552706553,"Proof. The ﬁrst equality follows, trivially, from the deﬁnition of Φ(s). All is left to prove is that
−Esk,i∼Ber(θk,i)[Φ(s)] = (1 −2Es1,i∼Ber(θ1,i)[XOR(s1)])(1 −2Es2,i∼Ber(θ2,i)[XOR(s2)]) for all
θ ∈[0, 1]n. We deﬁne, xk = XOR(sk), k ∈[2]. Note that, by deﬁnition, xk ∼Ber(pk), where
pk := Esk,i∼Ber(θk,i)[XOR(sk)]; hence,"
M,0.6566951566951567,"Esk,i∼Ber(θk,i)[Φ(s)] = Esk,i∼Ber(θk,i)[1 −2 · 1XOR(s1)=XOR(s2)] = Exk∼Ber(pk)[1 −2 · 1x1=x2]"
M,0.6581196581196581,= Exk∼Ber(pk)[(1 −2 XOR(x))] = 1 −2hp(2) = (1 −2p1)(1 −2p2)
M,0.6595441595441596,"= (1 −2Es1,i∼Ber(θ1,i)[XOR(s1)])(1 −2Es2,i∼Ber(θ2,i)[XOR(s2)]) ."
M,0.6609686609686609,"Proposition A.5. Let G = (n := n1+n2, S := {0, 1}n, u : S →Rn) be a 2-Team Hidden Matching
Pennies game with payoff function given by (16). Then, the RD of G is given by the dynamical system
of equations:"
M,0.6623931623931624,"˙θk,i = (−1)k−1 2"
M,0.6638176638176638,"nk
θk,i(1 −θk,i)
Y"
M,0.6652421652421653,k′∈[2] Y
M,0.6666666666666666,"i′∈[nk′]
(k,i)̸=(k′,i′)"
M,0.6680911680911681,"(1 −2θk,i) .
(D9)"
M,0.6695156695156695,Proof. From (17) we have that
M,0.6709401709401709,"Esk,i∼Ber(θk,i)[uk,i(s)]"
M,0.6723646723646723,= (−1)k−1 1
M,0.6737891737891738,"nk
(1 −2Es1,i∼Ber(θ1,i)[XOR(s1)])(1 −2Es2,i∼Ber(θ2,i)[XOR(s2)])"
M,0.6752136752136753,= (−1)k−1 1
M,0.6766381766381766,"nk
(1 −2hθ1(n1)(1 −2hθ2(n2) = (−1)k−1 1 nk Y k∈[2] Y"
M,0.6780626780626781,"i∈[nk]
(1 −2θk,i) ."
M,0.6794871794871795,"Then, by the deﬁnition of RD of G, we get"
M,0.6809116809116809,Published as a conference paper at ICLR 2022
M,0.6823361823361823,"˙θk,i := θk,iEsk,i∼Ber(θk,i)[uk,i(0, s−k,i) −uk,i(s)]"
M,0.6837606837606838,= (−1)k−1 1
M,0.6851851851851852,"nk
θk,i "
M,0.6866096866096866,"


 Y"
M,0.688034188034188,k′∈[2] Y
M,0.6894586894586895,"i′∈[nk′]
(k,i)̸=(k′,i′)"
M,0.6908831908831908,"(1 −2θk,i) −
Y"
M,0.6923076923076923,k′∈[2] Y
M,0.6937321937321937,"i′∈[nk′]
(1 −2θk,i) "
M,0.6951566951566952,"


"
M,0.6965811965811965,= (−1)k−1 2
M,0.698005698005698,"nk
θk,i(1 −θk,i)
Y"
M,0.6994301994301995,k′∈[2] Y
M,0.7008547008547008,"i′∈[nk′]
(k,i)̸=(k′,i′)"
M,0.7022792022792023,"(1 −2θk,i) ."
M,0.7037037037037037,"Theorem 3. Consider the Replicator Dynamics of G. Given any interior initial condition, the
resulting orbit is a cycle that satisﬁes the following n1 + n2 −1 independent invariant functions:"
M,0.7051282051282052,"Vi1,i2(θ) ="
X,0.7065527065527065,"2
X"
X,0.707977207977208,"k=1
nk[log(θk,ik) + log(1 −θk,ik)], ik ∈[nk], k ∈[2] .
(19)"
X,0.7094017094017094,"Proof. For all k ∈[2] and i ∈[nk], we have"
X,0.7108262108262108,∂Vi1i2(θ)
X,0.7122507122507122,"∂θk,i
= 
 "
X,0.7136752136752137,"nk(1 −2θk,i)"
X,0.7150997150997151,"θk,i(1 −θk,i) ,
if (k, i) ∈{(k′, ik′) | k′ ∈[2]}"
X,0.7165242165242165,"0,
otherwise ."
X,0.717948717948718,"Subsequently, we have"
X,0.7193732193732194,"˙Vi1i2(θ) = ⟨∇θVi1i2(θ), ˙θ⟩= 2
Y k∈[2] Y"
X,0.7207977207977208,"i∈[nk]
(1 −2θk,i) −2
Y k∈[2] Y"
X,0.7222222222222222,"i∈[nk]
(1 −2θk,i) = 0 ."
X,0.7236467236467237,"Observe that the above imply the existence of n1 + n2 −1 independent invariant functions. Hence,
the dynamics converge to a limit set of a single degree of freedom, i.e., a cycle."
X,0.7250712250712251,"Proposition A.6. Let G = (n := n1 + n2, S := {0, 1}n, u : S →Rn) be a 2-Team Hidden
Matching Pennies game with payoff function given by (16). Given a mixed-strategy proﬁle (θ, 1 −
θ), θ := (θ1, θ2) ∈(0, 1)n, (θ, 1 −θ) is an equilibrium of G if and only if ∃ik, ∀k ∈[2] such that
θk,ik = 1"
X,0.7264957264957265,"2, ∀k ∈[2]."
X,0.7279202279202279,"Proof. We know that G is equivalent to a Hidden Matching Pennies game (Example 1) with
payoff Ψ(θ) = (1 −2f(θ1))(1 −2g(θ2)), where f(θ1) := Es1,i∼Ber(θ1,i)[XOR(s1)], and
g(θ2) := Es2,i∼Ber(θ2,i)[XOR(s2)] are its hidden mappings. The only fully mixed-Nash equilib-
rium of this Hidden Matching Pennies game is (f(θ1), g(θ2)) = ( 1 2, 1"
X,0.7293447293447294,"2); hence, a mixed-strategy
proﬁle (θ, 1 −θ) is a fully mixed-Nash equilibrium of G, if and only if f(θ1) = g(θ2) = 1"
FROM,0.7307692307692307,"2. From
f(θ1) = 1"
WE GET,0.7321937321937322,2 we get
WE GET,0.7336182336182336,"Es1,i∼Ber(θ1,i)[XOR(s1)] = 1"
WE GET,0.7350427350427351,"2 ⇐⇒1 −2hθ1(n1) = 0 ⇐⇒ n1
Y"
WE GET,0.7364672364672364,"i=1
(1 −2θ1,i) = 0"
WE GET,0.7378917378917379,"⇐⇒∃i1 ∈[n1] : θ1,i = 1 2 ."
WE GET,0.7393162393162394,Published as a conference paper at ICLR 2022
WE GET,0.7407407407407407,"We remark that, for any ik ∈[nk], k ∈[2],"
WE GET,0.7421652421652422,"Vi1,i2(θ) :="
X,0.7435897435897436,"2
X"
X,0.7450142450142451,"k=1
nk[log(θk,ik) + log(1 −θk,ik)] ="
X,0.7464387464387464,"2
X"
X,0.7478632478632479,"k=1
2 · nk[ 1"
X,0.7492877492877493,"2 log(θk,ik) + 1"
X,0.7507122507122507,"2 log(1 −θk,ik)] ="
X,0.7521367521367521,"2
X"
X,0.7535612535612536,"k=1
2 · nkH(Ber( 1"
X,0.7549857549857549,"2), Ber(θk,ik))"
X,0.7564102564102564,"where H(p, q) is the cross-entropy of a distribution q relative to a distribution p. In other words,
every one of the invariant functions in (19) is the weighted sum of the cross entropy of two mixed-
strategies (of the i1-th member of team 1, and the i2-th member of team 2) relative to the uniform
strategy. That is, each invariant function measures (up to a constant) the Kullback–Leibler diver-
gence of two opposing members to an actual equilibrium of the game. Notice that, for any equi-
librium point of the Hidden Matching Pennies game, there exists at least on such pair of opposing
members i1 ∈[n1], and i2 ∈[n2] such that"
X,0.7578347578347578,DKL(Ber( 1
X,0.7592592592592593,"2) ∥Ber(θ1,i1)) = DKL(Ber( 1"
X,0.7606837606837606,"2) ∥Ber(θ2,i2)) = 0 ."
X,0.7621082621082621,"For the rest of this section we are going to consider the RD of a 2-Team Hidden Matching Pennies
game G = (n := n1 + n2, S := {0, 1}n, u : S →Rn) in a restricted setting, given by the following
dynamical system of equations:"
X,0.7635327635327636,"˙θk,i ="
X,0.7649572649572649,"





"
X,0.7663817663817664,"




"
X,0.7678062678062678,"0,
if θk,i /∈Sk,i
0,
if θk,i = αk,i and (−1)kDk,i(θ) < 0"
X,0.7692307692307693,"0,
if θk,i = βk,i and (−1)kDk,i(θ) > 0"
X,0.7706552706552706,(−1)k−1 2
X,0.7720797720797721,"nk
θk,i(1 −θk,i)Dk,i(θ),
otherwise (D10)"
X,0.7735042735042735,"where Dk,i(θ) := Q"
X,0.7749287749287749,"k′∈[2]
Q
j∈[nk′]
(k′,j)̸=(k,i)
(1 −2θk′,j), k ∈[2] i ∈[nk], and where we restrict each"
X,0.7763532763532763,"mixed-strategy proﬁle (θ, 1 −θ) such that θk,i ∈Sk,i := [αk,i, βk,i], ∀k ∈[2], i ∈[nk]. We let
Ωbe an orbit deﬁned by this RD whose initial conditions satisfy Assumption 3. This assumption
serves a dual purpose. To begin with, it ensures that any orbit is initialized inside the restricted
parameter space that is deﬁned by Sk,i. Furthermore, it makes sure the initial strategy proﬁle has
full support, i.e., (θk,i(0), 1−θk,i(0)) is an interior point of the simplex for all k ∈[2], and i ∈[nk].
It is easy to see that any dimension of the strategy space initially without support is impossible to be
updated by the RD in (D10); hence, it would be irrelevant for the analysis. Before we proceed, we
are going to introduce a couple of useful lemmas."
X,0.7777777777777778,"Lemma 4. ∀k ∈[2], and i ∈[nk] if θk,i(0) ∈Sk,i, then θk,i(t) ∈Sk,i, ∀t ≥0."
X,0.7792022792022792,"Proof. Let k ∈[2] and i ∈[nk] such that θk,i(0) ∈Sk,i =⇒αk,i ≤θk,i(0) ≤βk,i. We are going
to prove our case by abduction."
X,0.7806267806267806,"Suppose ∃t0 > 0 such that θk,i(t0) /∈Sk,i and, without any loss of the generality, let us assume that
θk,i(t0) > βk,i. We let T = {t ∈(0, t0) | θk,i(t) = βk,i and we note that, since θk,i(0) ≤βk,i, it
is implied by the continuity of θk,i(t) (Equation D10) and by the Intermediate Value Theorem that
T ̸= ∅. Finally we deﬁne tmax = max(T)."
X,0.782051282051282,"Let us now consider the value of θk,i(t) for some t ∈(tmax, t0) and observe that if θk,i(t) = βk,i
we have"
X,0.7834757834757835,t ∈T =⇒t ≤max(T) = tmax =⇒tmax < t ≤tmax .
X,0.7849002849002849,Published as a conference paper at ICLR 2022
X,0.7863247863247863,"This contradiction implies that θk,i(t) ̸= βk,i. However, if θk,i(t) < βk,i, the Intermediate Value
Theorem, once again, implies ∃t′ ∈(t, t0) such that θk,i(t′) = βk,i, which as before implies tmax <
t′ ≤tmax. Hence, it must be the case that, for all t ∈(tmax, t0):"
X,0.7877492877492878,"θk,i(t) > βk,i
(D10)
=⇒˙θk,i(t) = 0 ."
X,0.7891737891737892,"However, by applying the Mean Value Theorem, if follows ∃t′ ∈(tmax, t0) such that"
X,0.7905982905982906,"˙θk,i(t) = θk,i(0) −θk,i(tmax)"
X,0.792022792022792,"t0 −tmax
= θk,i(0) −βk,i"
X,0.7934472934472935,"t0 −tmax
> 0 ,"
X,0.7948717948717948,which is once again a contradiction.
X,0.7962962962962963,"Lemma 5. Under Assumption 3, if Sk = ∅, ∀k ∈[2] then the following hold:"
X,0.7977207977207977,"(a) Ψ1,i1(θ(t))Ψ2,i2(θ(t)) < 0 for all ik ∈[nk], k ∈[2], and t ≥0."
X,0.7991452991452992,"(b) Ψk,ik(θ(t)) preserves sign for all ik ∈[nk], k ∈[2]."
X,0.8005698005698005,"Proof. Since Sk = ∅for all k ∈[2], it follows, by deﬁnition, that"
X,0.801994301994302,"1
2 /∈Sk,i, ∀k ∈[2], i ∈[nk]"
X,0.8034188034188035,"Furthermore, by Assumption 3, we have that θk,i(0) ∈Sk,i for all k ∈[2], i ∈[nk]. Hence, it
follows, by Lemma 4, that ∀k ∈[2], i ∈[nk], and t ≥0:"
X,0.8048433048433048,"θk,i(t) ∈Sk,i =⇒θk,i(t) ̸= 1"
X,0.8062678062678063,"2 =⇒1 −2θk,i(t) ̸= 0 ."
X,0.8076923076923077,"Then, by Equation 17, we have"
X,0.8091168091168092,"Ψk,i(θ(t)) = (−1)k−1"
X,0.8105413105413105,"nk
(1 −2Es1,j∼Ber(θ1,j)[XOR(s1)])(1 −2Es2,j∼Ber(θ2,j)[XOR(s2)])"
X,0.811965811965812,= (−1)k−1
X,0.8133903133903134,"nk
(1 −2hθ1)(1 −2hθ2) = (−1)k−1 nk n1
Y"
X,0.8148148148148148,"j=1
(1 −2θ1,j) n2
Y"
X,0.8162393162393162,"j=1
(1 −2θ2,j)"
X,0.8176638176638177,= (−1)k−1 nk
Y,0.8190883190883191,"2
Y k′=1 nk′
Y"
Y,0.8205128205128205,"j=1
(1 −2θk′,j(t)) ̸= 0 ."
Y,0.8219373219373219,"That implies (a). We can now condition on Ψk,i(θ(0)). Since Ψk,i(θ(t)) ̸= 0 for all t ≥0, it must
be the case that either Ψk,i(θ(0)) > 0 or Ψk,i(θ(0)) < 0; let us, ﬁrst, assume the former case. We
are going to prove, by abduction, that Ψk,i(θ(t)) > 0, ∀t ≥0."
Y,0.8233618233618234,"Suppose ∃t′ > 0 such that Ψk,i(θ(t′)) ≤0. Since Ψk,i(θ(t)) ̸= 0 for all t ≥0 it follows that
Ψk,i(θ(t′)) < 0 must be the case. However, by the continuity of Ψk,i(θ(t)) and the Intermediate
Value Theorem, it follows that ∃t′′ ∈(0, t′) such that Ψk,i(θ(t′′)) = 0, and that is, indeed, a
contradiction. It follows that it must be the case Ψk,i(θ(t)) > 0, ∀t ≥0."
Y,0.8247863247863247,"In a similar manner we may prove that Ψk,i(θ(0)) < 0 =⇒Ψk,i(θ(t)) < 0, ∀t ≥0; hence, (b)
holds."
Y,0.8262108262108262,Published as a conference paper at ICLR 2022
Y,0.8276353276353277,"Theorem 4. Under Assumption 3, if Sk = ∅, ∀k ∈[2], the restricted RD of G converge to"
Y,0.8290598290598291,"θ∗
1,i =

α1,i,
if β1,i < 1"
Y,0.8304843304843305,"2
β1,i,
otherwise
i ∈[n1]"
Y,0.8319088319088319,"θ∗
2,i =

β2,i,
if β2,i < 1"
Y,0.8333333333333334,"2
α2,i,
otherwise
i ∈[n2]
|
{z
}
if |S| is even"
Y,0.8347578347578347,"or
θ∗
1,i =

β1,i,
if β1,i < 1"
Y,0.8361823361823362,"2
α1,i,
otherwise
i ∈[n1]"
Y,0.8376068376068376,"θ∗
2,i =

α2,i,
if β2,i < 1"
Y,0.8390313390313391,"2
β2,i,
otherwise
i ∈[n2]
|
{z
}
if |S| is odd (20)"
Y,0.8404558404558404,"where S := {(k, i) | k ∈[2], i ∈[nk], αk,i > 1 2}."
Y,0.8418803418803419,"Proof. We begin by conditioning on the value of |S| and, since the proof is similar in both cases,
and without any loss of the generality, we are going to assume that |S| is even. Since Sk = ∅for all
k ∈[2], i.e., 1"
Y,0.8433048433048433,"2 /∈Sk,i for all k ∈[2], i ∈[nk], and, by Assumption 3, θk,i(0) ∈Sk,i := [αk,i, βk,i]
for k ∈[2], i ∈[nk] it follows that θk,i(0) > 1"
Y,0.8447293447293447,"2, if αk,i > 1"
Y,0.8461538461538461,"2; θk,i(0) < 1"
Y,0.8475783475783476,"2; otherwise. Then, for all
i ∈[n1], we have"
Y,0.8490028490028491,"Ψ1,i(θ(0)) = 1 nk"
Y,0.8504273504273504,"2
Y k=1 nk
Y"
Y,0.8518518518518519,"j=1
(1 −2θk,j(0)) = 1 nk  Y"
Y,0.8532763532763533,"(k,j)∈S
(1 −2θk,j(0))  ·  Y"
Y,0.8547008547008547,"(k,j)/∈S
(1 −2θk,j(0))  > 0,"
Y,0.8561253561253561,"since |S| is even.
Subsequently, by Lemma 5, Ψ1,i(θ(t)) > 0, ∀t ≥0.
That implies that
Ψ1,i(θ(t)) > 0, ∀i ∈[n1], t ≥0, and, hence, by Lemma 5, we also have that Ψ2,i(θ(t)) <
0, ∀i ∈[n2], t ≥0. Take any k ∈[2], and i ∈[nk]. In order to complete the proof, we’ll have
to condition on the value of k, and on whether (k, i) ∈S. There are four cases in total that we”ll
have to consider, but, since in all of them we follow a similar reasoning, we’ll just go ahead and
demonstrate the single case of k = 1, and (k, i) ∈S."
Y,0.8575498575498576,"We are going to prove that there exists t∗
1,i ≥0 such that θ1,i(t) = β1, i, ∀t ≥t∗
1,i. By Assump-
tion 3, and Lemma 4, we have α1,i ≥θ1,i(t) ≥β1,i for all t ≥0. Furthermore, since (1, i) ∈S,
and, since Sk = ∅, we have that α1,i ≥β1,i > 1"
Y,0.8589743589743589,"2. That is, θ1,i(t) > 1"
Y,0.8603988603988604,"2 for all t ≥0, and, hence,"
Y,0.8618233618233618,"D1,i(θ(t)) :=
Y k∈[2] Y"
Y,0.8632478632478633,"j∈[nk]
(k,j)̸=(1,i)"
Y,0.8646723646723646,"(1 −2θk,j(t)) =
1
1 −2θ1,i(t) ·"
Y,0.8660968660968661,"2
Y k=1 nk
Y"
Y,0.8675213675213675,"j=1
(1 −2θk,j(t))"
Y,0.8689458689458689,"= n1 ·
1
1 −2θ1,i(t) · Ψ1,i(t) < 0 ."
Y,0.8703703703703703,"Then, by Equation D10 it follows that ∀t ≥0:"
Y,0.8717948717948718,"˙θ1,i(t) = 
 "
Y,0.8732193732193733,"0,
if θ1,i = β1,i
2
n1
θ1,i(1 −θ1,i)D1,i(θ),
otherwise .
(D11)"
Y,0.8746438746438746,"Hence, θ1,i = β1,i is the single attracting point of the dynamical system described by D11 and,
hence, by deﬁnition, ∃t∗
1,i ≥0 such that θ1,i(t) = θ∗
1,i = β1,i, ∀t ≥t∗
1,i. Similarly, we can prove
that ∃t∗
k,i ≥0 such that θk,i(t) = θ∗
k,i, ∀t ≥t∗
k,i, and by letting t∗= max
k∈[2]
i∈[nk]
(t∗
k,i), Theorem 4"
Y,0.8760683760683761,follows.
Y,0.8774928774928775,Published as a conference paper at ICLR 2022
Y,0.8789173789173789,"Theorem 5. Under Assumption 3, if Sk ̸= ∅, ∀k ∈[2], then the restricted RD of G converge to an
invariant set deﬁned by the |S1|+|S2|−1 independent invariant functions, Vi1,i2(θ), ik ∈Sk, ∀k ∈
[2], where Vi1,i2(θ) is given as in (19)."
Y,0.8803418803418803,Proof. We are going to begin by deﬁning the following time-dependent set:
Y,0.8817663817663818,"C(t) :=

{(k, i) | k ∈[2], i ∈[nk], θk,i(t) ∈{αk,i, βk,i}},
if t ≥0
∅,
otherwise ."
Y,0.8831908831908832,"We perform a time partitioning based on the values of C(t), t ≥0. Speciﬁcally, we let P =
{(t1, t2) | t1, t2 ∈T : t /∈T, ∀t ∈(t1, t2)}, where"
Y,0.8846153846153846,"T := {t ≥0 | ∃ϵ0 > 0 : C(t −ϵ) ̸= C(t), ∀ϵ ∈(0, ϵ0)} ."
Y,0.886039886039886,"It is not difﬁcult to see that the continuity of θ(t) implies that T consists entirely of isolated points
and, due to this fact, P is well-deﬁned. Let I ∈P be one of these partitions. We are going to prove
that ˙Vi1,i2(θ(t)) ≥0, ∀ik ∈Sk, ∀k ∈[2], t ∈I."
Y,0.8874643874643875,"First of all, observe that, by deﬁnition, the following two properties have to hold:"
Y,0.8888888888888888,"(a) C(t1) = C(t2), ∀t1, t2 ∈I"
Y,0.8903133903133903,"(b) By Equation D10, we have that ∀t ∈I,"
Y,0.8917378917378918,"˙θk,i(t) = 
 "
Y,0.8931623931623932,"0,
if (k, i) ∈C(t)"
Y,0.8945868945868946,(−1)k−1 2
Y,0.896011396011396,"nk
θk,i(1 −θk,i)Dk,i(θ),
otherwise .
(D12)"
Y,0.8974358974358975,"For any ik ∈Sk, k ∈[2], we proceed by conditioning on the value of C(t), t ∈I and, since
C(t1) = C(t2), ∀t1, t2 ∈I, we distinct only three cases:"
Y,0.8988603988603988,"(a) (k, ik) ∈C(t), ∀k ∈[2], t ∈I."
Y,0.9002849002849003,"(b) (k, ik) /∈C(t), ∀k ∈[2], t ∈I."
Y,0.9017094017094017,"(c) ∃k, k′ ∈[2] such that (k, ik) ∈C(t) and (k′, ik′) /∈C(t) for all t ∈I."
Y,0.9031339031339032,"The analysis of the ﬁrst two cases is relatively straightforward.
Let t ∈I; then, if (k, ik) ∈
C(t), ∀k ∈[2] is the case, then by Equation D12, we have that ˙θk,ik(t) = 0, ∀k ∈[2] and,
hence,"
Y,0.9045584045584045,"˙Vi1,i2(θ(t)) = (∇θ(t)Vi1,i2(θ(t)))⊺˙θ(t) ="
X,0.905982905982906,"2
X k=1"
X,0.9074074074074074," nk(1 −2θk,i(t))"
X,0.9088319088319088,"θk,i(t)(1 −θk,i(t)) · ˙θk,ik(t)

= 0 ."
X,0.9102564102564102,"On the other hand, if (k, ik) /∈C(t), ∀k ∈[2] is the case, then, once again, by Equation D12, we
have that ˙θk,ik(t) = (−1)k−1 2"
X,0.9116809116809117,"nk θk,i(1 −θk,i)Dk,i(θ), ∀k ∈[2]. That implies,"
X,0.9131054131054132,"˙Vi1,i2(θ(t)) ="
X,0.9145299145299145,"2
X k=1"
X,0.915954415954416," nk(1 −2θk,i(t))"
X,0.9173789173789174,"θk,i(t)(1 −θk,i(t)) · (−1)k−1 2"
X,0.9188034188034188,"nk
θk,i(t)(1 −θk,i(t))Dk,i(θ(t))
 = 2 ·"
Y,0.9202279202279202,"2
Y k=1 nk
Y"
Y,0.9216524216524217,"j=1
(1 −2θk,j(t)) ·"
X,0.9230769230769231,"2
X"
X,0.9245014245014245,"k=1
(−1)k−1 = 0 ."
X,0.9259259259259259,In order to proceed with the ﬁnal case we’ll ﬁrst need to prove a small technical lemma:
X,0.9273504273504274,Published as a conference paper at ICLR 2022
X,0.9287749287749287,"Lemma 6. If (k, i) ∈C(t) for some t ∈I, I ∈P = {(t1, t2) | t1, t2 ∈T : t /∈T, ∀t ∈(t1, t2)}
then one of the following holds:"
X,0.9301994301994302,"a) θk,i(t) = αk,i, ∀t ∈I."
X,0.9316239316239316,"b) θk,i(t) = βk,i, ∀t ∈I."
X,0.9330484330484331,"To see that, let I ∈P and (k, i) ∈C(t) for some t ∈I. Then it, holds, by deﬁnition, that"
X,0.9344729344729344,"C(t1) = C(t2), ∀t1, t2 ∈I =⇒C(t′) = C(t), ∀t′ ∈I"
X,0.9358974358974359,"And, since (k, i) ∈C(t), we also have that"
X,0.9373219373219374,"(k, i) ∈C(t′), ∀t′ ∈I =⇒θk,i(t′) ∈{αk,i, βk,i}"
X,0.9387464387464387,"Let us assume that θk,i(t) = αk,i and suppose ∃t′ ∈I such that θk,i(t′) ̸= αk,i =⇒θk,i(t′) = βk,i
and αk,i ̸= βk,i. By the Intermediate Value Theorem, it follows ∃t′′ ∈(min(t, t′), max(t, t′)) such
that"
X,0.9401709401709402,"θk,i(t′′) ∈(αk,i, βk,i) =⇒(k, i) /∈C(t′′)"
X,0.9415954415954416,"That is a contradiction, and, hence, a) holds. a) follows by a similar argument, assuming θk,i(t) =
βk, i is the case, instead."
X,0.9430199430199431,"Having established Lemma 6, we continue with the proof of Theorem 5. Let us assume that ∃k, k′ ∈
[2] such that (k, ik) ∈C(t) and (k′, ik′) /∈C(t). Then Lemma 6 implies that either θk,ik(t′) =
αk,ik, ∀t′ ∈I or θk,ik(t′) = βk,ik, ∀t′ ∈I. Let us assume, without any loss of the generality,
the former case and let us consider some t ∈I. Since Sk,ik ∈Sk, i.e., αk,ik ≤
1
2, that implies
1 −2θk,ik(t) ≥0. Next, Let us consider the value of (−1)k−1Dk,i(θ). We are going to prove by
abduction that (−1)k−1Dk,i(θ) ≤0."
X,0.9444444444444444,"If (−1)k−1Dk,i(θ) > 0 then by Equation D10 we have that ˙θk,ik(t) = (−1)k−1 2"
X,0.9458689458689459,"nk θk,i(1 −
θk,i)Dk,i(θ) ̸= 0, where the last inequality follows by Assumption 3. By the continuity θ(t), it
follows that ∃ϵ0 > 0 such that ∀ϵ ∈(0, ϵ0), θk,ik(t + ϵ) ̸= θk,ik(t) = αk,ik =⇒(k, ik) /∈C(t);
that is, indeed, a contradiction. It must then be the case that (−1)k−1Dk,i(θ) ≤0 and, thus, we
have that"
X,0.9472934472934473,"˙Vi1,i2(θ(t)) =
nk′(1 −2θk′,i(t))
θk′,i(t)(1 −θk′,i(t)) · (−1)k′−1 2"
X,0.9487179487179487,"nk′ θk′,i(t)(1 −θk′,i(t))Dk′,i(θ(t))"
X,0.9501424501424501,"= 2 · (−1)k′−1 · (1 −2θk′,i(t))Dk′,i(θ(t)) = 2 · (−1) · (−1)k−1 · (1 −2θk,i(t))Dk,i(θ(t)) ≥0 ."
X,0.9515669515669516,"Thus, we showed that in every case ˙Vi1,i2(θ(t)) ≥0, ∀t ∈I. However, since θ(t) is continuous
(since it is differentiable), we can extend this property for any t ≥0, i.e., ˙Vi1,i2(θ(t)) ≥0, ∀t ≥0.
Finally, notice that Vi1,i2(θ(t)) is bounded, and, hence, it follows that −Vi1,i2(θ(t)) is a Lyapunov
function (up to a constant) of the dynamical system described by Equation D10, and, Theorem 5
follows by the deﬁnition of a Lyapunov function."
X,0.9529914529914529,"B
SUPPLEMENTARY EXPERIMENTAL RESULTS"
X,0.9544159544159544,"B.1
EXPERIMENTAL RESULTS FOR 2-TEAM HIDDEN MATCHING PENNIES GAMES"
X,0.9558404558404558,"In this section we present exemplary settings for the dynamics presented in section 5.
For an
overview of the conditions that characterize the behaviors presented in these examples, we refer-"
X,0.9572649572649573,Published as a conference paper at ICLR 2022
X,0.9586894586894587,"Figure 3: An example of the RD of a 2-Team Hidden Matching Pennies (Center) with no
restrictions applied in the parameters space (θ, 1 −θ) in (Left), and (Right). In this case, the
RD cycles for any initial point (θ(0), 1 −θ(0)). The evolution of θ(t), as a function of time,
t ∈[0, 100] is depicted in (Bottom) along with the Lyapunov function given in (31)."
X,0.9601139601139601,"ence the interested reader to section 5; for formal deﬁnitions and the proofs of these concepts, see
§A.4."
X,0.9615384615384616,"Let G = (n := n1 + n2, S := {0, 1}n, u : S →Rn) be a 2-Team Hidden Matching Pennies game
with n1 = n2 = 3 and payoff function given by (16). Figure 3 depicts the behavior of the RD in
an unrestricted instance of G, i.e., Sk,i = [0, 1] for all i ∈[3], k ∈[2]. The restrictions applied to
team 1 and team 2 are visualized by the feasible range of each member’s strategies (Figure 3 (Left),
and (Right), respectively). The black dot in each range indicates the initial strategy of each member,
i.e., θk,i(0). Note that, by Assumption 3, this strategy proﬁle, (θ(0), 1 −θ(0)), is assumed to lie
inside the feasible region enforced by the constraints Sk,i."
X,0.9629629629629629,"We solve the initial value problem of the ODE that corresponds to this RD using the RADAU in-
tegration method implemented by the scipy package in Python 3, and we perform 500 evaluations
over the time interval t ∈[0, 100]. The orbit of the RD with initial parametrization θ(0) is depicted
as a curve on the F × G space deﬁned by the hidden mappings in (18) (Figure 3 (Center)). The red
dot indicates the the point (f(θ1(t)), g(θ2(t)) at the end of the simulation, i.e., at time t = 100. The
corresponding strategies of each member are indicated by blue (team 1) and orange (team 2) dots
inside the corresponding feasible regions (Figure 3 (Left), and (Right)). The individual trajectory of
each θk,i, i ∈[3], k ∈[2] is depicted in Figure 3 (Bottom), where the curve labeled TkPi corre-
sponds to the trajectory of the i-th member of team k if k = 1, or the i −n1-th member of team k if
k = 2. In the case that the RD cycle, e.g., in the unrestricted setting, we depict one of the Lyapunov
functions of the RD (Figure 3 (Bottom)). Speciﬁcally, the Lyapunov function of our choice is"
X,0.9643874643874644,V (t) :=
X,0.9658119658119658,"2
X k=1"
X,0.9672364672364673,"1
|Sk| X"
X,0.9686609686609686,"i∈Sk
nk · [log(θk,i(t)) + log(1 −θk,i(t))],
(31)"
X,0.9700854700854701,"the average value of all the linearly independent Lyapunov functions deﬁned in (19). We selected
this speciﬁc Lyapunov function as the average is, in general, more numerically stable and for com-
pactness."
X,0.9715099715099715,Published as a conference paper at ICLR 2022
X,0.9729344729344729,"In Figure 3, we verify that the RD of G, indeed, cycle in an unrestricted setting (Theorem 3).
The choice of initial points does not matter in this case; for completeness, we note that θ1 =
(0.1, 0.9, 0.2), and θ2 = (0.1, 0.9, 0.8) in all of the examples in this section."
X,0.9743589743589743,"Figure 4: An example of the RD of a 2-Team Hidden Matching Pennies (Center) in a re-
stricted setting where 1"
X,0.9757834757834758,"2 /∈Sk,i for all i ∈[3], k ∈[2] ((Left), and (Right)). In this case,
the RD converges for any initial interior point (θ(0), 1 −θ(0)). The evolution of θ(t), as a
function of time, t ∈[0, 100] is depicted in (Bottom)."
X,0.9772079772079773,"As the ﬁrst example in a restricted setting (Figure 4), we are going to enforce S1,1 = S1,3 =
S2,1 = [0.05, 0.4], and S1,2 = S2,2 = S2,3 = [0.6, 0.95]. First, observe that Sk,i ⊂(0, 1) for all
i ∈[3], k ∈[2]; hence, Assumption 3 is satisﬁed. Next, notice that 1"
X,0.9786324786324786,"2 /∈Sk,i for all i ∈[3], k ∈[2].
It follows, by Theorem 4, that the RD should converge in this case (Figure 4 (Center), and (Bottom)),
and the point of convergence is, indeed, θ∗
1 = (0.4, 0.6, 0.4), and θ∗
2 = (0.05, 0.95, 0.95), as given
in Theorem 4."
X,0.98005698005698,"Figure 5: An example of the RD of a 2-Team Hidden Matching Pennies (Center) in a re-
stricted setting where 1"
X,0.9814814814814815,"2 ∈S1,i, and 1"
X,0.9829059829059829,"2 /∈S2,i for all i ∈[3] ((Left), and (Right)). In this case
the RD converges for any initial interior point (θ(0), 1 −θ(0)), but the point of convergence
depends on the value of θ(0). The evolution of θ(t), as a function of time, t ∈[0, 100] is
depicted in (Bottom)."
X,0.9843304843304843,Published as a conference paper at ICLR 2022
X,0.9857549857549858,"There exists a similar setting (Figure 5), where the RD of G converge, as well. Note, in this case,
while 1"
X,0.9871794871794872,"2 /∈S2,i for all i ∈[3], we let 1"
X,0.9886039886039886,"2 ∈S1,i for some i ∈[3] (speciﬁcally, 1"
X,0.99002849002849,"2 ∈S1,i for all
i ∈[3] in this particular example). Although we did not provide a convergence analysis for this
case, it is not difﬁcult to see that the convergence guarantees from Theorem 4 can be generalized
to include this case. As is visualized in Figure 5 (Center), the exhibited behavior is a hybrid of the
two behaviors described by Theorem 4, and Theorem 5. In particular, the behavior of the dynamics
depends on the quadrant of the F × G space that the θ(t) lies. Ultimately though, the dynamics can
be shown to converge to a point θ∗, which depends not only on the initialization θ(0) but also on
the restrictions in the space parameters, Sk,i, i ∈[3], k ∈[2]. The particular point of convergence
for this example is θ∗
1 = (0.05, 0.95, 0.95), and θ∗
2 = (0.4, 0.6, 0.6), where the space restrictions
are given as S2,1 = [0.05, 0.4], S2,2 = S2,3 = [0.6, 0.95], and S1,i = [0.05, 0.95] for all i ∈[3]."
X,0.9914529914529915,"Figure 6: An example of the RD of a 2-Team Hidden Matching Pennies (Center) in a re-
stricted setting where 1"
X,0.9928774928774928,"2 ∈S1,i, and 1"
X,0.9943019943019943,"2 ∈S2,j for some i, j ∈[3] ((Left), and (Right)). In
this case the RD converge to a cycle deﬁned by, at least, |S1| + |S2| −1 linearly independent
invariant functions for any initial interior point (θ(0), 1 −θ(0)) The evolution of θ(t), as a
function of time, t ∈[0, 100] is depicted in (Bottom)."
X,0.9957264957264957,"Finally, we present an example of cycling behavior in the restricted setting (Figure 6) where we
applied the restrictions S1,1 = S1,3 = [0.05, 0.8], S1,2 = S2,2 = [0.6, 0.95], S2,1 = [0.05, 0.4], and
S2,3 = [0.2, 0.95]. Note that 1"
X,0.9971509971509972,"2 ∈S1,1, S1,3, S2,3; hence, |S1| + |S2| = 3, and there exists ik ∈[nk]
for all k ∈[2] such that 1"
X,0.9985754985754985,"2 ∈Sk,ik. By Theorem 5, it follows that the restricted RD cycle in this case;
speciﬁcally, they converge to an invariant set deﬁned by V1,3(θ) = 0, and V3,3(θ) = 0 as given
by (19). This behavior is depicted in Figure 6 (Center). Notice how the Lyapunov function, V (θ)
(Figure 6 (Bottom)) is monotonically increasing, and stabilizes at approximately t = 25."
