Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002457002457002457,"Regression that predicts continuous quantity is a central part of applications using
computational imaging and computer vision technologies. Yet, studying and un-
derstanding self-supervised learning for regression tasks – except for a particular
regression task, image denoising – have lagged behind. This paper proposes a gen-
eral self-supervised regression learning (SSRL) framework that enables learning
regression neural networks with only input data (but without ground-truth target
data), by using a designable operator that encapsulates domain knowledge of a
speciﬁc application. The paper underlines the importance of domain knowledge
by showing that under some mild conditions, the better designable operator is
used, the proposed SSRL loss becomes closer to ordinary supervised learning loss.
Numerical experiments for camera image denoising and low-dose computational
tomography denoising demonstrate that proposed SSRL signiﬁcantly improves
the denoising quality over several existing self-supervised denoising methods."
INTRODUCTION,0.004914004914004914,"1
INTRODUCTION"
INTRODUCTION,0.007371007371007371,"Deep regression neural network (NN)-based methods that can accurately predict real- or complex-
valued output have been rapidly gaining popularity in a wide range of computational imaging and
computer vision applications including image denoising (Vincent et al., 2010; Xie et al., 2012; Zhang
et al., 2017), image deblurring (Xu et al., 2014), image super-resolution (Dong et al., 2016; Kim
et al., 2016), light-ﬁeld reconstruction (Chun et al., 2020; Huang et al., 2020), object localization
(Szegedy et al., 2013), end-to-end autonomous driving (Bojarski et al., 2016). Yet, they lack a
general self-supervised learning framework."
INTRODUCTION,0.009828009828009828,"In training a regression NN f : RN →RM, the most prevalent supervised learning approach
minimizes the mean square error (MSE) between what f predicts from an input x ∈RN and a
ground-truth target y ∈RM:
min
f
Ex,y∥f(x) −y∥2
2.
(1)"
INTRODUCTION,0.012285012285012284,"Learning a denoising or reﬁning NN uses (1) with M = N – dubbed Noise2True – where x is
a corrupted image and y is a clean (i.e., ground-truth) image. However, it is challenging or even
impossible to collect many clean images y in many practical applications, motivating research on
self-supervised learning for image denoising (Ulyanov et al., 2018; Soltanayev & Chun, 2018; Krull
et al., 2019; Batson & Royer, 2019; Laine et al., 2019; Moran et al., 2020; Quan et al., 2020; Xu
et al., 2020; Hendriksen et al., 2020; Xie et al., 2020; Huang et al., 2021) – called self-supervised
image denoising. To learn a denoiser f with single noisy images, a popular self-supervised image
denoising method, Noise2Self (Batson & Royer, 2019) (see also the concurrent work (Krull et al.,
2019)), and its sophisticated relaxation, Noise2Same (Xie et al., 2020), study the following MSE
minimization problem:
min
f
Ex∥f(x) −x∥2
2.
(2)"
INTRODUCTION,0.014742014742014743,"These methods use some partitioning schemes in (2) to avoid that its optimal solution is just the
identity mapping I. Noise2Noise (Lehtinen et al., 2018) that learns a denoiser with pairs of two"
INTRODUCTION,0.0171990171990172,"This article has appendix and supplement. The appendix and supplement number sections, ﬁgures, and
tables with the preﬁx “A” and “S”, respectively."
INTRODUCTION,0.019656019656019656,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.022113022113022112,"independent noisy images, is a pioneer work for self-supervised image denoising. Motivated by
Noise2Noise, several self-supervised image denoising methods such as Noise2Inverse (Hendriksen
et al., 2020), Neighbor2Neighbor (Huang et al., 2021), and (Soltanayev & Chun, 2018; Moran et al.,
2020; Xu et al., 2020) emulate pairs of two independent noisy images, by applying partitioning or
adding simulated noise to single noisy measurements. All the aforementioned methods have been
developed based on some noise assumptions including pixel-wise independent noise (Krull et al.,
2019; Xie et al., 2020; Huang et al., 2021) and zero-mean noise (Lehtinen et al., 2018; Batson &
Royer, 2019; Xie et al., 2020). Yet, they lack design ﬂexibility that might relax such noise as-
sumptions and further improve the denoising performance of NNs. Some works estimate statistical
parameters of noise, such as noise histogram (Krull et al., 2020) and parameters of Gaussian mixture
noise model (Prakash et al., 2021)."
INTRODUCTION,0.02457002457002457,"This paper presents new insights on this topic. The paper proposes a general self-supervised learning
framework for regression problems, which we refer to as self-supervised regression learning (SSRL).
Proposed SSRL enables learning regression NNs with only input samples, by using a designable op-
erator that can encapsulate domain knowledge of a speciﬁc application. Our main results show that
under some mild conditions (e.g., in image denoising, statistical noise properties in x), the better
desinable operator is used, the proposed SSRL loss becomes closer to ordinary supervised learning
loss. In addition, a designable operator with good domain knowledge can relax noise assumptions of
existing self-supervised denoising methods. Numerical experiments for camera image and low-dose
computational tomography (CT) denoising with both simulated and real-world datasets – corrupted
by only single noise realization – demonstrate that the proposed SSRL framework signiﬁcantly
improves denoising quality compared to several existing self-supervised denoising methods. Put
together, our ﬁndings provide new insights into how using good domain knowledge can improve
self-supervised denoising, underscoring the beneﬁts of understanding application-speciﬁc knowl-
edge in SSRL. (Section S.1 further elaborates the contributions of the paper.)"
SSRL USING DOMAIN KNOWLEDGE,0.02702702702702703,"2
SSRL USING DOMAIN KNOWLEDGE"
SSRL USING DOMAIN KNOWLEDGE,0.029484029484029485,The proposed SSRL loss is given by
SSRL USING DOMAIN KNOWLEDGE,0.03194103194103194,"Ex∥f(x) −g(x)∥2
2,
(3)"
SSRL USING DOMAIN KNOWLEDGE,0.0343980343980344,"where g : RN →RM is a designable operator encapsulating domain knowledge of a speciﬁc
application. We will incorporate some sophisticated setups in (3) such that f obtained by minimizing
(3) cannot merely be g. Although related theorems (see later) hold for any M, we mainly focus on
practical image denoising applications with pseudo-target g(x) ̸≈y."
MOTIVATION,0.036855036855036855,"2.1
MOTIVATION"
MOTIVATION,0.03931203931203931,"PSNR = 25.8 dB
PSNR = 23.1 dB"
MOTIVATION,0.04176904176904177,"Figure 1: Error map comparisons of denoised im-
ages from (3) using g(x) = median(x) (left) and
g(x) = BM3D(x) (right) (blue and yellow de-
note 0 and 50 absolute errors, respectively). Peak
signal-to-noise ratio (PSNR) values are averaged."
MOTIVATION,0.044226044226044224,"This section empirically shows that understand-
ing domain knowledge is important for design-
ing g in the proposed SSRL loss (3).
The
following camera image denoising examples
demonstrate that well-designed g with good do-
main knowledge improves the denoising perfor-
mance of learned f via (3)."
MOTIVATION,0.04668304668304668,"Suppose that camera images are corrupted by
salt-and-pepper noise. Consider two example
setups for g(·), median ﬁltering and BM3D
denoiser (M¨akinen et al., 2020), denoted by
median(·) and BM3D(·), respectively.
Fig-
ure 1 compares the denoising performance of
minimum f ⋆with the two aforementioned g se-
tups: f ⋆with median ﬁltering signiﬁcantly im-
proved that with BM3D denoiser. This is not surprising, as median ﬁltering is widely known to be
effective in reducing salt-and-pepper noise (Bovik, 2010, §3.2). This result emphasizes the impor-
tance of understanding domain knowledge of speciﬁc applications in proposed SSRL."
MOTIVATION,0.04914004914004914,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.051597051597051594,"2.2
PRELIMINARIES"
PRELIMINARIES,0.05405405405405406,"We ﬁrst introduce the J -complement between two functions f and g:
Deﬁnition 1. For a given partition J = {J1, . . . , JB} (|J1| + . . . + |JB| = N) of the dimensions
of input x ∈RN, functions f : RN →RM and g : RN →RM are called J -complementary, if
f(xJc) does not depend on g(xJ) for all J ∈J , where Jc denotes the complement of J, and (·)J
denotes a vector restricted to J."
PRELIMINARIES,0.056511056511056514,"That is, f and g use information from outside and inside of J to predict output and give pseudo-
target, respectively. In denoiser learning (where M = N), Deﬁnition 1 specializes to the J -
invariance of f (Batson & Royer, 2019), by setting g = I. Incorporating Deﬁnition 1 into the
SSRL loss (3) is a straightforward approach to avoid that optimal f is just g in (3). The proposed
SSRL framework assumes the followings:"
PRELIMINARIES,0.05896805896805897,"Assumption 1) xJ and xJc are conditionally independent given y, i.e., p(x|y) = p(xJ|y)p(xJc|y).
Assumption 2) E[g(x)|y] = y.
Assumption 3) f and g are (Borel-)measurable."
PRELIMINARIES,0.06142506142506143,"In denoiser learning, Assumption 1 holds if noise in each subset J ∈J is conditionally independent
from that in Jc, given y. Assumption 1 can be satisﬁed in general regression NN learning, by adding
small randomized perturbations (independent of y) to either J or Jc, similar to Moran et al. (2020).
Assumption 2 suggests a direction for designing g: suppose that x has non-zero-mean noise; one
then can design g to make noise zero-mean using the domain knowledge. Assumption 3 is satisﬁed
if f and g are continuous. This condition is mild because many regression NNs f are continuous –
where their modules, convolution, matrix-vector multiplication, rectiﬁed linear unit activation, max
pooling, etc. are continuous – and one can design g with measurable or continuous function."
PRELIMINARIES,0.06388206388206388,"Finally, observe that the proposed SSRL loss (3) can be rewritten by"
PRELIMINARIES,0.06633906633906633,"Ex∥f(x) −g(x)∥2
2 = Ex,y∥f(x) −y∥2
2 + ∥g(x) −y∥2
2 −2⟨f(x) −y, g(x) −y⟩.
(4)"
PRELIMINARIES,0.0687960687960688,"We aim to either remove or control the third term, incorporating the J -complement and/or Assump-
tions 1-3 introduced above."
SSRL USING DOMAIN KNOWLEDGE WITH J -COMPLEMENT,0.07125307125307126,"2.3
SSRL USING DOMAIN KNOWLEDGE WITH J -COMPLEMENT"
SSRL USING DOMAIN KNOWLEDGE WITH J -COMPLEMENT,0.07371007371007371,"This section studies SSRL loss (3) minimization over f that is J -complementary of g. Our ﬁrst
main result shows that under Assumptions 1–3, the SSRL loss (3) with the J -complement is the
sum of the ordinary supervised learning loss and variance of g(x) −y, i.e., the third term in (4)
vanishes.
Theorem 2. Under Assumptions 1–3, the SSRL loss (3) with the J -complement in Deﬁnition 1
becomes
Ex∥f(x) −g(x)∥2
2 = Ex,y∥f(x) −y∥2
2 + ∥g(x) −y∥2
2.
(5)
The following equality similarly holds for any K ∈K: Ex∥f(x)K −g(x)K∥2
2 = Ex,y∥f(x)K −
yK∥2
2 + ∥g(x)K −yK∥2
2, where K is a partition of {1, . . . , M}, and f(·)K and g(·)K denote f(·)
and g(·) restricted to K, respectively. The optimal solution for (5) is given by"
SSRL USING DOMAIN KNOWLEDGE WITH J -COMPLEMENT,0.07616707616707617,"f ⋆(xJc) = E[g(xJ)|xJc] = E[y|xJc].
(6)"
SSRL USING DOMAIN KNOWLEDGE WITH J -COMPLEMENT,0.07862407862407862,Proof. See Section A.1 in the appendix.
SSRL USING DOMAIN KNOWLEDGE WITH J -COMPLEMENT,0.08108108108108109,"The result (6) suggests another direction for designing g: we aim to design good g that can make
g(xJ) close to y. Using such g, optimal solution of LHS in (6), E[g(xJ)|xJc], becomes close to its
supervision counterpart, E[y|xJc]. Consequently, such g reduces the second term Ex,y∥g(x)−y∥2
2 in
RHS of (5), leading (3) closer to (1). If g is ideal such that g(xJ) = y, the SSRL loss (3) becomes the
usual supervised learning loss. In designing g, domain knowledge of speciﬁc application is crucial.
Domain knowledge includes noise properties in x and pre-trained NN by existing self-supervised
denoising, such as Noise2Self (Batson & Royer, 2019) and Noise2Noise (Lehtinen et al., 2018)."
SSRL USING DOMAIN KNOWLEDGE WITH J -COMPLEMENT,0.08353808353808354,"Speciﬁcally, the proposed SSRL loss using the J -complement is given by"
SSRL USING DOMAIN KNOWLEDGE WITH J -COMPLEMENT,0.085995085995086,"Lind(f) =
∆X"
SSRL USING DOMAIN KNOWLEDGE WITH J -COMPLEMENT,0.08845208845208845,"J∈J
Ex∥f(xJc) −g(xJ)∥2
2.
(7)"
SSRL USING DOMAIN KNOWLEDGE WITH J -COMPLEMENT,0.09090909090909091,Under review as a conference paper at ICLR 2022
SSRL USING DOMAIN KNOWLEDGE WITH J -COMPLEMENT,0.09336609336609336,"Figure 2: Proposed SSRL models using the J -complement in denoiser learning. Top: f and g
use almost equal amount of information from input, i.e., |J| ≈|Jc|, where J and Jc are comple-
mentary checkerboard masks. Bottom: f and g use unbalanced amount of information from input,
speciﬁcally, |Jc| ≫|J|."
SSRL USING DOMAIN KNOWLEDGE WITH J -COMPLEMENT,0.09582309582309582,"Figure 2 illustrates (7) with complementary checkerboard masks Jc and J, where f and g use almost
equal amount of information, and its variant, where g uses much less information than f. The variant
computes MSE only on J ∈J ; in this setup, it is challenging for g to predict the entire image."
SSRL USING DOMAIN KNOWLEDGE WITH J -COMPLEMENT,0.09828009828009827,"Relation to previous self-supervised denoising works.
In denoiser learning, the proposed SSRL
loss (3) with the J -complement of f and g = I specializes to (2) with the J -invariance of f,
i.e., Noise2Self. Noise2Inverse (Hendriksen et al., 2020) and Neighbor2Neighbor (Huang et al.,
2021) that emulate pairs of two independent noisy images by partitioning single measurements
(e.g., CT ray measurements with independent noise and corrupted images with pixel-wise inde-
pendent noise) can be viewed as Noise2Self. Thus, SSRL loss (3) with the J -complement of f and
g = I specializes to the aforementioned Noise2Noise-motivated self-supervised denoising methods.
(Noise2Noise also can be viewed by SSRL (3) with the setup above, by constructing x with stacking
two independent noisy images, where an image is corrupted by two independent noise realizations.)"
SSRL USING DOMAIN KNOWLEDGE WITHOUT J -COMPLEMENT,0.10073710073710074,"2.4
SSRL USING DOMAIN KNOWLEDGE WITHOUT J -COMPLEMENT"
SSRL USING DOMAIN KNOWLEDGE WITHOUT J -COMPLEMENT,0.10319410319410319,"This section studies the SSRL loss (3) without using the J -complement in Deﬁnition 1.
Ob-
serve by (4) that Ex,y∥f(x) −y∥2
2 + ∥g(xJ) −y∥2
2 = Ex∥f(x) −g(xJ)∥2
2 + 2Ex,y⟨f(x) −
y, g(xJ) −y⟩. Inspired by Noise2Same (Xie et al., 2020), the second proposed SSRL approach
is to minimize an approximation of the right hand side in this equation that does not assume
that f and g are J -complementary. Our second main result ﬁnds an upper bound for the term
Ex,y⟨f(x) −y, g(xJ) −y⟩without relying on the J -complement (remind that this term vanishes if
f and g are J -complementary; see Theorem 2)."
SSRL USING DOMAIN KNOWLEDGE WITHOUT J -COMPLEMENT,0.10565110565110565,"Theorem 3. Assume that Var(g(xJ)m|y) ≤σ2, ∀m. Under Assumptions 1–3, the following bound
holds:"
SSRL USING DOMAIN KNOWLEDGE WITHOUT J -COMPLEMENT,0.10810810810810811,"Ex,y⟨f(x) −y, g(xJ) −y⟩≤σ
√"
SSRL USING DOMAIN KNOWLEDGE WITHOUT J -COMPLEMENT,0.11056511056511056,"M ·
 
Ex∥f(x) −f(xJc)∥2
2
1/2.
(8)"
SSRL USING DOMAIN KNOWLEDGE WITHOUT J -COMPLEMENT,0.11302211302211303,Under review as a conference paper at ICLR 2022
SSRL USING DOMAIN KNOWLEDGE WITHOUT J -COMPLEMENT,0.11547911547911548,"The following bound similarly holds for any K ∈K: Ex,y⟨f(x)K −yK, g(xJ)K −yK⟩≤σ
p"
SSRL USING DOMAIN KNOWLEDGE WITHOUT J -COMPLEMENT,0.11793611793611794,"|K|·
 
Ex∥f(x)K −f(xJc)K)∥2
2
1/2, where K and K are deﬁned in Theorem 2."
SSRL USING DOMAIN KNOWLEDGE WITHOUT J -COMPLEMENT,0.12039312039312039,Proof. See Section A.2 in the appendix.
SSRL USING DOMAIN KNOWLEDGE WITHOUT J -COMPLEMENT,0.12285012285012285,"Using Theorem 3, the proposed SSRL loss that does not rely on the J -complement is given by"
SSRL USING DOMAIN KNOWLEDGE WITHOUT J -COMPLEMENT,0.12530712530712532,"L(f) =
∆X"
SSRL USING DOMAIN KNOWLEDGE WITHOUT J -COMPLEMENT,0.12776412776412777,"J∈J
Ex∥f(x) −g(xJ)∥2
2 + 2σ
√"
SSRL USING DOMAIN KNOWLEDGE WITHOUT J -COMPLEMENT,0.13022113022113022,"M ·
 
Ex∥f(x) −f(xJc)∥2
2
1/2,
(9)"
SSRL USING DOMAIN KNOWLEDGE WITHOUT J -COMPLEMENT,0.13267813267813267,"where σ is given in Theorem 3. Here, a regression NN f can use information from the entire
input x, whereas Lind(f) in (7) uses only partial input xJc in f. The intuition for designing g in
Section 2.3 similarly applies here. Our aim is to design good g such that g(xJ) is closer to y,
consequently leading (9) close to (1). Similar to the variant of Lind(f) (see Section 2.3), if the
amount of information between two partitions J and Jc is unbalanced in denoiser learning, one can
modify (9) to compute the MSE only on J or Jc in either both terms or the right term in (9). (These
variants use the second result in Theorem 3.)"
SSRL USING DOMAIN KNOWLEDGE WITHOUT J -COMPLEMENT,0.13513513513513514,"We conjecture that how well designed g is, i.e., how close is pseudo-target g(xJ) to ground-truth y,
is captured by σ deﬁned in Theorem 3. We support the conjecture with examples in Section A.3 of
the appendix. Then, this “goodness” of g balances the two terms in (9) via σ. If g is well-designed
such that g(xJ) is close to y, i.e., σ2 is small, then the SSRL loss (9) relies more on the ﬁrst term
with good pseudo-target. If g is poorly-designed such that σ2 is large, then (9) puts more weight
more on the second term that can implicitly promote the J -invariance of f."
SSRL USING DOMAIN KNOWLEDGE WITHOUT J -COMPLEMENT,0.1375921375921376,"Relation to previous self-supervised denoising work.
The proposed SSRL loss (9) becomes the
Noise2Same loss (Xie et al., 2020, Thm. 2), by replacing g(xJ) with x and adding randomness
to K = J in the second term. In practice, one can tune σ in (9) without knowing its exact value,
similar to Noise2Same. One might use the aforementioned conjectured behavior of (9) with expected
performance of g."
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.14004914004914004,"2.5
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS"
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.14250614250614252,"This section explains how proposed SSRL (3) can relax noise assumptions of existing self-
supervised denoising methods. Noise assumptions of existing self-supervised denoising methods
include additive white Gaussian noise (AWGN with known variance) (Soltanayev & Chun, 2018),
pixel-wise independent noise (Krull et al., 2019; Xie et al., 2020; Huang et al., 2021), and zero-mean
noise (Lehtinen et al., 2018; Quan et al., 2020) or more generally E[x|y] = y (Batson & Royer, 2019;
Xie et al., 2020). Assumption 1 of proposed SSRL relaxes the AWGN and pixel-wise independent
noise assumptions, and is identical to the ﬁrst assumption of Noise2Self (Batson & Royer, 2019).
Assumption 2 of proposed SSRL can relax the second assumption of Noise2Self, E[x|y] = y, that is
also (implicitly) used in Noise2Same (Xie et al., 2020) and Noise2Noise (Lehtinen et al., 2018), by
using a desinable function g. For example, x is corrupted by additive non-zero-mean noise e that is
independent of y, i.e., x = y +e, then one can design g as follows: g(x) = x−E[e], where E[e] can
be estimated from calibration of imaging systems. The next section will explain how one can design
g using domain knowledge and select g if domain knowledge is unavailable in image denoising."
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.14496314496314497,"3
EXAMPLES OF HOW TO DESIGN g USING DOMAIN KNOWLEDGE, AND
EMPIRICAL-LOSS APPROACH FOR SELECTING g IN IMAGE DENOISING"
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.14742014742014742,"Understanding noise statistics or properties is the ﬁrst step towards accurate image recovery in com-
putational imaging. First, this section describes how to use domain knowledge for designing g in
two imaging applications with practical noise models: 1) camera image denoising in mixed Poisson–
Gaussian–Bernoulli noise, 2) low-dose CT denoising. Both applications have complicated noise
models or strong noise, where Assumptions 1–2 in Section 2.2 are not completely satisﬁed. Noisy
images in the ﬁrst application are corrupted by independent and identically distributed (i.i.d.) noise
with non-zero mean. The noise in the second application is approximately zero-mean but it is likely
non-i.i.d. In designing g for each application, we will use the suggestions in Sections 2.2–2.3, in-
vestigating Assumptions 1–3. Second, we propose a g-selection approach in image denoising that
calculates some empirical measure related to (6) using only input training data."
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.14987714987714987,Under review as a conference paper at ICLR 2022
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.15233415233415235,"3.1
DESIGNING g USING DOMAIN KNOWLEDGE: CAMERA IMAGE DENOISING"
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.1547911547911548,"The major noise sources in camera imaging (using charge coupled device) include object-dependent
photoelectrons in image sensors, readout in camera electronics, and analog-to-digital converter and
transmission errors that can be modeled by Poisson noise, AWGN, and Bernoulli (i.e., salt-and-
pepper) noise models (Snyder et al., 1993), (Bovik, 2010, p. 90). We use the following practical
mixed Poisson–Gaussian–Bernoulli noise model (Snyder et al., 1993; Batson & Royer, 2019):"
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.15724815724815724,"xn = Π[0,255](Bernoullip(Poisson(λyn)/λ + ϵn)),
ϵ ∼N(0, σ2
ϵ I),
n = 1, . . . , N,
(10)"
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.1597051597051597,"where Π[0,255] performs 8-bit quantization and clips pixel values outside of [0, 255], Bernoulli sub-
stitutes a pixel value with either 0 or 255 with probability p (0 and 255 are coined with equal prob-
ability), Poisson generates pixel intensity-dependent Poisson noise with gain parameter λ, and ϵ is
AWGN. Figure 3 (left) shows a noisy image corrupted by the mixed noise model (10). If an image y
is corrupted only by the mixed Poisson–Gaussian noise, E[x|y] = y in Assumption 2 can be satisﬁed
(E[Poisson(λyn)/λ+ϵ|y] = yn, ∀n). However, if Bernoulli noise is additionally considered as given
in (10), the assumption E[x|y] = y will not be satisﬁed (E[Bernoullip(yn)|y] = (1 −p)yn + 127.5p,
∀n). The quantization-clipping operator Π[0,255] also makes it hard to satisfy E[x|y] = y."
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.16216216216216217,"PSNR = 9.8 dB
RMSE = 48.3 HU"
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.16461916461916462,"Figure 3: An input noisy image in camera im-
age denoising in mixed noise (left) and low-dose
CT (right).
PSNR and root mean square error
(RMSE) values were averaged across all test sam-
ples."
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.16707616707616707,"Following the suggestion based on Assump-
tion 2 (see Section 2.2), we handcraft g to
“approximately” satisfy E[g(x)|y] = y with
a simple operator.
We interpret aforemen-
tioned Bernoulli noise and clipping artifact as
salt-and-pepper noise.
Median ﬁltering is a
computational efﬁcient method that is effec-
tive in reducing salt-and-pepper and impulse
noises (Bovik, 2010, §3.2). We design g by ap-
plying weighted median ﬁltering (Brownrigg,
1984) to a pixel with intensity either 0 or 255
at each color channel, aiming that this g design
“approximately” satisfy E[g(x)|y] = y by sup-
pressing the salt-and-pepper noise effects cased
by Π[0,255] and Bernoullip. (This is supported
by empirical results in Section S.4.)"
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.16953316953316952,"Assumption 1 is satisﬁed because the noise in (10) is i.i.d. and independent of y. In Assumption 3,
we conjecture that the above g design is measurable (median operator is measurable under some
conditions (Rustad, 2004))."
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.171990171990172,"3.2
DESIGNING g USING DOMAIN KNOWLEDGE: LOW-DOSE CT DENOISING"
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.17444717444717445,"In X-ray CT (with a monoenergetic source), the pre-log measurement data is usually modeled by
the Poisson model, i.e., Poisson{ρ0 exp(−[Ay]l)}, l = 1, . . . , L, where ρ0 is the number of incident
photons per ray, A ∈RL×N is a CT projection system matrix, and L is the number of measured
rays. Using the quadratic approximation to the log-likelihood of a Poisson model, the post-log
measurement z ∈RL given y can be approximated as the following Gaussian model (Sauer &
Bouman, 1993; Fessler, 2000): z|y ∼N(Ay, C), where C ∈RL×L is a diagonal covariance
matrix and its diagonal elements become more nonuniform in lower-dose CT. This model suggests
that post-log measurement may be modeled by z = Ay + ε, where ε ∼N(0, C). The ﬁltered
back-projection (FBP) method (Kak & Slaney, 1988, §3) performs computationally efﬁcient CT
reconstruction and has been widely used in commercial CT scanners (Pan et al., 2009). In low-dose
CT, however, reconstructed image x = Fz suffers from strong noise and streak artifacts, where
F ∈RN×L denotes a linear FBP operator, motivating research on learning denoising NNs. Figure 3
(right) shows a noisy FBP image in low-dose CT. Using the statistical results above, we model that
a reconstructed image by F is corrupted by an arbitrary additive noise e:"
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.1769041769041769,"x = y + e,
e = (FA −I)y + Fε.
(11)"
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.17936117936117937,"Low-dose CT uses all projection rays similar to standard-dose CT (but with substantially reduced
dose) where F approximately inverts A, i.e., FA ≈I, so we conclude that under (11), E[e] ≈0 and
E[x|y] ≈y. (See empirical results in Section S.4 that support E[e] ≈0.)"
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.18181818181818182,Under review as a conference paper at ICLR 2022
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.18427518427518427,"The above domain knowledge in low-dose CT indicates that handcrafting g to have zero-mean e
to satisfy Assumption 2 can be redundant. Following the suggestion motivated by Theorem 2 (see
Section 2.3), we set g as a pre-trained denoiser by the existing self-supervised denoising methods
(Batson & Royer, 2019; Hendriksen et al., 2020; Xie et al., 2020). Since such pre-trained g will have
some denoising capability and give better reference than g = I to (7) and (9) (see empirical results
in Section S.4), we expect that proposed SSRL losses (7) and (9) improve the denoising quality over
the aforementioned existing self-supervised denoising methods."
RELAXING NOISE ASSUMPTIONS OF EXISTING SELF-SUPERVISED DENOISING METHODS,0.18673218673218672,"Assumption 1 is unlikely satisﬁed because in FBP images, neighboring noise components are likely
to be correlated, i.e., Var(e) ≈FCF ⊤using FA ≈I and noise model (11). Assumption 3 is
satisﬁed as we use the conventional denoisiong NN, DnCNN (Zhang et al., 2017) and (modiﬁed)
U-Net (Ronneberger et al., 2015), that are a continuous function."
EMPIRICAL-LOSS APPROACH FOR SELECTING G IF DOMAIN KNOWLEDGE UNAVAILABLE,0.1891891891891892,"3.3
EMPIRICAL-LOSS APPROACH FOR SELECTING g IF DOMAIN KNOWLEDGE UNAVAILABLE"
EMPIRICAL-LOSS APPROACH FOR SELECTING G IF DOMAIN KNOWLEDGE UNAVAILABLE,0.19164619164619165,"If accurate domain knowledge of a speciﬁc application is unavailable, it would be challenging to
explicitly design g. In such cases in denoising, our general suggestion is to measure an existing self-
supervised denoising loss, an upper bound of ∥g(xJ)−y∥2
2 or its variant that measure the quality of g,
only with input training data. The lower quantity implies that g is better and implicitly encapsulates
better domain knowledge. In camera image denoising with the real-world dataset (Abdelhamed
et al., 2018), the empirical measure of the Neighbor2Neighbor loss (Huang et al., 2021) – E∥g(xJ)−
xJc∥2
2 – with setting g as I and median ﬁltering are 0.0052 and 0.0048, respectively. In low-dose CT
denoising with the real-world dataset (Moen et al., 2021), the empirical measure of the Noise2Self
loss (Batson & Royer, 2019) – E∥g(xJ)Jc −xJc∥2
2 – with setting g as I and pre-trained DnCNN
by Noise2Self are 22044.5 and 17062.0 (in HU2 where HU stands for modiﬁed Hounsﬁeld unit),
respectively. We expect better SSRL performance with the selected g designs over I."
EXPERIMENTAL RESULTS AND DISCUSSION,0.1941031941031941,"4
EXPERIMENTAL RESULTS AND DISCUSSION"
EXPERIMENTAL RESULTS AND DISCUSSION,0.19656019656019655,"We evaluated proposed SSRL in two practical imaging applications in Section 3 with both simu-
lated and real-world datasets. For these applications, we mainly focuses on comparisons with self-
supervised denoising methods using single noisy input samples, particularly when statistical noise
parameters are unavailable. We compared the performances of the following methods: Noise2Self
(Batson & Royer, 2019), Noise2Noise-motivated methods that emulate pairs of two independent
noisy images – Neighbor2Neighbor (Huang et al., 2021) or Noise2Inverse (Hendriksen et al., 2020)
– Noise2Same (Xie et al., 2020), and corresponding SSRL to each aforementioned method. We also
included Noise2True (1) results as baseline. For f or g in all the methods, we used the conventional
denoising NN architecture, DnCNN (Zhang et al., 2017) or modiﬁed U-Net used in Noise2Self. We
include experiment setup, and image and numerical results for/from real-world datasets in Sec. A.4."
EXPERIMENTAL SETUP FOR SIMULATED DATASETS,0.19901719901719903,"4.1
EXPERIMENTAL SETUP FOR SIMULATED DATASETS"
EXPERIMENTAL SETUP FOR SIMULATED DATASETS,0.20147420147420148,"Camera image denoising in mixed noise.
We evaluated the proposed SSRL framework with
three RGB camera image datasets, ImageNet ILSVRC 2012 Val (Russakovsky et al., 2015), BSD
300 (Martin et al., 2001), Set 5 (Bevilacqua et al., 2012). For training, we used the ImageNet
ILSVRC 2012 Val dataset with 20,000 images; for tests, we used the BSD 300 and Set 5 datasets
consisting of 300 and 5 images, respectively. We simulated noisy images with the following imaging
parameters introduced in (10): λ = 30, σϵ = 60, and p = 0.2. We evaluated the denoising quality
by the most conventional error metric in camera image denoising, PSNR and structural similarity
index measure (SSIM). In Neighbor2Neighbor setup, we emulated two independent noisy images
from single noisy images by random neighbor sub-sampling with 2×2-window (Huang et al., 2021)."
EXPERIMENTAL SETUP FOR SIMULATED DATASETS,0.20393120393120392,"Low-dose CT denoising.
We evaluated the proposed SSRL framework with The 2016 Low Dose
CT Grand Challenge data (McCollough, 2016). We selected 200 regular-dose chest images of size
N = 512 × 512 and the 3 mm slice thickness from four patients. For training, we used 170 (85%)
chest images from three patients; for tests, we used 30 (15%) chest images from the other patient. We
simulated low-dose sinograms using the Poisson model with the selected regular-dose chest datasets.
In particular, we simulated sinograms of size L = 736 × 1152 (‘detectors’ × ‘projection views’),"
EXPERIMENTAL SETUP FOR SIMULATED DATASETS,0.20638820638820637,Under review as a conference paper at ICLR 2022
EXPERIMENTAL SETUP FOR SIMULATED DATASETS,0.20884520884520885,"Reference
Noise2True
Noise2Self
Proposed SSRL in
Noise2Self setup"
EXPERIMENTAL SETUP FOR SIMULATED DATASETS,0.2113022113022113,"PSNR = 25.2 dB
SSIM = 0.819"
EXPERIMENTAL SETUP FOR SIMULATED DATASETS,0.21375921375921375,"PSNR = 20.6 dB
SSIM = 0.711"
EXPERIMENTAL SETUP FOR SIMULATED DATASETS,0.21621621621621623,"PSNR = 22.1 dB
SSIM = 0.748"
EXPERIMENTAL SETUP FOR SIMULATED DATASETS,0.21867321867321868,"Neighbor2Neighbor
Proposed SSRL in
Neighbor2Neighbor setup
Noise2Same
Proposed SSRL in
Noise2Same setup"
EXPERIMENTAL SETUP FOR SIMULATED DATASETS,0.22113022113022113,"PSNR = 20.2 dB
SSIM = 0.703"
EXPERIMENTAL SETUP FOR SIMULATED DATASETS,0.22358722358722358,"PSNR = 22.3 dB
SSIM = 0.756"
EXPERIMENTAL SETUP FOR SIMULATED DATASETS,0.22604422604422605,"PSNR = 20.1 dB
SSIM = 0.690"
EXPERIMENTAL SETUP FOR SIMULATED DATASETS,0.2285012285012285,"PSNR = 21.2 dB
SSIM = 0.705"
EXPERIMENTAL SETUP FOR SIMULATED DATASETS,0.23095823095823095,"Figure 4: Comparisons of denoised images (left) via DnCNNs from different learning methods and
their saturation error maps (right) in camera image denoising (blue and yellow denote 0 and 0.5
absolute error, respectively). PSNR & SSIM values were averaged across all BSD 300 test samples."
EXPERIMENTAL SETUP FOR SIMULATED DATASETS,0.2334152334152334,"with fan-beam geometry corresponding to a no-scatter monoenergetic source with ρ0 = 5 × 104.
We used FBP (Kak & Slaney, 1988, §3) to reconstruct images with resolution 0.69 mm × 0.69 mm.
We evaluated the denoising quality by the most conventional error metric in CT application, RMSE
in HU. In Noise2Inverse setup, we emulated two independent noisy images by partitioning single
sinograms with odd and even views and applying FBP to two partitioned independent sinograms."
COMPARISONS BETWEEN DIFFERENT SELF-SUPERVISED DENOISING METHODS,0.23587223587223588,"4.2
COMPARISONS BETWEEN DIFFERENT SELF-SUPERVISED DENOISING METHODS"
COMPARISONS BETWEEN DIFFERENT SELF-SUPERVISED DENOISING METHODS,0.23832923832923833,"Compare each existing self-supervised denoising method to its corresponding SSRL setup in Fig-
ures 4–5, and Figures S.1–S.4 and Tables S.1–S.3; see three comparison sets, each grouped by
red box. For both applications, proposed SSRL achieves signiﬁcantly better image denoising qual-
ity, i.e., closer to the Noise2True quality, compared to the existing methods, Noise2Self, Neigh-
bor2Neighbor, Noise2Inverse, and Noise2Same, regardless of the regression NN architecture. We
show DnCNN prediction uncertainty of all the methods in Figure S.5."
COMPARISONS BETWEEN DIFFERENT SELF-SUPERVISED DENOISING METHODS,0.24078624078624078,"Camera image denoising in mixed noise (simulated data).
Figures 4 and S.3 show that in
all the three comparison sets, SSRL gives closer image quality, particularly color saturation, to
Noise2True than existing methods, Noise2Self, Neighbor2Neighbor, and Noise2Same. Setting g as
weighted median ﬁltering avoids bias in SSRL loss caused by salt-and-pepper noise. Yet, compared
to Noise2True, denoised images obtained by proposed SSRL lack saturation and detail preservation.
For saturation and detail preservation comparisons, see Figures 4, S.1 and S.3."
COMPARISONS BETWEEN DIFFERENT SELF-SUPERVISED DENOISING METHODS,0.24324324324324326,"Comparing the three comparison sets in Figures 4 and S.3, and Tables S.1–S.2 shows that all the
Noise2Self, Neighbor2Neighbor, and Noise2Self setups have comparable results in terms of PSNR
values. The potential reason is that in this application, all the three setups similarly satisfy Assump-
tions 1–3 (see Section 2.2); in particular, Assumption 1 is well-satisﬁed by pixel-wise i.i.d. noise."
COMPARISONS BETWEEN DIFFERENT SELF-SUPERVISED DENOISING METHODS,0.2457002457002457,"In this application, the zero-mean noise assumption of the existing self-supervised denoising meth-
ods is violated, whereas its counterpart in proposed SSRL, Assumption 2, is “approximately” satis-
ﬁed by g in Section 3.1. This suggests the importance of satisfying Assumption 2 with good g."
COMPARISONS BETWEEN DIFFERENT SELF-SUPERVISED DENOISING METHODS,0.24815724815724816,"Low-dose CT denoising (simulated data).
In all the three comparison sets, SSRL better recovers
low-contrast regions (e.g., soft tissues) and small details, and signiﬁcantly reduces noise and artifacts
throughout the image, over existing methods, Noise2Self, Noise2Inverse, and Noise2Same. See"
COMPARISONS BETWEEN DIFFERENT SELF-SUPERVISED DENOISING METHODS,0.25061425061425063,Under review as a conference paper at ICLR 2022
COMPARISONS BETWEEN DIFFERENT SELF-SUPERVISED DENOISING METHODS,0.25307125307125306,"Reference
Noise2True
Noise2Self
Proposed SSRL in
Noise2Self setup"
COMPARISONS BETWEEN DIFFERENT SELF-SUPERVISED DENOISING METHODS,0.25552825552825553,"RMSE = 16.3 HU
RMSE = 36.2 HU
RMSE = 25.0 HU"
COMPARISONS BETWEEN DIFFERENT SELF-SUPERVISED DENOISING METHODS,0.257985257985258,"Noise2Inverse
Proposed SSRL in
Noise2Inverse setup
Noise2Same
Proposed SSRL in
Noise2Same setup"
COMPARISONS BETWEEN DIFFERENT SELF-SUPERVISED DENOISING METHODS,0.26044226044226043,"RMSE = 22.9 HU
RMSE = 21.9 HU
RMSE = 28.4 HU
RMSE = 26.0 HU"
COMPARISONS BETWEEN DIFFERENT SELF-SUPERVISED DENOISING METHODS,0.2628992628992629,"Figure 5: Comparisons of denoised images via DnCNNs from different learning methods in low-
dose CT (display window is [800, 1200] HU). RMSE values were averaged across all test samples."
COMPARISONS BETWEEN DIFFERENT SELF-SUPERVISED DENOISING METHODS,0.26535626535626533,"zoom-ins and circled small details in Figures 5 and S.4, and error images in Figure S.2, particularly
in ‘Noise2Self vs. Propose SSRL in Noise2Self setup’ and ‘Noise2Same vs. Proposed SSRL in
Noise2Same setup.’ The results might imply that simply setting g as pre-trained NN by existing self-
supervised denoising methods works like a charm in SSRL. Proposed SSRL in the Noise2Inverse
setup can provide images with image quality that is comparable to conventional FBP at 10 times
higher dose (when ρ0 = 5 × 105, RMSE = 20.5 HU on average; see DnCNN results in Figure 5)."
COMPARISONS BETWEEN DIFFERENT SELF-SUPERVISED DENOISING METHODS,0.2678132678132678,"Next, comparing Noise2Self and Noise2Same result sets to that of Noise2Inverse in Figures 5, S.2
and S.4, and Table S.3 shows that Noise2Inverse setup signiﬁcantly improves the denoising qual-
ity, compared to Noise2Self and Noise2Same setups. We conjecture that violation of Assump-
tion 1 – that is satisﬁed in the Noise2Inverse setup but unlikely to be satisﬁed in the Noise2Self and
Noise2Same setups in this application – degrades the performance."
COMPARISONS BETWEEN DIFFERENT SELF-SUPERVISED DENOISING METHODS,0.2702702702702703,"Camera image denoising and low-dose CT denoising with real-world datasets.
Denoised im-
age results in Figures A.2–A.3 and S.8–S.9 from the two real-world datasets (Abdelhamed et al.,
2018; Moen et al., 2021) demonstrate that SSRL improves existing self-supervised denoising meth-
ods, particularly Neighbor2Neighbor, Noise2Self, and Noise2Same, without having their exact noise
properties. The results well correspond to our expectation in Section 3.3."
CONCLUSION,0.2727272727272727,"5
CONCLUSION"
CONCLUSION,0.2751842751842752,"It is important to develop SSRL that enables comparable prediction performances to supervised
learning, because it is extremely challenging to collect many ground-truth target samples in many
practical computational imaging and computer vision applications. The proposed SSRL framework
bridges the gap between SSRL and supervised regression learning via domain knowledge of ap-
plications. To achieve closer prediction performance to supervised learning, SSRL uses domain
knowledge to design a better pseudo-predictor g such that g(xJ) becomes closer to y. For camera
image denoising and low-dose CT denoising with both simulated and real-world datasets, SSRL
achieves more accurate prediction compared to the existing self-supervised denoising methods (Bat-
son & Royer, 2019; Huang et al., 2021; Hendriksen et al., 2020; Xie et al., 2020). Remark, however,
that applying SSRL to other regressions problems may need careful investigations about g based on
their domain knowledge. Our future work is extending proposed SSRL to other machine learning
problems such as teacher-student models (see Section S.7) and meta-learning. On the application
side, our future work is applying SSRL to regression problems beyond image denoising."
CONCLUSION,0.27764127764127766,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY,0.2800982800982801,"6
REPRODUCIBILITY"
REPRODUCIBILITY,0.28255528255528256,"Section 2.2 speciﬁes all the theoretical assumptions. Section A.1 and Section A.2 in the appendix in-
clude the complete proofs of Theorem 2 and Theorem 3, respectively. Section S.2 in the supplement
includes the complete implementation details including the hyperparameter selection strategies and
the chosen hyperparameters. We included an anonymized zip ﬁle that includes test codes, test data,
trained models, and instructions and codes that provide complete description of the data processing
steps, as supplementary materials. We will make our codes (for data construction, training, and test)
and trained models publicly available on GitHub if the paper is accepted."
REFERENCES,0.28501228501228504,REFERENCES
REFERENCES,0.28746928746928746,"Abdelrahman Abdelhamed, Stephen Lin, and Michael S. Brown. A high-quality denoising dataset
for smartphone cameras. In Proc. IEEE CVPR, pp. 1692–1700, Salt Lake City, Utah, Jun. 2018."
REFERENCES,0.28992628992628994,"Joshua Batson and Loic Royer. Noise2Self: Blind denoising by self-supervision. In Proc. ICML,
pp. 524–533, Long Beach, CA, Jun. 2019."
REFERENCES,0.29238329238329236,"Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie Line Alberi-Morel.
Low-
complexity single-image super-resolution based on nonnegative neighbor embedding. In Proc.
BMVC, pp. 135.1–135.10, Surrey, UK, Sep. 2012."
REFERENCES,0.29484029484029484,"Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon
Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning
for self-driving cars. NIPS Deep Learning Symposium, Dec. 2016. arXiv:1604.07316."
REFERENCES,0.2972972972972973,"Alan C Bovik. Handbook of image and video processing. Academic Press, Florida, 2010."
REFERENCES,0.29975429975429974,"David RK Brownrigg. The weighted median ﬁlter. Commun. of the ACM, 27:807–818, 1984."
REFERENCES,0.3022113022113022,"Il Yong Chun, Zhengyu Huang, Hongki Lim, and Jeffrey A Fessler. Momentum-Net: Fast and
convergent iterative neural network for inverse problems. early access in IEEE Trans. Pattern
Anal. Mach. Intell., 2020. doi: 10.1109/TPAMI.2020.3012955."
REFERENCES,0.3046683046683047,"Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep
convolutional networks. IEEE Trans. Pattern Anal. Mach. Intell., 38(2):295–307, Feb. 2016."
REFERENCES,0.3071253071253071,"Jeffrey A Fessler.
Statistical image reconstruction methods for transmission tomography.
In
M Sonka and J Michael Fitzpatrick (eds.), Handbook of Medical Imaging, Volume 2. Medical
Image Processing and Analysis, pp. 1–70. SPIE, Bellingham, WA, 2000."
REFERENCES,0.3095823095823096,"Allard Adriaan Hendriksen, Dani¨el Maria Pelt, and K Joost Batenburg.
Noise2Inverse: Self-
supervised deep convolutional denoising for tomography. IEEE Trans. Comput. Imag., 6:1320–
1335, Aug. 2020."
REFERENCES,0.31203931203931207,"Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, and Jianzhuang Liu. Neighbor2Neighbor: Self-
supervised denoising from single noisy images. In Proc. IEEE/CVF CVPR, pp. 14781–14790,
Virtual, Jun. 2021."
REFERENCES,0.3144963144963145,"Zhengyu Huang, Jeffrey A Fessler, Theodore B Norris, and Il Yong Chun. Light-ﬁeld reconstruction
and depth estimation from focal stack images using convolutional neural networks. In Proc. IEEE
ICASSP, pp. 8648–8652, Barcelona, Spain, May 2020."
REFERENCES,0.31695331695331697,"A. C. Kak and M. Slaney. Principles of computerized tomographic imaging. IEEE Press, New York,
NY, 1988."
REFERENCES,0.3194103194103194,"Jiwon Kim, Kwon Jung Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep
convolutional networks. In Proc. IEEE CVPR, Las Vegas, NV, Jun. 2016."
REFERENCES,0.32186732186732187,"Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2Void - Learning denoising from
single noisy images. In Proc. IEEE/CVF CVPR, pp. 2129–2137, Long Beach, CA, Jun. 2019."
REFERENCES,0.32432432432432434,Under review as a conference paper at ICLR 2022
REFERENCES,0.32678132678132676,"Alexander Krull, Tom´aˇs Viˇcar, Mangal Prakash, Manan Lalit, and Florian Jug.
Probabilistic
noise2void: Unsupervised content-aware denoising. Frontiers in Computer Science, 2:5, 2020."
REFERENCES,0.32923832923832924,"Samuli Laine, Jaakko Lehtinen, and Timo Aila. High-quality self-supervised deep image denoising.
In Proc. NIPS, Vancouver, Canada, Dec. 2019."
REFERENCES,0.3316953316953317,"Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and
Timo Aila. Noise2Noise: Learning image restoration without clean data. In Proc. ICML, pp.
2965–2974, Stockholm, Sweden, Jul. 2018."
REFERENCES,0.33415233415233414,"Ymir M¨akinen, Lucio Azzari, and Alessandro Foi. Collaborative ﬁltering of correlated noise: Ex-
act transform-domain variance for improved shrinkage and patch matching. IEEE Trans. Image
Process., 29:8339–8354, Aug. 2020."
REFERENCES,0.3366093366093366,"D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its
application to evaluating segmentation algorithms and measuring ecological statistics. In Proc.
IEEE ICCV, pp. 416–423, Vancouver, Canada, July 2001."
REFERENCES,0.33906633906633904,"C. McCollough. TU-FG-207A-04: Overview of the low dose CT grand challenge. Med. Phys., 43
(6Part35):3759–3760, Jun. 2016."
REFERENCES,0.3415233415233415,"Taylor R Moen, Baiyu Chen, David R Holmes III, Xinhui Duan, Zhicong Yu, Lifeng Yu, Shuai
Leng, Joel G Fletcher, and Cynthia H McCollough. Low-dose CT image and projection dataset.
Med. Phys., 48(2):902–911, Feb. 2021."
REFERENCES,0.343980343980344,"Nick Moran, Dan Schmidt, Yu Zhong, and Patrick Coady. Noisier2Noise: Learning to denoise from
unpaired noisy data. In Proc. IEEE/CVF CVPR, pp. 12064–12072, Virtual, Jun. 2020."
REFERENCES,0.3464373464373464,"Xiaochuan Pan, Emil Y Sidky, and Michael Vannier. Why do commercial CT scanners still employ
traditional, ﬁltered back-projection for image reconstruction?
Inverse Probl., 25(12):123009,
2009."
REFERENCES,0.3488943488943489,"Mangal Prakash, Alexander Krull, and Florian Jug. Fully unsupervised diversity denoising with
convolutional variational autoencoders. In Proc. ICLR, Virtual, May. 2021."
REFERENCES,0.35135135135135137,"Yuhui Quan, Mingqin Chen, Tongyao Pang, and Hui Ji. Self2Self with dropout: Learning self-
supervised denoising from single image. In Proc. IEEE/CVF CVPR, pp. 1890–1898, Virtual, Jun.
2020."
REFERENCES,0.3538083538083538,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomed-
ical image segmentation. In Proc. Med. Image Computing and Computer Assist. Interven., pp.
234–241, Munich, Germany, Oct. 2015."
REFERENCES,0.35626535626535627,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. Int. J. Comput. Vision, 115(3):211–252, 2015."
REFERENCES,0.35872235872235875,"Alf B Rustad. The median in multidimensional spaces. Adv. App. Math., 33(2):366–396, Mar. 2004."
REFERENCES,0.36117936117936117,"Ken Sauer and Charles Bouman. A local update strategy for iterative reconstruction from projec-
tions. IEEE Trans. Signal Process., 41(2):534–548, Feb. 1993."
REFERENCES,0.36363636363636365,"Donald L. Snyder, Abed M. Hammoud, and Richard L. White. Image recovery from data acquired
with a charge-coupled-device camera. J. Opt. Soc. Am. A, 10(5):1014–1023, May 1993."
REFERENCES,0.36609336609336607,"Shakarim Soltanayev and Se Young Chun. Training deep learning based denoisers without ground
truth data. In Proc. NIPS, volume 31, Montreal, Canada, Dec. 2018."
REFERENCES,0.36855036855036855,"Christian Szegedy, Alexander Toshev, and Dumitru Erhan. Deep neural networks for object detec-
tion. 2, Dec. 2013."
REFERENCES,0.371007371007371,"Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proc. IEEE/CVF
CVPR, pp. 9446–9454, Salt Lake City, Utah, June 2018."
REFERENCES,0.37346437346437344,Under review as a conference paper at ICLR 2022
REFERENCES,0.3759213759213759,"Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.
Stacked denoising autoencoders: Learning useful representations in a deep network with a local
denoising criterion. J. Mach. Learn. Res., 11(12):3371–3408, Dec. 2010."
REFERENCES,0.3783783783783784,"Junyuan Xie, Linli Xu, and Enhong Chen. Image denoising and inpainting with deep neural net-
works. In Proc. NIPS, pp. 341–349, Lake Tahoe, NV, Dec. 2012."
REFERENCES,0.3808353808353808,"Yaochen Xie, Zhengyang Wang, and Shuiwang Ji. Noise2Same: Optimizing a self-supervised bound
for image denoising. In Proc. NIPS, volume 33, pp. 20320–20330, Vancouver, Canada, Dec. 2020."
REFERENCES,0.3832923832923833,"Jun Xu, Yuan Huang, Li Liu, Fan Zhu, Xingsong Hou, and Ling Shao. Noisy-As-Clean: Learning
unsupervised denoising from the corrupted image. IEEE Trans. Image Process., 29:9316–9329,
Sep. 2020."
REFERENCES,0.3857493857493858,"Li Xu, Jimmy SJ Ren, Ce Liu, and Jiaya Jia. Deep convolutional neural network for image decon-
volution. In Proc. NIPS, pp. 1790–1798, Montreal, Canada, Dec. 2014."
REFERENCES,0.3882063882063882,"Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a Gaussian denoiser:
Residual learning of deep CNN for image denoising. IEEE Trans. Image Process., 26(7):3142–
3155, Feb. 2017."
REFERENCES,0.3906633906633907,Under review as a conference paper at ICLR 2022
REFERENCES,0.3931203931203931,APPENDIX
REFERENCES,0.3955773955773956,"A.1
PROOFS FOR THEOREM 2"
REFERENCES,0.39803439803439805,"Observe ﬁrst that combining Assumptions 1 & 3 and the J -complement implies that f(x)m and
g(x)m are conditionally independent, given y, i.e.,"
REFERENCES,0.4004914004914005,"f(xJc)m|y ⊥⊥g(xJ)m|y,
∀m."
REFERENCES,0.40294840294840295,"Using this result with Assumption 2 and reminding that the J -complement implies Ex∥f(x) −
g(x)∥2
2 = Ex∥f(xJc) −g(xJ)∥2
2, we obtain the following result from (4):"
REFERENCES,0.40540540540540543,"Ex∥f(xJc) −g(xJ)∥2
2 = Ex,y∥f(xJc) −y∥2
2 + ∥g(xJ) −y∥2
2
where the equality uses"
REFERENCES,0.40786240786240785,"Ex,y⟨f(xJc) −y, g(xJ) −y⟩= EyEx|y⟨f(xJc) −y, g(xJ) −y⟩ =
X"
REFERENCES,0.4103194103194103,"m
Ey(Ex|y[f(xJc)m −ym])(Ex|y[g(xJ)m −ym]) = 0"
REFERENCES,0.41277641277641275,"in which the second equality uses the ﬁrst result above and the third equality holds by Assumption 2.
This completes the proofs."
REFERENCES,0.4152334152334152,"A.2
PROOFS FOR THEOREM 3"
REFERENCES,0.4176904176904177,We ﬁrst obtain the following bound:
REFERENCES,0.4201474201474201,"Ex,y⟨f(x) −y, g(xJ) −y⟩"
REFERENCES,0.4226044226044226,"= EyEx|y
X"
REFERENCES,0.4250614250614251,"m
(f(x)m −ym)(g(xJ)m −ym) =
X"
REFERENCES,0.4275184275184275,"m
Ey

Ex|y(f(x)m −ym)(g(xJ)m −ym) −Ex|y(f(x)m −ym)Ex|y(g(xJ)m −ym)
 =
X"
REFERENCES,0.42997542997543,"m
Ey [Cov(f(x)m −ym, g(xJ)m −ym|y)] =
X"
REFERENCES,0.43243243243243246,"m
Ey [Cov(f(x)m, g(xJ)m|y)] =
X"
REFERENCES,0.4348894348894349,"m
Ey [Cov(f(x)m −f(xJc)m, g(xJ)m|y)] ≤
X"
REFERENCES,0.43734643734643736,"m
Ey
h
(Var(f(x)m −f(xJc)m|y) · Var(g(xJ)m|y))1/2i ≤
X"
REFERENCES,0.4398034398034398,"m
Ey [Var(f(x)m −f(xJc)m|y) · Var(g(xJ)m|y)]1/2 ≤  M · M
X"
REFERENCES,0.44226044226044225,"m=1
Ey [Var(f(x)m −f(xJc)m|y) · Var(g(xJ)m|y)] !1/2 ≤  M · M
X"
REFERENCES,0.44471744471744473,"m=1
Ey

Var(f(x)m −f(xJc)m|y) · σ2
!1/2 ,"
REFERENCES,0.44717444717444715,"where the second equality holds by Assumption 2, the third equality uses Cov(X, Y |Z) =
E[XY |Z] −E[X|Z]E[Y |Z] where X, Y , and Z are random variables or vectors, the ﬁfth equal-
ity holds because f(xJc)m does not correlate with g(xJ)m, ∀m (due to Assumptions 1 and 3),
so subtracting f(xJc)m from f(x)m does not change the covariance. Now, the ﬁrst inequality
uses the Pearson correlation coefﬁcient bound, the second inequality uses the Jensen’s inequality
E
√ X ≤
√"
REFERENCES,0.44963144963144963,"EX, the third inequality uses the Jensen’s inequality P"
REFERENCES,0.4520884520884521,"m
√am ≤
p M ′ P"
REFERENCES,0.45454545454545453,m am for
REFERENCES,0.457002457002457,Under review as a conference paper at ICLR 2022
REFERENCES,0.4594594594594595,"any a ∈RM ′, and the last inequality holds by the conditional variance bound speciﬁed in Theo-
rem 3. We bound and rewrite the ﬁnal result above and this completes the proof:"
REFERENCES,0.4619164619164619,"Ex,y⟨f(x) −y, g(xJ) −y⟩≤σ
√ M · M
X"
REFERENCES,0.4643734643734644,"m=1
Ey [Var(f(x)m −f(xJc)m|y)] !1/2 ≤σ
√ M · M
X"
REFERENCES,0.4668304668304668,"m=1
Ey
h
Ex|y [f(x)m −f(xJc)m]2i!1/2 = σ
√ M · M
X"
REFERENCES,0.4692874692874693,"m=1
Ex [f(x)m −f(xJc)m]2
!1/2 ,"
REFERENCES,0.47174447174447176,where the equality uses the ﬁltration property of conditional expectation.
REFERENCES,0.4742014742014742,"A.3
EXAMPLES THAT SUPPORT CONJECTURED BEHAVIOR OF (9) WITH σ"
REFERENCES,0.47665847665847666,"The ﬁrst example in general regression models the pseudo-target as follows: g(xJ) = y + e1,
where e1 ∈RM is some arbitrarily additive noise independent of y. This gives Var(g(xJ)m|y) =
Var(ym + (e1)m|y) = Var(e1)m ≤σ2, m = 1, . . . , M. Under this model, how close is g(xJ) to y
is captured by σ."
REFERENCES,0.47911547911547914,"The second example in image denoising assumes that x is corrupted by AWGN e2 ∈RN that is
independent of y. Setting g as a linear mapping G ∈RJ×N gives Var(g(xJ)n|y) = Var((GyJ)n +
(G(e2)J)n|y) = Var((G(e2)J)n) ≤σ2, ∀n = 1, . . . , N. Under this model, how close is g(xJ) to
y is captured by σ."
REFERENCES,0.48157248157248156,"A.4
EXPERIMENTAL SETUP AND RESULTS FOR/FROM REAL-WORLD
DATASETS"
REFERENCES,0.48402948402948404,"A.4.1
EXPERIMENTAL SETUP FOR REAL-WORLD DATASETS"
REFERENCES,0.4864864864864865,"We also evaluated the proposed SSRL framework with real-world camera image and low-dose CT
datasets, where we do not have their complete noise properties/statistics. We chose the publicly
available SIDD sRGB Data (Abdelhamed et al., 2018) and Low Dose CT Image and Projection
Data (Moen et al., 2021), where both the datasets include high-quality images or standard-dose FBP
images so that one can run Noise2True experiments and obtain quantitative results. We used the
DnCNN architecture for all experiments. For each experiment, we used the same implementation
setup (such as hyperparameters and masking scheme) as that in the corresponding simulated data
experiment. In particular, g is median ﬁlter and pre-trained denoiser via existing self-supervised
denoising methods in camera image denoising and low-dose CT denoising experiments, respectively."
REFERENCES,0.48894348894348894,"PSNR = 23.7 dB
RMSE = 147.9 HU"
REFERENCES,0.4914004914004914,"Figure A.1: An input intrinsically-noisy image in
camera image denoising (left) and low-dose CT
(right). PSNR and RMSE values were averaged
across all test samples."
REFERENCES,0.49385749385749383,"Camera image denoising with SIDD sRGB
Data.
We used the SIDD sRGB training and
validation datasets for training and tests, re-
spectively. We chose the representative com-
parison setup, Neighbor2Neighbor, from the
camera image denoising experiments using
simulated data in Section 4.1."
REFERENCES,0.4963144963144963,"Low-dose CT denoising with Low Dose CT
Image and Projection Data.
We followed
the data construction setup in Section 4.1 that
was used in simulated data experiments; we re-
mark that chest CT scans in the Low Dose CT
Image and Projection Data use the 1 mm slice"
REFERENCES,0.4987714987714988,Under review as a conference paper at ICLR 2022
REFERENCES,0.5012285012285013,"thickness. We cannot run Noise2Inverse experiments because the Low Dose CT Image and Pro-
jection Data does not provide two independent half-view FBP images. We thus chose the other
comparison setups, Noise2Self and Noise2Same."
REFERENCES,0.5036855036855037,"A.4.2
MAIN EXPERIMENTAL RESULTS FROM REAL-WORLD DATASETS"
REFERENCES,0.5061425061425061,"This section includes main experimental results such as denoised images and calculated performance
measure, from the real-world datasets. Section S.3.2 in the supplement includes their supplementary
results."
REFERENCES,0.5085995085995086,"Reference
Noise2True
Neighbor2Neighbor
Proposed SSRL in
Neighbor2Neighbor setup"
REFERENCES,0.5110565110565111,"PSNR = 31.1 dB
SSIM = 0.841"
REFERENCES,0.5135135135135135,"PSNR = 27.2 dB
SSIM = 0.672"
REFERENCES,0.515970515970516,"PSNR = 28.2 dB
SSIM = 0.688"
REFERENCES,0.5184275184275184,"Figure A.2: Comparisons of denoised images (top) via DnCNNs from different learning methods
and their saturation error maps (bottom) in camera image denoising (blue and yellow denote 0 and
0.5 absolute errors, respectively). PSNR and SSIM values were averaged across all SIDD sRGB
validation samples."
REFERENCES,0.5208845208845209,"Reference
Noise2True
Noise2Self
Proposed SSRL in
Noise2Self setup"
REFERENCES,0.5233415233415234,"RMSE = 59.2 HU
RMSE = 106.8 HU
RMSE = 71.2 HU"
REFERENCES,0.5257985257985258,"Noise2Same
Proposed SSRL in
Noise2Same setup"
REFERENCES,0.5282555282555282,"RMSE = 90.5 HU
RMSE = 71.0 HU"
REFERENCES,0.5307125307125307,"Figure A.3: Comparisons of denoised images via DnCNNs from different learning methods in low-
dose CT (display window is [800, 1200] HU). RMSE values were averaged across all test samples."
REFERENCES,0.5331695331695332,Under review as a conference paper at ICLR 2022
REFERENCES,0.5356265356265356,"SELF-SUPERVISED REGRESSION LEARNING USING DO-
MAIN KNOWLEDGE:
APPLICATIONS TO IMPROVING
SELF-SUPERVISED IMAGE DENOISING (SUPPLEMENT)"
REFERENCES,0.538083538083538,"Anonymous authors
Paper under double-blind review"
REFERENCES,0.5405405405405406,"S.1
DETAILED PAPER CONTRIBUTIONS"
REFERENCES,0.542997542997543,This section elaborates the contributions of the proposed SSRL framework:
THE PAPER APPLIES THE PROPOSED SSRL GENERALIZATION TO SEVERAL RECENT REPRESENTATIVE SELF-,0.5454545454545454,"1. The paper applies the proposed SSRL generalization to several recent representative self-
supervised denoising methods, Noise2Self (Batson & Royer, 2019), Noise2Same (Xie
et al., 2020), Noise2Noise (Lehtinen et al., 2018), Noise2Inverse (Hendriksen et al., 2020),
and Neighbor2Neighbor (Huang et al., 2021). See Sections 2.3–2.4. With camera image
and low-dose CT denoising experiments using both real and synthetic datasets, the paper
demonstrates the outperforming performance of SSRL extensions over the aforementioned
self-supervised denoising methods."
THE PAPER APPLIES THE PROPOSED SSRL GENERALIZATION TO SEVERAL RECENT REPRESENTATIVE SELF-,0.547911547911548,"2. Section 2.5 shows that designable pseudo-predictor g in SSRL can relax noise assumptions
of existing self-supervised denoising methods. In addition, Section 4.2 includes experi-
ments studying how denoising performances change with satisfying noise assumption(s)
(i.e., Assumptions 1–2)."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5503685503685504,"3. The paper explains how to incorporate domain knowledge into self-supervised denoising
methods via g and why more accurate domain knowledge can improve them. See examples
in Sections 3.1–3.2. In addition, Section 3.3 proposes an empirical approach for selecting
g if domain knowledge of speciﬁc applications is unavailable."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5528255528255528,"4. The proposed SSRL framework in Section 2 considers regression NN learning beyond
denoiser learning, by using a desinable operator g. The paper is the ﬁrst step towards
self-supervised learning in regression problems, by showing that the proposed framework
extends well to image denoising problem."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5552825552825553,"S.2
IMPLEMENTATION DETAILS, DATA AND CODE LICENSES, AND LIBRARY
VERSIONS"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5577395577395577,"This section describes implementation details, lists hyperparameters, and speciﬁes license of
datasets and codes used in this study."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5601965601965602,"S.2.1
DATA AND CODE LICENCES, AND LIBRARY VERSIONS"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5626535626535627,"The ImageNet ILSVRC 2012 Val and BSD 300 datasets have the Custom license (re-
search, non-commercial), the Set 5 data has the Unknown license, and the SIDD sRGB
Data (Abdelhamed et al., 2018) has the MIT license.
We obtained The 2016 Low Dose
CT Grand Challenge data (McCollough, 2016) from https://aapm.app.box.com/s/
eaw4jddb53keg1bptavvvd1sf4x3pe9h/file/856956352254, and Low Dose CT Im-
age and Projection Data (Moen et al., 2021) from https://doi.org/10.7937/9npb-2637.
The 2016 Low Dose CT Grand Challenge data and Low Dose CT Image and Projection Data have
the Custom license and the patient information is fully redacted."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5651105651105651,"We implemented all the methods speciﬁed in Section 4 by modifying the Noise2Self code (Batson
& Royer, 2019) (GitHub repository: https://github.com/czbiohub/noise2self with"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5675675675675675,Under review as a conference paper at ICLR 2022
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5700245700245701,"version Dec. 17, 2019) that is licensed under the MIT license. For all training and testing experi-
ments, we used Pytorch 1.0.0 or 1.7.0 (Paszke et al., 2019) with the BSD-style license. For simu-
lating low-dose FBP images, we used the Michigan image reconstruction toolbox (MIRT) (Fessler,
2016) of which license information is declared on its release page. For sinogram generation and
FBP reconstruction, we used the “Gtomo2 dscmex.m” routine (updated on Dec. 10, 2006) and
the “fbp2.m” routine (updated on Dec. 21, 2005), respectively."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5724815724815725,"S.2.2
COMMON IMPLEMENTATION DETAILS IN BOTH APPLICATIONS"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5749385749385749,"For all the existing self-supervised denoising methods speciﬁed in Section 4 and Noise2True, we
ﬁnely tuned their hyperparameters, including the initial learning rate, learning rate decay parameters,
minibatch size, number of DnCNN layers, and balancing parameter σ (see, e.g., (9)), to achieve
the best numerical results. We simply applied the chosen hyperparameter sets to corresponding
SSRL setups. We applied the chosen learning rate decay parameters, minibatch size, and number of
DnCNN layers in Noise2True experiments to all the self-supervised denoising methods."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5773955773955773,"For the existing self-supervised denoising methods, Noise2Self (Batson & Royer, 2019) and
Noise2Same (Xie et al., 2020), we used their default masking setups. The Noise2Self default setup
uses the deterministic masking scheme for each J that equi-spacedly samples 6.25% of the num-
ber of pixels in each training image (i.e., a single pixel is selected in each 4 × 4 window). The
Noise2Same default setup uses the saturated sampling scheme (Xie et al., 2020; Krull et al., 2019)
for each J that randomly samples ≈0.5% of the number of pixels in each training image (i.e., a
single pixel is sampled in each 14 × 14 window). In training denoising NNs, both methods inter-
polate missing pixels in xJc by applying weighted average to their 8 neighboring pixels, and use
interpolated xJc as input to denoisers."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5798525798525799,"For the existing Noise2Noise-motivated methods that emulate pairs of two independent noisy im-
ages, Neighbor2Neighbor (Huang et al., 2021) and Noise2Inverse (Hendriksen et al., 2020), we
calculated their loss with non-masked images as proposed."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5823095823095823,"We tested all trained regression NNs to non-masked images – rather than masked images with Jc –
as this setup gave higher denoising accuracy than prediction with masked images (Batson & Royer,
2019; Xie et al., 2020)."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5847665847665847,"S.2.3
IMPLEMENTATION DETAILS FOR EXPERIMENTS WITH SYNTHETIC CAMERA IMAGE
DENOISING DATA"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5872235872235873,"The common hyperparameters for all learning methods were deﬁned as follows. (In this applica-
tion, these gave good image denoising performance across all existing self-supervised denoising
methods and Noise2True, since we rescaled or normalized training images; see details below.) We
used the default 17-layer DnCNN (Zhang et al., 2017) and the modiﬁed U-Net used in Noise2Self
(Batson & Royer, 2019), and trained all DnCNNs and U-Nets with the mini-batch version of Adam
(Kingma & Ba, 2015). We selected the initial learning rate, the batch size, and the number of
epochs as 8 × 10−4, 8, and 190, respectively, and decayed the learning rates by a factor of 0.5 every
50,000 iterations. (For Neighbor2Neighbor (Huang et al., 2021), we set the batch size as 32, as it
reduces training image size with 2 × 2 sub-sampling window.) We used the data augmentations in
Noise2Same (Xie et al., 2020), i.e., random crops with size 256 × 256, rotation and ﬂipping. Except
for Noise2Same, we rescaled all images to [0, 1], following (Batson & Royer, 2019; Huang et al.,
2021). In Noise2Same experiments (including SSRL-Noise2Same), we normalized each image by
subtracting its mean and dividing by its standard deviation at each channel, following (Xie et al.,
2020)."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5896805896805897,"For proposed SSRL in the Noise2Self and Noise2Same setups (referred to as SSRL-Noise2Self and
SSRL-Noise2Same, respectively), we used the deterministic masking scheme in Figure 2(bottom)
for each J with ≈11.1% and ≈1.2% sampling ratio, respectively – i.e., a single pixel is selected in
each 3×3 and 9×9 window, respectively. These setups gave more appealing results than the default
masking parameters (i.e., 4×4 window in Noise2Self and 14×14 window in Noise2Same); compare
Figure S.6 to corresponding results in Figure 4. We observed in this application that using sufﬁcient
amount of information for a linear interpolation in f is useful for giving good prediction. For
weighted median ﬁltering (Brownrigg, 1984) g in all SSRL setups, we used the following weights:"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5921375921375921,Under review as a conference paper at ICLR 2022
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5945945945945946,"[1, 2, 1; 2, 9 , 2; 1, 2, 1], where a box denotes the central weight. The dilation rates of weighted
median ﬁltering for SSRL in the Noise2Self, Noise2Same, and Neighbor2Neighbor setups are 3, 9,
and 1, respectively, corresponding to the distances between pixels in J. For SSRL-Noise2Self, we
computed Lind in (7) only on J (see Figure 2(bottom)). For SSRL-Noise2Same, we used the same
balancing parameter σ as Noise2Same used, i.e., σ = 1, and computed L (both terms) in (9) only
on J."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.597051597051597,"The DnCNN and U-Net training time for each experiment was less than 72 hours with an NVIDIA
TITAN V GPU."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.5995085995085995,"S.2.4
IMPLEMENTATION DETAILS FOR EXPERIMENTS WITH SYNTHETIC LOW-DOSE CT
DENOISING DATA"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.601965601965602,"We used fan-beam geometry for sinogram simulation, where width of each detector column is
1.2858 mm, source to detector distance is 1085.6 mm, and source to rotation center distance is
595 mm. For the FBP method, we used a ramp ﬁlter because in general, it better preserves the
sharpness of edges on reconstructed images than Hanning ﬁlter (but overall noise increases)."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6044226044226044,"The common hyperparameters for all learning methods were deﬁned as follows. We used 8-layer
DnCNN (Zhang et al., 2017) with its default setup and the modiﬁed U-Net used in Noise2Self (Bat-
son & Royer, 2019), and trained all DnCNNs and U-Nets with the mini-batch version of Adam
(Kingma & Ba, 2015). We selected the batch size and the number of epochs as 2 and 1,000, re-
spectively, and decayed the learning rates by a factor of 0.95 every 10 epochs. In training DnCNNs
and U-Nets, we selected the initial learning rates as 0.1 and 5 × 10−5, respectively, unless stated
otherwise."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6068796068796068,"For proposed SSRL-Noise2Self and SSRL-Noise2Same, we used complementary checkboard masks
J and Jc in Figure 2(top). We observed in this application that if g is set to use small amount of
information, i.e., |J| ≪|Jc|, then pre-trained g makes poor prediction. For SSRL-Noise2Self, we
set g as pre-trained NN by Noise2Self with complementary checkerboard masks (see its inference
results with 8-layer DnCNN and U-Net in Figures S.7(a) and S.4, respectively). In training DnC-
NNs, we used the same initial learning rate as that used in pre-training g, 0.01. We computed Lind
as given in (7) (see Figure 2(top))."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6093366093366094,"For SSRL in the Noise2Inverse setup, we set f and g as f/2 and I −g/2, respectively, where
g is pre-trained NN by Noise2Inverse. In training DnCNNs, we used the same initial learning
rate as that used in pre-training g, 0.001. In inference, we averaged the predictions from f and
g, as this corresponds to training setup above. In all Noise2Inverse inferences (including SSRL-
Noise2Inverse), we input full-view FBP images since this is consistent with other experiments and
gave better denoising performance than denoising half-view FBP images. For SSRL-Noise2Same,
we set g as pre-trained NN by Noise2Same with complementary checkerboard masks (see its test
results with DnCNN and U-Net in Figures S.7(b) and S.4, respectively), and computed L (both
terms) in (9) only on Jc. We chose the balancing parameter σ as 15, setting the ratio of the ﬁrst
term to the squared second term 2σ
√"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6117936117936118,"M∥f(x)Jc −f(xJc)Jc∥2
2 in (9) as 10. For Noise2Same with
either the default setup and complementary checkerboard masks, we chose the balancing parameter
σ as 500. For self-supervised denoising methods with complementary checkerboard masks, we
interpolated missing pixels in both xJc and xJ, by averaging their 4 neighboring pixels."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6142506142506142,"The DnCNN and U-Net training time for existing self-supervised denoising methods and proposed
SSRL methods was less than 10 hours and 12 hours, respectively, with an NVIDIA TITAN Xp GPU.
It took total less than 22 hours to train both f and g."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6167076167076168,"S.3
SUPPLEMENTARY EXPERIMENTAL RESULTS FOR SECTION 4"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6191646191646192,This section mainly includes supplementary materials to Section 4.
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6216216216216216,"S.3.1
SUPPLEMENTARY EXPERIMENTAL RESULTS WITH SYNTHETIC DATASETS"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6240786240786241,"Figures S.1 and S.2 show error maps of denoised images via DnCNNs from different learning meth-
ods in camera image denoising in mixed noise and low-dose CT, respectively. These show for both"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6265356265356266,Under review as a conference paper at ICLR 2022
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.628992628992629,"applications that in all the three comparison setups, SSRL signiﬁcantly reduces errors and artifacts
across the entire image, over existing self-supervised denoising methods. Red boxes compare exist-
ing self-supervised denoising method to proposed SSRL in the corresponding setup."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6314496314496314,"Input noise image
Noise2True
Noise2Self
Proposed SSRL in
Noise2Self setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6339066339066339,"PSNR = 9.8 dB
SSIM = 0.134"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6363636363636364,"PSNR = 25.2 dB
SSIM = 0.819"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6388206388206388,"PSNR = 20.6 dB
SSIM = 0.711"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6412776412776413,"PSNR = 22.1 dB
SSIM = 0.748"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6437346437346437,"Neighbor2Neighbor
Proposed SSRL in
Neighbor2Neighbor setup
Noise2Same
Proposed SSRL in
Noise2Same setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6461916461916462,"PSNR = 20.2 dB
SSIM = 0.703"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6486486486486487,"PSNR = 22.3 dB
SSIM = 0.756"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6511056511056511,"PSNR = 20.1 dB
SSIM = 0.690"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6535626535626535,"PSNR = 21.2 dB
SSIM = 0.705"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6560196560196561,"Figure S.1: Error map comparisons of denoised images via DnCNNs from different learning meth-
ods in camera image denoising (blue and yellow denote 0 and 50 absolute errors, respectively).
PSNR and SSIM values were averaged across all BSD 300 test samples."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6584766584766585,"Input noisy image
Noise2True
Noise2Self
Proposed SSRL in
Noise2Self setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6609336609336609,"RMSE = 48.3 HU
RMSE = 16.3 HU
RMSE = 36.2 HU
RMSE = 25.0 HU"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6633906633906634,"Noise2Inverse
Proposed SSRL in
Noise2Inverse setup
Noise2Same
Proposed SSRL in
Noise2Same setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6658476658476659,"RMSE = 22.9 HU
RMSE = 21.9 HU
RMSE = 28.4 HU
RMSE = 26.0 HU"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6683046683046683,"Figure S.2: Error map comparisons of denoised images via DnCNNs from different learning meth-
ods in low-dose CT (blue and yellow denote 0 and 50 absolute errors in HU, respectively). RMSE
values were averaged across all test samples from The 2016 Low Dose CT Grand Challenge data."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6707616707616708,Under review as a conference paper at ICLR 2022
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6732186732186732,"Figures S.3 and S.4 show denoised images via U-Nets from different learning methods in camera
image denoising in mixed noise and low-dose CT, respectively. These demonstrate for both appli-
cations that in all the three comparison setups, SSRL signiﬁcantly improves existing self-supervised
denoising methods regardless of the regression neural network architecture."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6756756756756757,"Reference
Noise2True
Noise2Self
Proposed SSRL in
Noise2Self setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6781326781326781,"PSNR = 26.0 dB
SSIM = 0.819"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6805896805896806,"PSNR = 20.9 dB
SSIM = 0.711"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.683046683046683,"PSNR = 22.5 dB
SSIM = 0.748"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6855036855036855,"Neighbor2Neighbor
Proposed SSRL in
Neighbor2Neighbor setup
Noise2Same
Proposed SSRL in
Noise2Same setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.687960687960688,"PSNR = 20.8 dB
SSIM = 0.703"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6904176904176904,"PSNR = 22.4 dB
SSIM = 0.756"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6928746928746928,"PSNR = 19.5 dB
SSIM = 0.612"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6953316953316954,"PSNR = 20.7 dB
SSIM = 0.660"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.6977886977886978,"Figure S.3: Comparisons of denoised images (left) via U-Nets from different learning methods
and their saturation error maps (right) in camera image denoising (blue and yellow denote 0 and
0.5 absolute errors, respectively). PSNR and SSIM values were averaged across all BSD 300 test
samples."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7002457002457002,"Reference
Noise2True
Noise2Self
(checkerboard mask)
Proposed SSRL in
Noise2Self setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7027027027027027,"RMSE = 18.5 HU
RMSE = 32.3 HU
RMSE = 26.7 HU"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7051597051597052,"Noise2Inverse
Proposed SSRL in
Noise2Inverse setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7076167076167076,"Noise2Same
(checkerboard mask)
Proposed SSRL in
Noise2Same setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7100737100737101,"RMSE = 24.0 HU
RMSE = 23.5 HU
RMSE = 31.1 HU
RMSE = 28.2 HU"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7125307125307125,"Figure S.4: Comparisons of denoised images via U-Nets from different learning methods in low-
dose CT (display window is [800, 1200] HU). RMSE values were averaged across all test samples
from The 2016 Low Dose CT Grand Challenge data."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.714987714987715,Under review as a conference paper at ICLR 2022
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7174447174447175,"Tables S.1–S.3 report quantitative image denoising results with DnCNN and U-Net with BSD 300
and Set 5 data in camera image denoising in mixed noise, and with chest slices of The 2016 Low
Dose CT Grand Challenge data in low-dose CT. Red boxes in Tables S.1–S.3 compare an existing
self-supervised denoising method to proposed SSRL in the corresponding setup."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7199017199017199,"Table S.1: Averaged test PSNR (dB) (ﬁrst and third rows) and SSIM (second and fourth rows)
comparisons with from different learning methods with DnCNN (ﬁrst and second rows) and U-Net
(third and fourth rows) in camera image denoising (simulated noisy BSD 300 dataset)."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7223587223587223,"Noise2True
Noise2Self
Proposed SSRL
in Noise2Self
setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7248157248157249,"Neighbor2-
Neighbor"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7272727272727273,"Proposed SSRL
in Neighbor2-
Neighbor setup
Noise2Same
Proposed SSRL
in Noise2Same
setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7297297297297297,"25.2
20.6
22.1
20.2
22.3
20.1
21.2
0.819
0.711
0.748
0.703
0.756
0.690
0.705"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7321867321867321,"26.0
20.9
22.5
20.8
22.4
19.5
20.7
0.849
0.730
0.765
0.728
0.767
0.612
0.660"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7346437346437347,"Table S.2: Averaged test PSNR (dB) (ﬁrst and third rows) and SSIM (second and fourth rows)
comparisons from different learning methods with DnCNN (ﬁrst and second rows) and U-Net (third
and fourth rows) in camera image denoising (simulated noisy Set 5 dataset)."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7371007371007371,"Noise2True
Noise2Self
Proposed SSRL
in Noise2Self
setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7395577395577395,"Neighbor2-
Neighbor"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.742014742014742,"Proposed SSRL
in Neighbor2-
Neighbor setup
Noise2Same
Proposed SSRL
in Noise2Same
setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7444717444717445,"26.4
19.3
21.2
19.0
21.9
18.9
20.0
0.890
0.743
0.785
0.744
0.806
0.729
0.753"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7469287469287469,"27.2
19.5
21.8
19.4
21.6
17.7
19.3
0.910
0.752
0.802
0.751
0.800
0.626
0.712"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7493857493857494,"Table S.3: Averaged test RMSE (HU) comparisons from different learning methods with DnCNN
(ﬁrst row) and U-Net (second row) in low-dose CT denoising (simulated low-dose CT dataset)."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7518427518427518,"Noise2True Noise2Self
Proposed SSRL
in Noise2Self
setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7542997542997543,"Noise2-
Inverse"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7567567567567568,"Proposed SSRL
in Noise2-
Inverse setup
Noise2Same
Proposed SSRL
in Noise2Same
setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7592137592137592,"16.3
36.2
25.0
22.9
21.9
28.4
26.0"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7616707616707616,"18.5
32.3
26.7
24.0
23.5
31.1
28.2"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7641277641277642,"Figure S.5 compares prediction uncertainty of trained DnCNN denoisers via different learning meth-
ods in both applications. The error bar graphs in Figure S.5 show that for both applications, in all
the three comparison sets, proposed SSRL gives similar or lower prediction uncertainty over the
existing self-supervised denoising methods, Noise2Self, Neighbor2Neighbor, Noise2Inverse, and
Noise2Same."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7665847665847666,"Figure S.6 shows denoised camera images from SSRL-Noise2Self and SSLR-Noise2Same using the
default masking parameters in Noise2Self and Noise2Same (see Section S.2.3). Compare the results
in Figure S.6 with the corresponding ones in Figure 4 using the designed setups (see Section S.2.3).
The comparisons demonstrate that the default and designed setups give very similar PSNR results,
i.e., ≤0.1 dB, but the designed setups gives slightly more visually appealing results than the default
ones in Noise2Self and Noise2Same."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.769041769041769,"Figure S.7 shows denoised images from Noise2Self and Noise2Same with DnCNN and comple-
mentary checkerboard masks J and Jc, and reports the corresponding quantitative test results, in
low-dose CT denoising. Comparing the results in Figure S.7 to those of Noise2Self and Noise2Same"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7714987714987716,Under review as a conference paper at ICLR 2022
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.773955773955774,"using the default masking setups in Figures 5 and S.2 shows that checkerboard masking improves
denoising quality over default masking in Noise2Self, and achieves comparable denoising perfor-
mance to default masking in Noise2Same. (See their default masking setups in Section S.2.2.)
SSRL-Noise2Self with checkerboard masking achieves signiﬁcantly better denoising quality com-
pared to Noise2Self with checkerboard masking. We used pre-trained DnCNN from these two setups
as g in the corresponding SSRL setup."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7764127764127764,"Figure S.5: Denoising performance error bars for different learning methods with DnCNN in camera
image denoising in mixed noise (with BSD 300) (left, 300 test images) and low-dose CT (right, 30
test images). Red asterisks denote the averaged test PSNR or RMSE values. Error bar represents
one standard deviation of test PSNR or RMSE values."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7788697788697788,(a) SSRL-Noise2Self with the default setup
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7813267813267813,"in Noise2Self (i.e., 4 × 4 window)"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7837837837837838,(b) SSRL-Noise2Same with the default setup
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7862407862407862,"in Noise2Same (i.e., 14 × 14 window)"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7886977886977887,"PSNR = 22.0 dB
PSNR = 21.1 dB"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7911547911547911,"Figure S.6:
Denoised images via DnCNNs and their corresponding error maps from SSRL-
Noise2Self and SSRL-Noise2Same with default masks setup in camera image denoising. (In error
maps, blue and yellow denote 0 and 50 absolute errors, respectively.) PSNR values were averaged
across all BSD 300 test samples."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7936117936117936,"(a) Noise2Self with complementary
checkerboard masks
(b) Noise2Same with complementary
checkerboard masks"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7960687960687961,"RMSE = 30.9 HU
RMSE = 28.7 HU"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.7985257985257985,"Figure S.7: Denoised images via DnCNNs and their corresponding error maps from Noise2Self
and Noise2Same with complementary checkerboard masks in low-dose CT. (The display window of
denoised images is [800, 1200] HU; in error maps, blue and yellow denote 0 and 50 absolute errors
in HU, respectively.) RMSE values were averaged across all test samples."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.800982800982801,Under review as a conference paper at ICLR 2022
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8034398034398035,"S.3.2
SUPPLEMENTARY EXPERIMENTAL RESULTS WITH REAL-WORLD DATASETS"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8058968058968059,"Figures S.8 and S.9 show error maps of denoised images via DnCNNs from different learning meth-
ods in camera image and low-dose CT denoising with real-world datasets, respectively. These show
for both applications that in Neighbor2Neighbor, Noise2Self, or Noise2Same comparison setups,
SSRL signiﬁcantly improve the entire image without having their exact noise properties."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8083538083538083,"Input noisy image
Noise2True
Neighbor2Neighbor
Proposed SSRL in
Neighbor2Neighbor setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8108108108108109,"PSNR = 23.7 dB
SSIM = 0.485"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8132678132678133,"PSNR = 31.1 dB
SSIM = 0.841"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8157248157248157,"PSNR = 27.2 dB
SSIM = 0.672"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8181818181818182,"PSNR = 28.2 dB
SSIM = 0.688"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8206388206388207,"Figure S.8: Error map comparisons of denoised images via DnCNNs from different learning meth-
ods in camera image denoising with real-world dataset (blue and yellow denote 0 and 50 absolute
errors, respectively). PSNR and SSIM values were averaged across all SIDD sRGB validation sam-
ples."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8230958230958231,"Input noisy image
Noise2Self
Proposed SSRL in
Noise2Self setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8255528255528255,"RMSE = 147.9 HU
RMSE = 106.8 HU
RMSE = 71.2 HU"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.828009828009828,"Noise2True
Noise2Same
Proposed SSRL in
Noise2Same setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8304668304668305,"RMSE = 59.2 HU
RMSE = 90.5 HU
RMSE = 71.0 HU"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8329238329238329,"Figure S.9: Error map comparisons of denoised images via DnCNNs from different learning meth-
ods in low-dose CT with real-world dataset (blue and yellow denote 0 and 100 absolute errors in
HU, respectively). RMSE values were averaged across all test samples."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8353808353808354,"S.4
EMPIRICAL RESULTS TO SUPPORT SOME CLAIMS IN MAIN PAPER"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8378378378378378,"The following empirical results support that proposed SSRL loss better approximates the supervision
(Noise2True) loss than existing self-supervised learning, particularly when Assumptions 1–2 are not
completely satisﬁed. We used the representative self-supervised denoising setup, Noise2Self. In the
camera image denoising experiments in Section 4.1, the empirical loss values (at the last epoch) of
{Noise2Self, SSRL-Noise2Self, Noise2True} are {0.296, 0.244, 0.170} (in RMSE); in the low-dose
CT denoising experiments in Section 4.1, those are {2186.4, 329.1, 270.2} (in HU2)."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8402948402948403,Under review as a conference paper at ICLR 2022
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8427518427518428,"Figure S.10 empirically supports our claim in Section 3.1 that g design “approximately” satisfy
E[g(x)|y] = y in camera image denoising, with both simulated and real-word datasets. We calcu-
lated empirical E[x −y|y] and E[g(x) −y|y] with simulated noisy BSD 300 test samples (using
noise model (10)) and the real-world noisy dataset (speciﬁcally, SIDD sRGB validation samples)
in Section A.4.1, where g is median ﬁltering. In the simulated dataset, the empirical measures
for {avg(|E[x −y|y]|), avg(|E[g(x) −y|y]|)} are {0.0201, 0.0098}, where avg denotes averaging
across pixels. In the real-word dataset, those are {0.0113, 0.0083}. These support our claim that
E[g(x)|y] = y is approximately satisﬁed with median ﬁltering g."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8452088452088452,"Figure S.11 empirically supports our claim in Section 3.2 that noise of FBP-reconstructed images in
low-dose CT, i.e., e in (11), has approximately zero-mean. The position of the patient table base is
similar across FBP images, so it gave higher errors in the calculated sample mean; see the bottom of
the image in Figure S.11."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8476658476658476,"The following empirical results support the claim in Section 3.2 that pre-trained g will give better
reference than g = I in low-dose CT denoising: the empirical measure of the Noise2Self loss
(Batson & Royer, 2019) – E∥g(xJ)Jc −xJc∥2
2 – with simulated noisy CT test samples by setting g
as I and pre-trained DnCNN by Noise2Self are 2455.7 and 1839.1 (in HU2), respectively."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8501228501228502,(a) Simulated noisy dataset
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8525798525798526,(BSD 300 test samples)
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.855036855036855,"(b) Real-world noisy dataset
(SIDD sRGB validation samples)"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8574938574938575,"Figure S.10: Empirical observations of |E[x −y|y]| (left) and |E[g(x) −y|y]| (right) in camera
image denoising ((a) Blue and yellow denote 0 and 0.1 absolute errors, respectively. (b) Blue and
yellow denote 0 and 0.02 absolute errors, respectively.)"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.85995085995086,"Figure S.11: Sample mean of noise in low-dose FBP images – e in (11). We calculated the sample
mean with 200 samples."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8624078624078624,"S.5
EXPERIMENTAL RESULTS WITH GAUSSIAN AND POISSON+GAUSSIAN
NOISE MODELS"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8648648648648649,"This section studies the performance of the proposed SSRL framework with benchmark noisy
datasets, MIT-Adobe FiveK data (Bychkovsky et al., 2011), corrupted by sole Gaussian and Pois-
son+Gaussian noises."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8673218673218673,Under review as a conference paper at ICLR 2022
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8697788697788698,"S.5.1
EXPERIMENTAL SETUP"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8722358722358723,"In Guassian denoising experiments, we simulated AWGN with the standard deviation value σϵ = 25
(Huang et al., 2021; Xu et al., 2020), and selected g as Wiener ﬁltering that is known to be optimal
in the sense of minimum MSE in Gaussian denoising. In Poisson+Gaussian denoising experiments,
we followed the noise simulation setup (Byun et al., 2021, Tab. 5: (α, σ) = (0.05, 0.02)) that cor-
responds to (λ, σϵ) = (20, 5.1) where λ and σϵ are deﬁned in (10). To better visualize the results,
we enhanced the brightness with gamma correction (with the parameter 3). We choose the repre-
sentative comparison setup, Neighbor2Neighbor, from the camera image denoising experiments in
Section 4.1. This is also a state-of-the-art self-supervised denoising method, particularly when only
single noisy images are available (Huang et al., 2021)."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8746928746928747,"S.5.2
COMPARISONS BETWEEN DIFFERENT LEARNING METHODS"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8771498771498771,"Two observations in Figure S.12. First, in both Gaussian and Poisson+Gaussian denoising, proposed
SSRL using “good” g further improved a state-of-the-art self-supervised denoising method, Neigh-
bor2Neighbor. Second, in both Gaussian and Poisson+Gaussian denoising, the performance gap
between Noise2True and Neighbor2Neighbor, is small, where the PSNR gap numbers well corre-
spond to existing literature (Huang et al., 2021; Byun et al., 2021). We conjecture that this is because
the noise assumptions of self-supervised denoising method are well satisﬁed in the aforementioned
two experiments. This is connected to our conjecture in Section 4.2 that self-supervised denoising
performance degrades, if its assumptions are not completely satisﬁed."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8796068796068796,"Reference image
Noise2True
Neighbor2Neighbor
Proposed SSRL in
Neighbor2Neighbor setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8820638820638821,"PSNR = 34.61 dB
PSNR = 34.34 dB
PSNR = 34.50 dB"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8845208845208845,(a) Gaussian denoising (i.i.d. with σϵ = 25)
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8869778869778869,"PSNR = 37.77 dB
PSNR = 37.45 dB
PSNR = 37.59 dB"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8894348894348895,"(b) Poisson+Gaussian denoising (i.i.d. with (λ, σϵ) = (20, 5.1))"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8918918918918919,"Figure S.12: Comparisons of denoised images via DnCNNs from different learning methods in
camera image denoising. PSNR vlaues were averaged across all MIT-Adobe FiveK test samples."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8943488943488943,"S.6
COMPARISONS TO SELF2SELF"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8968058968058968,"This section compares the proposed SSRL framework with a state-of-the-art blind image denoising
(a.k.a. self-supervised denoising with a single image) method, Self2Self (Quan et al., 2020), with
synthetic and real-world noisy datasets for each application (see Section 4). (We used the authors’
Self2Self implementation.)"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.8992628992628993,"First, in Figure S.13 and S.14, compare Self2Self with Neighbor2Neighbor and Noise2Self (the rep-
resentative comparison setup in each simulated imaging experiment in Section 4.1), respectively.
The comparisons show that Self2Self gives comparable results to Neighbor2Neighbor/Noise2Self."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.9017199017199017,Under review as a conference paper at ICLR 2022
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.9041769041769042,"Remind, however, that Self2Self is a blind denoising method, so it needs a signiﬁcantly larger com-
putations than Neighbor2Neighbor, when one denoises a new noisy image. With good g designs
(see Sections 3.1–3.2), proposed SSRL signiﬁcantly outperformed Self2Self in both applications,
regardless of whether their dataset is real or simulated."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.9066339066339066,"Noise2True
Self2Self
Neighbor2Neighbor
Proposed SSRL in
Neighbor2Neighbor setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.9090909090909091,"PSNR = 27.2 dB
PSNR = 19.0 dB
PSNR = 19.4 dB
PSNR = 21.6 dB"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.9115479115479116,(a) Synthetic noisy dataset (Set 5) using imaging simulation in Section 4.1
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.914004914004914,"PSNR = 31.1 dB
PSNR = (to be included)
PSNR = 27.2 dB
PSNR = 28.2 dB"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.9164619164619164,(b) Real-world noisy dataset (SIDD sRGB validation)
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.918918918918919,"Figure S.13: Comparisons of denoised images (right) from different learning methods and their
error maps (left) in camera image denoising (blue and yellow 0 and 50 absolute errors, respectively).
PSNR values were averaged across all test samples."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.9213759213759214,"Noise2True
Self2Self
Noise2Self
Proposed SSRL in
Noise2Self setup"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.9238329238329238,"RMSE = 18.5 HU
RMSE = 31.6 HU
RMSE = 32.3 HU
RMSE = 26.7 HU"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.9262899262899262,(a) Synthetic dataset (The 2016 Low Dose CT Grand Challenge data) using imaging sim. in Section 4.1
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.9287469287469288,"RMSE = 59.2 HU
RMSE = 104.1 HU
RMSE = 106.8 HU
RMSE = 71.2 HU"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.9312039312039312,(b) Real-world dataset (Low Dose CT Image and Projection Data)
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.9336609336609336,"Figure S.14: Comparisons of denoised images from different learning methods in low-dose CT with
synthetic and real-world datasets (display window is [800, 1200] HU). RMSE values were averaged
across all test samples."
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.9361179361179361,Under review as a conference paper at ICLR 2022
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.9385749385749386,"S.7
PRELIMINARY RESULTS WITH THE TEACHER-STUDENT LEARNING
PERSPECTIVE"
THE PAPER EXPLAINS HOW TO INCORPORATE DOMAIN KNOWLEDGE INTO SELF-SUPERVISED DENOISING,0.941031941031941,"The proposed SSRL framework is applicable to teacher-student learning (Wang & Yoon, 2021; Hin-
ton et al., 2015; Buciluˇa et al., 2006) that aims to learn a smaller student network from bigger teacher
network(s). We ran preliminary experiments in self-supervised low-dose CT denoising (using sim-
ulated data). The teacher model g is the pre-trained 8-layer DnCNN by Noise2Self (with checker-
board masking), and we set the student model f as {8, 7, 6, 5, 4, 3}-layer DnCNN. Applying SSRL-
Noise2Self, we obtained the following numerical results: the RMSE (HU) values of student models
with {8, 7, 6, 5, 4, 3}-layer DnCNNs are {25.0, 25.3, 25.5, 25.4, 25.2, 27.5}, respectively. The stu-
dent DnCNNs that have the equal or lower complexity compared to its teacher network, signiﬁcantly
improves its teacher model of which RMSE value is 30.9 (in HU). The results might imply that stu-
dent models learned from SSRL can outperform their teacher model, if they retain sufﬁciently high
network complexity as compared to their teacher’s (e.g., 3-layer DnCNN). In addition, we have addi-
tional SSRL experiment in low-dose CT denoising with the “iterative” teacher-student perspective.
The teacher model g is pre-trained 5-layer DnCNN from the non-iterative teacher-student SSRL
method above, and we set the student model f as 3-layer DnCNN. We obtained 27.3 RMSE (in
HU), implying only marginal improvement over the 3-layer DnCNN obtained by the non-iterative
teacher-student SSRL method. We conjecture that iterative teacher-student SSRL needs more so-
phisticated g-setups, such as an ensemble of teacher models (Hinton et al., 2015)."
REFERENCES,0.9434889434889435,REFERENCES
REFERENCES,0.9459459459459459,"Abdelrahman Abdelhamed, Stephen Lin, and Michael S. Brown. A high-quality denoising dataset
for smartphone cameras. In Proc. IEEE CVPR, pp. 1692–1700, Salt Lake City, Utah, Jun. 2018."
REFERENCES,0.9484029484029484,"Joshua Batson and Loic Royer. Noise2Self: Blind denoising by self-supervision. In Proc. ICML,
pp. 524–533, Long Beach, CA, Jun. 2019."
REFERENCES,0.9508599508599509,"David RK Brownrigg. The weighted median ﬁlter. Commun. of the ACM, 27:807–818, 1984."
REFERENCES,0.9533169533169533,"Cristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proc.
ACM. SIGKDD., pp. 535–541, 2006."
REFERENCES,0.9557739557739557,"Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fr´edo Durand. Learning photographic global
tonal adjustment with a database of input / output image pairs. In Proc. IEEE CVPR, pp. 97–104,
Colorado Springs, CO, Jun. 2011."
REFERENCES,0.9582309582309583,"Jaeseok Byun, Sungmin Cha, and Taesup Moon.
Fbi-denoiser: Fast blind image denoiser for
poisson-gaussian noise. In Proc. IEEE CVPR, pp. 5768–5777, Virtual, Jun. 2021."
REFERENCES,0.9606879606879607,"J. A. Fessler. Michigan image reconstruction toolbox (MIRT) for Matlab. Available from http:
//web.eecs.umich.edu/˜fessler, 2016."
REFERENCES,0.9631449631449631,"Allard Adriaan Hendriksen, Dani¨el Maria Pelt, and K Joost Batenburg.
Noise2Inverse: Self-
supervised deep convolutional denoising for tomography. IEEE Trans. Comput. Imag., 6:1320–
1335, Aug. 2020."
REFERENCES,0.9656019656019657,"Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In
Proc. NIPS Workshop, pp. 1790–1798, Montreal, Canada, Dec. 2015."
REFERENCES,0.9680589680589681,"Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, and Jianzhuang Liu. Neighbor2Neighbor: Self-
supervised denoising from single noisy images. In Proc. IEEE/CVF CVPR, pp. 14781–14790,
Virtual, Jun. 2021."
REFERENCES,0.9705159705159705,"Diederik P Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In Proc. ICLR
2015, pp. 1–15, San Diego, CA, May 2015."
REFERENCES,0.972972972972973,"Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2Void - Learning denoising from
single noisy images. In Proc. IEEE/CVF CVPR, pp. 2129–2137, Long Beach, CA, Jun. 2019."
REFERENCES,0.9754299754299754,Under review as a conference paper at ICLR 2022
REFERENCES,0.9778869778869779,"Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and
Timo Aila. Noise2Noise: Learning image restoration without clean data. In Proc. ICML, pp.
2965–2974, Stockholm, Sweden, Jul. 2018."
REFERENCES,0.9803439803439803,"C. McCollough. TU-FG-207A-04: Overview of the low dose CT grand challenge. Med. Phys., 43
(6Part35):3759–3760, Jun. 2016."
REFERENCES,0.9828009828009828,"Taylor R Moen, Baiyu Chen, David R Holmes III, Xinhui Duan, Zhicong Yu, Lifeng Yu, Shuai
Leng, Joel G Fletcher, and Cynthia H McCollough. Low-dose CT image and projection dataset.
Med. Phys., 48(2):902–911, Feb. 2021."
REFERENCES,0.9852579852579852,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style,
high-performance deep learning library. In Proc. NIPS, volume 32, pp. 8026–8037, Vancouver,
Canada, Dec. 2019."
REFERENCES,0.9877149877149877,"Yuhui Quan, Mingqin Chen, Tongyao Pang, and Hui Ji. Self2Self with dropout: Learning self-
supervised denoising from single image. In Proc. IEEE/CVF CVPR, pp. 1890–1898, Virtual, Jun.
2020."
REFERENCES,0.9901719901719902,"Lin Wang and Kuk-Jin Yoon. Knowledge distillation and student-teacher learning for visual intelli-
gence: A review and new outlooks. IEEE Trans. Pattern Anal. Mach. Intell., Jan. 2021."
REFERENCES,0.9926289926289926,"Yaochen Xie, Zhengyang Wang, and Shuiwang Ji. Noise2Same: Optimizing a self-supervised bound
for image denoising. In Proc. NIPS, volume 33, pp. 20320–20330, Vancouver, Canada, Dec. 2020."
REFERENCES,0.995085995085995,"Jun Xu, Yuan Huang, Li Liu, Fan Zhu, Xingsong Hou, and Ling Shao. Noisy-As-Clean: Learning
unsupervised denoising from the corrupted image. IEEE Trans. Image Process., 29:9316–9329,
Sep. 2020."
REFERENCES,0.9975429975429976,"Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a Gaussian denoiser:
Residual learning of deep CNN for image denoising. IEEE Trans. Image Process., 26(7):3142–
3155, Feb. 2017."
