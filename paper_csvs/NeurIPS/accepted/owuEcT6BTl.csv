Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002398081534772182,"Modern generative models demonstrate impressive capabilities, likely stemming
from an ability to identify and manipulate abstract concepts underlying their train-
ing data. However, fundamental questions remain: what determines the concepts a
model learns, the order in which it learns them, and its ability to manipulate those
concepts? To address these questions, we propose analyzing a model’s learning
dynamics via a framework we call the concept space, where each axis represents
an independent concept underlying the data generating process. By characterizing
learning dynamics in this space, we identify how the speed at which a concept is
learned, and hence the order of concept learning, is controlled by properties of
the data we term concept signal. Further, we observe moments of sudden turns
in the direction of a model’s learning dynamics in concept space. Surprisingly,
these points precisely correspond to the emergence of hidden capabilities, i.e.,
where latent interventions show the model possesses the capability to manipulate
a concept, but these capabilities cannot yet be elicited via naive input prompting.
While our results focus on synthetically defined toy datasets, we hypothesize a
general claim on emergence of hidden capabilities may hold: generative models
possess latent capabilities that emerge suddenly and consistently during training,
though a model might not exhibit these capabilities under naive input prompting."
INTRODUCTION,0.004796163069544364,"1
Introduction"
INTRODUCTION,0.007194244604316547,"Modern generative models, such as text-conditioned diffusion models, show unprecedented capabili-
ties [1–8]. These abilities have led to use of such models in applications as valuable as training control
policies for robotics [9–11] and models for weather forecasting [12], to as drastic as campaigning in
democratic elections [13, 14]. Similar claims can be made for generative models of other modalities,
e.g., large language models (LLMs) [15–18], speech and audio models [14, 19], or even systems
designed for enabling scientific applications such as drug discovery [20, 21]. Arguably, acquiring such
general capabilities requires for models to internalize the data-generating process and disentangle the
concepts (aka latent variables or factors of variation) underlying it [22, 23]. Flexibly manipulating
these concepts can then enable generation of novel samples that are entirely out-of-distribution (OOD)
with respect to the ones used for training [24–28]."
INTRODUCTION,0.009592326139088728,"As shown in prior work, modern generative models do exhibit signs of disentangling concepts
underlying the data generating process and learning of corresponding capabilities to manipulate said"
INTRODUCTION,0.011990407673860911,"* equal contribution, † equal advising (see detailed list). Email: corefranciscopark@g.harvard.edu,
ajyl@umich.edu, {mayaokawa, ekdeeplubana, hidenori_tanaka}@fas.harvard.edu"
INTRODUCTION,0.014388489208633094,38th Conference on Neural Information Processing Systems (NeurIPS 2024).
INTRODUCTION,0.016786570743405275,"Figure 1: Concept Learning Geometry underlies emergence. (a) Top: A multimodal model learns
to generate the concepts in the order of “astronaut”, “horse”, and finally “riding” as it scales up
(adapted from Yu et al.[45]). Middle: “blue square apple” is generated in the order of “apple”,
""blue"", and “square”” (adapted from Li et al.[46]). Bottom: Despite its simplicity, our model trained
on synthetic data shows concept learning dynamics where it first learns “shape” and then “color”.
(b) Concept space is an abstract coordinate space where individual axes correspond to different
concepts and a given point corresponds to a “concept class”, i.e., a predefined collection of concepts
(e.g., large blue circles on bottom left corner). Traversal along axes of the concept space
yield change in a specific property of the sample (e.g., going from large blue circle to large
red circle along object color axis). Trajectories show a model’s dynamics in concept space
for learning to generate classes shown in-distribution (blue nodes) versus out of distribution (pink
/ red nodes). As we show, dynamics in concept space are highly interpretable, enabling precise
comments on which concepts the model learns first, why, and what order it follows. (c) Measuring
how accurately a model generates samples from a given concept class, showing an order of concept
learning: first background color is learned, then size, and then object color."
INTRODUCTION,0.019184652278177457,"concepts [29–37]. At the same time, contemporary work has argued that models’ capabilities can
be unreliable, arbitrarily failing for a given input and succeeding on another [38–44]. Thus, critical
questions remain on how generative models acquire their capabilities (see Fig. 1): what determines
whether the model will disentangle a concept and learn the capability to manipulate it; are all concepts
and corresponding capabilities learned at the same time; and is there a structure to the order of
concept learning such that, given insufficient time, the model learns capabilities to manipulate only a
subset of concepts but not others?"
INTRODUCTION,0.02158273381294964,"This work. To address the questions above, we propose to analyze a model’s learning dynamics at
the granularity of concepts. Since performing such an analysis on realistic, off-the-shelf datasets can
be challenging, we develop synthetic toy datasets of 2D objects with different concepts (e.g., shape,
size, color) that give us thorough control over the data-generating process and allow for an exhaustive
characterization of the model’s learning dynamics (see Fig. 1). Our contributions are as follows."
INTRODUCTION,0.023980815347721823,"• Introducing Concept Space. We propose to evaluate a model’s learning dynamics in the concept
space—an abstract coordinate system whose axes correspond to specific concepts underlying
the data-generating process. We instantiate a notion of underspecification in concept space and
establish its effects on a model’s (in)ability to disentangle concepts.
• Concept Signal Dictates Speed of Learning. We find the speed at which a model disentangles
a concept and learns the capability to manipulate it is dictated by the sensitivity of the data-
generating process to changes in values of said concept—a quantity we call concept signal. We
show concept signals shape the geometry of a learning trajectory and hence control the overall
order of concept learning.
• Sudden Transitions in Concept Learning. We use analytical curves to explain the phenomenol-
ogy of learning dynamics in concept space, showing the dynamics can be divided into two broad
phases: (P1) learning of a hidden capability, whereby even if the model does not produce the
desired output for a given input, there exist systematic latent interventions that lead the model to
generate the desired output, and (P2) learning to generate the desired output from the input space."
INTRODUCTION,0.026378896882494004,"Overall, while our results focus on a toy synthetic task and text-to-image diffusion models, we
hypothesize a broader claim on hidden emergence of latent capabilities holds true: generative models
possess latent capabilities that are learned suddenly and consistently during training, but these
capabilities are not immediately apparent since prompting the model via the input space may not elicit
them, hence hiding how “competent” the model actually is [47, 48]. Empirically, signs of “hidden
capabilities” have already been shown in generative models at scale [45, 49–53], and our results help
provide a formal framework to partially ground such results."
RELATED WORK,0.02877697841726619,"2
Related Work"
RELATED WORK,0.03117505995203837,"Concept learning. The term concepts as used in this work is broadly equivalent to the notion of
factors of variation from prior work on disentangled representation learning [23, 30, 54–61], and is
motivated by use of the term in a similar sense in cognitive science [62–67]. The focus of literature
on disentanglement has been to prove identifiability results, e.g., when will a generative model
trained on a dataset learn to invert the data-generating process, hence identifying the latent variables,
i.e., concepts, that underlie it. Given the success of modern generative models, we argue, despite
impossibility results from prior work, these models are in fact learning to disentangle some, if not
all, concepts. Precisely what guides which concepts are disentangled however requires studying the
learning dynamics—the target of our work."
RELATED WORK,0.03357314148681055,"Interpretability. A growing line of papers demonstrate highly interpretable representations of
intuitive concepts exist in generative models, especially LLMs [68–74]. Similar work in this vein in
image diffusion models has found semantic representations in various components of the model [37,
52, 75–77]: e.g., existence of linear representations for concepts such as 3D depth or object versus
background distinctions [78, 79] . These papers further bolster our argument, modern generative
models are truly inverting the data-generating process and identifying the concepts underlying it."
RELATED WORK,0.03597122302158273,"Competence vs. performance. In cognitive science, a system’s competence on a task is often
contrasted with its performance [47, 48, 80–83]: competence is the system’s possession of a capability
(e.g., to converse in a language) and performance is system’s use of that capability in concrete
situations [80]. For example, a bilingual person may generally converse in their primary language L,
despite possessing knowledge of another language L′, unless it is crucial to use the latter—clearly,
they are competent in both languages, but gauging their performance on L′ requires appropriately
“prompting” them to use it. One can analogize this distinction with remarks on a neural network
possessing a capability versus us being able to elicit it on predefined benchmarks and measure
their performance [84–87]. For example, on the BigBench benchmark [88], LLMs were shown
to have perform poorly on several tasks, but follow up work [89] showed mere chain-of-thought
prompting [49, 50] leads to huge boosts on all tasks. This indicates the evaluated models were in fact
“competent”, but inappropriate prompting led to undermining how “performant” they are."
RELATED WORK,0.03836930455635491,"3
Concept Space: A Framework for Analyzing Concept Learning Dynamics"
RELATED WORK,0.0407673860911271,"We first formally introduce our framework of concept spaces. See Fig. 12 for a schematic. Motivated
by a rich body of literature on disentangled representation learning [23, 30, 54–60], this framework
allows us to systematically analyze how different concepts underlying the data generating process
and corresponding capabilities to manipulate them are learned during training. We highlight that
since our primary empirical focus in this paper will be an abstraction of text-to-image generative
models, parts of the framework will be specifically instantiated with text-to-image generation tasks
in mind. To this end, we note our model class of interest is a generative model F that is trained
using conditioning information h to produce images y. For example, F may be instantiated using a
diffusion model that uses embeddings h of textual descriptions of a scene to produce images y. We
next define a concept space."
RELATED WORK,0.04316546762589928,"Definition 1. (Concept Space.) Consider an invertible data-generating process G : Z →X that
samples vectors z ∼P(Z) from a vector space Z ⊂Rd and maps them to the observation space
X ∈Rn. We assume the sampling prior is factorizable, i.e., P(z ∈Z) = Πd
i=1P(zi), and individual
dimensions of Z correspond to semantically meaningful concepts. Then, a concept space S is defined
as the multidimensional space composed of all possible concept vectors z, i.e., S := {z | z ∼P(Z)}"
RELATED WORK,0.045563549160671464,"As an example, consider a concept space defined using three concepts, say, z1 = shape, z2 = size,
and z3 = color that maps to a dataset of images with objects of different combinations of shapes,
sizes, and colors (see Fig. 2). The assumption that concepts are independently distributed implies
one can intervene on a given concept without affecting the other ones. For example, given a sample
from the dataset above, the concept color in an image can be altered by changing the value of the
relevant latent variable and mapping it to the image space via the data-generating process. Now, using
a mixing function M that yields conditioning information h := M(z), we can train a conditional
generative model and define a notion of capabilities relevant to our work as follows."
RELATED WORK,0.047961630695443645,"Definition 2. (Capability.) A concept class C denotes the set of concept vectors zC such that a subset
of dimensions of these vectors are fixed to predefined values. Classes C and C′ are said to differ in the
kth concept if ∀z ∈zC, there exists z′ ∈zC′ with z[k] ̸= z′[k] and z[i] = z′[i] for i ̸= k. We say a
model possesses the “capability to alter the kth concept” if for any class C whose samples were seen
during training, the model can produce samples from class C′ that differs from C in the kth concept."
RELATED WORK,0.050359712230215826,"Intuitively, the definition above comments on whether the model can flexibly manipulate concepts of
classes seen during training to produce samples from classes that were not seen, i.e., classes that are
entirely out-of-distribution. As an example, consider a concept space with shape, color, and size
as concepts. If shape and color are fixed to circle and blue respectively, we get the class of
blue circles; i.e., ∀z ∈zblue circles, the first and second dimension respectively take on values
that correspond to the shape circle and color blue. Then, given a conditional diffusion model that
was shown blue circles during training, we will say this model possesses the capability to alter
the concept color if it can produce samples from concept classes with the same shape as circles,
but different colors (e.g., red or green circles). Analyzing learning dynamics in the concept
space will thus provide a direct lens into the model’s capabilities as they are acquired."
RELATED WORK,0.05275779376498801,"We also note that the definition above is not dependent on the precise manner via which the model is
prompted to elicit an output, i.e., it need not be the case that the conditioning information h that is used
for training the model is used to evaluate the model capability. In fact, in our experiments, we will
try alternative protocols such as intervening on the latent representations to show that substantially
before the model can be prompted using h to generate samples from an OOD concept class, it can
generate samples from said class via such latent interventions. To this end, we also define a measure
that assesses how much learning signal the data provides towards disentanglement of a concept, and
hence learning of a capability to manipulate it."
RELATED WORK,0.05515587529976019,"Definition 3. (Concept Signal.) The concept signal σi for a concept zi measures the sensitivity of
the data-generating process to change in the value of a concept variable, i.e., σi := |∂G(z)/∂zi|."
RELATED WORK,0.05755395683453238,"Intuitively, if the training objective is factorized at the granularity of concepts, concept signal indicates
how much the model would benefit from learning about a concept. For example, consider a diffusion
model trained using the MSE loss with conditioning h := z to predict the noise added to an image
G(z). σi is then merely the component of the loss representing how much change in conditioning
h yields changes in concept zi, as it is represented in the image. Concept signal will thus serve as
a crucial knob in our experiments to analyze learning dynamics in concept space and the order in
which concepts are learned."
EXPERIMENTAL AND EVALUATION SETUP,0.05995203836930456,"3.1
Experimental and Evaluation Setup"
EXPERIMENTAL AND EVALUATION SETUP,0.06235011990407674,"0 0
1 0 1 1 Color Size 0 1 Color Size"
EXPERIMENTAL AND EVALUATION SETUP,0.06474820143884892,"1 1
0 1"
EXPERIMENTAL AND EVALUATION SETUP,0.0671462829736211,"0 0
1 0
Figure 2:
Concept spaces with differ-
ent concept values see different concept
signal. Consider a concept space com-
prised of concepts size and color. (Left)
The color separation between the classes
is stronger than the size separation, re-
sulting in a stronger concept signal in the
color dimension. (right) The size sepa-
ration between the classes is stronger, thus
resulting in a stronger signal for size."
EXPERIMENTAL AND EVALUATION SETUP,0.06954436450839328,"Before proceeding further, we discuss our experimen-
tal setup that concretely instantiates the formalization
above. Our data-generating process is motivated by
prior work on disentanglement [54–60] and involves
concept classes defined by three concepts, each with
two values: color = {red, blue}, size = {large, small},
and shape={circle, triangle}. In Sec. 4.1 and Sec. 4.4,
we use two attributes: color (with red labeled as 0
and blue as 1) and size (large labeled as 0 and small
as 1). We generate a total of 2048 images for each
class, with objects randomly positioned within each
image. We train models on classes 00 (large red
circles), 01 (large blue circles), and 10 (small
red circles), shown as blue nodes in Fig. 2, and test
using class 11 (small blue circles), shown as pink
nodes, to evaluate a model’s ability to manipulate con-
cepts and generalize OOD (see App. B.1 for further
details). In Sec. 5, we will restrict to two attributes,
shape={circle, triangle} and color = {red, blue}, and
study the effect of noisy conditioning, i.e., what happens
when concepts are correlated in the conditioning information h due to some non-linear mixing"
EXPERIMENTAL AND EVALUATION SETUP,0.07194244604316546,"function. For all experiments, we use a variational diffusion model [90] to generate 3 × 32 × 32
images conditioned on vectors h (see App. B.2 for further training details)."
EXPERIMENTAL AND EVALUATION SETUP,0.07434052757793765,"Evaluation Metric.
To assess whether a generated image matches the desired concept class without
human intervention, we follow literature on disentangled representation learning [23, 30, 54, 55, 91–
94] and train classifier probes for individual concepts using the diffusion model’s training data.
The probe architecture involves a U-Net [95] followed by an average pooling layer and n MLP
classification heads for the n concept variables. See App. B.2 for further details."
LEARNING DYNAMICS IN CONCEPT SPACE,0.07673860911270983,"4
Learning Dynamics in Concept Space"
CONCEPT SIGNAL DETERMINES LEARNING SPEED,0.07913669064748201,"4.1
Concept Signal Determines Learning Speed"
CONCEPT SIGNAL DETERMINES LEARNING SPEED,0.0815347721822542,"Pixel distance
0.002
0.003
0.004
Pixel distance
0.005 0.01
0.015"
CONCEPT SIGNAL DETERMINES LEARNING SPEED,0.08393285371702638,"Color distance
Size distance"
CONCEPT SIGNAL DETERMINES LEARNING SPEED,0.08633093525179857,"Figure 3: Concept signal determines learn-
ing speed.
The speed of concept learning
as an inverse of the time in gradient steps
when the separation in color (left) and size
(right) between different classes increases.
Concept learning is faster when pixel differ-
ences among concept class and hence con-
cepts are larger."
CONCEPT SIGNAL DETERMINES LEARNING SPEED,0.08872901678657075,"We first demonstrate the utility of concept signal as a
tool to gauge at what rate the model learns a concept
and the capability to manipulate it. To this end, we
develop controlled variants of our data-generating
process by changing the level of concept signal of
each concept and train diffusion models conditioned
with the latent concept vector z on them. We primar-
ily focus on concepts color and size, altering their
concept signal by respectively adjusting the RGB con-
trast between red and blue and the size difference
between large and small objects (see App. B.1 for
details). We define the speed of learning each concept
as inverse of the number of gradient steps required to
reach 80% accuracy for class 11, i.e., the OOD class
that requires learning the capability to manipulate
concepts as seen during training. Results comparing
different concept signals are shown in Fig. 3. For
both color and size, we observe that concept sig-
nal dictates the speed at which individual concepts
are learned. We also find that when different concepts have varying strengths of concept signals, this
leads to differences in the learning speed for each concept."
CONCEPT SIGNAL GOVERNS GENERALIZATION DYNAMICS,0.09112709832134293,"4.2
Concept Signal Governs Generalization Dynamics"
CONCEPT SIGNAL GOVERNS GENERALIZATION DYNAMICS,0.09352517985611511,"We next examine the model’s learning dynamics in concept space under various levels of concept
signal for the concepts color and size. For completeness, we evaluate a model’s ability to generalize"
CONCEPT SIGNAL GOVERNS GENERALIZATION DYNAMICS,0.09592326139088729,Color concept signal
CONCEPT SIGNAL GOVERNS GENERALIZATION DYNAMICS,0.09832134292565947,"0 0
1 0"
CONCEPT SIGNAL GOVERNS GENERALIZATION DYNAMICS,0.10071942446043165,"1 1
(a)"
CONCEPT SIGNAL GOVERNS GENERALIZATION DYNAMICS,0.10311750599520383,"Color
Color Size Size"
CONCEPT SIGNAL GOVERNS GENERALIZATION DYNAMICS,0.10551558752997602,"0 0
1 0"
CONCEPT SIGNAL GOVERNS GENERALIZATION DYNAMICS,0.1079136690647482,"1 1
0 1
0 1
(b)
OOD generalization
ID generalization for class 00"
CONCEPT SIGNAL GOVERNS GENERALIZATION DYNAMICS,0.11031175059952038,"Figure 4: Concept signal governs generalization dynamics. (a) Learning dynamics in the concept
space for in-distribution concept class 00 (bottom left). (b) Learning dynamics for out-of-distribution
(OOD) concept class 11 (top right). We plot the accuracy for color on the x-axis and size on the
y-axis. The [0,1) normalized color concept signal level is color coded. Two trajectories for 01 and
10 are shown to illustrate concept memorization. See App. D.3 for uncertainties."
CONCEPT SIGNAL GOVERNS GENERALIZATION DYNAMICS,0.11270983213429256,"both in-distribution (ID) and OOD, but only the latter is deemed learning of a capability to manipulate
a concept. Results are shown in Fig. 4, with panel (a) showing dynamics for learning to generate
samples from ID class 00 and panel (b) showing dynamics for OOD class 11. Results on OOD
generalization show concept memorization, which we define as the phenomenon where the model’s
generations on an OOD conditioning h are biased towards the training class that helps define the
strongest concept signal. For example, for the unseen conditioning 11, the generations are more
alike 01 when the concept signal is stronger for size (e.g., blue curve in Fig. 4(b)) and alike 10
when the signal is stronger for color (e.g., red curve). Interestingly, we observe that for settings
with high imbalance in concept signals, e.g., the blue curve in Fig. 4 (b), the endpoint of concept
memorization is very biased towards one training class, here 01, delaying its out-of-distribution
(OOD) generalization. For the learning trajectories of all classes, see Fig. 15 in App. D.2."
CONCEPT SIGNAL GOVERNS GENERALIZATION DYNAMICS,0.11510791366906475,"Broadly, our results imply that an early stopped text-to-image model can witness concept memoriza-
tion and hence simply associate an unseen conditioning to the nearest concept class when asked to
generate OOD samples (see Kang et al. [96] for a similar result in LLMs). However, given sufficient
time, the model will disentangle concepts underlying the data-generating process and learn to generate
entirely novel, OOD samples. For futher evidence in this vein, we also confirm our results across
more general scenarios, including with the real-world CelebA dataset (App. D.4) and using three
concept variables: color, background color, and size (App. D.5)."
TOWARDS A LANDSCAPE THEORY OF LEARNING DYNAMICS,0.11750599520383694,"4.3
Towards a Landscape Theory of Learning Dynamics Color (a) Size (b)"
TOWARDS A LANDSCAPE THEORY OF LEARNING DYNAMICS,0.11990407673860912,"0 0
1 0"
TOWARDS A LANDSCAPE THEORY OF LEARNING DYNAMICS,0.1223021582733813,"1 1
0 1 Color Size"
TOWARDS A LANDSCAPE THEORY OF LEARNING DYNAMICS,0.12470023980815348,"0 0
1 0"
TOWARDS A LANDSCAPE THEORY OF LEARNING DYNAMICS,0.12709832134292565,"1 1
0 1 a"
TOWARDS A LANDSCAPE THEORY OF LEARNING DYNAMICS,0.12949640287769784,"Figure 5: A Phenomenological Model of Learn-
ing Dynamics in Concept Space. Using Eq. 1,
we simulate the learning trajectory for concept
class 00 in panel (a) and the OOD class 11 in panel
(b). Initially, target values are set at (0, 1) or (1, 0)
based on the concept signal strengths for color
or size, respectively. As the model progressively
learns each concept, the target values gradually
shift towards (1, 1). This simple toy model accu-
rately reproduces the observed curves in Fig. 3(c),
which arise from concept memorization."
TOWARDS A LANDSCAPE THEORY OF LEARNING DYNAMICS,0.13189448441247004,"Fig. 4 indicates models undergo phases of un-
derstanding of concepts at different stages of
training. In fact, an intriguing property of tra-
jectories shown in Fig. 4 (b) is that there is a
sudden turn in the learning dynamics from con-
cept memorization to OOD generalization (e.g.,
see the top-left quadrant in Fig. 4 (b)). To in-
vestigate this further, we propose a minimal toy
model that captures the geometry of model’s
learning trajectories shown in that figure. Specif-
ically, we use the following dynamics equation,
d(t) := ˜z + (ˆz −˜z) ·
1
1+e−(t−ˆt) , where ˆz is the
target point we want to get to and ˜z is the ini-
tial, “biased” target. For example, consider the
case with color and size concepts in Fig. 4 (b).
The model’s generated samples are more alike
class 01 and biased towards (˜z1, ˜z2) = (0, 1)
when the size concept signal σ2 is stronger than
σ1; and to 10, (˜z1, ˜z2) = (1, 0) when the color
concept signal σ1 is stronger than σ2. We define (ˆz1, ˆz2) as the target values or directions we want
the learning to head towards (e.g., (1, 1) for OOD generalization). Based on this framework, we can
derive the following energy function. dz"
TOWARDS A LANDSCAPE THEORY OF LEARNING DYNAMICS,0.1342925659472422,"dt = −∇zL, L(z1, z2) ="
TOWARDS A LANDSCAPE THEORY OF LEARNING DYNAMICS,0.1366906474820144,"(
1
2a
 
d(t −ˆt1) −z1
2 + a"
TOWARDS A LANDSCAPE THEORY OF LEARNING DYNAMICS,0.13908872901678657,"2(1 −z2)2
if σ1 > σ2,"
TOWARDS A LANDSCAPE THEORY OF LEARNING DYNAMICS,0.14148681055155876,"1
2a(1 −z1)2 + a"
TOWARDS A LANDSCAPE THEORY OF LEARNING DYNAMICS,0.14388489208633093,"2
 
d(t −ˆt2) −z2
2
otherwise.
(1)"
TOWARDS A LANDSCAPE THEORY OF LEARNING DYNAMICS,0.14628297362110312,"Here, a is determined by the difference |σ1 −σ2| and ˆt1 and ˆt2 denote the times when the model
learns concepts z1 and z2, respectively. Fig. 5 illustrates the simulated trajectories for classes 00 and
11, based on Eq. 1. Panels (a) and (b) correspond to classes 00 and 11, respectively. We define the
actual target points (ˆz1, ˆz2) as (0, 0) for class 00 and (1, 1) for class 11. For the initial targets (˜z1, ˜z2),
we set both values to (0, 0) for class 00. For class 11, the targets are set to (1, 0) when σ1 > σ2 and
to (0, 1) when σ1 < σ2. We find our toy model effectively captures the actual learning dynamics for
both in-distribution (Fig. 3(b)) and out-of-distribution (OOD) concept classes (Fig. 3(c)). Notably,
our simulation accurately replicates the two types of curves: clockwise (blue trajectory in Fig. 3(b))
and counterclockwise (red trajectory)."
TOWARDS A LANDSCAPE THEORY OF LEARNING DYNAMICS,0.1486810551558753,"An important conclusion that follows from the results above is that the network’s learning dynamics
can be precisely decomposed into two stages, hence yielding the sudden turns seen in trajectories in"
TOWARDS A LANDSCAPE THEORY OF LEARNING DYNAMICS,0.1510791366906475,"(a)
(b)
(c)"
TOWARDS A LANDSCAPE THEORY OF LEARNING DYNAMICS,0.15347721822541965,"Figure 6: Emergence of hidden capabilities. We plot accuracy as a function of gradient steps for five
different runs, using three different protocols for prompting the model to generate outputs for OOD
concept classes. (a) The baseline, naive prompting protocol; (b) linear latent intervention, applied in
the activation space; and (c) overprompting, akin to an intervention on the input space."
TOWARDS A LANDSCAPE THEORY OF LEARNING DYNAMICS,0.15587529976019185,"Fig. 4. We hypothesize there is a phase change underlying this decomposition and the model acquires
the capability to alter concepts at this point of phase change. We investigate this next."
SUDDEN TRANSITIONS IN CONCEPT LEARNING DYNAMICS,0.15827338129496402,"4.4
Sudden Transitions in Concept Learning Dynamics"
SUDDEN TRANSITIONS IN CONCEPT LEARNING DYNAMICS,0.1606714628297362,"The concept space visualization of learning dynamics observed in Fig. 4 (b) and our toy models
analysis in Fig. 5 indicate that there exists a phase in which the model departs from concept mem-
orization and disentangles each of the concepts, but still produces incorrect images. We claim
that at the point of departure, the model has in fact already disentangled concepts underlying the
data-generating process and acquired the relevant capabilities to manipulate them, hence yielding
a change of direction in its learning trajectory. However, naive input prompting is insufficient to
elicit these capabilities and generate samples from OOD classes, giving the impression the model
is not yet “competent”. This then leads to the second phase in the learning dynamics, wherein an
alignment between the input space and underlying concept representations is learned. We take the
model corresponding to the second from left curve (the green curve) in Fig. 4 (b) to investigate this
claim in detail. Specifically, given intermittent checkpoints along the model’s learning trajectory, we
use the following two protocols for prompting the model to produce images corresponding to the
class 11 (blue, small). See App. C for further details."
SUDDEN TRANSITIONS IN CONCEPT LEARNING DYNAMICS,0.1630695443645084,"1. Activation Space: Linear Latent Intervention. Given conditioning vectors h, during inference
we add or subtract components that correspond to specific concepts (e.g., hblue).
2. Input Space: Overprompting. We simply enhance the color conditioning to values of higher
magnitude, e.g. (r, g, b) = (0.4, 0.4, 0.6) to (0.3, 0.3, 0.7)."
SUDDEN TRANSITIONS IN CONCEPT LEARNING DYNAMICS,0.16546762589928057,"Fig. 6 shows the accuracy for five independent runs under: (a) naive input prompting, (b) linear latent
interventions, and (c) overprompting. In Fig. 6 (a), we observe that some runs can produce samples
from the target concept class (blue, small) with ∼100% accuracy after around 8,000 gradient steps,
while other runs fail to do so. However, in Fig. 6 (b, c), we find alternative protocols for prompting
the model can consistently elicit the desired outputs much earlier than input prompting, e.g., at around
as early as 6,000 gradient steps. This indicates the model does possess the capability to alter concepts
and generalize OOD! Furthermore, we note that different protocols enable elicitation of the capability
at approximately the same number of gradient steps, irrespective of the seed, and that this is precisely
the point of sudden turn in the learning dynamics in Fig. 4! Interestingly, experiments with Classifier
Free Guidance (CFG) [97] show that CFG only becomes effective after this transition (Fig. 21)."
SUDDEN TRANSITIONS IN CONCEPT LEARNING DYNAMICS,0.16786570743405277,"We further explore the second phase of learning in Appendix D.7 by patching the embedding module
used for processing the conditioning information from the final checkpoint to an intermediate one.
Our results show that when the final checkpoint does enable use of naive input prompting for eliciting
a capability, the embedding module can be patched to an intermediate checkpoint and we can retrieve
the desired output at approximately the same time that alternative prompting protocols start to work
well. This suggests the second phase of learning primarily involves aligning the input space to
intermediate representations that enable eliciting the model capabilities. Overall, our results above
yield the following hypothesis."
SUDDEN TRANSITIONS IN CONCEPT LEARNING DYNAMICS,0.17026378896882494,"Figure 7: Validating our Findings on CelebA. (a) Concept space dynamics of generated images
concepts Gender and With Hat. We find the generalization class (Female, With Hat) ends up near
(Female, No Hat). (b) Compositional Generalization Accuracy when performing latent interventions
vs. naive prompting. This rise of accuracy near 5 × 105 gradient steps clearly demonstrates that the
model is capable of generalizing out-of-distribution, but is not performant when evaluated via naive
prompting."
SUDDEN TRANSITIONS IN CONCEPT LEARNING DYNAMICS,0.17266187050359713,"Hypothesis 1. (Emergence of Hidden Capabilities.) Generative models possess hidden capabilities
that are learned suddenly and consistently during training, but naive input prompting may not elicit
these capabilities, hence hiding how “competent” the model actually is."
ADDITIONAL RESULTS ON REALISTIC DATA,0.1750599520383693,"4.5
Additional Results on Realistic Data"
ADDITIONAL RESULTS ON REALISTIC DATA,0.1774580335731415,"To provide further support for our hypothesis, we provide results in a more realistic and natural-
istic data. Specifically, we use the CelebA dataset [98], which contains fine-grained attributes
corresponding to concepts like Gender, With Hat, and Smiling, and analyze two settings."
ADDITIONAL RESULTS ON REALISTIC DATA,0.17985611510791366,"• Using concepts Gender and Smiling. Results are in Fig. 18, where we find that concept learning
dynamics discovered in the relatively simple setting in prior sections generalizes. We see that
there is first a phase of concept memorization, wherein the model learns to generate images
of (Female, Smiling). However, as training proceeds, there is a sudden turn in the learning
dynamics and the model learns to generalize out-of-distribution.
• Using concepts Gender and With Hat. In this setting, given a compute budget of 2M gradient
steps, we find that the model seem to never learn to generalize out-of-distribution (see Fig. 7).
However, when we perform latent interventions on the model’s representations, we are able to
force the model to produce images from the class (Female, With Hat). These results reiterate
that the model is indeed capable of generalizing out-of-distribution, but is not performant when
naively prompted."
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.18225419664268586,"5
Effect of Underspecification on Learning Dynamics in Concept Space"
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.18465227817745802,"Figure 8: Underspecification de-
lays out-of-distribution (OOD)
generalization.
The number of
gradient steps required to reach ac-
curacy above 0.8, as the percent-
age of masked prompts increases.
A higher proportion of masked
prompts slows down the speed of
concept learning."
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.18705035971223022,"In the results above, we use conditioning information that per-
fectly specifies concepts underlying the data-generating process,
i.e., h = z. In practice, however, instructions are underspec-
ified and one can thus expect correlations between concepts
in the conditioning information extracted from those instruc-
tions [99–103]. For example, images of a strawberry are
often correlated with the color red (see Fig. 9(a)). Corre-
spondingly, unless a text-to-image model is shown explicit data
stating “red strawberry” or images of non-red strawberries,
the model’s ability to disentangle the concept color from the
concept strawberry may be impeded (see generations for
“yellow strawberry” in Fig. 9). Motivated by this, we next
investigate the effects of using underspecified conditioning in-
formation on a model’s ability to learn concepts and capabilities
to manipulate them."
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.18944844124700239,"Experimental setup.
The data generation and evaluation
process follows the protocol described in Sec. 3.1. We select"
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.19184652278177458,"Figure 9: Underspecification and Concept Learning. (a) The state-of-the-art generative mod-
els [104] erroneously produces a red strawberry (top right corner) for the prompt “yellow
strawberry”. (b) Without underspecification in the training data, a model F accurately learns the
concepts of shape and color, successfully generalizes to the unseen node blue triangle (leftmost).
As masks are applied to the word red for the prompt red triangle, concept signal for triangle
increasingly starts to correlate with the concept red. This causes the output images to change from
blue to purple as the level of masking increases (panels left to right). Eventually, the color dimension
for triangle collapses, biasing the model towards generating solely red triangles (rightmost). Color Shape"
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.19424460431654678,"(a)
(b)"
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.19664268585131894,"0 0
1 0"
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.19904076738609114,100% 80%  60%  50%  40%  20%  0%
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.2014388489208633,"0 1
1 1 Color Shape"
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.2038369304556355,"0 0
1 0"
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.20623501199040767,"0 1
1 1"
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.20863309352517986,Masked:
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.21103117505995203,Percentage of mask α
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.21342925659472423,"Figure 10: Underspecification hinders out-of-distribution (OOD) generalization. (a) The learning
dynamics with varying levels of prompt masking, from 0% to 100%, and the generated images. At
0% masking (top right image), the model correctly produces an image of blue triangle from the
prompt “blue triangle.” As the masking increases (from right to left), the images gradually shift
towards the incorrect color, red. (b) The simulation of the learning dynamics under underspecification
in concept space based on Eq. 2. Our toy model replicates a trained network’s learning dynamics."
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.2158273381294964,"color (red and blue) and shape (circle and triangle) as the concepts, drawing an analogy to the
“yellow strawberry” example. To simulate underspecification, we randomly select training sam-
ples that have a specific combination of shape and color (e.g., “red triangle”). We then mask
the token representing the color (“red”) and train the model on three concept classes {00, 01, 10},
represented by blue nodes in Fig. 9, with some prompts masked. We test using class 11 (pink node)
with no prompts masked to see if the model can generalize OOD."
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.2182254196642686,"Underspecification Delays and Hinders OOD generalization. Fig. 8 shows how underspecification
(masked prompts) affects the speed of concept learning. We see that as the percentage of masked
prompts increases, the speed of learning a concept decreases, suggesting that underspecification leads
to slower learning of concepts. Further, Fig. 10 (a) shows models’ learning dynamics in concept
space at varying levels of underspecification. With 0% masking, the model accurately produces an
image of blue triangle (see Fig. 9(b)). However, as the percentage of masking increases, the
color of generated images shifts from blue to purple (middle), and finally red. This demonstrates
that when prompts are masked, the model’s understanding of shape triangle becomes intertwined
with color red; even when blue is specified in the prompt, the dynamics are biased towards red."
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.22062350119904076,"Toy Model of Learning Dynamics with Underspecification.
When prompts are masked (i.e.,
underspecification occurs), target values for the concept variables are shifted: e.g., in our setup,
with no mask applied, the target directions for the class 11 is (1, 1). When the word “red” in “red
triangle” is fully masked, the target shifts to (0, 1). Assuming this shift is linear with respect to
the percentage of masked prompts α, we can derive the following energy function. dz"
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.22302158273381295,"dt = −∇zL, L(z1, z2) =
 
(1 −sα) −z1
2 +
 
1 −z2
2.
(2)"
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.22541966426858512,"In the above, parameter s represents the impact of underspecification. Fig. 10 (b) shows the simulation
of model behavior in the concept space according to Eq. 2 (s = 0.01). As the masking level increases,
the target directions shift from z2 = 1 in the top right corner to z2 = 0 in the top left corner. Our
simulated dynamics thus match well with the model’s learning dynamics shown in Fig. 10 (a)."
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.2278177458033573,"(a)
(b)"
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.2302158273381295,"Figure 11: Underspecification and Hidden
Capabilities.
We use the over-prompting
protocol from Sec. 4.4 to assess if the ca-
pabilities to enable OOD generalization are
learned before naive input prompting. (a) Ac-
curacy for OOD generalization across five
different seed runs under naive prompting;
(b) Accuracy under over-prompting, with a
fixed masking percentage of 50%."
EFFECT OF UNDERSPECIFICATION ON LEARNING DYNAMICS IN CONCEPT SPACE,0.23261390887290168,"Influence of Underspecification on Emergence of
Hidden Capabilities.
Following Sec. 4.4, we also
explore whether it is possible to elicit the desired out-
puts from a model trained with underspecified data.
Fig. 11 shows the accuracy results over five runs (a)
without using any prompting method, and (b) using
over-prompting. In both scenarios, the percentage of
masking is set at 50%. Results clearly demonstrate that
with over-prompting, the model achieves 100% accu-
racy after approximately 1,000 gradient steps, whereas
without over-prompting, it fails in three out of five runs
even after 2,000 gradient steps. These findings con-
firm that capability can develop prior to observable
behavior, even in cases of underspecification."
DISCUSSION,0.23501199040767387,"6
Discussion"
DISCUSSION,0.23741007194244604,"Why study the concept space? One might ask why concept space could be useful beyond synthetic
datasets. In Fig. 14, we show the loss, accuracy, and training trajectory in concept space of a model
from Fig. 6. The moment during training when the model acquires the capability to manipulate size
and color independently is not evident from either the loss or accuracy curve. However, in concept
space, one can directly see the divergence of the trajectory from concept memorization. Benchmarking
generative models is a challenging task, still often involving humans in the loop [105, 106]. Our
concept space framework suggests that benchmarking core generalization capabilities can potentially
be reduced to monitoring the learning trajectory in concept space. Moreover, this information can be
used to develop better training schemes as discussed in Ren et al. [107]."
DISCUSSION,0.23980815347721823,"Concept learning vs. grokking
We make a distinction between the potentially delayed elicitation
of capabilities in our model versus grokking [108]. The phenomenology of delayed increase in
performance on the test set, as seen in Fig. 14, is shared. However, we deal with out-of-distribution
evaluations, which is different from setups like modular arithmetic or polynomial regression in
which grokking is usually studied [108–112]. Secondly, research on grokking using hidden progress
measures indicates that the model gradually builds representations toward an ideal one [111, 113];
however, our results find that even at the level of latent representations, there is a sudden emergence
whereby the model learns the capability to manipulate concepts underlying the data-generating
process and generalize OOD."
DISCUSSION,0.2422062350119904,"Is concept learning a phase transition?
In Sec. 4.4, we have demonstrated that before the
capability to manipulate a concept is learned, the desired outputs cannot be generated regardless
of the protocol used to prompt the model. Moreover, we showed that different protocols yield the
desired outputs at the exact same time, which is also highly independent of model initialization
(Fig. 6, Fig. 17). We thus hypothesize that: Concept learning is a well-controlled phase transition
in model capability. However, the ability to elicit this capability through predefined, single-input
prompting can be delayed arbitrarily, depending on factors like the strength of the concept signal."
DISCUSSION,0.2446043165467626,"Limitations
One limitation of our work is that the compositional setup used in this study is rather
simple. Real world concepts are often hierarchical [114, 115] or relational [116]. We also make
assumptions that concepts are linearly embedded in the vector space Z in Sec. 3. However, this
assumption seems to be at least partially justified as seen in Sec. D.8. Another limitation is that the
findings are mainly drawn from synthetic data. While we show that our findings generalize to realistic
data to some extent, as seen in Sec 4.5, Fig. 18 and Fig. 7, further studies are needed to investigate
how our findings apply to real data in general."
DISCUSSION,0.24700239808153476,Acknowledgments and Disclosure of Funding
DISCUSSION,0.24940047961630696,"CFP and HT gratefully acknowledges the support of Aravinthan D.T. Samuel. CFP, MO, ESL and HT
are supported by NTT Research under the CBS-NTT Physics of Intelligence program. ESL’s time at
University of Michigan was partially supported by the National Science Foundation (IIS-2008151
and CNS-2211509). The computations in this paper were run on the FASRC cluster supported by
the FAS Division of Science Research Computing Group at Harvard University. The authors thanks
Zechen Zhang, Helena Casademunt, Carolina Cuesta-Lazaro, Shivam Raval, Yongyi Yang and Itamar
Pres for useful discussions."
DISCUSSION,0.2517985611510791,Contributions
DISCUSSION,0.2541966426858513,"Motivated by prior work from MO, ESL and HT [30], CFP and HT started investigating the order in
which concepts are learned by a diffusion model, leading to CFP identifying the notion of concept
signal and HT proposing to study the learning dynamics in concept space. This kickstarted the
project. ESL and HT defined the formal setup (Sec. 3). ESL hypothesized the point of sudden turn
marks the emergence of hidden abilities, hence defining the project narrative around competence
versus performance. AL and CFP led the experimental verification of this hypothesis. AL led
identification of linear representation of concepts and performed the latent and MLP intervention,
while CFP contributed to the input space one. MO and HT led formulation of the landscape theory
(Sec. 4.3), with inputs from CFP. MO led the underspecification picture of learning dynamics, with
inputs from ESL and HT (Sec. 5). Paper writing was led by MO and ESL, with inputs from all
authors. Visualizations were mainly led by MO, with inputs from HT and CFP. Experiments on
CelebA learning dynamics were conducted by CFP in discussions with MO and corresponding latent
intervention were led by AL."
REFERENCES,0.2565947242206235,References
REFERENCES,0.2589928057553957,"[1] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri,
and Michal Irani. Imagic: Text-based real image editing with diffusion models. arXiv preprint
arXiv:2210.09276, 2022."
REFERENCES,0.26139088729016785,"[2] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe
Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video
generation models as world simulators. 2024. URL https://openai.com/research/
video-generation-models-as-world-simulators."
REFERENCES,0.2637889688249401,"[3] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung,
Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: A large
language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023."
REFERENCES,0.26618705035971224,"[4] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,
Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High
definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022."
REFERENCES,0.2685851318944844,"[5] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad
Norouzi. Image super-resolution via iterative refinement. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2022."
REFERENCES,0.2709832134292566,"[6] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using
2d diffusion. arXiv preprint arXiv:2209.14988, 2022."
REFERENCES,0.2733812949640288,"[7] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla,
Pratul Srinivasan, Jonathan T Barron, and Ben Poole. Cat3d: Create anything in 3d with
multi-view diffusion models. arXiv preprint arXiv:2405.10314, 2024."
REFERENCES,0.27577937649880097,"[8] Ziyang Chen, Daniel Geng, and Andrew Owens. Images that sound: Composing images and
sounds on a single canvas. arXiv preprint arXiv:2405.12221, 2024."
REFERENCES,0.27817745803357313,"[9] Yilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B Tenenbaum, Dale
Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation.
arXiv e-prints, pages arXiv–2302, 2023."
REFERENCES,0.2805755395683453,"[10] Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet,
Tianhe Yu, Pieter Abbeel, Joshua B Tenenbaum, et al. Video language planning. arXiv preprint
arXiv:2310.10625, 2023."
REFERENCES,0.2829736211031175,"[11] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes,
Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative
interactive environments. arXiv preprint arXiv:2402.15391, 2024."
REFERENCES,0.2853717026378897,"[12] Samar Khanna, Patrick Liu, Linqi Zhou, Chenlin Meng, Robin Rombach, Marshall Burke,
David Lobell, and Stefano Ermon. Diffusionsat: A generative foundation model for satellite
imagery. arXiv preprint arXiv:2312.03606, 2023."
REFERENCES,0.28776978417266186,"[13] Yan Zhuang (New York Times).
Imran Khan’s ‘Victory Speech’ From Jail Shows
A.I.’s Peril and Promise, 2024. https://www.nytimes.com/2024/02/11/world/asia/
imran-khan-artificial-intelligence-pakistan.html."
REFERENCES,0.290167865707434,"[14] Mark
Sullivan
(FastCompany).
AI
deepfakes
get
very
real
as
2024
elec-
tion
season
begins,
2024.
https://www.fastcompany.com/91020077/
ai-deepfakes-taylor-swift-joe-biden-2024-election."
REFERENCES,0.29256594724220625,"[15] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece
Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general
intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023."
REFERENCES,0.2949640287769784,"[16] Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of
context. Technical report, Google DeepMind, 2024. https://storage.googleapis.com/
deepmind-media/gemini/gemini_v1_5_report.pdf."
REFERENCES,0.2973621103117506,"[17] OpenAI. GPT-4 System Card. Technical report, OpenAI, 2023. https://cdn.openai.com/
papers/gpt-4-system-card.pdf."
REFERENCES,0.2997601918465228,"[18] Claude team. Introducing the next generation of Claude. Technical report, Anthropic AI, 2024."
REFERENCES,0.302158273381295,https://www.anthropic.com/news/claude-3-family.
REFERENCES,0.30455635491606714,"[19] Generative Media Team (Google Deepmind). Generating audio for video, 2024. https:
//deepmind.google/discover/blog/generating-audio-for-video/."
REFERENCES,0.3069544364508393,"[20] Shengchao Liu, Yanjing Li, Zhuoxinran Li, Anthony Gitter, Yutao Zhu, Jiarui Lu, Zhao Xu,
Weili Nie, Arvind Ramanathan, Chaowei Xiao, et al. A text-guided protein design framework.
arXiv preprint arXiv:2302.04611, 2023."
REFERENCES,0.30935251798561153,"[21] MIT
News.
Speeding
up
drug
discovery
with
diffusion
generative
models,
2024.
https://news.mit.edu/2023/
speeding-drug-discovery-with-diffusion-generative-models-diffdock-0331."
REFERENCES,0.3117505995203837,"[22] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and
new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):
1798–1828, 2013."
REFERENCES,0.31414868105515587,"[23] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard
Schölkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised
learning of disentangled representations. In Proc. int. conf. on machine learning (ICML), 2019."
REFERENCES,0.31654676258992803,"[24] Jivat Neet Kaur, Emre Kiciman, and Amit Sharma. Modeling the data-generating process is
necessary for out-of-distribution generalization. arXiv preprint. arXiv:2206.07837, 2022."
REFERENCES,0.31894484412470026,"[25] Liwei Jiang, Jena D Hwang, Chandra Bhagavatula, Ronan Le Bras, Jenny Liang, Jesse Dodge,
Keisuke Sakaguchi, Maxwell Forbes, Jon Borchardt, Saadia Gabriel, et al. Can machines learn
morality? the delphi experiment. arXiv e-prints, pages arXiv–2110, 2021."
REFERENCES,0.3213429256594724,"[26] Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner,
Anirudh Goyal, and Yoshua Bengio. Towards causal representation learning. arXiv preprint.
arXiv:2102.11107, 2021."
REFERENCES,0.3237410071942446,"[27] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference:
foundations and learning algorithms. The MIT Press, 2017."
REFERENCES,0.3261390887290168,"[28] Jean Kaddour, Aengus Lynch, Qi Liu, Matt J Kusner, and Ricardo Silva. Causal machine
learning: A survey and open problems. arXiv preprint arXiv:2206.15475, 2022."
REFERENCES,0.328537170263789,"[29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical
text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022."
REFERENCES,0.33093525179856115,"[30] Maya Okawa, Ekdeep Singh Lubana, Robert P. Dick, and Hidenori Tanaka. Compositional
abilities emerge multiplicatively: Exploring diffusion models on a synthetic task, 2024."
REFERENCES,0.3333333333333333,"[31] Hu Yu, Hao Luo, Fan Wang, and Feng Zhao. Uncovering the text embedding in text-to-image
diffusion models. arXiv preprint arXiv:2404.01154, 2024."
REFERENCES,0.33573141486810554,"[32] Lingjing Kong, Guangyi Chen, Biwei Huang, Eric P Xing, Yuejie Chi, and Kun Zhang.
Learning discrete concepts in latent hierarchical models. arXiv preprint arXiv:2406.00519,
2024."
REFERENCES,0.3381294964028777,"[33] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale, Trung Bui, Tong Yu, Zhe Lin,
Yang Zhang, and Shiyu Chang. Uncovering the disentanglement capability in text-to-image
diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 1900–1910, 2023."
REFERENCES,0.3405275779376499,"[34] Rohit Gandikota, Joanna Materzynska, Tingrui Zhou, Antonio Torralba, and David Bau.
Concept sliders: Lora adaptors for precise control in diffusion models.
arXiv preprint
arXiv:2311.12092, 2023."
REFERENCES,0.34292565947242204,"[35] Nan Liu, Yilun Du, Shuang Li, Joshua B Tenenbaum, and Antonio Torralba. Unsupervised
compositional concepts discovery with text-to-image generative models. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pages 2085–2095, 2023."
REFERENCES,0.34532374100719426,"[36] Michel Besserve, Arash Mehrjou, Rémy Sun, and Bernhard Schölkopf. Counterfactuals
uncover the modular structure of deep generative models. arXiv preprint. arXiv:1812.03253,
2018."
REFERENCES,0.34772182254196643,"[37] Zihao Wang, Lin Gui, Jeffrey Negrea, and Victor Veitch. Concept algebra for (score-based)
text-controlled generative models. Advances in Neural Information Processing Systems, 36,
2024."
REFERENCES,0.3501199040767386,"[38] Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip HS Torr, Adel
Bibi, Samuel Albanie, and Matthias Bethge.
No"" zero-shot"" without exponential data:
Pretraining concept frequency determines multimodal model performance. arXiv preprint
arXiv:2404.04125, 2024."
REFERENCES,0.35251798561151076,"[39] Colin Conwell and Tomer Ullman. Testing relational understanding in text-guided image
generation. arXiv preprint arXiv:2208.00005, 2022."
REFERENCES,0.354916067146283,"[40] Colin Conwell and Tomer Ullman. A comprehensive benchmark of human-like relational
reasoning for text-to-image foundation models. In ICLR 2023 Workshop on Mathematical and
Empirical Understanding of Foundation Models, 2023."
REFERENCES,0.35731414868105515,"[41] Evelina Leivada, Elliot Murphy, and Gary Marcus. Dall-e 2 fails to reliably capture common
syntactic processes. arXiv preprint arXiv:2210.12889, 2022."
REFERENCES,0.3597122302158273,"[42] Tejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, Eric Horvitz, Ece Kamar, Chitta
Baral, and Yezhou Yang. Benchmarking spatial relationships in text-to-image generation.
arXiv preprint arXiv:2212.10015, 2022."
REFERENCES,0.36211031175059955,"[43] Gautam Singh, Fei Deng, and Sungjin Ahn. Illiterate dall-e learns to compose. arXiv preprint
arXiv:2110.11405, 2021."
REFERENCES,0.3645083932853717,"[44] Royi Rassin, Shauli Ravfogel, and Yoav Goldberg. Dalle-2 is seeing double: flaws in word-to-
concept mapping in text2image models. arXiv preprint arXiv:2210.10606, 2022."
REFERENCES,0.3669064748201439,"[45] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay
Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive
models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022."
REFERENCES,0.36930455635491605,"[46] Hao Li, Yang Zou, Ying Wang, Orchid Majumder, Yusheng Xie, R Manmatha, Ashwin
Swaminathan, Zhuowen Tu, Stefano Ermon, and Stefano Soatto. On the scalability of diffusion-
based text-to-image generation. arXiv preprint arXiv:2404.02883, 2024."
REFERENCES,0.37170263788968827,"[47] Raphaël Millière and Cameron Buckner. A philosophical introduction to language models –
part i: Continuity with classic debates, 2024."
REFERENCES,0.37410071942446044,"[48] Raphaël Millière and Cameron Buckner. A philosophical introduction to language models-part
ii: The way forward. arXiv preprint arXiv:2405.03207, 2024."
REFERENCES,0.3764988009592326,"[49] Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
Large language models are zero-shot reasoners. In Advances in Neural Information Processing
Systems, volume 35, 2022."
REFERENCES,0.37889688249400477,"[50] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny
Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint
arXiv:2201.11903, 2022."
REFERENCES,0.381294964028777,"[51] Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid,
Tamera Lanham, Daniel M Ziegler, Tim Maxwell, Newton Cheng, et al. Sleeper agents:
Training deceptive llms that persist through safety training. arXiv preprint arXiv:2401.05566,
2024."
REFERENCES,0.38369304556354916,"[52] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have a semantic
latent space. In The Eleventh International Conference on Learning Representations, 2023.
URL https://openreview.net/forum?id=pd1P2eUBVfq."
REFERENCES,0.38609112709832133,"[53] Sébastien Bubeck. Sparks of artificial general intelligence: Early experiments with GPT-4
(Talk), 2023. URL https://www.youtube.com/watch?v=qbIk7-JPB2c."
REFERENCES,0.38848920863309355,"[54] Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In International Conference on
Machine Learning, pages 2649–2658. PMLR, 2018."
REFERENCES,0.3908872901678657,"[55] Sjoerd Van Steenkiste, Francesco Locatello, Jürgen Schmidhuber, and Olivier Bachem. Are
disentangled representations helpful for abstract visual reasoning? Adv. in Neural Information
Processing Systems (NeurIPS), 2019."
REFERENCES,0.3932853717026379,"[56] Aapo Hyvarinen and Hiroshi Morioka. Nonlinear ICA of temporally dependent stationary
sources. In Proc. Int. Conf. on Artificial Intelligence and Statistics (AISTATS), 2017."
REFERENCES,0.39568345323741005,"[57] Aapo Hyvarinen and Hiroshi Morioka. Unsupervised feature extraction by time-contrastive
learning and nonlinear ica. Adv. in Neural Information Processing Systems (NeurIPS), 2016."
REFERENCES,0.3980815347721823,"[58] Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ICA using auxiliary variables
and generalized contrastive learning. In The 22nd Int. Conf. on Artificial Intelligence and
Statistics (AISTATS), 2019."
REFERENCES,0.40047961630695444,"[59] Julius Von Kügelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Schölkopf,
Michel Besserve, and Francesco Locatello. Self-supervised learning with data augmentations
provably isolates content from style. Adv. in Neural Information Processing Systems (NeurIPS),
2021."
REFERENCES,0.4028776978417266,"[60] Luigi Gresele, Julius Von Kügelgen, Vincent Stimper, Bernhard Schölkopf, and Michel
Besserve. Independent mechanism analysis, a new concept? Adv. in Neural Information
Processing Systems (NeurIPS), 2021."
REFERENCES,0.4052757793764988,"[61] Yi Ren, Samuel Lavoie, Mikhail Galkin, Danica J. Sutherland, and Aaron Courville. Improving
compositional generalization using iterated learning and simplicial embeddings, 2023. URL
https://arxiv.org/abs/2310.18777."
REFERENCES,0.407673860911271,"[62] Susan Carey. The origin of concepts. Journal of Cognition and Development, 1(1):37–41,
2000."
REFERENCES,0.41007194244604317,"[63] Susan Carey. Knowledge acquisition: Enrichment or conceptual change. The epigenesis of
mind: Essays on biology and cognition, pages 257–291, 1991."
REFERENCES,0.41247002398081534,"[64] Susan Carey. The origin and evolution of everyday concepts. Cognitive models of science, 15:
89–128, 1992."
REFERENCES,0.4148681055155875,"[65] Susan Carey and Elizabeth Spelke. Domain-specific knowledge and conceptual change.
Mapping the mind: Domain specificity in cognition and culture, 169:200, 1994."
REFERENCES,0.4172661870503597,"[66] Susan Carey. Bootstrapping & the origin of concepts. Daedalus, 133(1):59–68, 2004."
REFERENCES,0.4196642685851319,"[67] Susan Carey. Where our number concepts come from. The Journal of philosophy, 106(4):220,
2009."
REFERENCES,0.42206235011990406,"[68] Samuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in large
language model representations of true/false datasets. arXiv preprint arXiv:2310.06824, 2023."
REFERENCES,0.4244604316546763,"[69] Samuel Marks, Can Rager, Eric J Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller.
Sparse feature circuits: Discovering and editing interpretable causal graphs in language models.
arXiv preprint arXiv:2403.19647, 2024."
REFERENCES,0.42685851318944845,"[70] Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris
Bertsimas. Finding neurons in a haystack: Case studies with sparse probing. arXiv preprint
arXiv:2305.01610, 2023."
REFERENCES,0.4292565947242206,"[71] Wes Gurnee and Max Tegmark. Language models represent space and time. arXiv preprint
arXiv:2310.02207, 2023."
REFERENCES,0.4316546762589928,"[72] Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Rimsky, Wes Gurnee, and
Neel Nanda. Refusal in language models is mediated by a single direction. arXiv preprint
arXiv:2406.11717, 2024."
REFERENCES,0.434052757793765,"[73] Danny Halawi, Jean-Stanislas Denain, and Jacob Steinhardt. Overthinking the truth: Under-
standing how language models process false demonstrations. arXiv preprint arXiv:2307.09476,
2023."
REFERENCES,0.4364508393285372,"[74] Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the
geometry of large language models, 2024. URL https://arxiv.org/abs/2311.03658."
REFERENCES,0.43884892086330934,"[75] René Haas, Inbar Huberman-Spiegelglas, Rotem Mulayoff, and Tomer Michaeli. Discovering
interpretable directions in the semantic latent space of diffusion models. arXiv preprint
arXiv:2303.11073, 2023."
REFERENCES,0.4412470023980815,"[76] Yong-Hyun Park, Mingi Kwon, Junghyo Jo, and Youngjung Uh. Unsupervised discovery of
semantic latent directions in diffusion models. arXiv preprint arXiv:2302.12469, 2023."
REFERENCES,0.44364508393285373,"[77] Samyadeep Basu, Keivan Rezaei, Ryan Rossi, Cherry Zhao, Vlad Morariu, Varun Manjunatha,
and Soheil Feizi. On mechanistic knowledge localization in text-to-image generative models.
arXiv preprint arXiv:2405.01008, 2024."
REFERENCES,0.4460431654676259,"[78] Yida Chen, Fernanda Viégas, and Martin Wattenberg. Beyond surface statistics: Scene
representations in a latent diffusion model. arXiv preprint arXiv:2306.05720, 2023."
REFERENCES,0.44844124700239807,"[79] Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael
Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. Probing the
3d awareness of visual foundation models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 21795–21806, 2024."
REFERENCES,0.45083932853717024,"[80] Noam Chomsky. Aspects of the Theory of Syntax. Number 11. MIT press, 2014."
REFERENCES,0.45323741007194246,"[81] John Collins. Linguistic competence without knowledge of language. Philosophy Compass, 2
(6):880–895, 2007."
REFERENCES,0.4556354916067146,"[82] Jill G De Villiers and Peter A De Villiers. Competence and performance in child language:
Are children really competent to judge? Journal of Child Language, 1(1):11–22, 1974."
REFERENCES,0.4580335731414868,"[83] Gillian Brown, Kirsten Malmkjær, and John Williams. Performance and competence in second
language acquisition. Cambridge university press, 1996."
REFERENCES,0.460431654676259,"[84] METR.
Guidelines for Capabilities Elicitation, 2024.
https://metr.github.io/
autonomy-evals-guide/elicitation-protocol/."
REFERENCES,0.4628297362110312,"[85] Ryan Greenblatt, Fabien Roger, Dmitrii Krasheninnikov, and David Krueger. Stress-testing
capability elicitation with password-locked models. arXiv preprint arXiv:2405.19550, 2024."
REFERENCES,0.46522781774580335,"[86] Tom Davidson, Jean-Stanislas Denain, Pablo Villalobos, and Guillem Bas. Ai capabilities
can be significantly improved without expensive retraining. arXiv preprint arXiv:2312.07413,
2023."
REFERENCES,0.4676258992805755,"[87] Stephen Casper, Carson Ezell, Charlotte Siegmann, Noam Kolt, Taylor Lynn Curtis, Benjamin
Bucknall, Andreas Haupt, Kevin Wei, Jérémy Scheurer, Marius Hobbhahn, et al. Black-
box access is insufficient for rigorous ai audits. In The 2024 ACM Conference on Fairness,
Accountability, and Transparency, pages 2254–2272, 2024."
REFERENCES,0.47002398081534774,"[88] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,
Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al.
Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
arXiv preprint arXiv:2206.04615, 2022."
REFERENCES,0.4724220623501199,"[89] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won
Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-
bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261,
2022."
REFERENCES,0.4748201438848921,"[90] Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models,
2023."
REFERENCES,0.47721822541966424,"[91] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In Proc. Int. Conf. on Learning Representations (ICLR),
2017."
REFERENCES,0.47961630695443647,"[92] Cian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of
disentangled representations. In International Conference on Learning Representations, 2018."
REFERENCES,0.48201438848920863,"[93] Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources
of disentanglement in variational autoencoders. Advances in neural information processing
systems, 31, 2018."
REFERENCES,0.4844124700239808,"[94] Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of
disentangled latent concepts from unlabeled observations. arXiv preprint arXiv:1711.00848,
2017."
REFERENCES,0.486810551558753,"[95] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for
biomedical image segmentation, 2015."
REFERENCES,0.4892086330935252,"[96] Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, and Sergey Levine. Unfamiliar fine-
tuning examples control how language models hallucinate. arXiv preprint arXiv:2403.05612,
2024."
REFERENCES,0.49160671462829736,"[97] Jonathan Ho and Tim Salimans.
Classifier-free diffusion guidance.
arXiv preprint
arXiv:2207.12598, 2022."
REFERENCES,0.4940047961630695,"[98] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the
wild. In Proceedings of International Conference on Computer Vision (ICCV), December
2015."
REFERENCES,0.49640287769784175,"[99] Ben Hutchinson, Jason Baldridge, and Vinodkumar Prabhakaran. Underspecification in scene
description-to-depiction tasks, 2022."
REFERENCES,0.4988009592326139,"[100] Alexander D’Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex
Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D Hoffman, et al. Un-
derspecification presents challenges for credibility in modern machine learning. Journal of
Machine Learning Research, 23(226):1–61, 2022."
REFERENCES,0.5011990407673861,"[101] Sandro Pezzelle. Dealing with semantic underspecification in multimodal nlp. In Proceedings
of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers)."
REFERENCES,0.5035971223021583,"[102] Yingtian Tang, Yutaro Yamada, Yoyo Zhang, and Ilker Yildirim. When are lemons purple? the
concept association bias of vision-language models. In Proceedings of the 2023 Conference
on Empirical Methods in Natural Language Processing, 2023."
REFERENCES,0.5059952038369304,"[103] Royi Rassin, Eran Hirsch, Daniel Glickman, Shauli Ravfogel, Yoav Goldberg, and Gal Chechik.
Linguistic binding in diffusion models: Enhancing attribute correspondence through attention
map alignment. In Thirty-seventh Conference on Neural Information Processing Systems,
2023. URL https://openreview.net/forum?id=AOKU4nRw1W."
REFERENCES,0.5083932853717026,"[104] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang,
James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion
transformer for photorealistic text-to-image synthesis, 2023."
REFERENCES,0.5107913669064749,"[105] Sharon Zhou, Mitchell L. Gordon, Ranjay Krishna, Austin Narcomey, Li Fei-Fei, and
Michael S. Bernstein. Hype: A benchmark for human eye perceptual evaluation of gen-
erative models, 2019."
REFERENCES,0.513189448441247,"[106] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed
Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes,
Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-
image diffusion models with deep language understanding, 2022."
REFERENCES,0.5155875299760192,"[107] Yi Ren, Shangmin Guo, and Danica J. Sutherland. Better supervisory signals by observing
learning paths, 2022. URL https://arxiv.org/abs/2203.02485."
REFERENCES,0.5179856115107914,"[108] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint
arXiv:2201.02177, 2022."
REFERENCES,0.5203836930455635,"[109] Ziming Liu, Ouail Kitouni, Niklas S Nolte, Eric Michaud, Max Tegmark, and Mike Williams.
Towards understanding grokking: An effective theory of representation learning. Advances in
Neural Information Processing Systems, 35:34651–34663, 2022."
REFERENCES,0.5227817745803357,"[110] Ziming Liu, Eric J Michaud, and Max Tegmark. Omnigrok: Grokking beyond algorithmic
data. In The Eleventh International Conference on Learning Representations, 2022."
REFERENCES,0.5251798561151079,"[111] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress
measures for grokking via mechanistic interpretability, 2023."
REFERENCES,0.5275779376498801,"[112] Tanishq Kumar, Blake Bordelon, Samuel J Gershman, and Cengiz Pehlevan. Grokking as the
transition from lazy to rich training dynamics. arXiv preprint arXiv:2310.06110, 2023."
REFERENCES,0.5299760191846523,"[113] Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang.
Hidden progress in deep learning: Sgd learns parities near the computational limit. Advances
in Neural Information Processing Systems, 35:21750–21764, 2022."
REFERENCES,0.5323741007194245,"[114] Blaž Zupan, Marko Bohanec, Janez Demšar, and Ivan Bratko. Learning by discovering concept
hierarchies. Artificial Intelligence, 109(1-2):211–242, 1999."
REFERENCES,0.5347721822541966,"[115] Yangqing Jia, Joshua T Abbott, Joseph L Austerweil, Tom Griffiths, and Trevor Darrell.
Visual concept learning: Combining machine vision and bayesian generalization on concept
hierarchies. Advances in Neural Information Processing Systems, 26, 2013."
REFERENCES,0.5371702637889688,"[116] Martin Wattenberg and Fernanda B Viégas. Relational composition in neural networks: A
survey and call to action. arXiv preprint arXiv:2407.14662, 2024."
REFERENCES,0.539568345323741,"[117] Core Francisco Park, Victoria Ono, Nayantara Mudur, Yueying Ni, and Carolina Cuesta-Lazaro.
Probabilistic reconstruction of dark matter fields from biased tracers using diffusion models,
2023."
REFERENCES,0.5419664268585132,"[118] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition, 2015."
REFERENCES,0.5443645083932853,"[119] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016."
REFERENCES,0.5467625899280576,"[120] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2023."
REFERENCES,0.5491606714628298,"[121] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023."
REFERENCES,0.5515587529976019,"[122] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019."
REFERENCES,0.5539568345323741,"[123] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR,
2015."
REFERENCES,0.5563549160671463,"[124] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
high-performance deep learning library, 2019."
REFERENCES,0.5587529976019184,"[125] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning,
pages 8748–8763. PMLR, 2021."
REFERENCES,0.5611510791366906,"[126] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
High-resolution image synthesis with latent diffusion models, 2022."
REFERENCES,0.5635491606714629,"A
Schematic for Concept Space"
REFERENCES,0.565947242206235,Fig. 12 illustrates the concept space framework discussed in Sec. 3.
REFERENCES,0.5683453237410072,"Figure 12: The concept space framework. G is an invertible data-generating that maps process
maps sampled vectors z ∼P(Z) (where Z ⊂Rd) to an observation x ∈Rn (an image in this
work). The concept space, S := {z|z ∼P(Z)}, is defined as a space of all possible concept vectors
generated from a compositional prior P(Z). The mixing function M masks some concept variables,
the masked concept variables are thus underspecified."
REFERENCES,0.5707434052757794,"B
Experimental Details"
REFERENCES,0.5731414868105515,"B.1
Synthetic data"
REFERENCES,0.5755395683453237,"We design a data generation process (DGP) compatible with the concept space framework intro-
duced in Sec. 3. Our full DGP has six concepts: shape={circle, triangle}, x coordinate∈R, y
coordinate∈R, color={red, blue}, size={big, small}, and background color={bright, dark}.
We generally explore only a subset of these at a given time in our experiments."
REFERENCES,0.5779376498800959,"In Sec. 4, we fix shape=circle and x coordinate, y coordinate, background color
are masked out.
Thus the conditioning vector h only specifies color={red, blue} and
size={big, small}. We sample both of these concept variables from a mixture of two uniform distri-
butions, one component for each class (big, small) and (red, blue). Each value in each dimension
is sampled from U(mi −s, mi +s), where mi is the class dependent mean. For example, color=red
could be sampled from U(0.7, 0.9) × U(0.1, 0.3) × U(0.1, 0.3), where m0 = (0.8, 0.2, 0.2) For
brevity, we name the four resulting classes “00”, “01”, “10”, and “11”, where the class “11” is kept
as the unseen target to evaluate out-of-distribution (OOD) generalization. For each training run, the
DGP was initialized with a set random seed and 2048 images were generated in each class."
REFERENCES,0.580335731414868,"In Sec. 5, we used two concept variables, shape and size. The training dataset included the classes
“00”, “01”, and “10”. For evaluation, we used the class “11”. For each class, we generated a total of
1,000 images, each featuring objects of varying positions and sizes to ensure variability in our dataset."
REFERENCES,0.5827338129496403,"In App. D.5, we add the concept variable background color∈R3 to have a three dimensional
concept space of (color, size, background color). In this case our training data is composed of
the classes (“000”, “001”, “010”, “100”) and the test classes are (“011”, “101”, “110”, “111”)"
REFERENCES,0.5851318944844125,"Our DGP is resolution agnostic, but we work with square images of 32x32 for fast experiments."
REFERENCES,0.5875299760191847,"Adjusting concept signal To adjust concept signals, we vary the class dependent mean of the two
components of the mixture distributions. For example a strong color concept signal is achieved by
drawing color=red from the mean mred = (0.9, 0.1, 0.1) and color=blue from the mean mblue =
(0.1, 0.1, 0.9) whereas a weak color concept signal can be drawn using mred = (0.6, 0.4, 0.4) and
mblue = (0.4, 0.4, 0.6). We scale the standard deviation of each component by the same ratio the"
REFERENCES,0.5899280575539568,"0 0
1 0 1 1 Color Size 0 1 Color Size"
REFERENCES,0.592326139088729,"1 1
0 1"
REFERENCES,0.5947242206235012,"0 0
1 0
Figure 13: Different distributions in concept values result in different concept signal. (Left) The
color separation between the classes is stronger than the size separations resulting in a stronger
concept signal in the color dimension. (right) The size separation between the classes is stronger,
thus resulting in a stronger concept signal in size"
REFERENCES,0.5971223021582733,"difference between the class means has been scaled. This latter is important to avoid the model’s
natural interpolation or extrapolation abilities from confounding with the generalization dynamics."
REFERENCES,0.5995203836930456,"Fig. 13 illustrates two datasets with varying concept signals. Fig. 13 (left) illustrates a case where the
color concept signal is stronger than the size concept signal, while Fig. 13 (right) illustrates the
inverse case where the size concept signal is stronger."
REFERENCES,0.6019184652278178,"B.2
Model Details"
REFERENCES,0.60431654676259,"Model Architecture We used a network architecture similar to the U-Net [95] used in Variational
Diffusion Models [90]. We used the implementation publicly available in [117]. The architecture
has 2 ResNet [118] blocks before each downsampling layer and a has self-attention layer in the
bottleneck layer of the network. The U-Net has 64, 128, 256 channels at each resolution levels, an has
a LayerNorm [119] and a GELU [120] activation. We used a sinusoidal timestep embedding [121]
of 64 dimensions followed by a 2-layer MLP with hidden dimension 256 and a conditioning vector
embedding using a 2-layer MLP with hidden dimensions 256 independent in every residual block."
REFERENCES,0.6067146282973621,"Optimizer We use the AdamW [122] optimizer with learning rate 0.001 and weight decay 0.01 to
optimize the parameters of our network. We train our networks for 20K gradient steps. We use the
default values for the decay rates: β1 = 0.9, β2 = 0.999."
REFERENCES,0.6091127098321343,"Classifier Free Guidance At training time we drop the conditioning to a −1.0 filled null vector ϕ
with probability 0.2. This unconventional choice was made instead of the dropping it to the null
vector since the null vector was near the interpolation limit of our model in the color subspace of
the conditioning. The wcfg parameter is used to estimate the noise in the diffusion process by:
ˆϵt−1 = fθ(xt|ϕ) + wcfg ∗(fθ(xt|h) −fθ(xt|ϕ)), where θ denotes parameters of the network f."
REFERENCES,0.6115107913669064,"Diffusion Process Hyperparameters We used the continuous time variational diffusion model [90]
in order to (i) keep the sampling step parameter T an inference time hyperparameter and (ii) to allow
the model to the adjust its optimal noise schedule for these images, especially since our synthetic
images are expected to be different in SNR from natural images. In particular, we initialize our
network with γi = −5.0 and γf = 10.0 (see [90] for the definition of γ) and use a learned linear
schedule of γ(t). We use a reconstruction loss corresponding to a negative log likelihood from a
standard normal with σ = 10−3 centered at the data for the first step of the diffusion process."
REFERENCES,0.6139088729016786,"Evaluation probe Details. We used a U-Net [95] backbone with 64 output channels followed by a
max pooling layer and n× 1-layer MLP classifier for each of the n classes to estimate each concept of
an image independently. We sample from the same data distribution but with maximal data diversity,
i.e., with si values in App. B.1 maximized within the range allowing perfect classification (no overlap
of z between classes). We sample 4096 images per class from the DGP and train the classifier for 10K
gradient steps with AdamW [122] and achieve a 100% accuracy on the held out test set. At evaluation
phase, we average the classifier softmax output over 5 data generation / model initialization seeds
and 32 inference samples to construct the concept space representation of the generations."
REFERENCES,0.6163069544364509,"Hyperparameter Search We conducted a hyperparameter search, testing batch sizes from 32 to 256,
number of channels per layer from 64 to 512, learning rates between 10−4 and 10−3, the number
of steps in the diffusion process from 100 to 400, weight decay between 3 × 10−3 and 5 × 10−2,"
REFERENCES,0.6187050359712231,"and model weight initialization scale between N(0, 0.003) and N(0, 1). We also tried the Adam
optimizer [123] with β1 = 0.9, β2 = 0.99, and weight decay of 10−5. No qualitative change were
found in our results."
REFERENCES,0.6211031175059952,"Computational Details. We implement our models in PyTorch [124]. The diffusion model was
trained on four Nvidia A100 GPUs and 64 CPUs for the data generating process. A standard model
run (e.g., in Sec. 4.2) took ∼20 minutes on a single NVIDIA A100 40GB GPU. The CelebA runs
took ∼24 hours on the same GPU. The full research required roughly 500 GPU hours, while the
experiments in the paper would require roughly 100 GPU hours."
REFERENCES,0.6235011990407674,"B.3
Code Availability"
REFERENCES,0.6258992805755396,Our code is available at https://github.com/cfpark00/concept-learning.
REFERENCES,0.6282973621103117,"C
Details on Alternative Protocols for Eliciting Model Capabilities"
REFERENCES,0.6306954436450839,"Input Space: Overprompting.
Our model is trained on a distribution of conditioning vectors
centered around a class dependent mean: pi(zj) = U(mi
j −sj, mi
j + sj) for each class i. We
prompt the model with conditioning vectors extrapolated in the direction ⃗
m1 −⃗
m0. For instance,
assuming the red conditioning was (0.6, 0.4, 0.4) and the blue conditioning was (0.4, 0.4, 0.6), we
“prompt” the model with (0.2, 0.2, 0.8) for blue. In practice, we use 5 conditionings (0.4, 0.4, 0.6),
(0.35, 0.35, 0.65), (0.25, 0.25, 0.75), (0.15, 0.15, 0.85), (0.05, 0.05, 0.95), and report the maximum
joint accuracy."
REFERENCES,0.6330935251798561,"Activation Space: Linear Latent Intervention.
We demonstrate the ability to compose capabilities
by manipulating the conditional vectors ⃗h. Namely, we create a condition vector ⃗hi for a specific
concept i by specifying a concept of interest (e.g., ⃗hblue = M(zblue). During the forward pass, given
⃗h, we can compute the component of each concept in⃗h by projecting onto a specific concept-condition
vector (⃗hblue). We can then enhance or reduce the component of each concept by scaling each of these
projected components. In practice, we perform the following operation: ⃗h′ = ⃗h + α⃗hblue −β⃗hlarge,
where α, β are hyperparameters. We sweep over [0.1, 1, 2, 4] for α and [0.1, 0.25, 0.5, 1] for β."
REFERENCES,0.6354916067146283,"⃗hblue is constructed by first deriving a blue direction in condition embedding space (⃗h5,5,95) by
embedding a concept vector (⃗z5,5,95) where its RGB components are set to (0.05, 0.05, 0.95):
⃗h5,5,95 = M(⃗z5,5,95). We then project ⃗h onto this direction to derive ⃗hblue. ⃗hlarge is generated
similarly using ⃗zsize=0.7. The model then generates an image conditioned on ⃗h′."
REFERENCES,0.6378896882494005,"D
Additional Results"
REFERENCES,0.6402877697841727,"D.1
Loss and Accuracy versus Concept Space (Fig. 14)"
REFERENCES,0.6426858513189448,"In Fig. 14, we plot loss, accuracy, and OOD generalization trajectory in concept space of the models
for a training run in Fig. 6. The time at which the model acquires a capability to manipulate the
concept color is not evident from the loss or accuracy curves; however, in concept space, one can
identify when the trajectory deviates from concept memorization and thus when the compositional
ability has emerged. Benchmarking generative models is a challenging task, still often involving
humans in the loop [105, 106]. Our concept space framework suggests that benchmarking out-of-
distribution (OOD) generalization can potentially be reduced to monitoring the learning trajectory in
concept space."
REFERENCES,0.645083932853717,"D.2
Additional trajectories of learning dynamics (Fig. 15)"
REFERENCES,0.6474820143884892,"Here we show all concept space trajectories for the experiments mentioned in Fig. 4 (a,b), for all
classes and color concept signal levels. We find asymmetric behavior for the 01 class and the 10
class when adjusting the color concept signal level. The dynamics of the generations in the training
set matches our intuitions of concept signal as discussed in the main text. At low color concept"
REFERENCES,0.6498800959232613,"Figure 14: Loss vs. accuracy vs. concept space. (left) The training and validation loss on ID classes
during training. (center) The accuracy on the training classes and the test (OOD generalization)
class. (right) Concept space trajectory of the generalization class. Loss and accuracy do not always
intuitively reflect what capabilities the model has acquired during training. However, as one can see
in the rightmost panel, the point of sudden turn in concept space corresponds to when the capability
has emerged, i.e., the moment when well defined prompting protocols can elicit the desired output
from the model, which we indicate using the pink star (50% capability)."
REFERENCES,0.6522781774580336,"signal, we observe that the dynamics fit size first for both 00 and 10, and afterwards find their
correct colors."
REFERENCES,0.6546762589928058,"0 0
1 0 1 1"
REFERENCES,0.657074340527578,OOD generalization Color Size 0 1 Color Size
REFERENCES,0.6594724220623501,"0 0
1 0"
REFERENCES,0.6618705035971223,"1 1
0 1 Color Size"
REFERENCES,0.6642685851318945,"0 0
1 0"
REFERENCES,0.6666666666666666,"1 1
0 1 Color Size"
REFERENCES,0.6690647482014388,"0 0
1 0"
REFERENCES,0.6714628297362111,"1 1
0 1"
REFERENCES,0.6738609112709832,Color concept signal
REFERENCES,0.6762589928057554,"ID generalization for class 00
ID generalization for class 01
ID generalization for class 10"
REFERENCES,0.6786570743405276,"Figure 15: Concept space dynamics for all classes (00, 01, 10, 11). The experiment is identical as
in Fig. 4. The [0,1) normalized color concept signal is color coded in every trajectory. Two training
data trajectories are shown in gray in the last panel to illustrate concept memorization."
REFERENCES,0.6810551558752997,"D.3
Error Quantification (Fig. 16, Fig. 17)"
REFERENCES,0.6834532374100719,"We show the standard error of the mean (s.e.m.) of concept space trajectories of Fig. 4 and Fig. 15
in Fig. 16 (a). Fig. 16 (b) illustrates the 50% capability point across 4 different model initialization
seeds. Fig. 16 (c) illustrates the 50% capability point across different color concept signal level.
Overall, our results are consistent over random seeds and concept signal levels."
REFERENCES,0.6858513189448441,"Figure 16: Error quantification on concept space trajectories. (a) Concept space trajectories
and their standard error of the mean (n=5) for different concept signal levels. (b) Concept space
trajectories for 4 different seeds. The pink stars denote the point of 50% capability. (c) Concept space
trajectories for different color concept signal level. The stars again denote the 50% capability point
and the 25% −75% capability regions are in solid lines."
REFERENCES,0.6882494004796164,"We show the standard deviation of compositional generalization accuracy for different prompting
methods in Fig. 17. We find that the standard deviation is large for the naive prompting method, while
linear latent interpolation and overprompting extract the hidden capability robustly across seeds,
resulting in a small standard deviation."
REFERENCES,0.6906474820143885,"Figure 17: Standard deviation of compositional generalization accuracy. We show the compo-
sitional generalization accuracy and shade the 1σ region for (a) Naive prompting (b) Linear latent
intervention (c) Overprompting"
REFERENCES,0.6930455635491607,"D.4
Additional Experiments with real data, CelebA (Fig. 18)"
REFERENCES,0.6954436450839329,"To assess our findings on a more realistic dataset, we ran experiments on the CelebA [98] dataset.
The experiments in Fig. 7 and Fig. 18, share the same experiment hyperparameters, described below."
REFERENCES,0.697841726618705,"We selected the Female and Smiling features as the two concept dimensions to explore. This
choice was motivated by the relatively balanced number of samples in all four classes (00=(Male,
Not Smiling), 01=(Male, Smiling), 10=(Female, Not Smiling), 11=(Female, Smiling)), from
which we randomly sampled 30K images in each class. To construct the concept space, we trained a
fully convolutional network with a average pooling layer followed by a classification MLP head to
classify the two attributes with independent cross entropy losses. (See B.2). We trained the classifier
with the AdamW [122] optimizer with learning rate 10−3 and weight decay 10−5 for 10K gradient
steps. The classifier achieved a final accuracy of respectively 95% and 97% on the held out validation
set, which was 10% of the entire dataset."
REFERENCES,0.7002398081534772,"We trained the same diffusion model (See App. B.2) from our synthetic experiments on 64x64 resized
images from the classes (00, 01, 10) and assessed the out-of-distribution (OOD) generalization to 11.
We used a color jitter of 0.1 for brightness, contrast and saturation and randomly flipped the images
horizontally. As the class attributes are categorical, they were one hot encoded and concatenated to
an input conditioning vector of 4 dimensions. The diffusion model was trained for 2 × 106 gradient
steps with a batch size of 64 with the same optimizer as in the main experiments."
REFERENCES,0.7026378896882494,"The concept space dynamics of generations from the in distribution conditioning and OOD con-
ditioning are shown in Fig. 18. We find similar observations from the concept space trajectories
as in our synthetic experiments. Initially, the images corresponding to the class 11 follows the
concept space trajectory of 10, optimizing Female, which we intuitively expect to have a stronger
concept signal, although it is intractable to compute since the DGP is unknown. Similar to our
synthetic experiments, we see a transition in the concept space trajectory of the compositional class
corresponding to the model disentangling the concepts. After this transition, concept learning begins
but we observe a substantial bias towards the training distribution. We see a trade-off of moving
in the right direction towards one concept degrades the other one similarly to the observations in
Fig. 4 (b) for high color concept signal levels. The visual generations in Fig. 18 confirm that our
findings are not merely a result of an ill-calibrated classifier model. However, in this case, we do not
observe full out-of-distribution (OOD) generalization at 2M gradient steps. We expect this task of
generating (Female, Smiling) is inherently harder than our synthetic setup. We note that the goal of
this experiment is not to show good compositional generalization but it is more on confirming that
our qualitative findings generalize to real data without touching the model or training method."
REFERENCES,0.7050359712230215,"Figure 18: Concept space dynamics with CelebA. We train on the classes 00=(Male, NOT Smiling),
01=(Male, Smiling),
10=(Female, NOT Smiling) (plotted in blue) and test on the class
11=(Female, Smiling) (plotted in pink). We find similar observations as in Fig. 4.2, concept memo-
rization and the resulting bias."
REFERENCES,0.7074340527577938,"D.5
Experiments with 3D concept space (Fig. 19, Fig. 20)"
REFERENCES,0.709832134292566,"To further verify our findings in Sec. 4, we explore a setup where the concept conditioning specifies
three concepts: color, size and background color. Fig. 19 illustrates two example scenarios
in which the concept signal in color and background color are varied, respectively resulting in
cases of success and failure of OOD generalization by naive prompting. The length of edges of the
cuboids represent their concept signal magnitude. In Fig. 19 (a), we see that the object color has a
strong concept signal and this is reflected in the concept space trajectories as this direction being
split first. In this case we see, similarly to the blue generalization curve in Fig. 4 (b) and Fig. 15
(rightmost panel), a slow generalization process for the compositional class 111. Again similarly to
our findings in Sec. 4.2, we observe that the 011 class initially undergoes concept memorization for
class 010, which shares the two stronger concept signals color and background color, and shows
a transition where it suddenly curves out from this trajectory. In Fig. 19 (b), we see a case where
out-of-distribution (OOD) generalization did not succeed within the given 80K gradient steps. In this
case, we see two classes, 011 and 111, which didn’t reach the right compositional class via naive
prompting. However, the concept space trajectories suggest that the capability to compositionally
generalize background color has already emerged as visible from the sharp turns."
REFERENCES,0.7122302158273381,"Fig. 20 quantifies the accuracy of each concept separately. Fig. 20 (a) shows that stronger concept
signal accelerates the corresponding concept learning. Fig. 20 (b) shows that other concepts’ learning
speed can vary, albeit smoothly, when changing a single concept signal while the corresponding
concept is the most affected."
REFERENCES,0.7146282973621103,"D.6
Experiments with classifier free guidance (Fig. 21)"
REFERENCES,0.7170263788968825,"An implication of our conclusions in Sec. 4.4 is that before the model has passed the transition
point where the model has the capability to compose concepts, the model should not be able to
generate small blue circles no matter how well “prompted”. Here, instead of prompting, we explore
Classifier Free Guidance (CFG) [97] to see if our findings apply to a conditional diffusion model
trained with CFG. In Fig. 21, we see that even the models with CFG show this transition from concept
memorization to out-of-distribution (OOD) generalization. In a scenario where there isn’t a sharp
acquisition of the capability to compositionally generalize, we would expect the sharp transition to
disappear with CFG scale. However, as we observe in Fig. 21, the sharp turn in concept space still
exists with CFG, perhaps even more pronounced than baseline experiments. These results suggest (a) 000 100 101"
REFERENCES,0.7194244604316546,"111
011 010 001 110 (b)"
REFERENCES,0.7218225419664268,"000
100"
REFERENCES,0.7242206235011991,"101
001"
REFERENCES,0.7266187050359713,"111
011 010 110"
REFERENCES,0.7290167865707434,"Figure 19: Generalization dynamics in a 3 concept setting. (a) Generalization dynamics when
color carries the strongest concept signal. (b) Generalization dynamics when size carries the
strongest concept signal. We observe sharp turns corresponding to learning background color,
however, naive prompting cannot fully elicit compositonal generalization."
REFERENCES,0.7314148681055156,"(b)
(a)"
REFERENCES,0.7338129496402878,"Figure 20: Accuracy on individual concepts in a 3 concept setting. a) We show the dynamics
of color and background color accuracy during model training, depending on different concept
signal levels b) We show the average accuracy until the end of training as a metric of learning speed
for different concepts, as we change a single concept signal level."
REFERENCES,0.7362110311750599,"that the acquisition of the compositional generalization ability is required for CFG to enhance OOD
generalization."
REFERENCES,0.7386091127098321,"0 0
1 0 1 1 Color Size 0 1 Color Size"
REFERENCES,0.7410071942446043,"0 0
1 0"
REFERENCES,0.7434052757793765,"1 1
0 1 Color Size"
REFERENCES,0.7458033573141487,"0 0
1 0"
REFERENCES,0.7482014388489209,"1 1
0 1 Color Size"
REFERENCES,0.750599520383693,"0 0
1 0"
REFERENCES,0.7529976019184652,"1 1
0 1 wcfg"
REFERENCES,0.7553956834532374,"OOD generalization
ID generalization for class 00
ID generalization for class 01
ID generalization for class 10"
REFERENCES,0.7577937649880095,"Figure 21: Learning dynamics in Concept space for classifier-free Guidance We show the learning
dynamics in the concept space for all the classes (00, 01, 10, 11). The generalization task (rightmost)
shows a sharp transition from concept memorization to OOD generalization independently of the
classifier-free guidance scale."
REFERENCES,0.7601918465227818,"D.7
Patching the embedding module (Fig. 22)"
REFERENCES,0.762589928057554,"We test an additional elicitation method where we swap the conditioning vector (h)’s embedding
module with that of the last checkpoint. An interpretation of this method is that the embedding module
disentangles the concepts, i.e., generates a representation for each concept, while the U-Net [95] then"
REFERENCES,0.7649880095923262,"utilizes such representations. This would imply that the U-Net [95] learns how to utilize concept
representations early during training, while many more gradient steps are needed for the concepts to
be disentangled when naively prompted."
REFERENCES,0.7673860911270983,"We test our approach on 5 random initialization seeds. Results are shown in Fig. 22. We find that for
some seeds, we are able to elicit the target behavior at around the same time in which overprompting
and linear interventions also elicit the target behavior; for other seeds, this is not the case. These
results demonstrates that the role of the embedding module and the U-Net might be separated during
training for some runs without an explicit term enhancing this separation of roles."
REFERENCES,0.7697841726618705,"(a)
(b)"
REFERENCES,0.7721822541966427,"Figure 22: Embedding patching. We patch the embedding module (an MLP) used for transforming
the conditioning information into an embedding that the model processes from the last checkpoint
to intermediate checkpoints. Panel (a) shows the baseline accuracy for out-of-distribution (OOD)
generalization across five different seed runs, while panel (b) shows accuracy achieved when the
patched embedding module is used."
REFERENCES,0.7745803357314148,"D.8
Proof of Concept Experiments with Frontier Models (Fig. 23)"
REFERENCES,0.7769784172661871,"We show simple proof of concept experiments on frontier models in Fig. 23. In Fig. 23 (a), we show
that CLIP [125] already embeds text/images with compositional concepts in its vector space as a cube
respecting the Hamming graph of concepts. In Fig. 23 (b), we show that simple over-prompting can
enhance compositional generalization ability in Stable Diffusion v1.4 [126]. These experiments show
that the assumptions we made to construct the concept space framework in Sec. 3 and the elicitation
method we used work at least to some extent in frontier models."
REFERENCES,0.7793764988009593,PCA 1 (object)
REFERENCES,0.7817745803357314,PCA 2 (object color)
REFERENCES,0.7841726618705036,PCA 3 (background color )
REFERENCES,0.7865707434052758,"(a)
(b)"
REFERENCES,0.7889688249400479,"Naive promp<ng
Over-promp<ng"
REFERENCES,0.7913669064748201,"Case 1
Case 2"
REFERENCES,0.7937649880095923,"Prompt:  
""Triangular credit card"""
REFERENCES,0.7961630695443646,"red banana w/  
black background"
REFERENCES,0.7985611510791367,"red strawberry w/ 
white background"
REFERENCES,0.8009592326139089,"red banana w/  
white background"
REFERENCES,0.8033573141486811,"yellow banana w/ 
white background"
REFERENCES,0.8057553956834532,"yellow banana w/  
black background"
REFERENCES,0.8081534772182254,yellow strawberry w/
REFERENCES,0.8105515587529976,black background
REFERENCES,0.8129496402877698,red strawberry w/
REFERENCES,0.815347721822542,"black background
yellow strawberry  
w/ white background"
REFERENCES,0.8177458033573142,"Figure 23: Proof of concept experiments on frontier models (a) CLIP [125] embeds compositional
text/images as a concept cube in its vector space. (b) Simple over-prompting can enhance composi-
tional generalization in Stable Diffusion v1.4 [126]"
REFERENCES,0.8201438848920863,NeurIPS Paper Checklist
CLAIMS,0.8225419664268585,1. Claims
CLAIMS,0.8249400479616307,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.8273381294964028,Answer: [Yes]
CLAIMS,0.829736211031175,"Justification: Section 4.2, Section 4.4"
CLAIMS,0.8321342925659473,Guidelines:
CLAIMS,0.8345323741007195,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.8369304556354916,2. Limitations
LIMITATIONS,0.8393285371702638,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.841726618705036,Answer: [Yes]
LIMITATIONS,0.8441247002398081,Justification: Section 6
LIMITATIONS,0.8465227817745803,Guidelines:
LIMITATIONS,0.8489208633093526,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.8513189448441247,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.8537170263788969,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.8561151079136691,Answer: [NA]
THEORY ASSUMPTIONS AND PROOFS,0.8585131894484412,"Justification:
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.8609112709832134,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
THEORY ASSUMPTIONS AND PROOFS,0.8633093525179856,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Section B.1, Section B.2, Section C, Section B.3.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.8657074340527577,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
THEORY ASSUMPTIONS AND PROOFS,0.86810551558753,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
THEORY ASSUMPTIONS AND PROOFS,0.8705035971223022,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.8729016786570744,Justification: Section B.3.
THEORY ASSUMPTIONS AND PROOFS,0.8752997601918465,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.8776978417266187,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
THEORY ASSUMPTIONS AND PROOFS,0.8800959232613909,6. Experimental Setting/Details
THEORY ASSUMPTIONS AND PROOFS,0.882494004796163,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
THEORY ASSUMPTIONS AND PROOFS,0.8848920863309353,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.8872901678657075,"Justification: Section B.1, Section B.2"
THEORY ASSUMPTIONS AND PROOFS,0.8896882494004796,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.8920863309352518,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.894484412470024,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8968824940047961,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8992805755395683,Answer: [No]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9016786570743405,"Justification: While we report some error bars and show multiple seeds when applicable, we
did not test for statistical significance as most conclusions were qualitative."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9040767386091128,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9064748201438849,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors)."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9088729016786571,"• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9112709832134293,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9136690647482014,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9160671462829736,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9184652278177458,Justification: Section B.2
EXPERIMENTS COMPUTE RESOURCES,0.920863309352518,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9232613908872902,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.9256594724220624,9. Code Of Ethics
CODE OF ETHICS,0.9280575539568345,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9304556354916067,Answer: [Yes]
CODE OF ETHICS,0.9328537170263789,Justification:
CODE OF ETHICS,0.935251798561151,Guidelines:
CODE OF ETHICS,0.9376498800959233,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9400479616306955,10. Broader Impacts
BROADER IMPACTS,0.9424460431654677,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9448441247002398,Answer: [NA]
BROADER IMPACTS,0.947242206235012,Justification:
BROADER IMPACTS,0.9496402877697842,Guidelines:
BROADER IMPACTS,0.9520383693045563,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations."
BROADER IMPACTS,0.9544364508393285,"• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9568345323741008,11. Safeguards
SAFEGUARDS,0.9592326139088729,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9616306954436451,Answer: [NA]
SAFEGUARDS,0.9640287769784173,Justification:
SAFEGUARDS,0.9664268585131894,Guidelines:
SAFEGUARDS,0.9688249400479616,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9712230215827338,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.973621103117506,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9760191846522782,Answer: [NA]
LICENSES FOR EXISTING ASSETS,0.9784172661870504,Justification:
LICENSES FOR EXISTING ASSETS,0.9808153477218226,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9832134292565947,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided."
LICENSES FOR EXISTING ASSETS,0.9856115107913669,"• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
LICENSES FOR EXISTING ASSETS,0.988009592326139,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification:
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9904076738609112,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
LICENSES FOR EXISTING ASSETS,0.9928057553956835,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification:
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9952038369304557,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification:
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9976019184652278,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
