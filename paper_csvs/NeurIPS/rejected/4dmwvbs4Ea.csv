Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0006501950585175553,"We study offline Reinforcement Learning in large infinite-horizon discounted
1"
ABSTRACT,0.0013003901170351106,"Markov Decision Processes (MDPs) when the reward and transition models are
2"
ABSTRACT,0.0019505851755526658,"linearly realizable under a known feature map. Starting from the classic linear-
3"
ABSTRACT,0.002600780234070221,"program formulation of the optimal control problem in MDPs, we develop a new
4"
ABSTRACT,0.003250975292587776,"algorithm that performs a form of gradient ascent in the space of feature occupan-
5"
ABSTRACT,0.0039011703511053317,"cies, defined as the expected feature vectors that can potentially be generated by
6"
ABSTRACT,0.004551365409622887,"executing policies in the environment. We show that the resulting simple algorithm
7"
ABSTRACT,0.005201560468140442,"satisfies strong computational and sample complexity guarantees, achieved under
8"
ABSTRACT,0.005851755526657998,"the least restrictive data coverage assumptions known in the literature. In particu-
9"
ABSTRACT,0.006501950585175552,"lar, we show that the sample complexity of our method scales optimally with the
10"
ABSTRACT,0.007152145643693108,"desired accuracy level and depends on a weak notion of coverage that only requires
11"
ABSTRACT,0.007802340702210663,"the empirical feature covariance matrix to cover a single direction in the feature
12"
ABSTRACT,0.008452535760728219,"space (as opposed to covering a full subspace). Additionally, our method is easy
13"
ABSTRACT,0.009102730819245773,"to implement and requires no prior knowledge of the coverage ratio (or even an
14"
ABSTRACT,0.00975292587776333,"upper bound on it), which altogether make it the strongest known algorithm for
15"
ABSTRACT,0.010403120936280884,"this setting to date.
16"
INTRODUCTION,0.011053315994798439,"1
Introduction
17"
INTRODUCTION,0.011703511053315995,"We study Offline Reinforcement Learning (ORL) in sequential decision making problems whereby
18"
INTRODUCTION,0.01235370611183355,"a learner aims to find a near-optimal policy with sole access to a static dataset of interactions with
19"
INTRODUCTION,0.013003901170351105,"the underlying environment [Levine et al., 2020]. This line of work is naturally relevant to real-
20"
INTRODUCTION,0.013654096228868661,"world tasks for which learning an accurate simulator of the environment is potentially intractable
21"
INTRODUCTION,0.014304291287386216,"or impossible, trial-and-error learning could have grave consequences, yet logged interaction data
22"
INTRODUCTION,0.01495448634590377,"is readily available. For example, in a high-stake application such as autonomous driving, building
23"
INTRODUCTION,0.015604681404421327,"a sufficiently accurate simulator for the vehicle and its environment would require modelling very
24"
INTRODUCTION,0.016254876462938883,"complex systems, which can be intractable both statistically and computationally. At the same time,
25"
INTRODUCTION,0.016905071521456438,"running experiments in the real world could endanger the lives of other road users or result in damages
26"
INTRODUCTION,0.017555266579973992,"to the vehicle. Yet, with the advent of tools for efficient sensory-data collection and processing, large
27"
INTRODUCTION,0.018205461638491547,"volumes of logged data from human drivers are readily available.
28"
INTRODUCTION,0.0188556566970091,"An efficient ORL method is one which finds a near-optimal policy after a tractable number of
29"
INTRODUCTION,0.01950585175552666,"elementary computations and samples from the dataset. It is well-known in this setting that the quality
30"
INTRODUCTION,0.020156046814044214,"of the solution has to heavily depend on the quality of the data, and in particular one cannot hope
31"
INTRODUCTION,0.02080624187256177,"to find a near-optimal policy if the data covers the space of states and actions poorly. To formalize
32"
INTRODUCTION,0.021456436931079324,"this intuition, many notions of data coverage have been proposed in the offline RL literature, ranging
33"
INTRODUCTION,0.022106631989596878,"from a very restrictive uniform coverage assumption that requires the data-generating policy to cover
34"
INTRODUCTION,0.022756827048114433,"the entire state-action space [Munos and Szepesvári, 2008] to a variety of partial coverage conditions
35"
INTRODUCTION,0.02340702210663199,"whereby this exploratory condition is only required for state-action pairs that are of interest to the
36"
INTRODUCTION,0.024057217165149546,"optimal policy [Liu et al., 2020, Rashidinejad et al., 2021, Uehara and Sun, 2021, Zhan et al., 2022,
37"
INTRODUCTION,0.0247074122236671,"Rashidinejad et al., 2022, Li et al., 2024]. In the present work, we study the setting of linear Markov
38"
INTRODUCTION,0.025357607282184655,"Decision Processes (MDPs) [Jin et al., 2020, Yang and Wang, 2019] where the reward and transition
39"
INTRODUCTION,0.02600780234070221,"matrix admit a low rank structure in terms of a known feature map, and data-coverage assumptions
40"
INTRODUCTION,0.026657997399219768,"can be defined in the space of features. As shown by [Zanette et al., 2021], in this setting it is possible
41"
INTRODUCTION,0.027308192457737322,"to obtain strong guarantees if the offline data is well-aligned with the expectation of the feature vector
42"
INTRODUCTION,0.027958387516254877,"generated by the optimal policy (as opposed to requiring alignment with the entire distribution of
43"
INTRODUCTION,0.02860858257477243,"features as required by other common offline RL methods [Jin et al., 2021, Xie et al., 2021, Uehara
44"
INTRODUCTION,0.029258777633289986,"and Sun, 2021, Zhang et al., 2022]). In the present paper, we propose a simple and efficient algorithm
45"
INTRODUCTION,0.02990897269180754,"that yields the best known sample complexity guarantees for this problem setting, all while only
46"
INTRODUCTION,0.0305591677503251,"requiring the weakest known data-coverage assumptions of Zanette et al. [2021].
47"
INTRODUCTION,0.031209362808842653,"Our approach is based on the LP formulation of optimal control in infinite-horizon discounted MDPs
48"
INTRODUCTION,0.03185955786736021,"due to Manne [1960], and more specifically on its low-dimensional saddle-point reparametrization
49"
INTRODUCTION,0.032509752925877766,"for linear MDPs proposed by Gabbianelli et al. [2024] (which itself builds on earlier work by Neu
50"
INTRODUCTION,0.03315994798439532,"and Okolo, 2023 and Bas-Serrano et al., 2021). Primal variables of this saddle-point objective
51"
INTRODUCTION,0.033810143042912875,"correspond to expectations of feature vectors under the state-action distribution of each policy (called
52"
INTRODUCTION,0.03446033810143043,"feature occupancies), and dual variables correspond to parameters of linear approximations of action-
53"
INTRODUCTION,0.035110533159947985,"value functions. We design an algorithm based on the idea of optimizing the unconstrained primal
54"
INTRODUCTION,0.03576072821846554,"function that is derived from the saddle-point objective by eliminating the dual variables via a classic
55"
INTRODUCTION,0.036410923276983094,"dualization trick. More precisely, we design a sample-based estimator of the primal function and
56"
INTRODUCTION,0.03706111833550065,"optimize it via a variant of gradient ascent in the space of feature occupancies.
57"
INTRODUCTION,0.0377113133940182,"This approach is to be contrasted with the method of Gabbianelli et al. [2024], which instead optimized
58"
INTRODUCTION,0.03836150845253576,"the original saddle-point objective via stochastic primal-dual methods. Their algorithm interleaved a
59"
INTRODUCTION,0.03901170351105332,"sequence of “policy improvement” steps with an inner loop performing “policy evaluation”, which
60"
INTRODUCTION,0.03966189856957087,"resulted in a suboptimal use of sample transitions due to the costly inner loop. This issue was
61"
INTRODUCTION,0.04031209362808843,"addressed in the very recent work of Hong and Tewari [2024] who, instead of relying on stochastic
62"
INTRODUCTION,0.04096228868660598,"optimization, built an estimator of the saddle-point objective and optimized it via a deterministic
63"
INTRODUCTION,0.04161248374512354,"primal-dual method. Our approach is directly inspired by their idea of estimating the saddle-point
64"
INTRODUCTION,0.04226267880364109,"objective, but our algorithm design is significantly simpler: instead of directly optimizing the
65"
INTRODUCTION,0.04291287386215865,"primal function in terms of feature occupancies, Hong and Tewari [2024] relied on a sophisticated
66"
INTRODUCTION,0.043563068920676205,"reparametrization of the primal variables, and used a computationally involved procedure to update
67"
INTRODUCTION,0.044213263979193757,"the dual variables. Both of these steps required prior knowledge of a tight bound on the feature-
68"
INTRODUCTION,0.044863459037711315,"coverage ratio of the optimal policy, which is typically not available in problems of practical interest.
69"
INTRODUCTION,0.045513654096228866,"Such knowledge is not required by our algorithm, thanks to the incorporation of a recently proposed
70"
INTRODUCTION,0.046163849154746424,"stabilization trick that we make use of in our algorithm [Jacobsen and Cutkosky, 2023, Neu and
71"
INTRODUCTION,0.04681404421326398,"Okolo, 2024]. We provide a more detailed discussion of these closely related works in Section 5.
72"
INTRODUCTION,0.04746423927178153,"Notation.
We use boldface lowercase letters m to denote vectors and and bold uppercase M for
73"
INTRODUCTION,0.04811443433029909,"matrices. We define the Euclidean ball in Rd of radius D by Bd(D) = {x ∈Rd| ∥x∥2 ≤D} and
74"
INTRODUCTION,0.04876462938881664,"the A-simplex over a finite set A of cardinality A as ∆A = {p ∈RA
+| ∥p∥1 = 1}.
75"
PRELIMINARIES,0.0494148244473342,"2
Preliminaries
76"
PRELIMINARIES,0.05006501950585176,"We consider infinite-horizon Discounted Markov Decision Processes (DMDPs) [Puterman, 1994]
77"
PRELIMINARIES,0.05071521456436931,"of the form (X, A, r, P , γ) where X denotes a finite (yet large) set of X states and A is a finite
78"
PRELIMINARIES,0.05136540962288687,"action space of cardinality A = |A|. We refer to r ∈[0, 1]XA as the reward vector, P ∈RXA×X
+
79"
PRELIMINARIES,0.05201560468140442,"the transition matrix and γ ∈(0, 1) the discount factor. For a state-action pair (x, a) ∈X × A
80"
PRELIMINARIES,0.05266579973992198,"we also use the notation r (x, a) = r[(x, a)] to denote the reward of taking action a in state x and
81"
PRELIMINARIES,0.053315994798439535,"p (x′|x, a) = P [(x, a) , x′] as the probability of ending up in state x′ afterwards.
82"
PRELIMINARIES,0.053966189856957086,"The MDP models a sequential decision making process where an agent interacts with its environment
83"
PRELIMINARIES,0.054616384915474644,"as follows. For each step k = 0, 1, 2, · · · ,, the agent observes the current state Xk of the environment
84"
PRELIMINARIES,0.055266579973992196,"and then goes on to select its action Ak. Based on this action in the current state, it receives a reward
85"
PRELIMINARIES,0.055916775032509754,"r (Xk, Ak), transits to a new state Xk+1 ∼p (·|Xk, Ak) and the process continues. The objective of
86"
PRELIMINARIES,0.056566970091027305,"the agent is to find a decision-making rule that maximizes its total discounted reward when the initial
87"
PRELIMINARIES,0.05721716514954486,"state X0 is sampled according to a fixed initial-state distribution distribution ν0 ∈∆X . Without loss
88"
PRELIMINARIES,0.05786736020806242,"of generality, we assume that the initial state is fixed almost surely as X0 = x0, and use ν0 to refer to
89"
PRELIMINARIES,0.05851755526657997,"the corresponding delta distribution. It is known that this objective can be achieved by executing a
90"
PRELIMINARIES,0.05916775032509753,"stationary stochastic policy π : X →∆A, with π(a|x) denoting the probability of the agent selecting
91"
PRELIMINARIES,0.05981794538361508,"action Ak = a in state Xk = x for all k. We will use Π to denote the set of all such behavior rules
92"
PRELIMINARIES,0.06046814044213264,"and will often simply call them policies. We define the normalized discounted return of each policy π
93 as
94"
PRELIMINARIES,0.0611183355006502,"ρ (π) = (1 −γ) Eν0,π "" ∞
X"
PRELIMINARIES,0.06176853055916775,"k=0
γkr (xk, ak) # ,"
PRELIMINARIES,0.06241872561768531,"where the role of the discount factor γ ∈(0, 1) is to emphasize the importance of earlier rewards, and
95"
PRELIMINARIES,0.06306892067620286,"the notation Eν0,π [·] highlights that the initial state is sampled from ν0 and all actions are sampled
96"
PRELIMINARIES,0.06371911573472042,"according to the policy π. We will use π∗to denote any policy that maximizes the return.
97"
PRELIMINARIES,0.06436931079323797,"We will consider the offline RL setting where we are given access to a data set of n sample transitions
98"
PRELIMINARIES,0.06501950585175553,"Dn = {(Xi, Ai, Ri, X′
i)}n
i=1, where X′
i ∼p(·|Xi, Ai) is sampled independently for each i and Ri =
99"
PRELIMINARIES,0.06566970091027308,"r(Xi, Ai). Otherwise, no assumption is made about the state-action pairs (Xi, Ai), and in particular
100"
PRELIMINARIES,0.06631989596879063,"we do not require these to be generated by a fixed behavior policy or to be independent of each other.
101"
PRELIMINARIES,0.06697009102730819,"For describing the approach we take towards solving this problem, we need to introduce some
102"
PRELIMINARIES,0.06762028608582575,"further standard notations. The value function and action-value function associated with policy π are
103"
PRELIMINARIES,0.06827048114434331,"respectively defined as
104"
PRELIMINARIES,0.06892067620286085,"vπ (x) = Ea∼π(·|x) [qπ (x, a)] ,
qπ (x, a) = Eπ "" ∞
X"
PRELIMINARIES,0.06957087126137841,"k=0
γkr (xk, ak)"
PRELIMINARIES,0.07022106631989597,"x0 = x, a0 = a # ,"
PRELIMINARIES,0.07087126137841353,"and the state-occupancy and state-action-occupancy measures under π as
105"
PRELIMINARIES,0.07152145643693109,"νπ (x) =
X"
PRELIMINARIES,0.07217165149544863,"a
µπ (x, a) ,
µπ (x, a) = (1 −γ) Eν0,π "" ∞
X"
PRELIMINARIES,0.07282184655396619,"k=0
γkI{xk,ak} # ."
PRELIMINARIES,0.07347204161248375,"The value functions and occupancy measures adhere to the following recursive equations, respectively
106"
PRELIMINARIES,0.0741222366710013,"termed the Bellman equation and Bellman flow condition [Bellman, 1966]:
107"
PRELIMINARIES,0.07477243172951886,"qπ = r + γP vπ,
µπ = π ◦[(1 −γ) ν0 + γP
Tµπ]."
PRELIMINARIES,0.0754226267880364,"Here, the composition operation ◦is defined so that for any policy π and state distribution ν ∈RX,
108"
PRELIMINARIES,0.07607282184655396,"we have (π ◦ν) (x, a) = π (a|x) ν (x). Notice that we can express the return of π in terms of value
109"
PRELIMINARIES,0.07672301690507152,"functions and occupancy measures as ρ (π) = (1 −γ) ⟨ν0, vπ⟩= ⟨µπ, r⟩. On this note, for a given
110"
PRELIMINARIES,0.07737321196358908,"target accuracy ε > 0, we say policy π is ε-optimal if it satisfies

µπ∗−µπ, r

≤ε.
111"
PRELIMINARIES,0.07802340702210664,"In the present work, we well make use of the linear MDP assumption due to Jin et al. [2020], Yang
112"
PRELIMINARIES,0.07867360208062418,"and Wang [2019], which is defined formally as follows:
113"
PRELIMINARIES,0.07932379713914174,"Definition 2.1 (Linear MDP). An MDP is called linear if both the transition and reward functions
114"
PRELIMINARIES,0.0799739921976593,"can be expressed as a linear function of a given feature map φ : X × A →Rd. That is, there exist
115"
PRELIMINARIES,0.08062418725617686,"ψ : X →Rd and ω ∈Rd such that, for every x, x′ ∈X and a ∈A:
116"
PRELIMINARIES,0.0812743823146944,"r(x, a) = ⟨φ(x, a), ω⟩,
p (x′|x, a) = ⟨φ(x, a), ψ(x′)⟩."
PRELIMINARIES,0.08192457737321196,"We denote by Φ ∈R|X×A|×d the feature matrix with rows given by φ(x, a)T and Ψ ∈Rd×|X| as the
117"
PRELIMINARIES,0.08257477243172952,"weight matrix with columns ψ(x). Further, we will assume that ∥ω∥2 ≤
√"
PRELIMINARIES,0.08322496749024708,"d, that ∥Ψv∥2 ≤B
√ d
118"
PRELIMINARIES,0.08387516254876463,"holds for all v ∈[−B, B], and that all feature vectors satisfy ∥φ(x, a)∥2 ≤R for some R ≥1.
119"
PRELIMINARIES,0.08452535760728218,"An immediate consequence of this assumption is that the action-value function of any policy π can
120"
PRELIMINARIES,0.08517555266579974,"be written as a linear function of the features as qπ = Φθπ, with θπ = ω + γΨvπ ∈Rd. For the
121"
PRELIMINARIES,0.0858257477243173,"rest of the paper we explicitly assume that the feature matrix Φ is full rank – which is enough to
122"
PRELIMINARIES,0.08647594278283485,"ensure uniqueness of θπ. It is common to assume that the feature dimension d ≪X such that the
123"
PRELIMINARIES,0.08712613784135241,"transition operator is low-rank. As common in this setting, we will suppose throughout the paper that
124"
PRELIMINARIES,0.08777633289986995,"the feature map Φ is known.
125"
PRELIMINARIES,0.08842652795838751,"Our algorithm design will be based on the linear programming formulation of MDPs, first proposed
126"
PRELIMINARIES,0.08907672301690507,"in a number of papers in the 1960’s [Manne, 1960, de Ghellinck, 1960, d’Epenoux, 1963, Denardo,
127"
PRELIMINARIES,0.08972691807542263,"1970]. This formulation frames the problem of finding an optimal control policy as the following pair
128"
PRELIMINARIES,0.09037711313394019,"of primal and dual linear programs:
129"
PRELIMINARIES,0.09102730819245773,"maximize
⟨µ, r⟩
subject to
E
Tµ = (1 −γ)ν0 + γP
Tµ
µ ≥0,
(1)"
PRELIMINARIES,0.09167750325097529,"minimize
(1 −γ)⟨ν0, v⟩
subject to
Ev ≥r + γP v.
(2)
130"
PRELIMINARIES,0.09232769830949285,"Here, the operator E ∈RXA×X is defined such that for each x, a and vectors µ ∈RXA, v ∈RX,
131"
PRELIMINARIES,0.0929778933680104,"(E
Tµ) (x) =
X"
PRELIMINARIES,0.09362808842652796,"a∈A
µ (x, a) ,
(Ev) (x, a) = v (x) ."
PRELIMINARIES,0.09427828348504551,"It is known that the occupancy measure of an optimal policy µπ∗is an optimal solution of the primal
132"
PRELIMINARIES,0.09492847854356307,"LP (1). In fact, the feasible set of the primal is precisely the space of valid state-action occupancy
133"
PRELIMINARIES,0.09557867360208062,"measures that can be induced by stationary policies. Therefore, given any feasible solution µ, we can
134"
PRELIMINARIES,0.09622886866059818,"extract the inducing policy as πµ (a|x) = µ (x, a) / P"
PRELIMINARIES,0.09687906371911574,"a′ µ (x, a′) when P"
PRELIMINARIES,0.09752925877763328,"a µ (x, a) ̸= 0. Likewise,
135"
PRELIMINARIES,0.09817945383615084,"the state value function of the optimal policy π∗is an optimal solution to the dual LP. That said, since
136"
PRELIMINARIES,0.0988296488946684,"the LP features XA variables and constraints, it cannot be solved directly in large MDPs.
137"
PRELIMINARIES,0.09947984395318596,"In view of the above limitations, we consider the following reduced version of the above intractable
138"
PRELIMINARIES,0.10013003901170352,"LPs due to Gabbianelli et al. [2024] (see also Neu and Okolo, 2023, Bas-Serrano et al., 2021):
139"
PRELIMINARIES,0.10078023407022106,"maximize
⟨λ, ω⟩
subject to
E
Tµ = (1 −γ)ν0 + γΨ
Tλ
λ = Φ
Tµ
µ ≥0, (3)"
PRELIMINARIES,0.10143042912873862,"minimize
(1 −γ)⟨ν0, v⟩
subject to
Ev ≥Φθ
θ = ω + γΨv.
(4)
140"
PRELIMINARIES,0.10208062418725618,"In view of the second constraint of the primal LP (3), λ should be thought of as expectations of feature
141"
PRELIMINARIES,0.10273081924577374,"vectors under occupancy measures, and we thus refer to them as feature occupancy vectors. Similarly,
142"
PRELIMINARIES,0.1033810143042913,"the second constraint of the dual LP (4) suggests that θ should be thought of as parameters of the
143"
PRELIMINARIES,0.10403120936280884,"approximate action-value function qθ = Φθ = Φ (ω + γΨv) = r+γP v. We use λπ∗= ΦTµπ∗to
144"
PRELIMINARIES,0.1046814044213264,"denote the feature occupancy associated with the optimal policy π∗and θπ∗to denote the parameter-
145"
PRELIMINARIES,0.10533159947984395,"vector of the optimal action-value function qπ∗. The Lagrangian corresponding to the LPs is given as
146"
PRELIMINARIES,0.10598179453836151,"L(λ, µ; v, θ) = (1 −γ)⟨ν0, v⟩+ ⟨λ, ω + γΨv −θ⟩+ ⟨µ, Φθ −Ev⟩
= ⟨λ, ω⟩+ ⟨v, (1 −γ)ν0 + γΨ
Tλ −E
Tµ⟩+ ⟨θ, Φ
Tµ −λ⟩.
(5)"
PRELIMINARIES,0.10663198959687907,"It is easy to verify that by the linear MDP property, the feasible sets of the above LPs coincide
147"
PRELIMINARIES,0.10728218465539661,"with those of the original LPs in an appropriate sense, and their optimal solutions correspond to
148"
PRELIMINARIES,0.10793237971391417,"the optimal state-action occupancy measure and state-value function respectively (see Appendix A).
149"
PRELIMINARIES,0.10858257477243173,"In order to further reduce the complexity of the LPs above, we introduce a policy π and parametrize
150"
PRELIMINARIES,0.10923276983094929,"the remaining high-dimensional variables v and µ as
151"
PRELIMINARIES,0.10988296488946683,"vθ,π(s) =
X"
PRELIMINARIES,0.11053315994798439,"a
π(a|s) ⟨θ, φ(x, a)⟩,
µλ,π(x, a) = π(a|x)
h
(1 −γ)ν0(x) + γ⟨ψ(x), λ⟩
i
.
(6)"
PRELIMINARIES,0.11118335500650195,"Plugging this choice back into the Lagrangian, we obtain the objective
152"
PRELIMINARIES,0.11183355006501951,"f(λ, π; θ) = L(λ, µλ,π; vθ,π, θ)
= (1 −γ)⟨ν0, vθ,π⟩+ ⟨λ, ω + γΨvθ,π −θ⟩
(7)
= ⟨λ, ω⟩+ ⟨θ, Φ
Tµλ,π −λ⟩."
PRELIMINARIES,0.11248374512353707,"It is easy to see that for any π and λπ = ΦTµπ, we have f(λπ, π; θ) = ⟨µπ, r⟩for all θ ∈
153"
PRELIMINARIES,0.11313394018205461,"Rd. Furthermore, whenever λ ̸= λπ then the θ-player has a winning strategy that can force
154"
PRELIMINARIES,0.11378413524057217,"minθ f(λ, π; θ) = −∞. This (informally) suggests that an optimal policy can be found by solving the
155"
PRELIMINARIES,0.11443433029908973,"unconstrained saddle-point optimization problem maxλ∈Rd,π∈Π minθ∈Rd f(λ, π; θ). Furthermore,
156"
PRELIMINARIES,0.11508452535760728,"since the optimal policy can be written as π∗(a|x) = I{a=argmaxb⟨θπ∗,φ(x,b)⟩}, it is sufficient to
157"
PRELIMINARIES,0.11573472041612484,"consider softmax policies of the form
158"
PRELIMINARIES,0.11638491547464239,Π (Dπ) = (
PRELIMINARIES,0.11703511053315994,"πθ (a|x) =
e⟨φ(x,a),θ⟩
P
a′ e⟨φ(x,a′),θ⟩"
PRELIMINARIES,0.1176853055916775,"θ ∈Bd(Dπ) ) ,"
PRELIMINARIES,0.11833550065019506,"which can approximate π∗to good precision when the diameter Dπ is set to be large enough. This
159"
PRELIMINARIES,0.11898569570871262,"parametrization effectively reduces the high-dimensional LP into a low-dimensional saddle-point
160"
PRELIMINARIES,0.11963589076723016,"optimization problem.
161"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.12028608582574772,"3
Feature-occupancy gradient ascent for offline RL in linear MDPs
162"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.12093628088426528,"A natural idea for developing RL methods is to build an empirical approximation of the function f
163"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.12158647594278284,"defined in the previous section, and use primal-dual methods to find a saddle-point of the resulting
164"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1222366710013004,"approximation. For offline RL, this approach has been explored by Gabbianelli et al. [2024] and
165"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.12288686605981794,"Hong and Tewari [2024]. In this work, we develop an alternative approach that seeks to directly
166"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1235370611183355,"optimize the return by approximately maximizing the unconstrained primal function f ∗: Rd × Π,
167"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.12418725617685306,"defined for each feature-occupancy vector λ and policy π as
168"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.12483745123537061,"f ∗(λ, π) =
min
θ∈Bd(Dθ) f(λ, π; θ),"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.12548764629388817,"for an appropriately chosen feasible set Bd(Dθ). Given the discussion in the previous section, maxi-
169"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.12613784135240572,"mizing this function with respect to λ and π is rightly expected to result in an optimal policy (which
170"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1267880364109233,"intuition will be made formal in our analysis). Notably, the so-called objective f in Equation (7)
171"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.12743823146944083,"depends on the transition weight matrix Ψ which is unknown in general. As we soon show, this
172"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.12808842652795838,"matrix dominates the loss of the θ-player and λ-player. Based on these observations, our approach
173"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.12873862158647595,"consists of building a well-chosen estimator bf of f, and then maximizing the associated primal
174"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1293888166449935,"function bf ∗defined as
175"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.13003901170351106,"bf ∗(λ, π) =
min
θ∈Bd(Dθ)
bf(λ; θ, π)."
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1306892067620286,"The objective bf is built via a least-squares estimator inspired by the classic LSTD model estimate
176"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.13133940182054615,"of Bradtke and Barto [1996], Parr et al. [2008], which has been successfully used for analyzing
177"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.13198959687906373,"finite-horizon linear MDPs in a variety of recent works (e.g., Jin et al., 2020, Neu and Pike-Burke,
178"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.13263979193758127,"2020). In particular, we fit an estimator bΨ of the true matrix Ψ using samples from the dataset
179"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.13328998699609884,"Dn = {(Xi, Ai, Ri, X′
i)}n
i=1 as follows. Let φi = φ(Xi, Ai) denote the feature vector of (Xi, Ai)
180"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.13394018205461639,and Λn = βIn + 1
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.13459037711313393,"n
Pn
i=1 φiφT
i the empirical feature covariance matrix. We define the regularized
181"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1352405721716515,"least squares estimate of Ψ at x ∈X as
182"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.13589076723016905,"b
ψ (x) = arg min
ψ(x)∈Rd
1
n n
X i=1"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.13654096228868662,"
⟨φi, ψ (x)⟩−I{x=X′
i}"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.13719115734720416,"2
+ β ∥ψ (x)∥2
2 ,"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1378413524057217,"so that the estimate can be written as
183"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.13849154746423928,"bΨ =
X"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.13914174252275682,"x∈X
b
ψ(x)e
T
x = 1"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1397919375812744,"nΛ−1
n n
X"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.14044213263979194,"i=1
φie
T
X′
i.
(8)"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.14109232769830948,"With this matrix at hand, we define bf as
184"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.14174252275682706,"bf(λ, π; θ) = (1 −γ)⟨ν0, vθ,π⟩+ ⟨λ, ω + γ bΨvθ,π −θ⟩= ⟨λ, ω⟩+ ⟨θ, Φ
T bµλ,π −λ⟩,"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1423927178153446,"where bµλ,π(x, a) = π(a|x)
h
(1 −γ)ν0(x) + γ⟨b
ψ(x), λ⟩
i
is a sample-based approximation of µλ,π.
185"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.14304291287386217,"For the purpose of optimization, we will employ appropriately chosen versions of mirror as-
186"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.14369310793237972,"cent [Nemirovski and Yudin, 1983, Beck and Teboulle, 2003] to iteratively optimize the pri-
187"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.14434330299089726,"mal variables. Denoting the iterates for each t = 1, 2, . . . , T by λt and πt, and defining θt =
188"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.14499349804941483,"arg minθ∈Bd(Dθ) bf(λt, πt; θ), the updates are defined as follows. Using gλ(t) = ∇λt bf ∗(λt, πt) to
189"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.14564369310793238,"denote the gradient of bf ∗with respect to the feature occupancies, the first set of variables is updated as
190"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.14629388816644995,"λt+1 = arg max
λ∈Rd"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1469440832249675,"n
⟨λ, gλ(t)⟩−1"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.14759427828348504,"2η ∥λ −λt∥2
Λ−1
n −ϱ"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1482444733420026,"2 ∥λ∥2
Λ−1
n"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.14889466840052015,"o
,
(9)"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.14954486345903772,"where the first regularization term acts as proximal regularization (necessary for mirror-ascent-style
191"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.15019505851755527,"methods), and the second one has a stabilization effect whose role will be made clear later in the
192"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1508452535760728,"analysis. The resulting update can be written in closed form, and is equivalent to a preconditioned
193"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.15149544863459039,"gradient-ascent step on bf ∗. The policies are updated in each state-action pair x, a as
194"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.15214564369310793,"πt+1(a|x) =
πt(a|x)eα⟨φ(x,a),θt⟩
P"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1527958387516255,"a′ πt(a′|x)eα⟨φ(x,a′),θt⟩=
π1(a|x)eα⟨φ(x,a),Pt
k=1 θk⟩
P"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.15344603381014305,"a′ π1(a′|x)eα⟨φ(x,a′),Pt
k=1 θk⟩,"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1540962288686606,Algorithm 1 Feature-Occupancy Gradient Ascent (FOGAS)
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.15474642392717816,"Input: Learning rates α, ϱ, η, initial points λ1 ∈Rd, π1 ∈Π (Dπ) , ¯θ0 = 0, and dataset Dn.
for t = 1 to T do"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1553966189856957,"// Value-parameter update
Compute
ΦT bµλt,πt = (1 −γ) P"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.15604681404421328,"a πt(a|x0)φ(x0, a) + γ 1"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.15669700910273082,"n
Pn
i=1
P"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.15734720416124837,"a πt(a|X′
i)φ(X′
i, a)

φi, Λ−1
n λt"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.15799739921976594,"θt = arg minθ∈Bd(Dθ)⟨θ, ΦT bµλt,πt −λt⟩"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.15864759427828348,"// Policy update
Update ¯θt = ¯θt−1 + θt
πt+1 = σ
 
αΦ¯θt
"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.15929778933680103,"// Feature-occupancy update
Compute bΨvθt,πt = 1"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1599479843953186,"nΛ−1
n
Pn
i=1 φivθt,πt(X′
i)
Compute gλ(t) = ω + γ bΨvθt,πt −θt
λt+1 =
1
1+ϱη (λt + ηΛngλ(t))"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.16059817945383614,"end for
return πJ with J ∼U(1, · · · , T)."
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.16124837451235371,"corresponding to performing an entropy-regularized mirror ascent step in each state x (cf. Neu et al.,
195"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.16189856957087126,"2017). We use the shorthand notation πt+1 = σ
 
αΦ Pt
k=1 θk

to denote the resulting softmax
196"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1625487646293888,"policy, and note that it is fully specified by a d-dimensional vector that can be stored compactly.
197"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.16319895968790638,"After the final iterate is computed, the algorithm picks the index J uniformly at random and outputs
198"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.16384915474642392,"the policy πJ. We refer to the resulting algorithm as Feature-Occupancy Gradient AScent (FOGAS),
199"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1644993498049415,"and present its detailed pseudocode featuring the explicit expressions of λt and θt as Algorithm 1.
200"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.16514954486345904,"The following theorem states our main result regarding the performance of FOGAS.
201"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.16579973992197658,"Theorem 3.1. Let π1 be the uniform policy and λ1 = 0. Also set Dθ =
√"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.16644993498049415,"d/ (1 −γ), Dπ = αTDθ
202"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1671001300390117,and δ > 0. Suppose that we run FOGAS for T ≥2R2n log A
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.16775032509752927,"log(1/δ)
rounds with parameters β = R2/dT as
203"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1684005201560468,"well as
204 α = s"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.16905071521456436,2 (1 −γ)2 log A
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.16970091027308193,"R2dT
,
ϱ = γ s"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.17035110533159947,320d2 log (2T/δ)
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.17100130039011704,"(1 −γ)2 n
,
η = s"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1716514954486346,(1 −γ)2
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.17230169050715213,27R2d2T .
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1729518855656697,"Then, with probability at least 1 −δ, the following bound is satisfied for any comparator policy π∗
205"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.17360208062418725,"and the associated feature-occupancy vector λπ∗= ΦTµπ∗:
206"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.17425227568270482,"EJ
hD
µπ∗−µπJ, r
Ei
= O  "
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.17490247074122237,"λπ∗2"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1755526657997399,"Λ−1
n + 1"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.17620286085825748,"1 −γ
· r"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.17685305591677503,"d2 log (2T/δ) n  ,"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1775032509752926,"with the expectation taken with respect to the random index J.
207"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.17815344603381014,"The most important factor in the bound of Theorem 3.1 is ∥λ∗∥2
Λ−1
n , which measures the extent
208"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1788036410923277,"to which the data Dn covers the comparator policy π∗in feature space. We accordingly refer to
209"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.17945383615084526,"this quantity as the feature coverage ratio between the policy π∗and the data set Dn, and we
210"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1801040312093628,"discuss its relationship with other notions of data coverage in Section 5. Notably, the bound holds
211"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.18075422626788037,"simultaneously for all comparator policies π∗, and thus it can be restated in an oracle-inequality form.
212"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.18140442132639792,"On the same note, FOGAS does not need any prior upper bounds on the comparator norm ∥λ∗∥2
Λ−1
n ,
213"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.18205461638491546,"and in particular it does not project the iterates λt to a bounded set. These nontrivial properties are
214"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.18270481144343304,"enabled by a recently proposed stabilization trick due to Jacobsen and Cutkosky [2023] and Neu and
215"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.18335500650195058,"Okolo [2024], which amounts to augmenting the standard mirror-ascent update of Equation (9) with
216"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.18400520156046815,the regularization term ϱ
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1846553966189857,"2 ∥λ∥2
Λ−1
n . Without this additional regularization, the bounds would feature
217"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.18530559167750324,an additional factor of the order 1
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.1859557867360208,"T
PT
t=1 ∥λt∥2
Λ−1
n , which cannot be controlled without projecting the
218"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.18660598179453836,"iterates and in any case make it impossible to prove a comparator-adaptive bound. We defer further
219"
FEATURE-OCCUPANCY GRADIENT ASCENT FOR OFFLINE RL IN LINEAR MDPS,0.18725617685305593,"discussion of the result to Section 5.
220"
ANALYSIS,0.18790637191157347,"4
Analysis
221"
ANALYSIS,0.18855656697009102,"This section is dedicated to proving our main result, Theorem 3.1. While we have defined FOGAS as a
222"
ANALYSIS,0.1892067620286086,"“primal-only” algorithm above, its analysis will be most convenient if we regard it as a primal-dual
223"
ANALYSIS,0.18985695708712613,"algorithm with implicitly defined dual updates. In particular, we will view the updates of FOGAS
224"
ANALYSIS,0.1905071521456437,"as a sequence of steps in a zero sum game between two teams of players: the max players that
225"
ANALYSIS,0.19115734720416125,"control λt and πt, and the min player that picks θt. The min player uses the simple best-response
226"
ANALYSIS,0.1918075422626788,"strategy of picking θt = arg minθ∈Bd(Dθ) bf(λ, πt), and the other two players perform their updates
227"
ANALYSIS,0.19245773732119636,"via appropriate versions mirror ascent on their respective objectives. Importantly, the updates of the
228"
ANALYSIS,0.1931079323797139,"λ-player are based on the gradients of bf ∗, which satisfy
229"
ANALYSIS,0.19375812743823148,"gλ(t) = ∇λt bf ∗(λt, πt) = ∇λt"
ANALYSIS,0.19440832249674903,"
min
θ∈Bd(Dθ)
bf(λt, πt; θ)

= ∇λt bf(λt, πt; θt),"
ANALYSIS,0.19505851755526657,"where the last equality follows from an application of Danskin’s theorem. This property enables
230"
ANALYSIS,0.19570871261378414,"a major conceptual simplification that allows the interpretation of the updates as optimizing the
231"
ANALYSIS,0.19635890767230169,"unconstrained primal bf ∗directly. We refer the interested reader to Chapter 6 of Bertsekas [1997] for
232"
ANALYSIS,0.19700910273081926,"more context on such use of primal-dual analysis.
233"
ANALYSIS,0.1976592977893368,"More concretely, we make use of an analysis technique first developed by Neu and Okolo [2023],
234"
ANALYSIS,0.19830949284785435,"and further refined by Gabbianelli et al. [2024] and Hong and Tewari [2024]. The core idea is to
235"
ANALYSIS,0.19895968790637192,"introduce the dynamic duality gap defined on a sequence of iterates {(λt, πt, θt)}T
t=1 produced by
236"
ANALYSIS,0.19960988296488946,"some iterative method, and a set of well-chosen comparators
 
λ∗, π∗; {θ∗
t }T
t=1

as
237"
ANALYSIS,0.20026007802340703,"GT
 
λ∗, π∗; {θ∗
t }T
t=1

= 1 T T
X"
ANALYSIS,0.20091027308192458,"t=1
(f(λ∗, π∗; θt) −f(λt, πt; θ∗
t )) ."
ANALYSIS,0.20156046814044212,"Similar to Lemma 4.1 of Gabbianelli et al. [2024], we show in Lemma 4.1 below that with an
238"
ANALYSIS,0.2022106631989597,"appropriate choice of the comparator points, we can relate the gap to the expected suboptimality of
239"
ANALYSIS,0.20286085825747724,"policy πJ where J ∼U(1, · · · , T). We leave the proof in Appendix B.1.1.
240"
ANALYSIS,0.2035110533159948,"Lemma 4.1. Suppose that Dθ =
√"
ANALYSIS,0.20416124837451236,"d/(1 −γ). Choose (λ∗, π∗, θ∗
t ) =
 
ΦTµπ∗, π∗, θπt
∈
241"
ANALYSIS,0.2048114434330299,"Rd × Π (Dπ) × Bd(Dθ) for t = 1, · · · , T where µπ∗is a valid occupancy measure induced by π∗.
242"
ANALYSIS,0.20546163849154747,"Then,
243"
ANALYSIS,0.20611183355006502,"EJ
hD
µπ∗−µπJ, r
Ei
= GT

Φ
Tµπ∗, π∗, {θπt}T
t=1

."
ANALYSIS,0.2067620286085826,"We will show below that the dynamic duality gap can be written in terms of the regrets of each player
244"
ANALYSIS,0.20741222366710013,"and an additional term related to the estimation error of bf, and then proceed to provide bounds on all
245"
ANALYSIS,0.20806241872561768,"of these quantities. Specifically, the regrets of each player with respect to each of their respective
246"
ANALYSIS,0.20871261378413525,"comparators are defined as
247"
ANALYSIS,0.2093628088426528,"RT (π∗) = T
X t=1 X"
ANALYSIS,0.21001300390117036,"x
ν∗(x)
X"
ANALYSIS,0.2106631989596879,"a
(π∗(a|x) −πt(a|x)) qt(x, a),"
ANALYSIS,0.21131339401820545,"RT (λ∗) = T
X"
ANALYSIS,0.21196358907672302,"t=1
bf(λ∗, πt; θt) −bf(λt, πt; θt) = T
X"
ANALYSIS,0.21261378413524057,"t=1
⟨λ∗−λt, ω + γ bΨvθt,πt −θt⟩,"
ANALYSIS,0.21326397919375814,"RT (θ∗
1:T ) = T
X"
ANALYSIS,0.21391417425227569,"t=1
bf(λt, πt; θt) −bf(λt, πt; θ∗
t ) = T
X"
ANALYSIS,0.21456436931079323,"t=1
⟨θt −θ∗
t , Φ
T bµλt,πt −λt⟩."
ANALYSIS,0.2152145643693108,"where ν∗= (1 −γ)ν0(x) + γ⟨ψ(x), λ∗⟩. Furthermore, we define the gap-estimation error as
248"
ANALYSIS,0.21586475942782835,"err b
Ψ = T
X t=1"
ANALYSIS,0.21651495448634592,"λ∗,
 
Ψ −bΨ

vθt,πt

+ T
X t=1"
ANALYSIS,0.21716514954486346,"λt,
  bΨ −Ψ

vθ∗
t ,πt

.
(10)"
ANALYSIS,0.217815344603381,"The following lemma rewrites the duality gap using the above terms.
249"
ANALYSIS,0.21846553966189858,"Lemma 4.2. The dynamic duality gap satisfies
250"
ANALYSIS,0.21911573472041612,"GT (λ∗, π∗, θ∗
1:T ) = 1"
ANALYSIS,0.21976592977893367,T RT (π∗) + 1
ANALYSIS,0.22041612483745124,T RT (λ∗) + 1
ANALYSIS,0.22106631989596878,"T RT (θ∗
1:T ) + γ"
ANALYSIS,0.22171651495448635,"T err b
Ψ."
ANALYSIS,0.2223667100130039,"The proof directly follows from a straightforward calculation similar to the proof of Lemma 4.2
251"
ANALYSIS,0.22301690507152144,"of Gabbianelli et al. [2024] and Section E.1 of Hong and Tewari [2024] which is reproduced in
252"
ANALYSIS,0.22366710013003901,"Appendix B.1.2 for completeness. It remains to bound the regret of the players, as well as the
253"
ANALYSIS,0.22431729518855656,"gap-estimation error. An obstacle we need to face in the analysis is that our bound of the latter error
254"
ANALYSIS,0.22496749024707413,term scale with 1
ANALYSIS,0.22561768530559168,"T
PT
t=1 ∥λt∥2
Λ−1
n , which is undesirable given our aspiration to achieve bounds that
255"
ANALYSIS,0.22626788036410922,"scale only with the comparator norm ∥λ∗∥2
Λ−1
n without requiring prior upper bounds on this quantity
256"
ANALYSIS,0.2269180754226268,"(that would enable us to project the iterates to a bounded domain). This challenge is addressed by
257"
ANALYSIS,0.22756827048114434,"making use of the stabilization technique of Jacobsen and Cutkosky [2023] and Neu and Okolo
258"
ANALYSIS,0.2282184655396619,"[2024] in the updates for the λ-player, which effectively eliminates these problematic terms. We
259"
ANALYSIS,0.22886866059817945,"briefly outline the remaining parts of the analysis below.
260"
REGRET ANALYSIS,0.229518855656697,"4.1
Regret analysis
261"
REGRET ANALYSIS,0.23016905071521457,"The regrets of each player are respectively controlled by the following three lemmas.
262"
REGRET ANALYSIS,0.2308192457737321,"Lemma 4.3. Suppose that ν∗∈∆X . Let π1 be the uniform policy which selects all actions with
263"
REGRET ANALYSIS,0.23146944083224968,"equal probability in each state. Under the conditions on the feature map in Definition 2.1, the regret
264"
REGRET ANALYSIS,0.23211963589076723,of the π-player against π∗satisfies 1
REGRET ANALYSIS,0.23276983094928477,T RT (π∗) ≤log A
REGRET ANALYSIS,0.23342002600780234,"αT + αR2D2
θ
2
.
265"
REGRET ANALYSIS,0.2340702210663199,"The proof is a standard application of the analysis of exponential-weight updates, stated as Lemma E.1.
266"
REGRET ANALYSIS,0.23472041612483746,"Lemma 4.4. Let λ1 = 0 and C = 6β
 
d + D2
θ

+ 3d (1 + RDθ)2 + 3γ2dR2D2
θ. Then, the regret
267"
REGRET ANALYSIS,0.235370611183355,"of the λ-player against any comparator λ∗∈Rd satisfies
268"
REGRET ANALYSIS,0.23602080624187255,"1
T RT (λ∗) ≤
 1"
REGRET ANALYSIS,0.23667100130039012,2ηT + ϱ 2
REGRET ANALYSIS,0.23732119635890767,"
∥λ∗∥2
Λ−1
n + ηC 2 −ϱ"
T,0.23797139141742524,"2T T
X"
T,0.23862158647594278,"t=1
∥λt∥2
Λ−1
n ."
T,0.23927178153446033,"The proof (provided in Appendix B.2.2) follows from applying the standard analysis of composite-
269"
T,0.2399219765929779,"objective mirror descent due to Duchi et al. [2010] (stated as Lemma C.1 in the Appendix) and the
270"
T,0.24057217165149544,"bound ∥Λngλ(t)∥2
Λ−1
n
≤C on the weighted norm of the gradients for all t provided in Lemma C.2.
271"
T,0.24122236671001301,"Lemma 4.5. Let Dθ =
√"
T,0.24187256176853056,d/ (1 −γ). The regret of the θ-player satisfies 1
T,0.2425227568270481,"T RT (θ∗
1:T ) ≤0.
272"
T,0.24317295188556567,"As we show in Appendix B.2.3, the above statement holds trivially thanks to the “best-response”
273"
T,0.24382314694408322,"definition of θt. This concludes our regret analysis.
274"
BOUNDING THE GAP-ESTIMATION ERROR,0.2444733420026008,"4.2
Bounding the gap-estimation error
275"
BOUNDING THE GAP-ESTIMATION ERROR,0.24512353706111834,"The following statement (proved in Appendix B.3) provides a bound on err b
Ψ:
276"
BOUNDING THE GAP-ESTIMATION ERROR,0.24577373211963588,"Lemma 4.6. Suppose that ∥φ(x, a)∥2 ≤R for all (x, a) ∈X × A, Dθ =
√"
BOUNDING THE GAP-ESTIMATION ERROR,0.24642392717815345,"d/(1 −γ) and
277 α =
q"
BOUNDING THE GAP-ESTIMATION ERROR,0.247074122236671,"2 (1 −γ)2 log A/R2dT to optimize RT (π∗). Then, for any T ≥2R2n log A"
BOUNDING THE GAP-ESTIMATION ERROR,0.24772431729518857,"log(1/δ)
and and ξ ≥0,
278"
BOUNDING THE GAP-ESTIMATION ERROR,0.2483745123537061,"the following holds with probability at least 1 −δ:
279"
BOUNDING THE GAP-ESTIMATION ERROR,0.24902470741222366,"err b
Ψ ≤1 2ξ "
BOUNDING THE GAP-ESTIMATION ERROR,0.24967490247074123,"∥λ∗∥2
Λ−1
n + 1 T T
X"
BOUNDING THE GAP-ESTIMATION ERROR,0.2503250975292588,"t=1
∥λt∥2
Λ−1
n !"
BOUNDING THE GAP-ESTIMATION ERROR,0.25097529258777634,+ T 2ξ
BOUNDING THE GAP-ESTIMATION ERROR,0.2516254876462939,320d2 log (2T/δ)
BOUNDING THE GAP-ESTIMATION ERROR,0.25227568270481143,n (1 −γ)2 ! .
BOUNDING THE GAP-ESTIMATION ERROR,0.252925877763329,"4.3
The proof of Theorem 3.1
280"
BOUNDING THE GAP-ESTIMATION ERROR,0.2535760728218466,"The proof follows from applying Lemmas 4.1 and Lemma 4.2 when (λ∗, π∗, θ∗
t )
=
281
 
ΦTµπ∗, π∗, θπt
∈Rd × Π (Dπ) × Bd(Dθ) for t = 1, · · · , T. Then, adding up the bounds
282"
BOUNDING THE GAP-ESTIMATION ERROR,0.2542262678803641,"stated in Lemmas 4.3–4.6 under the respective conditions, yields
283"
BOUNDING THE GAP-ESTIMATION ERROR,0.25487646293888166,"EJ
hD
µπ∗−µπJ, r
Ei
≤ s"
BOUNDING THE GAP-ESTIMATION ERROR,0.2555266579973992,d log (1/δ)
BOUNDING THE GAP-ESTIMATION ERROR,0.25617685305591675,"n (1 −γ)2 +
 1"
BOUNDING THE GAP-ESTIMATION ERROR,0.25682704811443435,2ηT + ϱ
BOUNDING THE GAP-ESTIMATION ERROR,0.2574772431729519,"2 +
γ
2ξT"
BOUNDING THE GAP-ESTIMATION ERROR,0.25812743823146944," λπ∗
2"
BOUNDING THE GAP-ESTIMATION ERROR,0.258777633289987,"Λ−1
n
+ ηC 2 +
 γ"
BOUNDING THE GAP-ESTIMATION ERROR,0.25942782834850453,"ξT −ϱ
 1"
T,0.26007802340702213,"2T T
X"
T,0.2607282184655397,"t=1
∥λt∥2
Λ−1
n + γTξ"
T,0.2613784135240572,320d2 log (2T/δ)
T,0.26202860858257476,n (1 −γ)2 ! .
T,0.2626788036410923,"Then, setting ρ =
γ
ξT simplifies the second term and eliminates the third term. The claim then follows
284"
T,0.2633289986996099,"after optimizing the hyperparameters, with the full details provided in Appendix B.4.
285"
DISCUSSION,0.26397919375812745,"5
Discussion
286"
DISCUSSION,0.264629388816645,"We discuss various aspects of our results below.
287"
DISCUSSION,0.26527958387516254,"Relation with previous work.
As discussed in the introduction, our work draws heavily on previous
288"
DISCUSSION,0.2659297789336801,"contributions of Gabbianelli et al. [2024] and Hong and Tewari [2024]. In particular, our idea of
289"
DISCUSSION,0.2665799739921977,"building a least-squares estimator of the transition function is directly borrowed from the latter of
290"
DISCUSSION,0.2672301690507152,"these works, and our implicit update rule for θt is also inspired by their work to a good extent. Their
291"
DISCUSSION,0.26788036410923277,"approach, however, failed to reach the same degree of efficiency due to a number of suboptimal design
292"
DISCUSSION,0.2685305591677503,"choices. First, they used an alternative parametrization of the feature occupancies which only allowed
293"
DISCUSSION,0.26918075422626786,"them to work under a more restrictive coverage condition, so that their bounds depend on ∥λ∗∥Λ−2
n
294"
DISCUSSION,0.26983094928478546,"which can be much larger than the feature coverage ratio appearing in our bounds. Second, their
295"
DISCUSSION,0.270481144343303,"algorithm required a prior upper bound on this coverage parameter, with the guarantees scaling with
296"
DISCUSSION,0.27113133940182055,"the bound rather than the actual coverage. Such bounds are typically difficult to obtain in practice.
297"
DISCUSSION,0.2717815344603381,"Third, the implementation of their algorithm required intricate computational steps necessitated by
298"
DISCUSSION,0.27243172951885564,"their feature-occupancy parametrization. Our work has successfully removed these limitations and
299"
DISCUSSION,0.27308192457737324,"reduced the complexity of their method, thanks to a new primal-only analysis style that we hope will
300"
DISCUSSION,0.2737321196358908,"find further uses in reinforcement learning.
301"
DISCUSSION,0.2743823146944083,"Computational and statistical efficiency.
As can be inferred from our main result, the sample com-
302"
DISCUSSION,0.27503250975292587,"plexity of finding an ε-optimal policy using our algorithm is of the order d2 ∥λ∗∥2
Λ−1
n /ε2 (1 −γ)2,
303"
DISCUSSION,0.2756827048114434,"which is optimal in terms of scaling with ε. The rate can be improved to scale linearly with the feature
304"
DISCUSSION,0.276332899869961,"coverage ratio ∥λ∗∥Λ−1
n , if a tight upper bound is known on it which can be used for hyperparameter
305"
DISCUSSION,0.27698309492847856,"tuning. We find this scenario to be unlikely, and are curious to see if future work can attain this
306"
DISCUSSION,0.2776332899869961,"improved scaling without such prior knowledge. As for computational complexity, we point out
307"
DISCUSSION,0.27828348504551365,"that the cost of each iteration of our method scales linearly with the sample size n, due to having to
308"
DISCUSSION,0.2789336801040312,"compute the matrix-vector products bΨvθt,πt. Indeed, the matrix bΨ is sparse with n non-zero rows,
309"
DISCUSSION,0.2795838751625488,"and as such computing this product takes linear time in n. Since the iteration complexity of FOGAS
310"
DISCUSSION,0.28023407022106633,"scales linearly with the sample size n, this makes for an overall runtime complexity of order n2. This
311"
DISCUSSION,0.2808842652795839,"limitation is of course shared with all methods using the same least-squares transition estimator for
312"
DISCUSSION,0.2815344603381014,"the transition model, including all work that builds on Jin et al. [2020], but we nevertheless wonder if
313"
DISCUSSION,0.28218465539661897,"a substantial improvement is possible on this front.
314"
DISCUSSION,0.28283485045513657,"Data coverage assumptions.
The only works we are aware of that scale with the feature-coverage
315"
DISCUSSION,0.2834850455136541,"ratio ∥λ∗∥Λ−1
n
are due to Zanette et al. [2021] and Gabbianelli et al. [2024]. The latter work only
316"
DISCUSSION,0.28413524057217165,"achieves this bound under the assumption that the data is drawn i.i.d. from a fixed behavior policy
317"
DISCUSSION,0.2847854356306892,"with known feature covariance matrix, which is a much more restricted setting that we consider
318"
DISCUSSION,0.28543563068920674,"here. Such assumptions are not needed by Zanette et al. [2021], however their results are restricted
319"
DISCUSSION,0.28608582574772434,"to the simpler finite-horizon MDP setting, and their algorithm is arguably more complex than ours.
320"
DISCUSSION,0.2867360208062419,"Using our notation, their approach can be interpreted as solving a “pessimistic” version of the
321"
DISCUSSION,0.28738621586475943,"the relaxed dual LP (4) that features some additional quadratic constraints. This approach is not
322"
DISCUSSION,0.288036410923277,"computationally viable for the infinite-horizon discounted case we consider, as it requires solving a
323"
DISCUSSION,0.2886866059817945,"fixed-point equation with respect to the estimated transition operator (cf. Wei et al., 2021).
324"
DISCUSSION,0.2893368010403121,"Possible extensions.
Our approach can be extended and generalized in a variety of ways. First,
325"
DISCUSSION,0.28998699609882966,"following Gabbianelli et al. [2024], we believe that it is straightforward to extend our analysis to
326"
DISCUSSION,0.2906371911573472,"undiscounted infinite-horizon MDPs. Second, we similarly believe that an extension to constrained
327"
DISCUSSION,0.29128738621586475,"MDPs is possible without major challenges, following Hong and Tewari [2024]. We did not pursue
328"
DISCUSSION,0.2919375812743823,"these extensions because we believe that they add little additional insight. There are other potential
329"
DISCUSSION,0.2925877763328999,"directions that we did not explore because we found them to be too ambitious for the moment.
330"
DISCUSSION,0.29323797139141744,"These include extending our results beyond linear MDPs to other MDP models with linear function
331"
DISCUSSION,0.293888166449935,"approximation, including MDPs with low inherent Bellman rank (which may be within reach of
332"
DISCUSSION,0.29453836150845253,"the current theory, c.f. Zanette et al., 2020), linearly Qπ-realizable MDPs (which are known to be
333"
DISCUSSION,0.2951885565669701,"challenging, c.f. Weisz et al., 2022, 2024). Even more ambitiously, one can ask if it is possible to
334"
DISCUSSION,0.2958387516254877,"extend our methods to work under more general notions of function approximation. This looks very
335"
DISCUSSION,0.2964889466840052,"challenging given the central role of feature occupancies in our formalism, which are strictly tied to
336"
DISCUSSION,0.29713914174252276,"linear function approximation. We are nevertheless optimistic that the ideas presented in this work
337"
DISCUSSION,0.2977893368010403,"will find use in other contexts, possibly including nonlinear function approximation in the future.
338"
REFERENCES,0.29843953185955785,"References
339"
REFERENCES,0.29908972691807545,"Y. Abbasi-Yadkori, D. Pál, and C. Szepesvári. Improved algorithms for linear stochastic bandits.
340"
REFERENCES,0.299739921976593,"Advances in neural information processing systems, 24, 2011.
341"
REFERENCES,0.30039011703511054,"J. Bas-Serrano, S. Curi, A. Krause, and G. Neu. Logistic q-learning. In International Conference on
342"
REFERENCES,0.3010403120936281,"Artificial Intelligence and Statistics, pages 3610–3618. PMLR, 2021.
343"
REFERENCES,0.3016905071521456,"A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex
344"
REFERENCES,0.3023407022106632,"optimization. Operations Research Letters, 31(3):167–175, 2003.
345"
REFERENCES,0.30299089726918077,"R. Bellman. Dynamic programming. science, 153(3731):34–37, 1966.
346"
REFERENCES,0.3036410923276983,"D. P. Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48(3):
347"
REFERENCES,0.30429128738621586,"334–334, 1997.
348"
REFERENCES,0.3049414824447334,"S. J. Bradtke and A. G. Barto. Linear least-squares algorithms for temporal difference learning.
349"
REFERENCES,0.305591677503251,"Machine Learning, 22:33–57, 1996.
350"
REFERENCES,0.30624187256176855,"N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge university press, 2006.
351"
REFERENCES,0.3068920676202861,"G. de Ghellinck. Les problèmes de décisions séquentielles. Cahiers du Centre d’Études de Recherche
352"
REFERENCES,0.30754226267880364,"Opérationnelle, 2:161–179, 1960.
353"
REFERENCES,0.3081924577373212,"E. V. Denardo. On linear programming in a Markov decision problem. Management Science, 16(5):
354"
REFERENCES,0.3088426527958388,"281–288, 1970.
355"
REFERENCES,0.3094928478543563,"F. d’Epenoux. A probabilistic production and inventory problem. Management Science, 10(1):
356"
REFERENCES,0.31014304291287387,"98–108, 1963.
357"
REFERENCES,0.3107932379713914,"J. C. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite objective mirror descent. In
358"
REFERENCES,0.31144343302990896,"COLT, volume 10, pages 14–26. Citeseer, 2010.
359"
REFERENCES,0.31209362808842656,"Y. Freund and R. E. Schapire.
A decision-theoretic generalization of on-line learning and an
360"
REFERENCES,0.3127438231469441,"application to boosting. Journal of Computer and System Sciences, 55:119–139, 1997.
361"
REFERENCES,0.31339401820546164,"G. Gabbianelli, G. Neu, M. Papini, and N. M. Okolo. Offline primal-dual reinforcement learning for
362"
REFERENCES,0.3140442132639792,"linear mdps. In International Conference on Artificial Intelligence and Statistics, pages 3169–3177.
363"
REFERENCES,0.31469440832249673,"PMLR, 2024.
364"
REFERENCES,0.3153446033810143,"K. Hong and A. Tewari. A primal-dual algorithm for offline constrained reinforcement learning with
365"
REFERENCES,0.3159947984395319,"low-rank mdps. arXiv preprint arXiv:2402.04493, 2024.
366"
REFERENCES,0.3166449934980494,"A. Jacobsen and A. Cutkosky. Unconstrained online learning with unbounded losses. In International
367"
REFERENCES,0.31729518855656696,"Conference on Machine Learning, pages 14590–14630. PMLR, 2023.
368"
REFERENCES,0.3179453836150845,"C. Jin, Z. Yang, Z. Wang, and M. I. Jordan. Provably efficient reinforcement learning with linear
369"
REFERENCES,0.31859557867360205,"function approximation. In Conference on learning theory, pages 2137–2143. PMLR, 2020.
370"
REFERENCES,0.31924577373211965,"Y. Jin, Z. Yang, and Z. Wang. Is pessimism provably efficient for offline RL? In International
371"
REFERENCES,0.3198959687906372,"Conference on Machine Learning, pages 5084–5096. PMLR, 2021.
372"
REFERENCES,0.32054616384915474,"T. Lattimore and C. Szepesvári. Bandit algorithms. Cambridge University Press, 2020.
373"
REFERENCES,0.3211963589076723,"S. Levine, A. Kumar, G. Tucker, and J. Fu. Offline reinforcement learning: Tutorial, review, and
374"
REFERENCES,0.32184655396618983,"perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
375"
REFERENCES,0.32249674902470743,"G. Li, L. Shi, Y. Chen, Y. Chi, and Y. Wei. Settling the sample complexity of model-based offline
376"
REFERENCES,0.323146944083225,"reinforcement learning. The Annals of Statistics, 52(1):233–260, 2024.
377"
REFERENCES,0.3237971391417425,"N. Littlestone and M. Warmuth. The weighted majority algorithm. Information and Computation,
378"
REFERENCES,0.32444733420026006,"108:212–261, 1994.
379"
REFERENCES,0.3250975292587776,"Y. Liu, A. Swaminathan, A. Agarwal, and E. Brunskill. Provably good batch off-policy reinforcement
380"
REFERENCES,0.3257477243172952,"learning without great exploration. Advances in neural information processing systems, 33:
381"
REFERENCES,0.32639791937581275,"1264–1274, 2020.
382"
REFERENCES,0.3270481144343303,"A. S. Manne. Linear programming and sequential decisions. Management Science, 6(3):259–267,
383"
REFERENCES,0.32769830949284784,"1960.
384"
REFERENCES,0.3283485045513654,"R. Munos and C. Szepesvári. Finite-time bounds for fitted value iteration. Journal of Machine
385"
REFERENCES,0.328998699609883,"Learning Research, 9(5), 2008.
386"
REFERENCES,0.3296488946684005,"A. Nemirovski and D. Yudin. Problem Complexity and Method Efficiency in Optimization. Wiley
387"
REFERENCES,0.33029908972691807,"Interscience, 1983.
388"
REFERENCES,0.3309492847854356,"G. Neu and N. Okolo. Efficient global planning in large MDPs via stochastic primal-dual optimization.
389"
REFERENCES,0.33159947984395316,"In International Conference on Algorithmic Learning Theory, pages 1101–1123. PMLR, 2023.
390"
REFERENCES,0.33224967490247076,"G. Neu and N. Okolo. Dealing with unbounded gradients in stochastic saddle-point optimization.
391"
REFERENCES,0.3328998699609883,"arXiv preprint arXiv:2402.13903, 2024.
392"
REFERENCES,0.33355006501950585,"G. Neu and C. Pike-Burke. A unifying view of optimism in episodic reinforcement learning. In
393"
REFERENCES,0.3342002600780234,"Advances in Neural Information Processing Systems, pages 1392–1403, 2020.
394"
REFERENCES,0.33485045513654094,"G. Neu, A. Jonsson, and V. Gómez. A unified view of entropy-regularized Markov decision processes.
395"
REFERENCES,0.33550065019505854,"arXiv preprint arXiv:1705.07798, 2017.
396"
REFERENCES,0.3361508452535761,"R. Parr, L. Li, G. Taylor, C. Painter-Wakefield, and M. L. Littman. An analysis of linear models, linear
397"
REFERENCES,0.3368010403120936,"value-function approximation, and feature selection for reinforcement learning. In Proceedings of
398"
REFERENCES,0.33745123537061117,"the 25th international conference on Machine learning, pages 752–759, 2008.
399"
REFERENCES,0.3381014304291287,"M. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley
400"
REFERENCES,0.3387516254876463,"& Sons, 1994.
401"
REFERENCES,0.33940182054616386,"P. Rashidinejad, B. Zhu, C. Ma, J. Jiao, and S. Russell. Bridging offline reinforcement learning and
402"
REFERENCES,0.3400520156046814,"imitation learning: A tale of pessimism. Advances in Neural Information Processing Systems, 34:
403"
REFERENCES,0.34070221066319895,"11702–11716, 2021.
404"
REFERENCES,0.3413524057217165,"P. Rashidinejad, H. Zhu, K. Yang, S. Russell, and J. Jiao. Optimal conservative offline rl with general
405"
REFERENCES,0.3420026007802341,"function approximation via augmented lagrangian. arXiv preprint arXiv:2211.00716, 2022.
406"
REFERENCES,0.34265279583875163,"S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algorithms.
407"
REFERENCES,0.3433029908972692,"Cambridge university press, 2014.
408"
REFERENCES,0.3439531859557867,"M. Uehara and W. Sun. Pessimistic model-based offline reinforcement learning under partial coverage.
409"
REFERENCES,0.34460338101430427,"arXiv preprint arXiv:2107.06226, 2021.
410"
REFERENCES,0.34525357607282187,"V. Vovk. Aggregating strategies. In Proceedings of the third annual workshop on Computational
411"
REFERENCES,0.3459037711313394,"learning theory (COLT), pages 371–386, 1990.
412"
REFERENCES,0.34655396618985695,"C.-Y. Wei, M. J. Jahromi, H. Luo, and R. Jain. Learning infinite-horizon average-reward mdps with
413"
REFERENCES,0.3472041612483745,"linear function approximation. In International Conference on Artificial Intelligence and Statistics,
414"
REFERENCES,0.34785435630689204,"pages 3007–3015. PMLR, 2021.
415"
REFERENCES,0.34850455136540964,"G. Weisz, A. György, T. Kozuno, and C. Szepesvári. Confident approximate policy iteration for
416"
REFERENCES,0.3491547464239272,"efficient local planning in qπ-realizable mdps. Advances in Neural Information Processing Systems,
417"
REFERENCES,0.34980494148244473,"35:25547–25559, 2022.
418"
REFERENCES,0.3504551365409623,"G. Weisz, A. György, and C. Szepesvári. Online rl in linearly qπ-realizable mdps is as easy as in
419"
REFERENCES,0.3511053315994798,"linear mdps if you learn what to ignore. Advances in Neural Information Processing Systems, 36,
420"
REFERENCES,0.3517555266579974,"2024.
421"
REFERENCES,0.35240572171651496,"T. Xie, C.-A. Cheng, N. Jiang, P. Mineiro, and A. Agarwal. Bellman-consistent pessimism for offline
422"
REFERENCES,0.3530559167750325,"reinforcement learning. Advances in neural information processing systems, 34:6683–6694, 2021.
423"
REFERENCES,0.35370611183355005,"L. Yang and M. Wang. Sample-optimal parametric Q-learning using linearly additive features. In
424"
REFERENCES,0.3543563068920676,"International conference on machine learning, pages 6995–7004. PMLR, 2019.
425"
REFERENCES,0.3550065019505852,"A. Zanette, A. Lazaric, M. Kochenderfer, and E. Brunskill. Learning near optimal policies with low
426"
REFERENCES,0.35565669700910274,"inherent bellman error. In International Conference on Machine Learning, pages 10978–10989.
427"
REFERENCES,0.3563068920676203,"PMLR, 2020.
428"
REFERENCES,0.35695708712613783,"A. Zanette, M. J. Wainwright, and E. Brunskill. Provable benefits of actor-critic methods for offline
429"
REFERENCES,0.3576072821846554,"reinforcement learning. Advances in neural information processing systems, 34:13626–13640,
430"
REFERENCES,0.358257477243173,"2021.
431"
REFERENCES,0.3589076723016905,"W. Zhan, B. Huang, A. Huang, N. Jiang, and J. Lee. Offline reinforcement learning with realizability
432"
REFERENCES,0.35955786736020806,"and single-policy concentrability. In Conference on Learning Theory, pages 2730–2775. PMLR,
433"
REFERENCES,0.3602080624187256,"2022.
434"
REFERENCES,0.36085825747724315,"X. Zhang, Y. Chen, X. Zhu, and W. Sun. Corruption-robust offline reinforcement learning. In
435"
REFERENCES,0.36150845253576075,"International Conference on Artificial Intelligence and Statistics, pages 5757–5773. PMLR, 2022.
436"
REFERENCES,0.3621586475942783,"Appendix
437"
REFERENCES,0.36280884265279584,"A
Missing proofs of Section 2
438"
REFERENCES,0.3634590377113134,"A.1
Properties of the relaxed LP
439"
REFERENCES,0.3641092327698309,"In this section we prove a basic result about the feasible sets of the relaxed linear programs defined in
440"
REFERENCES,0.3647594278283485,"Equations (3) and (4). We remark that similar results have been previously shown in Proposition 4 of
441"
REFERENCES,0.36540962288686607,"Bas-Serrano et al. [2021] and Appendix A.1 of Neu and Okolo [2023].
442"
REFERENCES,0.3660598179453836,"Lemma A.1. Suppose that the MDP satisfies the linear MDP assumption in the sense of Definition 2.1,
443"
REFERENCES,0.36671001300390116,"consider the relaxed linear programs 3 and 4 and their respective feasible sets:
444"
REFERENCES,0.3673602080624187,"MP
Φ =

(λ, µ) ∈Rd × RXA
+
 E
Tµ = (1 −γ)ν0 + γΨ
Tλ,
λ = Φ
Tµ
	
,"
REFERENCES,0.3680104031209363,"MD
Φ =

(v, θ) ∈RX × Rd  Ev ≥Φθ,
θ = ω + γΨv
	
."
REFERENCES,0.36866059817945385,"Then, the following statements hold:
445"
REFERENCES,0.3693107932379714,"• The set M =

µ : (λ, µ) ∈MP
Φ
	
coincides with the feasible set of the primal LP (1). Fur-
446"
REFERENCES,0.36996098829648894,"thermore, for all (λ∗, µ∗) ∈arg max(λ,µ)∈MP
Φ ⟨λ, ω⟩, we have that µ∗is the occupancy
447"
REFERENCES,0.3706111833550065,"measure of an optimal policy.
448"
REFERENCES,0.3712613784135241,"• The set V =

v : (v, θ) ∈MD
Φ
	
coincides with the feasible set of the dual LP (2). Further-
449"
REFERENCES,0.3719115734720416,"more, the optimal value function vπ∗and the parameter vector θπ∗satisfying qπ∗= Φθπ∗
450"
REFERENCES,0.37256176853055917,"satisfy (vπ∗, θπ∗) ∈arg min(v,θ)∈MD
Φ(1 −γ) ⟨ν0, v⟩.
451"
REFERENCES,0.3732119635890767,"Proof. We first show that for any feasible point µ of the LP (1), the tuple (λ, µ) is feasible for the
452"
REFERENCES,0.37386215864759426,"relaxed LP with λ = ΦTµ. This choice of λ satisfies the second primal constraint by definition, so it
453"
REFERENCES,0.37451235370611186,"remains to verify that the first constraint is also satisfied. Indeed, this follows from
454"
REFERENCES,0.3751625487646294,"E
Tµ −(1 −γ)ν0 −γΨ
Tλ = E
Tµ −(1 −γ)ν0 −γΨ
TΦ
Tµ
= E
Tµ −(1 −γ)ν0 −γP
Tµ = 0,"
REFERENCES,0.37581274382314694,"where we have used the linear MDP property to write ΨTΦT = P T in the first step and that µ is a
455"
REFERENCES,0.3764629388816645,"valid occupancy measure in the last one. Conversely, supposing that (λ, µ) ∈MP
Φ are feasible for
456"
REFERENCES,0.37711313394018203,"the relaxed LP, we have that
457"
REFERENCES,0.37776332899869963,"E
Tµ −(1 −γ)ν0 −γP
Tµ = E
Tµ −(1 −γ)ν0 −γΨ
TΦ
Tµ
= E
Tµ −(1 −γ)ν0 −γΨ
Tλ = 0,"
REFERENCES,0.3784135240572172,"thus verifying that µ is indeed a valid occupancy measure. Optimality of (λ∗, µ∗) follows from the
458"
REFERENCES,0.3790637191157347,"fact that for any (λ, µ) ∈MP
Φ, we can write the LP objective as ⟨λ, ω⟩= ⟨µ, r⟩by the linear MDP
459"
REFERENCES,0.37971391417425226,"assumption, and the standard fact that any solution µ∗to the primal LP 1 is the occupancy measure
460"
REFERENCES,0.3803641092327698,"of an optimal policy (cf. Theorem 6.9.4 in Puterman, 1994). This concludes the first part of the proof.
461"
REFERENCES,0.3810143042912874,"For the second part of the proof, let us first consider a feasible solution v for the original dual LP (2).
462"
REFERENCES,0.38166449934980495,"Then, the choice θ = ω + γΨv satisfies the second dual constraint by definition. The first constraint
463"
REFERENCES,0.3823146944083225,"can be verified by writing
464"
REFERENCES,0.38296488946684004,"Ev −Φθ = Ev −r −γPv ≥0,"
REFERENCES,0.3836150845253576,"where we used the choice of θ in the first step and the feasibility of v for the original LP in the second
465"
REFERENCES,0.3842652795838752,"step. Conversely, supposing that (θ, v) ∈MD
Φ, we note that
466"
REFERENCES,0.38491547464239273,"Ev −r −γPv = Ev −Φθ ≥0,"
REFERENCES,0.3855656697009103,"which implies the feasibility of v in the LP 2. Optimality of v∗for both LPs follows from the fact
467"
REFERENCES,0.3862158647594278,"that their objectives are identical, and the standard fact that v∗is an optimal solution of the dual
468"
REFERENCES,0.38686605981794536,"LP (2) (cf. Theorem 6.2.2 in Puterman, 1994).
469"
REFERENCES,0.38751625487646296,"B
Missing proofs of Section 4
470"
REFERENCES,0.3881664499349805,"In this section, we provide performance guarantees for Algorithm 1 in terms of the expected subopti-
471"
REFERENCES,0.38881664499349805,"mality of the output policy πJ, and in particular prove the lemmas provided in Section 4 in the main
472"
REFERENCES,0.3894668400520156,"text. Auxiliary lemmas and technical results for proving some of these are included in Appendix E.
473"
REFERENCES,0.39011703511053314,"B.1
Properties of the Dynamic Duality Gap
474"
REFERENCES,0.39076723016905074,"We first prove our claims regarding the dynamic duality gap introduced in Section 4 of the main text.
475"
REFERENCES,0.3914174252275683,"First, we relate the gap to the expected suboptimality (in terms of return) of πJ against a comparator
476"
REFERENCES,0.3920676202860858,"policy π∗in Appendix B.1.1. Next, we relate the dynamic duality gap to the average regret of each
477"
REFERENCES,0.39271781534460337,"player in Appendix B.1.2.
478"
REFERENCES,0.3933680104031209,"B.1.1
Proof of Lemma 4.1
479"
REFERENCES,0.3940182054616385,"By definition of the the dynamic duality gap, we have that
480"
REFERENCES,0.39466840052015606,"GT

Φ
Tµπ∗, π∗, {θπt}T
t=1

= 1 T T
X"
REFERENCES,0.3953185955786736,"t=1
f(Φ
Tµπ∗, π∗; θt) −f(λt, πt; θπt)."
REFERENCES,0.39596879063719115,"Considering the first term, we see that
481"
REFERENCES,0.3966189856957087,"f(Φ
Tµπ∗, π∗; θt) =
D
Φ
Tµπ∗, ω
E
+
D
θt, Φ
Tµλ∗,π∗−Φ
Tµπ∗E"
REFERENCES,0.3972691807542263,"(a)
=
D
µπ∗, r
E
+
D
θt, Φ
Tµλ∗,π∗−Φ
Tµπ∗E"
REFERENCES,0.39791937581274384,"(b)
=
D
µπ∗, r
E
,"
REFERENCES,0.3985695708712614,"where we have used (a) the linear MDP property (definition 2.1) and (b) the following relation:
482"
REFERENCES,0.3992197659297789,"µλ∗,π∗(x, a) = π∗(a|x)
h
(1 −γ)ν0(x) + γ
D
ψ(x), Φ
Tµπ∗Ei"
REFERENCES,0.39986996098829647,"= π∗(a|x)
h
(1 −γ)ν0(x) + γ
X"
REFERENCES,0.40052015604681407,"x′,a′
p (x|x′, a′) µπ∗(x′, a′)
i
= µπ∗(x, a)."
REFERENCES,0.4011703511053316,"Now for the second term, we have
483"
REFERENCES,0.40182054616384916,"f(λt, πt; θπt) = (1 −γ) ⟨ν0, vθπt,πt⟩+ ⟨λt, ω + γΨvθπt,πt −θπt⟩
= ⟨µπt, r⟩+ ⟨λt, ω + γΨvπt −θπt⟩
= ⟨µπt, r⟩,"
REFERENCES,0.4024707412223667,"where we have used the Bellman equations qπt = Φθπ
t = r + γP vπt = Φ (ω + γΨvπt), which
484"
REFERENCES,0.40312093628088425,"together with the fact that Φ is full rank implies that θπt = ω + γΨvπt. Substituting the above
485"
REFERENCES,0.40377113133940185,"expressions for f(ΦTµπ∗, π∗; θt) and f(λt, πt; θπt) in the dynamic duality gap and noting that πJ
486"
REFERENCES,0.4044213263979194,is such that 1
REFERENCES,0.40507152145643693,"T
PT
t=1 ⟨µπt, r⟩= EJ [⟨µπJ, r⟩] we get
487"
REFERENCES,0.4057217165149545,"GT

Φ
Tµπ∗, π∗, {θπt}T
t=1

= EJ
hD
µπ∗−µπJ, r
Ei
."
REFERENCES,0.406371911573472,"This completes the proof.
488"
REFERENCES,0.4070221066319896,"B.1.2
Proof of Lemma 4.2
489"
REFERENCES,0.40767230169050717,"Recall that for any comparator points
 
λ∗, π∗; {θ∗
t }T
t=1

, the dynamic duality gap is defined as
490"
REFERENCES,0.4083224967490247,"GT
 
λ∗, π∗; {θ∗
t }T
t=1

= 1 T T
X"
REFERENCES,0.40897269180754225,"t=1
(f(λ∗, π∗; θt) −f(λt, πt; θ∗
t )) ."
REFERENCES,0.4096228868660598,"Then, by adding and subtracting some terms we express the dynamic duality gap in terms of the
491"
REFERENCES,0.4102730819245774,"average loss of each player with respect to the objective f(λ, π; θ). This gives
492"
REFERENCES,0.41092327698309494,"GT (λ∗, π∗, θ∗
1:T ) = 1 T T
X"
REFERENCES,0.4115734720416125,"t=1
f(λ∗, π∗; θt) −f(λ∗, πt; θt) + 1 T T
X"
REFERENCES,0.41222366710013003,"t=1
f(λ∗, πt; θt) −f(λt, πt; θt) + 1 T T
X"
REFERENCES,0.4128738621586476,"t=1
f(λt, πt; θt) −f(λt, πt; θ∗
t ).
(11)"
REFERENCES,0.4135240572171652,"Consider the first set of terms from the above expression. By definition of f in Equation (7), we
493"
REFERENCES,0.4141742522756827,"immediately obtain the instantaneous regret of the π-player as
494"
REFERENCES,0.41482444733420026,"f(λ∗, π∗; θt) −f(λ∗, πt; θt) = ⟨θt, Φ
Tµλ∗,π∗−Φ
Tµλ∗,πt⟩ =
X"
REFERENCES,0.4154746423927178,"x
ν∗(x)
X"
REFERENCES,0.41612483745123535,"a
(π∗(a|x) −πt(a|x)) qt(x, a) =
X"
REFERENCES,0.41677503250975295,"x
ν∗(x)
X"
REFERENCES,0.4174252275682705,"a
(π∗(a|x) −πt(a|x)) qt(x, a),"
REFERENCES,0.41807542262678804,"where ν∗(x) = (1 −γ)ν0(x) + γ ⟨ψ(x), λ∗⟩. For the regret of the λ and θ-players, notice that we
495"
REFERENCES,0.4187256176853056,"can express the estimator bf in terms of the objective f as follows:
496"
REFERENCES,0.41937581274382313,"bf(λ, π; θ) = (1 −γ) ⟨ν0, vθ,π⟩+
D
λ, ω + γ bΨvθ,π −θ
E"
REFERENCES,0.42002600780234073,"= f(λ, π; θ) + γ
D
λ, bΨvθ,π −Ψvθ,π
E
."
REFERENCES,0.4206762028608583,"Taking advantage of this relation, we now consider the last two set of terms in Equation (11). Indeed,
497"
REFERENCES,0.4213263979193758,"for the second set of terms in the equation, we write
498"
REFERENCES,0.42197659297789336,"f(λ∗, πt; θt) −f(λt, πt; θt)"
REFERENCES,0.4226267880364109,"= bf(λ∗, πt; θt) −bf(λt, πt; θt) −γ
D
λ∗, bΨvθt,πt −Ψvθt,πt
E
+ γ
D
λt, bΨvθt,πt −Ψvθt,πt
E"
REFERENCES,0.4232769830949285,"=
D
λ∗−λt, ω + γ bΨvθt,πt −θt
E
−γ
D
λ∗, bΨvθt,πt −Ψvθt,πt
E
+ γ
D
λt, bΨvθt,πt −Ψvθt,πt
E
,"
REFERENCES,0.42392717815344605,"Notice that the last equality follows directly from definition of bf. Along these lines, we can also
499"
REFERENCES,0.4245773732119636,"express the last set of terms in Equation (11) as follows:
500"
REFERENCES,0.42522756827048114,"f(λt, πt; θt) −f(λt, πt; θ∗
t )"
REFERENCES,0.4258777633289987,"= bf(λt, πt; θt) −bf(λt, πt; θ∗
t ) −γ
D
λt, bΨvθt,πt −Ψvθt,πt
E
+ γ
D
λt, bΨvθt,πt −Ψvθ∗
t ,πt
E"
REFERENCES,0.4265279583875163,"= ⟨θt −θ∗
t , Φ
T bµλt,πt −λt⟩−γ
D
λt, bΨvθt,πt −Ψvθt,πt
E
+ γ
D
λt, bΨvθt,πt −Ψvθ∗
t ,πt
E
,"
REFERENCES,0.4271781534460338,"Plugging the above derivations in the dynamic duality gap, we have that
501"
REFERENCES,0.42782834850455137,"GT (λ∗, π∗, θ∗
1:T ) = 1 T T
X t=1 X"
REFERENCES,0.4284785435630689,"x
ν∗(x)
X"
REFERENCES,0.42912873862158646,"a
(π∗(a|x) −πt(a|x)) qt(x, a) + 1 T T
X t=1"
REFERENCES,0.42977893368010406,"D
λ∗−λt, ω + γ bΨvθt,πt −θt
E + 1 T T
X"
REFERENCES,0.4304291287386216,"t=1
⟨θt −θ∗
t , Φ
T bµλt,πt −λt⟩ + γ T T
X t=1"
REFERENCES,0.43107932379713915,"D
λ∗, Ψvθt,πt −bΨvθt,πt
E
+ γ T T
X t=1"
REFERENCES,0.4317295188556567,"D
λt, bΨvθ∗
t ,πt −Ψvθ∗
t ,πt
E
."
REFERENCES,0.43237971391417424,"This matches the claim of the lemma, thus completing the proof.
502"
REFERENCES,0.43302990897269183,"B.2
Bounding the Regret Terms
503"
REFERENCES,0.4336801040312094,"In this section we provide the proofs of the our claims made in the main text about the regret of each
504"
REFERENCES,0.4343302990897269,"player—precisely, Lemmas 4.3–4.5.
505"
REFERENCES,0.43498049414824447,"B.2.1
Proof of Lemma 4.3
506"
REFERENCES,0.435630689206762,"Consider the regret of the π-player introduced in the main text as,
507"
REFERENCES,0.4362808842652796,"RT (π∗) = T
X t=1 X"
REFERENCES,0.43693107932379716,"x
ν∗(x)
X"
REFERENCES,0.4375812743823147,"a
(π∗(a|x) −πt(a|x)) qt(x, a)"
REFERENCES,0.43823146944083224,"(a)
≤
P"
REFERENCES,0.4388816644993498,x ν∗(x)DKL (π∗(·|x)∥π1 (·|x))
REFERENCES,0.43953185955786733,"α
+ αTR2D2
θ
2
(b)
≤log A"
REFERENCES,0.44018205461638493,"α
+ αTR2D2
θ
2
."
REFERENCES,0.4408322496749025,"We have used (a) the standard Mirror descent analysis of softmax policy iterates recalled in
508"
REFERENCES,0.44148244473342,"Lemma E.1 for completeness, and (b) the fact that π1 is a uniform policy and ν∗∈∆X . Dividing
509"
REFERENCES,0.44213263979193757,"the above expression by T completes the proof.
510"
REFERENCES,0.4427828348504551,"B.2.2
Proof of Lemma 4.4
511"
REFERENCES,0.4434330299089727,"Recall the total regret of the λ-player against any fixed comparator λ∗∈Rd is given as
512"
REFERENCES,0.44408322496749025,"RT (λ∗) = T
X"
REFERENCES,0.4447334200260078,"t=1
⟨λ∗−λt, ω + γ bΨvθt,πt −θt⟩."
REFERENCES,0.44538361508452534,"Since the feature-occupancy updates of Algorithm 1 simply implements a version of the composite-
513"
REFERENCES,0.4460338101430429,"objective mirror descent scheme due to Duchi et al. [2010] we apply the standard analysis of this
514"
REFERENCES,0.4466840052015605,"method (recalled as Lemma C.1 in Appendix C) to bound the instantaneous regret as
515"
REFERENCES,0.44733420026007803,"⟨λ∗−λt, ω + γ bΨvθt,πt −θt⟩"
REFERENCES,0.4479843953185956,"≤
∥λt −λ∗∥2
Λ−1
n −∥λt+1 −λ∗∥2
Λ−1
n
2η
+ η"
REFERENCES,0.4486345903771131,"2 ∥Λngλ(t)∥2
Λ−1
n + ϱ"
REFERENCES,0.44928478543563066,"2 ∥λ∗∥2
Λ−1
n −ϱ"
REFERENCES,0.44993498049414826,"2 ∥λt+1∥2
Λ−1
n ."
REFERENCES,0.4505851755526658,"Then, taking the sum for t = 1, · · · , T, evaluating the telescoping sums and upper-bounding some
516"
REFERENCES,0.45123537061118335,"negative terms by zero yields the expression
517 T
X"
REFERENCES,0.4518855656697009,"t=1
⟨λ∗−λt, ω + γ bΨvθt,πt −θt⟩"
REFERENCES,0.45253576072821844,"≤
∥λ1 −λ∗∥2
Λ−1
n
2η
+ η 2 T
X"
REFERENCES,0.45318595578673604,"t=1
∥Λngλ(t)∥2
Λ−1
n + ϱT"
REFERENCES,0.4538361508452536,"2 ∥λ∗∥2
Λ−1
n −ϱ 2 T
X"
REFERENCES,0.4544863459037711,"t=1
∥λt∥2
Λ−1
n + ϱ"
REFERENCES,0.45513654096228867,"2 ∥λ1∥2
Λ−1
n =
 1"
REFERENCES,0.4557867360208062,2η + ϱT 2
REFERENCES,0.4564369310793238,"
∥λ∗∥2
Λ−1
n + η 2 T
X"
REFERENCES,0.45708712613784136,"t=1
∥Λngλ(t)∥2
Λ−1
n −ϱ 2 T
X"
REFERENCES,0.4577373211963589,"t=1
∥λt∥2
Λ−1
n ."
REFERENCES,0.45838751625487645,"In the equality, we have used that λ1 = 0. Dividing the resulting term by T gives the following
518"
REFERENCES,0.459037711313394,"bound on the average regret:
519"
REFERENCES,0.4596879063719116,"1
T RT (λ∗) ≤
 1"
REFERENCES,0.46033810143042914,2Tη + ϱ 2
REFERENCES,0.4609882964889467,"
∥λ∗∥2
Λ−1
n + η"
T,0.4616384915474642,"2T T
X"
T,0.46228868660598177,"t=1
∥Λngλ(t)∥2
Λ−1
n −ϱ"
T,0.46293888166449937,"2T T
X"
T,0.4635890767230169,"t=1
∥λt∥2
Λ−1
n ."
T,0.46423927178153446,"The proof is completed by applying Lemma C.2 to bound the norm of the gradients and plugging the
520"
T,0.464889466840052,"result into the bound above.
521"
T,0.46553966189856955,"B.2.3
Proof of Lemma 4.5
522"
T,0.46618985695708715,"For the regret of the θ-player, first note that for any policy π with corresponding state-action value
523"
T,0.4668400520156047,"function weights θπ = ω + γΨvπ, we have
524"
T,0.46749024707412223,"∥θπ∥2 = ∥ω + γΨvπ∥2 ≤∥ω∥2 + γ ∥Ψvπ∥2 ≤
√"
T,0.4681404421326398,"d +
γ
√"
T,0.4687906371911573,"d
(1 −γ) = √"
T,0.4694408322496749,"d
(1 −γ),"
T,0.47009102730819247,"where we have used the triangle inequality in the second line. The last inequality uses Definition 2.1
525"
T,0.47074122236671,"and the fact that ∥vπ∥∞≤
1
(1−γ) since the rewards are bounded in [0, 1]. Thanks to this bound, we
526"
T,0.47139141742522755,"can ensure that θ∗
t = θπt ∈Bd(Dθ) holds with the choice Dθ =
√"
T,0.4720416124837451,"d/ (1 −γ) as required by the
527"
T,0.4726918075422627,"lemma.Therefore, by construction of value-parameter updates in Algorithm 1, we have
528"
T,0.47334200260078024,"⟨θt −θ∗
t , Φ
T bµλt,πt −λt⟩≤0
for t = 1, · · · , T."
T,0.4739921976592978,"This concludes the proof.
529"
T,0.47464239271781533,"B.3
Bounding the gap-estimation error
530"
T,0.4752925877763329,"In this section, we provide the proof of Lemma 4.6 which bounds the gap-estimation error defined for
531"
T,0.4759427828348505,"an arbitrary comparator sequence (λ∗, πt, θ∗
t ) ∈Rd × Π (Dπ) × Bd(Dθ) for t = 1, . . . , T as,
532"
T,0.476592977893368,"err b
Ψ = T
X t=1"
T,0.47724317295188556,"λ∗,
 
Ψ −bΨ

vθt,πt

+ T
X t=1"
T,0.4778933680104031,"λt,
  bΨ −Ψ

vθ∗
t ,πt

."
T,0.47854356306892065,"We control the above term with the now-classic techniques developed by Jin et al. [2020] for bounding
533"
T,0.47919375812743825,"model-estimation errors for linear MDPs. These results also make heavy use of self-normalized tail
534"
T,0.4798439531859558,"inequalities as popularized by Abbasi-Yadkori et al. [2011] (see also Lattimore and Szepesvári, 2020).
535"
T,0.48049414824447334,"To make this clear, we first note that, for any λ ∈Rd, v ∈RX, and ξ > 0,
536"
T,0.4811443433029909,"D
λ,

bΨ −Ψ

v
E (a)
≤∥λ∥Λ−1
n"
T,0.48179453836150843,"Λn

bΨ −Ψ

v

Λ−1
n"
T,0.48244473342002603,"(b)
≤
∥λ∥2
Λ−1
n
2Tξ
+ Tξ 2"
T,0.4830949284785436,"Λn

bΨ −Ψ

v

2"
T,0.4837451235370611,"Λ−1
n
."
T,0.48439531859557866,"Here, we have first used (a) the Cauchy–Schwarz inequality, and (b) the inequality of arithmetic and
537"
T,0.4850455136540962,"geometric means. Using this expression, we can upper-bound the gap estimation error as
538"
T,0.4856957087126138,"err b
Ψ ≤
∥λ∗∥2
Λ−1
n
2ξ
+ T
X t=1"
T,0.48634590377113135,"∥λt∥2
Λ−1
n
2Tξ + Tξ 2 T
X t=1"
T,0.4869960988296489,"Λn

bΨ −Ψ

vθt,πt

2"
T,0.48764629388816644,"Λ−1
n
+ Tξ 2 T
X t=1"
T,0.488296488946684,"Λn

bΨ −Ψ

vπt

2"
T,0.4889466840052016,"Λ−1
n
.
(12)"
T,0.4895968790637191,"To control the last two terms in the bound, we employ two main lemmas stated below.
539"
T,0.49024707412223667,"Lemma B.1. Let v ∈[−B, B]X. With probability at least 1 −δ, we have that:
540"
T,0.4908972691807542,"Λn

bΨ −Ψ

v

Λ−1
n
≤2B
√n"
T,0.49154746423927176,"v
u
u
td log "
T,0.49219765929778936,1 + R2 dβ !
T,0.4928478543563069,+ 2 log 1
T,0.49349804941482445,"δ + B
p dβ."
T,0.494148244473342,"Lemma B.2. Consider the function class,
541"
T,0.49479843953185954,"V =
n
vπ,θ : X →[−RDθ, RDθ]
π ∈Π (Dπ) , θ ∈Bd(Dθ)
o
,"
T,0.49544863459037713,"Let Dπ = αTDθ so that vθt,πt ∈V. For any ϵ ∈(0, 1), with probability at least 1 −δ,
542"
T,0.4960988296488947,"Λn

bΨ −Ψ

vθt,πt

Λ−1
n"
T,0.4967490247074122,"≤2RDθ
√n"
T,0.49739921976592977,"v
u
u
td log "
T,0.4980494148244473,1 + R2 dβ !
T,0.4986996098829649,"+ 4d log

1 + 4αTR2D2
θ
ϵ"
T,0.49934980494148246,"
+ 2 log 1 δ"
T,0.5,"+ RDθ
p"
T,0.5006501950585176,"dβ +
p"
T,0.5013003901170351,"β + 1

ϵ
√ d."
T,0.5019505851755527,"The rather tedious but otherwise standard proofs of the above lemmas are given in Appendix D. Now,
543"
T,0.5026007802340702,"taking into account the fact that for Dθ large enough vπt ∈V yields Corollary B.3 below.
544"
T,0.5032509752925878,"Corollary B.3. In the linear MDP setting described in Definition 2.1, notice that vπt = vθπt,πt with
545"
T,0.5039011703511054,"θπt = ω + γΨvθπt,πt. Furthermore, with RDθ = R
√"
T,0.5045513654096229,"d/(1 −γ) ≥∥vπt∥∞and Dπ = αTDθ we
546"
T,0.5052015604681405,"have that vπt ∈V. Therefore, for all ϵ > 0 with probability at least 1 −δ, the following holds:
547
Λn

bΨ −Ψ

vπt

Λ−1
n ≤
2
√"
T,0.505851755526658,"d
√n (1 −γ)"
T,0.5065019505851756,"v
u
u
td log "
T,0.5071521456436932,1 + R2 dβ !
T,0.5078023407022106,+ 4d log 
T,0.5084525357607282,1 + 4αTR2d
T,0.5091027308192457,ϵ (1 −γ)2 !
T,0.5097529258777633,+ 2 log 1 δ
T,0.5104031209362809,"+
d√β
(1 −γ) +
p"
T,0.5110533159947984,"β + 1

ϵ
√ d."
T,0.511703511053316,"In the following, we apply these results to bound the last two terms in the right-hand side of Equa-
548"
T,0.5123537061118335,"tion (12). Precisely, using Dθ =
√"
T,0.5130039011703511,"d/(1−γ), α =
p"
T,0.5136540962288687,"2 log A/R2D2
θT =
q"
T,0.5143042912873862,"2 (1 −γ)2 log A/R2dT
549"
T,0.5149544863459038,"(which follows from optimizing the regret of the π-player in Lemma 4.3), as well as ϵ =
550"
T,0.5156046814044213,"4αR2d/ (1 −γ)2 =
√"
T,0.5162548764629389,"32R2d log A
(1−γ)
√"
T,0.5169050715214565,"T
and β = R2/dT we have that in any round t, with probabil-
551"
T,0.517555266579974,"ity at least 1 −δ,
552"
T,0.5182054616384916,"Λn

bΨ −Ψ

vθt,πt

Λ−1
n
≤ s"
T,0.5188556566970091,20d2 log (2T/δ)
T,0.5195058517555267,"n (1 −γ)2
+ s R2d"
T,0.5201560468140443,T (1 −γ)2 + r
T,0.5208062418725617,"R4d log A T 2
+ s"
T,0.5214564369310793,32R2d2 log A
T,0.5221066319895968,T (1 −γ)2 .
T,0.5227568270481144,"Likewise,
553"
T,0.523407022106632,"Λn

bΨ −Ψ

vπt

Λ−1
n
≤ s"
T,0.5240572171651495,20d2 log (2T/δ)
T,0.5247074122236671,"n (1 −γ)2
+ s R2d"
T,0.5253576072821846,T (1 −γ)2 + r
T,0.5260078023407022,"R4d log A T 2
+ s"
T,0.5266579973992198,32R2d2 log A
T,0.5273081924577373,T (1 −γ)2 .
T,0.5279583875162549,"Then plugging in the above bounds in Equation (12) with T ≥
2R2n log A"
T,0.5286085825747724,"log(1/δ) , it follows that for
554"
T,0.52925877763329,"D b
Ψ =
q"
T,0.5299089726918076,320d2 log(2T/δ)
T,0.5305591677503251,"n(1−γ)2
,
555"
T,0.5312093628088427,"err b
Ψ ≤
∥λ∗∥2
Λ−1
n
2ξ
+ T
X t=1"
T,0.5318595578673602,"∥λt∥2
Λ−1
n
2Tξ
+ T 2ξD2
b
Ψ,"
T,0.5325097529258778,"with probability at least 1 −δ, thus proving the claim.
556"
T,0.5331599479843954,"B.4
Full proof of Theorem 3.1
557"
T,0.5338101430429129,"To control the expected suboptimality of the output policy πJ of Algorithm 1, we study the repective
558"
T,0.5344603381014305,"regret and gap-estimation error at the selected comparator points. Precisely, combining Lemma 4.1
559"
T,0.5351105331599479,"and 4.2 when (λ∗, π∗, θ∗
1:T ) =
 
ΦTµπ∗, π∗, θπt
∈Rd × Π (Dπ) × Bd(Dθ), we have that,
560"
T,0.5357607282184655,"EJ
hD
µπ∗−µπJ, r
Ei
= 1"
T,0.5364109232769831,T RT (π∗) + 1
T,0.5370611183355006,"T RT

λπ∗
+ 1"
T,0.5377113133940182,"T RT (θ∗
1:T ) + γ"
T,0.5383615084525357,"T err b
Ψ.
(13)"
T,0.5390117035110533,"where,
561"
T,0.5396618985695709,"RT (π∗) = T
X t=1 X"
T,0.5403120936280884,"x
ν∗(x)
X"
T,0.540962288686606,"a
(π∗(a|x) −πt(a|x)) qt(x, a),"
T,0.5416124837451235,"RT

λπ∗
= T
X"
T,0.5422626788036411,"t=1
⟨λπ∗−λt, ω + γ bΨvθt,πt −θt⟩,"
T,0.5429128738621587,"RT (θ∗
1:T ) = T
X"
T,0.5435630689206762,"t=1
⟨θt −θπt
t , Φ
T bµλt,πt −λt⟩"
T,0.5442132639791938,"err b
Ψ = T
X t=1"
T,0.5448634590377113,"λπ∗,
 
Ψ −bΨ

vθt,πt

+ T
X t=1"
T,0.5455136540962289,"λt,
  bΨ −Ψ

vπt
."
T,0.5461638491547465,"Notice that for this choice of λ∗, by Definition 2.1 ν∗(x) = (1 −γ)ν0(x) + γ⟨ψ(x), µ∗⟩= νπ∗(x)
562"
T,0.546814044213264,"is a valid state occupancy measure. Next, introducing the bounds stated in Lemmas 4.3–4.6 under
563"
T,0.5474642392717816,"the required conditions Dθ =
√"
T,0.548114434330299,"d/ (1 −γ), α =
q"
T,0.5487646293888166,"2 (1 −γ)2 log A/R2dT, Dπ = αTDθ =
564 p"
T,0.5494148244473342,2T log A/R2 and T ≥2R2n log A
T,0.5500650195058517,"log(1/δ) , as well as ξ ≥0 and D b
Ψ =
q"
T,0.5507152145643693,320d2 log(2T/δ)
T,0.5513654096228868,"n(1−γ)2
yields,
565"
T,0.5520156046814044,"EJ
hD
µπ∗−µπJ, r
Ei
≤ s"
T,0.552665799739922,2R2d log A
T,0.5533159947984395,"(1 −γ)2 T +
 1"
T,0.5539661898569571,2ηT + ϱ 2
T,0.5546163849154746," λπ∗
2"
T,0.5552665799739922,"Λ−1
n
+ ηC 2 −ϱ"
T,0.5559167750325098,"2T T
X"
T,0.5565669700910273,"t=1
∥λt∥2
Λ−1
n"
T,0.5572171651495449,"+
γ
2Tξ"
T,0.5578673602080624,"λπ∗
2"
T,0.55851755526658,"Λ−1
n
+ 1 T T
X"
T,0.5591677503250976,"t=1
∥λt∥2
Λ−1
n !"
T,0.5598179453836151,"+ γTξD2
b
Ψ,"
T,0.5604681404421327,"with probability at least 1−δ, where C = 6β
 
d + D2
θ

+3d (1 + RDθ)2+3γ2dR2D2
θ. Rearranging
566"
T,0.5611183355006502,"the bound and selecting ϱ = γ/ξT to eliminate the (potentially large) norm of the iterates, we obtain
567"
T,0.5617685305591678,"EJ
hD
µπ∗−µπJ, r
Ei
≤ s"
T,0.5624187256176854,d log (1/δ)
T,0.5630689206762028,"n (1 −γ)2 +
 1"
T,0.5637191157347204,2ηT + ϱ
T,0.5643693107932379,"2 +
γ
2ξT"
T,0.5650195058517555," λπ∗
2"
T,0.5656697009102731,"Λ−1
n
+ ηC 2 +
 γ"
T,0.5663198959687906,"ξT −ϱ
 1"
T,0.5669700910273082,"2T T
X"
T,0.5676202860858257,"t=1
∥λt∥2
Λ−1
n + γTξD2
b
Ψ = s"
T,0.5682704811443433,d log (1/δ)
T,0.5689206762028609,"n (1 −γ)2 +
 1"
T,0.5695708712613784,2ηT + γ ξT
T,0.570221066319896," λπ∗
2"
T,0.5708712613784135,"Λ−1
n
+ ηC"
T,0.5715214564369311,"2 + γTξD2
b
Ψ."
T,0.5721716514954487,"Furthermore, choosing ξ = 1/TD b
Ψ i.e ϱ = γD b
Ψ, we further simplify the above bound on the regret
568"
T,0.5728218465539662,"in terms of the optimization error arising from the policy and feature occupancy updates as,
569"
T,0.5734720416124838,"EJ
hD
µπ∗−µπJ, r
Ei
≤ s"
T,0.5741222366710013,d log (1/δ)
T,0.5747724317295189,"n (1 −γ)2 +
1
2ηT"
T,0.5754226267880365,"λπ∗
2"
T,0.576072821846554,"Λ−1
n
+ ηC"
T,0.5767230169050716,"2 + γ
λπ∗
2"
T,0.577373211963589,"Λ−1
n
+ 1

D b
Ψ."
T,0.5780234070221066,"Moving our attention to our earlier bound on the norm of gλ(t),
570"
T,0.5786736020806242,"C = 6β
 
d + D2
θ

+ 3d (1 + RDθ)2 + 3γ2dR2D2
θ ≤27R2d2"
T,0.5793237971391417,(1 −γ)2 .
T,0.5799739921976593,"The inequality follows from our earlier choice of β = R2/dT and that T ≥1/d2. Plugging the values
571"
T,0.5806241872561768,"of C and D b
Ψ in the bound, then choosing η =
q"
T,0.5812743823146944,(1−γ)2
T,0.581924577373212,27R2d2T and using the condition T ≥2R2n log A
T,0.5825747724317295,"log(1/δ) ,
572"
T,0.5832249674902471,"we have that with probability at least 1 −δ,
573"
T,0.5838751625487646,"EJ
hD
µπ∗−µπJ, r
Ei
≤ s"
T,0.5845253576072822,d log (1/δ)
T,0.5851755526657998,"n (1 −γ)2 +
λπ∗
2"
T,0.5858257477243173,"Λ−1
n
+ 1
 s"
T,0.5864759427828349,27d2 log (1/δ)
T,0.5871261378413524,8n log A (1 −γ)2
T,0.58777633289987,"+ γ
λπ∗
2"
T,0.5884265279583876,"Λ−1
n
+ 1
 s"
T,0.5890767230169051,320d2 log (2T/δ)
T,0.5897269180754227,n (1 −γ)2 = O  
T,0.5903771131339401,"λπ∗2"
T,0.5910273081924577,"Λ−1
n + 1"
T,0.5916775032509753,(1 −γ) r
T,0.5923276983094928,d2 log (2T/δ) n  .
T,0.5929778933680104,"This completes the proof.
574"
T,0.5936280884265279,"C
Missing proofs of Section B.2
575"
T,0.5942782834850455,"Lemma C.1. (cf. Lemma 1 of Duchi et al. [2010]) Let gλ(t) = ω + γ bΨvθt,πt −θt Given λ1 = 0
576"
T,0.5949284785435631,"and ϱ, η > 0 and the sequence of iterates {λt}T
t=2 defined for t = 1, · · · , T as:
577"
T,0.5955786736020806,"λt+1 = arg min
λ∈Rd"
T,0.5962288686605982,"n
−⟨λ, gλ(t)⟩+ 1"
T,0.5968790637191157,"2η ∥λ −λt∥2
Λ−1
n + ϱ"
T,0.5975292587776333,"2 ∥λ∥2
Λ−1
n"
T,0.5981794538361509,"o
.
(14)"
T,0.5988296488946684,"Then, for any λ∗∈Rd,
578"
T,0.599479843953186,"⟨λ∗−λt, ω + γ bΨvθt,πt −θt⟩"
T,0.6001300390117035,"≤
∥λt −λ∗∥2
Λ−1
n −∥λt+1 −λ∗∥2
Λ−1
n
2η
+ η"
T,0.6007802340702211,"2 ∥Λngλ(t)∥2
Λ−1
n + ϱ"
T,0.6014304291287387,"2 ∥λ∗∥2
Λ−1
n −ϱ"
T,0.6020806241872562,"2 ∥λt+1∥2
Λ−1
n ."
T,0.6027308192457738,"Proof. The proof of Lemma C.1 follows directly from the referenced Lemma from Duchi et al.
579"
T,0.6033810143042913,"[2010]. Consider,
580"
T,0.6040312093628089,"⟨λ∗−λt, gλ(t)⟩+ ϱ"
T,0.6046814044213265,"2 ∥λt+1∥2
Λ−1
n −ϱ"
T,0.6053315994798439,"2 ∥λ∗∥2
Λ−1
n"
T,0.6059817945383615,"=

λt+1 −λ∗, −gλ(t) + 1"
T,0.606631989596879,"η Λ−1
n (λt+1 −λt) + ϱΛ−1
n λt+1"
T,0.6072821846553966,"+ ⟨λt+1 −λt, gλ(t)⟩"
T,0.6079323797139142,"−

λt+1 −λ∗, 1"
T,0.6085825747724317,"η Λ−1
n (λt+1 −λt) + ϱΛ−1
n λt+1 + ϱ"
T,0.6092327698309493,"2 ∥λt+1∥2
Λ−1
n −ϱ"
T,0.6098829648894668,"2 ∥λ∗∥2
Λ−1
n"
T,0.6105331599479844,"(a)
≤⟨λt+1 −λt, gλ(t)⟩−1"
T,0.611183355006502,"η

λt+1 −λ∗, Λ−1
n (λt+1 −λt)"
T,0.6118335500650195,"+ ϱ

λ∗, Λ−1
n λt+1

−ϱ"
T,0.6124837451235371,"2 ∥λt+1∥2
Λ−1
n −ϱ"
T,0.6131339401820546,"2 ∥λ∗∥2
Λ−1
n"
T,0.6137841352405722,"(b)
≤⟨λt+1 −λt, gλ(t)⟩+ 1"
T,0.6144343302990898,"η

λt+1 −λ∗, Λ−1
n (λt −λt+1)"
T,0.6150845253576073,"(c)
= ⟨λt+1 −λt, gλ(t)⟩−1"
T,0.6157347204161249,"2η ∥λt+1 −λt∥2
Λ−1
n + 1 2η"
T,0.6163849154746424,"
∥λ∗−λt∥2
Λ−1
n −∥λ∗−λt+1∥2
Λ−1
n  ≤1"
T,0.61703511053316,"η sup
y∈Rd"
T,0.6176853055916776,"D
y, ηΛ1/2
n gλ(t)
E
−1"
T,0.618335500650195,"2 ∥y∥2
2 
+ 1 2η"
T,0.6189856957087126,"
∥λ∗−λt∥2
Λ−1
n −∥λ∗−λt+1∥2
Λ−1
n "
T,0.6196358907672301,"(d)
= η"
T,0.6202860858257477,"2 ∥Λngλ(t)∥2
Λ−1
n + 1 2η"
T,0.6209362808842653,"
∥λ∗−λt∥2
Λ−1
n −∥λ∗−λt+1∥2
Λ−1
n "
T,0.6215864759427828,"We have used
581"
T,0.6222366710013004,"(a) The first order optimality condition on Equation (14):
582"
T,0.6228868660598179,"For any λ ∈Rd,
583"
T,0.6235370611183355,"λt+1 −λ, −gλ(t) + 1"
T,0.6241872561768531,"η Λ−1
n (λt+1 −λt) + ϱΛ−1
n λt+1 ≤0."
T,0.6248374512353706,"(b) The relation:
584"
T,0.6254876462938882,"ϱ

λ∗, Λ−1
n λt+1

−ϱ"
T,0.6261378413524057,"2 ∥λt+1∥2
Λ−1
n −ϱ"
T,0.6267880364109233,"2 ∥λ∗∥2
Λ−1
n
= −ϱ"
T,0.6274382314694408,"2 ∥λt+1 −λ∗∥2
Λ−1
n
≤0."
T,0.6280884265279584,"(c) By definition of the squared L2-norm for vectors a = Λ−1/2
n
(λt+1 −λ∗) and b =
585"
T,0.628738621586476,"Λ−1/2
n
(λt −λt+1):
586"
T,0.6293888166449935,"⟨a, b⟩= 1 2"
T,0.6300390117035111,"
−∥b∥2
2 + ∥a + b∥2
2 −∥a∥2
2

."
T,0.6306892067620286,"Note that a and b are well defined since Λn is both symmetric and positive definite.
587"
T,0.6313394018205462,(d) By definition of the Fenchel conjugate of 1
T,0.6319895968790638,"2 ∥y∥2
2 for y ∈Rd.
588"
T,0.6326397919375812,"Rearranging the terms and plugging in gλ(t) = ω + γ bΨvθt,πt −θt completes the proof.
589"
T,0.6332899869960988,"Finally, we will use the following result that bounds the gradient norms appearing in the bound above.
590"
T,0.6339401820546163,"Lemma C.2. Under the conditions of the linear MDP setting we have that,
591"
T,0.6345903771131339,"∥Λngλ(t)∥2
Λ−1
n
≤6β
 
d + D2
θ

+ 3d (1 + RDθ)2 + 3γ2dR2D2
θ."
T,0.6352405721716515,"Proof. Recall that for t = 1, · · · , T gλ(t) = ω + γ bΨvθt,πt −θt. Then,
592"
T,0.635890767230169,"∥Λngλ(t)∥2
Λ−1
n
=
Λn
h
ω + γ bΨvθt,πt −θt
i
2 Λ−1
n ="
T,0.6365409622886866,"β (ω −θt) + 1 n n
X"
T,0.6371911573472041,"i=1
φi (r (xi, ai) −⟨φi, θt⟩) + γΛn bΨvθt,πt  2 Λ−1
n"
T,0.6378413524057217,"≤3 ∥β (ω −θt)∥2
Λ−1
n + 3"
N,0.6384915474642393,"1
n n
X"
N,0.6391417425227568,"i=1
φi (r (xi, ai) −⟨φi, θt⟩)  2"
N,0.6397919375812744,"Λ−1
n
+ 3γ2 Λn bΨvθt,πt

2"
N,0.6404421326397919,"Λ−1
n
."
N,0.6410923276983095,"Now to bound each of the three terms, we use that
593"
N,0.6417425227568271,"∥β (ω −θt)∥2
Λ−1
n
≤2β2 Λ−1
n

2
 
d + D2
θ

≤2β
 
d + D2
θ

,"
N,0.6423927178153446,"where the first inequality uses the assumption that ∥ω∥2 ≤
√"
N,0.6430429128738622,"d (cf. Definition 2.1) and θt ∈Bd(Dθ).
594"
N,0.6436931079323797,"Next, we have that
595"
N,0.6443433029908973,"1
n n
X"
N,0.6449934980494149,"i=1
φi (r (xi, ai) −⟨φi, θt⟩)  2"
N,0.6456436931079323,"Λ−1
n
≤1 n n
X"
N,0.64629388816645,"i=1
∥φi∥2
Λ−1
n |r (xi, ai) −⟨φi, θt⟩|2"
N,0.6469440832249674,≤d (1 + RDθ)2 .
N,0.647594278283485,"The last step follows from the fact that the rewards are bounded in [0, 1], ∥φi∥≤R, θt ∈Bd(Dθ)
596"
N,0.6482444733420026,"and Equation (17). The last remaining term is bounded as
597"
N,0.6488946684005201,"Λn bΨvθt,πt

2"
N,0.6495448634590377,"Λ−1
n
="
N,0.6501950585175552,"1
n n
X"
N,0.6508452535760728,"i=1
φivθt,πt (x′
i)  2 Λ−1
n ≤1 n n
X"
N,0.6514954486345904,"i=1
∥φi∥2
Λ−1
n ∥vθt,πt∥2
∞≤dR2D2
θ"
N,0.6521456436931079,"Therefore, we obtain
598"
N,0.6527958387516255,"∥Λngλ(t)∥2
Λ−1
n
≤6β
 
d + D2
θ

+ 3d (1 + RDθ)2 + 3γ2dR2D2
θ"
N,0.653446033810143,"and this completes the proof.
599"
N,0.6540962288686606,"D
Missing proofs of Section B.3
600"
N,0.6547464239271782,"In this section, we prove the lemmas stated in Section B.3.
601"
N,0.6553966189856957,"D.1
Proof of Lemma B.1
602"
N,0.6560468140442133,"By definition of Λn Section 3 and bΨ in Equation (8), we can write:
603"
N,0.6566970091027308,"Λn

bΨ −Ψ

v = Λn"
N,0.6573472041612484,"1
nΛ−1
n n
X"
N,0.657997399219766,"i=1
φie
T
x′
i ! v − "
N,0.6586475942782835,"βIn + 1 n n
X"
N,0.659297789336801,"i=1
φiφ
T
i ! Ψv = 1 n n
X"
N,0.6599479843953185,"i=1
φi[v (x′
i) −⟨p (·|xi, ai) , v⟩] −βΨv"
N,0.6605981794538361,"In the last equality we used definition 2.1 to write φT
iΨ = p (·|xi, ai)
T. Let ξi = v (x′
i) −
604"
N,0.6612483745123537,"⟨p (·|xi, ai) , v⟩. Then,
605"
N,0.6618985695708712,"Λn

bΨ −Ψ

v

Λ−1
n
≤"
N,0.6625487646293888,"1
n n
X"
N,0.6631989596879063,"i=1
φiξi"
N,0.6638491547464239,"Λ−1
n
+ ∥βΨv∥Λ−1
n ."
N,0.6644993498049415,"We easily control the second term with the relation:
606"
N,0.665149544863459,"∥βΨv∥Λ−1
n
≤β
Λ−1/2
n

2 ∥Ψv∥2 ≤B
p"
N,0.6657997399219766,"dβ
(15)"
N,0.6664499349804941,"The last inequality follows from the fact that
Λ−1/2
n

2 ≤1/√β and by definition 2.1 ∥Ψv∥2 ≤
607 B
√"
N,0.6671001300390117,"d for v ∈[−B, B]X.
608"
N,0.6677503250975293,"Now, to handle the first term, let D0 = ∅. We construct a filtration Fi−1 = Di−1 ∪(x0
i , xi, ai, ri)
609"
N,0.6684005201560468,"for i = 1, 2, · · · , n. Notice that by construction of the dataset ξi is a martingale difference sequence
610"
N,0.6690507152145644,"(i.e E [ξi |Fi−1 ] = 0) taking values in the range [−2B, 2B]. Then, we can directly apply Lemma E.3
611"
N,0.6697009102730819,"to obtain a bound on the first term as:
612"
N,0.6703511053315995,"1
n n
X"
N,0.6710013003901171,"i=1
φiξi"
N,0.6716514954486346,"Λ−1
n
=
1
√n"
N,0.6723016905071522,"v
u
u
u
t  n
X"
N,0.6729518855656696,"i=1
φiξi  2"
N,0.6736020806241872,"(nΛn)−1
≤2B
√n"
N,0.6742522756827048,"v
u
u
t2 log"
N,0.6749024707412223,det (nΛn)1/2 det (nβI)−1/2 δ !
N,0.6755526657997399,"≤2B
√n"
N,0.6762028608582574,"v
u
u
td log "
N,0.676853055916775,1 + R2 dβ !
N,0.6775032509752926,+ 2 log 1 δ .
N,0.6781534460338101,"with probability 1 −δ. In the last inequality we have used the AM-GM inequality and bound on the
613"
N,0.6788036410923277,"feature vectors:
614"
N,0.6794538361508452,"det (nΛn) ≤
tr (nΛn) d"
N,0.6801040312093628,"d
=

nβ + tr (Pn
i=1 φiφT
i)
d"
N,0.6807542262678804,"d
≤

nβ + nR2 d d
."
N,0.6814044213263979,"Putting everything together, we have that w.p 1 −δ,
615"
N,0.6820546163849155,"Λn

bΨ −Ψ

v

Λ−1
n
≤2B
√n"
N,0.682704811443433,"v
u
u
td log "
N,0.6833550065019506,1 + R2 dβ !
N,0.6840052015604682,+ 2 log 1
N,0.6846553966189857,"δ + B
p dβ."
N,0.6853055916775033,"This completes the proof.
616"
N,0.6859557867360208,"D.2
Proof of Lemma B.2
617"
N,0.6866059817945384,"Unlike Lemma B.1, we now aim to control the error term
Λn

bΨ −Ψ

v

Λ−1
n
when v is random.
618"
N,0.687256176853056,"Also, notice that with π1 (a|x) =
e⟨φ(x,a),0⟩
P"
N,0.6879063719115734,"a′∈A e⟨φ(x,a′),0⟩as the uniform policy, for t = 1, · · · , T we have
619"
N,0.688556566970091,"that,
620"
N,0.6892067620286085,"πt+1(a|x) =
π1(a|x)eα⟨φ(x,a),Pt
k=1 θk⟩
P"
N,0.6898569570871261,"a′ π1(a′|x)eα⟨φ(x,a′),Pt
k=1 θk⟩=
e⟨φ(x,a),α Pt
k=0 θk⟩
P"
N,0.6905071521456437,"a′ e⟨φ(x,a′),α Pt
k=0 θk⟩,"
N,0.6911573472041612,"where θ0 = 0. Furthermore, since {θt}T
t=1 ⊂Bd(Dθ), for any t
α Pt
k=0 θk

2 ≤αTDθ. Hence,
621"
N,0.6918075422626788,"with Dπ = αTDθ, πt ∈Π (Dπ) and vθt,πt ∈V.
622"
N,0.6924577373211963,"Therefore, as we have seen in previous works [Jin et al., 2020, Hong and Tewari, 2024], the quantity
623
Λn

bΨ −Ψ

vθt,πt

Λ−1
n
can be controlled without any dependence on the size of the state space
624"
N,0.6931079323797139,"with a uniform covering argument over V. Let Cv be an ϵ-cover of V. That is, for vπt,θt ∈V, there
625"
N,0.6937581274382315,"exists v′ ∈Cv such that ∥vπ,θt −v′∥∞≤ϵ. Then, we can write:
626"
N,0.694408322496749,"Λn

bΨ −Ψ

vθt,πt

Λ−1
n"
N,0.6950585175552666,"≤
Λn

bΨ −Ψ

v′
Λ−1
n
+
Λn bΨ (vθt,πt −v′)

Λ−1
n
+ ∥ΛnΨ (v′ −vθt,πt)∥Λ−1
n
(16)"
N,0.6957087126137841,"Consider the first term in the bound. Note that v′ is still random with respect to uncertainty in the
627"
N,0.6963589076723017,"learning process. However, due to the structure of V we know that Cv exists and has cardinality
628"
N,0.6970091027308193,"log |Cv| = O
 
d log
 
1 + 4RDπRDθ"
N,0.6976592977893368,"ϵ

(see Lemma E.6). Inspired by Lemma B.1, consider the event:
629 Ev = ("
N,0.6983094928478544,"exists v ∈Cv :
Λn

bΨ −Ψ

v

Λ−1
n
> 2RDθ
√n"
N,0.6989596879063719,"v
u
u
td log "
N,0.6996098829648895,1 + R2 dβ !
N,0.7002600780234071,+ 2 log 1
N,0.7009102730819246,"δ′ +RDθ
p dβ )"
N,0.7015604681404422,"Since Cv ⊆V, we know from Lemma B.1 that P (Ev) ≤δ′. Now, taking the union bound over the
630"
N,0.7022106631989596,"cover Cv we have that,
631 P ["
N,0.7028608582574772,"v∈Cv
Ev !"
N,0.7035110533159948,≤|Cv|δ′.
N,0.7041612483745123,"Therefore for any v′ ∈Cv with probability at least 1 −δ,
632"
N,0.7048114434330299,"Λn

bΨ −Ψ

v′
Λ−1
n"
N,0.7054616384915474,"≤2RDθ
√n"
N,0.706111833550065,"v
u
u
td log "
N,0.7067620286085826,1 + R2 dβ !
N,0.7074122236671001,+ 2 log |Cv|
N,0.7080624187256177,"δ
+ RDθ
p dβ"
N,0.7087126137841352,"≤2RDθ
√n"
N,0.7093628088426528,"v
u
u
td log "
N,0.7100130039011704,1 + R2 dβ !
N,0.7106631989596879,"+ 4d log

1 + 4RDπRDθ ϵ"
N,0.7113133940182055,"
+ 2 log 1"
N,0.711963589076723,"δ + RDθ
p dβ"
N,0.7126137841352406,"Now, for the second term in Equation (16) we write,
633"
N,0.7132639791937582,"Λn bΨ (vθt,πt −v′)

2"
N,0.7139141742522757,"Λ−1
n
="
N,0.7145643693107933,"1
n n
X"
N,0.7152145643693107,"i=1
φi (vθt,πt (x′
i) −v′ (x′
i))  2 Λ−1
n"
N,0.7158647594278283,"(a)
≤1 n n
X"
N,0.716514954486346,"i=1
|vθt,πt (x′
i) −v′ (x′
i)|2 ∥φi∥2
Λ−1
n ≤ϵ2 1 n n
X"
N,0.7171651495448634,"i=1
∥φi∥2
Λ−1
n"
N,0.717815344603381,"(b)
≤ϵ2d."
N,0.7184655396618985,"We have used (a) Jensen’s inequality and (b) since Λn ≻0, the relation,
634"
N,0.7191157347204161,"1
n n
X"
N,0.7197659297789337,"i=1
φ
T
iΛ−1
n φi = 1 n n
X"
N,0.7204161248374512,"i=1
tr
 
Λ−1
n φiφ
T
i

= tr "
N,0.7210663198959688,"Λ−1
n
1
n n
X"
N,0.7217165149544863,"i=1
φiφ
T
i !"
N,0.7223667100130039,"≤tr (I) = d.
(17)"
N,0.7230169050715215,"For the last term, notice that:
635"
N,0.723667100130039,"∥ΛnΨ (v′ −vθt,πt)∥Λ−1
n
="
N,0.7243172951885566,"βΨ (v′ −vθt,πt) + 1 n n
X"
N,0.7249674902470741,"i=1
φi
hX"
N,0.7256176853055917,"x′
p (x′|xi, ai) (v′ (x′) −vθt,πt (x′))
i
Λ−1
n"
N,0.7262678803641093,"(a)
≤ϵ
p dβ +"
N,0.7269180754226268,"v
u
u
t"
N,0.7275682704811444,"1
n n
X"
N,0.7282184655396619,"i=1
φi
hX"
N,0.7288686605981795,"x′
p (x′|xi, ai) (v′ (x′) −vθt,πt (x′))
i 2 Λ−1
n"
N,0.729518855656697,"(b)
≤ϵ
p dβ +"
N,0.7301690507152145,"v
u
u
t 1 n n
X"
N,0.7308192457737321,"i=1
∥v′ −vθt,πt∥2
∞∥φi∥2
Λ−1
n"
N,0.7314694408322496,"(c)
≤ϵ
p"
N,0.7321196358907672,"dβ + ϵ
√"
N,0.7327698309492848,"d = ϵ
√ d
p"
N,0.7334200260078023,"β + 1

."
N,0.7340702210663199,"This follows from (a) Equation (15) since v = v′ −vθt,πt ∈[−ϵ, ϵ]X and (b) monotonicity of the
636"
N,0.7347204161248374,"square root function as well as Jensen’s inequality and (c) Equation (17).
637"
N,0.735370611183355,"Finally, plugging the above results back into Equation (16), we have that with probability at least
638"
N,0.7360208062418726,"1 −δ,
639"
N,0.7366710013003901,"Λn

bΨ −Ψ

vθt,πt

Λ−1
n"
N,0.7373211963589077,"≤2RDθ
√n"
N,0.7379713914174252,"v
u
u
td log "
N,0.7386215864759428,1 + R2 dβ !
N,0.7392717815344604,"+ 4d log

1 + 4RDπRDθ ϵ"
N,0.7399219765929779,"
+ 2 log 1 δ"
N,0.7405721716514955,"+ RDθ
p"
N,0.741222366710013,"dβ +
p"
N,0.7418725617685306,"β + 1

ϵ
√ d"
N,0.7425227568270482,"The proof of Lemma B.2 is complete.
640"
N,0.7431729518855656,"E
Auxiliary Lemmas
641"
N,0.7438231469440832,"Lemma E.1. Let q1, · · · , qt be a sequence of iterates satisfying ∥qt∥∞≤RDθ by virtue of
642"
N,0.7444733420026007,"definition 2.1 and θt ∈Bd(Dθ). Given an initial policy π1 and learning rate α > 0, and sequence of
643"
N,0.7451235370611183,"policies {πt}T
t=2 defined as:
644"
N,0.7457737321196359,"πt+1(a|x) =
πt(a|x)eαqt(x,a)
P"
N,0.7464239271781534,"a′ πt(a′|x)eαqt(x,a′) ,"
N,0.747074122236671,"Then, for any comparator policy π∗and ν∗some state distribution,
645 T
X t=1 X"
N,0.7477243172951885,"x
ν∗(x)
X"
N,0.7483745123537061,"a
(π∗(a|x) −πt(a|x)) qt(x, a) ≤
P"
N,0.7490247074122237,x ν∗(x)DKL (π∗(·|x)∥π1 (·|x))
N,0.7496749024707412,"α
+ αTR2D2
θ
2
."
N,0.7503250975292588,"The proof of the lemma follows from bounding the regret of the π-player in each state x as
646 T
X t=1 X"
N,0.7509752925877763,"a
(π∗(a|x) −πt(a|x)) qt(x, a) ≤DKL (π∗(·|x)∥π1 (·|x)) α
+ α 2 T
X"
N,0.7516254876462939,"t=1
∥qt(x, ·)∥2
∞,"
N,0.7522756827048115,"via the application of the standard analysis of the exponentially weighted forecaster of Vovk [1990],
647"
N,0.752925877763329,"Littlestone and Warmuth [1994], Freund and Schapire [1997] (see, e.g., Theorem 2.2 in Cesa-Bianchi
648"
N,0.7535760728218466,"and Lugosi, 2006), and noting that ∥qt∥∞≤RDθ for all t.
649"
N,0.7542262678803641,"Lemma E.2. Suppose that ∥φ(x, a)∥2 ≤R for all (x, a) ∈X × A. Let πθ, πθ′ be softmax policies.
650"
N,0.7548764629388817,"Then, for all states x ∈X we have that:
651 X"
N,0.7555266579973993,"a
|πθ (a|x) −πθ′ (a|x)| ≤R ∥θ −θ′∥2"
N,0.7561768530559168,"holds for any θ, θ′ ∈Rd.
652"
N,0.7568270481144344,"Proof. Recall that,
653"
N,0.7574772431729518,Π (Dπ) = (
N,0.7581274382314694,"πθ (a|x) =
e⟨φ(x,a),θ⟩
P"
N,0.758777633289987,"a′ e⟨φ(x,a′),θ⟩"
N,0.7594278283485045,θ ∈Bd(Dπ) ) .
N,0.7600780234070221,"For πθ, πθ′ ∈Π (Dπ) using Pinsker’s inequality we have that,
654"
N,0.7607282184655396,"∥πθ (·|x) −πθ′ (·|x)∥1 ≤
p"
N,0.7613784135240572,"2DKL (πθ (·|x)∥πθ′ (·|x))
for x ∈X.
(18)"
N,0.7620286085825748,"Furthermore, taking into account the specific structure of the policies, we can write:
655"
N,0.7626788036410923,"DKL (πθ (·|x)∥πθ′ (·|x)) =
X"
N,0.7633289986996099,"a
πθ (a|x) log πθ (a|x)"
N,0.7639791937581274,"πθ′ (a|x) = −
X"
N,0.764629388816645,"a
πθ (a|x) ⟨φ(x, a), θ′ −θ⟩+ log
P
a e⟨φ(x,a),θ′⟩
P"
N,0.7652795838751626,"a e⟨φ(x,a),θ⟩"
N,0.7659297789336801,"(a)
= −
X"
N,0.7665799739921977,"a
πθ (a|x) ⟨φ(x, a), θ′ −θ⟩+ log
X"
N,0.7672301690507152,"a
πθ (a|x) e⟨φ(x,a),θ′−θ⟩"
N,0.7678803641092328,"(b)
= R2 ∥θ −θ′∥2
2
2"
N,0.7685305591677504,"using that (a) the relation,
656 log
P"
N,0.7691807542262679,"a e⟨φ(x,a),θ′⟩
P"
N,0.7698309492847855,"a e⟨φ(x,a),θ⟩= log
X a"
N,0.770481144343303,"e⟨φ(x,a),θ′⟩
P"
N,0.7711313394018205,"a′ e⟨φ(x,a′),θ⟩· e⟨φ(x,a),θ⟩"
N,0.7717815344603381,"e⟨φ(x,a),θ⟩= log
X"
N,0.7724317295188556,"a
πθ (a|x) e⟨φ(x,a),θ′−θ⟩,"
N,0.7730819245773732,"and (b) Hoeffding’s lemma (cf. Lemma A.1 of Cesa-Bianchi and Lugosi [2006]). The final statement
657"
N,0.7737321196358907,"follows from substituting this result in Equation (18).
658"
N,0.7743823146944083,"Lemma E.3. (Self-Normalized Bound for Vector-Valued Martingales - Theorem 1 of Abbasi-Yadkori
659"
N,0.7750325097529259,"et al. [2011]) Let {Fi−1}∞
i=1 be a filtration and {ξi}∞
i=1 a real-valued stochastic process such that ξi
660"
N,0.7756827048114434,"for i = 1, · · · is zero-mean (i.e E [ξi |Fi−1 ] = 0) and conditionally s-subgaussian for s ≥0. That is,
661"
N,0.776332899869961,"for all b ∈R,
662"
N,0.7769830949284785,"E

ebξi |Fi−1

≤e
b2s2 2 ."
N,0.7776332899869961,"Also, let {φi}∞
i=1 be Fi−1-measurable. Then,
663  n
X"
N,0.7782834850455137,"i=1
φiξi  2"
N,0.7789336801040312,"(nΛn)−1
≤2s2 log"
N,0.7795838751625488,"""
det (nΛn)1/2 det (nβI)−1/2 δ # ."
N,0.7802340702210663,"Lemma E.4. (e.g. see Chapter 27 of Shalev-Shwartz and Ben-David [2014]) For all ϵ > 0,
664"
N,0.7808842652795839,"log N (Bd(r), ∥·∥∞, ϵ) ≤d log

1 + 2r ϵ 
."
N,0.7815344603381015,"Corollary E.5. Under the conditions of Lemma E.2, for all ϵ > 0,
665"
N,0.782184655396619,"log N

Π (Dπ) , ∥·∥∞,1 , ϵ

≤log N

Bd(Dπ), ∥·∥∞, ϵ R"
N,0.7828348504551366,"
≤d log

1 + 2RDπ ϵ 
."
N,0.783485045513654,"Lemma E.6. Consider the function class,
666"
N,0.7841352405721717,"V =
n
vπ,θ : X →[−RDθ, RDθ]
π ∈Π (Dπ) , θ ∈Bd(Dθ)
o
,"
N,0.7847854356306893,"we have that:
667"
N,0.7854356306892067,"N (V, ∥·∥∞, ϵ) ≤N

Π (Dπ) , ∥·∥∞,1 , ϵ/2RDθ

× N (Bd(Dθ), ∥·∥2 , ϵ/2R) ,"
N,0.7860858257477243,"and,
668"
N,0.7867360208062418,"log N (V, ∥·∥∞, ϵ) ≤2d log

1 + 4RDπRDθ ϵ "
N,0.7873862158647594,"Proof. Let Cπ denote the ϵπ-cover of Π (Dπ) with respect to the norm ∥·∥∞,1 and Cθ the ϵθ-cover
669"
N,0.788036410923277,"of Bd(Dθ) under the L2-norm. For (π, θ) ∈Π (Dπ) × Bd(Dθ) and (π′, θ′) ∈Cπ × Cθ, it follows
670"
N,0.7886866059817945,"that for any state x ∈X,
671"
N,0.7893368010403121,"|vπ,θ(s) −vπ′,θ′(s)| =  X"
N,0.7899869960988296,"a∈A
π(a|x) ⟨φ(x, a), θ⟩−π′(a|x) ⟨φ(x, a), θ′⟩  =  X"
N,0.7906371911573472,"a∈A
(π(a|x) −π′(a|x)) ⟨φ(x, a), θ⟩+
X"
N,0.7912873862158648,"a∈A
π′(a|x) ⟨φ(x, a), θ −θ′⟩ "
N,0.7919375812743823,"≤RDθ
X"
N,0.7925877763328999,"a∈A
|π(a|x) −π′(a|x)| + R
X"
N,0.7932379713914174,"a∈A
π′(a|x) ∥θ −θ′∥2"
N,0.793888166449935,"Let Cv =
n
vπ,θ : X →[−RDθ, RDθ]
π ∈Cπ, θ ∈Cθ
o
. Then, Cv is an ϵ-cover of V with respect
672"
N,0.7945383615084526,"to the L∞-norm when ϵπ = ϵ/2RDθ and ϵθ = ϵ/2R. Therefore, we can derive a bound on the
673"
N,0.7951885565669701,"covering number of Cv as:
674"
N,0.7958387516254877,"N (V, ∥·∥∞, ϵ) ≤N

Π (Dπ) , ∥·∥∞,1 , ϵ/2RDθ

× N (Bd(Dθ), ∥·∥2 , ϵ/2R)"
N,0.7964889466840052,"≤

1 + 4RDπRDθ ϵ"
N,0.7971391417425228,"d 
1 + 4RDθ ϵ d
."
N,0.7977893368010404,"Hence,
675"
N,0.7984395318595578,"log N (V, ∥·∥∞, ϵ) ≤2d log

1 + 4RDπRDθ ϵ "
N,0.7990897269180754,"This completes the proof.
676"
N,0.7997399219765929,"NeurIPS Paper Checklist
677"
CLAIMS,0.8003901170351105,"1. Claims
678"
CLAIMS,0.8010403120936281,"Question: Do the main claims made in the abstract and introduction accurately reflect the
679"
CLAIMS,0.8016905071521456,"paper’s contributions and scope?
680"
CLAIMS,0.8023407022106632,"Answer: [Yes]
681"
CLAIMS,0.8029908972691807,"Justification:
682"
CLAIMS,0.8036410923276983,"Guidelines:
683"
CLAIMS,0.8042912873862159,"• The answer NA means that the abstract and introduction do not include the claims
684"
CLAIMS,0.8049414824447334,"made in the paper.
685"
CLAIMS,0.805591677503251,"• The abstract and/or introduction should clearly state the claims made, including the
686"
CLAIMS,0.8062418725617685,"contributions made in the paper and important assumptions and limitations. A No or
687"
CLAIMS,0.8068920676202861,"NA answer to this question will not be perceived well by the reviewers.
688"
CLAIMS,0.8075422626788037,"• The claims made should match theoretical and experimental results, and reflect how
689"
CLAIMS,0.8081924577373212,"much the results can be expected to generalize to other settings.
690"
CLAIMS,0.8088426527958388,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
691"
CLAIMS,0.8094928478543563,"are not attained by the paper.
692"
LIMITATIONS,0.8101430429128739,"2. Limitations
693"
LIMITATIONS,0.8107932379713915,"Question: Does the paper discuss the limitations of the work performed by the authors?
694"
LIMITATIONS,0.811443433029909,"Answer: [Yes]
695"
LIMITATIONS,0.8120936280884266,"Justification:
696"
LIMITATIONS,0.812743823146944,"Guidelines:
697"
LIMITATIONS,0.8133940182054616,"• The answer NA means that the paper has no limitation while the answer No means that
698"
LIMITATIONS,0.8140442132639792,"the paper has limitations, but those are not discussed in the paper.
699"
LIMITATIONS,0.8146944083224967,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
700"
LIMITATIONS,0.8153446033810143,"• The paper should point out any strong assumptions and how robust the results are to
701"
LIMITATIONS,0.8159947984395318,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
702"
LIMITATIONS,0.8166449934980494,"model well-specification, asymptotic approximations only holding locally). The authors
703"
LIMITATIONS,0.817295188556567,"should reflect on how these assumptions might be violated in practice and what the
704"
LIMITATIONS,0.8179453836150845,"implications would be.
705"
LIMITATIONS,0.8185955786736021,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
706"
LIMITATIONS,0.8192457737321196,"only tested on a few datasets or with a few runs. In general, empirical results often
707"
LIMITATIONS,0.8198959687906372,"depend on implicit assumptions, which should be articulated.
708"
LIMITATIONS,0.8205461638491548,"• The authors should reflect on the factors that influence the performance of the approach.
709"
LIMITATIONS,0.8211963589076723,"For example, a facial recognition algorithm may perform poorly when image resolution
710"
LIMITATIONS,0.8218465539661899,"is low or images are taken in low lighting. Or a speech-to-text system might not be
711"
LIMITATIONS,0.8224967490247074,"used reliably to provide closed captions for online lectures because it fails to handle
712"
LIMITATIONS,0.823146944083225,"technical jargon.
713"
LIMITATIONS,0.8237971391417426,"• The authors should discuss the computational efficiency of the proposed algorithms
714"
LIMITATIONS,0.8244473342002601,"and how they scale with dataset size.
715"
LIMITATIONS,0.8250975292587777,"• If applicable, the authors should discuss possible limitations of their approach to
716"
LIMITATIONS,0.8257477243172952,"address problems of privacy and fairness.
717"
LIMITATIONS,0.8263979193758128,"• While the authors might fear that complete honesty about limitations might be used by
718"
LIMITATIONS,0.8270481144343304,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
719"
LIMITATIONS,0.8276983094928478,"limitations that aren’t acknowledged in the paper. The authors should use their best
720"
LIMITATIONS,0.8283485045513654,"judgment and recognize that individual actions in favor of transparency play an impor-
721"
LIMITATIONS,0.8289986996098829,"tant role in developing norms that preserve the integrity of the community. Reviewers
722"
LIMITATIONS,0.8296488946684005,"will be specifically instructed to not penalize honesty concerning limitations.
723"
THEORY ASSUMPTIONS AND PROOFS,0.8302990897269181,"3. Theory Assumptions and Proofs
724"
THEORY ASSUMPTIONS AND PROOFS,0.8309492847854356,"Question: For each theoretical result, does the paper provide the full set of assumptions and
725"
THEORY ASSUMPTIONS AND PROOFS,0.8315994798439532,"a complete (and correct) proof?
726"
THEORY ASSUMPTIONS AND PROOFS,0.8322496749024707,"Answer: [Yes]
727"
THEORY ASSUMPTIONS AND PROOFS,0.8328998699609883,"Justification:
728"
THEORY ASSUMPTIONS AND PROOFS,0.8335500650195059,"Guidelines:
729"
THEORY ASSUMPTIONS AND PROOFS,0.8342002600780234,"• The answer NA means that the paper does not include theoretical results.
730"
THEORY ASSUMPTIONS AND PROOFS,0.834850455136541,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
731"
THEORY ASSUMPTIONS AND PROOFS,0.8355006501950585,"referenced.
732"
THEORY ASSUMPTIONS AND PROOFS,0.8361508452535761,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
733"
THEORY ASSUMPTIONS AND PROOFS,0.8368010403120937,"• The proofs can either appear in the main paper or the supplemental material, but if
734"
THEORY ASSUMPTIONS AND PROOFS,0.8374512353706112,"they appear in the supplemental material, the authors are encouraged to provide a short
735"
THEORY ASSUMPTIONS AND PROOFS,0.8381014304291288,"proof sketch to provide intuition.
736"
THEORY ASSUMPTIONS AND PROOFS,0.8387516254876463,"• Inversely, any informal proof provided in the core of the paper should be complemented
737"
THEORY ASSUMPTIONS AND PROOFS,0.8394018205461639,"by formal proofs provided in appendix or supplemental material.
738"
THEORY ASSUMPTIONS AND PROOFS,0.8400520156046815,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
739"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.840702210663199,"4. Experimental Result Reproducibility
740"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8413524057217165,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
741"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.842002600780234,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
742"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8426527958387516,"of the paper (regardless of whether the code and data are provided or not)?
743"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8433029908972692,"Answer: [NA]
744"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8439531859557867,"Justification: This is a theory paper, there are no experiments.
745"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8446033810143043,"Guidelines:
746"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8452535760728218,"• The answer NA means that the paper does not include experiments.
747"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8459037711313394,"• If the paper includes experiments, a No answer to this question will not be perceived
748"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.846553966189857,"well by the reviewers: Making the paper reproducible is important, regardless of
749"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8472041612483745,"whether the code and data are provided or not.
750"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8478543563068921,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
751"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8485045513654096,"to make their results reproducible or verifiable.
752"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8491547464239272,"• Depending on the contribution, reproducibility can be accomplished in various ways.
753"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8498049414824448,"For example, if the contribution is a novel architecture, describing the architecture fully
754"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8504551365409623,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
755"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8511053315994799,"be necessary to either make it possible for others to replicate the model with the same
756"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8517555266579974,"dataset, or provide access to the model. In general. releasing code and data is often
757"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.852405721716515,"one good way to accomplish this, but reproducibility can also be provided via detailed
758"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8530559167750326,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
759"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.85370611183355,"of a large language model), releasing of a model checkpoint, or other means that are
760"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8543563068920677,"appropriate to the research performed.
761"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8550065019505851,"• While NeurIPS does not require releasing code, the conference does require all submis-
762"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8556566970091027,"sions to provide some reasonable avenue for reproducibility, which may depend on the
763"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8563068920676203,"nature of the contribution. For example
764"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8569570871261378,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
765"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8576072821846554,"to reproduce that algorithm.
766"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8582574772431729,"(b) If the contribution is primarily a new model architecture, the paper should describe
767"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8589076723016905,"the architecture clearly and fully.
768"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8595578673602081,"(c) If the contribution is a new model (e.g., a large language model), then there should
769"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8602080624187256,"either be a way to access this model for reproducing the results or a way to reproduce
770"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8608582574772432,"the model (e.g., with an open-source dataset or instructions for how to construct
771"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8615084525357607,"the dataset).
772"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8621586475942783,"(d) We recognize that reproducibility may be tricky in some cases, in which case
773"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8628088426527959,"authors are welcome to describe the particular way they provide for reproducibility.
774"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8634590377113134,"In the case of closed-source models, it may be that access to the model is limited in
775"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.864109232769831,"some way (e.g., to registered users), but it should be possible for other researchers
776"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8647594278283485,"to have some path to reproducing or verifying the results.
777"
OPEN ACCESS TO DATA AND CODE,0.8654096228868661,"5. Open access to data and code
778"
OPEN ACCESS TO DATA AND CODE,0.8660598179453837,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
779"
OPEN ACCESS TO DATA AND CODE,0.8667100130039012,"tions to faithfully reproduce the main experimental results, as described in supplemental
780"
OPEN ACCESS TO DATA AND CODE,0.8673602080624188,"material?
781"
OPEN ACCESS TO DATA AND CODE,0.8680104031209362,"Answer: [NA]
782"
OPEN ACCESS TO DATA AND CODE,0.8686605981794538,"Justification: This is a theory paper, there is no data or code.
783"
OPEN ACCESS TO DATA AND CODE,0.8693107932379714,"Guidelines:
784"
OPEN ACCESS TO DATA AND CODE,0.8699609882964889,"• The answer NA means that paper does not include experiments requiring code.
785"
OPEN ACCESS TO DATA AND CODE,0.8706111833550065,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
786"
OPEN ACCESS TO DATA AND CODE,0.871261378413524,"public/guides/CodeSubmissionPolicy) for more details.
787"
OPEN ACCESS TO DATA AND CODE,0.8719115734720416,"• While we encourage the release of code and data, we understand that this might not be
788"
OPEN ACCESS TO DATA AND CODE,0.8725617685305592,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
789"
OPEN ACCESS TO DATA AND CODE,0.8732119635890767,"including code, unless this is central to the contribution (e.g., for a new open-source
790"
OPEN ACCESS TO DATA AND CODE,0.8738621586475943,"benchmark).
791"
OPEN ACCESS TO DATA AND CODE,0.8745123537061118,"• The instructions should contain the exact command and environment needed to run to
792"
OPEN ACCESS TO DATA AND CODE,0.8751625487646294,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
793"
OPEN ACCESS TO DATA AND CODE,0.8758127438231469,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
794"
OPEN ACCESS TO DATA AND CODE,0.8764629388816645,"• The authors should provide instructions on data access and preparation, including how
795"
OPEN ACCESS TO DATA AND CODE,0.8771131339401821,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
796"
OPEN ACCESS TO DATA AND CODE,0.8777633289986996,"• The authors should provide scripts to reproduce all experimental results for the new
797"
OPEN ACCESS TO DATA AND CODE,0.8784135240572172,"proposed method and baselines. If only a subset of experiments are reproducible, they
798"
OPEN ACCESS TO DATA AND CODE,0.8790637191157347,"should state which ones are omitted from the script and why.
799"
OPEN ACCESS TO DATA AND CODE,0.8797139141742523,"• At submission time, to preserve anonymity, the authors should release anonymized
800"
OPEN ACCESS TO DATA AND CODE,0.8803641092327699,"versions (if applicable).
801"
OPEN ACCESS TO DATA AND CODE,0.8810143042912874,"• Providing as much information as possible in supplemental material (appended to the
802"
OPEN ACCESS TO DATA AND CODE,0.881664499349805,"paper) is recommended, but including URLs to data and code is permitted.
803"
OPEN ACCESS TO DATA AND CODE,0.8823146944083224,"6. Experimental Setting/Details
804"
OPEN ACCESS TO DATA AND CODE,0.88296488946684,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
805"
OPEN ACCESS TO DATA AND CODE,0.8836150845253576,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
806"
OPEN ACCESS TO DATA AND CODE,0.8842652795838751,"results?
807"
OPEN ACCESS TO DATA AND CODE,0.8849154746423927,"Answer: [NA]
808"
OPEN ACCESS TO DATA AND CODE,0.8855656697009102,"Justification: This is a theory paper, there are no experiments.
809"
OPEN ACCESS TO DATA AND CODE,0.8862158647594278,"Guidelines:
810"
OPEN ACCESS TO DATA AND CODE,0.8868660598179454,"• The answer NA means that the paper does not include experiments.
811"
OPEN ACCESS TO DATA AND CODE,0.8875162548764629,"• The experimental setting should be presented in the core of the paper to a level of detail
812"
OPEN ACCESS TO DATA AND CODE,0.8881664499349805,"that is necessary to appreciate the results and make sense of them.
813"
OPEN ACCESS TO DATA AND CODE,0.888816644993498,"• The full details can be provided either with the code, in appendix, or as supplemental
814"
OPEN ACCESS TO DATA AND CODE,0.8894668400520156,"material.
815"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8901170351105332,"7. Experiment Statistical Significance
816"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8907672301690507,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
817"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8914174252275683,"information about the statistical significance of the experiments?
818"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8920676202860858,"Answer: [NA]
819"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8927178153446034,"Justification: This is a theory paper, there are no experiments.
820"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.893368010403121,"Guidelines:
821"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8940182054616385,"• The answer NA means that the paper does not include experiments.
822"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8946684005201561,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
823"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8953185955786735,"dence intervals, or statistical significance tests, at least for the experiments that support
824"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8959687906371911,"the main claims of the paper.
825"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8966189856957087,"• The factors of variability that the error bars are capturing should be clearly stated (for
826"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8972691807542262,"example, train/test split, initialization, random drawing of some parameter, or overall
827"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8979193758127438,"run with given experimental conditions).
828"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8985695708712613,"• The method for calculating the error bars should be explained (closed form formula,
829"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8992197659297789,"call to a library function, bootstrap, etc.)
830"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8998699609882965,"• The assumptions made should be given (e.g., Normally distributed errors).
831"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.900520156046814,"• It should be clear whether the error bar is the standard deviation or the standard error
832"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9011703511053316,"of the mean.
833"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9018205461638491,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
834"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9024707412223667,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
835"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9031209362808843,"of Normality of errors is not verified.
836"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9037711313394018,"• For asymmetric distributions, the authors should be careful not to show in tables or
837"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9044213263979194,"figures symmetric error bars that would yield results that are out of range (e.g. negative
838"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9050715214564369,"error rates).
839"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9057217165149545,"• If error bars are reported in tables or plots, The authors should explain in the text how
840"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9063719115734721,"they were calculated and reference the corresponding figures or tables in the text.
841"
EXPERIMENTS COMPUTE RESOURCES,0.9070221066319896,"8. Experiments Compute Resources
842"
EXPERIMENTS COMPUTE RESOURCES,0.9076723016905072,"Question: For each experiment, does the paper provide sufficient information on the com-
843"
EXPERIMENTS COMPUTE RESOURCES,0.9083224967490247,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
844"
EXPERIMENTS COMPUTE RESOURCES,0.9089726918075423,"the experiments?
845"
EXPERIMENTS COMPUTE RESOURCES,0.9096228868660599,"Answer: [NA]
846"
EXPERIMENTS COMPUTE RESOURCES,0.9102730819245773,"Justification: This is a theory paper, there are no experiments.
847"
EXPERIMENTS COMPUTE RESOURCES,0.9109232769830949,"Guidelines:
848"
EXPERIMENTS COMPUTE RESOURCES,0.9115734720416124,"• The answer NA means that the paper does not include experiments.
849"
EXPERIMENTS COMPUTE RESOURCES,0.91222366710013,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
850"
EXPERIMENTS COMPUTE RESOURCES,0.9128738621586476,"or cloud provider, including relevant memory and storage.
851"
EXPERIMENTS COMPUTE RESOURCES,0.9135240572171651,"• The paper should provide the amount of compute required for each of the individual
852"
EXPERIMENTS COMPUTE RESOURCES,0.9141742522756827,"experimental runs as well as estimate the total compute.
853"
EXPERIMENTS COMPUTE RESOURCES,0.9148244473342002,"• The paper should disclose whether the full research project required more compute
854"
EXPERIMENTS COMPUTE RESOURCES,0.9154746423927178,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
855"
EXPERIMENTS COMPUTE RESOURCES,0.9161248374512354,"didn’t make it into the paper).
856"
CODE OF ETHICS,0.9167750325097529,"9. Code Of Ethics
857"
CODE OF ETHICS,0.9174252275682705,"Question: Does the research conducted in the paper conform, in every respect, with the
858"
CODE OF ETHICS,0.918075422626788,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
859"
CODE OF ETHICS,0.9187256176853056,"Answer: [Yes]
860"
CODE OF ETHICS,0.9193758127438232,"Justification:
861"
CODE OF ETHICS,0.9200260078023407,"Guidelines:
862"
CODE OF ETHICS,0.9206762028608583,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
863"
CODE OF ETHICS,0.9213263979193758,"• If the authors answer No, they should explain the special circumstances that require a
864"
CODE OF ETHICS,0.9219765929778934,"deviation from the Code of Ethics.
865"
CODE OF ETHICS,0.922626788036411,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
866"
CODE OF ETHICS,0.9232769830949284,"eration due to laws or regulations in their jurisdiction).
867"
BROADER IMPACTS,0.923927178153446,"10. Broader Impacts
868"
BROADER IMPACTS,0.9245773732119635,"Question: Does the paper discuss both potential positive societal impacts and negative
869"
BROADER IMPACTS,0.9252275682704811,"societal impacts of the work performed?
870"
BROADER IMPACTS,0.9258777633289987,"Answer: [NA]
871"
BROADER IMPACTS,0.9265279583875162,"Justification: This is a theory paper, with no specific societal impacts foreseen in the near
872"
BROADER IMPACTS,0.9271781534460338,"future.
873"
BROADER IMPACTS,0.9278283485045513,"Guidelines:
874"
BROADER IMPACTS,0.9284785435630689,"• The answer NA means that there is no societal impact of the work performed.
875"
BROADER IMPACTS,0.9291287386215865,"• If the authors answer NA or No, they should explain why their work has no societal
876"
BROADER IMPACTS,0.929778933680104,"impact or why the paper does not address societal impact.
877"
BROADER IMPACTS,0.9304291287386216,"• Examples of negative societal impacts include potential malicious or unintended uses
878"
BROADER IMPACTS,0.9310793237971391,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
879"
BROADER IMPACTS,0.9317295188556567,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
880"
BROADER IMPACTS,0.9323797139141743,"groups), privacy considerations, and security considerations.
881"
BROADER IMPACTS,0.9330299089726918,"• The conference expects that many papers will be foundational research and not tied
882"
BROADER IMPACTS,0.9336801040312094,"to particular applications, let alone deployments. However, if there is a direct path to
883"
BROADER IMPACTS,0.9343302990897269,"any negative applications, the authors should point it out. For example, it is legitimate
884"
BROADER IMPACTS,0.9349804941482445,"to point out that an improvement in the quality of generative models could be used to
885"
BROADER IMPACTS,0.9356306892067621,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
886"
BROADER IMPACTS,0.9362808842652796,"that a generic algorithm for optimizing neural networks could enable people to train
887"
BROADER IMPACTS,0.9369310793237972,"models that generate Deepfakes faster.
888"
BROADER IMPACTS,0.9375812743823146,"• The authors should consider possible harms that could arise when the technology is
889"
BROADER IMPACTS,0.9382314694408322,"being used as intended and functioning correctly, harms that could arise when the
890"
BROADER IMPACTS,0.9388816644993498,"technology is being used as intended but gives incorrect results, and harms following
891"
BROADER IMPACTS,0.9395318595578673,"from (intentional or unintentional) misuse of the technology.
892"
BROADER IMPACTS,0.9401820546163849,"• If there are negative societal impacts, the authors could also discuss possible mitigation
893"
BROADER IMPACTS,0.9408322496749024,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
894"
BROADER IMPACTS,0.94148244473342,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
895"
BROADER IMPACTS,0.9421326397919376,"feedback over time, improving the efficiency and accessibility of ML).
896"
SAFEGUARDS,0.9427828348504551,"11. Safeguards
897"
SAFEGUARDS,0.9434330299089727,"Question: Does the paper describe safeguards that have been put in place for responsible
898"
SAFEGUARDS,0.9440832249674902,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
899"
SAFEGUARDS,0.9447334200260078,"image generators, or scraped datasets)?
900"
SAFEGUARDS,0.9453836150845254,"Answer: [NA]
901"
SAFEGUARDS,0.9460338101430429,"Justification: This is a theory paper, this question is not relevant.
902"
SAFEGUARDS,0.9466840052015605,"Guidelines:
903"
SAFEGUARDS,0.947334200260078,"• The answer NA means that the paper poses no such risks.
904"
SAFEGUARDS,0.9479843953185956,"• Released models that have a high risk for misuse or dual-use should be released with
905"
SAFEGUARDS,0.9486345903771132,"necessary safeguards to allow for controlled use of the model, for example by requiring
906"
SAFEGUARDS,0.9492847854356307,"that users adhere to usage guidelines or restrictions to access the model or implementing
907"
SAFEGUARDS,0.9499349804941483,"safety filters.
908"
SAFEGUARDS,0.9505851755526658,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
909"
SAFEGUARDS,0.9512353706111834,"should describe how they avoided releasing unsafe images.
910"
SAFEGUARDS,0.951885565669701,"• We recognize that providing effective safeguards is challenging, and many papers do
911"
SAFEGUARDS,0.9525357607282184,"not require this, but we encourage authors to take this into account and make a best
912"
SAFEGUARDS,0.953185955786736,"faith effort.
913"
LICENSES FOR EXISTING ASSETS,0.9538361508452535,"12. Licenses for existing assets
914"
LICENSES FOR EXISTING ASSETS,0.9544863459037711,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
915"
LICENSES FOR EXISTING ASSETS,0.9551365409622887,"the paper, properly credited and are the license and terms of use explicitly mentioned and
916"
LICENSES FOR EXISTING ASSETS,0.9557867360208062,"properly respected?
917"
LICENSES FOR EXISTING ASSETS,0.9564369310793238,"Answer: [NA]
918"
LICENSES FOR EXISTING ASSETS,0.9570871261378413,"Justification: This is a theory paper, there are no assets used.
919"
LICENSES FOR EXISTING ASSETS,0.9577373211963589,"Guidelines:
920"
LICENSES FOR EXISTING ASSETS,0.9583875162548765,"• The answer NA means that the paper does not use existing assets.
921"
LICENSES FOR EXISTING ASSETS,0.959037711313394,"• The authors should cite the original paper that produced the code package or dataset.
922"
LICENSES FOR EXISTING ASSETS,0.9596879063719116,"• The authors should state which version of the asset is used and, if possible, include a
923"
LICENSES FOR EXISTING ASSETS,0.9603381014304291,"URL.
924"
LICENSES FOR EXISTING ASSETS,0.9609882964889467,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
925"
LICENSES FOR EXISTING ASSETS,0.9616384915474643,"• For scraped data from a particular source (e.g., website), the copyright and terms of
926"
LICENSES FOR EXISTING ASSETS,0.9622886866059818,"service of that source should be provided.
927"
LICENSES FOR EXISTING ASSETS,0.9629388816644994,"• If assets are released, the license, copyright information, and terms of use in the
928"
LICENSES FOR EXISTING ASSETS,0.9635890767230169,"package should be provided. For popular datasets, paperswithcode.com/datasets
929"
LICENSES FOR EXISTING ASSETS,0.9642392717815345,"has curated licenses for some datasets. Their licensing guide can help determine the
930"
LICENSES FOR EXISTING ASSETS,0.9648894668400521,"license of a dataset.
931"
LICENSES FOR EXISTING ASSETS,0.9655396618985695,"• For existing datasets that are re-packaged, both the original license and the license of
932"
LICENSES FOR EXISTING ASSETS,0.9661898569570871,"the derived asset (if it has changed) should be provided.
933"
LICENSES FOR EXISTING ASSETS,0.9668400520156046,"• If this information is not available online, the authors are encouraged to reach out to
934"
LICENSES FOR EXISTING ASSETS,0.9674902470741222,"the asset’s creators.
935"
NEW ASSETS,0.9681404421326398,"13. New Assets
936"
NEW ASSETS,0.9687906371911573,"Question: Are new assets introduced in the paper well documented and is the documentation
937"
NEW ASSETS,0.9694408322496749,"provided alongside the assets?
938"
NEW ASSETS,0.9700910273081924,"Answer: [NA]
939"
NEW ASSETS,0.97074122236671,"Justification: This is a theory paper, there are no new assets.
940"
NEW ASSETS,0.9713914174252276,"Guidelines:
941"
NEW ASSETS,0.9720416124837451,"• The answer NA means that the paper does not release new assets.
942"
NEW ASSETS,0.9726918075422627,"• Researchers should communicate the details of the dataset/code/model as part of their
943"
NEW ASSETS,0.9733420026007802,"submissions via structured templates. This includes details about training, license,
944"
NEW ASSETS,0.9739921976592978,"limitations, etc.
945"
NEW ASSETS,0.9746423927178154,"• The paper should discuss whether and how consent was obtained from people whose
946"
NEW ASSETS,0.9752925877763329,"asset is used.
947"
NEW ASSETS,0.9759427828348505,"• At submission time, remember to anonymize your assets (if applicable). You can either
948"
NEW ASSETS,0.976592977893368,"create an anonymized URL or include an anonymized zip file.
949"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9772431729518856,"14. Crowdsourcing and Research with Human Subjects
950"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9778933680104032,"Question: For crowdsourcing experiments and research with human subjects, does the paper
951"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9785435630689207,"include the full text of instructions given to participants and screenshots, if applicable, as
952"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9791937581274383,"well as details about compensation (if any)?
953"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9798439531859557,"Answer: [NA]
954"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9804941482444733,"Justification: This is a theory paper, we haven’t used crowdsourced data or worked with
955"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9811443433029909,"human subjects.
956"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9817945383615084,"Guidelines:
957"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.982444733420026,"• The answer NA means that the paper does not involve crowdsourcing nor research with
958"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9830949284785435,"human subjects.
959"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9837451235370611,"• Including this information in the supplemental material is fine, but if the main contribu-
960"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9843953185955787,"tion of the paper involves human subjects, then as much detail as possible should be
961"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9850455136540962,"included in the main paper.
962"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856957087126138,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
963"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9863459037711313,"or other labor should be paid at least the minimum wage in the country of the data
964"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9869960988296489,"collector.
965"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9876462938881665,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
966"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988296488946684,"Subjects
967"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9889466840052016,"Question: Does the paper describe potential risks incurred by study participants, whether
968"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9895968790637191,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
969"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902470741222367,"approvals (or an equivalent approval/review based on the requirements of your country or
970"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9908972691807543,"institution) were obtained?
971"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9915474642392718,"Answer: [NA]
972"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9921976592977894,"Justification: This is a theory paper, we haven’t worked with human subjects.
973"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9928478543563068,"Guidelines:
974"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934980494148244,"• The answer NA means that the paper does not involve crowdsourcing nor research with
975"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.994148244473342,"human subjects.
976"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947984395318595,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
977"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9954486345903771,"may be required for any human subjects research. If you obtained IRB approval, you
978"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9960988296488946,"should clearly state this in the paper.
979"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967490247074122,"• We recognize that the procedures for this may vary significantly between institutions
980"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9973992197659298,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
981"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980494148244473,"guidelines for their institution.
982"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9986996098829649,"• For initial submissions, do not include any information that would break anonymity (if
983"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9993498049414824,"applicable), such as the institution conducting the review.
984"
