Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.000942507068803016,"While there has been a large body of research attempting to circumvent tokenization
for language modeling (Clark et al., 2022; Xue et al., 2022), the current consen-
sus is that it is a necessary initial step for designing state-of-the-art performant
language models. In this paper, we investigate tokenization from a theoretical
point of view by studying the behavior of transformers on simple data generating
processes. When trained on data drawn from certain simple kth-order Markov
processes for k > 1, transformers exhibit a surprising phenomenon - in the ab-
sence of tokenization, they empirically are incredibly slow or fail to learn the right
distribution and predict characters according to a unigram model (Makkuva et al.,
2024). With the addition of tokenization, however, we empirically observe that
transformers break through this barrier and are able to model the probabilities of
sequences drawn from the source near-optimally, achieving small cross-entropy
loss. With this observation as starting point, we study the end-to-end cross-entropy
loss achieved by transformers with and without tokenization. With the appropriate
tokenization, we show that even the simplest unigram models (over tokens) learnt
by transformers are able to model the probability of sequences drawn from kth-
order Markov sources near optimally. Our analysis provides a justification for the
use of tokenization in practice through studying the behavior of transformers on
Markovian data."
INTRODUCTION,0.001885014137606032,"1
Introduction"
INTRODUCTION,0.002827521206409048,"The training of language models is typically not an end-to-end process. Language models are often
composed of a “tokenizer”, which encodes a sequence of characters into a sequence of token ids,
which map to substrings. The subsequent language modeling task is carried out by a neural network
or transformer, which is pre-trained and fine-tuned on large datasets. The ideal goal is to jointly
train the tokenizer and transformer with end-to-end accuracy as the objective. This is a challenging
problem to solve efficiently, and thus, the tokenizer is generally adapted on a portion of the training
dataset and frozen before the transformer is trained. In practice, byte-level/character level models
such as ByT5 (Xue et al., 2022) and CANINE (Clark et al., 2022) which avoid tokenization often
perform worse for the reason that semantic relationships can be harder to capture at the character
level (Libovick`y et al., 2021; Itzhak and Levy, 2021)."
INTRODUCTION,0.003770028275212064,"Though used most commonly, tokenization at the subword level often has sharp edges. Test sequences
may contain rare tokens which were never seen in the training dataset. The presence of such tokens
may induce undesirable behavior in the outputs of models (Rumbelow and Watkins, 2023; Kharitonov
et al., 2021; Yu et al., 2021) and present an attack surface for bad actors. Moreover, tokenized
models struggle on tasks that involve manipulation at the character level, such as spelling out words
or reversing sentences. For similar reasons, LLMs with standard tokenizers also struggle to carry
out basic arithmetic (Golkar et al., 2023). Despite this brittleness, tokenization is used in nearly all
state-of-the-art LLM architectures."
INTRODUCTION,0.00471253534401508,"In this paper, we introduce a statistical formulation for tokenization for next-word-prediction. We
study the class of models transformers are observed to express empirically under simple data generat-
ing processes, which often can have simpler descriptions. Taking a step back, rather than focusing
on proxy evaluation metrics, which lead to an ever-changing goalpost, we focus on understanding
the behavior of the end-to-end cross-entropy loss, L(·). In this paper, we study a simplification of
real world data generating processes and study the case where data sources are kth-order Markov
processes. Within this framework we can compare tokenizers against each other, and in the process
capture several interesting phenomena. Our main results are as follows,"
THERE ARE VERY SIMPLE KTH-ORDER MARKOV PROCESSES SUCH THAT IN THE ABSENCE OF ANY TOKENIZA-,0.005655042412818096,"1. There are very simple kth-order Markov processes such that in the absence of any tokeniza-
tion, transformers trained on data drawn this source empirically predict characters according
to a unigram model. This phenomenon is observed under a wide variety of hyperparameter
choices. This is problematic because unigram models such as that induced by the stationary
distribution are poor at modeling Markovian data and suffer from a high cross-entropy loss.
This phenomenon was also recently observed in Makkuva et al. (2024)."
THERE ARE VERY SIMPLE KTH-ORDER MARKOV PROCESSES SUCH THAT IN THE ABSENCE OF ANY TOKENIZA-,0.006597549481621112,"2. When trained with tokenization, transformers are empirically observed to break through this
barrier and are able to capture the probability of sequences under the Markov distribution
near-optimally. In other words, in the presence of tokenization, transformers appear to
achieve near-optimal cross-entropy loss. This phenomenon is observed with a multitude of
tokenizers used commonly in practice."
WE ANALYZE A TOY TOKENIZER WHICH ADDS ALL LENGTH-K SEQUENCES INTO THE DICTIONARY AND SHOW,0.007540056550424128,"3. We analyze a toy tokenizer which adds all length-k sequences into the dictionary and show
that as dictionary size grows, unigram models trained on the tokens get better at modeling
the probabilities of sequences drawn from Markov sources. We then theoretically prove
that tokenizers used in practice, such as the LZW tokenizer (Zouhar et al., 2023a) and a
variant of the BPE tokenizer (Gage, 1994; Sennrich et al., 2016) which are learnt from
data also satisfy this property but require much smaller dictionaries to achieve any target
cross-entropy loss."
WE ANALYZE A TOY TOKENIZER WHICH ADDS ALL LENGTH-K SEQUENCES INTO THE DICTIONARY AND SHOW,0.008482563619227144,"In our framework, the most challenging hurdle and the biggest departure from previous work such as
(Zouhar et al., 2023b) is the element of generalization - understanding how a tokenizer performs on
new sequences that it was not trained on. This generalization turns out to be a delicate phenomenon -
we show in Appendix D that there exist tokenizers which generalize poorly in the sense that they may
compress the dataset they are trained on into a short sequence of tokens, but fail to generalize to new
sequences. In Appendix E we show that there exist dictionaries which generalize well (in the sense
of having low cross-entropy loss) to new sequences under one encoding algorithm, but completely
fail to generalize under another."
RELATED WORK,0.00942507068803016,"1.1
Related Work"
RELATED WORK,0.010367577756833177,"Tokenization has a long history of empirical study in natural language processing. In the literature,
a number of tokenizers have been developed for various domains such as math (Singh and Strouse,
2024), code (Zheng et al., 2023; Parr, 2013) and morphology-aware tokenizers for different languages
like Japanese (Tolmachev et al., 2018; Den et al., 2007) and Arabic (Alyafeai et al., 2023) among
many others. In modern LLMs, the most commonly used tokenizers are variants of BPE (Gage,
1994), Wordpiece (Schuster and Nakajima, 2012) and the Unigram tokenizer (Kudo, 2018) which
learn a dictionary from data, rather than hard-coding language dependent rules. There has been a
long line of work interpreting tokenization from various lenses (Grefenstette and Tapanainen, 1994;
Palmer, 2000; Zouhar et al., 2023b)."
RELATED WORK,0.011310084825636193,"The theoretical study of transformers has also received much attention recently. We discuss the closest
relatives to our work below. Edelman et al. (2024) study the learning trajectory of transformers
trained on data drawn from 1st-order Markov chains. While the authors empirically observe that
the models eventually learn to predict tokens correctly according to the Markov kernel, simplicity
bias slows down optimization - the models initially predict tokens according to a unigram model (in
context unigrams), which delays learning the optimal solution. This phenomenon was also observed
in Makkuva et al. (2024). On the positive side, Nichani et al. (2024) study an in-context causal
learning task that generalizes learning in-context bigrams for 1st-order Markov processes and analyze
the trajectory of gradient descent. 0
1 p q"
RELATED WORK,0.012252591894439209,"Figure 1: 2-state switching process. The above state diagram describes the distribution of Xn
conditioned on Xn−1. kth-order extension: the conditional probability of Xn only depends on Xn−k
through the kernel, Pr(Xn = 1|Xn−k = 0) = p and Pr(Xn = 0|Xn−k = 1) = q."
RELATED WORK,0.013195098963242224,"Notation.
All logarithms are base e, unless specified otherwise. The Shannon entropy H(X) of
a categorical random variable X is −P"
RELATED WORK,0.01413760603204524,"x∈supp(X) p(x) log p(x). HBER(p) captures the entropy of a
Bernoulli random variable with parameter p. The notation Op,q,r(f(n)) (likewise Ω{·} and Θ{·})
indicate that the underlying constant depends polynomially on the parameters p, q and r and eO(f(n))
(likewise, eΘ and eΩ) ignores polylog(n) terms. For a set S, S⋆= ∪∞
k=1Sk, the set of all sequences
with elements drawn from S. For a sequence t, ti:j = (ti, ti+1, · · · , tj) returns a slice."
FORMULATION,0.015080113100848256,"2
Formulation"
FORMULATION,0.016022620169651274,"We consider a setting where the learner’s objective is to learn a language model which models
probabilities of sequences over an input alphabet A. The data to be modeled is generated according
to an unknown probability model P : A⋆→[0, 1] over strings. A tokenizer is a tuple T =
(Dict, enc(·), dec(·)). Here Dict is a collection of tokens The encoding function enc(·) : A⋆→
Dict⋆, maps strings of characters to a sequence of tokens, and likewise, the decoding function
dec(·) : Dict⋆→A⋆maps a sequence of tokens to a string of characters. We assume that the
tokenizer is “consistent”, namely, dec(enc(·)) is the identity function."
FORMULATION,0.016965127238454288,"We consider a setting where the learner has access to a training dataset which is a sequence of length
n sampled from a data source1. We study the likelihood maximization problem, where the objective
of the learner is to learn an end to end model such that the cross-entropy loss is minimized. In
the presence of tokenization, we have a model of the form Qend = Q ◦enc(·) where Q is a joint
distribution across sequences of tokens when the tokenizer corresponding to enc(·) is used. The
cross-entropy loss, i.e. the log-perplexity, can be written down as,"
FORMULATION,0.017907634307257305,"Lm(Qend) ≜−E[log Q(enc(s))],
(1)"
FORMULATION,0.01885014137606032,"with the objective to minimize it. Here, the expectation is over s, a fresh test sequence of length
m sampled from the data generating process. Fixing a tokenizer, let Q denote a family of joint
distributions over tokens (i.e. likelihood models). The objective is to jointly design a tokenizer (with
encoding function enc(·)) and likelihood model Q ∈Q with small test loss Lm(Q ◦enc(·))."
FORMULATION,0.019792648444863337,"Finally, for a dictionary Dict, the unigram family of models, Q1-gram, is defined as below: Q ∈Q1-gram
associates probability Q(t1, t2, · · · , tj) = Q#(j) Qj
i=1 Qtok(ti) to the sequence of tokens t1, · · · , tj
for measures Q# and Qtok supported on N and Dict respectively."
DATA GENERATING PROCESS,0.020735155513666354,"2.1
Data generating process"
DATA GENERATING PROCESS,0.021677662582469368,"In this paper, we consider a simplification of real-world data generating processes by considering the
case where the data generating distribution is a kth-order Markov process over characters. Studying
the behavior of transformers trained on Markov data was the subject of the works Makkuva et al.
(2024) and Edelman et al. (2024), where a number of interesting phenomena were unearthed. When
a transformer is trained on data from certain simple Markov processes like the one considered in
Figure 1, a very peculiar phenomenon occurs - within a reasonably large number of iterations, the
transformer fails to improve beyond the loss incurred by the best unigram model. This phenomenon
is reproducible across a wide number of hyperparameters, including the number of feed-forward
layers in the model, the embedding dimension, and the number of attention heads. In Figure 3a this is
made clearer - the transformer fails to improve its test loss beyond that of the best unigram model."
DATA GENERATING PROCESS,0.022620169651272386,1This can be thought of as the concatenation of all the individual sequences in the training dataset.
DATA GENERATING PROCESS,0.0235626767200754,"0
50
100
150
200
250
300
350
400
450
500
Sequence length"
DATA GENERATING PROCESS,0.024505183788878417,"0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19"
DATA GENERATING PROCESS,0.02544769085768143,Token id 0.2 0.4 0.6
DATA GENERATING PROCESS,0.02639019792648445,"Figure 2: Token distribution returned by the transformer tokenized by a learnt BPE encoder with a
dictionary size of 20. A test sequence is generated from the stochastic source and encoded into a
token sequence t. Each narrow vertical column represents the distribution over next tokens returned
by the transformer when the first x tokens of t are fed into the model, where x is varied from 0 to the
length of t. For most values of x, the model appears to predict the same distribution over the next
token."
DATA GENERATING PROCESS,0.027332704995287466,"How bad can a unigram model be? It turns out that the gap between the cross-entropy of the best
unigram model and that of the optimal model can be characterized precisely."
DATA GENERATING PROCESS,0.02827521206409048,"Theorem 2.1. Consider any ergodic data source with stationary distribution over characters π. The
unconstrained optimal likelihood model achieves cross-entropy loss, minQ Lm(Q) = H(P). In
contrast, the cross-entropy loss under any unigram model Q ∈Q1-gram satisfies, Lm(Q) ≥mH(π)."
DATA GENERATING PROCESS,0.029217719132893498,"The ratio of the optimal loss H(P), and the optimal unigram loss, mH(π) can be arbitrarily large. In
particular, for the switching chain in Figure 1, as p, q →0, the ratio diverges to ∞."
DATA GENERATING PROCESS,0.030160226201696512,"While transformers are a powerful class of models, it is concerning that they fail to learn very simple
distributions such as kth-order Markov processes. Why do they work so well in practice if they can
be so slow to learn Markovian data? It turns out that there is a simple missing ingredient in all the
architectures considered so far: tokenization. All the models trained in Figure 3a operate on raw
character sequences drawn from the stochastic source. To understand the role of tokenization, we
run another experiment and train the transformer on sequences generated from the stochastic source
which are encoded into tokens by a BPE tokenizer learnt from data. The transformer now operates on
sequences of tokens, rather than sequences of individual symbols. In Figure 3b we plot the results
of this experiment - in the presence of tokenization, the cross-entropy loss of the end-to-end model
breaks past the unigram barrier and approaches the optimal bound within a small number of iterations."
DATA GENERATING PROCESS,0.03110273327049953,"Let’s peek into the model a bit more and understand its behavior. In Figure 2 we run the following
experiment: we sample a random sequence of length 2000 from a Markov chain and feed it into the
transformer after tokenization, resulting in ≈500 tokens. We plot the next-token distribution predicted
by the transformer at every single position in the input, generated by autoregressive masking. In
Figure 2 we stitch together these next-token distributions, each of which is a narrow column heatmap.
Visually, we observe that the plot is approximately homogeneous along the x-axis, implying that
the next-token distribution learned does not depend strongly on the prefix at that position. Thus the
transformer learns what is essentially a unigram model."
DATA GENERATING PROCESS,0.03204524033930255,"Thus, we come to a surprising conclusion: the behavior of the transformer on the kth-order switching
source in Figure 1 with and without tokenization is essentially the same. In both cases, the model
learns a unigram model over the tokens - in the absence of tokenization this unigram model is in
fact the stationary distribution induced by the source. If the transformer learns a unigram model in
both cases, how come there is such a large gap in performance between the two? To understand this
in more detail, we analyze a toy tokenizer. As a simplification, we will analyze the behavior of an
arbitrary, but exact unigram model under this tokenizer."
UNIGRAM MODELS UNDER TOKENIZATION,0.03298774740810556,"3
Unigram models under tokenization"
UNIGRAM MODELS UNDER TOKENIZATION,0.033930254476908575,"Let’s consider a toy tokenizer which assigns all possible substrings of length r as tokens in the
dictionary and study what happens when a unigram model is trained on the tokenized sequences. The"
UNIGRAM MODELS UNDER TOKENIZATION,0.034872761545711596,"0
25
50
75
100
125
150
175
Iteration 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25"
UNIGRAM MODELS UNDER TOKENIZATION,0.03581526861451461,Test loss L
UNIGRAM MODELS UNDER TOKENIZATION,0.036757775683317624,"Best transformer model
Optimal cross-entropy
Cross-entropy of best unigram model"
UNIGRAM MODELS UNDER TOKENIZATION,0.03770028275212064,"(a) The loss of the transformer fails to converge to the
optimal cross-entropy loss (dashed line) and instead
converges to that of the best unigram model (dotted
line). The shaded blue region captures how the test
loss curves vary as hyperparameters (number of layers,
embedding dimension etc.) are changed."
UNIGRAM MODELS UNDER TOKENIZATION,0.03864278982092366,"0
50
100
150
200
250
300
Iteration 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90"
UNIGRAM MODELS UNDER TOKENIZATION,0.03958529688972667,Test loss L
UNIGRAM MODELS UNDER TOKENIZATION,0.04052780395852969,"layers = 1
layers = 2
layers = 3
layers = 4
Optimal cross-entropy
Cross-entropy of best unigram model"
UNIGRAM MODELS UNDER TOKENIZATION,0.04147031102733271,"(b) In the presence of tokenization, the test loss of the
model approaches the optimal bound (dashed line). It
is worth noting that the models trained here are sig-
nificantly smaller than those considered in Figure 3a,
having up to 70× fewer parameters and yet are able to
achieve the optimal cross-entropy loss."
UNIGRAM MODELS UNDER TOKENIZATION,0.04241281809613572,"Figure 3: Transformers trained on the order-2 switching Markov process (Figure 1) with p = q = 0.8.
On the left we have the model trained without tokenization and on the right the model uses BPE with
a dictionary of size 10 learnt from data."
UNIGRAM MODELS UNDER TOKENIZATION,0.043355325164938736,"total dictionary size d = 2r. A sequence of characters is mapped to a sequence of tokens by simply
chunking it into a sequences of r characters which are replaced by the corresponding token index2.
The resulting stochastic process on the tokens is still Markovian, but over a state space of size 2r. For
any unigram model Q on the tokens, the cross-entropy loss can be written down as,"
UNIGRAM MODELS UNDER TOKENIZATION,0.04429783223374175,"Lm(Q ◦enc(·)) = E
X"
UNIGRAM MODELS UNDER TOKENIZATION,0.04524033930254477,"t∈enc(s) log(1/Qtok(t))

+ Θ(log(m)),"
UNIGRAM MODELS UNDER TOKENIZATION,0.046182846371347785,"where we choose Q# = Unif([m]), which contributes an additive log(m) to the loss. Choosing
Qtok(t) = π(t1) Qr−1
i=1 P(ti+1|ti) as the stationary probability the Markov process associates with t,"
UNIGRAM MODELS UNDER TOKENIZATION,0.0471253534401508,"1
mLm(Q ◦enc(·)) ≈−1 mE """
UNIGRAM MODELS UNDER TOKENIZATION,0.04806786050895382,"log(P(s) +
Xm/k−1"
UNIGRAM MODELS UNDER TOKENIZATION,0.049010367577756835,"i=0
log

π(ski+1)
P(ski+1|ski) #"
UNIGRAM MODELS UNDER TOKENIZATION,0.04995287464655985,"(i)
≈1"
UNIGRAM MODELS UNDER TOKENIZATION,0.05089538171536286,"mH(P) +
1
mk
 
mH(π) −H(P)
"
UNIGRAM MODELS UNDER TOKENIZATION,0.051837888784165884,= H(P) m
UNIGRAM MODELS UNDER TOKENIZATION,0.0527803958529689,"
1 −
1
log2(d)"
UNIGRAM MODELS UNDER TOKENIZATION,0.05372290292177191,"
+ H(π)"
UNIGRAM MODELS UNDER TOKENIZATION,0.05466540999057493,"log2(d).
(2)"
UNIGRAM MODELS UNDER TOKENIZATION,0.05560791705937795,"the approximation in (i) uses the fact that as m grows large,
1
m
Pm/k
i=0 log(P(ski+ℓ+1|ski+ℓ) ap-
proaches H(P )"
UNIGRAM MODELS UNDER TOKENIZATION,0.05655042412818096,"k
. With d = 2 (i.e., r = 1), we recover the performance of the character tokenizer
in Theorem 2.1. An immediate implication of this simple calculation is that as m →∞, there is a
unigram model which is nearly optimal as the dictionary size grows to ∞."
UNIGRAM MODELS UNDER TOKENIZATION,0.057492931196983975,"While this toy tokenizer allows us to glean this intuition behind why tokenization allows unigram
models to be near-optimal, there are some obvious issues. One, the tokenizer does not adapt to the
distribution of the data. Indeed, for the switching Markov source in Figure 1, as p = q = δ →0, the
source contains increasingly longer sequences of contiguous 0’s and 1’s. In this case, it makes since
to have a dictionary containing such sequences, rather than all possible length-r sequences, many of
which would be seen very few times (if at all) in a test sequence. At a more technical level, in eq. (2),
to get to a cross-entropy loss of 2H(P), the size of the dictionary required by the toy tokenizer is
emH(π)/H(P ). As discussed in Example A.1 for the switching Markov process with p = q = δ, this
dictionary size can be extremely large and scales exponentially (in 1/δ) as e1/δ log(1/δ) when δ is"
UNIGRAM MODELS UNDER TOKENIZATION,0.058435438265786996,"2The last few characters which do not add up to r in total are left untokenized. These boundary effects will
not matter as the test sequences grow in length"
UNIGRAM MODELS UNDER TOKENIZATION,0.05937794533459001,"small. In general, on stochastic sources on a much larger alphabet, such as English/ASCII, this toy
tokenizer would result in a prohibitively large dictionary."
UNIGRAM MODELS UNDER TOKENIZATION,0.060320452403393024,"Larger dictionaries are usually correlated with the presence of rare tokens which appear infrequently
at training time. This presents a problem in practice - a lot more data is often required to see enough
examples of such tokens to learn good embeddings for them. More importantly, in the absence of
this volume of data, rare tokens present an attack surface to elicit undesirable behavior in the model
(Rumbelow and Watkins, 2023). In practice, this issue present with the toy tokenizer is, to an extent,
resolved by using tokenization algorithms such as BPE or Wordpiece, which learn dictionaries from
data. In the process, they are able to avoid learning extremely rare tokens, by enforcing a lower bound
on the number of their occurrences in the training data to be allocated as a token. By minimizing the
number of such rare tokens, the model is able to utilize its token budget in a more efficient manner."
UNIGRAM MODELS UNDER TOKENIZATION,0.061262959472196045,"We now introduce the main theoretical result of this paper, showing that with the appropriate
tokenization algorithm with a token budget of d, a unigram model is not only asymptotically able
to achieve the optimal cross-entropy loss, but also requires far smaller dictionaries to match the
performance of the toy tokenizer considered earlier. In order to avoid dealing with the transient
characteristics of the source, we consider the cross-entropy loss in eq. (1) under the assumption that
the test sequences s are of length m →∞. Namely, define the normalized loss,"
UNIGRAM MODELS UNDER TOKENIZATION,0.06220546654099906,"L(·) = lim
m→∞
1
mLm(·)"
UNIGRAM MODELS UNDER TOKENIZATION,0.06314797360980208,"Theorem 3.1. Consider a Markov data generating process which satisfies Assumption 3.2. Let d
denote a budget on the size of the dictionary. Then, there exists a tokenizer with at most d tokens and
encoding function enc(·), such that,"
UNIGRAM MODELS UNDER TOKENIZATION,0.0640904806786051,"min
Q∈Q1-gram L(Q ◦enc(·)) ≤
1
1 −ε min
Q′ L(Q′)
(3)"
UNIGRAM MODELS UNDER TOKENIZATION,0.06503298774740811,"where ε is log(1/δ)/0.99 log(d)3. Furthermore, a tokenizer satisfying eq. (3) with probability
≥1 −d−Ωδ(log(d)) can be learnt from a dataset of eOδ(d) characters."
UNIGRAM MODELS UNDER TOKENIZATION,0.06597549481621112,"The tokenizers considered in this theorem are far more efficient with their token budget than the toy
tokenizer - to achieve a cross entropy loss within a factor 2 of optimal, the dictionary size required
by these tokenizer is d ≈1/δ2 on any source satisfying Assumption 3.2. In comparison, the toy
tokenizer requires a dictionary size of e1/δ log(1/δ) to achieve the same error. We show that the LZW
tokenizer proposed in (Zouhar et al., 2023a) achieves the upper bound in eq. (3) when trained on a
dataset of size eO(d). Likewise, we also show that a sequential variant of BPE achieves the upper
bound in eq. (3) up to a factor of 2 and with a worse dependency in ε when trained on a dataset of size
eO(d2). What is interesting is that neither of these algorithms explicitly learn a unigram likelihood
model, Q, while constructing the dictionary. Yet they are able to perform as well as the tokenizers
which are jointly optimized with a likelihood model, such as the Unigram tokenizer (Kudo, 2018)."
UNIGRAM MODELS UNDER TOKENIZATION,0.06691800188501414,"Key insight.
While the toy tokenizer provides a high level intuition as to why tokenization might
enable unigram models to model Markov sources well, here we present a different explanation which
captures tokenization from an operational viewpoint. Tokenizers which do a good job at learning
patterns in the data and assigning these frequent patterns as tokens in the dictionary are compatible
with an i.i.d. model over tokens. A hypothetical example motivating this point: consider a tokenizer
such that the distribution of tokens in the encoding of a fresh string sampled from the source is
distributed i.i.d., except that whenever the token t′ appears, it is always followed by t′′. An i.i.d.
model on the tokens is a poor approximation since P(t′t′′) ≫P(t′)P(t′′). However, by merging t′
and t′′ into a new token t and adding this to the dictionary, the new distribution over tokens is i.i.d.
In general, this motivates why it is desirable for a tokenizer to allocate new tokens to substrings
which appear next to each other frequently, i.e. a pattern in the data. As more tokens are added to
the dictionary, one might expect the cross-entropy loss incurred by the best unigram model to improve."
LEARNING PATTERNS IN THE SOURCE,0.06786050895381715,"3.1
Learning patterns in the source"
LEARNING PATTERNS IN THE SOURCE,0.06880301602262016,"The main result of this section is a generic reduction: dictionaries which typically encode new strings
into a few long tokens (defined in a formal sense in Theorem 3.4), result in tokenizers achieving"
LEARNING PATTERNS IN THE SOURCE,0.06974552309142319,3ε is assumed to be < 1 in this statement. The constant 0.99 can be made arbitrarily close to 1.
LEARNING PATTERNS IN THE SOURCE,0.0706880301602262,"near-optimal cross-entropy loss. We prove this result for Markovian sources under a regularity
assumption, which is that the associated connectivity graph of the chain is complete. The analogous
assumption for kth-order sources is that the transition kernel is entry-wise bounded away from 0.
This assumption is satisfied by all the sources considered in the paper thus far, such as the kth-order
switching processes in Figure 1."
LEARNING PATTERNS IN THE SOURCE,0.07163053722902922,"Assumption 3.2 (Data generating process). Assume that the data source is an ergodic Markov process
with transition P(·|·) and stationary distribution π. Assume that mina,a′∈A P(a′|a) ≜δ > 0."
LEARNING PATTERNS IN THE SOURCE,0.07257304429783223,"Remark 3.3. Assumption 3.2 (and its kth-order extension) impose that there is a small but non-
zero probability of observing any particular symbol after any preceding sequence. This limits the
applicability of these processes in real-world scenarios where such a phenomenon may not occur.
However, our motivation for this assumption is different: δ allows parameterizing the Markov process
in a way which interpolates between i.i.d. (δ = 1/|A|) and highly non-i.i.d. (δ →0)."
LEARNING PATTERNS IN THE SOURCE,0.07351555136663525,"For a substring s and a character a, define P(s|a) = P(s1|a) Q|s|
i=2 P(si|si−1) denote the condi-
tional probability of the substring s. We now state the main result of this section."
LEARNING PATTERNS IN THE SOURCE,0.07445805843543826,"Theorem 3.4 (Bound on cross-entropy loss of dictionaries under greedy encoder). Consider a source
satisfying Assumption 3.2 and any tokenizer T equipped with the greedy encoder, encgre(·) with
finitely long tokens. Define, P(t) = Ea∼π[P(t|a)] and suppose H(QMLE, P) ≥1"
LEARNING PATTERNS IN THE SOURCE,0.07540056550424128,"ε log(1/δ) for some
ε < 1. Then,"
LEARNING PATTERNS IN THE SOURCE,0.0763430725730443,"min
Q∈Q1-gram L(Q ◦encgre(·)) ≤minQ L(Q)"
LEARNING PATTERNS IN THE SOURCE,0.07728557964184732,"1 −ε
."
LEARNING PATTERNS IN THE SOURCE,0.07822808671065033,"Interpretation.
H(QMLE, P) = Et∼QMLE[log(1/P(t))] is large when the encoder places higher
mass (i.e. larger values of QMLE(·)) on tokens which have low probability under P, i.e. which
correspond to longer substrings. Intuitively, this metric is higher for tokenizers which typically use
long tokens (i.e. low P(·)) to encode new strings."
LZW TOKENIZER,0.07917059377945335,"3.2
LZW tokenizer"
LZW TOKENIZER,0.08011310084825636,"In this section we study the Lempel-Ziv-Welch (LZW) based tokenization scheme introduced by
Zouhar et al. (2023a) and establish guarantees of the form of Theorem 3.1 for this tokenizer."
LZW TOKENIZER,0.08105560791705937,"Definition 3.5 (LZW tokenizer). Iterating from left to right, the shortest prefix of the training dataset
which does not already exist as a token is assigned as the next token in the dictionary. This substring
is removed and the process is iterated on the remainder of the dataset. The tokenizer uses the greedy
encoding algorithm (Definition A.3) to encode new strings into tokens."
LZW TOKENIZER,0.08199811498586239,"An example of the LZW tokenizer: For the dataset 0100111, the dictionary created is {0, 1, 00, 11}."
LZW TOKENIZER,0.08294062205466542,"The LZW tokenizer is based on the LZW algorithm for compression (Ziv and Lempel, 1978; Welch,
1984). The dictionary satisfies the property that if some substring s′ exists as a token in the dictionary,
then all of its prefixes must also belong to the dictionary. In the next theorem, we show that the LZW
tokenizer approximately achieves the optimal cross-entropy loss."
LZW TOKENIZER,0.08388312912346843,"Theorem 3.6. Suppose the LZW tokenizer is trained on a dataset of length at most d (thereby learning
a dictionary with at most d tokens). For Markov sources satisfying Assumption 3.2, with probability
≥1 −d−Oδ(log(d)), the resulting tokenizer satisfies,"
LZW TOKENIZER,0.08482563619227144,"min
Q∈Q1-gram L(Q · encgre(·)) ≤minQ L(Q)"
LZW TOKENIZER,0.08576814326107446,"1 −ε
."
LZW TOKENIZER,0.08671065032987747,"where ε =
log(1/δ)
0.99 log(d)
4."
LZW TOKENIZER,0.08765315739868049,"The proof of this result considers all substrings t with P(t) ≥1/d0.99. These substrings are
reasonably high probability and observed many times in a dataset of eΩ(d) characters. We show that
with high probability, the LZW tokenizer learns all of these substrings as tokens in the dictionary.
Now, when processing a new string, since the greedy algorithm only emits the longest substring"
LZW TOKENIZER,0.0885956644674835,4The constant 0.99 can be made arbitrarily close to 1.
LZW TOKENIZER,0.08953817153628653,"5
10
15
20
25
30
Training time (s) 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2"
LZW TOKENIZER,0.09048067860508954,Test loss L
LZW TOKENIZER,0.09142318567389256,"Tokenized
Untokenized
Optimal cross-entropy
Cross-entropy of best unigram model"
LZW TOKENIZER,0.09236569274269557,"(a) Convergence rate of smallest model which is within
10% of the optimal-cross entropy by 300 epochs. The
smallest untokenized model has 9010 parameters (3
layers, embedding dimension = 10). The smallest to-
kenized model with a dictionary size of 10 has 17880
parameters (3 layers, embedding dimension = 20).
The tokenized model has more parameters but the wall-
clock time taken to reach any loss value is smaller."
LZW TOKENIZER,0.09330819981149858,"0
5
10
15
20
25
30
Training time (s) 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2"
LZW TOKENIZER,0.0942507068803016,Test loss L
LZW TOKENIZER,0.09519321394910461,"Tokenized
Untokenized
Optimal cross-entropy
Cross-entropy of best unigram model"
LZW TOKENIZER,0.09613572101790764,"(b) Convergence rate of models with the same embed-
ding dimension (20), number of heads (1) and layers
(3) with and without tokenization. The model with to-
kenization (dictionary size of 20) appears to converge
more quickly, but the error floor is subtly higher com-
pared to the model without tokenization. Both models
are trained on input sequences of length 512. The width
of the tokenized model is smaller (145)."
LZW TOKENIZER,0.09707822808671066,"Figure 4: Test loss vs. wall-clock time for the tokenized and untokenized models when trained on the
order-1 switching Markov chain (Figure 1) with p = q = 0.8. The tokenizer used is BPE."
LZW TOKENIZER,0.09802073515551367,"which matches a token, every token allocated must fall on the “boundary” of this set, having P(t) ≤
O(1/d0.99). By definition, this means that H(QMLE, P) = Et∼QMLE[log(1/P(t))] = 0.99 log(d).
Combining this with Theorem 3.4 completes the proof. At a high level, on the infinite tree of
substrings A⋆we study which nodes are populated as tokens by LZW. This structure forms a Digital
Search Tree (DST) and prior work analyzes the mean and variance of the profile of the DST under
various source processes (Jacquet et al., 2001; Drmota and Szpankowski, 2011; Hun and Vallée, 2014;
Drmota et al., 2021). A detailed proof of Theorem 3.6 is provided in Appendix A.6."
EXPERIMENTAL RESULTS,0.09896324222431668,"4
Experimental Results"
EXPERIMENTAL RESULTS,0.0999057492931197,"Experiment 1 (Figures 4a and 4b)
In this experiment we study the order-1 switching Markov
chain. Transformers without tokenization empirically achieve a small cross-entropy on this learning
task as seen in Figure 4a and earlier in Makkuva et al. (2024). We vary hyperparameters to find the
smallest untokenized model which achieves a loss within 10% of the optimal-cross entropy within
300 epochs. Fixing a token dictionary size of 20, we also find the smallest tokenized model which
achieves the same loss. Although the smallest model with tokenization is larger than the smallest
model without tokenization in terms of the number of parameters, the wall-clock time taken to
optimize the model to any target test loss is observed to be smaller. Thus, tokenization appears
to reduce the compute time required to train the model to a target test loss in the toy example we
consider. In Figure 4b we compare models with the same architecture trained with and without
tokenization5. The model with tokenization appears to converge more quickly, although the limiting
error achieved is subtly higher in comparison with the model without tokenization."
EXPERIMENTAL RESULTS,0.10084825636192271,"Experiment 1 (Figure 5).
In this experiment, we train tokenizers on the Wikitext-103-raw-v1
dataset (Merity et al., 2016) and compare the performance of unigram models trained on the GLUE
dataset as the model size scales. Since the character-level tokenizer operates on a fixed vocabulary,
in order to compare with the other tokenizers, we plot the number of unique k-grams observed in
the training dataset along the x-axis. While this is not an apples-to-apples comparison, we use the
number of unique k-grams in the dataset as a proxy for the complexity of the likelihood model trained."
EXPERIMENTAL RESULTS,0.10179076343072573,"5The model with tokenization has width equal to the typical length of sequences after encoding, which is
smaller."
EXPERIMENTAL RESULTS,0.10273327049952875,"One may also use the total number of possible k-grams as a proxy; however a large fraction of these
k-grams would likely never be observed in a real dataset (especially as k grows)."
EXPERIMENTAL RESULTS,0.10367577756833177,"Experiment 2 (Table 1).
In this experiment, we compare the cross entropy loss of the best unigram
model trained on pre-trained tokenizers on an array of datasets. All the considered tokenizers have
dictionary sizes in the range 31K-51K. The best bigram model under the character tokenizer is
consistently outperformed by the best unigram likelihood model trained under a number of pre-
trained tokenizers on a variety of datasets: Rotten Tomatoes (8.5K sequences), GLUE (105K), Yelp
review (650K) and Wikitext-103-v1 (1.8M)."
EXPERIMENTAL RESULTS,0.10461828463713478,"0
20000
40000
60000
80000
Size of dictionary 0.8 1.0 1.2 1.4 1.6 1.8 2.0"
EXPERIMENTAL RESULTS,0.1055607917059378,Cross-entropy loss of learnt unigram model 1e7
EXPERIMENTAL RESULTS,0.10650329877474081,1-gram (159)
EXPERIMENTAL RESULTS,0.10744580584354382,2-gram (3393)
EXPERIMENTAL RESULTS,0.10838831291234684,3-gram (23098)
EXPERIMENTAL RESULTS,0.10933081998114987,4-gram (89524)
EXPERIMENTAL RESULTS,0.11027332704995288,"LZW
BPE
Unigram
Wordpiece
Character"
EXPERIMENTAL RESULTS,0.1112158341187559,"Figure 5: Performance vs. dictionary size. Tokeniz-
ers are trained on the Wikitext-103 dataset. For all
other tokenizers we train unigram models while for
the the character-level tokenizer, we train k-gram
models for k ∈{1, 2, 3, 4}. Likelihood models are
trained on the GLUE dataset. The parentheses in-
dicates the number of distinct observed k-grams,
which lower bounds the k-gram model complexity."
EXPERIMENTAL RESULTS,0.11215834118755891,"RT
Wiki
Yelp
GLUE
BERT
1.58
1.55
1.60
1.50
Tinyllama
1.75
1.84
1.82
1.70
GPT-neox
1.57
1.64
1.66
1.48
Mistral
1.69
1.80
1.75
1.66
Phi-2
1.54
1.62
1.64
1.45
Character
2.40
2.45
2.46
2.38"
EXPERIMENTAL RESULTS,0.11310084825636192,"Table 1: Cross-entropy loss estimates (using
eq. (55)) of unigram models trained on pre-
trained tokenizers under a number of datasets.
The last row (blue) is the character level tok-
enizer, on which a more powerful bigram model
is trained. BERT is based on Wordpiece, and
the remaining tokenizers are BPE based. The
character-level tokenizer we use is ByT5."
ADDITIONAL THEORETICAL RESULTS,0.11404335532516494,"4.1
Additional theoretical results"
ADDITIONAL THEORETICAL RESULTS,0.11498586239396795,"We present some additional theoretical results in the appendix which we discuss briefly below. In
Appendix B, we do a theoretical study of the cross-entropy loss achieved by the popular BPE tokenizer.
We show that a variant of BPE achieves the upper bound on the RHS of eq. (3) (Theorem 3.1) up to a
factor approaching 2 as the dictionary size grows. It is an interesting question for future research to
understand whether this factor of 2 can be removed, since transformers are observed to achieve the
near-optimal cross-entropy loss as the dictionary size grows (cf. Figure 3b). In Appendix C, we prove
finite sample bounds on the end-to-end model under the LZW tokenizer with a smoothed empirical
estimator as the unigram model. This analysis reveals that there is a sweet spot for the dictionary
size - too small a dictionary, and the statistical error floor is significant, too large a dictionary, and the
statistical error incurred by the likelihood model dominates the overall loss. We also take a closer
look into the aspect of generalization for tokenizers, which arises from the fact that the tokenizer
is evaluated on data that it was not trained on. Prior work such as Zouhar et al. (2023b) show that
BPE is an approximation algorithm for finding the sequence of merges which minimizes the size
of the compressed dataset. This does not imply any guarantees on the end-to-end performance,
or even compression power of the tokenizer on new sequences. In particular, in Appendix D we
show that there exist tokenizers which compress the dataset into a short sequence of tokens, but
do so in a way which fails to generalize to new sequences. Thus measuring the performance of
a tokenizer necessitates understanding its behavior on data it was not trained on. In Appendix E,
we show a different kind of intricacy - there exist tokenizers under which the best unigram model
achieves low cross-entropy loss. However, the same dictionary under a different encoding algorithm
performs nearly as poorly as the character-level tokenizer. The interaction between the dictionary and
encoding algorithm is a poorly studied subject in the tokenization literature; this result emphasizes
the importance of understanding this relationship."
OPEN QUESTIONS,0.11592836946277098,"5
Open questions"
OPEN QUESTIONS,0.11687087653157399,"In this section, we discuss some limitations of our work and open questions stemming from them. We
show that when transformers are trained with or without tokenization, they learn to approximately
represent k-gram models for different values of k. Transformers are capable of representing far more
complex behavior, which are elicited under more complex data generating processes. Extending
our formulation to these settings presents an avenue to develop an even better understanding of
tokenization, and would allow finer-grained comparisons between tokenizers. The behavior and role
of tokenizers may be very different in these contexts. Below we discuss some concrete questions."
OPEN QUESTIONS,0.117813383600377,"Our theory assumes that the underlying Markov chain has every transition occurring with non-zero
probability, which is a limitation. However, the analysis for the toy tokenizer in eq. (2) shows that
when the dictionary size scales as exp(mH(π)/H(P)), even in the absence of Assumption 3.2, the
tokenizer achieves the optimal cross-entropy to within a factor of 2. This leads to the following
conjecture."
OPEN QUESTIONS,0.11875589066918002,"Conjecture 1. In the spirit of eliminating Assumption 3.2, is it possible to establish a version of
Theorem 3.1 applicable to data drawn from any Markov chain, where ε = log(1/δ)/0.99 log(d) is
replaced by ε = log(mH(π)/H(P))/0.99 log(d)."
OPEN QUESTIONS,0.11969839773798303,"In Appendix B, we analyze a variant of the BPE tokenizer, which carries out a version of sample
splitting, and establish a weaker variant of Theorem 3.1 for this tokenizer. This is to simplify the
statistical dependencies arising from the fact that while learning its dictionary, BPE makes a run over
the entire training dataset each time a new token is added. It remains an open question to analyze and
establish a variant of Theorem 3.1 for the standard BPE tokenizer."
CONCLUSION,0.12064090480678605,"6
Conclusion"
CONCLUSION,0.12158341187558906,"We present a theoretical framework to compare and analyze different tokenization algorithms. We
study the end-to-end cross-entropy loss of the tokenizer + likelihood model, and focus on the case
where the data generating process is Markovian. We empirically observe that transformers with
tokenization are drastically more efficient at learning kth-order Markov processes, compared to
without tokenization. We prove that algorithms such as LZW and a sequential variant of BPE learn
tokenizers such that the best unigram likelihood model trained on them approaches the cross-entropy
loss of the optimal likelihood model, as the vocabulary size d grows."
ACKNOWLEDGEMENTS,0.12252591894439209,"7
Acknowledgements"
ACKNOWLEDGEMENTS,0.1234684260131951,"JJ and NR were partially supported by NSF Grants IIS-1901252 and CCF-2211209. KR was partially
supported by NSF Grant CCF-2211209."
REFERENCES,0.12441093308199812,References
REFERENCES,0.12535344015080113,"Zaid Alyafeai, Maged S Al-shaibani, Mustafa Ghaleb, and Irfan Ahmad. Evaluating various tokenizers
for arabic text classification. Neural Processing Letters, 55(3):2911–2933, 2023."
REFERENCES,0.12629594721960416,"Dietrich Braess and Thomas Sauer. Bernstein polynomials and learning theory. Journal of Approxi-
mation Theory, 128(2):187–206, 2004."
REFERENCES,0.12723845428840716,"Yen-Chi Chen. Stochastic modeling of scientific data, Autumn 2018."
REFERENCES,0.1281809613572102,"Jonathan H Clark, Dan Garrette, Iulia Turc, and John Wieting. Canine: Pre-training an efficient
tokenization-free encoder for language representation. Transactions of the Association for Compu-
tational Linguistics, 10:73–91, 2022."
REFERENCES,0.1291234684260132,"Yasuharu Den, Toshinobu Ogiso, Hideki Ogura, Atsushi Yamada, Nobuaki Minematsu, Kiyotaka
Uchimoto, and Hanae Koiso. The development of an electronic dictionary for morphological
analysis and its application to japanese corpus linguistics, Oct 2007. URL https://repository.
ninjal.ac.jp/api/records/2201."
REFERENCES,0.13006597549481622,"Michael Drmota and Wojciech Szpankowski. The expected profile of digital search trees. Journal of
Combinatorial Theory, Series A, 118(7):1939–1965, 2011."
REFERENCES,0.13100848256361922,"Michael Drmota, Michael Fuchs, Hsien-Kuei Hwang, and Ralph Neininger. Node profiles of
symmetric digital search trees: Concentration properties. Random Structures & Algorithms, 58(3):
430–467, 2021."
REFERENCES,0.13195098963242224,"Benjamin L Edelman, Ezra Edelman, Surbhi Goel, Eran Malach, and Nikolaos Tsilivis. The evolution
of statistical induction heads: In-context learning markov chains. arXiv preprint arXiv:2402.11004,
2024."
REFERENCES,0.13289349670122527,"Tanja Eisner, Bálint Farkas, Markus Haase, and Rainer Nagel. Operator theoretic aspects of ergodic
theory, volume 272. Springer, 2015."
REFERENCES,0.13383600377002827,"Philip Gage. A new algorithm for data compression. C Users Journal, 12(2):23–38, 1994."
REFERENCES,0.1347785108388313,"Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, Geraud Krawezik,
Francois Lanusse, Michael McCabe, Ruben Ohana, Liam Parker, et al. xval: A continuous number
encoding for large language models. arXiv preprint arXiv:2310.02989, 2023."
REFERENCES,0.1357210179076343,"Robert M Gray and RM Gray. Probability, random processes, and ergodic properties, volume 1.
Springer, 2009."
REFERENCES,0.13666352497643733,"Gregory Grefenstette and Pasi Tapanainen. What is a word, what is a sentence?: problems of
tokenisation. 1994."
REFERENCES,0.13760603204524033,"Yanjun Han, Soham Jana, and Yihong Wu. Optimal prediction of markov chains with and without
spectral gap. Advances in Neural Information Processing Systems, 34:11233–11246, 2021."
REFERENCES,0.13854853911404336,"Kanal Hun and Brigitte Vallée. Typical depth of a digital search tree built on a general source. In 2014
Proceedings of the Eleventh Workshop on Analytic Algorithmics and Combinatorics (ANALCO),
pages 1–15. SIAM, 2014."
REFERENCES,0.13949104618284638,"Itay Itzhak and Omer Levy. Models in a spelling bee: Language models implicitly learn the character
composition of tokens. arXiv preprint arXiv:2108.11193, 2021."
REFERENCES,0.14043355325164938,"Philippe Jacquet, Wojciech Szpankowski, and Jing Tang. Average profile of the lempel-ziv parsing
scheme for a markovian source. Algorithmica, 31:318–360, 2001."
REFERENCES,0.1413760603204524,"Eugene Kharitonov, Marco Baroni, and Dieuwke Hupkes. How bpe affects memorization in trans-
formers. arXiv preprint arXiv:2110.02782, 2021."
REFERENCES,0.1423185673892554,"Taku Kudo. Subword regularization: Improving neural network translation models with multiple
subword candidates. arXiv preprint arXiv:1804.10959, 2018."
REFERENCES,0.14326107445805844,"N Jesper Larsson and Alistair Moffat. Off-line dictionary-based compression. Proceedings of the
IEEE, 88(11):1722–1732, 2000."
REFERENCES,0.14420358152686144,"Jindˇrich Libovick`y, Helmut Schmid, and Alexander Fraser. Why don’t people use character-level
machine translation? arXiv preprint arXiv:2110.08191, 2021."
REFERENCES,0.14514608859566447,"Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji Kim,
and Michael Gastpar. Attention with markov: A framework for principled analysis of transformers
via markov chains. arXiv preprint arXiv:2402.04161, 2024."
REFERENCES,0.1460885956644675,"Ben Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell,
S Agarwal, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020."
REFERENCES,0.1470311027332705,"Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843, 2016."
REFERENCES,0.14797360980207352,"Jaouad Mourtada and Stéphane Gaïffas. An improper estimator with optimal excess risk in misspeci-
fied density estimation and logistic regression. The Journal of Machine Learning Research, 23(1):
1384–1432, 2022."
REFERENCES,0.14891611687087652,"Assaf Naor, Shravas Rao, and Oded Regev. Concentration of markov chains with bounded moments.
2020."
REFERENCES,0.14985862393967955,"Gonzalo Navarro and Luís MS Russo. Re-pair achieves high-order entropy. In DCC, page 537.
Citeseer, 2008."
REFERENCES,0.15080113100848255,"Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with
gradient descent. arXiv preprint arXiv:2402.14735, 2024."
REFERENCES,0.15174363807728558,"David D Palmer. Tokenisation and sentence segmentation. Handbook of natural language processing,
pages 11–35, 2000."
REFERENCES,0.1526861451460886,"Terence Parr. The Definitive ANTLR 4 Reference. Pragmatic Bookshelf, Raleigh, NC, 2 edition,
2013. ISBN 978-1-93435-699-9. URL https://www.safaribooksonline.com/library/
view/the-definitive-antlr/9781941222621/."
REFERENCES,0.1536286522148916,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.15457115928369464,"Jessica Rumbelow and Matthew Watkins. Solidgoldmagikarp. https://www.alignmentforum.
org/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation, 2023."
REFERENCES,0.15551366635249764,"Mike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In 2012 IEEE international
conference on acoustics, speech and signal processing (ICASSP), pages 5149–5152. IEEE, 2012."
REFERENCES,0.15645617342130066,"Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–1725, Berlin,
Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162.
URL https://aclanthology.org/P16-1162."
REFERENCES,0.15739868049010367,"Aaditya K Singh and DJ Strouse. Tokenization counts: the impact of tokenization on arithmetic in
frontier llms. arXiv preprint arXiv:2402.14903, 2024."
REFERENCES,0.1583411875589067,"Arseny Tolmachev, Daisuke Kawahara, and Sadao Kurohashi. Juman++: A morphological analysis
toolkit for scriptio continua. In Eduardo Blanco and Wei Lu, editors, Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing: System Demonstrations,
pages 54–59, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi:
10.18653/v1/D18-2010. URL https://aclanthology.org/D18-2010."
REFERENCES,0.15928369462770972,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023."
REFERENCES,0.16022620169651272,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019.
In the Proceedings of ICLR."
REFERENCES,0.16116870876531575,"Terry A. Welch. A technique for high-performance data compression. Computer, 17(06):8–19, 1984."
REFERENCES,0.16211121583411875,"Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam
Roberts, and Colin Raffel. Byt5: Towards a token-free future with pre-trained byte-to-byte models.
Transactions of the Association for Computational Linguistics, 10:291–306, 2022."
REFERENCES,0.16305372290292178,"Sangwon Yu, Jongyoon Song, Heeseung Kim, Seong-min Lee, Woo-Jong Ryu, and Sungroh Yoon.
Rare tokens degenerate all tokens: Improving neural text generation via adaptive gradient gating
for rare token embeddings. arXiv preprint arXiv:2109.03127, 2021."
REFERENCES,0.16399622997172478,"Wenqing Zheng, SP Sharan, Ajay Kumar Jaiswal, Kevin Wang, Yihan Xi, Dejia Xu, and Zhangyang
Wang. Outline, then details: Syntactically guided coarse-to-fine code generation. In International
Conference on Machine Learning, pages 42403–42419. PMLR, 2023."
REFERENCES,0.1649387370405278,"Jacob Ziv and Abraham Lempel. Compression of individual sequences via variable-rate coding.
IEEE transactions on Information Theory, 24(5):530–536, 1978."
REFERENCES,0.16588124410933083,"Vilém Zouhar, Clara Meister, Juan Gastaldi, Li Du, Mrinmaya Sachan, and Ryan Cotterell. To-
kenization and the noiseless channel.
In Anna Rogers, Jordan Boyd-Graber, and Naoaki
Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages 5184–5207, Toronto, Canada, July 2023a.
Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.284. URL https:
//aclanthology.org/2023.acl-long.284."
REFERENCES,0.16682375117813383,"Vilém Zouhar, Clara Meister, Juan Luis Gastaldi, Li Du, Tim Vieira, Mrinmaya Sachan, and Ryan
Cotterell. A formal perspective on byte-pair encoding. arXiv preprint arXiv:2306.16837, 2023b."
REFERENCES,0.16776625824693686,Appendix
REFERENCES,0.16870876531573986,Contents
REFERENCES,0.1696512723845429,"A
Analysis of LZW: Proofs of Theorems 3.4 and 3.6
14
A.1
Notation and definitions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
A.2
A basic result about the optimal achievable cross-entropy loss . . . . . . . . . . . .
14
A.3
Proof of Theorem 2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
A.4
Maximum likelihood unigram model . . . . . . . . . . . . . . . . . . . . . . . . .
15
A.5
Proof of Theorem 3.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
A.6
Heavy-hitter dictionaries and a proof of Theorem 3.6
. . . . . . . . . . . . . . . .
17"
REFERENCES,0.1705937794533459,"B
Additional Theoretical Results I: A sequential variant of BPE
22
B.1
Analysis of Algorithm 1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
B.2
Analysis for the large dictionary case: |Dict| > d0 . . . . . . . . . . . . . . . . . .
25
B.3
Analysis in the small dictionary case . . . . . . . . . . . . . . . . . . . . . . . . .
32"
REFERENCES,0.17153628652214892,"C
Additional Theoretical Results II: Learning the likelihood model
35
C.1
Proof of Theorem C.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36"
REFERENCES,0.17247879359095195,"D
Additional Theoretical Results III: The generalization ability of tokenizers
40"
REFERENCES,0.17342130065975495,"E
Additional Theoretical Results IV: Interaction between the dictionary and encoding
algorithm
41
E.1
Stochastic source and dictionary. . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
E.2
Minimal encoder achieves the optimal cross-entropy loss up to a constant.
. . . . .
42
E.3
Greedy-encoder achieves poor cross-entropy loss
. . . . . . . . . . . . . . . . . .
44"
REFERENCES,0.17436380772855797,"F
Experiment details
46"
REFERENCES,0.17530631479736097,"G
NeurIPS Paper Checklist
48"
REFERENCES,0.176248821866164,"A
Analysis of LZW: Proofs of Theorems 3.4 and 3.6"
REFERENCES,0.177191328934967,"A.1
Notation and definitions"
REFERENCES,0.17813383600377003,"For each character a ∈A let T ⋆
a denote an infinite tree, with root vertex ∅, and subsequent vertices
labelled by strings t ∈A⋆. The edge from a parent vertex t to any child ta′ is labelled with
the probability P(ta′|t) unless t = ∅, in which case the edge probability is P(a′|a). An infinite
trajectory sampled on the tree T ⋆
a corresponds to an infinite string sampled from the stochastic source
conditioned on the first character of the string being a. In this paper we only consider ergodic sources
(Gray and Gray, 2009) for which we can define the “entropy rate”. The entropy rate fundamentally
captures the compressibility of the source, and can be defined as H∞≜limm→∞1"
REFERENCES,0.17907634307257306,"mH(P) where s
is a length m string drawn from the source. By Theorem 2.1, H∞, captures minQ L(Q)."
REFERENCES,0.18001885014137606,"A.2
A basic result about the optimal achievable cross-entropy loss"
REFERENCES,0.18096135721017909,"The ratio of H(P) and mH(π) can be made arbitrarily large for the switching Markov chains in
Figure 1 as the switching probabilities p and q approach 0 or 1. See Example A.1 for more details.
Example A.1. Consider the switching Markov process in Figure 1 on {0, 1} with p = q = 1 −δ.
For this process, limm→∞1"
REFERENCES,0.18190386427898209,"mH(P) = HBer(δ) = δ log(1/δ) + (1 −δ) log(1/(1 −δ)), but π =
{1/2, 1/2} and so H(π) = HBer(1/2) = log(2). The ratio limm→∞
mH(π)"
REFERENCES,0.1828463713477851,H(P ) goes to ∞as δ →0.
REFERENCES,0.18378887841658811,"A.3
Proof of Theorem 2.1"
REFERENCES,0.18473138548539114,"We first characterize the minimum achievable cross-entropy loss Lm(Q) without any restrictions
on the likelihood model class Q. Choosing Q(enc(s)) = Q(s) = P(s), the true probability of
the sequence s, we have Lm(Q ◦enc(·)) = H(s) where H(·) is the entropy function. It is not
that difficult to see that this is also the minimum cross-entropy loss that can be achieved. For any
distribution Q,"
REFERENCES,0.18567389255419417,"Lm(Q) = E[log(1/Q(s)]
= E[log(P(s)/Q(s)] + E[log(1/P(s))]
= H(P) + DKL(P∥Q)."
REFERENCES,0.18661639962299717,"On the other hand, the cross-entropy loss under any unigram model Q ∈Q1-gram satisfies,"
REFERENCES,0.1875589066918002,"1
mLm(Q ◦enc(·))
(i)
= −1 m m
X"
REFERENCES,0.1885014137606032,"i=1
E[log Qtok(ti)] −1"
REFERENCES,0.18944392082940623,mE[log Q#(m)]
REFERENCES,0.19038642789820923,"(ii)
≥−
X"
REFERENCES,0.19132893496701225,"a∈A
π(a) log Qtok(a) ≥H(π)"
REFERENCES,0.19227144203581528,"where in (i), we use the definition of the unigram model Q, and in (ii), π is the stationary distribution
over characters induced by the stochastic source, and the ergodicity of the source is used. The last
equation lower bounds H(X, Y ) ≥H(X)."
REFERENCES,0.19321394910461828,"A.4
Maximum likelihood unigram model"
REFERENCES,0.1941564561734213,"A number of our results (Theorems 3.4 and 3.6 to name a few) are related to bounding
minQ∈Q1-gram L(Q ◦enc(·)) for some tokenizer T . In this section we introduce the maximum
likelihood unigram model which captures the optimizer over Q for any given tokenizer."
REFERENCES,0.1950989632422243,"For the character level tokenizer, an examination of Theorem 2.1 shows that the optimal unigram
likelihood model associates probability Qtok(a) = π(a), i.e. the limiting fraction of times the
character a is observed in the sequence. More generally, for a non-trivial tokenizer, the corresponding
optimal unigram model Q⋆
tok(t) ends up being the limiting expected fraction of times t is observed
in an encoding of a sequence. This is the maximum likelihood unigram model, which we formally
define below. The unigram MLE likelihood model associates probability,"
REFERENCES,0.19604147031102734,"QMLE(t) ←lim
m→∞E"
REFERENCES,0.19698397737983034,"""
nt
P
t nt # (4)"
REFERENCES,0.19792648444863337,"to each token, where nt is the random variable capturing the number of occurrences of the token t
in the encoding of the length-m string s. Restricting the class of likelihood models to the unigram
models, Q1-gram, QMLE captures the model which minimizes eq. (1)."
REFERENCES,0.1988689915174364,"The unigram MLE model cannot be computed without an infinite amount of data, but can be
approximated well with a finite amount of data, which forms the basis for Theorem C.1. For
certain encoding algorithms, we can show that the quantity nt/ P
t nt asymptotically converges to
its expectation (Lemma A.4). This is the reason the unigram model in eq. (4) is referred to as a
“maximum likelihood” model, since limm→∞nt/ P"
REFERENCES,0.1998114985862394,"t nt is the limit as |s| = m →∞of the solution
to the following likelihood maximization problem: given a sequence s, find the distribution over
tokens, Q, which maximizes
Y"
REFERENCES,0.20075400565504242,"t∈enc(s)
Q(t) ≡
Y"
REFERENCES,0.20169651272384542,t∈Dict
REFERENCES,0.20263901979264845," 
Q(t)
nt ."
REFERENCES,0.20358152686145145,"As discussed previously, the unigram MLE model over tokens in eq. (4) induces a joint distribution
over sequences of tokens by looking at the product of the marginal probabilities of the composed
tokens; in particular,"
REFERENCES,0.20452403393025448,"QMLE(t1, · · · , tj) = QMLE(j) jY"
REFERENCES,0.2054665409990575,"i=1
QMLE(ti),"
REFERENCES,0.2064090480678605,"where QMLE(j) is a distribution on the total number of tokens generated and is instantiated as
Unif([m]).
Remark A.2. Note that the unigram MLE model specifies a distribution over tokens which is a function
of the underlying encoding algorithm, enc(·). Different encoders result in different population level
distributions over tokens, and consequently different unigram MLE models.
Definition A.3 (greedy encoder). Given a dictionary Dict, the greedy encoder encgre(s) encodes a
string s into tokens by greedily matching from left to right, the largest substring that exists as a token
in Dict. This substring is then removed and the process iterated on the remainder of s. The greedy
decoder decgre(·) is a lookup table - a sequence of tokens is decoded by replacing each occurrence of
a token by the corresponding substring it maps to in Dict."
REFERENCES,0.20735155513666353,"Lemma A.4. limm→∞
nt
P"
REFERENCES,0.20829406220546653,"t′ nt′
a.s.
= limm→∞E
h
nt
P"
REFERENCES,0.20923656927426956,"t′ nt′
i
for any tokenizer having a finite vocabulary
and finitely long tokens, using the greedy encoder."
REFERENCES,0.21017907634307256,"Proof. This result is essentially true because under the greedy encoder, the tokens in an encoding of
a fresh string t may be generated by an rth-order Markov process for some r. For such processes, the
Cesàro average of the state distributions converges to a stationary distribution of the process (i.e., the
Krylov–Bogolyubov argument)."
REFERENCES,0.2111215834118756,"Tokens are generated as follows. Suppose the previous tokens generated were t1, t2, · · · , ti. The
next token ti+1 is sampled by drawing an infinite trajectory from T ⋆
a for a ∼P(·|ti) and returning
the longest prefix t of this trajectory which is a token in Dict, conditional on satisfying the conditions,
tjtj+1 · · · tit ̸∈Dict for all j ∈{1, 2, · · · , i}. This process is repeated sequentially to generate all
the tokens."
REFERENCES,0.21206409048067862,"Suppose the length of the longest token in the dictionary is ℓmax. Then, the distribution from a which
a token is sampled depends on at most the previous ℓmax tokens. The reason for this is that the
dependency of the (i + 1)th token, ti+1, on the previously sampled tokens emerges in the constraint
tjtj+1 · · · titi+1 ̸∈Dict, satisfied by any candidate ti+1. Since each token is of length at least one,
this condition is vacuously satisfied if j < i −ℓmax."
REFERENCES,0.21300659754948162,"With this view, the evolution of the state, defined as stater = (trℓmax, trℓmax−1, · · · , t(r−1)ℓmax)
evolves in a Markovian fashion. By the Krylov–Bogolyubov argument (cf. Proposition 4.2 in
Chen (2018)), the time averaged visitation frequencies of a Markov chain coordinate-wise asymp-
totically converges to its expectation, almost surely. This expectation exists by Theorems 8.5 and
8.22 of Eisner et al. (2015) which shows that for a matrix A such that supt∈N ∥At∥op < ∞the
limit limt→∞1"
REFERENCES,0.21394910461828465,"t
Pt
i=1 Ai exists. For the finite-state Markov transition A which captures the token
generation process, condition supt∈N ∥At∥op ≤|Dict|ℓmax < ∞. This means that the limit of
the time averaged state distribution exists. Moreover, for any initial distribution π0 over tokens,
π = limt→∞1"
REFERENCES,0.21489161168708765,"t
Pt
i=1 π0Ai satisfies the condition πA = π, implying that the limiting time-averaged
state distribution is a stationary distribution of A. Since the limiting time-averaged measure on
the state stater = (trℓmax, · · · , trℓmax−1, · · · , t(r−1)ℓmax) exists, this implies that the limiting time-
averaged measure of trℓmax−r′ for each r′ ∈{0, 1, · · · , ℓmax} exists. By taking the uniform average
over r′ and r, the limiting time-averaged measure of ti over i ∈N exists."
REFERENCES,0.21583411875589067,"A.5
Proof of Theorem 3.4"
REFERENCES,0.21677662582469368,"Consider a string s of length m →∞which is encoded into a sequence of tokens (ti : i ∈
[|encgre(s)|]). By the Asymptotic Equipartition Property (AEP) for ergodic sources, i.e. the Shan-
non–McMillan–Breiman theorem,"
REFERENCES,0.2177191328934967,"Pr

lim
m→∞−1"
REFERENCES,0.21866163996229973,m log P(s) = H∞
REFERENCES,0.21960414703110273,"
= 1.
(5)"
REFERENCES,0.22054665409990576,"Here limm→∞
H(P )"
REFERENCES,0.22148916116870876,"m
also happens to be the entropy rate of the source. We use this property to bound
the length of the greedy encoding, |encgre(s)|. Indeed, the probability of s may be decomposed as,"
REFERENCES,0.2224316682375118,P(s) = P(t1)
REFERENCES,0.2233741753063148,"|encgre(s)|
Y"
REFERENCES,0.22431668237511782,"i=2
P
 
ti
ti−1)

≤"
REFERENCES,0.22525918944392084,"|encgre(s)|
Y"
REFERENCES,0.22620169651272384,"i=1
max
a∈A P
 
ti
a

."
REFERENCES,0.22714420358152687,"Noting that δ mina P(t|a) ≥maxa P(t|a), up to a δ factor we may replace the max over a by an
expectation over a ∼π where π is the stationary distribution of the stochastic source. In particular,"
REFERENCES,0.22808671065032987,P(s) ≤
REFERENCES,0.2290292177191329,"|encgre(s)|
Y"
REFERENCES,0.2299717247879359,"i=1
P(ti)/δ."
REFERENCES,0.23091423185673893,"By invoking the AEP, eq. (5),"
REFERENCES,0.23185673892554196,"lim
m→∞
1
m"
REFERENCES,0.23279924599434496,X|encgre(s)|
REFERENCES,0.23374175306314798,"i=1
−log
 
P(ti)) −log(1/δ)
 a.s.
≤H∞"
REFERENCES,0.23468426013195098,"Recall that the greedy encoder satisfies Lemma A.4 and for any t ∈Dict, limm→∞
nt
|encgre(s)|
a.s.
="
REFERENCES,0.235626767200754,"QMLE(t). Furthermore, note that for any token t ∈Dict, P(t) > δ|t| > 0, and |encgre(s)| ≤m
surely. By almost sure convergence,"
REFERENCES,0.236569274269557,"lim
m→∞
|encgre(s)| m X"
REFERENCES,0.23751178133836004,"t∈Dict −
nt
|encgre(s)|"
REFERENCES,0.23845428840716307,"
log
 
P(t) −log(1/δ)
"
REFERENCES,0.23939679547596607,"a.s.
=
lim
m→∞
|encgre(s)|"
REFERENCES,0.2403393025447691,"m
 
H(QMLE, P) −log(1/δ)
"
REFERENCES,0.2412818096135721,"Furthermore, utilizing the assumption that εH(QMLE, P) ≥log(1/δ) satisfied by the tokenizer,"
REFERENCES,0.24222431668237512,"lim
m→∞
(1 −ε)|encgre(s)|
 
H(QMLE, P)
 m"
REFERENCES,0.24316682375117812,"a.s.
≤H∞.
(6)"
REFERENCES,0.24410933081998115,"Now we are ready to bound the expected cross-entropy loss of the tokenizer. Define the unigram model
Pπ(t1, t2, · · · , tj) = Punif(j) Qj
i=1 P(ti) where Punif is the uniform measure over [m]. Note that we
have the inequality minQ∈Q1-gram limm→∞1"
REFERENCES,0.24505183788878418,mLm(Q ◦encgre(·)) ≤limm→∞1
REFERENCES,0.24599434495758718,"mLm(Pπ ◦encgre(·))
and therefore, it suffices to upper bound the RHS. In particular,"
REFERENCES,0.2469368520263902,"Lm(Pπ ◦encgre(·)) = −E[log Punif(|encgre(s)|)] −E
X"
REFERENCES,0.2478793590951932,"t∈encgre(s) log
 
P(t)
"
REFERENCES,0.24882186616399624,"≤log(m) −E
X"
REFERENCES,0.24976437323279924,"t∈encgre(s) log
 
P(t)

(7)"
REFERENCES,0.25070688030160226,"where the last inequality uses the fact that Punif(|encgre(s)|) = 1/m. Note that as m →∞, by
assumption on the tokenizer, the fraction of times the token t appears in the encoding of s converges
almost surely converges to QMLE(t). Since |encgre(s)| ≤m surely and P(t) > δ|t| > 0, by an
application of the Dominated Convergence Theorem,"
REFERENCES,0.25164938737040526,"−lim
m→∞
1
mE
X"
REFERENCES,0.2525918944392083,"t∈encgre(s) log
 
P(t)

= −lim
m→∞
1
mE

|encgre(s)| ·
X"
REFERENCES,0.2535344015080113,"t∈Dict QMLE(t) log
 
P(t)
"
REFERENCES,0.2544769085768143,"= lim
m→∞
1
mE

|encgre(s)|H(QMLE, P)

(8)"
REFERENCES,0.2554194156456173,"Combining eq. (7) with eq. (8) and setting limm→∞log(m)/m = 0, and invoking eq. (6),"
REFERENCES,0.2563619227144204,"min
Q∈Q1-gram lim
m→∞
1
mLm(QMLE ◦encgre(·)) = lim
m→∞
1
mE

|encgre(s)|H(QMLE, P)
 ≤H∞"
REFERENCES,0.2573044297832234,"1 −ε.
(9)"
REFERENCES,0.2582469368520264,"By Theorem 2.1, we have that minQ limm→∞1"
REFERENCES,0.25918944392082943,"mLm(Q◦encgre(·)) = limm→∞
H(P )"
REFERENCES,0.26013195098963243,"m
= H∞, which
uses the fact that the source is ergodic. Combining with eq. (9) completes the proof."
REFERENCES,0.26107445805843543,"A.6
Heavy-hitter dictionaries and a proof of Theorem 3.6"
REFERENCES,0.26201696512723843,"In this section we prove Theorem 3.6 and introduce the notion of a heavy-hitting dictionary. At a
high level, these dictionaries contain all the substrings which have reasonably high probability of
being observed many times in a dataset of size n = eΩδ(d). We first show in Lemma A.6 that heavy
hitting dictionaries generalize well in the sense of having H(QMLE, P) being large (in conjunction
with Theorem 3.4 this implies an upper bound on the cross-entropy loss of the best unigram model).
Next, we will prove that the LZW algorithm (Definition 3.5) results in a heavy hitting dictionary with
high probability."
REFERENCES,0.2629594721960415,"Figure 6: The circled nodes indicates substrings which are tokens in Dict. Red nodes indicate the
set of “maximal tokens”, which are the set of tokens which the greedy encoder assigns, leaving out
those which can only be assigned as the last token of some string. Tokens like “b” are never assigned
by the greedy encoder (save as the last token of the encoding of a string) since any sufficiently long
trajectory starting with b must have a longer prefix which is also a token, namely, one of ba, bc, bba,
bbb or bbc. The vertices of the tree which are assigned by the greedy encoder as tokens (together with
all their prefixes) forms a cut of the tree, which marks the dotted red line. The heavy hitting property
asserts that this cut is uniformly far away from the root node ∅, and that every vertex s marked red
has P(s) ≤1/dβ."
REFERENCES,0.2639019792648445,"Definition A.5 (β-heavy-hitting dictionary). A token t of a dictionary is said to be maximal if there
exists an arbitrary substring containing t as a strict prefix, and in addition, t is also the largest prefix
of the substring which is a token. A dictionary Dict is said to be β-heavy hitting if the set of maximal
tokens is a subset of {s′ : maxa∈A P(s′|a) ≤1/dβ}."
REFERENCES,0.2648444863336475,"A pictorial depiction of the heavy hitting property is illustrated in Figure 6.
Lemma A.6. For a β-heavy-hitting dictionary, with the greedy encoder, H(QMLE, P) ≥β log(d)."
REFERENCES,0.26578699340245054,"Proof. Note that the greedy encoder assigns tokens only among the set of maximal substrings (save
for potentially the last token). If every maximal substring has maxa∈A P(s|a) ≤1/dβ, by the
heavy-hitting property, for any token t,"
REFERENCES,0.26672950047125354,"P(t) ≤max
a∈A P(s′|a) ≤1/dβ."
REFERENCES,0.26767200754005654,"Therefore,"
REFERENCES,0.26861451460885954,"H(QMLE, P) = Et∼QMLE[log(1/P(t))] ≥β log(d)."
REFERENCES,0.2695570216776626,"Define Mβ = {t : maxa∈A P(t|a) ≥δ/dβ}. These are the set of “high-probability” substrings
under the stochastic source. We will show that for β bounded away from 1, with high probability, every
substring in Mβ is added as a token to the dictionary in a run of the LZW tokenizer (Definition 3.5).
Note that if every substring in Mβ is assigned as a token by LZW, then the algorithm must be
β-heavy hitting since there always exists a maximal token on the “boundary” of the set Mβ which is
strictly contained in {s′ : maxa∈A P(s′|a) ≤1/dβ}."
REFERENCES,0.2704995287464656,Lemma A.7. Every substring in Mβ has length at most ℓ⋆≜δ−1(β log(d) + log(1/δ)).
REFERENCES,0.2714420358152686,"Proof. Note that mina,a′∈A P(a|a′) = δ, which implies that the probability of any transition must
be bounded away from 1, i.e., maxa,a′∈A P(a|a′) ≤1 −δ. This implies that,"
REFERENCES,0.27238454288407166,"max
a∈A P(t|a) ≤(1 −δ)|t| ≤e−δ|t|.
(10)"
REFERENCES,0.27332704995287466,"By definition, for any substring t ∈Mβ, maxa∈A P(t|a) ≥δ/dβ. In conjunction with eq. (10), this
implies the statement of the lemma."
REFERENCES,0.27426955702167766,"In the remainder of this section, let n be the size of the dataset on which LZW is run. We show that
the number of tokens added to the dictionary by LZW, d, is eΘδ(n). Rather than running the algorithm
with early stopping (i.e., ceasing to add new tokens once the budget is hit), instead, we assume that
the algorithm runs on a prefix of the dataset of length d. The number of tokens added this way cannot
exceed d.
Lemma A.8. With probability ≥1 −d−Ω(log(d/δ)/δ), in a run of the LZW algorithm, no substring t
added as a token to the dictionary satisfies |t| ≥ℓmax ≜4 log(d|A|)/δ."
REFERENCES,0.27521206409048066,"Proof. Consider any s ∈N and any substring t of length s. In order for t to be assigned as a token,
each of its prefixes must disjointly appear at least once in the string. Since there are at most d tokens,
we can upper bound the probability that t is assigned as a token as,"
REFERENCES,0.2761545711592837,"P(t is assigned as a token) ≤
d
s 
s
Y"
REFERENCES,0.2770970782280867,"i=1
max
a∈A P(t1:i|a)"
REFERENCES,0.2780395852968897,"(i)
≤
d
s"
REFERENCES,0.27898209236569277,"
(1 −δ)s(s−1)/2"
REFERENCES,0.27992459943449577,"≤es log(d)−δs(s−1)/2,"
REFERENCES,0.28086710650329877,"where (i) uses the fact that maxa∈A P(t1:i) ≤Qi
j=1 maxa∈A P(tj|a) ≤(1 −δ)i. By union
bounding across the |A|s strings of length s,"
REFERENCES,0.28180961357210177,P(any length s string is assigned as a token) ≤es log(|A|)+s log(d)−δs(s−1)/2.
REFERENCES,0.2827521206409048,"When s = 4 log(d|A|)/δ+1 ≜ℓmax +1, the RHS is upper bounded by e−δℓ2
max/4 ≤d−Ω(log(d/δ)/δ).
With the same small probability, no substring of length s′ > s can become a token, since their length-s
prefixes are never assigned as tokens."
REFERENCES,0.2836946277097078,"Corollary A.9. With probability ≥1 −d−Ωδ(log(d)), learns a dictionary with at least d⋆= d/ℓmax
tokens when run on a training sequence of length n drawn from a stochastic source satisfying
Assumption 3.2."
REFERENCES,0.2846371347785108,"Lemma A.10. For any constant β < 1, with probability ≥1 −d−Ω(log(d/δ)/δ) −exp(−eΩδ(d1−β))
over the source dataset, every substring in Mβ is added as a token to the dictionary in a run of the
LZW algorithm. In other words, with the same probability, the LZW tokenizer results in a β-heavy
hitting dictionary."
REFERENCES,0.2855796418473139,"By Corollary A.9, note that with high probability the LZW tokenizer adds at least d⋆tokens to
the dictionary when processing a length d training sequence in entirety. In this proof, instead of
generating d samples, we sequentially sample d⋆tokens from their joint distribution, and generate a
dictionary from these samples. From Corollary A.9, with high probability this results in at most d
samples being generated, implying that the dictionary generated by sampling d⋆tokens is a subset
of the dictionary generated by a full run of the LZW tokenizer. Here, we use the fact that the LZW
tokenizer adds tokens to the dictionary in a left to right fashion, and therefore a subset of the dictionary
learnt by the LZW tokenizer can be generated by processing a portion of the dataset."
REFERENCES,0.2865221489161169,"Next we consider a joint view for generating the dataset from the stochastic source and the dictionary
learnt by LZW simultaneously. The stochastic source is sampled as a sequence of tokens. Suppose
the last character of the previous token was a′. Sample a character a ∼P(·|a′) and an infinite
trajectory on the tree T ⋆
a . Consider the first node visited in this trajectory which does not already"
REFERENCES,0.2874646559849199,"exist as a token in the dictionary. The substring corresponding to this node is added as a token
in the dictionary. By repeating this process, the dictionary and the source dataset are constructed
sequentially and simultaneously. As alluded to before, we truncate this token sampling process to
repeat at most d⋆times, which results in a subset of the dictionary output by the LZW algorithm
with high probability (Corollary A.9). This is simply a variant of the “Poissonization” trick to avoid
statistical dependencies across tokens. Denote the set of infinite trajectories generated on the forest
{T ⋆
a : a ∈A} as {traji : i ∈[d⋆]}."
REFERENCES,0.2884071630537229,"With this view of the sampling process, observe that if the substring t sampled was a prefix of traji at
least |t| times across different values of i, then t must be assigned as a token. In particular, in each of
these |t| trajectories, each of the prefixes of t is assigned as a token. With this observation, the event
that t is not assigned as a token is contained in the event that t is visited at most |t| −1 times across
the d⋆trajectories. Observe that,"
REFERENCES,0.28934967012252594,P(t is not assigned as a token) ≤
REFERENCES,0.29029217719132894,"|t|−1
X i=0 d⋆ i"
REFERENCES,0.29123468426013194,"
max
a∈A(P(t|a))i(1 −P(t|a))d⋆−i."
REFERENCES,0.292177191328935,"Since we aim to upper bound this probability across the substrings in t ∈Mβ, note that (i)
maxa∈A P(t|a) ≥δ/dβ, and (ii) tokens in Mβ have length at most ℓ⋆= δ−1(β log(d) + log(1/δ))
(Lemma A.7), implying there are at most 2|A|ℓ⋆substrings in this set. By union bounding,"
REFERENCES,0.293119698397738,"P(∃t ∈Mβ not assigned as a token) ≤2|A|ℓ⋆
ℓ⋆−1
X i=0 d⋆ i"
REFERENCES,0.294062205466541,"
max
x≥δ/dβ xi(1 −x)d⋆−i.
(11)"
REFERENCES,0.295004712535344,"Case I. For i ≤ℓ⋆and x ≥1/2,"
REFERENCES,0.29594721960414705,"|A|ℓ⋆
d⋆ i"
REFERENCES,0.29688972667295005,"
xi(1 −x)d⋆−i ≤|A|ℓ⋆(d⋆)ℓ⋆ 2d⋆/2"
REFERENCES,0.29783223374175305,≤2ℓ⋆log(d⋆|A|)−d⋆/2
REFERENCES,0.2987747408105561,"≤2−Ωβ,δ(d⋆),
(12)"
REFERENCES,0.2997172478793591,"where the last inequality uses the fact that ℓ⋆= Oβ,δ(log(d))."
REFERENCES,0.3006597549481621,"Case II. For i ≤ℓ⋆and δ/dβ ≤x ≤1/2,"
REFERENCES,0.3016022620169651,"|A|ℓ⋆
d⋆ i"
REFERENCES,0.30254476908576816,"
xi(1 −x)d⋆−i ≤|A|ℓ⋆
d⋆ i"
REFERENCES,0.30348727615457116,"
(1 −x)d⋆"
REFERENCES,0.30442978322337416,≤|A|ℓ⋆(d⋆)ℓ⋆e−d⋆x
REFERENCES,0.3053722902921772,≤eℓ⋆log(|A|)+ℓ⋆log(d⋆)−d⋆x
REFERENCES,0.3063147973609802,≤e−Ω(δ2n/dβ/ log(d/δ))
REFERENCES,0.3072573044297832,"≤e−Ω(δ2d1−β/ log(d/δ)),
(13)"
REFERENCES,0.3081998114985862,"where the last inequality uses the fact that ℓ⋆= O(log(d)), x ≥δ/dβ, d⋆= Ω(dδ/ log(d/δ)). By
combining eq. (12) and eq. (13) with eq. (11) completes the proof, as long as β is a constant bounded
away from 1."
REFERENCES,0.3091423185673893,"Lemma A.11. Fix a constant γ > 0. Then, with probability ≥1 −d−Ωγ,δ(log(d)), none of the
substrings in the set Nγ =

s′ : maxa∈A P(s′|a) ≤δ/d1+γ	
are assigned as tokens in a run of
LZW."
REFERENCES,0.3100848256361923,"Proof. Define the following set of substrings,"
REFERENCES,0.3110273327049953,"Sγ =

t : δ/d1+γ/2 ≤max
a∈A P(t|a) ≤1/d1+γ/2
"
REFERENCES,0.31196983977379833,"Since the width of this band is sufficiently large, by Assumption 3.2 every substring t such that
maxa∈A P(t|a) ≤δ/d1+γ/2 has at least one prefix which falls in Sγ, and denote the longest such"
REFERENCES,0.31291234684260133,"prefix tγ. Define Tγ = {tγ : t ∈Nγ} as the set of longest prefixes in Sγ. Intuitively, if we think of
the strings in Sγ (or Tγ) as being intermediate in length, the strings in Nγ can be thought of as being
particularly long: the value of maxa∈A P(t|a) for any t ∈Tγ and for any t ∈Nγ are separated by a
factor of at least 1/dγ/2. In particular, since the probability of any character is lower bounded by
δ, each substring in t ∈Nγ must be at least ∆=
γ log(d)
2 log(1/δ) symbols longer than its corresponding
longest prefix in Tγ, tγ. An implication of this is that for t to be assigned as a token, tγ must
be observed at least ∆+ 1 times disjointly in s. However, note that tγ already has low marginal
probability to begin with (≪1/d) so the odds of seeing this substring so many times disjointly is
very small. Furthermore, note that Tγ has at most d1+γ/2/δ substrings, which allows the probability
of this event occurring simultaneously across all substrings in Tγ to be controlled by union bound.
Under this condition, none of the substrings in Nγ are made into tokens."
REFERENCES,0.31385485391140433,"In order to argue that the dictionary does not contain certain tokens, we may argue this property about
any superset of the dictionary. In contrast, in Lemma A.10, we construct a subset of the dictionary by
running LZW on the concatenation of d⋆tokens sampled from their joint distribution. The superset
we consider here is just to sample d tokens from their joint distribution and concatenate them together
to result in a string of length ≥d, and running LZW on this sequence (which simply would result
in these d tokens). As in Lemma A.10, let {traji : i ∈[d]} denote the infinite trajectories generated
from the Markov chain which are truncated to result in tokens. A sufficient condition for the event
that no substring t ∈Nγ is assigned as a token by LZW is to the event that every substring t′ ∈Tγ is
observed as a prefix of traji for ∆or fewer choices of i ∈[d]. To this end define E(t′) as the event
that |i ∈[d] : t′ is a prefix of traji| ≤∆. Then,"
REFERENCES,0.31479736098020733,"Pr(E(t′)) ≤
n
∆"
REFERENCES,0.3157398680490104,"
(max
a∈A P(t′|a))∆"
REFERENCES,0.3166823751178134,"(i)
≤e∆log(n)

1
d1+γ/2 ∆ ≤e−γ"
REFERENCES,0.3176248821866164,"2 ∆log(d),
(14)"
REFERENCES,0.31856738925541944,where (i) uses the fact that maxa∈A P(t′|a) ≤1/d1+γ/2 since the substring t′ belongs to Tγ.
REFERENCES,0.31950989632422244,"Note that the number of substrings in Sγ (and by extension, Tγ) is at most Oδ(d1+γ/2). Recall that
these substrings satisfy the condition maxa∈A P(t|a) ≥δ/d1+γ/2. Observe that,"
REFERENCES,0.32045240339302544,"δ|Sγ|
d1+γ/2 ≤
X"
REFERENCES,0.32139491046182844,"t∈Sγ
max
a∈A P(t|a) ≤
X t∈Sγ X"
REFERENCES,0.3223374175306315,"a∈A
P(t|a) ≤|A| ≤1 δ ."
REFERENCES,0.3232799245994345,"This implies that there are at most d1+γ/2/δ2 substrings in Sγ. Finally, in conjunction with eq. (14),"
REFERENCES,0.3242224316682375,P(∃t′ ∈Sγ : E(t′)) ≤d1+γ/2
REFERENCES,0.32516493873704055,"δ2
e−γ"
REFERENCES,0.32610744580584355,"2 ∆log(d) = d−Ωγ,δ(log(d)),"
REFERENCES,0.32704995287464655,"which implies that with high probability, no token in Sγ is observed as a prefix of si for more than ∆
choices of the index i ∈[d]. Under this event, no substring in Nγ is assigned as a token."
REFERENCES,0.32799245994344955,"A.6.1
Proof of Theorem 3.6"
REFERENCES,0.3289349670122526,"Choosing β = 0.99 in Lemma A.10, with probability ≥1 −d−Ωδ(log(d)), the LZW tokenizer results
in a 0.99-heavy-hitting dictionary. As a consequence of Lemma A.6, this implies that under the same
event,"
REFERENCES,0.3298774740810556,"H(QMLE, P) ≥0.99 log(d)."
REFERENCES,0.3308199811498586,"Finally, combining with Theorem 3.4 completes the proof."
REFERENCES,0.33176248821866167,"B
Additional Theoretical Results I: A sequential variant of BPE"
REFERENCES,0.33270499528746467,"While the main results in the paper focused on understanding the limits of tokenization under a bound
on the dictionary size, in this section we take a more practical look and try to analyze tokenizers
used commonly in practice. The Byte-Pair-Encoding (BPE) algorithm (Gage, 1994; Sennrich et al.,
2016), discovered in the compression literature as REPAIR (Larsson and Moffat, 2000; Navarro and
Russo, 2008) was proposed as a faster alternative to LZW. It remains as one of the most commonly
implemented tokenizers in natural language processing for various downstream tasks (Radford et al.,
2019; Mann et al., 2020; Touvron et al., 2023). A large proportion of open source and commercial
LLMs currently use BPE as the tokenization algorithm of choice, such as GPT-2/3, Llama 1/2 and
Mistral-7B to name a few."
REFERENCES,0.33364750235626767,"The BPE algorithm is based on constructing the dictionary iteratively by merging pairs of tokens to
result in a tokens. In each iteration, the pair of tokens which appear most frequently next to each
other are merged together into a single token. Subsequently, every occurrence of the pair of tokens
are replaced by the newly added token, breaking ties arbitrarily. The dictionary is thus an ordered
mapping of the form t ←(t′, t′′). To encode a new string, the BPE encoder iterates through the
dictionary and for each rule t ←(t′, t′′) replaces every consecutive occurrence of t′ and t′′ by the
token t breaking ties arbitrarily."
REFERENCES,0.33459000942507067,"To warm up our main results, it is worth understanding the behavior of the BPE tokenizer in a bit more
detail. Unlike the toy tokenizer, it is a priori unclear whether unigram models trained on sequences
tokenized by BPE even asymptotically (in the dictionary size) achieve the optimal cross-entropy loss.
Indeed, for δ > 0, consider a training sequence of length m of the form, s = "
REFERENCES,0.3355325164938737,"
01 · · · 01
|
{z
}
2/δ
10 · · · 10
|
{z
}
2/δ  
"
REFERENCES,0.3364750235626767,"|
{z
}
× mδ 4 (15)"
REFERENCES,0.3374175306314797,"The probability that this sequence is generated by the order-2 switching Markov source with p = q =
δ is,"
REFERENCES,0.3383600377002828,"≈(1 −δ)
mδ 4 × 4"
REFERENCES,0.3393025447690858,"δ ×(1−δ)(δ)
mδ"
REFERENCES,0.3402450518378888,"4 ×4 = e−H(P ),"
REFERENCES,0.3411875589066918,"which uses the fact that H(P) = mδ log(1/δ) + m(1 −δ) log(1/(1 −δ)). This implies that even
though the string has exponentially small probability, it is one of the typical sequences for this
order-2 Markov source. Let’s understand what happens when the BPE tokenizer is trained on this
dataset. Assuming that ties are broken arbitrarily, consider the run of the BPE algorithm detailed
in Table 2. Here, we assume that 1/δ −1 is a power of 2 and denote r = log2(1/δ −1). The
algorithm first merges 0 and 1 into a single token t1, which results in a long sequence of the form
t1 · · · · · · t11t1 · · · · · · t10 repeated mδ/4 times. In subsequent rounds, the tokens (t1, t1) is merged
into t2, then (t2, t2) is merged into t3 and so on, until is no longer possible. Finally, the resulting
sequence is a repeating sequence of 5 tokens where within each sequence, no pair of tokens appears
more than once next to each other. Eventually these 5 tokens are merged into a single token labelled
tr+4, and in subsequent rounds the tokens (tr+4, tr+4) are merged into tr+5, (tr+5, tr+5) is merged
into tr+6 and so on, until is no longer possible."
REFERENCES,0.34213006597549483,"Observe that in the initial training dataset the substrings 0000 and 1111 never appears as a contiguous
sequence. However, in a test sequence of length m sampled from the 2nd-order Markov source, with
high probability these substrings disjointly occur Θ(m) times each. The learnt dictionary associates
each such disjoint occurrence of these substrings with at least 1 token, for 0000, the 3rd 0 must
necessarily be tokenized as the token “0”. Likewise, in 1111, the 3rd 1 must necessarily be tokenized
as the token “1”. Therefore, when a new test string of length m is tokenized, with high probability
the tokens “0” and “1” form a constant fraction of the total collection of tokens."
REFERENCES,0.34307257304429783,"Thus on freshly sampled test sequences, the BPE tokenizer appears to behave like the character-level
tokenizer on a constant fraction of the input sequence. In particular, a simple calculation shows that
the cross-entropy loss of any unigram model trained on this tokenizer must be far from the optimal"
REFERENCES,0.34401508011310084,"Initial
01 · · · · · · 0110 · · · · · · 10| · · ·
t1 ←(0, 1)
t1 · · · · · · t11t1 · · · · · · t10| · · ·
t2 ←(t1, t1)
t2 · · · t21t2 · · · t20| · · ·
...
...
tr ←(tr−1, tr−1)
trt11tr0| · · ·
tr+1 ←(tr, t1)
tr+11tr0| · · ·
tr+2 ←(tr, 0)
tr+11tr+2| · · ·
tr+3 ←(tr+1, 1)
tr+3tr+2| · · ·
tr+4 ←(tr+3, tr+2)
tr+4| · · ·
tr+5 ←(tr+4, tr+4)
tr+5| · · ·
tr+6 ←(tr+5, tr+5)
tr+6| · · · .
... ..."
REFERENCES,0.3449575871819039,"Table 2: A representation of the behavior of BPE when trained on the dataset in eq. (15). We assume
that 1/δ −1 is a power of 2 and define r = log2(1/δ −1)."
REFERENCES,0.3459000942507069,"bound of mHBER(δ) especially as δ becomes smaller,"
REFERENCES,0.3468426013195099,"min
Q∈Q1-gram Lm(Q ◦enc(·))"
REFERENCES,0.3477851083883129,"≥
min
Q∈Q1-gram E

n0 log(1/Qtok(0) + n1 log(1/Qtok(1)
"
REFERENCES,0.34872761545711595,"(i)
≥Ω(m) ·
min
Q∈Q1-gram"
REFERENCES,0.34967012252591895," 
log(1/Ptok(0) + log(1/Qtok(1)
"
REFERENCES,0.35061262959472195,≥Ω(m).
REFERENCES,0.351555136663525,"where (i) uses the fact that E[n0], E[n1] ∈Ω(m) and the last inequality uses Ptok(0)Ptok(1) ≤1/4
(AM-GM inequality) since they sum up to at most 1. The purpose of this example is to show that
there exist pathological training datasets which appear to be drawn from a stochastic source, but on
which BPE fails to learn a good dictionary for the source. Thus proving a result such as Theorem 3.1
for BPE would require arguing that training datasets such as that in eq. (15) are unlikely to be seen."
REFERENCES,0.352497643732328,"The analysis of the standard variant of BPE turns out to be complicated for other reasons too. After
every token is added the training dataset becomes a mix of all the previously added tokens, and
arguing about the statistics of which pair of tokens appears most frequently for the next iteration
becomes involved. For instance, adding 00 as a token may reduce the frequency of occurrence of
the substring 01, but will not affect 11. Thus, even though 01 may a priori have been seen more
frequently, it may not be chosen by BPE as the next token after 00."
REFERENCES,0.353440150801131,"To avoid dealing with these issues, we consider a sequential/sample-splitting variant of BPE. At a
high level, the algorithm breaks down a dataset of size Θ(d2) into d chunks and learns at most 1
token from each chunk. The algorithm iterates over the chunks and finding the pair of tokens which
appear most frequently next to each other in each chunk and adding it to the dictionary if it appears
more than log(d) times. Every consecutive occurrence of the pair of tokens is replaced by the newly
assigned token in the dataset. Thus, in each iteration i, at most 1 token is added, depending on the
statistics of the ith chunk and the tokens added so far to the dictionary. Based on the final size of the
dictionary a different encoder/decoder pair is used - if the algorithm adds sufficiently many tokens to
the dictionary, the greedy encoder is used, and if not, a parallel implementation of BPE’s encoding
algorithm is used (Definition B.1). A formal description of the algorithm is in Algorithm 1."
REFERENCES,0.354382657869934,"Definition B.1 (BPE.split encoder). The BPE.split encoder parses a new string into tokens as follows.
The algorithm partitions the string into contiguous chunks of length d. Then, BPE’s encoder is applied
on each chunk, which iterates through DS and replaces t′t′′ by t for every rule t ←(t′, t′′) in DS,
breaking ties arbitrarily. The individual token sequences are finally spliced together and returned."
REFERENCES,0.35532516493873706,"The main result of this section is that up to a small additive error, Algorithm 1 approaches a 2-
approximation to the optimal cross-entropy loss."
REFERENCES,0.35626767200754006,Algorithm 1 Sequential implementation of BPE
REFERENCES,0.35721017907634306,"Input: ϵ ∈(0, 1); a dataset of size n = Θ(d2), split into d contiguous texts {text1, · · · , textd} of
length Θ(d) each.
Output: A tokenizer T .
// Generate Dictionary
for i = 1, · · · , d do"
REFERENCES,0.3581526861451461,"if ∃a pair of tokens/characters (t′, t′′) appearing ≥log(d) times consecutively in texti then"
REFERENCES,0.3590951932139491,"Append the rule t ←(t′, t′′) to DS
for j = i + 1, · · · , d do"
REFERENCES,0.3600377002827521,"textj ←APPLYt←(t′,t′′)(textj);
// Can be implemented in parallel"
REFERENCES,0.3609802073515551,"end for
end if
end for"
REFERENCES,0.36192271442035817,"// Encoder and Decoder
if |Dict| < d0 ≜ϵd/2 log(4|A|) then"
REFERENCES,0.36286522148916117,"T ←(Dict, DS, encBPE.split(·), decBPE.split(·))
else"
REFERENCES,0.36380772855796417,"T ←(Dict, ∅, encgre(·), decgre(·))
end if"
REFERENCES,0.3647502356267672,"def APPLYt←(t1,t2)(text):
Replace every consecutive occurrence of (t′, t′′) in text by t, breaking ties arbitrarily."
REFERENCES,0.3656927426955702,"Theorem B.2. For any ϵ ∈(0, 1), run Algorithm 1 on a dataset of n = Θ(d2) characters to learn a
dictionary with at most d tokens. The resulting tokenizer T satisfies with probability ≥1 −e−Ω(dϵ2),"
REFERENCES,0.36663524976437323,"min
Q∈Q1-gram L(Q ◦enc(·)) ≤(2 + ε) min
Q L(Q) + ϵ"
REFERENCES,0.36757775683317623,"where ε = O

log(1/ϵ) log3(1/δ)"
REFERENCES,0.3685202639019793,"ϵδ9 log(d)

."
REFERENCES,0.3694627709707823,"While the guarantees established for the sequential BPE tokenizer are weaker than those in Theo-
rem 3.1, the analysis turns out to be quite involved. Theorem B.2 implies that unigram models trained
on the sequential BPE tokenizer asymptotically approach the optimal cross-entropy loss up to a factor
of 2."
REFERENCES,0.3704052780395853,"The formal proof of this result is presented in Appendix B. What is the intuition behind using a
different encoder in Algorithm 1 depending on the number of tokens in the dictionary? When the
number of tokens in the dictionary is smaller than d0, we know that on a 1 −d0/d fraction of the
iterations of Algorithm 1, a token is not added to the dictionary, i.e. every pair of tokens already
appears at most log(d) times together. This is a datapoint of “evidence” that under the dictionary in
that iteration, the BPE encoder is already good at encoding new strings (of length Θ(d)) in a way
where pairs of tokens do not appear consecutively with high frequency. Since future dictionaries
only have more rules appended to them, dictionaries only get better at encoding new strings into
tokens where pairs do not frequently appear consecutively. In other words, the BPE encoder satisfies
a monotonicity property. It remains to show that dictionaries which encode new sequences in a way
where no pair of tokens appear too frequently have large H(QMLE, P) (to invoke Theorem 3.4). This
follows from ideas introduced in (Navarro and Russo, 2008)."
REFERENCES,0.37134778510838834,"The case where the number of tokens is large (≥d0) turns out to present significant technical
challenges for analyzing the BPE encoder. There is no longer much “evidence” that the dictionary in
each iteration is good at encoding strings since in a large number of iterations a pair of tokens appear
consecutively with high frequency. Analyzing the greedy encoder also presents its own challenges
- although the algorithm has allocated a large number of tokens, it is possible that there are short
tokens t which are maximal (i.e. they are not prefixes of other tokens). This is similar to the problem
encountered by BPE when trained on the dataset in eq. (15) - although the algorithm has allocated a
large number of tokens, the token 1 is maximal since every other token begins with the character 0."
REFERENCES,0.37229029217719134,"However, it turns out that such tokens, although present in the dictionary, are not observed frequently
while encoding a fresh string drawn from the source."
REFERENCES,0.37323279924599434,"B.1
Analysis of Algorithm 1"
REFERENCES,0.37417530631479734,"In this section we prove a rephrased version of Theorem B.2 which implies the statement in the main
paper. Define d0 =
ϵd
2 log(4|A|)."
REFERENCES,0.3751178133836004,"Theorem B.3 (Rephrased Theorem B.2). Run Algorithm 1 on a dataset of n = Θ(d2) characters to
learn a dictionary with at most d tokens. The resulting tokenizer T satisfies one of the following 3
conditions,"
REFERENCES,0.3760603204524034,"1. Either, |Dict| > d0, and,"
REFERENCES,0.3770028275212064,"min
Q∈Q1-gram L(Q ◦enc(·)) ≤H∞ 1 −ε."
REFERENCES,0.37794533459000945,"Here, ε = O

log3(1/δ) log(1/ϵ)"
REFERENCES,0.37888784165881245,"ϵδ9 log(d)

."
REFERENCES,0.37983034872761545,"2. Pr(|Dict| < d0) = e−Ω(ϵ2d/ log2(1/δ)), or,"
REFERENCES,0.38077285579641845,"3. Conditional on |Dict| < d0, with probability ≥1 −e−Ω(ϵ2d/ log2(1/δ)),"
REFERENCES,0.3817153628652215,"min
Q∈Q1-gram L(Q ◦enc(·)) ≤

1 −2d0 d "
REFERENCES,0.3826578699340245,"2H∞+ O

1
log(d) ! + 2d0"
REFERENCES,0.3836003770028275,d log(4|A|).
REFERENCES,0.38454288407163056,With the choice of d0 = ϵd/2 log(4|A|) we get the statement of Theorem B.2.
REFERENCES,0.38548539114043356,"B.2
Analysis for the large dictionary case: |Dict| > d0"
REFERENCES,0.38642789820923656,"In the large dictionary case, Algorithm 1 uses the greedy encoder/decoder pair in conjunction with
the dictionary. The proof of Theorem B.2 relies on establishing that the cross-entropy H(QMLE, P)
of the tokenizer is large. Namely, we prove that,"
REFERENCES,0.38737040527803956,"Lemma B.4. In Algorithm 1, assuming at least d0 tokens are allocated,"
REFERENCES,0.3883129123468426,"H(QMLE, P) = Ω"
REFERENCES,0.3892554194156456,"ϵδ9 log(d)
log(1/ϵ) log3(1/δ) ! ."
REFERENCES,0.3901979264844486,"To show this, it suffices to argue that conditioned on any previous set of tokens, with nontrivial
probability over the underlying string generated from the stochastic source, the next token is long (i.e.
having conditional probability at most O(1/
√ d))."
REFERENCES,0.3911404335532517,"Lemma B.5. Suppose that in a run of Algorithm 1, at least d0 tokens are allocated. Suppose a set of
tokens t1, · · · , tk have been sampled so far by the greedy encoder. Let Ti+1 be the random variable
which denotes the next token returned by the greedy encoder, where the randomness comes from the
underlying string being tokenized. Then,"
REFERENCES,0.3920829406220547,"Pr

P(Ti+1|ti) ≤1/
√"
REFERENCES,0.3930254476908577,"Cδd
t1, · · · , ti

≥
d0δ6(1 −δ)2"
REFERENCES,0.3939679547596607,"8Cd∆|A| log(2|A|)nD
= Ω ϵδ9"
REFERENCES,0.39491046182846373,log3(1/δ) log(1/ϵ) !
REFERENCES,0.39585296889726673,"Proof sketch of Lemma B.5. The proof will proceed in 2 parts. We first show in Lemma B.9 that
there is a set Dvalid of Ω(d) tokens in the dictionary which are neither prefixes nor suffixes of any
other token in Dict. The reason for considering this set of tokens is twofold,"
REFERENCES,0.39679547596606973,"1. Irrespective of what the previous set of tokens were, it is legal for a token Dvalid to be
sampled in the current step by the greedy encoder, since for any candidate t ∈Dvalid, by
definition, tj · · · tit ̸∈Dict for every j ≤i."
REFERENCES,0.3977379830348728,"2. Suppose a sequence of tokens t1, · · · , ti have already been sampled, ending with the
character a. Then, we may sample the next token using rejection sampling. Sample
a′ ∼P(·|a) and an infinitely long trajectory on T ⋆
a′. Return the last token on this trajectory
which belongs to Dict, and if it so happens that ∃j ∈[i] such that tj · · · tit ∈Dict, then
reject this trajectory and repeat. Since all the tokens in Dvalid are not prefixes of another
token, any trajectory which reaches a token in Dvalid must terminate the rejection sampling
process."
REFERENCES,0.3986804901036758,"Next, in Lemma B.10, we show that since the number of tokens in Dvalid is sufficiently large, Ω(d),
with constant (in d) probability, a trajectory rolled out in the first round of the rejection sampling
process will reach a token t ∈Dvalid which has small probability, i.e. maxa∈A P(t|a) ≤1/poly(d).
By the previous arguments, this must mean that the rejection sampling process terminates on this
“low probability” token, resulting in the statement of the lemma."
REFERENCES,0.3996229971724788,"Figure 7: Jointly generating a sequence and its greedy encoding: In this example we use the greedy
encoder under the dictionary composed of all the substrings shadowed red. The first character (a) is
sampled from the stationary distribution. Then an infinite string is sampled on the tree with a as root
(green path). The last substring on this path which is a token (t1 = abb) is returned by the greedy
encoder. Then the next character x = b is sampled from the source conditioned on the previous
character (b) and further conditioned on t1x ̸∈Dict. Finally, another infinite string is sampled on the
tree with x = b as root (purple path) and the last substring on this path which is a token (t2 = ba) is
returned by the greedy encoder. Repeating this process, we can generate a string, here, abbba · · · , as
well as its greedy encoding, (abb, ba, · · · )."
REFERENCES,0.4005655042412818,"Proof of Theorem B.3.1 and Lemma B.4
It is easy to see why Lemma B.5 implies a lower bound
on the cross entropy H(QMLE, P) of the tokenizer. By Lemma A.4 for the greedy encoder,"
REFERENCES,0.40150801131008484,"QMLE(t) = lim
m→∞E"
REFERENCES,0.40245051837888784,"""
nt
P"
REFERENCES,0.40339302544769085,t′ nt′ #
REFERENCES,0.4043355325164939,"a.s.
=
lim
m→∞
nt
P"
REFERENCES,0.4052780395852969,"t′ nt′ .
(16)"
REFERENCES,0.4062205466540999,"Since the limit m →∞of the RHS exists by Lemma A.4, we may let m →∞in any way we
like, and in particular we may simply sample i⋆tokens, t1, · · · , ti⋆sequentially according to the
process in Figure 7. Here, the first token sampled is returned by generating an infinitely long string
on T ⋆
a where a ∼π and then truncating this trajectory to the longest token which belongs to Dict.
Subsequently for every i > 1, ti is generated by sampling a fresh infinitely long string from T ⋆
a
where a is sampled from the P(·|a′) where a′ is the last character ti−1 and then returning the largest
prefix of this string which is a token in Dict, conditioned on tj · · · ti−1ti ̸∈Dict for any j < i."
REFERENCES,0.4071630537229029,and concatenate the corresponding substrings to get an m = Pi⋆
REFERENCES,0.40810556079170596,"i=1 |ti| length character string. Letting
i⋆→∞, we must have m →∞surely since m ≥i⋆. In this view, eq. (16) can be rewritten as,"
REFERENCES,0.40904806786050896,"QMLE(t) = lim
i⋆→∞
nt"
REFERENCES,0.40999057492931196,"i⋆= lim
i⋆→∞
1
i⋆ i⋆
X"
REFERENCES,0.410933081998115,"i=1
I(ti = t)
a.s.
=
lim
i⋆→∞
1
i⋆ i⋆
X"
REFERENCES,0.411875589066918,"i=1
E[I(ti = t)|t1, · · · , ti−1]
(17)"
REFERENCES,0.412818096135721,"where the last inequality follows by the sequential nature of the token sampling process and a
martingale argument. Consider the set of tokens T such that t ∈T satisfies maxa∈A P(t|a) ≤
p"
REFERENCES,0.413760603204524,"1/Cδ3d. From eq. (17), summing across t ∈T, we have that, X"
REFERENCES,0.41470311027332707,"t∈T
QMLE(t)
a.s.
=
lim
i⋆→∞
1
i⋆ i⋆
X"
REFERENCES,0.41564561734213007,"i=1
Pr (ti ∈T|t1, · · · , ti−1) = Ω ϵδ9"
REFERENCES,0.41658812441093307,log3(1/δ) log(1/ϵ) ! (18)
REFERENCES,0.4175306314797361,"where in the last inequality, we use Lemma B.5 and the fact that δ maxa∈A P(t|a) ≥mina∈A P(t|a).
Therefore,"
REFERENCES,0.4184731385485391,"H(QMLE, P) ≥
X"
REFERENCES,0.4194156456173421,"t∈T
QMLE(t) log(1/P(t)) ≥
X"
REFERENCES,0.4203581526861451,"t∈T
QMLE(t) log(
√ Cδ3d)"
REFERENCES,0.4213006597549482,"where in (i) we use the fact that for t ∈T, maxa∈A P(t|a) ≤1/
√"
REFERENCES,0.4222431668237512,"Cδ3d, which implies that
P(t) ≤1/
√"
REFERENCES,0.4231856738925542,"Cδ3d. Finally, combining with eq. (18) completes the proof of Lemma B.4. Furthermore,
since the cross-entropy H(QMLE, P) was established to be large, by invoking the reduction in
Theorem 3.4, we complete the proof of Theorem B.3.1."
REFERENCES,0.42412818096135724,"Figure 8: The circled nodes indicate substrings which are tokens in Dict. The red boundary is the
set of substrings t such that maxa∈A P(t|a) ≥1/Cd. By Lemma B.8, none of the nodes which fall
outside this boundary are assigned as tokens in a run of Algorithm 1. The set of circled substrings
are the set of tokens in Dict. Among them, the ones circled green are the tokens in Dvalid, which are
not prefixes or suffixes of any other tokens in Dict. Substrings such as cb or ba which are tokens
in Dict do not belong to Dvalid because they are prefixes of longer tokens (in this case, cbb and bab
respectively). On the other hand, substrings like ab do not belong to Dvalid since they are suffixes of
tokens in Dict, in this case, bab. Lemma B.9 asserts that the number of tokens in Dvalid are Ω(d) in
number, assuming that Dict has Ω(d) tokens to begin with."
REFERENCES,0.42507068803016024,"Notation.
For each a ∈A and j ∈N ∪{0}, define a level set of substrings,"
REFERENCES,0.42601319509896324,"Sa
j =
n
(1 −δ)j+1 < P(t|t1 = a) ≤(1 −δ)jo"
REFERENCES,0.42695570216776624,Dictionary ...
REFERENCES,0.4278982092365693,"t1 ←(·, ·)"
REFERENCES,0.4288407163053723,"t2 ←(·, ·) ..."
REFERENCES,0.4297832233741753,"t′ ←(·, ·) ..."
REFERENCES,0.43072573044297835,"t′′ ←(·, ·)"
REFERENCES,0.43166823751178135,"· · ·
s′
· · ·
s′
· · ·"
REFERENCES,0.43261074458058435,"s1s2s1 · · · s7
s1s2s1 · · · s7"
REFERENCES,0.43355325164938735,"· · · t1 · · · t1 · · ·
· · · t1 · · · t1 · · ·"
REFERENCES,0.4344957587181904,"· · · t1t2 · · · t1 · · · t2 · · ·
· · · t1t2 · · · t1 · · · t2 · · · t′
t′ t′′"
REFERENCES,0.4354382657869934,"at each step, both
copies of s′ encode
to the same
sequence of tokens"
REFERENCES,0.4363807728557964,"suppose it
encodes to t′
suppose it
encodes to t′′
=⇒"
REFERENCES,0.43732327992459946,"At each iteration,
both copies of s′
perfectly encode to
a sequence of tokens"
REFERENCES,0.43826578699340246,"both copies of s′
map to the same
token at this step,
a contradiction"
REFERENCES,0.43920829406220546,Figure 9: A pictorial representation of the proof of Lemma B.6
REFERENCES,0.44015080113100846,"where t1 denotes the first character of t. And likewise, define the sets Sj = ∪a∈ASa
j , Sa
≤j and Sa
≥j
as the union of Sa
j′ over j′ ≥j, j′ ≤j and S≤j and S≥j as the union of Sa
≤j and Sa
≥j over a ∈A.
Furthermore for a large universal constant C > 0, define parameters,"
REFERENCES,0.4410933081998115,"∆=
log(δ)
log(1 −δ) ≍Θ
log(1/δ) δ"
REFERENCES,0.4420358152686145,"
;
nD = 1 −2 log(4Cd/δd0)"
REFERENCES,0.4429783223374175,"log(1 −δ)
≍Θ
log(1/ϵδ) δ"
REFERENCES,0.4439208294062206,"
.
(19)"
REFERENCES,0.4448633364750236,"We first begin by stating a folklore result: every pair of tokens assigned by a merging-based dictionary
generation algorithm have distinct character representations.
Lemma B.6. If Algorithm 1 assigns a new token in some round, it’s character representation must
be distinct from that of all previously assigned tokens."
REFERENCES,0.4458058435438266,"Proof. A pictorial proof is in Figure 9. We will prove this result by contradiction. Suppose t and t′
are tokens which decode to the same character substring, s′. Consider all occurrences of s′ in the
dataset which in some iteration encode into t′ or t′′, and denote these disjoint locations S. Recall that
at these locations, s′ eventually is assumed to map to a singular token t′ or t′′. Therefore, at every
step in the merging process these occurrences of s′ must perfectly map to a sequence of tokens."
REFERENCES,0.4467483506126296,"Now consider the merging process at the first time before any of the rules corresponding to tokens in
t′ or t′′ are implemented. Prior to this time, all the occurrences of s′ corresponding to the locations
in S have not been tokenized yet. When the first rule corresponding to one of the tokens in {t′, t′′} is
implemented, all the strings in S must be modified identically. This uses the fact that we can isolate
each of these occurrences of s′ while carrying out the merging process, since each location must be
distinct. At every step, the encodings of these copies of s′ must be the same, and therefore t′ and t′′
cannot be two distinct tokens."
REFERENCES,0.44769085768143263,"Lemma B.7. The size of the level set Sa
j is bounded by (1 −δ)−(j+1)."
REFERENCES,0.44863336475023563,"Proof. Since the probability of any transition is at most 1 −δ, this implies that any infinite trajectory
on the tree T ⋆
a can intersect at most one vertex in Sa
j . Therefore, P"
REFERENCES,0.44957587181903863,"t∈Sa
j P(t|t1 = a) ≤1. By the
lower bound on P(t|t1 = a) for t ∈Sa
j , this implies the statement of the lemma."
REFERENCES,0.4505183788878417,"Next we show that with high probability none of the substrings t having probability mass (under P)
of at most δ/Cd conditioned on the first character, are assigned as tokens by Algorithm 1.
Lemma B.8. In a run of Algorithm 1, for a sufficiently large constant C > 0, with probability
d−Ω(1)poly(1/δ) all assigned tokens t ∈Dict satisfy maxa∈A P(t|a) ≥1/Cd. In other words, none
of the substrings in S≥j⋆are added as tokens to the dictionary in a run of Algorithm 1, where,"
REFERENCES,0.4514608859566447,j⋆≜log(δ/Cd)/ log(1 −δ)
REFERENCES,0.4524033930254477,"Proof. Consider some j ≥j⋆and a ∈A and substring t ∈Sa
j . In the ith stage of the algorithm
where texti is being processed, for t to be assigned as a token, at the very least, t must appear at least
log(d) times disjointly in texti. Therefore,"
REFERENCES,0.4533459000942507,"P(t ∈Sa
j is assigned as a token in texti) ≤

d
log(d)"
REFERENCES,0.45428840716305374," 
max
a∈A P(t|a)
log(d)"
REFERENCES,0.45523091423185674,"≤dlog(d)
 1"
REFERENCES,0.45617342130065974,Cd(1 −δ)j−j⋆log(d)
REFERENCES,0.4571159283694628,≤d−log(C)(1 −δ)(j−j⋆) log(d)
REFERENCES,0.4580584354382658,"Union bounding over Sa
j over j ≥j⋆using the bound on |Sa
j | in Lemma B.7, and over a ∈A and
i ∈[d] results in the bound,"
REFERENCES,0.4590009425070688,P(t ∈S≥j⋆is assigned as a token in step i for some i ∈[d]) ≤d−Ω(1) X j≥j⋆
REFERENCES,0.4599434495758718,(1 −δ)(j−j⋆) log(d)
REFERENCES,0.46088595664467485,"(1 −δ)j+1
≤d−Ω(1)"
REFERENCES,0.46182846371347785,δ(1 −δ)
REFERENCES,0.46277097078228085,"Lemma B.9. Consider the set of tokens Dvalid which are not a prefix or a suffix of any other token
in Dict. That is, Dvalid = {t ∈Dict :̸ ∃s : st ∈Dict} ∩{t ∈Dict :̸ ∃s : ts ∈Dict}. If |Dict| ≥d0,
then,"
REFERENCES,0.4637134778510839,"|Dvalid| ≥
d0
4nD
."
REFERENCES,0.4646559849198869,where nD is defined in eq. (19).
REFERENCES,0.4655984919886899,"Proof. For any token t ∈Dvalid, there may be at most 2|t| tokens which are suffixes or prefixes of it
and belong to Dict. More importantly, every token in Dict not belonging to Dvalid must either be a
prefix or a suffix of some token in Dvalid. Split the suffixes and prefixes of the tokens in Dvalid into
four sets,"
REFERENCES,0.4665409990574929,"1. Ssuff,min = S"
REFERENCES,0.46748350612629597,"t∈Dvalid{t′ ∈Dict : t′ ∈suff(t), |t′| ≤|t| −nD},"
REFERENCES,0.46842601319509897,"2. Ssuff,max = S"
REFERENCES,0.46936852026390197,"t∈Dvalid{t′ ∈Dict : t′ ∈suff(t), |t′| > |t| −nD},"
REFERENCES,0.470311027332705,"3. Spre,min = S"
REFERENCES,0.471253534401508,"t∈Dvalid{t′ ∈Dict : t′ ∈pre(t), |t′| ≤|t| −nD},"
REFERENCES,0.472196041470311,"4. Spre,max = S"
REFERENCES,0.473138548539114,"t∈Dvalid{t′ ∈Dict : t′ ∈pre(t), |t′| > |t| −nD}."
REFERENCES,0.4740810556079171,"where nD is defined in eq. (19).
Note from Lemma B.8 that all the tokens t ∈Dict all
satisfy maxa∈A P(t|a) ≥1/Cd.
Therefore, the tokens in Spre,min and Ssuff,min all satisfy,
maxa∈A P(t|a) ≥d/C(1 −δ)nD. By summing Lemma B.7 over appropriate j, we get that
|Spre,min| + |Ssuff,min| ≤2Cd(1 −δ)nD−1/δ."
REFERENCES,0.4750235626767201,"On the other hand, corresponding to any t ∈Dvalid, there are at most nD tokens in Spre,max or Ssuff,max
and and therefore |Spre,max|, |Ssuff,max| ≤nD · |Dvalid|. Since every token in Dict either belongs to
Dvalid or is a suffix of some token in Dvalid, Spre,min ∪Spre,max ∪Ssuff,min ∪Ssuff,max = |Dict| and,"
REFERENCES,0.4759660697455231,"2nD · |Dvalid| + 2C(1 −δ)nD−1d δ
≥d0"
REFERENCES,0.47690857681432614,Recalling the choice of nD = 1 −2 log(4Cd/δd0)
REFERENCES,0.47785108388312914,"log(1−δ)
, we get that,"
REFERENCES,0.47879359095193214,"|Dvalid| ≥
d0
4nD
."
REFERENCES,0.47973609802073514,"Lemma B.10. Suppose Algorithm 1 assigns at least d0 tokens. For any character a ∈A, sample an
a′ ∼P(·|a) and an infinite trajectory on the tree T ⋆
a′, denoted traj. Then,"
REFERENCES,0.4806786050895382,"Ea′∼P (·|a) """
REFERENCES,0.4816211121583412,"Pr
traj∼T ⋆
a′"
REFERENCES,0.4825636192271442,"
min
t∈traj∩Dvalid P(t|a) ≤
p"
REFERENCES,0.48350612629594725,"δ/Cd
a′
#"
REFERENCES,0.48444863336475025,≥d0δ6(1 −δ)2
REFERENCES,0.48539114043355325,"8Cd∆|A|nD
."
REFERENCES,0.48633364750235625,"where the notation T ⋆
a′ is used to overload the distribution over infinite trajectories on T ⋆
a′. The
parameters nD and ∆are defined in eq. (19)."
REFERENCES,0.4872761545711593,"Proof. By Lemma B.8, recall that the ≥d0 tokens assigned in a run of Algorithm 1, with high
probability, are substrings in S≤j⋆. For any a ∈A, the total number of substrings in S≤j⋆can be
bounded as,"
REFERENCES,0.4882186616399623,"|S≤j⋆| ≤
X a∈A j⋆
X"
REFERENCES,0.4891611687087653,"j=0
|Sa
j | ≤
X a∈A j⋆
X j=0"
REFERENCES,0.49010367577756836,"1
(1 −δ)j+1 ≤C|A|d"
REFERENCES,0.49104618284637136,"δ(1 −δ).
(20)"
REFERENCES,0.49198868991517436,"In order to prove this result, we use a counting argument and the fact that no tokens in S>j⋆are
assigned. Consider some character a and all the leaves in the forest S≤j⋆. Since every transition has
≥δ probability of occurring, across all leaf nodes t ∈S≤j⋆, P(t|a′) are within a δ2(1 −δ) factor of
each other across different a′ ∈A. In particular, by counting the number of paths in T ⋆(i.e. paths in
T ⋆
a from ∅to leaf nodes in Sa
≤j⋆across a ∈A) along which a token in Dict exists in S≥j⋆/2, we can
also compute the probability mass across such trajectories up to a factor of δ2(1 −δ)."
REFERENCES,0.49293119698397736,"Taking the union across a ∈A, consider the paths in T ⋆
a from ∅to leaf nodes in Sa
≤j⋆. From
Lemma B.9, P"
REFERENCES,0.4938737040527804,"j≤j⋆|Dvalid ∩Sj| ≥d0/4nD, where nD = 1 −2 log(4Cd/δd0)/ log(1 −δ). Note
that for sufficiently large d = Ω(log(1/ϵδ)/δ5), by Lemma B.7, P"
REFERENCES,0.4948162111215834,"j≤j⋆/2 |Sj| =
p"
REFERENCES,0.4957587181903864,"Cd/δ/δ(1−δ) ≤
d0/8nD. Therefore,
X"
REFERENCES,0.49670122525918947,"j⋆/2<j≤j⋆
|Dvalid ∩Sj| ≥
d0
8nD
.
(21)"
REFERENCES,0.49764373232799247,"Define ∆= log(δ)/ log(1 −δ). Combining eq. (21) with eq. (20) and applying the probabilistic
method, there exists an i⋆≥j⋆/2 such that,"
REFERENCES,0.49858623939679547,|Dvalid ∩(Si⋆+1 ∪· · · ∪Si⋆+∆)|
REFERENCES,0.49952874646559847,"|Si⋆+1 ∪· · · ∪Si⋆+∆|
≥δ(1 −δ)d0"
REFERENCES,0.5004712535344015,"8Cd|A|nD
.
(22)"
REFERENCES,0.5014137606032045,"Note that ∆is chosen to be sufficiently large, so that every infinite trajectory on T ⋆
a′ must intersect at
least once with the band of vertices Sa′
i⋆+∆+1 ∪· · · ∪Sa′
i+2∆. Note that this band is different from the
one considered in eq. (22). Define La′ as the set of longest prefixes across infinite trajectories in T ⋆
a′
which belong to Sa′
i⋆+∆+1 ∪· · · ∪Sa′
i+2∆."
REFERENCES,0.5023562676720076,"Note that our objective is to show that an infinite trajectory sampled on T ⋆
a′ where a′ ∼P(·|a), has a
long prefix in Dict. We can truncate this trajectory to lower bound this probability, and therefore, we
assume that the infinite trajectories on T ⋆
a′ terminate once they reach a substring in La′. Furthermore,
note that although ∆is large, it is still a constant depending on δ. Therefore, the band of states
Sa′
i⋆+∆+1 ∪· · · ∪Sa′
i⋆+2∆is not too wide, and all the substrings in La′ have approximately similar
probabilities to each other. In particular, for any character a ∈A, and for any a′ ∈A and t ∈La′,
decomposing P(t|a) as P(t|t1 = a′)P(a′|a),"
REFERENCES,0.5032987747408105,"δ2(1 −δ) · (1 −δ)i+∆(i)
≤P(t|a)
(ii)
≤(1 −δ)i+∆.
(23)"
REFERENCES,0.5042412818096136,"Inequality (i) follows from the fact that all transition probabilities are at least δ, so every leaf node
in La′ must have P(t|t1 = a′) ≥(1 −δ)i+2∆+1, and the fact that P(a′|a) ≥δ. Inequality (ii)"
REFERENCES,0.5051837888784166,"follows similarly from the fact that t is a leaf node of La′ and therefore P(t|t1 = a′) ≤(1 −δ)i+∆.
Therefore, instead of bounding the probability of any event under the distribution over substrings
in La′ induced by truncating the infinite strings sampled on T ⋆
a′, it suffices to count the fraction of
substrings in La′ satisfying the event (which are equivalent up to a δ(1 −δ) factor). Define,"
REFERENCES,0.5061262959472196,"pre(t) = (t1, t1:2, t1:3, · · · , t1:|t|)"
REFERENCES,0.5070688030160226,"As the set of prefixes of t (including t). Note that at most ∆of the prefixes of any substring t can
intersect with Sa
i⋆+1 ∪· · · ∪Sa
i⋆+∆. Therefore,
X a′∈A X"
REFERENCES,0.5080113100848256,"t∈La′
1(pre(t) ∩Dvalid ∩(Sa′
i⋆+1 ∪· · · ∪Sa′
i⋆+∆) ̸= ∅) ≥
X a′∈A X t∈La′"
REFERENCES,0.5089538171536286,"|pre(t) ∩Dvalid ∩(Sa′
i⋆+1 ∪· · · ∪Sa′
i⋆+∆)|
∆"
REFERENCES,0.5098963242224317,"(i)
≥
X a′∈A"
REFERENCES,0.5108388312912346,"|Dvalid ∩(Sa′
i⋆+1 ∪· · · ∪Sa′
i⋆+∆)|
∆"
REFERENCES,0.5117813383600377,"(ii)
≥
δd0(1 −δ)
8Cd∆|A|nD X"
REFERENCES,0.5127238454288408,"a′∈A
|Sa′
i⋆+1 ∪· · · ∪Sa′
i⋆+∆|"
REFERENCES,0.5136663524976437,"(iii)
≥
δ3d0(1 −δ)
8Cd∆|A|nD X"
REFERENCES,0.5146088595664468,"a′∈A
|La′|,"
REFERENCES,0.5155513666352498,"where (i) uses the fact that the prefixes of t ∈La′ cover all the substrings in Sa′
≤i⋆+∆, and therefore
∪t∈La′pre(t) ⊃Sa′
i⋆+1 ∪· · · ∪Sa′
i⋆+∆, and (ii) uses eq. (22). Finally, (iii) uses the fact that ∆is not
too large, and therefore, for any substring t′ ∈Sa′
i⋆+1 ∪· · · ∪Sa′
i⋆+∆, there are at most 1/(1 −δ)2∆=
1/δ2 substrings t ∈La′ which contain it as a prefix. This means, |La′| ≤|Sa′
i⋆+1 ∪· · · ∪Sa′
i⋆+∆|/δ2.
After dividing by P"
REFERENCES,0.5164938737040528,"a′∈A |La′| on both sides, this implies,"
REFERENCES,0.5174363807728558,"Ea′∼Unif(A) """
REFERENCES,0.5183788878416589,"Pr
t∼Unif(La′)"
REFERENCES,0.5193213949104618,"
pre(t) ∩Dvalid ∩(Sa′
i⋆+1 ∪· · · ∪Sa′
i⋆+∆) ̸= ∅
a′#"
REFERENCES,0.5202639019792649,≥δ3d0(1 −δ)
REFERENCES,0.5212064090480678,"8Cd∆|A|nD
. (24)"
REFERENCES,0.5221489161168709,"The event inside the inner probability term is the event that an infinitely long string (truncated at La′)
has a prefix which lies in Dvalid and which intersects with Sa′
i⋆+1 ∪· · · ∪Sa′
i⋆+∆, which implies that it
has probability P(t|a) ≤
p"
REFERENCES,0.5230914231856739,"δ/Cd. Therefore, we have that for any a ∈A, sampling an a′ ∼P(·|a)
and an infinite trajectory traj ∼T ⋆
a′,"
REFERENCES,0.5240339302544769,"Ea′∼P (·|a) """
REFERENCES,0.5249764373232799,"Pr
traj∼T ⋆
a′"
REFERENCES,0.525918944392083,"
min
t∈traj∩Dvalid P(t|a) ≤
p"
REFERENCES,0.5268614514608859,"δ/Cd
a′
#"
REFERENCES,0.527803958529689,"(i)
≥δ2(1 −δ) · Ea′∼P (·|a) "
REFERENCES,0.528746465598492,"
Pr
t′∼Unif(La′) "
REFERENCES,0.529688972667295,"min
t∈pre(t′)∩Dvalid P(t|a) ≤
p δ/Cd a′
! "
REFERENCES,0.530631479736098,"(ii)
≥δ2(1 −δ) · Ea′∼P (·|a) """
REFERENCES,0.5315739868049011,"Pr
t′∼Unif(La′)"
REFERENCES,0.532516493873704,"
pre(t′) ∩Dvalid ∩(Sa′
i⋆+1 ∪· · · ∪Sa′
i⋆+∆) ̸= ∅
a′#"
REFERENCES,0.5334590009425071,"(iii)
≥δ3(1 −δ) · Ea′∼Unif(A) """
REFERENCES,0.53440150801131,"Pr
t′∼Unif(La′)"
REFERENCES,0.5353440150801131," 
pre(t′) ∩Dvalid ∩(Sa
i⋆+1 ∪· · · ∪Sa
i⋆+∆) ̸= ∅
a′
#"
REFERENCES,0.5362865221489161,≥δ3(1 −δ) · δ3d0(1 −δ)
REFERENCES,0.5372290292177191,"8Cd∆|A|nD
."
REFERENCES,0.5381715362865221,"Here (i) follows by truncating the trajectory traj to terminate at a node in ∪a′∈ALa′ and from eq. (23),
(ii) follows by arguing that i⋆≤j⋆/2 and therefore if a prefix of t′ lies in Sa′
i⋆+1 ∪· · · ∪Sa′
i⋆+∆, then
it must have P(t|a) ≤
p"
REFERENCES,0.5391140433553252,"δ/Cd. Inequality (iii) follows by noting that all the transitions P(a′|a)
have probability ≥δ, and the last inequality follows from eq. (24)."
REFERENCES,0.5400565504241281,Proof of Lemma B.5
REFERENCES,0.5409990574929312,"Lemma B.10 concludes that given any previous sequence of tokens terminating in a character a,
with constant probability, an infinite trajectory sampled from T ⋆
a′ with a′ ∼P(·|a) has as prefix,
a substring t, which not only has low probability, with P(t|a) ≤
p"
REFERENCES,0.5419415645617343,"δ/Cd, but also belongs to the
subset of tokens Dvalid. Note that regardless of the previously sampled tokens, it is legal to sample
any token in Dvalid as the current token, since by definition, these tokens are not the suffixes of any
other tokens in Dict. Moreover, if any trajectory on T ⋆
a′ reaches a token in Dvalid, then it must be
largest token along that trajectory, since none of the tokens in Dvalid are prefixes of another token in
Dict."
REFERENCES,0.5428840716305372,"Consider generating a new token by rejection sampling. Suppose the set of previous tokens t1, · · · , ti
end in some character a. Sample the next character a′ ∼P(·|a) and an infinite trajectory on T ⋆
a′. If it
reaches an illegal token t such that tjtj+1 · · · tit already exists in Dict, this token is rejected and the
trajectory is resampled. By the prefix-free property of these tokens, if this trajectory visits a token in
Dvalid, it must immediately be output as the next token. Note that this probability is lower bounded
by,"
REFERENCES,0.5438265786993403,"Ea′∼P (·|a) """
REFERENCES,0.5447690857681433,"Pr
traj∼T ⋆
a′"
REFERENCES,0.5457115928369463,"
min
t∈traj∩Dvalid P(t|a) ≤
p"
REFERENCES,0.5466540999057493,"δ/Cd
a′
#"
REFERENCES,0.5475966069745523,"which is lower bounded by poly(ϵ, δ), the subject of Lemma B.10. Therefore with this probability,
the process terminates in the first step with a token in Dvalid being sampled."
REFERENCES,0.5485391140433553,"B.3
Analysis in the small dictionary case"
REFERENCES,0.5494816211121584,"In this section, we will prove Theorem B.3.2 and Theorem B.3.3. In particular we show that, either,"
REFERENCES,0.5504241281809613,"1. The dictionary is small with low probability. i.e., Pr(|Dict| < d0) = e−Ω(ϵ2d/ log2(1/δ)), or,
2. Or conditioned on the dictionary being small, |Dict| < d0, with high probability ≥1 −
e−Ω(ϵ2d/ log2(1/δ)),"
REFERENCES,0.5513666352497644,"min
Q∈Q1-gram L(Q ◦enc(·)) ≤4 "
REFERENCES,0.5523091423185674,1 −2d0
REFERENCES,0.5532516493873704,"d
+ O

1
log(d) !"
REFERENCES,0.5541941564561734,H∞+ 2d0
REFERENCES,0.5551366635249765,"d
· log(2|A|)."
REFERENCES,0.5560791705937794,"For i ∈[d], define the indicator random variable,"
REFERENCES,0.5570216776625825,"X(s′, Dict) = 1(∃a pair of tokens in encBPE(s′) under Dict appears at least log(d) times)."
REFERENCES,0.5579641847313855,"which captures the event that the string s′ is compressed well by the dictionary Dict under the
sequential encoder."
REFERENCES,0.5589066918001885,"Let Dicti denote the dictionary stored by Algorithm 1 right after texti is processed. The key
insight behind this lemma is the following statement, asserting that the sequential encoder satisfies a
“monotonicity” property: for any j and string s′, if there exists a pair of tokens appearing more than
log(d) times consecutively in the sequential encoding of s′ under Dictj, then there must exist a pair of
tokens appearing more than log(d) times consecutively in the greedy encoding of s′ under Dicti for
any i < j. This implies that X(s′, Dictj) ≤X(s′, Dicti) if i < j for any string s′. This monotonicity
property implies that the last dictionary output by the learner, Dictd sequentially encodes a 1 −ϵ
fraction of the previously seen texts, texti in a way where every pair of tokens appears at most log(d)
times. While Dictd is correlated with these texts, we can circumvent this correlation by using a
martingale argument to prove the statement of the lemma.
Lemma B.11. Let Dict be the dictionary returned by Algorithm 1. Then,"
REFERENCES,0.5598491988689915,"min

Pr

E

X
 
s′, Dict
Dict

≥2d0/d
|Dict| < d0

, Pr
 
|Dict| < d0

≤e−ϵ2d/8 log2(1/δ)."
REFERENCES,0.5607917059377945,where s′ is a fresh substring of length d sampled from the stochastic source.
REFERENCES,0.5617342130065975,"Proof. Let Dicti denote the state of dictionary returned by Algorithm 1 right after texti is processed.
Then, Dictd is the final dictionary returned by Algorithm 1. Suppose E

X
 
s′, Dictd
Dictd

≥2d0/d,"
REFERENCES,0.5626767200754006,"where s′ is a fresh substring of length d sampled from the stochastic source. Using monotonicity of
the sequential encoder, almost surely for any string s′, X(s′, Dicti) ≤X(s′, Dictj) for any j > i.
Therefore,"
REFERENCES,0.5636192271442035,"E

X
 
s′, Dictd
Dictd

≥2d0/d =⇒
Xd−1"
REFERENCES,0.5645617342130066,"i=1 E

X
 
s′, Dicti
Dicti

≥2d0 · d −1"
REFERENCES,0.5655042412818096,"d
(25)"
REFERENCES,0.5664467483506126,"Note in this expectation, s′ is an independent string of length d sampled from the stochastic source.
Since Dicti and texti+1 are independent, we may instead write,
Xd−1"
REFERENCES,0.5673892554194157,"i=1 E

X
 
texti+1, Dicti
Dicti, texti, Dicti−1, · · · , Dict1, text1

≥2d0 · d −1 d
."
REFERENCES,0.5683317624882187,"For
brevity,
denote
Xi
=
X(texti+1, Dicti)
and
define
the
filtration
Fi
=
σ({text1, Dict1, · · · , texti, Dicti}).
Note that Pi
j=1 Xj −E[Xj|Fi] forms a martingale se-
quence under the filtration {Fi : i ∈[d]}. Therefore, by the Azuma-Hoeffding inequality, for any
η > 0,"
REFERENCES,0.5692742695570217,"Pr
Xd−1"
REFERENCES,0.5702167766258247,"i=1 E[Xi|Fi] −Xi ≤−η

≤e−η2.
(26)"
REFERENCES,0.5711592836946278,"Under Case I, we have that Pd
i=1 Xi ≤d0. Therefore, from eq. (25) and eq. (26),"
REFERENCES,0.5721017907634307,"Pr

|Dict| < d0; E

X(s′, Dict)
Dict

≥2d0/d

≤Pr "
REFERENCES,0.5730442978322338,"
d−1
X"
REFERENCES,0.5739868049010367,"i=1
Xi < d0; d−1
X"
REFERENCES,0.5749293119698398,"i=1
E [Xi|Fi] ≥2d0 · d −1 d   ≤Pr "
REFERENCES,0.5758718190386428,"
d−1
X"
REFERENCES,0.5768143261074458,"i=1
E [Xi|Fi] −Xi ≥d0 · d −2 d  "
REFERENCES,0.5777568331762488,"≤e−d2
0(1−2/d)2"
REFERENCES,0.5786993402450519,"≤e−d2
0/2 = e−ϵ2d/8 log2(1/δ)."
REFERENCES,0.5796418473138548,"Finally, using the inequality Pr(A, B) = Pr(A|B) Pr(B) ≥(min{Pr(A), Pr(B)})2 completes the
proof."
REFERENCES,0.5805843543826579,"Proofs of Theorem B.3.2 and Theorem B.3.3
If Pr(|Dict| < d0) ≤ϵ−ϵ2d/8 log2(1/δ) the proof
of Theorem B.3.2 concludes. Otherwise, consider the case Pr(|Dict| < d0) > ϵ−ϵ2d/8 log2(1/δ),
whereby, E[X(s′, Dict)|Dict] ≤2d0/d with probability ≥1 −e−ϵ2d/8 log2(1/δ) conditioned on
|Dict| < d0 by Lemma B.11. Recall that when |Dict| < d0, Algorithm 1 uses a parallel implementa-
tion of the sequential encoder which chunks a new string into pieces of length d, denoted {chunki :
i ∈[d]} and uses the sequential encoder under Dictd to tokenize each chunk. Note that since the
source is Markovian, the chunked process {chunki = (Xid+1, Xid+2, · · · , X(i+1)d) : i = 1, 2, · · · }
is also Markovian and ergodic. Therefore, by a similar limiting argument as in Lemma A.4, using the
Krylov–Bogolyubov argument (cf. Proposition 4.2 in Chen (2018)) for Markov processes, we have
that,"
REFERENCES,0.5815268614514609,"lim
ℓ→∞"
REFERENCES,0.5824693685202639,"Pℓ
i=1 X(chunki, Dict)"
REFERENCES,0.5834118755890669,"ℓ
= E[X(s′, Dict)] ≤2d0 d ."
REFERENCES,0.58435438265787,"where s′ is a fresh string of length d sampled with initial state distribution as the stationary measure of
the stochastic source. On the remaining (limiting) 1 −2d0/d fraction of the chunks, their sequential
encodings have every pair of tokens appearing at most log(d) times consecutively. Using Theorem 1
of Navarro and Russo (2008), the number of tokens in the encoding of each of these chunks cannot
be too large, and satisfies,"
REFERENCES,0.5852968897266729,"|encBPE(chunki)| · log |encBPE(chunki)| ≤2dH∞+ O(d/ log(d))
=⇒|encBPE(chunki)| · log d ≤2dH∞+ O(d/ log(d))
(27)"
REFERENCES,0.586239396795476,"For the (limiting) 2d0/d fraction of the “bad” chunks, their sequential encodings may have one or
more pairs of tokens which appear more than log(d) times consecutively."
REFERENCES,0.5871819038642789,"Define Ei = {X(chunki, Dict) = 1} where Dict = Dictd is the dictionary returned by Algorithm 1
and consider the unigram model Quni(t) = 1"
REFERENCES,0.588124410933082,2Q1(t) + 1
REFERENCES,0.589066918001885,"2Q2(t), which is the uniform mixture of two
models,"
REFERENCES,0.590009425070688,"Q1(t) ∝
1
(2|A|)|t| ,
and
Q2(t) = E"
REFERENCES,0.590951932139491,"""
n1
t
|encBPE.split(chunk1)| Ec
1 # ,"
REFERENCES,0.5918944392082941,"and let Quni(t1, · · · , ti) = Q#(j) Qi
j=1 Quni(ti) for some distribution Q#(i) over the number of
tokens to be chosen later. We will analyze the case where the total number of chunks ℓis finite and
take the limit m →∞later. Then, the overall loss of the algorithm is,"
REFERENCES,0.592836946277097,"Lm(Quni ◦enc(·))
= −E[log Quni(encBPE.split(s))] = −
X"
REFERENCES,0.5937794533459001,"t∈Dict
E[nt log Quni(t) + log Quni(|encBPE.split(s)|)]"
REFERENCES,0.5947219604147032,"(i)
= − ℓ
X i=1
E  X"
REFERENCES,0.5956644674835061,"t∈Dict
ni
t log Quni(t) "
REFERENCES,0.5966069745523092,"+ log(m) = − ℓ
X i=1
E  X"
REFERENCES,0.5975494816211122,"t∈Dict
ni
t log Quni(t) Ei "
REFERENCES,0.5984919886899152,Pr(Ei) + E  X
REFERENCES,0.5994344957587182,"t∈Dict
ni
t log Quni(t) Ec
i "
REFERENCES,0.6003770028275212,"Pr(Ec
i ) + log(m). (28)"
REFERENCES,0.6013195098963242,"where ni
t is the number of times t is observed in the BPE encoding of chunki and (i) uses the fact
that |encBPE.split(s)| follows some distribution supported on [m], which implies its entropy is upper
bounded by log(m). First observe that, ℓ
X i=1
E  X"
REFERENCES,0.6022620169651273,"t∈Dict
ni
t log Quni(t) Ec
i  ≤ ℓ
X i=1
E """
REFERENCES,0.6032045240339302,"|encBPE(chunki)| ·
X"
REFERENCES,0.6041470311027333,"t∈Dict
ni
t
|encBPE(chunki)| log Quni(t) Ec
i #"
REFERENCES,0.6050895381715363,"(i)
≤ℓ
2dH∞+ O(d/ log(d))"
REFERENCES,0.6060320452403393,log(d)  X
REFERENCES,0.6069745523091423,t∈Dict Q2(t) log Quni(t)
REFERENCES,0.6079170593779454,"where (i) uses the upper bound on |encBPE.split(chunki)| under the event Ec
i (eq. (27)). Since
Quni(t) = 1"
REFERENCES,0.6088595664467483,2Q1(t) + 1
REFERENCES,0.6098020735155514,2Q2(t) ≥1
REFERENCES,0.6107445805843544,"2Q2(t) and Q2 is a distribution supported on at most d tokens, this
term results in the upper bound, ℓ
X i=1
E  X"
REFERENCES,0.6116870876531574,"t∈Dict
ni
t log Quni(t) Ec
i "
REFERENCES,0.6126295947219604,"≤ℓ
2dH∞+ O(d/ log(d))"
REFERENCES,0.6135721017907634,log(d)
REFERENCES,0.6145146088595664,"
log(2d).
(29)"
REFERENCES,0.6154571159283695,"On the other hand, since Quni(t) ≥1"
REFERENCES,0.6163996229971724,"2Q1(t), ℓ
X i=1
E  X"
REFERENCES,0.6173421300659755,"t∈Dict
ni
t log(1/Quni(t)) Ei  ≤ ℓ
X i=1
E  X"
REFERENCES,0.6182846371347785,"t∈Dict
ni
t log(2/Q1(t)) Ei   ≤ ℓ
X i=1
E  X"
REFERENCES,0.6192271442035815,"t∈Dict
ni
t
 
log(2) + |t| log(2|A|)


Ei  "
REFERENCES,0.6201696512723845,"≤ℓd log(2) + ℓd log(2|A|)
(30)"
REFERENCES,0.6211121583411876,where the last inequality uses the fact that P
REFERENCES,0.6220546654099905,"t∈Dict ni
t ≤d and P"
REFERENCES,0.6229971724787936,"t∈Dict |t|ni
t = d computes the length
of chunki."
REFERENCES,0.6239396795475967,"Overall, since Pℓ
i=1 Pr(Ei) ≤2d0/d by eq. (27), combining this with eqs. (28) to (30),"
REFERENCES,0.6248821866163996,"Lm(Quni ◦enc(·)) ≤

1 −2d0 d"
REFERENCES,0.6258246936852027,"
ℓ
2dH∞+ O(d/ log(d))"
REFERENCES,0.6267672007540056,log(d)
REFERENCES,0.6277097078228087,"
log(2d) + 2d0"
REFERENCES,0.6286522148916117,d ℓd log(4|A|).
REFERENCES,0.6295947219604147,"Dividing throughout by the length of the character sequence m ∈[d(ℓ−1), dℓ] and letting ℓ→∞,"
REFERENCES,0.6305372290292177,"min
Q∈Q1-gram L(Q ◦enc(·)) ≤L(Quni ◦enc(·)) ≤

1 −2d0 d "
REFERENCES,0.6314797360980208,"2H∞+ O

1
log(d) ! + 2d0"
REFERENCES,0.6324222431668237,d log(4|A|).
REFERENCES,0.6333647502356268,"C
Additional Theoretical Results II: Learning the likelihood model"
REFERENCES,0.6343072573044298,"The guarantees we prove in Theorems 3.1, 3.6 and B.2 on various tokenizers assume that the
downstream model is trained optimally. In practice, these models are trained from a finite dataset
and the sample complexity of learning this likelihood model scales with the number of tokens in the
dictionary. In this section, we step away from the transformer architecture and focus on analyzing the
performance of a simple estimator for the unigram model based on Laplace smoothing. We leave the
problem of analyzing the finite-sample statistical error of simple transformer models trained with
gradient descent as an interesting open direction for future research."
REFERENCES,0.6352497643732328,"The result of Theorem 3.1 establishes that under appropriate assumptions on the Markov source, there
exists a tokenizer T and a unigram model over tokens Q⋆∈Q1-gram such that,"
REFERENCES,0.6361922714420358,"lim
m→∞
1
mE

log(1/Q⋆(enc(s))
"
REFERENCES,0.6371347785108389,"≤(1 + ε) · lim
m→∞
1
mE

log(1/P(s))
"
REFERENCES,0.6380772855796418,"Or in other words,"
REFERENCES,0.6390197926484449,"lim
m→∞
1
mKL(P, Q⋆(enc(·))) ≤ε · lim
m→∞
1
mE

log(1/P(s))

."
REFERENCES,0.6399622997172478,"This implies that with the appropriate tokenization, the measure associated to the string by the best
unigram model over tokens is close to that induced by the true Markov distribution over characters
in KL divergence. In this section, we establish finite-sample guarantees on learning Q⋆specifically
for the LZW tokenizer. The approach we consider for distribution learning is a smoothed Laplace
estimator described in more detail in Algorithm 2."
REFERENCES,0.6409048067860509,"For any constant θ ∈(0, 1), define Eθ as the event that every maximal token t (Definition A.5) in
the LZW dictionary satisfies 1/d1−θ ≥maxa P(t|a) ≥δ/d1+θ. By Lemmas A.10 and A.11 when
the LZW tokenizer is trained on a dataset of size eΩδ(d) drawn from a stochastic source satisfying
Assumption 3.2, Eθ occurs with probability ≥1 −d−Ωθ,δ(log(d)).
Theorem C.1. Consider any constant θ ∈(0, 1), failure probability η ∈(0, 1) and approximation
error ξ ∈(0, 1). Assume that the learnt LZW tokenizer TLZW satisfies the event Eθ, which occurs
with probability ≥1 −d−Ωθ,δ(log(d)). Assume that d1−3θ ≥1 + δ−2 and that the stochastic source
satisfies Assumption 3.2. For an absolute constant C > 0, assume that the size of the training dataset
is at least n⋆
lm(ξ), where,"
REFERENCES,0.6418473138548539,"n⋆
lm ≜Cd1+θ log3(d/ηδ) log log(d/η) δξ2"
REFERENCES,0.6427898209236569,"Then, Algorithm 2 learns a unigram model bQ such that,"
REFERENCES,0.6437323279924599,"L( bQ ◦encgre(·)) ≤(1 + ξ)
min
Q∈Q1-gram L(Q ◦encgre(·))"
REFERENCES,0.644674835061263,with probability ≥1 −η.
REFERENCES,0.6456173421300659,"In conjunction with Theorem 3.6, this gives end-to-end guarantees on the cross-entropy loss of the
LZW tokenizer (with vocabulary size ≤d) with the Laplace estimator as the downstream unigram
model. We instantiate this result choosing θ = 0.01 in Theorem C.1.
Corollary C.2. Choose any ξ ∈(0, 1). Suppose the data source satisfies Assumption 3.2. On a
dataset of size eΩδ(d) drawn from the source, train an LZW tokenizer TLZW with d tokens. Subsequently,
using Algorithm 2, learn a unigram model bQ using a dataset of size at least eΩ(d1.01/δξ2) drawn
from the source. Then, with probability ≥1 −d−Ωδ(log(d)),"
REFERENCES,0.646559849198869,L( bQ ◦encgre(·)) ≤1 + ξ
REFERENCES,0.647502356267672,"1 −ε min
Q L(Q),"
REFERENCES,0.648444863336475,"where ε =
log(1/δ)
0.99 log(d)."
REFERENCES,0.649387370405278,"The analysis of Theorem C.1 relies on showing that the distribution over tokens induced when a string
sampled from the data source is encoded into tokens by the greedy encoder and the LZW dictionary is
a Markov process. In general, given a set of previously sampled tokens t1, · · · , ti, the next token ti+1
is sampled from the distribution P(ti+1|ti; ∀j ∈[i], ti−j+1 · · · titi+1 ̸∈Dict). The conditioning
is to simply guarantee that the previous tokens which were sampled were indeed maximal, since
if titi+1 ∈Dict, then the previous token returned would in fact have been this longer token and
not ti (and likewise for ti−1titi+1 and so on). While in general, this process is complicated and
depends on all the previous tokens sampled, for the LZW dictionary, we show that the conditioning
{∀j ∈[i], ti−j+1 · · · titi+1 ̸∈Dict} can be removed, thereby resulting in a simple Markov process
over tokens."
REFERENCES,0.6503298774740811,"Furthermore, we establish that this Markov process has a relatively large spectral gap. The optimal
unigram model ends up being the stationary distribution over tokens induced by greedy encoder.
Given the large spectral gap of the Markov process over tokens, estimating the stationary distribution
of this process in KL divergence ends up being closely related to estimating a distribution from
i.i.d. samples in the same metric. For this problem, the de-facto choice of estimator is the Laplace
estimator, and several existing results provide finite-sample bounds on the KL divergence (Braess
and Sauer, 2004; Han et al., 2021; Mourtada and Gaïffas, 2022). The Laplace estimator (Line 6
of Algorithm 2) is simply a smoothed empirical estimate to account for the degeneracy of the KL
divergence in its second argument as any coordinate approaches 0. The non-i.i.d.ness of the Markov
process is circumvented by using concentration inequalities which are a function of the spectral gap
(Naor et al., 2020)."
REFERENCES,0.651272384542884,Algorithm 2 Training likelihood model on tokens
REFERENCES,0.6522148916116871,"Input: A training dataset of size nlm, likelihood model class Q, likelihood model training
algorithm TrainLM
Output: Likelihood model Q ∈Q.
1: Tokenize the training dataset into a sequence of tokens T = (t1, · · · , ti).
2: Train a likelihood model Q on the tokenized dataset T using the TrainLM(T , Q) subroutine."
REFERENCES,0.65315739868049,"// In the case of Q = Q1-gram use the Laplace estimator
def TrainLM(T , Q1-gram):
3: Truncate the dataset to the first n′ = ⌊nlm/ℓmax⌋tokens where ℓmax = 4 log(d|A|)/δ. Let the
truncated dataset be Ttrunc
4: Construct the unigram model bQ with bQ# = Unif([m]) and bQtok(t) =
nt+1
nt+|Dict|.
// nt is the number of times t appears in Ttrunc.
// Test sequences are assumed to be of length m."
REFERENCES,0.6540999057492931,"C.1
Proof of Theorem C.1"
REFERENCES,0.6550424128180962,"Since TLZW uses the greedy encoder, the cross-entropy loss of the unigram model learnt by Algo-
rithm 2 is,"
REFERENCES,0.6559849198868991,"L( bQ ◦encgre(·)) −
min
Q∈Q1-gram L(Q ◦encgre(·))"
REFERENCES,0.6569274269557022,"=
max
Q∈Q1-gram lim
m→∞
1
mE[log(Q(encgre(s))/ bQ(encgre(s)))]"
REFERENCES,0.6578699340245052,"(i)
=
max
Q∈Q1-gram lim
m→∞
1
mE "
REFERENCES,0.6588124410933082,"|encgre(s)|
X"
REFERENCES,0.6597549481621112,t∈Dict
REFERENCES,0.6606974552309143,"nt
|encgre(s)| log(Qtok(t)/ bQtok(t)) "
REFERENCES,0.6616399622997172,+ log(m) m
REFERENCES,0.6625824693685203,"(ii)
≤
lim
m→∞
1
mE "
REFERENCES,0.6635249764373233,"|encgre(s)|
X"
REFERENCES,0.6644674835061263,t∈Dict
REFERENCES,0.6654099905749293,"nt
|encgre(s)| log"
REFERENCES,0.6663524976437323,nt/|encgre(s)|
REFERENCES,0.6672950047125353,bQtok(t) !
REFERENCES,0.6682375117813384,+ log(m) m
REFERENCES,0.6691800188501413,"where in (i) we use the fact that bQ# = Unif([m]) and in (ii) we take the max{·} inside the limit and
the expectation (Fatou’s lemma and Jensen’s inequality) and plug in the maximizer of the negative
cross-entropy, Qtok(t) =
nt
|encgre(s)|. Note that limm→∞
nt
|encgre(s)|
a.s.
= QMLE(t) by Lemma A.4."
REFERENCES,0.6701225259189444,"Moreover, since |enc(s)|/m ≤1 and bQtok(t) > 0 surely, by the Dominated Convergence Theorem,"
REFERENCES,0.6710650329877474,"L( bQ ◦encgre(·)) −
min
Q∈Q1-gram L(Q ◦encgre(·)) ≤lim
m→∞
1
mE[|encgre(s)|] · KL(QMLE, bQtok)
(31)"
REFERENCES,0.6720075400565504,"By eq. (6), we have that for any tokenizer using the greedy encoder,"
REFERENCES,0.6729500471253534,"lim
m→∞
|encgre(s)|
 
H(QMLE, P) −log(1/δ)
 m"
REFERENCES,0.6738925541941565,"a.s.
≤H∞."
REFERENCES,0.6748350612629594,"Furthermore under the event Eθ which implies that the learnt dictionary is (1 −θ)-heavy hitting (cf.
Definition A.5), which implies that,"
REFERENCES,0.6757775683317625,"H(QMLE, P) ≥(1 −θ) log(d)."
REFERENCES,0.6767200754005656,"Therefore, by almost sure boundedness, we have that,"
REFERENCES,0.6776625824693685,"lim
m→∞
1
mE

|encgre(s)|

≤
H∞
(1 −θ) log(d) −log(1/δ) ≤minQ∈Q1-gram L(Q ◦enc(·))"
REFERENCES,0.6786050895381716,(1 −θ) log(d) −log(1/δ)
REFERENCES,0.6795475966069745,"Putting this together with eq. (31), we have that,"
REFERENCES,0.6804901036757776,"L( bQ ◦encgre(·)) ≤

1 + KL(QMLE, bQtok)

min
Q∈Q1-gram L(Q ◦encgre(·)),
(32)"
REFERENCES,0.6814326107445806,"which uses the assumption (1 −θ) log(d) ≥1 + log(1/δ). In the remainder of the proof we upper
bound the KL term."
REFERENCES,0.6823751178133836,"By the law of large numbers established in eq. (34) and the fact that
nt
P"
REFERENCES,0.6833176248821866,"t′ nt′ ∈[0, 1], we have that,"
REFERENCES,0.6842601319509897,"QMLE(t) = lim
m→∞E"
REFERENCES,0.6852026390197926,"""
nt
P"
REFERENCES,0.6861451460885957,t′ nt′ #
REFERENCES,0.6870876531573987,"= lim
m→∞
E [nt]
E
P"
REFERENCES,0.6880301602262017,"t′ nt′ = π(t),"
REFERENCES,0.6889726672950047,"where π(t) denote the stationary distribution over tokens induced by the greedy encoding process,
which exists for the LZW tokenizer. This distribution is in fact an ergodic Markov process, as we
discuss next."
REFERENCES,0.6899151743638078,"By Lemmas A.10 and A.11, for any constant θ ∈(0, 1), with probability ≥1 −d−Ωθ,δ(log(d)), every
maximal token in the the LZW dictionary satisfies 1/d1−θ ≥maxa P(t|a) ≥δ/d1+θ. Let Sgre
denote the set of tokens which have a non-zero probability (over a string drawn from the Markov
source) of being chosen by the greedy encoder while encoding the string. More importantly, note
that for any sequence of tokens t1, · · · , ti, the next token is necessarily in Sgre and can be any
token in this set. The reason for this is that for any ti, t ∈Sgre, the concatenation tit ̸∈Sgre since
maxa∈A P(tit|a) ≤1/δd2(1−θ), which is smaller than the maxa∈A P(t′|a) ≥δ/d1+θ for any token
t′ ∈Sgre as long as d1−3θ ≥1/δ2. This constraint implies that in the sampling procedure in Figure 7,
it suffices to drop the conditioning on the event tjtj+1 · · · tit ̸∈Dict while sampling the next token
t. This condition automatically implies that the sequence of tokens conditionally follows a Markov
process with Pr(ti+1 = t|t1, · · · , ti) = P(t|last(ti)). Since the probability of every transition is
lower bounded, this means that the Markov chain is ergodic. Moreover, the pseudo-spectral gap
(Naor et al., 2020), 1 −λ can be lower bounded by the Dobrushin contraction coefficient, κ,"
REFERENCES,0.6908576814326107,"1 −λ ≤κ ≜
max
(t,t′)∈Dict2 ∥Pr(·|t) −Pr(·|t′)∥TV"
REFERENCES,0.6918001885014138,"=
max
(t,t′)∈Dict2 1 −
X"
REFERENCES,0.6927426955702167,"t′′∈Dict
min{Pr(t′′|t), Pr(t′′|t′)}"
REFERENCES,0.6936852026390198,≤1 −δd/d1+θ
REFERENCES,0.6946277097078228,"= 1 −δd−θ.
(33)"
REFERENCES,0.6955702167766258,"Recall that the learner is given a training dataset of nlm characters to train the likelihood model. By
Lemma A.8, with probability ≥1 −d−Ω(log(d/δ)/δ), in the run of the LZW tokenization algorithm,
every token in the dictionary has length at most ℓmax = 4 log(d|A|)/δ. Therefore, suppose the learner"
REFERENCES,0.6965127238454288,"always truncates the dataset to the first n′ = ⌊nlm/ℓmax⌋tokens and runs the Laplace estimator on
this truncated dataset. With this, we move onto upper bounding,"
REFERENCES,0.6974552309142319,"KL(QMLE, bQtok) =
X"
REFERENCES,0.6983977379830348,"t∈Dict
π(t) log

π(t)/ bQtok(t)
"
REFERENCES,0.6993402450518379,"which necessitates lower bounding bQtok(t) for every t. Recall that the learner’s estimate bQ(t) in
Algorithm 2 is the Laplace estimator,
nt+1
P"
REFERENCES,0.700282752120641,"t′ nt′+Dict, where {nt : t ∈Dict} is computed by truncating
the dataset to the first n′ tokens. Firstly, by invoking Corollary 1.3 of Naor et al. (2020) for the
function nt = Pn′"
REFERENCES,0.7012252591894439,"i=1 I(ti = t), Pr "
REFERENCES,0.702167766258247,|nt −E[nt]| ≥c r
REFERENCES,0.70311027332705,"E[nt]
1 −λ · log(1/η) !"
REFERENCES,0.704052780395853,"≤η
(34)"
REFERENCES,0.704995287464656,"for a universal constant c > 0. In particular, this implies that with probability ≥1−η, simultaneously
for all t,"
REFERENCES,0.705937794533459,|nt −E[nt]| ≤∆t ≜ r dθ
REFERENCES,0.706880301602262,"δ E[nt] · log(|Dict|/η), and, E[nt] −nt ≥E[nt]."
REFERENCES,0.7078228086710651,"Under this event, for any t, the estimate is lower bounded by,"
REFERENCES,0.708765315739868,"bQtok(t) =
nt + 1
n′ + |Dict| ≥E[nt] + 1 −min{E[nt], ∆t}"
REFERENCES,0.7097078228086711,n′ + |Dict|
REFERENCES,0.7106503298774741,"≥max

π(t) −(∆t −1) n′ + |Dict|E[nt]"
REFERENCES,0.7115928369462771,"(n′)2 + n′|Dict|
,
1
n′ + |Dict| "
REFERENCES,0.7125353440150801,"≥max

π(t) −∆tn′ + |Dict|E[nt]"
REFERENCES,0.7134778510838832,"(n′)2
,
1
n′ + |Dict| "
REFERENCES,0.7144203581526861,"Suppose the following condition is satisfied,"
REFERENCES,0.7153628652214892,n′ = 4rdθ|Dict| log(|Dict|/η)
REFERENCES,0.7163053722902922,"δ
(C1)"
REFERENCES,0.7172478793590952,"for some r ≥4. Under this condition, we have that n′ ≥2√r∆and n′ ≥4r|Dict|."
REFERENCES,0.7181903864278982,Case I. ∆tn′ ≥|Dict|E[nt].
REFERENCES,0.7191328934967012,"In this case, we have the upper bound,"
REFERENCES,0.7200754005655042,"bQtok(t) ≥max

π(t) −2∆t"
REFERENCES,0.7210179076343073,"n′ ,
1
n′ + |Dict|  = max 

"
REFERENCES,0.7219604147031102,"

π(t) −2 q dθ"
REFERENCES,0.7229029217719133,δ E[nt] · log(|Dict|/η)
REFERENCES,0.7238454288407163,"n′
,
1
n′ + |Dict| 

 
 ≥max 
"
REFERENCES,0.7247879359095193,π(t) − s
REFERENCES,0.7257304429783223,"π(t)
r|Dict|,
1
2n′ 
 ."
REFERENCES,0.7266729500471254,where the last inequality uses eq. (C1).
REFERENCES,0.7276154571159283,"Consider two sub-cases,"
REFERENCES,0.7285579641847314,"Sub-case I. π(t) ≥2/r|Dict|. Define this event CI. Here,"
REFERENCES,0.7295004712535345,π(t) log(π(t)/ bQtok(t)) ≤−π(t) log  1 − s
REFERENCES,0.7304429783223374,"1
π(t)r|Dict|  ≤3 2 s"
REFERENCES,0.7313854853911405,"π(t)
r|Dict|.
(35)"
REFERENCES,0.7323279924599434,"Sub-case II. π(t) ≤2/r|Dict|. Define this event CII. Here,"
REFERENCES,0.7332704995287465,"π(t) log(π(t)/ bQtok(t)) ≤π(t) log
 
2n′π(t)

≤max ("
REFERENCES,0.7342130065975495,"0,
2
r|Dict| log

4n′"
REFERENCES,0.7351555136663525,r|Dict| )
REFERENCES,0.7360980207351555,"≤
2
r|Dict| log

16dθ log(|Dict|/η)

(36)"
REFERENCES,0.7370405278039586,Case II. ∆tn′ < |Dict|E[nt]. Define this event CIII.
REFERENCES,0.7379830348727615,"In this case we have the upper bound,"
REFERENCES,0.7389255419415646,bQtok(t) ≥π(t) −2|Dict|E[nt]
REFERENCES,0.7398680490103676,"(n′)2
≥π(t) −π(t)"
R,0.7408105560791706,2r
R,0.7417530631479736,"where the last inequality follows from eq. (C1). This implies that,"
R,0.7426955702167767,π(t) log(π(t)/ bQtok(t)) ≤−π(t) log(1 −1/2r) ≤π(t)
R,0.7436380772855796,"r
.
(37)"
R,0.7445805843543827,"By using the geometric ergodicity of this Markov process (eq. (33)), when n′ tokens are sampled
from an arbitrary initial distribution,"
R,0.7455230914231856,"
1 −κn′
π(t) ≤E[nt]"
R,0.7464655984919887,"n′
≤κn′ +

1 −κn′
π(t) =⇒π(t) ≤
bQtok(t)
1 −e−4r|Dict| log(|Dict|/η) =
bQtok(t)
1 −d−r"
R,0.7474081055607917,"where in the implication, we use the condition on n′ in eq. (C1) and the bound on the contraction
coefficient κ in eq. (33)."
R,0.7483506126295947,"KL(QMLE, bQtok) =
X"
R,0.7492931196983977,"t∈Dict
π(t) log(π(t)/ bQtok(t)) ≤
X"
R,0.7502356267672008,t∈Dict
R,0.7511781338360037,"π(t)
1 −d−r log(π(t)/ bQtok(t)) −log(1 −d−r)"
R,0.7521206409048068,"≤
1
1 −d−r
X"
R,0.7530631479736098,"t∈Dict
I(CI)π(t) log(π(t)/ bQtok(t)) + I(CII)π(t) log(π(t)/ bQtok(t)) + I(CIII)π(t) log(π(t)/ bQtok(t)) + 2d−r"
R,0.7540056550424128,"≤
1
1 −d−r
X"
R,0.7549481621112158,"t∈Dict
I(CI)3 2 s"
R,0.7558906691800189,"π(t)
r|Dict|
|
{z
}
eq. (35)"
R,0.7568331762488218,"+ I(CII)
2
r|Dict| log

16dθ log(|Dict|/η)
"
R,0.7577756833176249,"|
{z
}
eq. (36)"
R,0.7587181903864278,+ I(CIII)π(t)
R,0.7596606974552309,"r
|
{z
}
eq. (37) +2d−r"
R,0.760603204524034,"≤
1
1 −d−r  3 2 s"
R,0.7615457115928369,"|Dict|
r|Dict| + 2"
R,0.76248821866164,r log(16dθ log(|Dict|/η)) + 1 r 
R,0.763430725730443,+ 2d−r
R,0.764373232799246,"≤
5
√r log(16dθ log(|Dict|/η))"
R,0.765315739868049,"Combining with eq. (32), we get the bound,"
R,0.7662582469368521,"L( bQ ◦encgre(·)) ≤

1 + KL(QMLE, bQ)

min
Q∈Q1-gram L(Q ◦encgre(·))"
R,0.767200754005655,"≤

1 + 5
√r log(16dθ log(d/η))

min
Q∈Q1-gram L(Q ◦encgre(·))."
R,0.7681432610744581,Rescaling r to be r(log(16dθ log(d/η)))2 completes the proof.
R,0.7690857681432611,"D
Additional Theoretical Results III: The generalization ability of tokenizers"
R,0.7700282752120641,"The proofs of the upper bounds in the paper (Theorems 3.6 and B.2) relied on showing that the
entropy H(QMLE, P) is large, or in other words, the algorithm typically encodes new strings into
long length (i.e. low probability under P) tokens. This statement about generalization to new strings
is fundamentally different from having a tokenizer which compresses the training dataset well. In
other words, consider the following modification: the measure QMLE is defined as the expected
empirical distribution over tokens when a new string is encoded into tokens, and not on the source
dataset used to construct the dictionary. Suppose the definition of QMLE is changed to the empirical
distribution over tokens in the source dataset. Under this new definition of the MLE unigram model,
the largeness of the H(QMLE, P) metric, in a sense, captures compressing the source dataset well.
However, we show that in general, this does not result in good tokenizers that minimize the population
cross-entropy loss, suffering from minQ∈Q1-gram L(Q ◦enc(·)) ≈H(π) ≫H∞."
R,0.7709707822808671,"Theorem D.1. Consider the stochastic source in example A.1 having entropy rate H∞= δ log(1/δ)+
(1 −δ) log(1/(1 −δ)). Consider a training dataset of size n. For a dictionary Dict and t ∈Dict,
define bQMLE(t) =
nt(ssrc)
|enc(ssrc)| as the empirical distribution over tokens induced by the greedy encoder
when encoding the training dataset, ssrc. There exists a dictionary Dict such that with probability
≥1 −e−Ω(√n) over the training dataset,"
R,0.7719132893496701,"H( bQMLE, Pγ) ≥nH∞(1 −O(n−1/4))"
R,0.7728557964184731,"is large. However, for this dictionary, for any encoding algorithm (including the greedy encoder), the
resulting tokenizer T = (Dict, ∅, enc(·), dec(·)) satisfies,"
R,0.7737983034872762,"min
Q∈Q1-gram L(Q ◦enc(·)) ≥(1 −ε)H(π)"
R,0.7747408105560791,where ε = 2ne−nH∞(1−O(n−1/4)).
R,0.7756833176248822,"Proof. Suppose the entire training dataset was compressed into a single token, tsrc. The dictionary
is A ∪tsrc. In the following argument, we show that the number of occurrences, ntsrc, of the entire
training dataset tsrc in a new string of length m generated from the stochastic source, s, converges to
its expectation as m →∞. Let π(i)
n denote the stationary distribution of the Markov process induced
by the stochastic source over length-n strings with a shift of i from the starting position, and let n(i)
t
denote the number of times t appears in the training dataset starting at the position i + rn for some
r > 0. Then,"
R,0.7766258246936852,"lim
m→∞
ntsrc m = 1"
R,0.7775683317624882,"n lim
m→∞ n−1
X i=0"
R,0.7785108388312912,"n(i)
tsrc
m/n"
R,0.7794533459000943,"a.s.
= 1 n n−1
X"
R,0.7803958529688972,"i=0
Et′∼π(i)
n [P(tsrc|t′)] ≤max
a∈A P(tsrc|a).
(38)"
R,0.7813383600377003,"The second equation follows by considering the Markov process induced over length n strings and
applying the Krylov–Bogolyubov argument for ergodic and homogeneous Markov processes."
R,0.7822808671065034,"In Lemma D.2, we show that with probability ≥1 −e−Ω(√n), the token tsrc constructed from the
source dataset satisfies, maxa∈A P(t|a) ≤e−nH∞(1−O(n−1/4)). In other words, the source string
has exponentially small probability. Combining this with eq. (38), with probability ≥1 −e−Ω(√n)
over the source dataset, the number of occurrences of the substring tsrc in a new string s is upper
bounded by,"
R,0.7832233741753063,"lim
m→∞
ntsrc m"
R,0.7841658812441094,"a.s.
≤e−nH∞(1−O(n−1/4)) ≜ε/2n."
R,0.7851083883129123,"By the Krylov–Bogolyubov argument, for each a ∈A = {0, 1}, limm→∞na"
R,0.7860508953817154,"m
a.s.
= π(a). More
importantly, the number of times a is made as a token is upper bounded by na and lower bounded by
na −nntsrc. Therefore,"
R,0.7869934024505184,(1 −ε)π(a) = π(a) −ε 2
R,0.7879359095193214,"a.s.
≤
lim
m→∞
na m"
R,0.7888784165881244,"a.s.
≤π(a) = 1"
R,0.7898209236569275,"2
(39)"
R,0.7907634307257304,"Finally, putting everything together,"
R,0.7917059377945335,"min
Q∈Q1-gram lim
m→∞
1
mLm(Q ◦enc(·)) =
min
Q∈Q1-gram lim
m→∞−1"
R,0.7926484448633365,"mE

log(Q#(|enc(s)|) +
X"
R,0.7935909519321395,"t∈Dict nt log Qtok(t)
"
R,0.7945334590009425,"≥
min
Q∈Q1-gram lim
m→∞−1 mE
X"
R,0.7954759660697456,"a∈A na log Qtok(a)
"
R,0.7964184731385485,"(i)
≥
min
Q∈Q1-gram −(1 −ε)
X"
R,0.7973609802073516,a∈A π(a) log Qtok(a)
R,0.7983034872761545,≥(1 −ε)H(π).
R,0.7992459943449576,where (i) follows from the lower bound on na/m in eq. (39). This completes the proof.
R,0.8001885014137606,"Lemma D.2. With probability ≥1 −e−Ω(√n) over the source dataset,"
R,0.8011310084825636,"max
a∈A P(tsrc|a) ≤e−nH(δ)(1−O(n−1/4))."
R,0.8020735155513666,"Proof. Let X denote the number of i ∈[n −1] such that si ̸= si+1 in s, the stochastic source. Since
the transition of the Markov process only depends on whether the next character is the same as the
previous character, we can write down,"
R,0.8030160226201697,"max
a∈A log P(tsrc|a) = −(X + 1) log(δ) −(n −1 −X) log(1 −δ)."
R,0.8039585296889726,"Note that X is a sum of n −1 i.i.d. random variables, since I(si ̸= si+1) ∼Ber(δ) does not depend
on whether si = 0 or = 1. In particular, by Hoeffding’s inequality, we have that with probability
≥1 −e−Ω(√n),

1
n max
a∈A log P(tsrc|a) −H(δ)
 ≤O

n−1/4
,"
R,0.8049010367577757,"which uses the fact that E[X] = δ(n −1) and H∞= δ log(1/δ) + (1 −δ) log(1/(1 −δ)). Taking
an exponential on both sides proves the statement of the lemma."
R,0.8058435438265787,"E
Additional Theoretical Results IV: Interaction between the dictionary and
encoding algorithm"
R,0.8067860508953817,"In this section, we show another kind of barrier to generalization, which brings out the relationship
between the encoding algorithm and the dictionary. We show that there exist dictionaries which
generalize under the minimal encoder, i.e. the encoding algorithm which encodes a string into the
shortest number of possible tokens, but at the same time, completely fail to generalize under the
greedy encoder. This means that in the process of constructing good tokenizers, it does not suffice to
think about the dictionary in isolation. Its interaction with the encoding algorithm is pertinent.
Definition E.1 (minimal encoder). The minimal encoder parses a new string into the fewest possible
number of tokens from the dictionary as possible. Ties are broken arbitrarily.
Theorem E.2. There exists a stochastic source parameterized by δ ∈(0, 0.5) and a dictio-
nary Dict such that under the minimal encoder/decoder pair, the resulting tokenizer, T
=
(Dict, ∅, encmin(·), decmin(·)) generalizes near-optimally,"
R,0.8077285579641847,"min
Q∈Q1-gram L(Q ◦encmin(·)) ≤1.273H∞.
(40)"
R,0.8086710650329878,"Here the entropy rate of the source, H∞, is δ log(
√"
R,0.8096135721017907,"2/δ) + (1 −δ) log(1/(1 −δ)). However, the
same dictionary Dict under the greedy encoder/decoder pair, i.e. T ′ = (Dict, ∅, encgre(·), decgre(·)),
generalizes poorly, suffering from cross-entropy scaling as,"
R,0.8105560791705938,"min
Q∈Q1-gram L(Q ◦encgre(·)) ≥1 −oδ(1)"
R,0.8114985862393967,"3
H(π).
(41)"
R,0.8124410933081998,where the entropy of the stationary distribution of the source is H(π) = 1
R,0.8133836003770029,"2 log(8) and the 1 −oδ(1)
term is (1 −δ)2(1 + δ)−1. 0
1
2 δ δ
2 δ
2
δ"
R,0.8143261074458058,Figure 10: order-1 Markov source used in the proof of Theorem E.2
R,0.8152686145146089,"This means that the greedy encoder is not really compatible with the dictionary in the sense that the
cross-entropy loss of the tokenizer is a constant multiple away from that achieved by the character-
level tokenizer. The separation between eq. (40), and eq. (41) only manifests as δ becomes smaller
and smaller."
R,0.8162111215834119,"In this section, we prove that generalization of a dictionary is a function of the underlying tokenization
algorithm used. In particular, the greedy encoder is not universal, and there exists dictionaries under
the minimum-length encoder/decoder which achieve small cross-entropy loss, which do not generalize
under the greedy encoder/decoder."
R,0.8171536286522149,"We split the proof of Theorem E.2 into two parts. We first define the stochastic source and dictionary
we consider. Then we show that under the minimum-length encoder, the asymptotic cross-entropy
loss is upper bounded by H∞up to a constant. Finally, we show that under the greedy-encoder, the
same dictionary suffers from high cross-entropy loss, which is a constant factor away from that of the
character encoder."
R,0.8180961357210179,"E.1
Stochastic source and dictionary."
R,0.819038642789821,"Consider an extension of the switching Markov source in example A.1 to A = {0, 1, 2}. The Markov
chain is described in Figure 10. The transition of the Markov chain is P(0|0) = P(1|1) = P(2|2) =
1 −δ, and P(1|0) = P(2|1) = δ and P(2|1) = P(0|1) = δ/2, with the remaining transitions being
0-probability. For a parameter ℓ> 0 to be instantiated later, define S1 (resp. S0, S2) as the set of
all-1 (resp. all-0, all-2) strings of length ≤ℓ−1, including the empty string. Consider a dictionary
composed of the following set of tokens, {1s : s ∈S0 ∪S1 ∪S2}. Therefore, the tokens follow the
template 10 · · · 0, 11 · · · 1 or 12 · · · 2 and are of length at most ℓ. ℓis chosen to be 1 + 2 log(1/δ)/δ."
R,0.8199811498586239,"Although we use the minimal encoder in the statement of Theorem E.2, for the purpose of analysis,
define the following encoding algorithm: if the new string is prefixed by 10 · · · 0 or 12 · · · 2, select
the largest prefix which exists in dictionary and assign it as a token. If the new string starts with a
sequence 11 · · · 1 of length x, consider the first max{ℓ, x −1} length prefix and assign it as a token.
Finally, if the string starts with 0 or 2, assign that character as token. Once the first token has been
assigned, remove it and repeat."
R,0.820923656927427,"E.2
Minimal encoder achieves the optimal cross-entropy loss up to a constant."
R,0.82186616399623,"First consider a simplification of the overall cross-entropy loss,"
R,0.822808671065033,"min
Q∈Q1-gram lim
m→∞
1
mLm(Q ◦enc(·))"
R,0.823751178133836,"=
min
Q∈Q1-gram lim
m→∞−1"
R,0.824693685202639,"mE

log Q#(|encmin(s)|) +
X"
R,0.825636192271442,"t∈Dict nt log Qtok(t)

(42)"
R,0.8265786993402451,"≤lim
m→∞
1
mE

log(m) + |encmin(s)| log |Dict|

,
(43)"
R,0.827521206409048,"where in the last inequality we upper bound by choosing Q# = Unif([m]) and Qtok(t) = 1/|Dict|.
Note that |Dict| ≤2ℓ+ 1 and letting limm→∞log(m)/m = 0,"
R,0.8284637134778511,"min
Q∈Q1-gram lim
m→∞
1
mLm(Q ◦enc(·)) ≤lim
m→∞
1
mE[|encmin(s)| log(2ℓ+ 1)]"
R,0.8294062205466541,"≤lim
m→∞
1
mE

|enc(s)| log(2ℓ+ 1)

,
(44)"
R,0.8303487276154571,"where in (i), we replace |encmin(s)| by |enc(s)|, which is the encoder we define in Appendix E.1.
By definition of the minimal encoder, |encmin(s)| ≤|enc(s)| surely. Recall that the encoder enc(·)
processes strings in a sequential (left-to-right) manner. In particular, by a similar argument as
Lemma A.4, we can show that under this encoder, the limit nt/ P"
R,0.8312912346842601,"t′ nt′ almost surely converges to
its expectation. More importantly, since, P"
R,0.8322337417530632,"t∈Dict |t|nt = m, we have that,"
R,0.8331762488218661,"lim
m→∞
|enc(s)| m"
R,0.8341187558906692,"a.s.
=
1
Et∼QMLE[|t|]."
R,0.8350612629594723,"converges to some limit almost surely. Therefore, from eq. (44),"
R,0.8360037700282752,"min
Q∈Q1-gram lim
m→∞
1
mLm(Q ◦enc(·)) ≤ess limsup
m→∞
|enc(s)|"
R,0.8369462770970783,"m
log(2ℓ+ 1).
(45)"
R,0.8378887841658812,"where the essential lim-sup captures the almost sure limit 1/Et∼QMLE[|t|]. The almost sure conver-
gence of |enc(s)|/m also implies that we can let the limit m go to ∞in any manner, and the limit
will remain the same. In particular, consider a process parameterized by i⋆for generating the source
string, such that surely m ≥i⋆, where the total number of characters, m, is a random variable. As
i⋆→∞, we will also have m →∞surely, and so the limit of |enc(s)|/m under this modified
stochastic process should also converge to the same limit."
R,0.8388312912346843,"Rather than sampling a string of a fixed length m from the source, consider the following sampling
model: for i⋆→∞, sample i⋆geometric random variables X1, · · · , Xi⋆i.i.d.
∼Geo(δ) and construct
the source string as the concatenation of i⋆strings alternating between successive 1’s and successive
0’s or 2’s (with the choice between the two made uniformly at random), with the ith string of length
Xi + 1. The overall number of characters sampled, m, is surely at least i⋆."
R,0.8397737983034873,"Under this stochastic process, the size of the encoding of the string is upper bounded by,"
R,0.8407163053722903,"|enc(s)| ≤|X1 + 1| + i⋆
X i=2"
R,0.8416588124410933,"
1 + (Xi + 1 −ℓ)+
"
R,0.8426013195098964,"This bound follows from the fact that in any substring s′ of successive 1’s followed by a substring s′′
of successive 0’s or 2’s, the encoder tokenizes the first max{ℓ, |s′| −1} length prefix of s′ as a token,
and the remaining characters in s′ into individual tokens except the last. Then, the last character of s′
and the first max{ℓ−1, |s′′|} characters of s′′ are assigned as token. The remainder of s′′ is assigned
as individual tokens. Each of s′ or s′′ of length x, is allocated into at most 1 + (x + 1 −ℓ)+ tokens."
R,0.8435438265786993,"For any i, Pr(Xi ≥u) = (1−δ)u, and therefore, summing over u ≥ℓ, we get that E[(Xi+1−ℓ)+] =
(1−δ)ℓ−1"
R,0.8444863336475024,"δ
. With ℓ= 1 + 2 log(1/δ)/δ, this expectation is upper bounded by δ. Therefore,"
R,0.8454288407163054,"lim
i⋆→∞
E[|enc(s)|]"
R,0.8463713477851084,"i⋆
≤lim
i⋆→∞
1
i⋆E

|X1| +
Xi⋆ i=2"
R,0.8473138548539114,"
1 + (Xi + 1 −ℓ)+

≤1 + δ"
R,0.8482563619227145,"More importantly, by the strong law of large numbers for a sum of independent random variables,
(|X1 +1|+Pi⋆"
R,0.8491988689915174,"i=2(1+(Xi +1−ℓ)+))/i⋆, and therefore |enc(s)|/i⋆is asymptotically almost surely
upper bounded as,"
R,0.8501413760603205,"lim
i⋆→∞
|enc(s)| i⋆"
R,0.8510838831291234,"a.s.
≤1 + δ,
(46)"
R,0.8520263901979265,"On the other hand, the number of characters generated, m, equals Pi⋆"
R,0.8529688972667295,"i=1(Xi + 1), and satisfies,
limi⋆→∞E[m]/i⋆= 1 + δ−1. By another application of the strong law of large numbers for a sum
of independent random variables,"
R,0.8539114043355325,"lim
i⋆→∞
m
i⋆
a.s.
= 1 + δ−1.
(47)"
R,0.8548539114043355,"By combining eqs. (46) and (47), we have that,"
R,0.8557964184731386,"lim
i⋆→∞
|enc(s)| m"
R,0.8567389255419415,"a.s.
≤
1 + δ
1 + δ−1 = 1 δ ."
R,0.8576814326107446,"Finally, combining with eq. (45) and the ensuing discussion, we may upper bound the limiting
cross-entropy loss by,"
R,0.8586239396795476,"min
Q∈Q1-gram lim
m→∞
1
mLm(Q ◦enc(·)) ≤δ log(2ℓ+ 1) = δ log(3 + 4 log(1/δ)/δ)."
R,0.8595664467483506,"Note for this Markovian source, it is a short calculation to see that,"
R,0.8605089538171536,"H∞= Ex∼π[H(P(·|x))] = δ log(
√"
R,0.8614514608859567,2/δ) + (1 −δ) log(1/(1 −δ))
R,0.8623939679547596,"Note that for any δ ≤1/2, numerical evaluation gives the inequality,"
R,0.8633364750235627,1 ≤δ log(3 + 4 log(1/δ)/δ)
R,0.8642789820923656,"H∞
≤1.273"
R,0.8652214891611687,"with the approximation factor improving as δ becomes smaller. Therefore, this tokenizer achieves a
normalized cross-entropy loss which asymptotically scales as a constant multiple of the entropy rate
of the source."
R,0.8661639962299718,"E.3
Greedy-encoder achieves poor cross-entropy loss"
R,0.8671065032987747,"Note that the greedy encoder picks the largest prefix of the string which is a token, assigns and
removes it, and iterates on the rest of the string. The greedy encoder’s behavior is easy to analyze -
every string of consecutive 1’s in the new string is broken into chunks of length ℓ(save potentially
the last chunk) and each chunk is assigned as a token in {1s : s ∈S1} ⊂Dict. If the length of this
substring of successive 1’s is not 1, ℓ+ 1, 2ℓ+ 1, · · · , or in general, ≡1 mod ℓ, every character in
the next sequence, composed of 0’s or 2’s is tokenized into individual characters."
R,0.8680490103675778,"Similar to eq. (42) to eq. (43), consider a simplification of the overall cross-entropy loss,"
R,0.8689915174363808,"min
Q∈Q1-gram lim
m→∞
1
mLm(Q ◦encgre(·))"
R,0.8699340245051838,"=
min
Q∈Q1-gram lim
m→∞−1 mE """
R,0.8708765315739868,"log Q#(|encgre(s)|) + |encgre(s)|
X"
R,0.8718190386427899,"t∈Dict
nt
|encgre(s)| log Qtok(t) #"
R,0.8727615457115928,"≥
min
Q∈Q1-gram lim
m→∞−1 mE """
R,0.8737040527803959,"|encgre(s)|
X"
R,0.8746465598491989,"t∈Dict
QMLE(t)>0
QMLE(t) log Qtok(t) # ,"
R,0.8755890669180019,"where
the
last
equation
uses
the
fact
that
by
Lemma
A.4,
for
the
greedy
en-
coder,
limm→∞
nt
|encmin(s)|
a.s.
=
QMLE(t).
The minimizer of this objective subject to
P
t∈Dict:QMLE(t)>0 Qtok(t) ≤1 is Qtok(t) = QMLE(t) resulting in the inequality,"
R,0.8765315739868049,"min
Q∈Q1-gram lim
m→∞
1
mLm(Q ◦encgre(·)) ≥lim
m→∞
1
mE

|encgre(s)|H(QMLE)

,
(48)"
R,0.8774740810556079,"where we use the convention 0 log(1/0) ≜limP →0 P log(1/P) = 0 and therefore we may sum over
tokens such that QMLE(t) = 0 for free."
R,0.8784165881244109,"Considering the same geometric sampling model as in Appendix E.2, and Lemma A.4, we may study
the almost sure limit QMLE(t) = limm→∞nt/|encgre(s)| by computing limi⋆→∞nt/|encgre(s)|
under the geometric sampling model since the almost sure limit exists. Recall that in the geometric
sampling model, we generate the overall source string by concatenating i⋆strings of length X1 +
1, · · · , Xi⋆+ 1 where Xi ∼Geo(δ), with the strings alternating between successive 1’s and
successive 0’s or 2’s (with the choice between the two made by the flip of a fair coin). For x ∈
{0, 1, 2}, let Ei(x) denote the event that Xi is a string composed only of all x’s. The length of the
greedy encoding of s is lower bounded by,"
R,0.879359095193214,"|encgre(s)| ≥ i⋆
X"
R,0.8803016022620169,"i=1
Xi · I(Xi−1 ̸≡1
mod ℓ)I(Ei(0) ∪Ei(2)).
(49)"
R,0.88124410933082,"Which captures for the fact that all 0’s and 2’s are encoded into singular tokens unless the previous
string of 1’s was of length ≡1 mod ℓ. By the law of large numbers of the RHS of eq. (49), the
following a.a.s. lower bound is satisfied,"
R,0.882186616399623,"lim
i⋆→∞
|encgre(s)| i⋆"
R,0.883129123468426,"a.s.
≥1 2δ  1 − ∞
X"
R,0.884071630537229,"u=0
δ(1 −δ)ℓu+1  = 1 2δ"
R,0.8850141376060321,"
1 −
δ(1 −δ)
1 −(1 −δ)ℓ"
R,0.885956644674835,"
≥1 −δ"
R,0.8868991517436381,"2δ ,
(50)"
R,0.8878416588124411,"where the last inequality uses the fact that ℓ= 1+2 log(1/δ)/δ. Likewise, observe that, |encgre(s)| ≤
m surely, and following the analysis in Appendix E.2 of eq. (47), we have that,"
R,0.8887841658812441,"lim
i⋆→∞
|encgre(s)|"
R,0.8897266729500471,"i⋆
≤lim
i⋆→∞
m
i⋆
a.s.
= 1 + δ−1.
(51)"
R,0.8906691800188501,"For x ∈{0, 2}, observe that the expected number of times the token x is observed in the encoding of
s, nx can be written as, nx ≥ i⋆
X i=1"
R,0.8916116870876531," 
(Xi + 1) · I(Xi−1 ̸≡1
mod ℓ)

I(Ei(x)).
(52)"
R,0.8925541941564562,"In particular, taking the expectation of eq. (52),"
R,0.8934967012252591,"E[nx|E1(0) ∪E1(2)], E[nx|E1(1)] ≥i⋆−1"
R,0.8944392082940622,"4
(1 + δ−1)  1 − ∞
X"
R,0.8953817153628653,"u=0
δ(1 −δ)ℓu+1 "
R,0.8963242224316682,≥i⋆−1
R,0.8972667295004713,"4
· 1 −δ2 δ
. (53)"
R,0.8982092365692743,"Note that in any realization of the geometric sampling process, in eq. (52), either the odd indexed
substrings are all-1’s or the even indexed substrings are all-1’s. Therefore, surely, all the non-zero
terms in the above summation are of the same parity. Moreover, since the ith term in the sum only
depends on Xi and Xi−1, conditioned on whether the non-zero parities are even or odd, nx can be
written as a sum of ≈i⋆/2 mutually independent terms. By the strong law of large numbers on each
of the conditional processes, eqs. (52) and (53) implies that for x ∈{0, 2},"
R,0.8991517436380773,"lim
i⋆→∞
nx i⋆"
R,0.9000942507068803,"a.s.
≥1 −δ2 4δ
."
R,0.9010367577756834,"To upper bound nx, note that it is upper bounded by the number of times the character x appears in
the source string, which by the strong law of large numbers a.a.s (after normalizing by i⋆), scales
as 1/4δ. Finally, to bound QMLE(t) which is the sequential nature of the encoder, using a similar
proof as Lemma A.4, we can show that nt/ P"
R,0.9019792648444863,"t′ nt′ converges to the unigram MLE model for this
tokenizer. For the token x ∈{0, 2},"
R,0.9029217719132894,"lim
i⋆→∞
nx
|enc(s)| = QMLE(x) ≤E

lim
i⋆→∞
nx
n2 + n0"
R,0.9038642789820923,"
(54)"
R,0.9048067860508954,"Using the a.a.s. upper and lower bounds on |enc(s)|, n0 and n2 derived in eqs. (51) and (54), we
arrive at lower and upper bounds on QMLE(x) for x ∈{0, 2},"
R,0.9057492931196984,"1
4 ≈1 −δ"
R,0.9066918001885014,"4
=
(1 −δ2)
4δ(1 + δ−1) ≤QMLE(x) ≤
1
2(1 −δ2) ≈1 2."
R,0.9076343072573044,"Since there are at least two tokens having probability bounded away from 0 and 1 by a constant under
the MLE unigram model, the entropy of QMLE must also be lower bounded by a constant. Indeed,"
R,0.9085768143261075,"H(QMLE) ≥2
min
1−δ"
R,0.9095193213949104,"4
≤y≤
1
2(1−δ2)
y log(1/y)."
R,0.9104618284637135,"It is easy to verify that for δ ≤0.5, the minimizer is achieved at y = 1−δ"
R,0.9114043355325165,"4 , which leads to the lower
bound,"
R,0.9123468426013195,"H(QMLE) ≥
1 −δ 2"
R,0.9132893496701225,"
log

4
1 −δ "
R,0.9142318567389256,"Architecture
GPT-2
Batch size
Grid-searched in {8, 16, 32}
Gradient acc. steps
1
Tokenizer dictionary size
{10, 20}
Tokenizer dataset size
10, 000
Optimizer
AdamW (β1 = 0.9, β2 = 0.95)
Learning rate
0.002
Scheduler
Cosine
# Iterations
8000
Weight decay
1 × 10−3"
R,0.9151743638077285,"Dropout
0
Sequence length
512
Embedding dimension
Grid-searched in {10, 20, 30, 40}
# layers
Grid-searched in {1, 2, 4, 8}
# heads
Grid-searched in {1, 2, 4, 8, 16}
Repetitions
5"
R,0.9161168708765316,Table 3: Hyperparameter choices
R,0.9170593779453345,"Finally, combining this lower bound on H(QMLE) with eq. (48), we have that,"
R,0.9180018850141376,"min
Q∈Q1-gram lim
m→∞
1
mLm(Q ◦enc(·)) = lim
i⋆→∞E
|encgre(s)|"
R,0.9189443920829407,"m
H(QMLE)
"
R,0.9198868991517436,"≥lim
i⋆→∞E
|encgre(s)| m"
R,0.9208294062205467,"
·
1 −δ 2"
R,0.9217719132893497,"
log

4
1 −δ "
R,0.9227144203581527,"(i)
≥
1 −δ
2δ(1 + δ−1) ·
1 −δ 2"
R,0.9236569274269557,"
log

4
1 −δ "
R,0.9245994344957588,≥(1 −δ)2
R,0.9255419415645617,3(1 + δ)H(π)
R,0.9264844486333648,"where (i) follows from the lower bound on |encgre(s)| in eq. (50) with the almost sure limit of m in
eq. (47) and noting that |encgre(s)|/m ≤1 surely. The last inequality follows by simplifying using
π = (1/4, 1/2, 1/4) and H(π) = 1"
R,0.9274269557021678,2 log(8).
R,0.9283694627709708,"F
Experiment details"
R,0.9293119698397738,"Experiment 1 (Figures 4a and 4b).
In this and previous experiments (Figures 2, 3a and 3b), we
train the transformers on a single GPU on an 8× A100 node. The wall-clock time measured does not
count time spent in validation loss evaluations. The hyperparameter choices are listed in Table 3."
R,0.9302544769085768,"Experiment 2 (Table 1).
We evaluate pre-trained tokenizers on various datasets. In this experiment,
we do not evaluate the likelihood model on test sequences, rather, we estimate the cross-entropy of
the best unigram model by using the approximation, −E "" X"
R,0.9311969839773798,"t∈Dict
nt log QMLE(t) # ≈−
X"
R,0.9321394910461829,"t∈Dict
bnt log( bQ(t))
(55)"
R,0.9330819981149858,"where bQ(t) =
bnt
P"
R,0.9340245051837889,"t bnt is the MLE unigram model learnt from a finite dataset, which we choose here as
GLUE (Wang et al., 2019), and bnt is the number of times the token t is observed in the encoding of
the dataset. This approximation allows us to separate the error stemming from learning a suboptimal
likelihood model which tends to have higher sample complexity requirements and focus on the
asymptotic error of the tokenizer."
R,0.9349670122525919,"We use Monte-carlo sampling to approximate the cross-entropy loss estimator in eq. (55). These
approximations tends to underestimate the true cross-entropy loss due to the concavity of x log(1/x)
close to 0. In general, the gap between the approximation and the true error is expected to grow
with k. Therefore, the true difference between the estimate of the best unigram model on a tokenizer
and the best k-gram model for k ≥2 on the character level tokenizer is likely to be larger than the
reported figures."
R,0.9359095193213949,"Experiment 3 (Figure 5).
We train the LZW, BPE, Unigram and Wordpiece tokenizers with
dictionary sizes {5000, 6000, 8000, 12000, 20000, 32000, 50000, 80000}. The cross-entropy loss
incurred by the best 1-gram model is estimated using eq. (55) while for k-gram models for k ≥2, we
use Monte-carlo sampling to estimate the cross-entropy of the empirical k-gram model computed
using the GLUE dataset. For the k-gram models trained on the character level tokenizer, since the
vocabulary size is fixed, we instead plot the number of distinct k-grams on the x-axis. While this is
not a true measure of the number of parameters in the underlying k-gram model, we use this as a
proxy for the same."
R,0.9368520263901979,"G
NeurIPS Paper Checklist"
R,0.937794533459001,"The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and precede the (optional) supplemental material. The checklist does NOT
count towards the page limit."
R,0.9387370405278039,"Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:"
R,0.939679547596607,"• You should answer [Yes] , [No] , or [NA] ."
R,0.94062205466541,"• [NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available."
R,0.941564561734213,• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
CLAIMS,0.942507068803016,1. Claims
CLAIMS,0.943449575871819,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.944392082940622,Answer: [Yes]
CLAIMS,0.9453345900094251,"Justification: The paper lists an empirical phenomenon (justified in Fig. 2) and theoretical
contributions justified in Theorems 3.1, 3.3 and 3.5"
CLAIMS,0.946277097078228,Guidelines:
CLAIMS,0.9472196041470311,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.9481621112158342,2. Limitations
LIMITATIONS,0.9491046182846371,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.9500471253534402,Answer: [Yes]
LIMITATIONS,0.9509896324222432,Justification: Remark 3.3
LIMITATIONS,0.9519321394910462,Guidelines:
LIMITATIONS,0.9528746465598492,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon."
LIMITATIONS,0.9538171536286523,"• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
LIMITATIONS,0.9547596606974552,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Assumption 3.2
Guidelines:"
LIMITATIONS,0.9557021677662583,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
LIMITATIONS,0.9566446748350612,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The code has been released along with the rest of the submission.
Guidelines:"
LIMITATIONS,0.9575871819038643,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm."
LIMITATIONS,0.9585296889726673,"(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.9594721960414703,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9604147031102733,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.9613572101790764,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9622997172478793,Justification: Instructions provided in the jupyter notebook.
OPEN ACCESS TO DATA AND CODE,0.9632422243166824,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9641847313854854,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.9651272384542884,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9660697455230914,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.9670122525918945,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9679547596606974,Justification: Table 3
OPEN ACCESS TO DATA AND CODE,0.9688972667295005,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9698397737983034,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9707822808671065,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9717247879359096,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9726672950047125,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9736098020735156,"Justification: All plots which allow for it, contain standard error bars."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9745523091423186,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9754948162111216,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9764373232799246,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9773798303487277,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9783223374175306,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9792648444863337,Justification: Appendix F contains this information.
EXPERIMENTS COMPUTE RESOURCES,0.9802073515551367,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9811498586239397,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.9820923656927427,9. Code Of Ethics
CODE OF ETHICS,0.9830348727615457,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9839773798303487,Answer: [Yes]
CODE OF ETHICS,0.9849198868991518,Justification: No NeurIPS code of ethics were violated.
CODE OF ETHICS,0.9858623939679547,Guidelines:
CODE OF ETHICS,0.9868049010367578,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9877474081055608,10. Broader Impacts
BROADER IMPACTS,0.9886899151743638,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This work is a primarily theoretical study on the behavior of tokenization on
toy problems (learning Markov chains). The societal impact of this research is not likely to
be significant.
Guidelines:"
BROADER IMPACTS,0.9896324222431668,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
BROADER IMPACTS,0.9905749293119699,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: No models with a high risk for misuse were trained or released.
Guidelines:"
BROADER IMPACTS,0.9915174363807728,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
BROADER IMPACTS,0.9924599434495759,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: Code has been properly credited, via citing the relevant paper.
Guidelines:"
BROADER IMPACTS,0.9934024505183789,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
BROADER IMPACTS,0.9943449575871819,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: No new assets released.
Guidelines:"
BROADER IMPACTS,0.9952874646559849,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
BROADER IMPACTS,0.9962299717247879,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: No crowdsourcing or research with human subjects.
Guidelines:"
BROADER IMPACTS,0.9971724787935909,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: No crowdsourcing or research with human subjects."
BROADER IMPACTS,0.998114985862394,Guidelines:
BROADER IMPACTS,0.9990574929311969,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
