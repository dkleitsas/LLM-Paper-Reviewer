Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001937984496124031,"Missing data is a commonly occurring problem in practice. Many imputation meth-
1"
ABSTRACT,0.003875968992248062,"ods have been developed to fill in the missing entries. However, not all of them
2"
ABSTRACT,0.005813953488372093,"can scale to high-dimensional data, especially the multiple imputation techniques.
3"
ABSTRACT,0.007751937984496124,"Meanwhile, the data nowadays tending toward high-dimensional. Therefore, in this
4"
ABSTRACT,0.009689922480620155,"work, we propose Principal Component Analysis Imputation (PCAI), a simple but
5"
ABSTRACT,0.011627906976744186,"versatile framework based on Principal Component Analysis (PCA) to speed up
6"
ABSTRACT,0.013565891472868217,"the imputation process and alleviate memory issue of many available imputation
7"
ABSTRACT,0.015503875968992248,"techniques, without sacrificing the imputation quality in term of MSE. In addition,
8"
ABSTRACT,0.01744186046511628,"the frameworks can be used even when some or all of the missing features are
9"
ABSTRACT,0.01937984496124031,"categorical, or when the number of missing features is large. Next, we introduce
10"
ABSTRACT,0.02131782945736434,"PCA Imputation - Classification (PIC), an application of PCAI for classification
11"
ABSTRACT,0.023255813953488372,"problem with some adjustment. We validate our approach by experiments on vari-
12"
ABSTRACT,0.025193798449612403,"ous scenarios, which shows that PCAI and PIC can work with various imputation
13"
ABSTRACT,0.027131782945736434,"algorithms, including the state-of-the-art ones and improve the imputation speed
14"
ABSTRACT,0.029069767441860465,"significantly, while achieving competitive mean square error/classification accuracy
15"
ABSTRACT,0.031007751937984496,"compared to direct imputation (i.e., impute directly on the missing data).
16"
INTRODUCTION,0.03294573643410853,"1
Introduction
17"
INTRODUCTION,0.03488372093023256,"Despite recent efforts in directly handling missing data [1, 2, 3, 4], missing data imputation approaches
18"
INTRODUCTION,0.03682170542635659,"[5, 6, 7] remain commonly used. This is because directly handling missing data can be complicated
19"
INTRODUCTION,0.03875968992248062,"and usually are developed for specific target problems or models, while imputation can be more
20"
INTRODUCTION,0.040697674418604654,"versatile. Specifically, an important advantage of imputation is that the imputed data becomes
21"
INTRODUCTION,0.04263565891472868,"complete, i.e., no longer have any missing values. Therefore, it is easier to continue with other
22"
INTRODUCTION,0.044573643410852716,"preprocessing steps, analysis, and data visualizations. Furthermore, one can deploy many models
23"
INTRODUCTION,0.046511627906976744,"and choose the best one by using the available packages or software tools for non-missing data.
24"
INTRODUCTION,0.04844961240310078,"Meanwhile, directly handling missing data strategies do not have these advantages. They are more
25"
INTRODUCTION,0.050387596899224806,"complicated and not that readily available.
26"
INTRODUCTION,0.05232558139534884,"Many techniques have been developed for missing data imputation, ranging from traditional tech-
27"
INTRODUCTION,0.05426356589147287,"niques such as MICE [5], K-Nearest Neighbors to recent machine learning/deep learning techniques
28"
INTRODUCTION,0.0562015503875969,"such as GAIN [6], DL-GSA [7]. However, most of them are computationally expensive for big
29"
INTRODUCTION,0.05813953488372093,"datasets. For example, experiments in [8] show that under their experiment settings, for Fashion
30"
INTRODUCTION,0.060077519379844964,"MNIST [9], a dataset of 70,000 samples and 784 features, the MICE [5] and missForest [10] tech-
31"
INTRODUCTION,0.06201550387596899,"niques are unable to finish the imputation process within three hours for a missing rate (the ratio
32"
INTRODUCTION,0.06395348837209303,"between the number of missing entries versus the total number entries in the dataset) of 20%. Since
33"
INTRODUCTION,0.06589147286821706,"datasets nowadays are trending towards larger sizes [11], with hundreds of thousands of features
34"
INTRODUCTION,0.06782945736434108,"[12], it is crucial to speed up the available imputation techniques. Taking into account resource
35"
INTRODUCTION,0.06976744186046512,"consumption and availability such speed up cannot be achieved by only providing more and better
36"
INTRODUCTION,0.07170542635658915,"hardware but by the development of new methods.
37"
INTRODUCTION,0.07364341085271318,"To achieve this goal, this work introduces two novel frameworks based on Principal Component
38"
INTRODUCTION,0.0755813953488372,"Analysis (PCA) to speed up the imputation process of many available techniques or the imputation-
39"
INTRODUCTION,0.07751937984496124,"classification process for missing data classification problems. The first framework, PCA Imputation
40"
INTRODUCTION,0.07945736434108527,"(PCAI) is proposed to speed up the imputation speed by partitioning the data into the fully observed
41"
INTRODUCTION,0.08139534883720931,"features partition and the partition of features with missing data. After that, the imputation of the
42"
INTRODUCTION,0.08333333333333333,"missing part is performed based on the union of the PCA - reduced version of the fully observed
43"
INTRODUCTION,0.08527131782945736,"part and the missing part. Interestingly, it turns out that the method has a great potential to aid the
44"
INTRODUCTION,0.0872093023255814,"performance of methods that rely on many parameters, such as Deep Learning imputation techniques.
45"
INTRODUCTION,0.08914728682170543,"Meanwhile, the second one, PCA Imputation - Classification (PIC) is proposed to deal with the
46"
INTRODUCTION,0.09108527131782945,"missing data classification problems where dimension reduction is desirable in advance of the model
47"
INTRODUCTION,0.09302325581395349,"training step. PIC is based on PCAI with some modifications. Note that these frameworks are
48"
INTRODUCTION,0.09496124031007752,"different from the methods developed for principal component analysis under missing data presented
49"
INTRODUCTION,0.09689922480620156,"in [13, 14], which are about how to conduct PCA when the data contains missing values.
50"
INTRODUCTION,0.09883720930232558,"In summary, the contributions of this article are: (i) we introduce PCAI to improve the imputation
51"
INTRODUCTION,0.10077519379844961,"speed of many available imputation techniques; (ii) we introduce PIC to deal with missing data
52"
INTRODUCTION,0.10271317829457365,"classification problems where dimension reduction is desirable; (iii) we analyze the potential strength
53"
INTRODUCTION,0.10465116279069768,"and drawbacks of these approaches; and (iv) we illustrate via experiments that our frameworks
54"
INTRODUCTION,0.1065891472868217,"can work with various imputation strategies while achieve comparable or even lower mean square
55"
INTRODUCTION,0.10852713178294573,"error/higher classification accuracies compared to the corresponding original approaches, and alleviate
56"
INTRODUCTION,0.11046511627906977,"the memory issue in some approaches.
57"
INTRODUCTION,0.1124031007751938,"The rest of the paper is organized as follows. In Section 2 and Section 3, we review some related
58"
INTRODUCTION,0.11434108527131782,"work in the field of missing data, and review two popular formulations of PCA. Next, in Section
59"
INTRODUCTION,0.11627906976744186,"4, Section 5, and Section 6, we introduce our novel PCAI and PIC frameworks, and study their
60"
INTRODUCTION,0.1182170542635659,"relation to previous works, respectively. After that, in Section 7, we demonstrate their capabilities
61"
INTRODUCTION,0.12015503875968993,"via experiments on various datasets. The paper ends with conclusions, remarks, and future works in
62"
INTRODUCTION,0.12209302325581395,"Section 8.
63"
RELATED WORKS,0.12403100775193798,"2
Related Works
64"
RELATED WORKS,0.12596899224806202,"Various works have been published on missing data imputation to deal with different data analysis
65"
RELATED WORKS,0.12790697674418605,"situations. As an example, if one is interested in modeling the uncertainty associated with the
66"
RELATED WORKS,0.1298449612403101,"imputation, suitable approaches can be multiple or Bayesian imputation techniques such as multiple
67"
RELATED WORKS,0.13178294573643412,"imputations using Deep Denoising Autoencoders [15], Bayesian Principal Component Analysis-
68"
RELATED WORKS,0.13372093023255813,"based imputation [16], and extreme learning machine multiple imputation [17]. In addition, graphical
69"
RELATED WORKS,0.13565891472868216,"models can be prominent candidates when transparency, estimability, and testability are desirable, and
70"
RELATED WORKS,0.1375968992248062,"these approaches can provide meaningful performance guarantees even if the missing values are not at
71"
RELATED WORKS,0.13953488372093023,"random [18]. Next, for continuous data, matrix completion techniques such as Fast Alternating Least
72"
RELATED WORKS,0.14147286821705427,"Squares [19], softImpute [20] can quickly give good results. In biology, the missing values are often
73"
RELATED WORKS,0.1434108527131783,"categorical, and the imputed values need to be interpretable. In such cases, classification techniques
74"
RELATED WORKS,0.14534883720930233,"or tree-based methods such as decision trees and fuzzy clustering with iterative learning (DIFC) [21],
75"
RELATED WORKS,0.14728682170542637,"missForest [10], the DMI algorithm [22], and sequential regression trees [23] are well-suited. In
76"
RELATED WORKS,0.14922480620155038,"addition, some recently developed methods that can handle mixed missing data are SICE [24], FEMI
77"
RELATED WORKS,0.1511627906976744,"[25], and HCMM-LD [26]. When the sample sizes are large enough compared to the number of
78"
RELATED WORKS,0.15310077519379844,"features, deep learning techniques such as Multiple Imputation Using Deep Denoising Autoencoders
79"
RELATED WORKS,0.15503875968992248,"[15], DL-GSA [7], and Swarm Intelligence-Deep Neural Network [27] can be powerful imputers.
80"
RELATED WORKS,0.1569767441860465,"However, it is worth noting that deep learning methods usually require more data than statistical
81"
RELATED WORKS,0.15891472868217055,"imputation approaches. Some other popularly used missing data imputation methods are multiple
82"
RELATED WORKS,0.16085271317829458,"imputation by chained equation (MICE) [5], K-nearest Neighbors imputation (KNNI) [28], and mean
83"
RELATED WORKS,0.16279069767441862,"imputation [28].
84"
RELATED WORKS,0.16472868217054262,"In addition, for the purpose of data imputation and data type, for classification, the impact of
85"
RELATED WORKS,0.16666666666666666,"imputation techniques on different classifiers may vary. Specifically, [28] compares the performance
86"
RELATED WORKS,0.1686046511627907,"of logistic regression with regularization, k-nearest neighbours (kNN), random forest, classification
87"
RELATED WORKS,0.17054263565891473,"tree, and xgboost classifiers [28] on datasets with missing entries. They use different imputation
88"
RELATED WORKS,0.17248062015503876,"methods (mean imputation/ MICE imputation [5]/ missForest [10]/ random imputation/ softImpute
89"
RELATED WORKS,0.1744186046511628,"[20]/ hot deck imputation, kNN imputation) and compare the performance. According to the paper,
90"
RELATED WORKS,0.17635658914728683,"mean imputation seems to outperform other counterparts for logistic regression with regularization
91"
RELATED WORKS,0.17829457364341086,"and kNN, random imputation wins for random forest, missForest seems to be the best imputer for
92"
RELATED WORKS,0.18023255813953487,"classification tree, and hot deck imputation is the best for xgboost.
93"
RELATED WORKS,0.1821705426356589,"With the rapid growth of data size [11, 12], it is necessary to speed up the available imputation
94"
RELATED WORKS,0.18410852713178294,"methods because many current approaches remain too slow for big datasets, as pointed out in an
95"
RELATED WORKS,0.18604651162790697,"example in Section 1. This is where a popular dimension reduction method like PCA can come to use.
96"
RELATED WORKS,0.187984496124031,"PCA projects the original higher-dimensional dataset into a representation of lower dimensionality
97"
RELATED WORKS,0.18992248062015504,"by extracting and retaining important information from the data and expressing this new information
98"
RELATED WORKS,0.19186046511627908,"based on a set of orthogonal vectors known as principal components. Its goal is to find linear
99"
RELATED WORKS,0.1937984496124031,"transformations of the original data that retain the maximal amount of variance. Note that there are
100"
RELATED WORKS,0.19573643410852712,"some works on PCA under data missingness. For example, [13] considers the problem of finding
101"
RELATED WORKS,0.19767441860465115,"principal components as an optimization problem of an objective function and proposes iterative
102"
RELATED WORKS,0.1996124031007752,"solutions to it. On the other hand, [29] proposes a multiple imputation method for the estimates of the
103"
RELATED WORKS,0.20155038759689922,"parameters (components and axes) of PCA to take into account the variability due to missing values.
104"
RELATED WORKS,0.20348837209302326,"However, our work is different from these works in the sense that they target the problem of how to
105"
RELATED WORKS,0.2054263565891473,"perform PCA for a dataset with missing data. Meanwhile, our frameworks utilize PCA to speed up
106"
RELATED WORKS,0.20736434108527133,"the imputation processor to reduce the ratio between the number of features and the sample size.
107"
PRELIMINARIES,0.20930232558139536,"3
Preliminaries
108"
PRELIMINARIES,0.21124031007751937,"Let X = [xij] where i = 1, ..., n; j = 1, ..., p be a input data matrix of n samples, p features. In
109"
PRELIMINARIES,0.2131782945736434,"addition, assume that the features are centered and scaled. We review two popular formulations of
110"
PRELIMINARIES,0.21511627906976744,"PCA, which we refer to as PCA formulation 1 (PCA-form1) and PCA formulation 2 (PCA-form2).
111"
PRELIMINARIES,0.21705426356589147,"3.1
PCA based on covariance matrix (PCA-form1)
112"
PRELIMINARIES,0.2189922480620155,"Let Σ be the covariance matrix of X. Next, let (λ1, v1), ..., (λp, vp) be the sorted eigenvalue-
113"
PRELIMINARIES,0.22093023255813954,"eigenvector pairs of Σ such that λ1 ≥λ2 ≥... ≥λp ≥0. Suppose that we choose the first r pairs
114"
PRELIMINARIES,0.22286821705426357,"for dimension reduction. Then the amount of variance explained by these r pairs is
115"
PRELIMINARIES,0.2248062015503876,"λ1 + λ2 + ... + λr
λ1 + λ2 + ... + λp (1)"
PRELIMINARIES,0.22674418604651161,"In addition, let V = [v1, v2, ..., vr]. Then the dimension reduced version of X is XV.
116"
PRELIMINARIES,0.22868217054263565,"3.2
PCA based on the input matrix X (PCA-form2)
117"
PRELIMINARIES,0.23062015503875968,"The solution of PCA can also be produced based on the singular value decomposition of X [30]:
118"
PRELIMINARIES,0.23255813953488372,"X = UDWT
(2)"
PRELIMINARIES,0.23449612403100775,"where U is an n × p orthogonal matrix, W is a p × p orthogonal matrix, and D is a p × p diagonal
119"
PRELIMINARIES,0.2364341085271318,"matrix whose diagonal elements are d1 ≥d2 ≥... ≥dp ≥0. Suppose that r eigenvalues are used,
120"
PRELIMINARIES,0.23837209302325582,"then the projection matrix is V = WrWT
r where Wr consists of the first r columns of W. Then
121"
PRELIMINARIES,0.24031007751937986,"the dimension reduced version of X is also XV.
122"
PRELIMINARIES,0.24224806201550386,"4
PCA Imputation (PCAI)
123"
PRELIMINARIES,0.2441860465116279,"In this section, we detail our PCAI framework, a PCA based framework that is capable of significantly
124"
PRELIMINARIES,0.24612403100775193,"improving the imputation speed of an imputer for high dimensional data, alleviating the memory
125"
PRELIMINARIES,0.24806201550387597,"issue for many approaches.
126"
PRELIMINARIES,0.25,"To start with some notations, let pca(A) be a function of a data matrix A. The function returns
127"
PRELIMINARIES,0.25193798449612403,"(RA, V ) where RA is the PCA-reduced version of A, and V is the projection matrix where the ith
128"
PRELIMINARIES,0.25387596899224807,"column of V is the eigenvector corresponding to the ith largest eigenvalue. In addition, denote by
129"
PRELIMINARIES,0.2558139534883721,"A ∪B the columnwise concatenation of two data partition A and B of relevant sizes. Next, suppose
130"
PRELIMINARIES,0.25775193798449614,"that we have a dataset D = F ∪M, where F consists of data from fully observed features and M
131"
PRELIMINARIES,0.2596899224806202,"consists of data from features with missing values.
132"
PRELIMINARIES,0.2616279069767442,"The framework is as depicted in Algorithm 1. We first conduct dimension reduction on the fully
133"
PRELIMINARIES,0.26356589147286824,"observed partition F, which produces a reduced version R of F. Then, the imputation of M is done
134"
PRELIMINARIES,0.2655038759689923,"on the set R ∪M instead of D = F ∪M as how imputations are usually done (i.e., impute directly
135"
PRELIMINARIES,0.26744186046511625,"on the original missing data). In conducting dimension reduction, we expect to reduce the dimension
136"
PRELIMINARIES,0.2693798449612403,"of the fully observed partition so that the imputation of M can be faster.
137"
PRELIMINARIES,0.2713178294573643,"Algorithm 1 PCAI framework
Require:"
PRELIMINARIES,0.27325581395348836,"- D = F ∪M where F is the fully observed partition and M is the partition with missing values
- Imputer I
- PCA algorithm pca
Procedure:"
PRELIMINARIES,0.2751937984496124,"(R, V ) ←pca(F)
M′ ←the imputed version of M based on R ∪M
return Imputed version M′ of M"
PRELIMINARIES,0.2771317829457364,"For the choice of the PCA formulation, note that if the number of samples is larger than the number
138"
PRELIMINARIES,0.27906976744186046,"of features in F, then the size of the covariance matrix is smaller than the size of F. Therefore, one
139"
PRELIMINARIES,0.2810077519379845,"may expect using the formulation of PCA based on the covariance matrix, as in Section 3.1, to be
140"
PRELIMINARIES,0.28294573643410853,"faster. Meanwhile, if the number of features in F is larger than the sample size, then the covariance
141"
PRELIMINARIES,0.28488372093023256,"matrix of F is larger than F. Therefore, in such a case, it is better to use the PCA formulation based
142"
PRELIMINARIES,0.2868217054263566,"on the data itself, i.e., formulation as in Section 3.2.
143"
PRELIMINARIES,0.28875968992248063,"One may reckon that using R ∪M instead of F ∪M may lead to loss of information due to
144"
PRELIMINARIES,0.29069767441860467,"dimension reduction and therefore lower the quality of imputation. However, as will be illustrated
145"
PRELIMINARIES,0.2926356589147287,"in the experiments, the differences between the mean squared error of the imputed version versus
146"
PRELIMINARIES,0.29457364341085274,"the ground truth for these approaches are only slightly different, and many times, PCAI seems to be
147"
PRELIMINARIES,0.29651162790697677,"slightly better. This is possibly because PCA retains the important information from the data while
148"
PRELIMINARIES,0.29844961240310075,"removing some noise, and therefore helps improving the imputation quality. However, PCAI also has
149"
PRELIMINARIES,0.3003875968992248,"some shortcomings. For problems where the sample size n is smaller than the number of features in
150"
PRELIMINARIES,0.3023255813953488,"the fully observed block q, if PCA-form1 is used, the covariance matrix has the size of q × q, which
151"
PRELIMINARIES,0.30426356589147285,"is bigger than the size n × q of the fully observed partition F. This may make the PCA dimension
152"
PRELIMINARIES,0.3062015503875969,"reduction process become computationally expensive, rendering PCAI to be slower than imputing
153"
PRELIMINARIES,0.3081395348837209,"directly on the original missing data. This issue will be illustrated in the experiment section.
154"
PRELIMINARIES,0.31007751937984496,"5
PCAI for classification (PIC)
155"
PRELIMINARIES,0.312015503875969,"In this section, we discuss a straightforward application of PCAI in classification, with a slight
156"
PRELIMINARIES,0.313953488372093,"modification for classification problems where it is desirable to conduct a dimension reduction before
157"
PRELIMINARIES,0.31589147286821706,"training a model, such as when the number of features is much larger than the sample size.
158"
PRELIMINARIES,0.3178294573643411,"Since PCAI conducts PCA on the fully observed partition F, it reduces the dimensions for a portion
159"
PRELIMINARIES,0.31976744186046513,"of the data. Therefore, rather than imputing values using the PCAI framework and then conducting
160"
PRELIMINARIES,0.32170542635658916,"a dimension reduction step on F ∪M′, one can perform dimension reduction on M′ to get R′, a
161"
PRELIMINARIES,0.3236434108527132,"PCA-reduced version of M′. Then, one can use F ∪R′ as reduced dimension data. As will be shown
162"
PRELIMINARIES,0.32558139534883723,"in the experiments, this speeds up the imputation and classification process significantly. This is the
163"
PRELIMINARIES,0.32751937984496127,"basic idea of our Principle component Imputation for Classification (PIC) framework.
164"
PRELIMINARIES,0.32945736434108525,"PIC operates as shown in Algorithm 2. The procedure starts by performing PCA on the training fully
165"
PRELIMINARIES,0.3313953488372093,"observed partition Ftrain, which gives the reduced version Rtrain of Ftrain and a projection matrix
166"
PRELIMINARIES,0.3333333333333333,"V . Next, we project Ftest on V to get the reduced version Rtest of Ftest. Then, we impute Mtrain
167"
PRELIMINARIES,0.33527131782945735,"on Rtrain ∪Mtrain to get the imputed version M′
train. Next, we impute Mtest on Rtest ∪Mtest
168"
PRELIMINARIES,0.3372093023255814,"to get the imputed version M′
test. After that, if reducemiss is set to true, we perform dimension
169"
PRELIMINARIES,0.3391472868217054,"reduction on M′
train, M′
test. Then, we train the classifier on Rtrain ∪R′
train, i.e., the union of the
170"
PRELIMINARIES,0.34108527131782945,"reduced version of Ftrain and the reduced version of Mtrain. For prediction of a vector x ∈D,
171"
PRELIMINARIES,0.3430232558139535,"we can decompose x into x = (xF, xM). After that, we can project xF on V to get a projection r.
172"
PRELIMINARIES,0.3449612403100775,"Similarly, we can project xM on V to a get projection r′. Finally, we can predict the label of x using
173"
PRELIMINARIES,0.34689922480620156,"the classifier C with input (r, r′).
174"
PRELIMINARIES,0.3488372093023256,"Note that reducemiss is an option. When the number of features in the missing partition M is
175"
PRELIMINARIES,0.3507751937984496,"large, one may be interested in reducing the dimension of M′, and therefore, set reducemiss to True.
176"
PRELIMINARIES,0.35271317829457366,"However, when the number of features in the missing partition is small, one may want to keep it to
177"
PRELIMINARIES,0.3546511627906977,"False. Also, since PIC is a straightforward application of PCAI for classification, the choice of PCA
178"
PRELIMINARIES,0.35658914728682173,"formulation should be used is similar to PCAI, which is analyzed in the previous section.
179"
PRELIMINARIES,0.35852713178294576,"Algorithm 2 PIC framework
Require:"
PRELIMINARIES,0.36046511627906974,"- D = F ∪M where F is the fully observed partition and M is the partition with missing values
- reducemiss = True/False: if True, perform dimension reduction on the imputed partitions; if
False, do not perform dimension reduction on the imputed partitions"
PRELIMINARIES,0.3624031007751938,"- Ftrain, Ftest: the training and testing data of the fully observed partition F, respectively
- Mtrain, Mtest: the training and testing data of the partition that has missing data M, respectively"
PRELIMINARIES,0.3643410852713178,"- Imputer I, classifier C, PCA algorithm pca
Procedure:"
PRELIMINARIES,0.36627906976744184,"(Rtrain, V ) ←pca(Ftrain)
Rtest ←FtestV
M′
train ←imputed version of Mtrain based on Rtrain ∪Mtrain
M′
test ←imputed version of Mtest based on Rtest ∪Mtest
if reducemiss then"
PRELIMINARIES,0.3682170542635659,"(R′
train, W) ←pca(M′
train)
R′
test ←M′
testV
Train the classifier C based on Rtrain ∪R′
train
Classify based on Rtest ∪R′
test,
else"
PRELIMINARIES,0.3701550387596899,"Train the classifier based on Rtrain ∪M′
train
Classify based on Rtest ∪M′
test
end if
return trained classifier C"
RELATION TO PREVIOUS WORKS,0.37209302325581395,"6
Relation to previous works
180"
RELATION TO PREVIOUS WORKS,0.374031007751938,"Various works have been done on PCA that are related to missing data, which mostly can be
181"
RELATION TO PREVIOUS WORKS,0.375968992248062,"categorized into missing values imputation using PCA, or dimension reduction using PCA under
182"
RELATION TO PREVIOUS WORKS,0.37790697674418605,"missing values. Some typical works that make use of PCA for missing values imputation are
183"
RELATION TO PREVIOUS WORKS,0.3798449612403101,"probabilistic PCA for missing flow volume data imputation [31]; chunk-wise iterative PCA for
184"
RELATION TO PREVIOUS WORKS,0.3817829457364341,"data imputation on datasets with many samples[32]; [14] proposes a fast algorithm for PCA under
185"
RELATION TO PREVIOUS WORKS,0.38372093023255816,"missing data that help in case of sparse, high dimensional data; [33] analyze maximum likelihood
186"
RELATION TO PREVIOUS WORKS,0.3856589147286822,"PCA (MLPCA) on maximum likelihood missing data imputation; and [34] proposed an imputation
187"
RELATION TO PREVIOUS WORKS,0.3875968992248062,"approach based on PCA and factorial analysis for mixed data.
188"
RELATION TO PREVIOUS WORKS,0.38953488372093026,"Next, PCA under missing values was first studied in [35], where only one component and one
189"
RELATION TO PREVIOUS WORKS,0.39147286821705424,"imputation iteration are used. After that, [36] proposes a method based on MLPCA, where the
190"
RELATION TO PREVIOUS WORKS,0.39341085271317827,"method assigns large variance to missing values prior to implementing the method, which aim to
191"
RELATION TO PREVIOUS WORKS,0.3953488372093023,"guide the algorithm to fit a PCA model disregarding those points. Also, [37] introduce EM algorithm
192"
RELATION TO PREVIOUS WORKS,0.39728682170542634,"for building a PCA model that can deal with missing data. More recently, [38] proposes new
193"
RELATION TO PREVIOUS WORKS,0.3992248062015504,"techniques for building a PCA model with missing data: known data regression (KDR), projection to
194"
RELATION TO PREVIOUS WORKS,0.4011627906976744,"the model plane, KDR with principal component regression.In addition, [39] studies estimation and
195"
RELATION TO PREVIOUS WORKS,0.40310077519379844,"imputation in Probabilistic PCA when the data is missing not at random.
196"
RELATION TO PREVIOUS WORKS,0.4050387596899225,"Different from the previous approaches, PCAI is a framework to speed up the imputation process,
197"
RELATION TO PREVIOUS WORKS,0.4069767441860465,"which can be used with various imputation methods, including the aforementioned PCA imputation
198"
RELATION TO PREVIOUS WORKS,0.40891472868217055,"algorithms and the state-of-the-art imputation algorithms such as softImpute [20], MissForest [10],
199"
RELATION TO PREVIOUS WORKS,0.4108527131782946,"GAIN [6]. In addition, note that since PCAI and PIC conduct dimension reduction on the fully
200"
RELATION TO PREVIOUS WORKS,0.4127906976744186,"observed partition F, and not the missing portion M if reducemiss = False, they can handle missing
201"
RELATION TO PREVIOUS WORKS,0.41472868217054265,"data even if categorical features presents in the missing portion M, when being used with imputers
202"
RELATION TO PREVIOUS WORKS,0.4166666666666667,"that’s capable of handling categorical/mixed data (MissForest [10], SICE [24], FEMI [25], etc.). In
203"
RELATION TO PREVIOUS WORKS,0.4186046511627907,Table 1: Description of datasets used in our experiments
RELATION TO PREVIOUS WORKS,0.42054263565891475,"Dataset
# Classes
# Features
# Samples"
RELATION TO PREVIOUS WORKS,0.42248062015503873,"Parkinson [42]
2
754
756
Fashion MNIST [9]
10
784
70000
Gene [43]
5
20531
801"
RELATION TO PREVIOUS WORKS,0.42441860465116277,"addition, even if there exists categorical and continuous features in M; or reducemiss = True and
204"
RELATION TO PREVIOUS WORKS,0.4263565891472868,"there exists categorical and continuous features in M, one can easily adjust the algorithm to conduct
205"
RELATION TO PREVIOUS WORKS,0.42829457364341084,"PCA on continuous features only. The previously mentioned PCA based approaches are, however,
206"
RELATION TO PREVIOUS WORKS,0.43023255813953487,"can only be used for continuous data, because PCA requires the data to be continuous.
207"
EXPERIMENTS,0.4321705426356589,"7
Experiments
208"
GENERAL EXPERIMENT SETTINGS,0.43410852713178294,"7.1
General experiment settings
209"
GENERAL EXPERIMENT SETTINGS,0.436046511627907,"We compare the speed (seconds) and MSE of PCAI with direct imputation (DI), i.e., use an
210"
GENERAL EXPERIMENT SETTINGS,0.437984496124031,"imputation algorithm directly on the dataset. The imputation approaches used for comparison:
211"
GENERAL EXPERIMENT SETTINGS,0.43992248062015504,"softImpute [20, 40], MissForest [10] 1 and Multiple Imputation by Chained Equation (MICE) [5, 41],
212"
GENERAL EXPERIMENT SETTINGS,0.4418604651162791,"kNN Imputation (KNNI), GAIN [6] are implemented with default configurations. The codes will be
213"
GENERAL EXPERIMENT SETTINGS,0.4437984496124031,"available upon the acceptance of the paper. For PIC, we compare the five fold cross-validation (CV)
214"
GENERAL EXPERIMENT SETTINGS,0.44573643410852715,"score (accuracy, speed) of PIC when dimension reduction is applied on the imputed missing part
215"
GENERAL EXPERIMENT SETTINGS,0.4476744186046512,"(PIC-reduce), when dimension reduction is not applied on the imputed missing part (PIC), and when
216"
GENERAL EXPERIMENT SETTINGS,0.4496124031007752,"PCA is applied to the imputed version on the full missing data (DI-reduce), and when no dimension
217"
GENERAL EXPERIMENT SETTINGS,0.45155038759689925,"reduction is applied to imputed data after direct imputation (DI). Here, the default PCA formulation
218"
GENERAL EXPERIMENT SETTINGS,0.45348837209302323,"is PCA-form1, unless specified otherwise. For all PCA computation, the number of eigenvectors is
219"
GENERAL EXPERIMENT SETTINGS,0.45542635658914726,"chosen so that the minimum amount of variance explained is 95%.
220"
GENERAL EXPERIMENT SETTINGS,0.4573643410852713,"Details of the datasets used in the experiments are available in Table 1. All experiments are run on
221"
GENERAL EXPERIMENT SETTINGS,0.45930232558139533,"an AMD Ryzen 7 3700X CPU with 8 Cores, 16 processing threads, 3.6GHz, and 16GB RAM.We
222"
GENERAL EXPERIMENT SETTINGS,0.46124031007751937,"terminate an experiment if no result is produced after 6,500 seconds of running or if there arises a
223"
GENERAL EXPERIMENT SETTINGS,0.4631782945736434,"memory allocating issue, and we denote this as NA in the result tables.
224"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.46511627906976744,"7.2
Performance of PCAI and PIC when the missing values in M are randomly simulated
225"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.46705426356589147,"Table 2: (MSE, speed) for PCAI and direct imputation (DI) on the Parkinson dataset with q = 700."
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.4689922480620155,missing rate
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.47093023255813954,"Imputer
Strategy
20%
40%
60%"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.4728682170542636,"softImpute
PCAI
(0.073, 0.860)
(0.185, 0.774)
(0.305, 0.875)
DI
(0.072, 4.097)
(0.188, 4.043)
(0.308, 4.467)"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.4748062015503876,"MICE
PCAI
(0.091, 139.811)
(0.186, 85.241)
(0.369, 109.815)
DI
NA
NA
NA"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.47674418604651164,"GAIN
PCAI
(0.254, 45.046)
(0.538, 43.938)
(0.779, 43.956)
DI
(0.608, 69.839)
(1.097, 70.548)
(1.369, 70.293)"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.4786821705426357,"missForest
PCAI
(0.064, 188.324)
(0.163, 178.849)
(0.292, 138.085)
DI
(0.058, 905.002)
(0.160, 692.150)
(0.258, 449.415)"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.4806201550387597,"KNNI
PCAI
(0.127, 0.355)
(0.299, 0.398)
(0.466, 0.416)
DI
(0.113, 0.310)
(0.274, 0.337)
(0.426, 0.372)"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.48255813953488375,"Note that any datasets can be rearranged so that the first q features are not missing and the remaining
226"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.4844961240310077,"ones are missing. Therefore, without loss of generality, we assume that the first q features of each
227"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.48643410852713176,1https://pypi.org/project/missingpy/
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.4883720930232558,"Table 3: (MSE, speed) for PCAI and DI on the Fashion MNIST dataset with q = 700. MissForest
results all are NA, and therefore are removed from the tables."
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.4903100775193798,missing rate
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.49224806201550386,"Imputer
Strategy
20%
40%
60%"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.4941860465116279,"softImpute
PCAI
(0.032, 22.408)
(0.066, 22.797)
(0.109, 25.603)
DI
(0.032, 67.627)
(0.064, 69.349)
(0.107, 77.233)"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.49612403100775193,"MICE
PCAI
(0.027, 2218.864)
(0.055, 1374.558)
(0.095, 1641.962)
DI
NA
NA
NA"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.49806201550387597,"GAIN
PCAI
(0.053, 65.730)
(0.091, 68.752)
(0.137, 69.743)
DI
(0.041, 97.898)
(0.079, 99.049)
(0.125, 96.317)"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5,"KNNI
PCAI
(0.055, 1607.850)
(0.115, 2033.153)
(0.180, 2272.370)
DI
(0.049, 3042.752)
(0.102, 3659.300)
(0.161, 3959.832)"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.501937984496124,"dataset are not missing, and the remaining ones contain missing value(s). Then, we simulated missing
228"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5038759689922481,"data randomly on the missing partition M with missing rates 20%, 40%, and 60%. Here, a missing
229"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5058139534883721,"rate of 20% means that 20% of the entries in the missing partition M are missing. The results for
230"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5077519379844961,"such experiments are reported in Tables 2, 3, 4. Due to space limit, the results related to PIC on
231"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5096899224806202,Fashion MNIST are reported in the Appendix.
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5116279069767442,"Table 4: Five fold CV results (accuracy, speed) of SVM on Parkinson with q = 700."
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5135658914728682,missing rate
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5155038759689923,"Imputer
Strategy
20%
40%
60%"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5174418604651163,"softImpute
PIC-reduce
(0.862, 1.026)
(0.862, 1.137)
(0.862, 1.161)
PIC
(0.858, 1.008)
(0.858, 1.079)
(0.859, 1.112)
DI-reduce
(0.861, 4.116)
(0.862, 4.424)
(0.861, 4.718)
DI
(0.858, 3.775)
(0.858, 3.912)
(0.855, 4.248)"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5193798449612403,"MICE
PIC-reduce
(0.859, 204.605)
(0.861, 256.340)
(0.861, 240.211)
PIC
(0.858, 524.739)
(0.859, 694.667)
(0.859, 925.426)
DI-reduce
NA
NA
NA
DI
NA
NA
NA"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5213178294573644,"GAIN
PIC-reduce
(0.857, 91.086)
(0.852, 102.861)
(0.848, 122.349)
PIC
(0.851, 89.984)
(0.853, 104.773)
(0.853, 123.233)
DI-reduce
(0.855, 130.349)
(0.851, 149.864)
(0.851, 181.135)
DI
(0.846, 129.702)
(0.849, 152.031)
(0.852, 183.67)"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5232558139534884,"missForest
PIC-reduce
(0.859, 204.850)
(0.861, 276.537)
(0.858, 153.783)
PIC
(0.858, 202.939)
(0.861, 277.067)
(0.858, 153.463)
DI-reduce
(0.861, 656.948)
(0.862, 729.872)
(0.861, 472.230)
DI
(0.858, 655.750)
(0.861, 730.013)
(0.858, 472.388)"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5251937984496124,"KNNI
PIC-reduce
(0.858, 0.533)
(0.861, 0.462)
(0.862, 0.625)
PIC
(0.858, 0.513)
(0.861, 0.462)
(0.862, 0.607)
DI-reduce
(0.862, 0.696)
(0.862, 0.642)
(0.859, 0.803)
DI
(0.859, 0.438)
(0.859, 0.45)
(0.858, 0.552) 232"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5271317829457365,"From the tables, it is clear that the proposed frameworks reduce the imputation time significantly
233"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5290697674418605,"while maintaining competitive MSE/classification accuracy compared to DI, in most of the cases.
234"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5310077519379846,"For example, at the missing rate 20% on the Parkinson dataset (Table 4), when using GAIN for
235"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5329457364341085,"imputation, the running time of PIC-reduce(91.086s) is much lower compared to DI-reduce (130.349),
236"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5348837209302325,"the running time of PIC (89.984s) is also much lower compared to DI (129.702). Another example
237"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5368217054263565,"can be seen from Table 2, for the Parkinson dataset, at 20% missing rate, when PCAI is applied to
238"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5387596899224806,"missForest, the running time reduces to 188.324s, which is almost 1/5 of the DI (905.002s). Next,
239"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5406976744186046,"on Fashion MNIST (Table 3), it is worth noticing that for MICE, DI cannot gives the results due to
240"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5426356589147286,"memory issue but PCAI can alleviate this issue and deliver the results.
241"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5445736434108527,"For KNNI, the running time for KNNI between the PCAI approach and direct imputation for
242"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5465116279069767,"Parkinson (Table 2) is not much different. However, for the Fashion MNIST dataset, KNNI using the
243"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5484496124031008,"PCAI framework obviously deliver a competitive result in a significantly shorter time. Specifically,
244"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5503875968992248,"KNNI at a missing rate of 20% on Fashion MNIST gives a result after only 1607.850 seconds, while
245"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5523255813953488,"DI takes up to 3,042.752 seconds. This is because Fashion MNIST (70000 samples) has much more
246"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5542635658914729,"samples than Parkinson (756 samples), and KNN need to do a lot of pairwise comparison. Therefore,
247"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5562015503875969,"PCAI and PIC would be extremely helpful for KNNI when the sample size and the number of fully
248"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5581395348837209,"observed features is large. Note that it does not require the number of features with missing data to
249"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.560077519379845,"be large or small.
250"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.562015503875969,"From Table 2, we can see that PCAI generates a lot of improvements in MSE for GAIN, in addition
251"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.563953488372093,"to improvements in speed. This is possibly because PCA reduces the number of features while the
252"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5658914728682171,"sample size remains the same, making such a deep learning approach more applicable to the newly
253"
PERFORMANCE OF PCAI AND PIC WHEN THE MISSING VALUES IN M ARE RANDOMLY SIMULATED,0.5678294573643411,"reduced data.
254"
PERFORMANCE ON NONRANDOMLY MISSING DATA,0.5697674418604651,"7.3
Performance on nonrandomly missing data
255"
PERFORMANCE ON NONRANDOMLY MISSING DATA,0.5717054263565892,"In many fields, the data are missing in a monotone pattern rather than random [44]. Therefore, we
256"
PERFORMANCE ON NONRANDOMLY MISSING DATA,0.5736434108527132,"generate one-step monotone missing data on Fashion MNIST by first, randomly choose 20%, 40%,
257"
PERFORMANCE ON NONRANDOMLY MISSING DATA,0.5755813953488372,"60% of the samples. Then, we make them become missing by deleting the lower right corner by
258"
PERFORMANCE ON NONRANDOMLY MISSING DATA,0.5775193798449613,"deleting the intersection between the last 8 rows and the last 13 columns of each image array. The
259"
PERFORMANCE ON NONRANDOMLY MISSING DATA,0.5794573643410853,"results are reported in Table 5. From the table, we can see that PIC-reduce is a great improvement in
260"
PERFORMANCE ON NONRANDOMLY MISSING DATA,0.5813953488372093,"speed compared to DI-reduce, and PIC is a significant improvement in speed compared to DI. This
261"
PERFORMANCE ON NONRANDOMLY MISSING DATA,0.5833333333333334,"illustrates that PIC can work effectively even for non-randomly missing data.
262"
PERFORMANCE ON NONRANDOMLY MISSING DATA,0.5852713178294574,"Table 5: Five fold CV results (accuracy, speed) of SVM on monotone data generated on Fashion
MNIST."
PERFORMANCE ON NONRANDOMLY MISSING DATA,0.5872093023255814,missing rate
PERFORMANCE ON NONRANDOMLY MISSING DATA,0.5891472868217055,"Imputer
Strategy
20%
40%
60%"
PERFORMANCE ON NONRANDOMLY MISSING DATA,0.5910852713178295,"softImpute
PIC-reduce
(0.889, 409.676)
(0.889, 421.671)
(0.889, 369.49)
PIC
(0.889, 507.626)
(0.89, 543.309)
(0.889, 480.452)
DI-reduce
(0.89, 439.268)
(0.89, 528.892)
(0.889, 395.646)
DI
(0.891, 738.494)
(0.891, 872.616)
(0.89, 646.781)"
PERFORMANCE ON NONRANDOMLY MISSING DATA,0.5930232558139535,"GAIN
PIC-reduce
(0.886, 462.478)
(0.883, 429.173)
(0.882, 449.786)
PIC
(0.886, 493.399)
(0.882, 484.232)
(0.881, 496.066)
DI-reduce
(0.891, 543.803)
(0.89, 431.947)
(0.89, 454.981)
DI
(0.892, 902.049)
(0.891, 754.794)
(0.891, 780.686)"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.5949612403100775,"7.4
PIC under different PCA formulations and number of missing features
263"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.5968992248062015,"The missing data in these experiments are generated at random as in Section 7.2 and the five fold
264"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.5988372093023255,"cross validation results of SVM on the Gene dataset with q = 15000, 20000, are shown in Table 6
265"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6007751937984496,"and Table 7. From these tables, one can see clearly that for datasets where the number of features
266"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6027131782945736,"are significantly higher than the number of samples such as Gene, PCA-form2, which is based on
267"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6046511627906976,"the input data (F specifically) gives much faster computations compared to PCA-form1, and also is
268"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6065891472868217,"faster than direct imputation-classification without PCA. In addition, when PCA-form1 is used, even
269"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6085271317829457,"though PIC and PIC-reduce are faster than PCA on directly imputed data (DI-reduce), they are still
270"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6104651162790697,"much slower than direct imputation - classification without PCA.
271"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6124031007751938,"Interestingly, the accuracy PIC and PIC-reduce are almost identical to PCA on directly imputed data,
272"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6143410852713178,"and are higher than direct imputation - classification without PCA. Next, note that the main idea of
273"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6162790697674418,"the proposed methods is to reduce the dimension of the F to speed up the imputation. Therefore, we
274"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6182170542635659,"have made no assumption about the number of features in the missing portion M. In Table 6 and
275"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6201550387596899,"Table 7, q = 15000, 20000, which means 5,531 and 531 missing features in M, respectively. This
276"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.622093023255814,"implies PIC can handle datasets where M has many features.
277"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.624031007751938,"Table 6: Five fold CV results (accuracy, speed) of SVM for softImpute based strategies on the Gene
dataset when q = 15000."
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.625968992248062,missing rate
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.627906976744186,"Strategy
20%
40%
60%"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6298449612403101,"PCA-form1
PIC-reduce
(0.994, 2250.451)
(0.992, 2412.082)
(0.992, 2415.434)
PIC
(0.992, 2429.114)
(0.992, 2276.354)
(0.992, 2284.414)
DI-reduce
(0.994, 5018.368)
(0.994, 4529.766)
(0.994, 3785.947)"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6317829457364341,"PCA-form2
PIC-reduce
(0.995, 69.444)
(0.992, 76.393)
(0.992, 85.2)
PIC
(0.992, 61.451)
(0.992, 68.571)
(0.992, 77.528)
DI-reduce
(0.995, 80.823)
(0.992, 92.265)
(0.994,100.751)"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6337209302325582,"No PCA
DI
(0.985, 71.884)
(0.985, 74.812)
(0.985, 92.309)"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6356589147286822,"Table 7: Five fold CV results (accuracy, speed) of SVM for softImpute based strategies on the Gene
dataset when q = 20000."
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6375968992248062,missing rate
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6395348837209303,"Strategy
20%
40%
60%"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6414728682170543,"PCA-form1
PIC-reduce
(0.994, 2578.910)
(0.994, 4001.717)
(0.994, 3848.950)
PIC
(0.994, 2583.717)
(0.994, 4144.157)
(0.994, 4057.188)
DI-reduce
(0.995, 2891.994)
(0.994, 4476.563)
(0.995, 4332.869)"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6434108527131783,"PCA-form2
PIC-reduce
(0.995, 67.753)
(0.992, 73.884)
(0.995, 81.27)
PIC
(0.995, 59.815)
(0.995, 66.096)
(0.995, 73.079)
DI-reduce
(0.995, 81.07)
(0.995, 82.407)
(0.995, 91.638)"
PIC UNDER DIFFERENT PCA FORMULATIONS AND NUMBER OF MISSING FEATURES,0.6453488372093024,"No PCA
DI
(0.985, 74.06)
(0.985, 71.6)
(0.985, 84.963)"
CONCLUSION AND REMARKS,0.6472868217054264,"8
Conclusion and Remarks
278"
CONCLUSION AND REMARKS,0.6492248062015504,"We have presented two novel frameworks for datasets where many continuous features are fully
279"
CONCLUSION AND REMARKS,0.6511627906976745,"observed, PCAI and PIC, that can speed up imputation algorithms significantly while having com-
280"
CONCLUSION AND REMARKS,0.6531007751937985,"petitive accuracy MSE/accuracy compared to direct imputation and alleviate the memory issue for
281"
CONCLUSION AND REMARKS,0.6550387596899225,"some imputation approaches such as MICE, kNN. In addition, the frameworks can be used even
282"
CONCLUSION AND REMARKS,0.6569767441860465,"when some or all of the missing features are categorical or when the number of missing features
283"
CONCLUSION AND REMARKS,0.6589147286821705,"is large. Note that when the sample size is significantly larger than the number of fully observed
284"
CONCLUSION AND REMARKS,0.6608527131782945,"features, PCA-form1 should be used since, in such a case, the covariance matrix is much smaller than
285"
CONCLUSION AND REMARKS,0.6627906976744186,"F, making it faster than PCA-form2. On the other hand, when the number of fully observed features
286"
CONCLUSION AND REMARKS,0.6647286821705426,"is significantly larger than the sample size, PCA-form2 should be preferred, as the covariance matrix
287"
CONCLUSION AND REMARKS,0.6666666666666666,"is bigger than F itself in such a case. A limitation of the proposed framework is that if there are not
288"
CONCLUSION AND REMARKS,0.6686046511627907,"many fully observed continuous features, then due to the computational cost of PCA, the proposed
289"
CONCLUSION AND REMARKS,0.6705426356589147,"frameworks may not lead to any improvement in speed.
290"
CONCLUSION AND REMARKS,0.6724806201550387,"Even though PIC is only introduced for classification, the same strategy can be applied to a regression
291"
CONCLUSION AND REMARKS,0.6744186046511628,"problem. We would like to explore that in the future. Moreover, since various dimension reduction
292"
CONCLUSION AND REMARKS,0.6763565891472868,"techniques such as sparse PCA [45], incremental PCA [46], truncated SVD [47] have been developed
293"
CONCLUSION AND REMARKS,0.6782945736434108,"to suit different scenarios, it is worth investigating different dimension reduction techniques for PCAI
294"
CONCLUSION AND REMARKS,0.6802325581395349,"and PIC. In addition, it would be interesting to explore if applying a PCA variant to the missing
295"
CONCLUSION AND REMARKS,0.6821705426356589,"partition M would result in even a more efficient method for datasets with continuous features in the
296"
CONCLUSION AND REMARKS,0.6841085271317829,"missing partition.
297"
REFERENCES,0.686046511627907,"References
298"
REFERENCES,0.687984496124031,"[1] Zachary C Lipton, David C Kale, Randall Wetzel, et al. Modeling missing data in clinical time
299"
REFERENCES,0.689922480620155,"series with rnns. Machine Learning for Healthcare, 56, 2016.
300"
REFERENCES,0.6918604651162791,"[2] Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recurrent
301"
REFERENCES,0.6937984496124031,"neural networks for multivariate time series with missing values. Scientific reports, 8(1):1–12,
302"
REFERENCES,0.6957364341085271,"2018.
303"
REFERENCES,0.6976744186046512,"[3] Mostafa Mehdipour Ghazi, Mads Nielsen, Akshay Pai, M Jorge Cardoso, Marc Modat, Sebastien
304"
REFERENCES,0.6996124031007752,"Ourselin, and Lauge Sørensen. Robust training of recurrent neural networks to handle missing
305"
REFERENCES,0.7015503875968992,"data for disease progression modeling. arXiv preprint arXiv:1808.05500, 2018.
306"
REFERENCES,0.7034883720930233,"[4] Thu Nguyen, Duy H.M. Nguyen, Huy Nguyen, Binh T. Nguyen, and Bruce A. Wade. Epem:
307"
REFERENCES,0.7054263565891473,"Efficient parameter estimation for multiple class monotone missing data. Information Sciences,
308"
REFERENCES,0.7073643410852714,"567:1–22, 2021.
309"
REFERENCES,0.7093023255813954,"[5] S van Buuren and Karin Groothuis-Oudshoorn. mice: Multivariate imputation by chained
310"
REFERENCES,0.7112403100775194,"equations in r. Journal of statistical software, pages 1–68, 2010.
311"
REFERENCES,0.7131782945736435,"[6] Jinsung Yoon, James Jordon, and Mihaela Schaar. Gain: Missing data imputation using
312"
REFERENCES,0.7151162790697675,"generative adversarial nets. In International conference on machine learning, pages 5689–5698.
313"
REFERENCES,0.7170542635658915,"PMLR, 2018.
314"
REFERENCES,0.7189922480620154,"[7] Ayush Garg, Deepika Naryani, Garvit Aggarwal, and Swati Aggarwal. Dl-gsa: a deep learning
315"
REFERENCES,0.7209302325581395,"metaheuristic approach to missing data imputation. In International Conference on Sensing and
316"
REFERENCES,0.7228682170542635,"Imaging, pages 513–521. Springer, (2018).
317"
REFERENCES,0.7248062015503876,"[8] Thu Nguyen, Khoi Minh Nguyen-Duy, Duy Ho Minh Nguyen, Binh T. Nguyen, and Bruce Alan
318"
REFERENCES,0.7267441860465116,"Wade. Dper: Direct parameter estimation for randomly missing data. Knowledge-Based Systems,
319"
REFERENCES,0.7286821705426356,"240:108082, 2022.
320"
REFERENCES,0.7306201550387597,"[9] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for
321"
REFERENCES,0.7325581395348837,"benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
322"
REFERENCES,0.7344961240310077,"[10] Daniel J Stekhoven and Peter Bühlmann. Missforest-non-parametric missing value imputation
323"
REFERENCES,0.7364341085271318,"for mixed-type data. Bioinformatics, 28(1):112–118, 2012.
324"
REFERENCES,0.7383720930232558,"[11] Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert P Trevino, Jiliang Tang, and
325"
REFERENCES,0.7403100775193798,"Huan Liu. Feature selection: A data perspective. ACM computing surveys (CSUR), 50(6):1–45,
326"
REFERENCES,0.7422480620155039,"2017.
327"
REFERENCES,0.7441860465116279,"[12] Isabelle Guyon, Jiwen Li, Theodor Mader, Patrick A Pletscher, Georg Schneider, and Markus
328"
REFERENCES,0.7461240310077519,"Uhr. Competitive baseline methods set new standards for the nips 2003 feature selection
329"
REFERENCES,0.748062015503876,"benchmark. Pattern recognition letters, 28(12):1438–1444, 2007.
330"
REFERENCES,0.75,"[13] Bjørn Grung and Rolf Manne. Missing values in principal component analysis. Chemometrics
331"
REFERENCES,0.751937984496124,"and Intelligent Laboratory Systems, 42(1-2):125–139, 1998.
332"
REFERENCES,0.7538759689922481,"[14] Alexander Ilin and Tapani Raiko. Practical approaches to principal component analysis in the
333"
REFERENCES,0.7558139534883721,"presence of missing values. The Journal of Machine Learning Research, 11:1957–2000, 2010.
334"
REFERENCES,0.7577519379844961,"[15] Lovedeep Gondara and Ke Wang. Multiple imputation using deep denoising autoencoders.
335"
REFERENCES,0.7596899224806202,"arXiv preprint arXiv:1705.02737, 2017.
336"
REFERENCES,0.7616279069767442,"[16] Vincent Audigier, François Husson, and Julie Josse. Multiple imputation for continuous
337"
REFERENCES,0.7635658914728682,"variables using a bayesian principal component analysis. Journal of statistical computation and
338"
REFERENCES,0.7655038759689923,"simulation, 86(11):2140–2156, 2016.
339"
REFERENCES,0.7674418604651163,"[17] Dušan Sovilj, Emil Eirola, Yoan Miche, Kaj-Mikael Björk, Rui Nian, Anton Akusok, and
340"
REFERENCES,0.7693798449612403,"Amaury Lendasse. Extreme learning machine for missing data using multiple imputations.
341"
REFERENCES,0.7713178294573644,"Neurocomputing, 174:220–231, 2016.
342"
REFERENCES,0.7732558139534884,"[18] Karthika Mohan and Judea Pearl. Graphical models for processing missing data. Journal of the
343"
REFERENCES,0.7751937984496124,"American Statistical Association, pages 1–42, 2021.
344"
REFERENCES,0.7771317829457365,"[19] Trevor Hastie, Rahul Mazumder, Jason D Lee, and Reza Zadeh. Matrix completion and
345"
REFERENCES,0.7790697674418605,"low-rank svd via fast alternating least squares. The Journal of Machine Learning Research,
346"
REFERENCES,0.7810077519379846,"16(1):3367–3402, 2015.
347"
REFERENCES,0.7829457364341085,"[20] Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for
348"
REFERENCES,0.7848837209302325,"learning large incomplete matrices. Journal of machine learning research, 11(Aug):2287–2322,
349"
REFERENCES,0.7868217054263565,"2010.
350"
REFERENCES,0.7887596899224806,"[21] Sanaz Nikfalazar, Chung-Hsing Yeh, Susan Bedingfield, and Hadi A Khorshidi. Missing data
351"
REFERENCES,0.7906976744186046,"imputation using decision trees and fuzzy clustering with iterative learning. Knowledge and
352"
REFERENCES,0.7926356589147286,"Information Systems, 62(6):2419–2437, 2020.
353"
REFERENCES,0.7945736434108527,"[22] Md Geaur Rahman and Md Zahidul Islam. Missing value imputation using decision trees and
354"
REFERENCES,0.7965116279069767,"decision forests by splitting and merging records: Two novel techniques. Knowledge-Based
355"
REFERENCES,0.7984496124031008,"Systems, 53:51–65, 2013.
356"
REFERENCES,0.8003875968992248,"[23] Lane F Burgette and Jerome P Reiter. Multiple imputation for missing data via sequential
357"
REFERENCES,0.8023255813953488,"regression trees. American journal of epidemiology, 172(9):1070–1076, 2010.
358"
REFERENCES,0.8042635658914729,"[24] Shahidul Islam Khan and Abu Sayed Md Latiful Hoque. Sice: an improved missing data
359"
REFERENCES,0.8062015503875969,"imputation technique. Journal of big data, 7(1):1–21, 2020.
360"
REFERENCES,0.8081395348837209,"[25] Md Geaur Rahman and Md Zahidul Islam. Missing value imputation using a fuzzy clustering-
361"
REFERENCES,0.810077519379845,"based em approach. Knowledge and Information Systems, 46(2):389–422, 2016.
362"
REFERENCES,0.812015503875969,"[26] Jared S Murray and Jerome P Reiter. Multiple imputation of missing categorical and continuous
363"
REFERENCES,0.813953488372093,"values via bayesian mixture models with local dependence. Journal of the American Statistical
364"
REFERENCES,0.8158914728682171,"Association, 111(516):1466–1479, 2016.
365"
REFERENCES,0.8178294573643411,"[27] Collins Leke and Tshilidzi Marwala. Missing data estimation in high-dimensional datasets:
366"
REFERENCES,0.8197674418604651,"A swarm intelligence-deep neural network approach. In International Conference on Swarm
367"
REFERENCES,0.8217054263565892,"Intelligence, pages 259–270. Springer, (2016).
368"
REFERENCES,0.8236434108527132,"[28] Katarzyna Wo´znica and Przemysław Biecek. Does imputation matter? benchmark for predictive
369"
REFERENCES,0.8255813953488372,"models. arXiv preprint arXiv:2007.02837, 2020.
370"
REFERENCES,0.8275193798449613,"[29] Julie Josse, François Husson, et al. Multiple imputation in principal component analysis.
371"
REFERENCES,0.8294573643410853,"Advances in data analysis and classification, 5(3):231–246, 2011.
372"
REFERENCES,0.8313953488372093,"[30] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. The elements
373"
REFERENCES,0.8333333333333334,"of statistical learning: data mining, inference, and prediction, volume 2. Springer, 2009.
374"
REFERENCES,0.8352713178294574,"[31] Li Qu, Li Li, Yi Zhang, and Jianming Hu. Ppca-based missing data imputation for traffic flow
375"
REFERENCES,0.8372093023255814,"volume: A systematical approach. IEEE Transactions on intelligent transportation systems,
376"
REFERENCES,0.8391472868217055,"10(3):512–522, 2009.
377"
REFERENCES,0.8410852713178295,"[32] Alfonso Iodice D’Enza, Francesco Palumbo, and Angelos Markos. Single imputation via
378"
REFERENCES,0.8430232558139535,"chunk-wise pca. In Conference of the International Federation of Classification Societies, pages
379"
REFERENCES,0.8449612403100775,"75–82. Springer, 2019.
380"
REFERENCES,0.8468992248062015,"[33] Abel Folch-Fortuny, Francisco Arteaga, and Alberto Ferrer. Assessment of maximum likelihood
381"
REFERENCES,0.8488372093023255,"pca missing data imputation. Journal of Chemometrics, 30(7):386–393, 2016.
382"
REFERENCES,0.8507751937984496,"[34] Vincent Audigier, François Husson, and Julie Josse. A principal component method to impute
383"
REFERENCES,0.8527131782945736,"missing values for mixed data. Advances in Data Analysis and Classification, 10(1):5–26, 2016.
384"
REFERENCES,0.8546511627906976,"[35] Robert Ernest Dear. A principal-component missing-data method for multiple regression models.
385"
REFERENCES,0.8565891472868217,"System Development Corporation, 1959.
386"
REFERENCES,0.8585271317829457,"[36] Darren T Andrews and Peter D Wentzell. Applications of maximum likelihood principal
387"
REFERENCES,0.8604651162790697,"component analysis: incomplete data sets and calibration transfer. Analytica Chimica Acta,
388"
REFERENCES,0.8624031007751938,"350(3):341–352, 1997.
389"
REFERENCES,0.8643410852713178,"[37] Sam Roweis. Em algorithms for pca and spca. Advances in neural information processing
390"
REFERENCES,0.8662790697674418,"systems, 10, 1997.
391"
REFERENCES,0.8682170542635659,"[38] Abel Folch-Fortuny, Francisco Arteaga, and Alberto Ferrer. Pca model building with missing
392"
REFERENCES,0.8701550387596899,"data: New proposals and a comparative study. Chemometrics and Intelligent Laboratory
393"
REFERENCES,0.872093023255814,"Systems, 146:77–88, 2015.
394"
REFERENCES,0.874031007751938,"[39] Aude Sportisse, Claire Boyer, and Julie Josse. Estimation and imputation in probabilistic
395"
REFERENCES,0.875968992248062,"principal component analysis with missing not at random data. Advances in Neural Information
396"
REFERENCES,0.877906976744186,"Processing Systems, 33:7067–7077, 2020.
397"
REFERENCES,0.8798449612403101,"[40] Alex Rubinsteyn and Sergey Feldman. fancyimpute: An imputation library for python. https:
398"
REFERENCES,0.8817829457364341,"//github.com/iskandr/fancyimpute, 2016.
399"
REFERENCES,0.8837209302325582,"[41] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
400"
REFERENCES,0.8856589147286822,"P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
401"
REFERENCES,0.8875968992248062,"M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
402"
REFERENCES,0.8895348837209303,"Learning Research, 12:2825–2830, 2011.
403"
REFERENCES,0.8914728682170543,"[42] C Okan Sakar, Gorkem Serbes, Aysegul Gunduz, Hunkar C Tunc, Hatice Nizam, Betul Erdogdu
404"
REFERENCES,0.8934108527131783,"Sakar, Melih Tutuncu, Tarkan Aydin, M Erdem Isenkul, and Hulya Apaydin. A comparative
405"
REFERENCES,0.8953488372093024,"analysis of speech signal processing algorithms for parkinson’s disease classification and the
406"
REFERENCES,0.8972868217054264,"use of the tunable q-factor wavelet transform. Applied Soft Computing, 74:255–263, 2019.
407"
REFERENCES,0.8992248062015504,"[43] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
408"
REFERENCES,0.9011627906976745,"[44] Jiang Li, Xiaowei S Yan, Durgesh Chaudhary, Venkatesh Avula, Satish Mudiganti, Hannah
409"
REFERENCES,0.9031007751937985,"Husby, Shima Shahjouei, Ardavan Afshar, Walter F Stewart, Mohammed Yeasin, et al. Im-
410"
REFERENCES,0.9050387596899225,"putation of missing values for electronic health record laboratory data. NPJ digital medicine,
411"
REFERENCES,0.9069767441860465,"4(1):1–14, 2021.
412"
REFERENCES,0.9089147286821705,"[45] Rodolphe Jenatton, Guillaume Obozinski, and Francis Bach. Structured sparse principal
413"
REFERENCES,0.9108527131782945,"component analysis. In Proceedings of the Thirteenth International Conference on Artificial
414"
REFERENCES,0.9127906976744186,"Intelligence and Statistics, pages 366–373. JMLR Workshop and Conference Proceedings, 2010.
415"
REFERENCES,0.9147286821705426,"[46] David A Ross, Jongwoo Lim, Ruei-Sung Lin, and Ming-Hsuan Yang. Incremental learning for
416"
REFERENCES,0.9166666666666666,"robust visual tracking. International journal of computer vision, 77(1):125–141, 2008.
417"
REFERENCES,0.9186046511627907,"[47] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness:
418"
REFERENCES,0.9205426356589147,"Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review,
419"
REFERENCES,0.9224806201550387,"53(2):217–288, 2011.
420"
REFERENCES,0.9244186046511628,"Checklist
421"
REFERENCES,0.9263565891472868,"1. For all authors...
422"
REFERENCES,0.9282945736434108,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
423"
REFERENCES,0.9302325581395349,"contributions and scope? [Yes] . See Table 2, Table 3, Table ??, Table ?? and Section
424"
REFERENCES,0.9321705426356589,"7.
425"
REFERENCES,0.9341085271317829,"(b) Have you read the ethics review guidelines and ensured that your paper conforms to
426"
REFERENCES,0.936046511627907,"them? [Yes] .
427"
REFERENCES,0.937984496124031,"(c) Did you discuss any potential negative societal impacts of your work? [N/A]
428"
REFERENCES,0.939922480620155,"(d) Did you describe the limitations of your work? [Yes] . See the last paragraph of Section
429"
REFERENCES,0.9418604651162791,"4.
430"
REFERENCES,0.9437984496124031,"2. If you are including theoretical results...
431"
REFERENCES,0.9457364341085271,"(a) Did you state the full set of assumptions of all theoretical results? [N/A]
432"
REFERENCES,0.9476744186046512,"(b) Did you include complete proofs of all theoretical results? [N/A]
433"
REFERENCES,0.9496124031007752,"3. If you ran experiments...
434"
REFERENCES,0.9515503875968992,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
435"
REFERENCES,0.9534883720930233,"mental results (either in the supplemental material or as a URL)? [Yes] . The code will
436"
REFERENCES,0.9554263565891473,"be available in the form of a Github repository upon acceptance, to preserve anonymity.
437"
REFERENCES,0.9573643410852714,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
438"
REFERENCES,0.9593023255813954,"were chosen)? [Yes] . See Section 7.
439"
REFERENCES,0.9612403100775194,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
440"
REFERENCES,0.9631782945736435,"ments multiple times)? [N/A]
441"
REFERENCES,0.9651162790697675,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
442"
REFERENCES,0.9670542635658915,"of GPUs, internal cluster, or cloud provider)? [Yes] . See Section 7.
443"
REFERENCES,0.9689922480620154,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
444"
REFERENCES,0.9709302325581395,"(a) If your work uses existing assets, did you cite the creators? [Yes] . See References list.
445"
REFERENCES,0.9728682170542635,"(b) Did you mention the license of the assets? [N/A]
446"
REFERENCES,0.9748062015503876,"(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
447"
REFERENCES,0.9767441860465116,". The code can be found on a github repository upon the acceptance of the paper.
448"
REFERENCES,0.9786821705426356,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
449"
REFERENCES,0.9806201550387597,"using/curating? [N/A]
450"
REFERENCES,0.9825581395348837,"(e) Did you discuss whether the data you are using/curating contains personally identifiable
451"
REFERENCES,0.9844961240310077,"information or offensive content? [N/A]
452"
REFERENCES,0.9864341085271318,"5. If you used crowdsourcing or conducted research with human subjects...
453"
REFERENCES,0.9883720930232558,"(a) Did you include the full text of instructions given to participants and screenshots, if
454"
REFERENCES,0.9903100775193798,"applicable? [N/A]
455"
REFERENCES,0.9922480620155039,"(b) Did you describe any potential participant risks, with links to Institutional Review
456"
REFERENCES,0.9941860465116279,"Board (IRB) approvals, if applicable? [N/A]
457"
REFERENCES,0.9961240310077519,"(c) Did you include the estimated hourly wage paid to participants and the total amount
458"
REFERENCES,0.998062015503876,"spent on participant compensation? [N/A]
459"
