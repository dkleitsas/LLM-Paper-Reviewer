Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0007446016381236039,"Data debugging is to find a subset of the training data such that the model obtained
1"
ABSTRACT,0.0014892032762472078,"by retraining on the subset has a better accuracy. A bunch of heuristic approaches
2"
ABSTRACT,0.0022338049143708115,"are proposed, however, none of them are guaranteed to solve this problem effec-
3"
ABSTRACT,0.0029784065524944155,"tively. This leaves an open issue whether there exists an efficient algorithm to find
4"
ABSTRACT,0.0037230081906180195,"the subset such that the model obtained by retraining on it has a better accuracy.
5"
ABSTRACT,0.004467609828741623,"To answer this open question and provide theoretical basis for further study on
6"
ABSTRACT,0.0052122114668652275,"developing better algorithms for data debugging, we investigate the computational
7"
ABSTRACT,0.005956813104988831,"complexity of the problem named DEBUGGABLE. Given a machine learning
8"
ABSTRACT,0.006701414743112435,"model M obtained by training on dataset D and a test instance (xtest, ytest) where
9"
ABSTRACT,0.007446016381236039,"M(xtest) ̸= ytest, DEBUGGABLE is to determine whether there exists a subset D′ of
10"
ABSTRACT,0.008190618019359643,"D such that the model M′ obtained by retraining on D′ satisfies M′(xtest) = ytest.
11"
ABSTRACT,0.008935219657483246,"To cover a wide range of commonly used models, we take SGD-trained linear
12"
ABSTRACT,0.00967982129560685,"classifier as the model and derive the following main results. (1) If the loss function
13"
ABSTRACT,0.010424422933730455,"and the dimension of the model are not fixed, DEBUGGABLE is NP-complete
14"
ABSTRACT,0.011169024571854059,"regardless of the training order in which all the training samples are processed
15"
ABSTRACT,0.011913626209977662,"during SGD. (2) For hinge-like loss functions, a comprehensive analysis on the
16"
ABSTRACT,0.012658227848101266,"computational complexity of DEBUGGABLE is provided; (3) If the loss function is a
17"
ABSTRACT,0.01340282948622487,"linear function, DEBUGGABLE can be solved in linear time, that is, data debugging
18"
ABSTRACT,0.014147431124348473,"can be solved easily in this case. These results not only highlight the limitations of
19"
ABSTRACT,0.014892032762472078,"current approaches but also offer new insights into data debugging.
20"
INTRODUCTION,0.01563663440059568,"1
Introduction
21"
INTRODUCTION,0.016381236038719285,"Given a machine learning model, data debugging is to find a subset of the training data such that
22"
INTRODUCTION,0.01712583767684289,"the model will have a better accuracy if retrained on that subset [1]. Data debugging serves as a
23"
INTRODUCTION,0.017870439314966492,"popular method of both data cleaning and machine learning interpretation. In the context of data
24"
INTRODUCTION,0.018615040953090096,"cleaning, data debugging (a.k.a. training data debugging [2] or data cleansing [1]) can be used
25"
INTRODUCTION,0.0193596425912137,"to improve the quality of the training data by removing the flaws leading to mispredictions [3–5].
26"
INTRODUCTION,0.020104244229337303,"When it comes to ML interpretation, data debugging locates the part of the training data responsible
27"
INTRODUCTION,0.02084884586746091,"for unexpected predictions of an ML model. Therefore it is also studied as a training data-based
28"
INTRODUCTION,0.021593447505584513,"(a.k.a. instance-based [6]) interpretation, which is crucial for helping system developers and ML
29"
INTRODUCTION,0.022338049143708117,"practitioners to debug ML system by reporting the harmful part of training data [7].
30"
INTRODUCTION,0.02308265078183172,"To solve the data debugging problem, existing researches adopt a two-phase score-based heuristic
31"
INTRODUCTION,0.023827252419955324,"approach [2]. In the first phase, a score representing the estimated impact on the model accuracy is
32"
INTRODUCTION,0.024571854058078928,"assigned to each training sample in the training data. It is hoped that the harmful part of training
33"
INTRODUCTION,0.02531645569620253,"data gets a lower score than the other part. In the second phase, training samples with lower scores
34"
INTRODUCTION,0.026061057334326135,"are removed greedily and the model is retrained on the modified training data. The two phases are
35"
INTRODUCTION,0.02680565897244974,"carried out iteratively until a well-trained model is obtained. Most of the related works focus on
36"
INTRODUCTION,0.027550260610573342,"developing algorithms to estimate the scores efficiently in the first phase [8–16], but rarely study the
37"
INTRODUCTION,0.028294862248696945,"effectiveness of the entire two-phase approach.
38"
INTRODUCTION,0.029039463886820552,"Since it is computationally intractable to estimate the score for all possible subsets of the training
39"
INTRODUCTION,0.029784065524944156,"data, it is often assumed that the score representing the impact of a subset is approximately equal
40"
INTRODUCTION,0.03052866716306776,"to the sum of the scores of each individual training samples from the subset. However, Koh et. al.
41"
INTRODUCTION,0.03127326880119136,"[10] showed this is not always the case. For a bunch of subsets sampled from the training data,
42"
INTRODUCTION,0.03201787043931496,"they empirically studied the difference between the estimated impact and the actual impact of each
43"
INTRODUCTION,0.03276247207743857,"subset by taking influence functions as the scoring method. The estimated impact is calculated by
44"
INTRODUCTION,0.03350707371556218,"summing up the score by influence function of each training samples in the subset, and the actual
45"
INTRODUCTION,0.03425167535368578,"impact is measured by the improvement of accuracy of the model retrained after removing the subset
46"
INTRODUCTION,0.034996276991809384,"from training data. They found that the estimated impact tends to underestimate the actual impact.
47"
INTRODUCTION,0.035740878629932984,"Removing a large number of training samples could result in a large deviation between estimated
48"
INTRODUCTION,0.03648548026805659,"and actual impacts. Although an upper bound of the deviation under certain assumptions has been
49"
INTRODUCTION,0.03723008190618019,"derived, it is still unknown whether the deviation can be reduced or eliminated efficiently.
50"
INTRODUCTION,0.0379746835443038,"The above deviation also poses challenges to the effectiveness of the entire approach. Suppose the
51"
INTRODUCTION,0.0387192851824274,"influence function is adopted as the scoring method, the accuracy of the model is not guaranteed
52"
INTRODUCTION,0.039463886820551006,"to improve due to the deviation reported in [10] if a large group of training samples are removed
53"
INTRODUCTION,0.040208488458674606,"during each iteration. Moreover, there is no theoretical analysis for the effectiveness of the greedy
54"
INTRODUCTION,0.04095309009679821,"approach in the second phase. Even if only one training sample is removed during each iteration
55"
INTRODUCTION,0.04169769173492182,"of the two-phase approach, the accuracy of the model is still not guaranteed to be improved. The
56"
INTRODUCTION,0.04244229337304542,"effectiveness of the entire two-phase approach is therefore not assured. This leaves the following
57"
INTRODUCTION,0.04318689501116903,"open problem:
58"
INTRODUCTION,0.04393149664929263,"Problem 1.1. Is there an efficient algorithm to find the subset of the training data, such that the
59"
INTRODUCTION,0.044676098287416234,"model obtained by retraining on it has a better accuracy?
60"
INTRODUCTION,0.045420699925539834,"The computational complexity results presented in this paper demonstrate that it is unlikely to solve
61"
INTRODUCTION,0.04616530156366344,"the data debugging problem efficiently in polynomial time. To figure out its hardness, we study the
62"
INTRODUCTION,0.04690990320178704,"problem DEBUGGABLE which is the decision version of data debugging when the test set consists of
63"
INTRODUCTION,0.04765450483991065,"only one instance. Formally, DEBUGGABLE is defined as follows:
64"
INTRODUCTION,0.04839910647803425,"Problem 1.2 (DEBUGGABLE). Given a classifier M, its training data T, a test instance (x, y). Is
65"
INTRODUCTION,0.049143708116157855,"there a T ′ ⊆T, such that M predicts y on x if retrained on T ′?
66"
INTRODUCTION,0.04988830975428146,"Basically, we prove that DEBUGGABLE is NP-complete, which means data debugging is unlikely
67"
INTRODUCTION,0.05063291139240506,"to be solved in polynomial time. This result answers the open question mentioned above directly,
68"
INTRODUCTION,0.05137751303052867,"this is, the large deviation of estimated impacts [10] cannot be reduced or eliminated efficiently. This
69"
INTRODUCTION,0.05212211466865227,"is because if the impact of a subset of the training data could be accurately estimated as the sum of
70"
INTRODUCTION,0.05286671630677588,"the impact of each training sample in the subset, data debugging can be solved in polynomial time,
71"
INTRODUCTION,0.05361131794489948,"which is impossible unless P=NP.
72"
INTRODUCTION,0.054355919583023084,"Although DEBUGGABLE is generally intractable, we still hope to develop efficient algorithms tailored
73"
INTRODUCTION,0.055100521221146684,"to specific cases. Thus it is necessary to figure out the root cause of the hardness for DEBUGGABLE.
74"
INTRODUCTION,0.05584512285927029,"Previous research are always conducted based on the belief that the complexity of data debugging is
75"
INTRODUCTION,0.05658972449739389,"due to the chosen model architecture is complicated. However, we show that at least for models trained
76"
INTRODUCTION,0.0573343261355175,"by stochastic gradient descent (SGD), the hardness stems from the hyper-parameter configuration
77"
INTRODUCTION,0.058078927773641105,"selected for the SGD training, which was not yet aware of by previous work. To cover a wide range of
78"
INTRODUCTION,0.058823529411764705,"commonly used machine learning models, we take linear classifiers as the model and show that even
79"
INTRODUCTION,0.05956813104988831,"for linear classifiers, DEBUGGABLE is NP-hard as long as they are trained by SGD. Moreover, we
80"
INTRODUCTION,0.06031273268801191,"provided a comprehensive analysis on hyper-parameter configurations that affect the computational
81"
INTRODUCTION,0.06105733432613552,"complexity of DEBUGGABLE, including the loss function, the model dimension and the training
82"
INTRODUCTION,0.06180193596425912,"order. Training order, a.k.a. training data order [17] or order of training samples [18], refers to the
83"
INTRODUCTION,0.06254653760238273,"order in which each training sample is considered during the SGD. Detailed complexity results are
84"
INTRODUCTION,0.06329113924050633,"shown in Table 1.
85"
INTRODUCTION,0.06403574087862993,"Our contribution can be concluded as follows:
86"
INTRODUCTION,0.06478034251675353,"• We studied the computational complexity of data debugging and showed that data debugging
87"
INTRODUCTION,0.06552494415487714,"is NP-hard for linear classifiers in the general setting for all possible training orders.
88"
INTRODUCTION,0.06626954579300075,"• We studied the complexity of DEBUGGABLE when the loss is fixed as the hinge-like
89"
INTRODUCTION,0.06701414743112435,"function. For 2 or higher dimension, DEBUGGABLE is NP-complete when the training order
90"
INTRODUCTION,0.06775874906924795,"Table 1: Computational complexity of the data debugging problem
Loss Function
Dimension
Training Order
Complexity"
INTRODUCTION,0.06850335070737155,"Not Fixed
Not Fixed
-
NP-hard
Hinge-like
≥2
Adversarially Chosen
NP-hard
Hinge-like, β < 0
1
Adversarially Chosen
NP-hard
Hinge-like, β ≥0
1
-
Linear Time
Linear
-
-
Linear Time"
INTRODUCTION,0.06924795234549516,"is adversarially chosen; For one-dimensional cases, DEBUGGABLE can be NP-hard when
91"
INTRODUCTION,0.06999255398361877,"the interception β < 0, and is solvable in linear time when β ≥0.
92"
INTRODUCTION,0.07073715562174236,"• We proved that DEBUGGABLE is solvable in linear time when the loss function is linear.
93"
INTRODUCTION,0.07148175725986597,"Moreover, we have a discussion on the implications of these complexity results for machine learning
94"
INTRODUCTION,0.07222635889798958,"interpretability and data quality, as well as limitations of score-based greedy methods. Our results
95"
INTRODUCTION,0.07297096053611318,"suggest the further study as follows. (1) It is better to characterize the training sample and find the
96"
INTRODUCTION,0.07371556217423679,"criterion which can be used to decide the existence of efficient algorithms; (2) Designing algorithms
97"
INTRODUCTION,0.07446016381236038,"with CSP-solver is a potential way to solve data debugging more efficiently than the brute-force one;
98"
INTRODUCTION,0.07520476545048399,"(3) Developing random algorithms is a potential way to solve data debugging successfully with high
99"
INTRODUCTION,0.0759493670886076,"probability.
100"
RELATED WORKS,0.0766939687267312,"1.1
Related Works
101"
RELATED WORKS,0.0774385703648548,"The solution of data debugging has applications in database query results reliability enhancement
102"
RELATED WORKS,0.0781831720029784,"[2, 19], training data cleaning [1] and machine learning interpretation[9, 8, 10, 20, 21]. Existing
103"
RELATED WORKS,0.07892777364110201,"works on data debugging mainly adopt a two-phase approach, which scores the training samples in the
104"
RELATED WORKS,0.07967237527922562,"first phase and greedily deletes training samples with lower scores in the second phase. Most of the
105"
RELATED WORKS,0.08041697691734921,"research focus on the first phase. There are mainly two ways of scoring adopted for data debugging in
106"
RELATED WORKS,0.08116157855547282,"practice. Leave-one-out (LOO) retraining is a widely studied way, which evaluates the contribution of
107"
RELATED WORKS,0.08190618019359643,"a training sample through the difference in the model’s accuracy trained without that training sample.
108"
RELATED WORKS,0.08265078183172003,"To avoid the cost of model retraining, Koh and Liang took influence functions as an approximation of
109"
RELATED WORKS,0.08339538346984364,"LOO [8]. After that, various extensions and improvements of the influence function based method
110"
RELATED WORKS,0.08413998510796723,"are proposed, such as Fisher kernel [9], influence function for group impacts [10], second-order
111"
RELATED WORKS,0.08488458674609084,"approximations [11] and scalable influence functions [12]. Another way is Shapley-based scoring,
112"
RELATED WORKS,0.08562918838421445,"where the impact of a training sample is measured by its average marginal contribution to all subsets
113"
RELATED WORKS,0.08637379002233805,"of the training data [13]. Since Shapley-base scoring suffers from expensive computational cost [22],
114"
RELATED WORKS,0.08711839166046165,"recent works focus on techniques that efficiently estimate the Shapley value, including Monte-Carlo
115"
RELATED WORKS,0.08786299329858525,"sampling [13], group testing [14, 15] and using proxy models such as k-NN [16, 3]. However,
116"
RELATED WORKS,0.08860759493670886,"those methods do not admit any theoretical guarantee on the effectiveness. This paper discusses the
117"
RELATED WORKS,0.08935219657483247,"limitations of the above methods and suggests some future directions on data debugging.
118"
PRELIMINARIES AND PROBLEM DEFINITION,0.09009679821295608,"2
Preliminaries and Problem Definition
119"
PRELIMINARIES AND PROBLEM DEFINITION,0.09084139985107967,"Linear classifiers. Formally, a (binary) linear classifier is a function λw : Rd →{−1, 1}, where d is
120"
PRELIMINARIES AND PROBLEM DEFINITION,0.09158600148920328,"called its dimension and w ∈Rd its parameter. Without loss of generality, the bias term of a linear
121"
PRELIMINARIES AND PROBLEM DEFINITION,0.09233060312732688,"classifier is set as zero in this paper. All vectors in this paper are assumed to be column vectors. For
122"
PRELIMINARIES AND PROBLEM DEFINITION,0.09307520476545049,"an input x, the value of λw is defined as
123"
PRELIMINARIES AND PROBLEM DEFINITION,0.09381980640357408,"λw(x) =
1
if w⊤x ≥0
−1
otherwise."
PRELIMINARIES AND PROBLEM DEFINITION,0.09456440804169769,"We denote the class of linear models as Λ.
124"
PRELIMINARIES AND PROBLEM DEFINITION,0.0953090096798213,"Training data. A training sample is a pair (x, y) in which x ∈Rd is the input and y ∈{−1, 1} is
125"
PRELIMINARIES AND PROBLEM DEFINITION,0.0960536113179449,"the label of x. The training data is a multiset of training samples. We employ w
T−→w′ to denote
126"
PRELIMINARIES AND PROBLEM DEFINITION,0.0967982129560685,"that the parameter w′ is obtained by training the parameter w on the training data T, and employ
127"
PRELIMINARIES AND PROBLEM DEFINITION,0.0975428145941921,"w
(x,y)
−−−→w′ to denote that w′ is obtained by training w on the training sample (x, y).
128"
PRELIMINARIES AND PROBLEM DEFINITION,0.09828741623231571,"Loss functions and learning rates. Binary linear classifiers typically use unary functions on yw⊤x
129"
PRELIMINARIES AND PROBLEM DEFINITION,0.09903201787043932,"as their loss functions [23]. Therefore we only consider loss functions of the form L : yw⊤x 7→R
130"
PRELIMINARIES AND PROBLEM DEFINITION,0.09977661950856292,"for the rest of the paper.
131"
PRELIMINARIES AND PROBLEM DEFINITION,0.10052122114668652,"The linear loss is in the form of
132"
PRELIMINARIES AND PROBLEM DEFINITION,0.10126582278481013,Llin(yw⊤x) = −α(yw⊤x + β).
PRELIMINARIES AND PROBLEM DEFINITION,0.10201042442293373,"The hinge-like loss function is defined as the following form
133"
PRELIMINARIES AND PROBLEM DEFINITION,0.10275502606105734,"Lhinge(yw⊤x) =
−α(yw⊤x + β),
yw⊤x < β
0,
otherwise."
PRELIMINARIES AND PROBLEM DEFINITION,0.10349962769918093,"We call β as the interception of Lhinge. We represent the learning rate of a model using a vector
134"
PRELIMINARIES AND PROBLEM DEFINITION,0.10424422933730454,"η = (η1, . . . , ηd), where ηi ≥0 and each parameter wi can be updated with the corresponding
135"
PRELIMINARIES AND PROBLEM DEFINITION,0.10498883097542815,"learning rate ηi.
136"
PRELIMINARIES AND PROBLEM DEFINITION,0.10573343261355175,"Stochastic gradient descent. The stochastic gradient descent (SGD) method updates parameter w
137"
PRELIMINARIES AND PROBLEM DEFINITION,0.10647803425167536,"from its initial value w(0) through several epochs. During each epoch, the SGD goes through the
138"
PRELIMINARIES AND PROBLEM DEFINITION,0.10722263588979895,"entire set of training samples in some training order through several iterations. The training order is
139"
PRELIMINARIES AND PROBLEM DEFINITION,0.10796723752792256,"defined as a sequence of training samples, in the form of (x1, y1) . . . (xn, yn). For 1 ≤i < j ≤n,
140"
PRELIMINARIES AND PROBLEM DEFINITION,0.10871183916604617,"(xi, yi) is considered before (xj, yj) during the SGD. We use wi to denote the i-th coordinate of w.
141"
PRELIMINARIES AND PROBLEM DEFINITION,0.10945644080416977,"We also use w(e,k) to denote the value of w at the end of k-th iteration of epoch e and use w(e) to
142"
PRELIMINARIES AND PROBLEM DEFINITION,0.11020104244229337,"denote the value of w after the end of epoch e. Assuming (x, y) to be the training sample considered
143"
PRELIMINARIES AND PROBLEM DEFINITION,0.11094564408041697,"at iteration k, the stochastic gradient descent (SGD) method updates parameter wi for each i by
144"
PRELIMINARIES AND PROBLEM DEFINITION,0.11169024571854058,"w(e,k)
i
←w(e,k−1)
i
−ηi · ∂L(y(w(e,k−1))⊤x)"
PRELIMINARIES AND PROBLEM DEFINITION,0.11243484735666419,"∂wi
(1)"
PRELIMINARIES AND PROBLEM DEFINITION,0.11317944899478778,"In other words, we have
145"
PRELIMINARIES AND PROBLEM DEFINITION,0.11392405063291139,"w(e,k) ←w(e,k−1) −η ⊗∇L(y(w(e,k−1))⊤x)"
PRELIMINARIES AND PROBLEM DEFINITION,0.114668652271035,where η ⊗∇L = (η1 ∂L
PRELIMINARIES AND PROBLEM DEFINITION,0.1154132539091586,"∂w1 , . . . , ηd ∂L"
PRELIMINARIES AND PROBLEM DEFINITION,0.11615785554728221,"∂wd ) is the Hadamard product. We say a training sample x
146"
PRELIMINARIES AND PROBLEM DEFINITION,0.1169024571854058,"is activated at iteration k during epoch e if ∇L(y(w(e,k−1))⊤x) ̸= 0. The SGD terminates at
147"
PRELIMINARIES AND PROBLEM DEFINITION,0.11764705882352941,"the end of epoch e if ∥w(e−1) −w(e)∥< ε for threshold ε or e reached some predetermined
148"
PRELIMINARIES AND PROBLEM DEFINITION,0.11839166046165302,"value. We denote w∗= w(e). A linear classifier trained by SGD with the meta-parameters
149"
PRELIMINARIES AND PROBLEM DEFINITION,0.11913626209977662,"mentioned above is denoted as SGDΛ(L, η, ε, T) = λw∗. With a slight abuse of notation, we define
150"
PRELIMINARIES AND PROBLEM DEFINITION,0.11988086373790022,"SGDΛ(L, η, ε, T, x) = λw∗(x). We also use SGDΛ(T, x) to avoid cluttering when the context is clear.
151"
PRELIMINARIES AND PROBLEM DEFINITION,0.12062546537602382,"Problem definition. With the above definitions, DEBUGGABLE for SGD-trained linear classifiers
152"
PRELIMINARIES AND PROBLEM DEFINITION,0.12137006701414743,"can be formalized as follows:
153"
PRELIMINARIES AND PROBLEM DEFINITION,0.12211466865227104,"DEBUGGABLE-LIN
Input: Training data T, loss function L, initial parameter w(0), learning
rate η, threshold ε and instance (xtest, ytest).
Output: “Yes”: if ∃∆⊆T such that SGDΛ(L, η, ε, T \ ∆, xtest) = ytest;
“No”: otherwise. 154"
PRELIMINARIES AND PROBLEM DEFINITION,0.12285927029039465,"We say SGDΛ(L, η, ε, T) is debuggable on (xtest, ytest) if (L, w(0), η, ε, T, xtest, ytest) is a yes-instance
155"
PRELIMINARIES AND PROBLEM DEFINITION,0.12360387192851824,"of DEBUGGABLE-LIN, and not debuggable on (xtest, ytest) otherwise.
156"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.12434847356664185,"3
Results for Unfixed Loss Functions
157"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.12509307520476545,"In this section, we prove the NP-hardness of DEBUGGABLE-LIN. Intuitively, DEBUGGABLE-LIN is
158"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.12583767684288905,"to determine whether there exists a subset T ′ ⊆T where activated training samples within T ′ drive
159"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.12658227848101267,"the parameter w toward the region defined by ytestw⊤xtest > 0. The activation of training samples
160"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.12732688011913626,"depends on the complex interaction between the training data and the model.
161"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.12807148175725985,"Theorem 3.1. DEBUGGABLE-LIN is NP-hard for all training orders.
162"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.12881608339538347,"We only show the proof sketch and leave the details in the appendix.
163"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.12956068503350707,"Proof Sketch. We build a reduction from an NP-hard problem MONOTONE 1-IN-3 SAT [24]:
164"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1303052866716307,"MONOTONE 1-IN-3 SAT
Input: A 3-CNF formula φ with no negation signs.
Output:“Yes”: if φ has a 1-in-3 assignment, under which each clause
contains exactly one true literal;
“No”: otherwise. 165"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.13104988830975428,"For example, φ1 = (x1 ∨x2 ∨x3) ∧(x2 ∨x3 ∨x4) is a yes-instance because (x1, x2, x3, x4) =
166"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.13179448994787787,"(T,F,F,T) is an 1-in-3 assignment; φ2 = (x1∨x2∨x3)∧(x2∨x3∨x4)∧(x1∨x2∨x4)∧(x1∨x3∨x4)
167"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1325390915860015,"is a no-instance.
168"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1332836932241251,"Given a 3-CNF formula φ, our goal is to construct a configuration of the training process, such that
169"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1340282948622487,"the resulting model outputs the correct answer if and only if its training data T ′ encodes an 1-in-3
170"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1347728965003723,"assignment ν of φ. This can be done by carefully designing the encoding so that for each xi ∈φ,
171"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1355174981384959,"ν(xi) = TRUE if and only if txi ∈T ′. Finally, we can construct some T with T ⊇T ′∪{txi|xi ∈φ},
172"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.13626209977661952,"such that some classifier trained on T is a yes-instance of DEBUGGABLE-LIN if and only if φ is a
173"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1370067014147431,"yes-instance of MONOTONE 1-IN-3 SAT, thereby finishing our proof.
174"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1377513030528667,"The reduction. Suppose φ has m clauses and n variables, let N = n+2m+1. We set the dimension
175"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.13849590469099032,"of the linear classifier to N.
176"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.13924050632911392,"The input. Each coordinate of the input is named as
177"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.13998510796723754,"x = (xc1, . . . , xcm, xx1, . . . , xxn, xb1, . . . , xbm, xdummy)⊤"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.14072970960536113,"We also use xi to denote the i-th coordinate of x.
178"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.14147431124348472,"The parameters. Each coordinate of the parameter is named as
179"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.14221891288160834,"w = (wc1, . . . , wcm, wx1, . . . , wxn, wb1, . . . , wbm, wdummy)⊤"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.14296351451973194,"We also use wi to denote the i-th coordinate of w. Each wxj represents the truth value of variable xj,
180"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.14370811615785556,"where 1 represents TRUE and -1 represents FALSE. Similarly, each wcj represents the truth value of
181"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.14445271779597915,"clause cj based on the value of its variables. wbj and wdummy are used for convenience of proof.
182"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.14519731943410275,"The initial value of the parameter is set to
183"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.14594192107222637,w(0) = (
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.14668652271034996,"m
z
}|
{
1
2, . . . , 1 2,"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.14743112434847358,"n
z
}|
{
−1, . . . , −1,"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.14817572598659717,"m
z
}|
{
−1, . . . , −1, 1)⊤"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.14892032762472077,"Loss function. We denote U(x0, δ) := {x|x0 −δ < x < x0 + δ} as the δ-neighborhood of x0 and
184"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1496649292628444,"define U(±x0, δ) = U(x0, δ) ∪U(−x0, δ). We define the local ramp function as
185"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.15040953090096798,"rx0,δ(x) = 
 "
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.15115413253909157,"0
, x ≤x0 −δ;
x −x0 + δ
, x ∈U(x0, δ);
2δ
, x ≥x0 + δ."
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1518987341772152,"The loss function is defined as
186"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1526433358153388,L = −12N
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1533879374534624,"5
r−5,0.01(yw⊤x) −r−1"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.154132539091586,"2 ,0.26(yw⊤x) −
1
1000N X"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1548771407297096,"x0∈{±1,±3}
rx0,0.01(yw⊤x)."
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.15562174236783322,"L is monotonically decreasing with derivatives
187"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1563663440059568,"∂L
∂wi
="
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.15711094564408043,"



"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.15785554728220402,"


 −12N"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.15860014892032762,"5
· yxi
, yw⊤x ∈U(−5, 0.01);
−yxi
, yw⊤x ∈U(−1"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.15934475055845124,"2, 0.26);
−
1
1000N yxi
, yw⊤x ∈S"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.16008935219657483,"x0∈{±1,±3} U(x0, 0.01);
0
, otherwise. (2)"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.16083395383469842,"Table 2: Training data for var(i) xxi
y 5
1"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.16157855547282204,"Table 3: Training data for clause(i, i1, i2, i3)"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.16232315711094564,"xci
xxi1
xxi2
xxi3
xbi
y"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.16306775874906926,"1
1
1
1
1
2
1"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.16381236038719285,"Learning rate. The learning rate for SGD is set to be
188 η = ("
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.16455696202531644,"m
z }| {
5, . . . , 5,"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.16530156366344007,"n
z
}|
{
1
6N , . . . , 1 6N ,"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.16604616530156366,"m
z
}|
{
2000N, . . . , 2000N, 1)⊤."
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.16679076693968728,"Training data. We define two gadgets, var(i) and clause(i, i1, i2, i3), as illustrated in Table 2 and
189"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.16753536857781087,"3. All the unspecified coordinates are set to zero. We use T0 to denote the training data. var(i)
190"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.16827997021593447,"is contained in T0 if and only if xi ∈φ, and clause(i, i1, i2, i3) is contained in T0 if and only if
191"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1690245718540581,"ci = (xi1 ∨xi2 ∨xi3) ∈φ.
192"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.16976917349218168,"Threshold and instance. The threshold ε can be any fixed value in R+. The instance is defined as
193"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.17051377513030527,"(xtest, ytest), where ytest = 1 and
194"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1712583767684289,xtest = (
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1720029784065525,"m
z }| {
1, . . . , 1,"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1727475800446761,"n+m
z }| {
0, . . . , 0, −11m + 5 2
)⊤."
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1734921816827997,"The following reduction works for all possible training orders. Intuitively, during the training process,
195"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1742367833209233,"each var(i) in the training data will set wxi to around 1 (that is, mark xi as TRUE) in the first epoch,
196"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.17498138495904692,"and each clause(i, i1, i2, i3) will set wci to near 11"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.1757259865971705,"2 in the second epoch, if and only if exactly one
197"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.17647058823529413,"of wxi1, wxi2, wxi3 is near 1 and the others near −1 (that is, mark ci as satisfied if exactly one of
198"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.17721518987341772,"its literals is TRUE and the others FALSE). The training process terminates at the end of the second
199"
RESULTS FOR UNFIXED LOSS FUNCTIONS,0.17795979151154132,"epoch.
200"
RESULTS FOR FIXED LOSS FUNCTIONS,0.17870439314966494,"4
Results for Fixed Loss Functions
201"
RESULTS FOR FIXED LOSS FUNCTIONS,0.17944899478778853,"We have proved the NP-hardness for DEBUGGABLE-LIN when the loss function is not fixed. In
202"
RESULTS FOR FIXED LOSS FUNCTIONS,0.18019359642591215,"this section, we study the complexity when the loss function is fixed as linear and hinge-like
203"
RESULTS FOR FIXED LOSS FUNCTIONS,0.18093819806403574,"functions. Assuming that SGD terminates after only one epoch with a fixed order, we will show
204"
RESULTS FOR FIXED LOSS FUNCTIONS,0.18168279970215934,"that DEBUGGABLE-LIN is solvable in linear time for linear loss. For hinge-like loss functions,
205"
RESULTS FOR FIXED LOSS FUNCTIONS,0.18242740134028296,"DEBUGGABLE-LIN can be solved in linear time only when the dimension d = 1 and the interception
206"
RESULTS FOR FIXED LOSS FUNCTIONS,0.18317200297840655,"β ≥0. For the rest cases, DEBUGGABLE-LIN becomes NP-hard.
207"
THE EASY CASE,0.18391660461653014,"4.1
The Easy Case
208"
THE EASY CASE,0.18466120625465376,"We start with the linear loss function L = −α(yw⊤x + β), with which all the training data are
209"
THE EASY CASE,0.18540580789277736,activated and w∗= w∗(T) = w(0)+P
THE EASY CASE,0.18615040953090098,"(x,y)∈T αyη⊗x. Since ytest ∈{−1, 1}, DEBUGGABLE-LIN
210"
THE EASY CASE,0.18689501116902457,"is equivalent to deciding whether
211"
THE EASY CASE,0.18763961280714817,"max
T ′⊆T{ytest(w∗(T ′))⊤xtest} > 0."
THE EASY CASE,0.18838421444527179,"A training sample (x, y) is “good” if ytest(αyη ⊗x)⊤xtest > 0 and “bad” otherwise. The good
212"
THE EASY CASE,0.18912881608339538,"training-sample assessment (GTA) algorithm, as shown in Algorithm 1, deals with this situation by
213"
THE EASY CASE,0.189873417721519,"greedily picking all “good” training samples.
214"
THE EASY CASE,0.1906180193596426,"Denoting T ∗as the set of all good data in T, it follows that
215"
THE EASY CASE,0.1913626209977662,"ytest(w∗(T ∗))⊤xtest = ytest(w(0))⊤xtest +
X"
THE EASY CASE,0.1921072226358898,"(x,y)∈T ∗
ytest(αyη ⊗x)⊤xtest"
THE EASY CASE,0.1928518242740134,"≥ytest(w(0))⊤xtest +
X"
THE EASY CASE,0.193596425912137,"(x,y)∈T ′
ytest(αyη ⊗x)⊤xtest"
THE EASY CASE,0.19434102755026061,"for all T ′ ⊆T. Hence maxT ′⊆T {ytest(w∗(T ′))⊤xtest} = ytest(w∗(T ∗))⊤xtest and DEBUGGABLE-
216"
THE EASY CASE,0.1950856291883842,"LIN can be solved by GTA in linear time. The following theorem is straightforward.
217"
THE EASY CASE,0.19583023082650783,"Theorem 4.1. DEBUGGABLE-LIN is linear time solvable for linear loss functions.
218"
THE EASY CASE,0.19657483246463142,Algorithm 1: Good Training-sample Assessment (GTA)
THE EASY CASE,0.19731943410275501,"Input: Training data T, loss function L, initial parameter w(0), learning rate η, threshold ε and
test instance (xtest, ytest).
Output: TRUE, iff SGDΛ(L, η, ε, T) is debuggable on (xtestytest)."
THE EASY CASE,0.19806403574087864,1 w ←w(0);
THE EASY CASE,0.19880863737900223,"2 for (x, y) ∈T do"
THE EASY CASE,0.19955323901712585,"3
if ytest(αyη ⊗x)⊤xtest > 0 then"
THE EASY CASE,0.20029784065524944,"4
w ←w + αyη ⊗x;"
END,0.20104244229337304,"5
end"
END,0.20178704393149666,6 end
END,0.20253164556962025,7 if ytestw⊤xtest ≥0 then
END,0.20327624720774387,"8
return TRUE;"
END,0.20402084884586746,9 end
END,0.20476545048399106,10 return FALSE; 219
END,0.20551005212211468,"GTA is still effective for one-dimensional classifiers trained with hinge-like losses when β ≥0.
220"
END,0.20625465376023827,"Theorem 4.2. DEBUGGABLE-LIN is linear time solvable for hinge-like loss functions, when d = 1
221"
END,0.20699925539836186,"and β ≥0.
222"
END,0.20774385703648549,"Proof. It suffices to prove that if ∃T ′ ⊆T such that SGDΛ(T ′, xtest) = ytest, SGDΛ(T ∗, xtest) = ytest.
223"
END,0.20848845867460908,"a) Suppose all the data in T ∗are activated, we have
224"
END,0.2092330603127327,"ytestw∗(T ∗)xtest = ytestw(0)xtest +
X"
END,0.2099776619508563,"(x,y)∈T ∗
ytestαyηxxtest"
END,0.21072226358897989,"≥ytestw(0)xtest +
X"
END,0.2114668652271035,"(x,y)∈T ′∩T ∗
ytestαyηxxtest +
X"
END,0.2122114668652271,"(x,y)∈T ′\T ∗
ytestαyηxxtest"
END,0.21295606850335072,= ytestw∗(T ′)xtest ≥0
END,0.2137006701414743,"b) Suppose (x, y) ∈T ∗is the first inactivated data during the training phase, and w is the current
225"
END,0.2144452717795979,"parameter, we have ywx > β. Since αη ·(xy)·(xtestytest) ≥0, we have (xtestytest)·w ≥0. Let T ′′ be
226"
END,0.21518987341772153,"the set of training data appeared before (x, y), we have ytestw∗(T ∗)xtest ≥ytestw∗(T ′′)xtest ≥0.
227"
THE HARD CASE,0.21593447505584512,"4.2
The Hard Case
228"
THE HARD CASE,0.21667907669396871,"The gradient of training data may not always be activated and could be affected by the training order.
229"
THE HARD CASE,0.21742367833209233,"When the training order is adversarially chosen, the following theorem shows that DEBUGGABLE-LIN
230"
THE HARD CASE,0.21816827997021593,"is NP-hard for all d ≥2 and β ∈R.
231"
THE HARD CASE,0.21891288160833955,"Theorem 4.3. If the training order is adversarially chosen and d ≥2, DEBUGGABLE-LIN is NP-hard
232"
THE HARD CASE,0.21965748324646314,"for each hinge-like loss function at every constant learning rate.
233"
THE HARD CASE,0.22040208488458674,"Proof sketch. Since the result can be easily extended for all d > 2 by padding the other d −2
234"
THE HARD CASE,0.22114668652271036,"dimensions with zeros, we only prove for the case of d = 2. We assume β ≥−1 and leave the
235"
THE HARD CASE,0.22189128816083395,"β < −1 case to the appendix. To avoid cluttering, we further assume η = 1 and α = 1. The proof
236"
THE HARD CASE,0.22263588979895757,"can be easily generalized by appropriately re-scaling the constructed vectors.
237"
THE HARD CASE,0.22338049143708116,"We build a reduction from the subset sum problem, which is well-known to be NP-hard:
238"
THE HARD CASE,0.22412509307520476,"SUBSET SUM
Input: A set of positive integer S, and a positive integer t.
Output: “Yes”: if ∃S′ ⊆S such that P"
THE HARD CASE,0.22486969471332838,"a∈S′ a = t;
“No”: otherwise. 239"
THE HARD CASE,0.22561429635145197,"Suppose n = |S|, m = maxa∈S{a}, γ = max{β, 1} and S = {a1, a2, . . . , an}. We further assume
240"
THE HARD CASE,0.22635889798957556,"n > 1. Let the training data be
241"
THE HARD CASE,0.22710349962769918,"T = {(x1, y1), (x2, y2), . . . , (xn, yn)} ∪{(xc, yc), (xb, yb), (xa, ya)}"
THE HARD CASE,0.22784810126582278,"where xiyi = (
√γ
n+1, 3√γai) for all 1 ≤i ≤n, xcyc = ((18n2m2 −2)√γ, −3t√γ), xbyb =
242"
THE HARD CASE,0.2285927029039464,"(√γ, −√γ), xaya = (√γ, √γ). Let w(0) = (−18n2m2√γ, 0). Let the test instance (xtest, ytest)
243"
THE HARD CASE,0.22933730454207,"satisfy xtestytest = (1, 0).
244"
THE HARD CASE,0.23008190618019358,"Let the training order be (x1, y1), (x2, y2), . . . , (xn, yn), (xc, yc), (xb, yb), (xa, ya).
245"
THE HARD CASE,0.2308265078183172,"For each 1 ≤i < n, suppose w(0)
T ∩{(xi,yi)|1≤j≤i}
−−−−−−−−−−−−→wi, we have
246"
THE HARD CASE,0.2315711094564408,"yi+1w⊤
i xi+1 ≤
√γ
n + 1(−18n2m2√γ +
√γi
n + 1) + 3√γai+1 i
X"
THE HARD CASE,0.23231571109456442,"j=1
3√γaj"
THE HARD CASE,0.233060312732688,"≤γ

−n −1"
THE HARD CASE,0.2338049143708116,"n + 1 · 9nm2 +
n
(n + 1)2"
THE HARD CASE,0.23454951600893523,"
< −1 ≤β"
THE HARD CASE,0.23529411764705882,"This means all the T \ {(xc, yc), (xb, yb), (xa, ya)} can be activated. Thus the resulting parameter
247"
THE HARD CASE,0.23603871928518244,"trained by T \ {(xc, yc), (xb, yb), (xa, ya)} is
248"
THE HARD CASE,0.23678332092330603,"wc = w(0) + n
X"
THE HARD CASE,0.23752792256142963,"i=1
xiyi = "
THE HARD CASE,0.23827252419955325,"−18n2m2√γ +
√γ|T ∗|"
THE HARD CASE,0.23901712583767684,"n + 1 , 3√γ n
X"
THE HARD CASE,0.23976172747580043,"i=1
ai ! ."
THE HARD CASE,0.24050632911392406,"It now suffices to prove that for all S′ ⊆S, P
a∈S′ a = t if and only if ∃T ′ ⊆T such that
249"
THE HARD CASE,0.24125093075204765,"w : w(0)
T ′
−→w satisfies ytestw⊤xtest > 0.
250"
THE HARD CASE,0.24199553239017127,If: Suppose ∃S′ ⊆S such that P
THE HARD CASE,0.24274013402829486,"a∈S a = t, we prove that ∃T ′ ⊆T such that ytest(w∗)⊤xtest > 0
251"
THE HARD CASE,0.24348473566641846,"for w∗satisfying w(0)
T ′
−→w∗.
252"
THE HARD CASE,0.24422933730454208,"Let T ∗= {(xi, yi)|ai ∈S′}, T ′ = T ∗∪{(xc, yc), (xb, yb), (xa, ya)}. We have
253"
THE HARD CASE,0.24497393894266567,"wc = (−18n2m2√γ +
√γ|T ∗|"
THE HARD CASE,0.2457185405807893,"n + 1 , 3√γ
X"
THE HARD CASE,0.24646314221891288,"ai∈S′
ai) = (−18n2m2√γ +
√γ|T ∗|"
THE HARD CASE,0.24720774385703648,"n + 1 , 3√γt)."
THE HARD CASE,0.2479523454951601,"And therefore ycw⊤
c xc = γ

(−18n2m2 + |T ∗|"
THE HARD CASE,0.2486969471332837,"n+1)(18n2m2 −2) −9t2
< −1 ≤β, so
254"
THE HARD CASE,0.24944154877140728,"wc
(xc,yc)
−−−−→wb = wc + xcyc = (√γ( |T ∗|"
THE HARD CASE,0.2501861504095309,"n + 1 −2), 0)."
THE HARD CASE,0.2509307520476545,"Note that ybw⊤
b xb = γ( |T ∗|"
THE HARD CASE,0.2516753536857781,"n+1 −2) < −1 ≤β, we have
255"
THE HARD CASE,0.2524199553239017,"wb
(xb,yb)
−−−−→wa = wb + xaya = (√γ( |T ∗|"
THE HARD CASE,0.25316455696202533,"n + 1 −1), −√γ)"
THE HARD CASE,0.2539091586001489,"Note also that yaw⊤
a xa = γ( |T ∗|"
THE HARD CASE,0.2546537602382725,"n+1 −2) < −1 ≤β, we have
256"
THE HARD CASE,0.25539836187639614,"wa
(xa,ya)
−−−−−→w∗= wa + xaya = (|T ∗|√γ"
THE HARD CASE,0.2561429635145197,"n + 1 , 0)"
THE HARD CASE,0.2568875651526433,"Therefore, ytest(w∗)⊤xtest = |T ∗|√γ"
THE HARD CASE,0.25763216679076695,"n+1
> 0.
257"
THE HARD CASE,0.25837676842889057,"Only if: For each T ′ ⊆T, let T ∗= T ′ \ {(xc, yc), (xb, yb), (xa, ya)}. If ytest(w∗)⊤xtest > 0 for
258"
THE HARD CASE,0.25912137006701413,"w∗satisfying w(0)
T ′
−→w∗, we prove that ∃S′ ⊆S such that P"
THE HARD CASE,0.25986597170513775,"a∈S′ a = t. We first show that for
259"
THE HARD CASE,0.2606105733432614,"each T ′ ⊆T, if w(w(0)
T ′
−→w) satisfying ytestw⊤xtest > 0, we have ∀k ∈{a, b, c}, (xk, yk) ∈
260"
THE HARD CASE,0.26135517498138494,"T ′, ykw⊤
k xk < γ, where w(0)
T ∗
−−→wc
(xc,yc)
−−−−→wb
(xb,yb)
−−−−→wa. Otherwise, suppose ∃k ∈{a, b, c}
261"
THE HARD CASE,0.26209977661950856,"such that (xk, yk) ̸∈T ′ or ykw⊤
k xk ≥γ, we have
262"
THE HARD CASE,0.2628443782576322,ytestw⊤xtest ≤√γ( |T ∗|
THE HARD CASE,0.26358897989575575,n + 1 −1) < 0
THE HARD CASE,0.26433358153387937,"which contradicts to the fact that ytestw⊤xtest ≥0.
263"
THE HARD CASE,0.265078183172003,"Let S′ = {ai|(xi, yi) ∈T ∗} and t′ = P"
THE HARD CASE,0.26582278481012656,"a∈S′ ai, it suffices to prove t′ = t. Notice that
264"
THE HARD CASE,0.2665673864482502,"w(0)
T ∗∩{(xi,yi)|1≤j≤i}
−−−−−−−−−−−−−→wc = (√γ(−18n2m2 + |T ∗|"
THE HARD CASE,0.2673119880863738,"n + 1), 3√γ
X"
THE HARD CASE,0.2680565897244974,"ai∈S′
ai)"
THE HARD CASE,0.268801191362621,= (√γ(−18n2m2 + |T ∗|
THE HARD CASE,0.2695457930007446,"n + 1), 3√γt′)"
THE HARD CASE,0.2702903946388682,"Hence ycw⊤
c xc = γ(−18n2m2 + |T ∗|"
THE HARD CASE,0.2710349962769918,"n+1)(18n2m2 −2) −9γtt′ < −1 ≤β, thus
265"
THE HARD CASE,0.2717795979151154,"wc
(xc,yc)
−−−−→wb = wc + xcyc = (√γ( |T ∗|"
THE HARD CASE,0.27252419955323903,"n + 1 −2), 3√γ(t′ −t))"
THE HARD CASE,0.2732688011913626,"(1) If t′ ≤t −1, we have ybw⊤
b xb = γ

|T ∗|
n+1 −2 + 3(t −t′)

> γ ≥β, a contradiction.
266"
THE HARD CASE,0.2740134028294862,"(2) If t′ ≥t + 1, we have yaw⊤
a xa = γ

|T ∗|
n+1 −2 + 3(t′ −t)

> γ ≥β, another contradiction.
267"
THE HARD CASE,0.27475800446760984,"Therefore t′ = t, and this completes the proof.
268"
THE HARD CASE,0.2755026061057334,"Moreover, DEBUGGABLE-LIN is NP-hard even when d = 1 and β < 0.
269"
THE HARD CASE,0.276247207743857,"Theorem 4.4. If the training order is adversarially chosen and d = 1, DEBUGGABLE-LIN remains
270"
THE HARD CASE,0.27699180938198065,"NP-hard for each hinge-like loss function with β < 0 at every constant learning rate.
271"
THE HARD CASE,0.27773641102010427,"Remarks. The training order in this section can be arbitrary as long as the last three training
272"
THE HARD CASE,0.27848101265822783,"samples are (xc, yc), (xb, yb), (xa, ya), respectively. All the training samples are “good” since for
273"
THE HARD CASE,0.27922561429635145,"each (x, y) ∈T we have x⊤xtestyytest > 0. This implies that DEBUGGABLE-LIN is NP-hard even if
274"
THE HARD CASE,0.2799702159344751,"all the training data are “good” training samples, and exemplifies why the GTA algorithm fails for
275"
THE HARD CASE,0.28071481757259864,"higher dimensions.
276"
DISCUSSION AND CONCLUSION,0.28145941921072226,"5
Discussion and Conclusion
277"
DISCUSSION AND CONCLUSION,0.2822040208488459,"In this paper, we provided a comprehensive analysis on the complexity of DEBUGGABLE. We focus
278"
DISCUSSION AND CONCLUSION,0.28294862248696945,"on the linear classifier that is trained using SGD, as it is a key component in the majority of popular
279"
DISCUSSION AND CONCLUSION,0.28369322412509307,"models.
280"
DISCUSSION AND CONCLUSION,0.2844378257632167,"Since DEBUGGABLE is a special case of data debugging, the above results proved the intractability
281"
DISCUSSION AND CONCLUSION,0.2851824274013403,"of data debugging and therefore gives a negative answer to Problem 1.1 declared in the introduction.
282"
DISCUSSION AND CONCLUSION,0.2859270290394639,"The complexity results also demonstrated that it is not accurate to estimate the impact of subset of
283"
DISCUSSION AND CONCLUSION,0.2866716306775875,"training data by summing up the score of each training samples in the subset, as long as the scores
284"
DISCUSSION AND CONCLUSION,0.2874162323157111,"can be calculated in polynomial time.
285"
DISCUSSION AND CONCLUSION,0.2881608339538347,"In Section 4, a training sample is said to be “good” if it can help the resulting model to predict
286"
DISCUSSION AND CONCLUSION,0.2889054355919583,"correctly on the test instance. That is, it can increase ytest(w∗)⊤xtest. However, in our proof we
287"
DISCUSSION AND CONCLUSION,0.2896500372300819,"showed that DEBUGGABLE remains NP-hard even if all training samples are “good”. This suggests
288"
DISCUSSION AND CONCLUSION,0.2903946388682055,"that the quality of a training sample does not depend only on some properties of itself but also on
289"
DISCUSSION AND CONCLUSION,0.2911392405063291,"the interaction between the rest of the training data, which should be taken into consideration when
290"
DISCUSSION AND CONCLUSION,0.29188384214445273,"developing data cleaning approaches.
291"
DISCUSSION AND CONCLUSION,0.2926284437825763,"Moreover, the NP-hardness of DEBUGGABLE implies that, it is in general intractable to figure out the
292"
DISCUSSION AND CONCLUSION,0.2933730454206999,"causality between even the prediction of a linear classifier and its training data. This may be seem
293"
DISCUSSION AND CONCLUSION,0.29411764705882354,"surprising since linear classifiers have long been considered “inherently interpretable”. As warned
294"
DISCUSSION AND CONCLUSION,0.29486224869694716,"in [25], a method being “inherently interpretable” needs to be verified before it can be trusted, the
295"
DISCUSSION AND CONCLUSION,0.2956068503350707,"concept of interpretability must be rigorously defined, or at least its boundaries specified.
296"
DISCUSSION AND CONCLUSION,0.29635145197319435,"Our results suggests the following directions for future research. Firstly, characterizing the training
297"
DISCUSSION AND CONCLUSION,0.29709605361131797,"sample may be helpful in designing efficient algorithms for data debugging; Secondly, designing
298"
DISCUSSION AND CONCLUSION,0.29784065524944153,"algorithms using CSP-solver is a potential way to solve data debugging more efficiently than the brute-
299"
DISCUSSION AND CONCLUSION,0.29858525688756515,"force algorithms; Finally, developing random algorithms is a potential way to solve data debugging
300"
DISCUSSION AND CONCLUSION,0.2993298585256888,"successfully with high probability.
301"
REFERENCES,0.30007446016381234,"References
302"
REFERENCES,0.30081906180193596,"[1] Satoshi Hara, Atsushi Nitanda, and Takanori Maehara. Data Cleansing for Models Trained with SGD.
303"
REFERENCES,0.3015636634400596,"Curran Associates Inc., Red Hook, NY, USA, 2019.
304"
REFERENCES,0.30230826507818315,"[2] Weiyuan Wu, Lampros Flokas, Eugene Wu, and Jiannan Wang. Complaint-driven training data debugging
305"
REFERENCES,0.30305286671630677,"for query 2.0. pages 1317–1334, 06 2020. doi: 10.1145/3318464.3389696.
306"
REFERENCES,0.3037974683544304,"[3] Bojan Karlaš, David Dao, Matteo Interlandi, Bo Li, Sebastian Schelter, Wentao Wu, and Ce Zhang. Data
307"
REFERENCES,0.304542069992554,"debugging with shapley importance over end-to-end machine learning pipelines, 2022.
308"
REFERENCES,0.3052866716306776,"[4] Felix Neutatz, Binger Chen, Ziawasch Abedjan, and Eugene Wu. From cleaning before ml to cleaning for
309"
REFERENCES,0.3060312732688012,"ml. IEEE Data Eng. Bull., 44:24–41, 2021. URL https://api.semanticscholar.org/CorpusID:
310"
REFERENCES,0.3067758749069248,"237542697.
311"
REFERENCES,0.3075204765450484,"[5] Peng Li, Xi Rao, Jennifer Blase, Yue Zhang, Xu Chu, and Ce Zhang. Cleanml: A study for evaluating the
312"
REFERENCES,0.308265078183172,"impact of data cleaning on ml classification tasks. In 2021 IEEE 37th International Conference on Data
313"
REFERENCES,0.3090096798212956,"Engineering (ICDE), pages 13–24, 2021. doi: 10.1109/ICDE51399.2021.00009.
314"
REFERENCES,0.3097542814594192,"[6] Juhan Bae, Nathan Ng, Alston Lo, Marzyeh Ghassemi, and Roger Grosse. If influence functions are
315"
REFERENCES,0.3104988830975428,"the answer, then what is the question? In Proceedings of the 36th International Conference on Neural
316"
REFERENCES,0.31124348473566643,"Information Processing Systems, NIPS ’22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN
317"
REFERENCES,0.31198808637379,"9781713871088.
318"
REFERENCES,0.3127326880119136,"[7] Romila Pradhan, Jiongli Zhu, Boris Glavic, and Babak Salimi. Interpretable data-based explanations for
319"
REFERENCES,0.31347728965003724,"fairness debugging. In Proceedings of the 2022 International Conference on Management of Data,
320"
REFERENCES,0.31422189128816086,"SIGMOD ’22, page 247–261, New York, NY, USA, 2022. Association for Computing Machinery.
321"
REFERENCES,0.3149664929262844,"ISBN 9781450392495. doi: 10.1145/3514221.3517886. URL https://doi.org/10.1145/3514221.
322"
REFERENCES,0.31571109456440805,"3517886.
323"
REFERENCES,0.31645569620253167,"[8] Pang Wei Koh and Percy Liang.
Understanding black-box predictions via influence functions.
In
324"
REFERENCES,0.31720029784065523,"Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17, page
325"
REFERENCES,0.31794489947877885,"1885–1894. JMLR.org, 2017.
326"
REFERENCES,0.3186895011169025,"[9] Rajiv Khanna, Been Kim, Joydeep Ghosh, and Oluwasanmi Koyejo. Interpreting black box predictions
327"
REFERENCES,0.31943410275502604,"using fisher kernels. In International Conference on Artificial Intelligence and Statistics, 2018. URL
328"
REFERENCES,0.32017870439314966,"https://api.semanticscholar.org/CorpusID:53085397.
329"
REFERENCES,0.3209233060312733,"[10] Pang Wei Koh, Kai-Siang Ang, Hubert Hua Kian Teo, and Percy Liang. On the accuracy of influence
330"
REFERENCES,0.32166790766939685,"functions for measuring group effects. In Neural Information Processing Systems, 2019. URL https:
331"
REFERENCES,0.32241250930752047,"//api.semanticscholar.org/CorpusID:173188850.
332"
REFERENCES,0.3231571109456441,"[11] Samyadeep Basu, Xuchen You, and Soheil Feizi. On second-order group influence functions for black-
333"
REFERENCES,0.3239017125837677,"box predictions. In Proceedings of the 37th International Conference on Machine Learning, ICML’20.
334"
REFERENCES,0.3246463142218913,"JMLR.org, 2020.
335"
REFERENCES,0.3253909158600149,"[12] Han Guo, Nazneen Rajani, Peter Hase, Mohit Bansal, and Caiming Xiong. FastIF: Scalable influence
336"
REFERENCES,0.3261355174981385,"functions for efficient model interpretation and debugging. In Marie-Francine Moens, Xuanjing Huang,
337"
REFERENCES,0.3268801191362621,"Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods
338"
REFERENCES,0.3276247207743857,"in Natural Language Processing, pages 10333–10350, Online and Punta Cana, Dominican Republic,
339"
REFERENCES,0.3283693224125093,"November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.808.
340"
REFERENCES,0.3291139240506329,"URL https://aclanthology.org/2021.emnlp-main.808.
341"
REFERENCES,0.3298585256887565,"[13] Amirata Ghorbani and James Y. Zou. Data shapley: Equitable valuation of data for machine learning.
342"
REFERENCES,0.33060312732688013,"ArXiv, abs/1904.02868, 2019. URL https://api.semanticscholar.org/CorpusID:102350503.
343"
REFERENCES,0.3313477289650037,"[14] R. Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nicholas Hynes, Nezihe Merve Gürel, Bo Li,
344"
REFERENCES,0.3320923306031273,"Ce Zhang, Dawn Xiaodong Song, and Costas J. Spanos. Towards efficient data valuation based on the
345"
REFERENCES,0.33283693224125094,"shapley value. ArXiv, abs/1902.10275, 2019. URL https://api.semanticscholar.org/CorpusID:
346"
REFERENCES,0.33358153387937456,"67855573.
347"
REFERENCES,0.3343261355174981,"[15] Ruoxi Jia, Fan Wu, Xuehui Sun, Jiacen Xu, David Dao, Bhavya Kailkhura, Ce Zhang, Bo Li, and Dawn
348"
REFERENCES,0.33507073715562175,"Song. Scalability vs. utility: Do we have to sacrifice one for the other in data importance quantification?
349"
REFERENCES,0.33581533879374537,"In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8235–8243,
350"
REFERENCES,0.33655994043186893,"2021. doi: 10.1109/CVPR46437.2021.00814.
351"
REFERENCES,0.33730454206999255,"[16] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nezihe Merve Gurel, Bo Li, Ce Zhang, Costas
352"
REFERENCES,0.3380491437081162,"Spanos, and Dawn Song. Efficient task-specific data valuation for nearest neighbor algorithms. Proc.
353"
REFERENCES,0.33879374534623974,"VLDB Endow., 12(11):1610–1623, jul 2019. ISSN 2150-8097. doi: 10.14778/3342263.3342637. URL
354"
REFERENCES,0.33953834698436336,"https://doi.org/10.14778/3342263.3342637.
355"
REFERENCES,0.340282948622487,"[17] Jeremy Mange. Effect of training data order for machine learning. In 2019 International Conference
356"
REFERENCES,0.34102755026061055,"on Computational Science and Computational Intelligence (CSCI), pages 406–407, 2019. doi: 10.1109/
357"
REFERENCES,0.34177215189873417,"CSCI49370.2019.00078.
358"
REFERENCES,0.3425167535368578,"[18] Ernie Chang, Hui-Syuan Yeh, and Vera Demberg. Does the order of training samples matter? improving
359"
REFERENCES,0.3432613551749814,"neural data-to-text generation with curriculum learning. ArXiv, abs/2102.03554, 2021. URL https:
360"
REFERENCES,0.344005956813105,"//api.semanticscholar.org/CorpusID:231846815.
361"
REFERENCES,0.3447505584512286,"[19] Yejia Liu, Weiyuan Wu, Lampros Flokas, Jiannan Wang, and Eugene Wu. Enabling sql-based training
362"
REFERENCES,0.3454951600893522,"data debugging for federated learning. Proceedings of the VLDB Endowment, 15:388–400, 02 2022. doi:
363"
REFERENCES,0.3462397617274758,"10.14778/3494124.3494125.
364"
REFERENCES,0.3469843633655994,"[20] Marc-Etienne Brunet, Colleen Alkalay-Houlihan, Ashton Anderson, and Richard Zemel. Understanding
365"
REFERENCES,0.347728965003723,"the origins of bias in word embeddings, 2019.
366"
REFERENCES,0.3484735666418466,"[21] Hao Wang, Berk Ustun, and Flavio P. Calmon. Repairing without retraining: Avoiding disparate impact
367"
REFERENCES,0.3492181682799702,"with counterfactual distributions, 2019.
368"
REFERENCES,0.34996276991809383,"[22] Xiaotie Deng and Christos H. Papadimitriou. On the complexity of cooperative solution concepts. Math.
369"
REFERENCES,0.35070737155621745,"Oper. Res., 19:257–266, 1994. URL https://api.semanticscholar.org/CorpusID:12946448.
370"
REFERENCES,0.351451973194341,"[23] Qi Wang, Yue Ma, Kun Zhao, and Yingjie Tian. A comprehensive survey of loss functions in machine
371"
REFERENCES,0.35219657483246464,"learning. Annals of Data Science, 9, 04 2022. doi: 10.1007/s40745-020-00253-5.
372"
REFERENCES,0.35294117647058826,"[24] Erik D. Demaine, William Gasarch, and Mohammad Hajiaghayi. Computational Intractability: A Guide to
373"
REFERENCES,0.3536857781087118,"Algorithmic Lower Bounds. MIT Press, 2024.
374"
REFERENCES,0.35443037974683544,"[25] Alon Jacovi and Yoav Goldberg. Towards faithfully interpretable nlp systems: How should we define and
375"
REFERENCES,0.35517498138495907,"evaluate faithfulness? In Annual Meeting of the Association for Computational Linguistics, 2020. URL
376"
REFERENCES,0.35591958302308263,"https://api.semanticscholar.org/CorpusID:215416110.
377"
REFERENCES,0.35666418466120625,"[26] Victor Parque. Tackling the subset sum problem with fixed size using an integer representation scheme.
378"
REFERENCES,0.3574087862993299,"In 2021 IEEE Congress on Evolutionary Computation (CEC), pages 1447–1453, 2021. doi: 10.1109/
379"
REFERENCES,0.35815338793745344,"CEC45853.2021.9504889.
380"
REFERENCES,0.35889798957557706,"A
Detailed Proofs for Section 3
381"
REFERENCES,0.3596425912137007,"Notations. Given some orderings {oe} of training data, where oe
t as the order of t in epoch e. We
382"
REFERENCES,0.3603871928518243,"use w(e,l)
xi
to denote the value of wxi after the l-th iteration in epoch e. We also denote xt and yt as
383"
REFERENCES,0.36113179448994787,"the feature and the label of training data t, respectively. We denote t(e,l) as the training sample being
384"
REFERENCES,0.3618763961280715,"considered during epoch e, iteration l.
385"
REFERENCES,0.3626209977661951,"Lemma A.1. Suppose T ⊆T0 is the training data and let T e
l,r = {t(e,l), t(e,l+1), . . . , t(e,r)}
386"
REFERENCES,0.3633655994043187,"be the set of consecutive training samples considered during epoch e from iteration l to r. For
387"
REFERENCES,0.3641102010424423,"1 ≤l ≤r ≤|T|, if clause(γ, i1, i2, i3)̸∈T e
l,r, then w(e,l−1)
cγ
= w(e,r)
cγ
.
388"
REFERENCES,0.3648548026805659,"Proof. For each t ∈T e
l,r, we have (xt)cγ = 0. Therefore
389"
REFERENCES,0.3655994043186895,"∂L
∂cγ t"
REFERENCES,0.3663440059568131,"≤max
−12N"
REFERENCES,0.3670886075949367,"5
yxcγ"
REFERENCES,0.3678332092330603,", | −yxcγ|,
−
1
1000N yxcγ"
REFERENCES,0.3685778108711839,", 0

= 0"
REFERENCES,0.36932241250930753,"Hence
∂L
∂cγ"
REFERENCES,0.37006701414743115,"t = 0, and
390"
REFERENCES,0.3708116157855547,"w(e,r)
cγ
= w(e,l−1)
cγ
−ηcγ
X"
REFERENCES,0.37155621742367834,"t∈T e
l,r"
REFERENCES,0.37230081906180196,"∂L
∂cγ"
REFERENCES,0.3730454206999255,"t
= w(e,l−1)
cγ"
REFERENCES,0.37379002233804914,"Similarly, (xt)bγ = 0, and
391"
REFERENCES,0.37453462397617276,"∂L
∂bγ t"
REFERENCES,0.37527922561429633,"≤max
−12N"
REFERENCES,0.37602382725241995,"5
yxbγ"
REFERENCES,0.37676842889054357,", | −yxbγ|,
−
1
1000N yxbγ"
REFERENCES,0.37751303052866714,", 0

= 0"
REFERENCES,0.37825763216679076,"Hence
∂L
∂bγ"
REFERENCES,0.3790022338049144,"t = 0, and
392"
REFERENCES,0.379746835443038,"w(e,r)
bγ
= w(e,l−1)
bγ
−ηbγ
X"
REFERENCES,0.38049143708116157,"t∈T e
l,r"
REFERENCES,0.3812360387192852,"∂L
∂bγ"
REFERENCES,0.3819806403574088,"t
= w(e,l−1)
bγ 393"
REFERENCES,0.3827252419955324,"Lemma A.2. Suppose T ⊆T0 is the training data and Tl := {t(1,1), . . . , t(1,l)}. ∀1 ≤i ≤n, 1 ≤
394"
REFERENCES,0.383469843633656,"l ≤|T|, w(1,l)
xi
∈U(1,
l+1
6000N 2 ) if var(i)∈Tl; Otherwise w(1,l)
xi
∈U(−1,
l+1
6000N2 ).
395"
REFERENCES,0.3842144452717796,"Proof. We prove this lemma by induction.
396"
REFERENCES,0.3849590469099032,"Basic Case: Note that for all 1 ≤i ≤n, w(0)
xi = −1, and for all 1 ≤γ ≤m, w(0)
cγ = 1/2, w(0)
bγ = −1.
397"
REFERENCES,0.3857036485480268,"We denote t = t(1,1) to avoid cluttering. For any fixed i:
398"
REFERENCES,0.3864482501861504,"(1) If t =var(i). We have yt(w(0))⊤x′
t = 5w(0)
xi = −5, hence
399"
REFERENCES,0.387192851824274,"∂L
∂wxi"
REFERENCES,0.3879374534623976,"t
= −12N"
REFERENCES,0.38868205510052123,"5
yt(xt)i = −12N"
REFERENCES,0.38942665673864485,"and
400"
REFERENCES,0.3901712583767684,"w(1,1)
xi
= w(0)
xi −ηxi
∂L
∂wxi"
REFERENCES,0.39091586001489204,"t
= −1 −1"
N,0.39166046165301566,6N
N,0.3924050632911392,"
−12N 5"
N,0.39314966492926284,"
= 1 ∈U(1,
2
6000N 2 )"
N,0.39389426656738646,"(2) If t =clause(γ, i, i′, i′′). We have
401"
N,0.39463886820551003,"yt(w(0))⊤x′
t = w(0)
xi + w(0)
xi′ + w(0)
xi′′ + w(0)
cγ + 1"
N,0.39538346984363365,"2w(0)
bγ = −3"
N,0.39612807148175727,"hence
402"
N,0.39687267311988084,"∂L
∂wxi"
N,0.39761727475800446,"t
= −
1
1000N yt(xt)xi = −
1
1000N"
N,0.3983618763961281,"and
403"
N,0.3991064780342517,"w(1,1)
xi
= w(0)
xi −ηxi
∂L
∂wxi"
N,0.39985107967237526,"t
= −1 −1"
N,0.4005956813104989,6N
N,0.4013402829486225,"
−
1
1000N "
N,0.40208488458674607,"= −1 +
1
6000N 2 ∈U(−1,
2
6000N 2 )"
N,0.4028294862248697,"(3) Otherwise, wxi will not be updated. Therefore w(1,1)
xi
= w(0)
xi = −1 ∈U(−1,
2
6000N 2 ).
404"
N,0.4035740878629933,"Hence this lemma is true for l = 1.
405"
N,0.4043186895011169,"Induction Step: Suppose the lemma is true for l < |T|. We prove that this lemma remains true for
406"
N,0.4050632911392405,"l + 1. We denote t = t(1,l+1) to avoid cluttering. This makes sense since l + 1 ≤|T| and thus t ∈T.
407"
N,0.4058078927773641,"For any fixed i:
408"
N,0.40655249441548774,"(1) If t =var(i), then var(i)̸∈Tl because there are at most one var(i) in T for each i.
409"
N,0.4072970960536113,"Therefore w(1,l)
xi
∈U(−1,
l+1
6000N2 ).
We have yt(w(1,l))⊤x′
t = 5w(1,l)
xi
∈U(−5, 0.01), and
410"
N,0.40804169769173493,"∂L
∂wxi"
N,0.40878629932985855,t = −12N
N,0.4095309009679821,"5 yt(xt)i = −12N. Hence
411"
N,0.41027550260610574,"w(1,l+1)
xi
= w(1,l)
xi
−ηxi
∂L
∂wxi"
N,0.41102010424422936,"t
= w(1,l)
xi
−1"
N,0.4117647058823529,6N
N,0.41250930752047654,"
−12N 5 "
N,0.41325390915860016,"= w(1,l)
xi
+ 2 ∈U(1,
l + 2
6000N 2 )"
N,0.41399851079672373,"(2) If t =clause(γ, i, i′, i′′). In this case, clause(γ, ·, ·, ·)̸∈T 1
1,l and by Lemma A.1 we have
412"
N,0.41474311243484735,"w(1,l)
cγ
= w(0)
cγ , w(1,l)
bγ
= w(0)
bγ . From the induction hypothesis we have
413"
N,0.41548771407297097,"w(1,l)
xi
, w(1,l)
xi′ , w(1,l)
xi′′ ∈U(±1,
l + 1
6000N 2 )"
N,0.4162323157110946,"and thus
414"
N,0.41697691734921816,"yt(w(1,l))⊤x′
t = w(1,l)
xi
+ w(1,l)
xi′
+ w(1,l)
xi′′ + w(1,l)
cγ
+ 1"
N,0.4177215189873418,"2w(1,l)
bγ"
N,0.4184661206254654,"= w(1,l)
xi
+ w(1,l)
xi′
+ w(1,l)
xi′′ ∈
["
N,0.41921072226358896,"x0∈{±1,±3}
U(x0, 3(l + 1)"
N,0.4199553239017126,"6000N 2 ) ⊆
["
N,0.4206999255398362,"x0∈{±1,±3}
U(x0, 0.01)"
N,0.42144452717795977,"We have
∂L
∂wxi"
N,0.4221891288160834,"t = −
1
1000N and w(1,l+1)
xi
= w(1,l)
xi
−ηxi
∂L
∂wxi"
N,0.422933730454207,"t = w(1,l)
xi
+
1
6000N2 . Consider the
415"
N,0.4236783320923306,"following cases:
416"
N,0.4244229337304542,"• If var(i)∈Tl, then var(i)∈Tl+1 and w(1,l)
xi
∈U(1,
l+1
6000N 2 ). Therefore w(1,l+1)
xi
∈
417"
N,0.4251675353685778,"U(1,
l+2
6000N2 ).
418"
N,0.42591213700670144,"• If var(i)̸∈Tl, then var(i)̸∈Tl+1 and w(1,l)
xi
∈U(−1,
l+1
6000N2 ). Therefore w(1,l+1)
xi
∈
419"
N,0.426656738644825,"U(−1,
l+2
6000N2 ).
420"
N,0.4274013402829486,"(3) Otherwise, wxi will not be updated, and w(1,l+1)
xi
= w(1,l)
xi
. If var(i)∈Tl then var(i)∈Tl+1 and
421"
N,0.42814594192107225,"w(1,l+1)
xi
∈U(1,
l+2
6000N2 ); Otherwise var(i)̸∈Tl+1 and w(1,l+1)
xi
∈U(−1,
l+2
6000N 2 ).
422"
N,0.4288905435591958,"Hence if the lemma is true for l < |T|, it is also true for l + 1. Therefore, the lemma is true for all
423"
N,0.42963514519731943,"1 ≤l ≤|T|.
424"
N,0.43037974683544306,"Corollary A.1. Suppose T ⊆T0 is the training data. ∀1 ≤i ≤n, 1 ≤l ≤|T|, if var(i)∈T, then
425"
N,0.4311243484735666,"w(1)
xi ∈U(1,
1
6000N ). Otherwise w(1)
xi ∈U(−1,
1
6000N ).
426"
N,0.43186895011169024,"Proof. Note that w(1)
xi = w(1,|T |)
xi
and N = 2m + n + 1. By Lemma A.2, if var(i)∈T we have
427"
N,0.43261355174981386,"w(1,|T |)
xi
∈U(1, |T| + 1"
N,0.43335815338793743,"6000N 2 ) ⊆U(1, m + n + 1"
N,0.43410275502606105,"6000N 2 ) ⊆U(1,
1
6000N )"
N,0.43484735666418467,"If var(i)̸∈T, we have
428"
N,0.4355919583023083,"w(1,|T |)
xi
∈U(−1, |T| + 1"
N,0.43633655994043186,"6000N 2 ) ⊆U(−1, m + n + 1"
N,0.4370811615785555,"6000N 2 ) ⊆U(−1,
1
6000N ) 429"
N,0.4378257632166791,"Lemma A.3. Suppose T ⊆T0 is the training data. ∀1 ≤γ ≤m, if ∃1 ≤i1, i2, i3 ≤n such that
430"
N,0.43857036485480266,"clause(γ, i1, i2, i3) ∈T, then w(1)
bγ = 0, w(1)
cγ = 1"
N,0.4393149664929263,"2 +
1
200N ; Otherwise, w(1)
bγ = −1, w(1)
cγ = 1"
N,0.4400595681310499,"2.
431"
N,0.44080416976917347,"Proof. (1) If such tγ =clause(γ, i1, i2, i3) exists in T, by Lemma A.2 we have
432"
N,0.4415487714072971,"w
(1,o1
tγ )
xi1
+ w
(1,o1
tγ )
xi2
+ w
(1,o1
tγ )
xi3
∈
["
N,0.4422933730454207,"x0∈{±1,±3}
U(x0,
3(o1
tγ + 1)"
N,0.4430379746835443,"6000N 2 ) ⊆
["
N,0.4437825763216679,"x0∈{±1,±3}
U(x0, 0.01)"
N,0.4445271779597915,"By Lemma A.1 we have w
(1,o1
tγ −1)
cγ
= w(0)
cγ and w
(1,o1
tγ −1)
bγ
= w(0)
bγ because clause(γ, ·, ·, ·)̸∈
433"
N,0.44527177959791514,"T 1
1,otγ −1. Hence
434"
N,0.4460163812360387,"ytγ(w(1,o1
tγ −1))⊤x′
tγ = w
(1,o1
tγ )
xi1
+ w
(1,o1
tγ )
xi2
+ w
(1,o1
tγ )
xi3
+ w
(1,o1
tγ −1)
cγ
+ 1"
W,0.4467609828741623,"2w
(1,o1
tγ −1)
bγ"
W,0.44750558451228595,"= w
(1,o1
tγ )
xi1
+ w
(1,o1
tγ )
xi2
+ w
(1,o1
tγ )
xi3
+ w
(1,o1
tγ −1)
cγ ∈
["
W,0.4482501861504095,"x0∈{±1,±3}
U(x0, 0.01)"
W,0.44899478778853313,"We have
∂L
∂wcγ"
W,0.44973938942665675,"tγ
= −
1
1000N , and
435"
W,0.4504839910647803,"w
(1,o1
tγ )
cγ
= w
(1,o1
tγ −1)
cγ
−ηcγ
∂L
∂wcγ"
W,0.45122859270290394,"tγ
= 1"
W,0.45197319434102756,"2 + 5 ×
1
1000N = 1"
W,0.4527177959791511,"2 +
1
200N"
W,0.45346239761727475,"Similarly,
∂L
∂wbγ"
W,0.45420699925539837,"tγ = −
1
2000N and
436"
W,0.454951600893522,"w
(1,o1
tγ )
bγ
= w
(1,o1
tγ −1)
bγ
−ηbγ
∂L
∂wcγ"
W,0.45569620253164556,"tγ
= −1 −2000N × (−
1
2000N ) = 0"
W,0.4564408041697692,"Note also that clause(γ, ·, ·, ·)̸∈T 1
otγ ,|T |, by Lemma A.1 we have
437"
W,0.4571854058078928,"w(1)
cγ = w(1,|T |)
cγ
= w
(1,o1
tγ )
cγ
= 1"
W,0.45793000744601636,"2 +
1
200N and w(1)
bγ = w(1,|T |)
bγ
= w
(1,o1
tγ )
bγ
= 0.
438"
W,0.45867460908414,"(2) If such tγ =clause(γ, i1, i2, i3) does not exist in T, by Lemma A.1 we have w(1)
cγ = w(0)
cγ = 1 2
439"
W,0.4594192107222636,"and w(1)
bγ = w(0)
bγ = −1.
440"
W,0.46016381236038717,"Lemma A.4. Suppose T ⊆T0 and Cl be the number of clause() in T 2
1,l. ∀1 ≤i ≤n, 1 ≤l ≤|T|,
441"
W,0.4609084139985108,"w(2,l)
xi
∈U(1, Cl+1/2"
N,0.4616530156366344,"6N
) if var(i)∈T; Otherwise w(2,l)
xi
∈U(−1, Cl+1/2"
N,0.462397617274758,"6N
).
442"
N,0.4631422189128816,"Proof. Similar to the proof of A.2, we prove this lemma by induction.
443"
N,0.4638868205510052,"Basic Case: Note that for all 1 ≤i ≤n, w(1)
xi = U(±1,
1
6000N ), and for all 1 ≤γ ≤m, w(1)
cγ ∈
444 { 1 2, 1"
N,0.46463142218912884,"2 +
1
200N }, w(1)
bγ ∈{−1, 0}. We denote t = t(2,1) to avoid cluttering. For any fixed i:
445"
N,0.4653760238272524,"(1) If t =var(i), C1 = 0. By Corollary A.1, w(1)
xi = U(1,
1
6000N ). We have
446"
N,0.466120625465376,"yt(w(1))⊤x′
t = 5w(1)
xi ∈U(5,
1
1200N )"
N,0.46686522710349965,"hence
∂L
∂wxi"
N,0.4676098287416232,"t = 0, and
447"
N,0.46835443037974683,"w(2,1)
xi
= w(1)
xi ∈U(1, 1"
N,0.46909903201787045,"6N ) = U(1, Cl + 1/2"
N,0.469843633655994,"6N
)"
N,0.47058823529411764,"(2) If t =clause(γ, i, i′, i′′), C1 = 1. By Lemma A.3, we have w(1)
cγ = 1"
N,0.47133283693224126,"2 +
1
200N and w(1)
bγ = 0.
448"
N,0.4720774385703649,"Therefore,
449"
N,0.47282204020848845,"yt(w(1))⊤x′
t = w(1)
xi + w(1)
xi′ + w(1)
xi′′ + w(1)
cγ + 1"
N,0.47356664184661207,"2w(1)
bγ"
N,0.4743112434847357,"= w(1)
xi + w(1)
xi′ + w(1)
xi′′ + 1"
N,0.47505584512285925,"2 −
1
200N
∈
["
N,0.4758004467609829,x0∈{ 1
N,0.4765450483991065,"2 ±1, 1"
N,0.47728965003723006,"2 ±3}
U(x0, 0.01)"
N,0.4780342516753537,"hence
∂L
∂wxi"
N,0.4787788533134773,"t ∈{0, −yxxi} = {−1, 0}, and ηxi
∂L
∂wxi"
N,0.47952345495160087,t ∈{−1
N,0.4802680565897245,"6N , 0}.
450"
N,0.4810126582278481,"By Corollary A.1, if var(i)∈T, we have
451"
N,0.48175725986597173,"w(2,1)
xi
= w(1)
xi −ηxi
∂L
∂wxi"
N,0.4825018615040953,"t
∈U(1, 3/2"
N,0.4832464631422189,"6N ) = U(1, Cl + 1/2"
N,0.48399106478034254,"6N
)"
N,0.4847356664184661,"If var(i)̸∈T, we have
452"
N,0.4854802680565897,"w(2,1)
xi
= w(1)
xi −ηxi
∂L
∂wxi"
N,0.48622486969471335,"t
∈U(−1, 3/2"
N,0.4869694713328369,"6N ) = U(−1, Cl + 1/2"
N,0.48771407297096053,"6N
)"
N,0.48845867460908415,"(3) Otherwise, wxi will not be updated and C1 ≤1. Therefore if var(i)∈T,
453"
N,0.4892032762472077,"w(2,1)
xi
= w(1)
xi ∈U(1, 3/2"
N,0.48994787788533134,"6N ) ⊆U(1, Cl + 1/2"
N,0.49069247952345496,"6N
)"
N,0.4914370811615786,"If var(i)̸∈T,
454"
N,0.49218168279970215,"w(2,1)
xi
= w(1)
xi ∈U(−1, 3/2"
N,0.49292628443782577,"6N ) ⊆U(−1, Cl + 1/2"
N,0.4936708860759494,"6N
)"
N,0.49441548771407295,"Hence this lemma is true for l = 1.
455"
N,0.4951600893521966,"Induction Step: Suppose the lemma is true for l < |T|. We prove that this lemma remains true for
456"
N,0.4959046909903202,"l + 1. We denote t = t(2,l+1) to avoid cluttering. This makes sense since l + 1 ≤|T| and thus t ∈T.
457"
N,0.49664929262844376,"For any fixed i:
458"
N,0.4973938942665674,"(1) If t =var(i), Cl+1 = Cl. By Corollary A.1, w(2,l)
xi
∈U(1, Cl+1/2"
N,0.498138495904691,"6N
).
459"
N,0.49888309754281457,"We have yt(w(2,l))⊤x′
t = 5w(2,l)
xi
∈U(5, 1/6) and
∂L
∂wxi"
N,0.4996276991809382,"t = 0.Hence w(2,l+1)
xi
= w(2,l)
xi
∈
460"
N,0.5003723008190618,"U(1, Cl+1+1/2"
N,0.5011169024571854,"6N
).
461"
N,0.501861504095309,"(2) If t =clause(γ, i, i′, i′′), Cl+1 = Cl + 1. In this case, clause(γ, ·, ·, ·)̸∈T 2
1,l and by Lemma
462"
N,0.5026061057334326,"A.1 and Lemma A.3 we have w(2,l)
cγ
= w(1)
cγ = 1"
N,0.5033507073715562,"2 +
1
200N , w(2,l)
bγ
= w(1)
bγ = 0. From the induction
463"
N,0.5040953090096798,"hypothesis we have w(2,l)
xi
, w(2,l)
xi′ , w(2,l)
xi′′ ∈U(±1, Cl+1/2"
N,0.5048399106478034,"6N
). Noting that
464"
N,0.505584512285927,Cl + 1/2
N,0.5063291139240507,"6N
≤m + 1/2"
N,0.5070737155621743,"6N
=
m + 1/2
(n + 2(m + 1/2)) ≤1"
WE HAVE,0.5078183172002978,"12
we have
465"
WE HAVE,0.5085629188384214,"yt(w(2,l))⊤x′
t = w(2,l)
xi
+ w(2,l)
xi′
+ w(2,l)
xi′′ + w(2,l)
cγ
+ 1"
WE HAVE,0.509307520476545,"2w(2,l)
bγ"
WE HAVE,0.5100521221146687,"= w(2,l)
xi
+ w(2,l)
xi′
+ w(2,l)
xi′′ + 1"
WE HAVE,0.5107967237527923,"2 +
1
200N ∈
["
WE HAVE,0.5115413253909159,x0∈{ 1
WE HAVE,0.5122859270290394,"2 ±1, 1"
WE HAVE,0.513030528667163,"2 ±3}
U

x0, 3(Cl + 1/2)"
N,0.5137751303052867,"6N
+
1
200N  ⊆
["
N,0.5145197319434103,x0∈{ 1
N,0.5152643335815339,"2 ±1, 1"
N,0.5160089352196575,"2 ±3}
U(x0, 0.26)"
N,0.5167535368577811,"And thus
∂L
∂wxi"
N,0.5174981384959046,"t ∈{0, −yxxi} = {−1, 0}, and ηxi
∂L
∂wxi"
N,0.5182427401340283,t ∈{−1
N,0.5189873417721519,"6N , 0}.
466"
N,0.5197319434102755,"By Corollary A.1, if var(i)∈T, w(2,l+1)
xi
= w(l)
xi −ηxi
∂L
∂wxi"
N,0.5204765450483991,"t ∈U(1, Cl+3/2"
N,0.5212211466865228,"6N
) = U(1, Cl+1+1/2"
N,0.5219657483246463,"6N
);
467"
N,0.5227103499627699,"if var(i)̸∈T, w(2,l+1)
xi
= w(l)
xi −ηxi
∂L
∂wxi"
N,0.5234549516008935,"t ∈U(−1, Cl+3/2"
N,0.5241995532390171,"6N
) = U(−1, Cl+1+1/2"
N,0.5249441548771407,"6N
).
468"
N,0.5256887565152644,"(3) Otherwise, wxi will not be updated. We have Cl+1 ≤Cl + 1 w(2,l+1)
xi
= w(2,l)
xi
. If var(i)∈T
469"
N,0.526433358153388,"then w(2,l+1)
xi
∈U(1, Cl+1+1/2"
N,0.5271779597915115,"6N
); If var(i)̸∈T then w(2,l+1)
xi
∈U(−1, Cl+1+1/2"
N,0.5279225614296351,"6N
).
470"
N,0.5286671630677587,"Hence if the lemma is true for l < |T|, it is also true for l + 1. Therefore, the lemma is true for all
471"
N,0.5294117647058824,"1 ≤l ≤|T|.
472"
N,0.530156366344006,"Corollary A.2. Suppose T ⊆T0 is the training data. ∀1 ≤i ≤n, if var(i)∈T, then w(2)
xi ∈
473"
N,0.5309009679821296,"U(1, 0.1). Otherwise w(2)
xi ∈U(−1, 0.1).
474"
N,0.5316455696202531,"Proof. Note that w(2)
xi = w(2,|T |)
xi
and C|T | ≤m. By Lemma A.4, if var(i)∈T we have
475"
N,0.5323901712583767,"w(2,|T |)
xi
∈U(1, C|T | + 1/2"
N,0.5331347728965004,"6N
) ⊆U(1, m + 1/2"
N,0.533879374534624,"6N
) ⊆U(1, 1"
N,0.5346239761727476,"12) ⊆U(1, 0.1)"
N,0.5353685778108712,"If var(i)̸∈T, we have
476"
N,0.5361131794489948,"w(1,|T |)
xi
∈U(−1, C|T | + 1/2"
N,0.5368577810871183,"6N
) ⊆U(−1, m + 1/2"
N,0.537602382725242,"6N
) ⊆U(−1, 1"
N,0.5383469843633656,"12) ⊆U(−1, 0.1) 477"
N,0.5390915860014892,"Lemma A.5. Suppose T ⊆T0 is the training data. ∀1 ≤i ≤m, if ∃1 ≤i1, i2, i3 ≤n such that
478"
N,0.5398361876396128,"clause(i, i1, i2, i3) ∈T, then
479"
N,0.5405807892777365,"1. w(2)
bj = 1000N;
480"
N,0.54132539091586,"2. w(2)
cj = 11"
N,0.5420699925539836,"2 +
1
200N if exactly one of var(i1), var(i2), var(i3) is in T. Otherwise w(2)
cj =
481"
N,0.5428145941921072,"1
2 +
1
200N .
482"
N,0.5435591958302308,"Otherwise, w(2)
bi = −1, w(2)
ci = 1"
N,0.5443037974683544,"2.
483"
N,0.5450483991064781,"Proof. (1) If such tγ =clause(γ, i1, i2, i3) exists in T, by Lemma A.4 we have
484"
N,0.5457930007446017,"w
(2,o1
tγ )
xi1
, w
(2,o1
tγ )
xi2
, w
(2,o1
tγ )
xi3
∈U(±1, m + 1/2"
N,0.5465376023827252,"6N
) ⊆U(±1,
1
12N )"
N,0.5472822040208488,"By Lemma A.1 we have w
(2,o1
tγ −1)
cγ
= w(1)
cγ
= 1"
N,0.5480268056589724,"2 +
1
200N and w
(2,o1
tγ −1)
bγ
= w(1)
bγ
= 0 because
485"
N,0.5487714072970961,"clause(γ, ·, ·, ·)̸∈T 1
1,otγ −1. Consider the following two cases:
486"
N,0.5495160089352197,"(a) If exactly one of var(i1), var(i2), var(i3) is in T, by Corollary A.2 we have
487"
N,0.5502606105733433,"ytγ(w(2,o1
tγ −1))⊤x′
tγ = w
(2,o1
tγ −1)
xi1
+ w
(2,o1
tγ −1)
xi2
+ w
(2,o1
tγ −1)
xi3
+ w
(2,o1
tγ −1)
cγ
+ 1"
W,0.5510052122114668,"2w
(2,o1
tγ −1)
bγ"
W,0.5517498138495904,"= w
(2,o1
tγ −1)
xi1
+ w
(2,o1
tγ −1)
xi2
+ w
(2,o1
tγ −1)
xi3
+ 1"
W,0.552494415487714,"2 +
1
200N ∈U(−1"
W,0.5532390171258377,"2,
3
12N +
1
200N ) ⊆U(−1"
W,0.5539836187639613,"2, 0.26)"
W,0.5547282204020849,"Hence
∂L
∂wcγ"
W,0.5554728220402085,"tγ = −1, and
488"
W,0.556217423678332,"w
(2,o1
tγ )
cγ
= w
(2,o1
tγ −1)
cγ
−ηcγ
∂L
∂wcγ"
W,0.5569620253164557,"tγ
= 1"
W,0.5577066269545793,"2 +
1
200N + 5 = 11"
W,0.5584512285927029,"2 +
1
200N"
W,0.5591958302308265,"Similarly,
489"
W,0.5599404318689502,"w
(2,o1
tγ )
bγ
= w
(2,o1
tγ −1)
bγ
−ηbγ
∂L
∂wbγ"
W,0.5606850335070737,"tγ
= 1000N"
W,0.5614296351451973,"Note also that clause(γ, ·, ·, ·)̸∈T 1
otγ ,|T |, by Lemma A.1 we have w(2)
cγ = w(2,|T |)
cγ
= w
(2,o1
tγ )
cγ
=
490 11"
W,0.5621742367833209,"2 −
1
200N and w(2)
bγ = w(2,|T |)
bγ
= w
(2,o1
tγ )
bγ
= 1000N.
491"
W,0.5629188384214445,"(b) Otherwise, we have
492"
W,0.5636634400595681,"ytγ(w(2,o1
tγ −1))⊤x′
tγ = w
(2,o1
tγ −1)
xi1
+ w
(2,o1
tγ −1)
xi2
+ w
(2,o1
tγ −1)
xi3
+ w
(2,o1
tγ −1)
cγ
+ 1"
W,0.5644080416976918,"2w
(2,o1
tγ −1)
bγ"
W,0.5651526433358154,"= w
(2,o1
tγ −1)
xi1
+ w
(2,o1
tγ −1)
xi2
+ w
(2,o1
tγ −1)
xi3
+ 1"
W,0.5658972449739389,"2 +
1
200N ∈
["
W,0.5666418466120625,"x0∈{−7 2 , 1 2 , 5"
W,0.5673864482501861,"2 }
U(x0,
3
12N +
1
200N ) ⊆
["
W,0.5681310498883098,"x0∈{−7 2 , 1 2 , 5"
W,0.5688756515264334,"2 }
U(x0, 0.26)"
W,0.569620253164557,"Hence
∂L
∂wcγ"
W,0.5703648548026806,"tγ =
∂L
∂wbγ"
W,0.5711094564408041,"tγ = 0, so w
(2,o1
tγ )
cγ
= w
(2,o1
tγ −1)
cγ
= 1"
W,0.5718540580789278,"2 +
1
200N , w
(2,o1
tγ )
bγ
= w
(2,o1
tγ −1)
bγ
= 0.
493"
W,0.5725986597170514,"Note also that clause(γ, ·, ·, ·)̸∈T 1
otγ ,|T |, by Lemma A.1 we have w(2)
cγ = w(2,|T |)
cγ
= w
(2,o1
tγ )
cγ
=
494"
W,0.573343261355175,"1
2 +
1
200N and w(2)
bγ = w(2,|T |)
bγ
= w
(2,o1
tγ )
bγ
= 0.
495"
W,0.5740878629932986,"(2) If such tγ =clause(γ, i1, i2, i3) does not exist in T, by Lemma A.1 and Lemma A.3 we have
496"
W,0.5748324646314222,"w(2)
cγ = w(1)
cγ = 1"
W,0.5755770662695457,"2 and w(2)
bγ = w(1)
bγ = −1.
497"
W,0.5763216679076694,"Moreover, w reaches its fixpoint at the end of the second epoch and will no longer be updated.
498"
W,0.577066269545793,"Lemma A.6. w(2) = w(3).
499"
W,0.5778108711839166,"Proof. Suppose w(2) ̸= w(3), then there exists 1 ≤i ≤N such that w(2)
i
̸= w(3)
i
, and there
500"
W,0.5785554728220402,"are some training sample t in the training data such that
∂L
∂w(2)
i"
W,0.5793000744601638,"t
̸= 0. Let t = (xt, yt) and
501"
W,0.5800446760982875,"I = U(−5, 0.01) ∪U(−1"
W,0.580789277736411,"2, 0.26) ∪
S
x0∈{±1,±3} U(x0, 0.01)

. By (2) we have yt(w(2))⊤xt′ ∈I.
502"
W,0.5815338793745346,"At least one of the following is true:
503"
W,0.5822784810126582,"1. ∃1 ≤i ≤n, t = var(i).
According to lemma A.2, yt(w(2))⊤xt′ = yw(2)
xi xi ∈
504"
W,0.5830230826507818,"U(5, 0.5) ⊆R \ I, contradicting to yt(w(2))⊤xt′ ∈I.
505"
W,0.5837676842889055,"2. ∃1 ≤i ≤m and 1 ≤i1, i2, i3 ≤n, such that t = clause(i, i1, i2, i3). According to
506"
W,0.5845122859270291,"lemma A.5, we have
507"
W,0.5852568875651526,"yt(w(2))⊤xt
′ = w(2)
bi + w(2)
ci + w(2)
xi1 + w(2)
xi2 + w(2)
xi3"
W,0.5860014892032762,≥1000N + 1
W,0.5867460908413998,"2 +
1
200N + 3 × (−1 −0.1)"
W,0.5874906924795235,≥1000 −3.3 ≥996
W,0.5882352941176471,"We have yt(w(2))⊤xt′ ̸∈I, another contradiction.
508"
W,0.5889798957557707,"Therefore w(2) = w(3), w reaches its fixpoint at the end of the second epoch. In other words,
509"
W,0.5897244973938943,"w∗= w(2).
510"
W,0.5904690990320178,"We are now ready to give a rigorous proof of theorem 3.1.
511"
W,0.5912137006701415,"Proof of theorem 3.1. It only suffices to prove the correctness of the reduction in section 3.
512"
W,0.5919583023082651,"If. Suppose φ ∈MONOTONE 1-IN-3 SAT, then there is a truth assignment ν(·) that assigns exactly
513"
W,0.5927029039463887,"one variable in each clause of φ is true. Let ∆= {var(i)|ν(xi) = FALSE}. Let w′ be the parameter
514"
W,0.5934475055845123,"of SGDΛ(T0 \ ∆). By Lemma A.5, (w′)cγ = 11"
W,0.5941921072226359,"2 +
1
200N for all 1 ≤γ ≤m, hence
515"
W,0.5949367088607594,"(w′)⊤xtest = m
X"
W,0.5956813104988831,"γ=1
w′
cγ ≥11m"
W,0.5964259121370067,"2
+ −11m + 5 2
= 5 2 > 0"
W,0.5971705137751303,"and λw′(xtest) = 1, thus SGDΛ(T0) is thus debuggable.
516"
W,0.5979151154132539,"Only if. Suppose SGDΛ(T0) is debuggable, there will be a ∆such that SGDΛ(T0, xtest) = ytest . We
517"
W,0.5986597170513775,"denote w′ as the parameter trained by SGD on T0 \ ∆. We have λw′(xtest) = 1 and (w′)⊤xtest ≥0.
518"
W,0.5994043186895012,"By Lemma A.5, w′
cγ = { 1"
W,0.6001489203276247,"2 +
1
200N , 11"
W,0.6008935219657483,"2 +
1
200N }. Suppose wc∗= 1"
W,0.6016381236038719,"2 +
1
200N , then
519"
W,0.6023827252419955,"(w′)⊤xtest = wc∗+
X"
W,0.6031273268801192,"cγ̸=c∗
wcγ ≤11"
W,0.6038719285182428,2 (m −1) + 1
W,0.6046165301563663,"2 +
m
200N −11m 2
+ 5 2 = −5"
W,0.6053611317944899,"2 +
m
200N ≤−5"
W,0.6061057334326135,"2 +
1
200 = −2.495 < 0"
W,0.6068503350707372,"leading to a contradiction.
520"
W,0.6075949367088608,"As a consequence, w′
cγ =
11"
W,0.6083395383469844,"2 +
1
200N for all 1 ≤γ ≤m. By Lemma A.5, exactly one of
521"
W,0.609084139985108,"var(i1),var(i2),var(i3) is in T0 \ ∆for each cγ = (xi1 ∨xi2 ∨xi3). Consider a truth assignment ν
522"
W,0.6098287416232315,"that maps every xi to FALSE where var(i)∈∆, and maps the rest to TRUE. Then ν assigns exactly
523"
W,0.6105733432613552,"one variable true in each cγ = (xi1 ∨xi2 ∨xi3) if and only if exactly one of var(i1),var(i2),var(i3)
524"
W,0.6113179448994788,"is in T0 \ ∆. Hence ν is a truth assignment that assigns true to exactly one variable in each clause of
525"
W,0.6120625465376024,"φ, and thus φ is a yes-instance of MONOTONE 1-IN-3 SAT.
526"
W,0.612807148175726,"B
Detailed Proofs for Section 4
527"
W,0.6135517498138496,"B.1
Proof of Theorem 4.4
528"
W,0.6142963514519731,"Proof. We build a reduction from the SUBSET SUM problem with a fixed size, which is NP-hard as a
529"
W,0.6150409530900968,"particular case of the class of knapsack problems [26]. Formally, it is defined as:
530"
W,0.6157855547282204,"SUBSET SUM with a fixed size
Input: A set of positive integer S, and two positive integers t, k.
Output: “Yes”: if ∃S′ ⊆S of size k such that P"
W,0.616530156366344,"a∈S′ a = t;
“No”: otherwise. 531"
W,0.6172747580044676,"The ordered training data T is constructed as
532"
W,0.6180193596425912,"T = {(x1, y1), (x2, y2), . . . , (xn, yn)} ∪{(xa, ya)}"
W,0.6187639612807149,where xiyi = 2
W,0.6195085629188384,"3 +
ai
3 P"
W,0.620253164556962,"a∈S a for all 1 ≤i ≤n and xaya = 1 +
1
6 P"
W,0.6209977661950856,"a∈S a. Let η = 1, α = 1, β = −1,
533"
W,0.6217423678332092,w(0) = −1 −2
W,0.6224869694713329,"3k −
t
3 P"
W,0.6232315711094565,"a∈S a and let the test instance (xtest, ytest) satisfy xtestytest = 1. It now suffices
534"
W,0.62397617274758,to prove that ∃S′ ⊆S such that |S′| = k and P
W,0.6247207743857036,"a∈S′ a = t if and only if ∃T ′ ⊆T such that
535"
W,0.6254653760238272,"w : w(0)
T ′
−→w satisfies ytestwxtest > 0.
536"
W,0.6262099776619509,If: Suppose ∃S′ ⊆S such that |S′| = k and P
W,0.6269545793000745,"a∈S a = t. Let T ∗= {(xi, yi)|ai ∈S′}, we prove
537"
W,0.6276991809381981,"that ytestw∗xtest > 0 for w∗satisfying w(0)
T ′=T ∗∪{(xa,ya)}
−−−−−−−−−−−→w∗.
538 Since"
W,0.6284437825763217,"w(0) +
X"
W,0.6291883842144452,"ai∈S′
xiyi = −1 −2"
W,0.6299329858525688,"3k −
t
3 P"
W,0.6306775874906925,"a∈S a +
X ai∈S′ 2"
W,0.6314221891288161,"3 +
ai
3 P a∈S a "
W,0.6321667907669397,= −1 −2
W,0.6329113924050633,"3k −
t
3 P"
W,0.6336559940431868,"a∈S a +
X ai∈S′"
W,0.6344005956813105,"2
3 +
P"
W,0.6351451973194341,"a∈S′ a
3 P"
W,0.6358897989575577,a∈S a = −1
W,0.6366344005956813,"and ∀1 ≤i ≤n, xiyi > 2"
W,0.637379002233805,"3, for each 1 ≤i < n, suppose w(0)
T ∗∩{(xj,yj)|1≤j≤i}
−−−−−−−−−−−−−→wi, we have"
W,0.6381236038719286,wixi+1yi+1 < 
W,0.6388682055100521,"w(0) +
X"
W,0.6396128071481757,"aj∈S′
xjyj −2 3  · 2"
W,0.6403574087862993,3 < −10
W,0.6411020104244229,9 < β.
W,0.6418466120625466,"That is, each training sample in T ∗is activated. Then for w(0)
T ∗
−−→wa, we have wa = −1. Then,
539"
W,0.6425912137006702,"since yawaxa = −(1 +
1
6 P"
W,0.6433358153387937,"a∈S a) < β and wa
(xa,ya)
−−−−−→w∗we have w∗= wa + xaya =
1
6 P"
W,0.6440804169769173,"a∈S a.
540"
W,0.6448250186150409,"Therefore, ytestw∗xtest =
1
6 P"
W,0.6455696202531646,"a∈S a > 0.
541"
W,0.6463142218912882,"Only if: For each T ′ ⊆T, let T ∗= T ′ \ {(xa, ya)} and c(T ∗) be the set of training samples in
542"
W,0.6470588235294118,"T ∗that are activated. If ytestw∗xtest ≥0 for w∗satisfying w(0)
T ′
−→w∗, we prove that the set
543"
W,0.6478034251675354,"S′ = {ai|(xi, yi) ∈c(T ∗)} satisfies |S′| = k and P"
W,0.6485480268056589,"a∈S′ a = t.
544"
W,0.6492926284437825,"We first show that ytestwaxtest < 0 for w(0)
c(T ∗)
−−−→wa. Otherwise, suppose ytestwaxtest ≥0 we
545"
W,0.6500372300819062,"have wa ≥0. Let (x, y) be the last training sample of c(T ′), since 2"
W,0.6507818317200298,"3 < xy ≤1, we have
546"
W,0.6515264333581534,"w′ ≥wa −xy ≥−1 for w′
(x,y)
−−−→wa. Thus yw′x ≥β, which contradicts to the definition of c(T ∗).
547"
W,0.652271034996277,"We next show that |S′| = k. Suppose |S′| ≤k −1, we have"
W,0.6530156366344005,"wa = w(0) +
X"
W,0.6537602382725242,"(xi,yi)∈c(T ∗)
xiyi = −1 −2"
W,0.6545048399106478,"3k −
t
3 P"
W,0.6552494415487714,"a∈S a +
X ai∈S′"
W,0.655994043186895,"2
3 +
P"
W,0.6567386448250186,"a∈S′ a
3 P a∈S a"
W,0.6574832464631423,< −1 −2
W,0.6582278481012658,3k + 2
W,0.6589724497393894,3(k −1) + 1
W,0.659717051377513,3 = −4
W,0.6604616530156366,"3
Thus w∗≤wa + xaya < −4"
W,0.6612062546537603,"3 + (1 +
1
6 P"
W,0.6619508562918839,"a∈S a) < 0 and then ytestw∗xtest < 0, which contradicts to
548"
W,0.6626954579300074,"the fact that ytestw∗xtest ≥0. Therefore |S′| ≥k.
549"
W,0.663440059568131,"Suppose |S′| ≥k + 1, we have"
W,0.6641846612062546,"wa = w(0) +
X"
W,0.6649292628443783,"(xi,yi)∈c(T ∗)
xiyi ≥−1 −2 3k −1 3 + 2"
W,0.6656738644825019,3(k + 1) = −2 3
W,0.6664184661206255,Then yawaxa ≥(−2
W,0.6671630677587491,"3) · (1 +
1
6 P"
W,0.6679076693968726,a∈S a) ≥−7
W,0.6686522710349962,"9 ≥β, that is, (xa, ya) is not activated and w∗= wa.
550"
W,0.6693968726731199,"Then since ytestwaxtest < 0, we have ytestw∗xtest = ytestwaxtest < 0, which contradicts to the fact that
551"
W,0.6701414743112435,"ytestw∗xtest ≥0. Therefore |S′| = k.
552"
W,0.6708860759493671,"It remains to prove that P
a∈S′ a = t. Otherwise, suppose P
a∈S′ a ≤t −1, we have"
W,0.6716306775874907,"wa = w(0) +
X"
W,0.6723752792256142,"(xi,yi)∈c(T ∗)
xiyi ≤−1 −2"
W,0.6731198808637379,"3k −
t
3 P"
W,0.6738644825018615,a∈S a + 2
W,0.6746090841399851,"3k +
t −1
3 P a∈S a"
W,0.6753536857781087,"= −1 −
1
3 P
a∈S a"
W,0.6760982874162323,"Thus ytestw∗xtest ≤ytest(wa + xaya)xtest ≤−
1
6 P"
W,0.676842889054356,"a∈S a < 0, which contradicts to the fact that
553"
W,0.6775874906924795,ytestw∗xtest ≥0. Therefore P
W,0.6783320923306031,"a∈S′ a ≥t.
554"
W,0.6790766939687267,Suppose P
W,0.6798212956068503,a∈S′ a ≥t + 1 we have
W,0.680565897244974,"wa = w(0) +
X"
W,0.6813104988830976,"(xi,yi)∈c(T ∗)
xiyi ≥−1 −2"
W,0.6820551005212211,"3k −
t
3 P"
W,0.6827997021593447,a∈S a + 2
W,0.6835443037974683,"3k +
t + 1
3 P a∈S a"
W,0.684288905435592,"= −1 +
1
3 P a∈S a"
W,0.6850335070737156,"Thus
yawaxa ≥(−1 +
1
3 P"
W,0.6857781087118392,"a∈S a) · (1 +
1
6 P"
W,0.6865227103499628,a∈S a)
W,0.6872673119880863,"≥−1 +
1
6 P"
W,0.68801191362621,"a∈S a +
1
18(P"
W,0.6887565152643336,a∈S a)2 ≥β.
W,0.6895011169024572,"That is, (xa, ya) is not activated and w∗= wa. Then since ytestwaxtest < 0, we have ytestw∗xtest =
555"
W,0.6902457185405808,"ytestwaxtest < 0, which contradicts to the fact that ytestw∗xtest ≥0. Therefore P"
W,0.6909903201787044,"a∈S′ a = t.
556"
W,0.691734921816828,"B.2
Proof of Theorem 4.3 for β < −1
557"
W,0.6924795234549516,"Proof. To avoid cluttering, we still assume η = 1 and α = 1. The proof can be generalized by
558"
W,0.6932241250930752,"appropriately re-scaling the constructed vectors.
559"
W,0.6939687267311988,"Let M = −β(n + 2) + 9βnm2(n + 1) + 3. Suppose n = |S| > 1, m = maxa∈S{a} and
560"
W,0.6947133283693224,"S = {a1, a2, . . . , an}. We further assume n > 1. Let the ordered set of training samples be
561"
W,0.695457930007446,"T = {(x1, y1), (x2, y2), . . . , (xn, yn)} ∪{(xc, yc), (xb, yb), (xa, ya)}"
W,0.6962025316455697,"where xiyi = (
1
n+1, −3βai) for all 1 ≤i ≤n, xcyc = (M + 3"
W,0.6969471332836932,"2β −1, β(3t −1"
W,0.6976917349218168,"2)), xbyb =
562"
W,0.6984363365599404,"(1, −1), xaya = (−3"
W,0.699180938198064,"2β, −3"
W,0.6999255398361877,"2β). Let w(0) = (−M, 0). Let the test instance (xtest, ytest) satisfy
563"
W,0.7006701414743113,"xtestytest = (1, 0).
564"
W,0.7014147431124349,"For each 1 ≤i < n, suppose w(0)
T ∩{(xi,yi)|1≤j≤i}
−−−−−−−−−−−−→wi, we have
565"
W,0.7021593447505584,"yi+1w⊤
i xi+1 ≤−M ·
1
n + 1 +
i
(n + 1)2 + 9β2ai+1 i
X"
W,0.702903946388682,"j=1
aj"
W,0.7036485480268057,"≤−M ·
1
n + 1 +
n
(n + 1)2 + 9β2nm2 < β"
W,0.7043931496649293,"This means all the (xi, yi) ∈T \ {(xc, yc), (xb, yb), (xa, ya)} can be activated and thus the resulting
566"
W,0.7051377513030529,"parameter trained by T \ {(xc, yc), (xb, yb), (xa, ya)} is
567"
W,0.7058823529411765,"wc = w(0) + n
X"
W,0.7066269545793,"i=1
xiyi = "
W,0.7073715562174236,−M + |T ∗|
W,0.7081161578555473,"n + 1, −3β n
X"
W,0.7088607594936709,"i=1
ai !"
W,0.7096053611317945,"It now suffices to prove that for all S′ ⊆S, P"
W,0.7103499627699181,"a∈S′ a = t if and only if ∃T ′ ⊆T such that
568"
W,0.7110945644080418,"w : w(0)
T ′
−→w such that ytestw⊤xtest > 0.
569"
W,0.7118391660461653,"If: Suppose ∃S′ ⊆S such that P
a∈S a = t, we prove that ∃T ′ ⊆T such that ytest(w∗)⊤xtest > 0
570"
W,0.7125837676842889,"for w∗satisfying w(0)
T ∗
−−→w∗.
571"
W,0.7133283693224125,"Let T ∗= {(xi, yi)|ai ∈S′}, T ′ = T ∗∪{(xc, yc), (xb, yb), (xa, ya)}. We have
572"
W,0.7140729709605361,wc = (−M + |T ∗|
W,0.7148175725986597,"n + 1, −3β
X"
W,0.7155621742367834,"ai∈S′
ai) = (−M + |T ∗|"
W,0.7163067758749069,"n + 1, −3βt)"
W,0.7170513775130305,"And ycw⊤
c xc = (−M + |T ∗|"
W,0.7177959791511541,n+1)(M + 3
W,0.7185405807892777,2β −1) −3tβ2(3t −1
W,0.7192851824274014,"2) < β, so
573"
W,0.720029784065525,"wc
(xc,yc)
−−−−→wb = wc + xcyc = ( |T ∗|"
W,0.7207743857036486,n + 1 + 3
W,0.7215189873417721,"2β −1, −1 2β)"
W,0.7222635889798957,"Note that β < −1, we have ybw⊤
b xb = |T ∗|"
W,0.7230081906180194,n+1 + 2β < (β + |T ∗|
W,0.723752792256143,"n+1) + β < β, and
574"
W,0.7244973938942666,"wb
(xb,yb)
−−−−→wa = wb + xaya = ( |T ∗|"
W,0.7252419955323902,n + 1 + 3
W,0.7259865971705137,"2β, −1"
W,0.7267311988086373,2β −1)
W,0.727475800446761,"Note also that yaw⊤
a xa = 3"
W,0.7282204020848846,2(−β)( |T ∗|
W,0.7289650037230082,"n+1 −1 + β) < β, we have
575"
W,0.7297096053611318,"wa
(xa,ya)
−−−−−→w∗= wa + xaya = ( |T ∗|"
W,0.7304542069992555,"n + 1, −2β −1)"
W,0.731198808637379,"Therefore, ytest(w∗)⊤xtest = |T ∗|"
W,0.7319434102755026,"n+1 ≥0.
576"
W,0.7326880119136262,"Only if: For each T ′ ⊆T, let T ∗= T ′ \ {(xc, yc), (xb, yb), (xa, ya)}, if ytest(w∗)⊤xtest for w∗
577"
W,0.7334326135517498,"satisfying w(0)
T ′
−→w∗, we prove that ∃S′ ⊆S such that P"
W,0.7341772151898734,"a∈S′ a = t. We first show that for
578"
W,0.7349218168279971,"each T ′ ⊆T, if w(w(0)
T ′
−→w) satisfying ytestw⊤xtest ≥0, we have ∀k ∈{a, b, c}, (xk, yk) ∈
579"
W,0.7356664184661206,"T ′, ykw⊤
k xk < β, where w(0)
T ∗
−−→wc
(xc,yc)
−−−−→wb
(xb,yb)
−−−−→wa. Otherwise, suppose ∃k ∈{a, b, c}
580"
W,0.7364110201042442,"such that (xk, yk) ̸∈T ′ or ykw⊤
k xk ≥β, we have
581"
W,0.7371556217423678,ytestw⊤xtest ≤−M + |T ∗|
W,0.7379002233804914,n + 1 + M + 3
W,0.7386448250186151,2β −1 + 1 −3
W,0.7393894266567387,"2β −min

1, M + 3"
W,0.7401340282948623,"2β −1, −3 2β
"
W,0.7408786299329858,= |T ∗|
W,0.7416232315711094,n + 1 −1 < 0
W,0.742367833209233,"which contradicts to the fact that ytestw⊤xtest ≥0.
582"
W,0.7431124348473567,"Let S′ = {ai|(xi, yi) ∈T ∗} and t′ = P"
W,0.7438570364854803,"a∈S′ ai, it suffices to prove t′ = t. Notice that
583"
W,0.7446016381236039,"w(0)
T ∗
−−→wc = (−M + |T ∗|"
W,0.7453462397617274,"n + 1, −3β
X"
W,0.746090841399851,"ai∈S′
ai)"
W,0.7468354430379747,= (−M + |T ∗|
W,0.7475800446760983,"n + 1, −3βt′)"
W,0.7483246463142219,"Hence ycw⊤
c xc = (−M + |T ∗|"
W,0.7490692479523455,n+1)(M + 3
W,0.7498138495904692,2β −1) −3t′β2(3t −1
W,0.7505584512285927,"2) < β, thus
584"
W,0.7513030528667163,"wc
(xc,yc)
−−−−→wb = wc + xcyc = ( |T ∗|"
W,0.7520476545048399,n + 1 + 3
W,0.7527922561429635,"2β −1, −3β(t′ −t) −1 2β)"
W,0.7535368577810871,"(1) If t′ ≤t −1, we have
585"
W,0.7542814594192108,"ybw⊤
b xb = |T ∗|"
W,0.7550260610573343,n + 1 −1 + 2β + 3β(t′ −t)
W,0.7557706626954579,≥|T ∗|
W,0.7565152643335815,n + 1 −(1 + β) > 0 > β
W,0.7572598659717051,"a contradiction. Hence wa = wb
(xb,yb)
−−−−→wa = ( |T ∗|"
W,0.7580044676098288,n+1 + 3
W,0.7587490692479524,"2β, −3β(t′ −t) −1"
W,0.759493670886076,"2β −1).
586"
W,0.7602382725241995,"(2) If t′ ≥t + 1, we have
587"
W,0.7609828741623231,"yaw⊤
a xa = −3β 2"
W,0.7617274758004468, |T ∗|
W,0.7624720774385704,"n + 1 −1 + β −3β(t′ −t)
 ≥−3β 2"
W,0.763216679076694, |T ∗|
W,0.7639612807148176,"n + 1 −1 −2β
 > −3β 2"
W,0.7647058823529411, |T ∗|
W,0.7654504839910647,"n + 1 + 1

> 0 > β"
W,0.7661950856291884,"another contradiction. Therefore t′ = t, and this completes the proof.
588 589"
W,0.766939687267312,"C
Limitations
590"
W,0.7676842889054356,"It is important to emphasize that the complexity results in section 4 requires the training order to
591"
W,0.7684288905435592,"be adversarially chosen. The complexity of DEBUGGABLE for randomly chosen training order is
592"
W,0.7691734921816828,"unclear and needs to be figured out in the future research.
593"
W,0.7699180938198064,"NeurIPS Paper Checklist
594"
CLAIMS,0.77066269545793,"1. Claims
595"
CLAIMS,0.7714072970960536,"Question: Do the main claims made in the abstract and introduction accurately reflect the
596"
CLAIMS,0.7721518987341772,"paper’s contributions and scope?
597"
CLAIMS,0.7728965003723008,"Answer: [Yes]
598"
CLAIMS,0.7736411020104245,"Justification: The main results are discussed in section 3 and section 4.
599"
CLAIMS,0.774385703648548,"Guidelines:
600"
CLAIMS,0.7751303052866716,"• The answer NA means that the abstract and introduction do not include the claims
601"
CLAIMS,0.7758749069247952,"made in the paper.
602"
CLAIMS,0.7766195085629188,"• The abstract and/or introduction should clearly state the claims made, including the
603"
CLAIMS,0.7773641102010425,"contributions made in the paper and important assumptions and limitations. A No or
604"
CLAIMS,0.7781087118391661,"NA answer to this question will not be perceived well by the reviewers.
605"
CLAIMS,0.7788533134772897,"• The claims made should match theoretical and experimental results, and reflect how
606"
CLAIMS,0.7795979151154132,"much the results can be expected to generalize to other settings.
607"
CLAIMS,0.7803425167535368,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
608"
CLAIMS,0.7810871183916605,"are not attained by the paper.
609"
LIMITATIONS,0.7818317200297841,"2. Limitations
610"
LIMITATIONS,0.7825763216679077,"Question: Does the paper discuss the limitations of the work performed by the authors?
611"
LIMITATIONS,0.7833209233060313,"Answer: [Yes]
612"
LIMITATIONS,0.7840655249441548,"Justification: See section C in the appendix.
613"
LIMITATIONS,0.7848101265822784,"Guidelines:
614"
LIMITATIONS,0.7855547282204021,"• The answer NA means that the paper has no limitation while the answer No means that
615"
LIMITATIONS,0.7862993298585257,"the paper has limitations, but those are not discussed in the paper.
616"
LIMITATIONS,0.7870439314966493,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
617"
LIMITATIONS,0.7877885331347729,"• The paper should point out any strong assumptions and how robust the results are to
618"
LIMITATIONS,0.7885331347728965,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
619"
LIMITATIONS,0.7892777364110201,"model well-specification, asymptotic approximations only holding locally). The authors
620"
LIMITATIONS,0.7900223380491437,"should reflect on how these assumptions might be violated in practice and what the
621"
LIMITATIONS,0.7907669396872673,"implications would be.
622"
LIMITATIONS,0.7915115413253909,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
623"
LIMITATIONS,0.7922561429635145,"only tested on a few datasets or with a few runs. In general, empirical results often
624"
LIMITATIONS,0.7930007446016382,"depend on implicit assumptions, which should be articulated.
625"
LIMITATIONS,0.7937453462397617,"• The authors should reflect on the factors that influence the performance of the approach.
626"
LIMITATIONS,0.7944899478778853,"For example, a facial recognition algorithm may perform poorly when image resolution
627"
LIMITATIONS,0.7952345495160089,"is low or images are taken in low lighting. Or a speech-to-text system might not be
628"
LIMITATIONS,0.7959791511541325,"used reliably to provide closed captions for online lectures because it fails to handle
629"
LIMITATIONS,0.7967237527922562,"technical jargon.
630"
LIMITATIONS,0.7974683544303798,"• The authors should discuss the computational efficiency of the proposed algorithms
631"
LIMITATIONS,0.7982129560685034,"and how they scale with dataset size.
632"
LIMITATIONS,0.7989575577066269,"• If applicable, the authors should discuss possible limitations of their approach to
633"
LIMITATIONS,0.7997021593447505,"address problems of privacy and fairness.
634"
LIMITATIONS,0.8004467609828742,"• While the authors might fear that complete honesty about limitations might be used by
635"
LIMITATIONS,0.8011913626209978,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
636"
LIMITATIONS,0.8019359642591214,"limitations that aren’t acknowledged in the paper. The authors should use their best
637"
LIMITATIONS,0.802680565897245,"judgment and recognize that individual actions in favor of transparency play an impor-
638"
LIMITATIONS,0.8034251675353685,"tant role in developing norms that preserve the integrity of the community. Reviewers
639"
LIMITATIONS,0.8041697691734921,"will be specifically instructed to not penalize honesty concerning limitations.
640"
THEORY ASSUMPTIONS AND PROOFS,0.8049143708116158,"3. Theory Assumptions and Proofs
641"
THEORY ASSUMPTIONS AND PROOFS,0.8056589724497394,"Question: For each theoretical result, does the paper provide the full set of assumptions and
642"
THEORY ASSUMPTIONS AND PROOFS,0.806403574087863,"a complete (and correct) proof?
643"
THEORY ASSUMPTIONS AND PROOFS,0.8071481757259866,"Answer: [Yes]
644"
THEORY ASSUMPTIONS AND PROOFS,0.8078927773641102,"Justification: The proof of theorem 3.1 is available in section A; The proof of theorem 4.1
645"
THEORY ASSUMPTIONS AND PROOFS,0.8086373790022338,"and theorem 4.2 are available in section 4; The proof of theorem 4.3 is available in section 4
646"
THEORY ASSUMPTIONS AND PROOFS,0.8093819806403574,"and section B; The proof of theorem 4.4 is available in section B.
647"
THEORY ASSUMPTIONS AND PROOFS,0.810126582278481,"Guidelines:
648"
THEORY ASSUMPTIONS AND PROOFS,0.8108711839166046,"• The answer NA means that the paper does not include theoretical results.
649"
THEORY ASSUMPTIONS AND PROOFS,0.8116157855547282,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
650"
THEORY ASSUMPTIONS AND PROOFS,0.8123603871928519,"referenced.
651"
THEORY ASSUMPTIONS AND PROOFS,0.8131049888309755,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
652"
THEORY ASSUMPTIONS AND PROOFS,0.813849590469099,"• The proofs can either appear in the main paper or the supplemental material, but if
653"
THEORY ASSUMPTIONS AND PROOFS,0.8145941921072226,"they appear in the supplemental material, the authors are encouraged to provide a short
654"
THEORY ASSUMPTIONS AND PROOFS,0.8153387937453462,"proof sketch to provide intuition.
655"
THEORY ASSUMPTIONS AND PROOFS,0.8160833953834699,"• Inversely, any informal proof provided in the core of the paper should be complemented
656"
THEORY ASSUMPTIONS AND PROOFS,0.8168279970215935,"by formal proofs provided in appendix or supplemental material.
657"
THEORY ASSUMPTIONS AND PROOFS,0.8175725986597171,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8183172002978406,"4. Experimental Result Reproducibility
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8190618019359642,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8198064035740878,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8205510052122115,"of the paper (regardless of whether the code and data are provided or not)?
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8212956068503351,"Answer: [NA]
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8220402084884587,"Justification: This paper does not include experiments.
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8227848101265823,"Guidelines:
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8235294117647058,"• The answer NA means that the paper does not include experiments.
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8242740134028295,"• If the paper includes experiments, a No answer to this question will not be perceived
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8250186150409531,"well by the reviewers: Making the paper reproducible is important, regardless of
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8257632166790767,"whether the code and data are provided or not.
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8265078183172003,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.827252419955324,"to make their results reproducible or verifiable.
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8279970215934475,"• Depending on the contribution, reproducibility can be accomplished in various ways.
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8287416232315711,"For example, if the contribution is a novel architecture, describing the architecture fully
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8294862248696947,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8302308265078183,"be necessary to either make it possible for others to replicate the model with the same
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8309754281459419,"dataset, or provide access to the model. In general. releasing code and data is often
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8317200297840656,"one good way to accomplish this, but reproducibility can also be provided via detailed
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8324646314221892,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8332092330603127,"of a large language model), releasing of a model checkpoint, or other means that are
679"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8339538346984363,"appropriate to the research performed.
680"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8346984363365599,"• While NeurIPS does not require releasing code, the conference does require all submis-
681"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8354430379746836,"sions to provide some reasonable avenue for reproducibility, which may depend on the
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8361876396128072,"nature of the contribution. For example
683"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8369322412509308,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8376768428890543,"to reproduce that algorithm.
685"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8384214445271779,"(b) If the contribution is primarily a new model architecture, the paper should describe
686"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8391660461653015,"the architecture clearly and fully.
687"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8399106478034252,"(c) If the contribution is a new model (e.g., a large language model), then there should
688"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8406552494415488,"either be a way to access this model for reproducing the results or a way to reproduce
689"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8413998510796724,"the model (e.g., with an open-source dataset or instructions for how to construct
690"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.842144452717796,"the dataset).
691"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8428890543559195,"(d) We recognize that reproducibility may be tricky in some cases, in which case
692"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8436336559940432,"authors are welcome to describe the particular way they provide for reproducibility.
693"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8443782576321668,"In the case of closed-source models, it may be that access to the model is limited in
694"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8451228592702904,"some way (e.g., to registered users), but it should be possible for other researchers
695"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.845867460908414,"to have some path to reproducing or verifying the results.
696"
OPEN ACCESS TO DATA AND CODE,0.8466120625465376,"5. Open access to data and code
697"
OPEN ACCESS TO DATA AND CODE,0.8473566641846612,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
698"
OPEN ACCESS TO DATA AND CODE,0.8481012658227848,"tions to faithfully reproduce the main experimental results, as described in supplemental
699"
OPEN ACCESS TO DATA AND CODE,0.8488458674609084,"material?
700"
OPEN ACCESS TO DATA AND CODE,0.849590469099032,"Answer: [NA]
701"
OPEN ACCESS TO DATA AND CODE,0.8503350707371556,"Justification: This paper does not include experiments requiring code.
702"
OPEN ACCESS TO DATA AND CODE,0.8510796723752793,"Guidelines:
703"
OPEN ACCESS TO DATA AND CODE,0.8518242740134029,"• The answer NA means that paper does not include experiments requiring code.
704"
OPEN ACCESS TO DATA AND CODE,0.8525688756515264,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
705"
OPEN ACCESS TO DATA AND CODE,0.85331347728965,"public/guides/CodeSubmissionPolicy) for more details.
706"
OPEN ACCESS TO DATA AND CODE,0.8540580789277736,"• While we encourage the release of code and data, we understand that this might not be
707"
OPEN ACCESS TO DATA AND CODE,0.8548026805658973,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
708"
OPEN ACCESS TO DATA AND CODE,0.8555472822040209,"including code, unless this is central to the contribution (e.g., for a new open-source
709"
OPEN ACCESS TO DATA AND CODE,0.8562918838421445,"benchmark).
710"
OPEN ACCESS TO DATA AND CODE,0.857036485480268,"• The instructions should contain the exact command and environment needed to run to
711"
OPEN ACCESS TO DATA AND CODE,0.8577810871183916,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
712"
OPEN ACCESS TO DATA AND CODE,0.8585256887565152,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
713"
OPEN ACCESS TO DATA AND CODE,0.8592702903946389,"• The authors should provide instructions on data access and preparation, including how
714"
OPEN ACCESS TO DATA AND CODE,0.8600148920327625,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
715"
OPEN ACCESS TO DATA AND CODE,0.8607594936708861,"• The authors should provide scripts to reproduce all experimental results for the new
716"
OPEN ACCESS TO DATA AND CODE,0.8615040953090097,"proposed method and baselines. If only a subset of experiments are reproducible, they
717"
OPEN ACCESS TO DATA AND CODE,0.8622486969471332,"should state which ones are omitted from the script and why.
718"
OPEN ACCESS TO DATA AND CODE,0.8629932985852569,"• At submission time, to preserve anonymity, the authors should release anonymized
719"
OPEN ACCESS TO DATA AND CODE,0.8637379002233805,"versions (if applicable).
720"
OPEN ACCESS TO DATA AND CODE,0.8644825018615041,"• Providing as much information as possible in supplemental material (appended to the
721"
OPEN ACCESS TO DATA AND CODE,0.8652271034996277,"paper) is recommended, but including URLs to data and code is permitted.
722"
OPEN ACCESS TO DATA AND CODE,0.8659717051377513,"6. Experimental Setting/Details
723"
OPEN ACCESS TO DATA AND CODE,0.8667163067758749,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
724"
OPEN ACCESS TO DATA AND CODE,0.8674609084139985,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
725"
OPEN ACCESS TO DATA AND CODE,0.8682055100521221,"results?
726"
OPEN ACCESS TO DATA AND CODE,0.8689501116902457,"Answer: [NA]
727"
OPEN ACCESS TO DATA AND CODE,0.8696947133283693,"Justification: This paper does not include experiments.
728"
OPEN ACCESS TO DATA AND CODE,0.870439314966493,"Guidelines:
729"
OPEN ACCESS TO DATA AND CODE,0.8711839166046166,"• The answer NA means that the paper does not include experiments.
730"
OPEN ACCESS TO DATA AND CODE,0.8719285182427401,"• The experimental setting should be presented in the core of the paper to a level of detail
731"
OPEN ACCESS TO DATA AND CODE,0.8726731198808637,"that is necessary to appreciate the results and make sense of them.
732"
OPEN ACCESS TO DATA AND CODE,0.8734177215189873,"• The full details can be provided either with the code, in appendix, or as supplemental
733"
OPEN ACCESS TO DATA AND CODE,0.874162323157111,"material.
734"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8749069247952346,"7. Experiment Statistical Significance
735"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8756515264333582,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
736"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8763961280714817,"information about the statistical significance of the experiments?
737"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8771407297096053,"Answer: [NA]
738"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.877885331347729,"Justification: This paper does not include experiments.
739"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8786299329858526,"Guidelines:
740"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8793745346239762,"• The answer NA means that the paper does not include experiments.
741"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8801191362620998,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
742"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8808637379002234,"dence intervals, or statistical significance tests, at least for the experiments that support
743"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8816083395383469,"the main claims of the paper.
744"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8823529411764706,"• The factors of variability that the error bars are capturing should be clearly stated (for
745"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8830975428145942,"example, train/test split, initialization, random drawing of some parameter, or overall
746"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8838421444527178,"run with given experimental conditions).
747"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8845867460908414,"• The method for calculating the error bars should be explained (closed form formula,
748"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.885331347728965,"call to a library function, bootstrap, etc.)
749"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8860759493670886,"• The assumptions made should be given (e.g., Normally distributed errors).
750"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8868205510052122,"• It should be clear whether the error bar is the standard deviation or the standard error
751"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8875651526433358,"of the mean.
752"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8883097542814594,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
753"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.889054355919583,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
754"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8897989575577067,"of Normality of errors is not verified.
755"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8905435591958303,"• For asymmetric distributions, the authors should be careful not to show in tables or
756"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8912881608339538,"figures symmetric error bars that would yield results that are out of range (e.g. negative
757"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8920327624720774,"error rates).
758"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.892777364110201,"• If error bars are reported in tables or plots, The authors should explain in the text how
759"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8935219657483247,"they were calculated and reference the corresponding figures or tables in the text.
760"
EXPERIMENTS COMPUTE RESOURCES,0.8942665673864483,"8. Experiments Compute Resources
761"
EXPERIMENTS COMPUTE RESOURCES,0.8950111690245719,"Question: For each experiment, does the paper provide sufficient information on the com-
762"
EXPERIMENTS COMPUTE RESOURCES,0.8957557706626954,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
763"
EXPERIMENTS COMPUTE RESOURCES,0.896500372300819,"the experiments?
764"
EXPERIMENTS COMPUTE RESOURCES,0.8972449739389426,"Answer: [NA]
765"
EXPERIMENTS COMPUTE RESOURCES,0.8979895755770663,"Justification: This paper does not include experiments.
766"
EXPERIMENTS COMPUTE RESOURCES,0.8987341772151899,"Guidelines:
767"
EXPERIMENTS COMPUTE RESOURCES,0.8994787788533135,"• The answer NA means that the paper does not include experiments.
768"
EXPERIMENTS COMPUTE RESOURCES,0.9002233804914371,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
769"
EXPERIMENTS COMPUTE RESOURCES,0.9009679821295606,"or cloud provider, including relevant memory and storage.
770"
EXPERIMENTS COMPUTE RESOURCES,0.9017125837676843,"• The paper should provide the amount of compute required for each of the individual
771"
EXPERIMENTS COMPUTE RESOURCES,0.9024571854058079,"experimental runs as well as estimate the total compute.
772"
EXPERIMENTS COMPUTE RESOURCES,0.9032017870439315,"• The paper should disclose whether the full research project required more compute
773"
EXPERIMENTS COMPUTE RESOURCES,0.9039463886820551,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
774"
EXPERIMENTS COMPUTE RESOURCES,0.9046909903201787,"didn’t make it into the paper).
775"
CODE OF ETHICS,0.9054355919583023,"9. Code Of Ethics
776"
CODE OF ETHICS,0.9061801935964259,"Question: Does the research conducted in the paper conform, in every respect, with the
777"
CODE OF ETHICS,0.9069247952345495,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
778"
CODE OF ETHICS,0.9076693968726731,"Answer: [Yes]
779"
CODE OF ETHICS,0.9084139985107967,"Justification: They authors have made sure that the research conducted in the paper conform
780"
CODE OF ETHICS,0.9091586001489204,"with the NeurIPS Code of Ethics.
781"
CODE OF ETHICS,0.909903201787044,"Guidelines:
782"
CODE OF ETHICS,0.9106478034251675,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
783"
CODE OF ETHICS,0.9113924050632911,"• If the authors answer No, they should explain the special circumstances that require a
784"
CODE OF ETHICS,0.9121370067014147,"deviation from the Code of Ethics.
785"
CODE OF ETHICS,0.9128816083395384,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
786"
CODE OF ETHICS,0.913626209977662,"eration due to laws or regulations in their jurisdiction).
787"
BROADER IMPACTS,0.9143708116157856,"10. Broader Impacts
788"
BROADER IMPACTS,0.9151154132539091,"Question: Does the paper discuss both potential positive societal impacts and negative
789"
BROADER IMPACTS,0.9158600148920327,"societal impacts of the work performed?
790"
BROADER IMPACTS,0.9166046165301563,"Answer: [NA]
791"
BROADER IMPACTS,0.91734921816828,"Justification: The impacts are discussed in section 5
792"
BROADER IMPACTS,0.9180938198064036,"Guidelines:
793"
BROADER IMPACTS,0.9188384214445272,"• The answer NA means that there is no societal impact of the work performed.
794"
BROADER IMPACTS,0.9195830230826508,"• If the authors answer NA or No, they should explain why their work has no societal
795"
BROADER IMPACTS,0.9203276247207743,"impact or why the paper does not address societal impact.
796"
BROADER IMPACTS,0.921072226358898,"• Examples of negative societal impacts include potential malicious or unintended uses
797"
BROADER IMPACTS,0.9218168279970216,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
798"
BROADER IMPACTS,0.9225614296351452,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
799"
BROADER IMPACTS,0.9233060312732688,"groups), privacy considerations, and security considerations.
800"
BROADER IMPACTS,0.9240506329113924,"• The conference expects that many papers will be foundational research and not tied
801"
BROADER IMPACTS,0.924795234549516,"to particular applications, let alone deployments. However, if there is a direct path to
802"
BROADER IMPACTS,0.9255398361876396,"any negative applications, the authors should point it out. For example, it is legitimate
803"
BROADER IMPACTS,0.9262844378257632,"to point out that an improvement in the quality of generative models could be used to
804"
BROADER IMPACTS,0.9270290394638868,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
805"
BROADER IMPACTS,0.9277736411020104,"that a generic algorithm for optimizing neural networks could enable people to train
806"
BROADER IMPACTS,0.9285182427401341,"models that generate Deepfakes faster.
807"
BROADER IMPACTS,0.9292628443782577,"• The authors should consider possible harms that could arise when the technology is
808"
BROADER IMPACTS,0.9300074460163812,"being used as intended and functioning correctly, harms that could arise when the
809"
BROADER IMPACTS,0.9307520476545048,"technology is being used as intended but gives incorrect results, and harms following
810"
BROADER IMPACTS,0.9314966492926284,"from (intentional or unintentional) misuse of the technology.
811"
BROADER IMPACTS,0.932241250930752,"• If there are negative societal impacts, the authors could also discuss possible mitigation
812"
BROADER IMPACTS,0.9329858525688757,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
813"
BROADER IMPACTS,0.9337304542069993,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
814"
BROADER IMPACTS,0.9344750558451228,"feedback over time, improving the efficiency and accessibility of ML).
815"
SAFEGUARDS,0.9352196574832464,"11. Safeguards
816"
SAFEGUARDS,0.93596425912137,"Question: Does the paper describe safeguards that have been put in place for responsible
817"
SAFEGUARDS,0.9367088607594937,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
818"
SAFEGUARDS,0.9374534623976173,"image generators, or scraped datasets)?
819"
SAFEGUARDS,0.9381980640357409,"Answer: [NA]
820"
SAFEGUARDS,0.9389426656738645,"Justification: This paper only provides theoretical results and poses no such risks.
821"
SAFEGUARDS,0.939687267311988,"Guidelines:
822"
SAFEGUARDS,0.9404318689501117,"• The answer NA means that the paper poses no such risks.
823"
SAFEGUARDS,0.9411764705882353,"• Released models that have a high risk for misuse or dual-use should be released with
824"
SAFEGUARDS,0.9419210722263589,"necessary safeguards to allow for controlled use of the model, for example by requiring
825"
SAFEGUARDS,0.9426656738644825,"that users adhere to usage guidelines or restrictions to access the model or implementing
826"
SAFEGUARDS,0.9434102755026061,"safety filters.
827"
SAFEGUARDS,0.9441548771407298,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
828"
SAFEGUARDS,0.9448994787788533,"should describe how they avoided releasing unsafe images.
829"
SAFEGUARDS,0.9456440804169769,"• We recognize that providing effective safeguards is challenging, and many papers do
830"
SAFEGUARDS,0.9463886820551005,"not require this, but we encourage authors to take this into account and make a best
831"
SAFEGUARDS,0.9471332836932241,"faith effort.
832"
LICENSES FOR EXISTING ASSETS,0.9478778853313478,"12. Licenses for existing assets
833"
LICENSES FOR EXISTING ASSETS,0.9486224869694714,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
834"
LICENSES FOR EXISTING ASSETS,0.9493670886075949,"the paper, properly credited and are the license and terms of use explicitly mentioned and
835"
LICENSES FOR EXISTING ASSETS,0.9501116902457185,"properly respected?
836"
LICENSES FOR EXISTING ASSETS,0.9508562918838421,"Answer: [NA]
837"
LICENSES FOR EXISTING ASSETS,0.9516008935219658,"Justification: This paper does not use existing assets.
838"
LICENSES FOR EXISTING ASSETS,0.9523454951600894,"Guidelines:
839"
LICENSES FOR EXISTING ASSETS,0.953090096798213,"• The answer NA means that the paper does not use existing assets.
840"
LICENSES FOR EXISTING ASSETS,0.9538346984363366,"• The authors should cite the original paper that produced the code package or dataset.
841"
LICENSES FOR EXISTING ASSETS,0.9545793000744601,"• The authors should state which version of the asset is used and, if possible, include a
842"
LICENSES FOR EXISTING ASSETS,0.9553239017125837,"URL.
843"
LICENSES FOR EXISTING ASSETS,0.9560685033507074,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
844"
LICENSES FOR EXISTING ASSETS,0.956813104988831,"• For scraped data from a particular source (e.g., website), the copyright and terms of
845"
LICENSES FOR EXISTING ASSETS,0.9575577066269546,"service of that source should be provided.
846"
LICENSES FOR EXISTING ASSETS,0.9583023082650782,"• If assets are released, the license, copyright information, and terms of use in the
847"
LICENSES FOR EXISTING ASSETS,0.9590469099032017,"package should be provided. For popular datasets, paperswithcode.com/datasets
848"
LICENSES FOR EXISTING ASSETS,0.9597915115413254,"has curated licenses for some datasets. Their licensing guide can help determine the
849"
LICENSES FOR EXISTING ASSETS,0.960536113179449,"license of a dataset.
850"
LICENSES FOR EXISTING ASSETS,0.9612807148175726,"• For existing datasets that are re-packaged, both the original license and the license of
851"
LICENSES FOR EXISTING ASSETS,0.9620253164556962,"the derived asset (if it has changed) should be provided.
852"
LICENSES FOR EXISTING ASSETS,0.9627699180938198,"• If this information is not available online, the authors are encouraged to reach out to
853"
LICENSES FOR EXISTING ASSETS,0.9635145197319435,"the asset’s creators.
854"
NEW ASSETS,0.964259121370067,"13. New Assets
855"
NEW ASSETS,0.9650037230081906,"Question: Are new assets introduced in the paper well documented and is the documentation
856"
NEW ASSETS,0.9657483246463142,"provided alongside the assets?
857"
NEW ASSETS,0.9664929262844378,"Answer: [NA]
858"
NEW ASSETS,0.9672375279225615,"Justification: This paper does not release new assets.
859"
NEW ASSETS,0.9679821295606851,"Guidelines:
860"
NEW ASSETS,0.9687267311988086,"• The answer NA means that the paper does not release new assets.
861"
NEW ASSETS,0.9694713328369322,"• Researchers should communicate the details of the dataset/code/model as part of their
862"
NEW ASSETS,0.9702159344750558,"submissions via structured templates. This includes details about training, license,
863"
NEW ASSETS,0.9709605361131795,"limitations, etc.
864"
NEW ASSETS,0.9717051377513031,"• The paper should discuss whether and how consent was obtained from people whose
865"
NEW ASSETS,0.9724497393894267,"asset is used.
866"
NEW ASSETS,0.9731943410275503,"• At submission time, remember to anonymize your assets (if applicable). You can either
867"
NEW ASSETS,0.9739389426656738,"create an anonymized URL or include an anonymized zip file.
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9746835443037974,"14. Crowdsourcing and Research with Human Subjects
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9754281459419211,"Question: For crowdsourcing experiments and research with human subjects, does the paper
870"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9761727475800447,"include the full text of instructions given to participants and screenshots, if applicable, as
871"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9769173492181683,"well as details about compensation (if any)?
872"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9776619508562919,"Answer: [NA]
873"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9784065524944154,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
874"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9791511541325391,"Guidelines:
875"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9798957557706627,"• The answer NA means that the paper does not involve crowdsourcing nor research with
876"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9806403574087863,"human subjects.
877"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9813849590469099,"• Including this information in the supplemental material is fine, but if the main contribu-
878"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9821295606850335,"tion of the paper involves human subjects, then as much detail as possible should be
879"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9828741623231572,"included in the main paper.
880"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9836187639612807,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
881"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9843633655994043,"or other labor should be paid at least the minimum wage in the country of the data
882"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9851079672375279,"collector.
883"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9858525688756515,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
884"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9865971705137752,"Subjects
885"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9873417721518988,"Question: Does the paper describe potential risks incurred by study participants, whether
886"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880863737900223,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
887"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9888309754281459,"approvals (or an equivalent approval/review based on the requirements of your country or
888"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9895755770662695,"institution) were obtained?
889"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9903201787043932,"Answer: [NA]
890"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910647803425168,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
891"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9918093819806404,"Guidelines:
892"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.992553983618764,"• The answer NA means that the paper does not involve crowdsourcing nor research with
893"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9932985852568875,"human subjects.
894"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9940431868950111,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
895"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947877885331348,"may be required for any human subjects research. If you obtained IRB approval, you
896"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9955323901712584,"should clearly state this in the paper.
897"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996276991809382,"• We recognize that the procedures for this may vary significantly between institutions
898"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970215934475056,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
899"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9977661950856291,"guidelines for their institution.
900"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9985107967237528,"• For initial submissions, do not include any information that would break anonymity (if
901"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9992553983618764,"applicable), such as the institution conducting the review.
902"
