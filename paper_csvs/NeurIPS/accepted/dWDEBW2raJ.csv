Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.003968253968253968,"Despite their prevalence in deep-learning communities, over-parameterized models
convey high demands of computational costs for proper training. This work studies
the fine-grained, modular-level learning dynamics of over-parameterized models
to attain a more efficient and fruitful training strategy. Empirical evidence reveals
that when scaling down into network modules, such as heads in self-attention
models, we can observe varying learning patterns implicitly associated with each
module’s trainability. To describe such modular-level learning capabilities, we
introduce a novel concept dubbed modular neural tangent kernel (mNTK), and
we demonstrate that the quality of a module’s learning is tightly associated with
its mNTK’s principal eigenvalue λmax. A large λmax indicates that the module
learns features with better convergence, while those miniature ones may impact
generalization negatively. Inspired by the discovery, we propose a novel training
strategy termed Modular Adaptive Training (MAT) to update those modules with
their λmax exceeding a dynamic threshold selectively, concentrating the model
on learning common features and ignoring those superfluous ones. Unlike most
existing training schemes with a complete BP cycle across all network modules,
MAT can significantly save computations by its partially-updating strategy and
can further improve performance. Experiments show that MAT nearly halves the
computational cost of model training and outperforms the accuracy of baselines."
INTRODUCTION,0.007936507936507936,"1
Introduction"
INTRODUCTION,0.011904761904761904,"The proliferation of large-scale pre-trained models (Devlin et al., 2018; Brown et al., 2020; Doso-
vitskiy et al., 2020) demonstrates the advantages of over-parameterized models. Those models
can adequately fit the training data with their sizeable parameters far exceeding the data scale (Du
et al., 2018), and such over-parameterizations are known to be essential in model optimization and
generalization and are also crucial to their success in deep learning (Liu et al., 2022). However,
challenges remain. Over-parameterized models are expensive to train; for example, training large
language models (LLMs) such as GPT-3 (Brown et al., 2020; Zaheer et al., 2020) may take weeks to
months, even on a large collection of powerful GPUs."
INTRODUCTION,0.015873015873015872,∗Corresponding authors.
INTRODUCTION,0.01984126984126984,"Figure 1: Characteristics of training BERT on WikiText-2. Figure (a) demonstrates the joint variation
of effective rank and λmax across the attention heads. Headl
i refers to the ith attention head in the
lth layer. Figure (b-left) illustrates the idea of MAT, which governs the heads training by a dynamic
threshold. Using MAT speeds up convergence and achieves lower validation loss (b-right)."
INTRODUCTION,0.023809523809523808,"Generally, most over-parameterized models consist of various functioning modules connected layer-
wisely, e.g., heads in self-attention models like Transformer (Vaswani et al., 2017), experts in
mixture-of-experts (MoE) blocks (Shazeer et al., 2017b), or filters in convolutional neural networks
(CNN). This module-based architecture inevitably leads to a question: Has the computational
resource been spent efficiently and effectively across modules during model optimization? To this
end, we dive into the modular-level training behaviors of over-parameterized models and pursue a
more efficient and fruitful training strategy. We propose modular Neural Tangent Kernel (mNTK),
which is derived from the vanilla NTK (Jacot et al., 2018; Fort et al., 2020), as a powerful tool for
understanding the fine-grained learning dynamics of each network module. We compute mNTK as the
first-order approximation of a network module’s evolution during training, and it reflects the gradient
correlation among the training data module-wisely. The eigenspectrum of mNTK describes the
learning dynamics of a module, and its principal eigenvalue λmax indicates the degree of consistency
in the gradient direction for learning the most common features in data."
INTRODUCTION,0.027777777777777776,"To illustrate the module-level training dynamics intuitively, we employ the standard BERT (Devlin
et al., 2018) as an example and consider each attention head as a specific module. In addition to λmax,
we adopt the effective rank (Roy & Vetterli, 2007) to measure the effective dimensionality of a feature
matrix, like an attention matrix, and those metrics are computed at the end of each training epoch.
Figure 1(a) presents the trend of training dynamics of BERT, and we can observe asynchronous
learning behaviors across modules and training iterations. We explain those observations using λmax
as an indicator of trainability to measure how effectively gradient descent can optimize network
parameters. As discussed in Bowman & Montufar (2022), the network is biased to learn the top
eigenfunctions of the NTK over the entire input space, while the parameters with very low eigenvalues
barely update. In Figure 1(a), the attention head in blue marker has large λmax, meaning that it
has large consistency over data samples on feature learning and is thus apt to converge with better
trainability. Indeed, these heads contain richer information as reflected through effective rank. In
contrast, λmax of the red attention head oscillates at a low level, likely due to learning diverse features
or even noise that may be hard to converge, and thus updating it brings little benefit to training loss."
INTRODUCTION,0.031746031746031744,"Our analysis (Section 2) on trainability and generalization demonstrates that over-parameterized
models exhibit modularly and temporally varying asynchronous learning behaviors. Modules with
large mNTK principal eigenvalues learn features that are more rapidly learned, and those miniature
ones with limited impact on training loss negatively influence generalization. Such asynchronous
modularly and temporally training dynamics result in inefficient resource utilization by existing
training schemes. Motivated by these analyses, we propose a simple yet effective training method
termed Modular Adaptive Training (MAT), aiming at dynamically training partial modules to improve
training efficiency. As shown in Figure 1(b), we design a dynamic threshold (the dash-dotted line),
according to modular difference and temporal variation of mNTK λmax. MAT only back-propagates
the parameters of those modules whose λmax is larger than the threshold. This forces the network to"
INTRODUCTION,0.03571428571428571,"concentrate on learning the common features and ignore the inconsistent ones, preventing it from
fitting superfluous features or noises. This work makes the following contributions."
INTRODUCTION,0.03968253968253968,"• We empirically and theoretically reveal the associations between modular-level training
dynamics and the proposed modular Neural Tangent Kernel (mNTK)."
INTRODUCTION,0.04365079365079365,"• We propose Modular Adaptive Training (MAT) with a dynamic threshold to selectively
update modules during back-propagation to improve learning efficiency."
INTRODUCTION,0.047619047619047616,"• Experimental results verify that MAT can significantly reduce the training computation,
improve performance, and generalize well to different over-parameterized models."
ANALYSIS OF MODULAR NEURAL TANGENT KERNEL IN OVER-PARAMETERIZED MODELS,0.051587301587301584,"2
Analysis of Modular Neural Tangent Kernel in Over-Parameterized Models"
ANALYSIS OF MODULAR NEURAL TANGENT KERNEL IN OVER-PARAMETERIZED MODELS,0.05555555555555555,"In this section, we first introduce the definition of Neural Tangent Kernel (NTK) and modular
Neural Tangent Kernel (mNTK). Next, we present empirical analysis revealing the eigenspectrum
characteristics of the mNTK, including the imbalanced distribution of mNTK eigenvalues and the
modular and temporal diversity of mNTK eigenvalues during training. Finally, we provide theoretical
analysis to show that trainability of over-parameterized structured models is closely related to
the largest mNTK eigenvalues; on the other hand, learning features associated with the miniature
eigenvalues have negative impact on generalization."
ANALYSIS OF MODULAR NEURAL TANGENT KERNEL IN OVER-PARAMETERIZED MODELS,0.05952380952380952,"2.1
Modular Neural Tangent Kernel (mNTK)"
ANALYSIS OF MODULAR NEURAL TANGENT KERNEL IN OVER-PARAMETERIZED MODELS,0.06349206349206349,"Let X denote the set of n training instance, which are i.i.d. drawn from an unknown distribution D,
and let Y ∈Rnk denotes the targets. We study the network function f parameterized by θ aiming to
map the input vector X to output vector Z, termed as Z = f(X; θ) where θ ∈Rm and Z ∈Rnk.
Following the original definition of Neural Tangent Kernel (NTK) (Jacot et al., 2018; Wang et al.,
2020), we introduce modular NTK for fine-grained analysis of structured deep networks as below."
ANALYSIS OF MODULAR NEURAL TANGENT KERNEL IN OVER-PARAMETERIZED MODELS,0.06746031746031746,"Definition 1 (Modular Neural Tangent Kernel (mNTK)). Suppose model f contains L disjoint mod-
ules θ = {θ1, θ2, . . . , θL} where θl denote the parameters of lth module. We define mNTK as a
matrix by Θl(X, X) = Jθl(X)Jθl(X)⊤, where Jθl = ∇θlf(X; θl) denotes the Jacobian of the
function f at the points X with respect to the lth module’s parameters θl."
ANALYSIS OF MODULAR NEURAL TANGENT KERNEL IN OVER-PARAMETERIZED MODELS,0.07142857142857142,"Θl is a positive semi-definite real symmetric matrix and can be eigen-decomposed as Θl =
UlΛlUl⊤= Pnk
i=1 λl
iul
iul
i
⊤with nk non-negative eigenvalues λ(Θl) = {λl
1, λl
2, ..., λl
nk}. Note
that a network can be partitioned into modules in flexible ways, for example, by dividing layers
within a deep network or components of a network with inherent structures (e.g., attention heads in
Transformer or convolutional filters in a CNN). Since each module has its own mNTK, the integral
NTK of a model can be computed as the sum of mNTKs of each module (Yang & Littwin, 2021):"
ANALYSIS OF MODULAR NEURAL TANGENT KERNEL IN OVER-PARAMETERIZED MODELS,0.07539682539682539,"Θ(X, X)
(i)
= m
X"
ANALYSIS OF MODULAR NEURAL TANGENT KERNEL IN OVER-PARAMETERIZED MODELS,0.07936507936507936,"p=1
Jθp(X)Jθp(X)⊤(ii)
= L
X l=1 X"
ANALYSIS OF MODULAR NEURAL TANGENT KERNEL IN OVER-PARAMETERIZED MODELS,0.08333333333333333,"θp∈θl
Jθl(X)Jθl(X)⊤(iii)
= L
X"
ANALYSIS OF MODULAR NEURAL TANGENT KERNEL IN OVER-PARAMETERIZED MODELS,0.0873015873015873,"l=1
Θl(X, X),
(1)"
ANALYSIS OF MODULAR NEURAL TANGENT KERNEL IN OVER-PARAMETERIZED MODELS,0.09126984126984126,"where (i) decomposes the matrix multiplication into the sum of vector multiplication; (ii) gathers
addends by each module; (iii) follows the definition of mNTK."
EMPIRICAL ANALYSIS,0.09523809523809523,"2.2
Empirical Analysis"
EMPIRICAL ANALYSIS,0.0992063492063492,"In this subsection, we apply BERT to WikiText-2 for language modeling and regard each layer as a
module to analyze the properties of eigenvalue distribution of all mNTKs, as well as the modular and
temporal variation of eigenvalues during the training process."
EMPIRICAL ANALYSIS,0.10317460317460317,"The principal eigenvalue of mNTK dominates the eigenspectrum of mNTK. Figure 2(a) shows
the first 16 eigenvalues of all layer-wise mNTKs at the 10th epoch. Apparently, the eigenvalue
distribution of each layer-wise mNTK is imbalanced where there exists a single large eigenvalue
λl
1 (λ1 will be denoted by λmax hereafter) and a large number of small eigenvalues. In particular,
the largest eigenvalue often has 2 to 4 orders of magnitude larger than that of the rest eigenvalues.
These findings indicate that the principal eigenvalue of mNTK dominates the spectrum of the mNTK."
EMPIRICAL ANALYSIS,0.10714285714285714,"Figure 2: Training dynamics characterized by layer-wise mNTK of BERT trained on WikiText-2. (a):
The first 16 eigenvalues distribution of layer-wise mNTKs at the 10th epoch. (b): Variation of λmax
of layer-wise mNTKs. (c): Variation of κ of layer-wise mNTKs during the first 20 epochs."
EMPIRICAL ANALYSIS,0.1111111111111111,"The phenomenon is also indicated by Xiao et al. (2020) in terms of NTK. Furthermore, according
to the connection between NTK and mNTK (Eq. 1), the spectrum of NTK is dominated by the
principal eigenvalues of all mNTKs. Below, we utilize λmax for fine-grained optimization analysis of
over-parameterized model."
EMPIRICAL ANALYSIS,0.11507936507936507,"The variation of principal eigenvalues of mNTKs is highly asynchronous across modules.
Figure 2(b) shows the variation of λmax of all layer-wise mNTKs during the training process. It is
worth noting that the principal eigenvalues of mNTKs exhibit different trends during the training
process. Intuitively speaking, NTK (also mNTK) captures the training dynamics throughout the
training process, and its principal eigenvector corresponds to the direction of the steepest descent
in loss. Specifically, for mNTKs of shallow layers, the magnitude of λmax is large after model
initialization, and it decreases and converges fast in the early training stages. A potential explanation
is that shallow layers learn simple and common features of data samples (Yosinski et al., 2014; Zhang
et al., 2019), which contribute significantly to model convergence in the early training stages. In
contrast, for mNTK of the deep layer, the magnitude of λmax is small after initialization and gradually
increases during training. The reason is that deep layers learn complex and unique features, relying
on the stability of simple features in shallow layers, which occurs in the later training stages."
EMPIRICAL ANALYSIS,0.11904761904761904,"To analyze the training dynamics, condition number is also commonly used (Xu et al., 2021; Xiao
et al., 2020), defined as κ = λmax/λmin. Figure 2(c) shows the variation of κ for different modules
during training. We observe that the layer-wise condition number exhibits asynchronization among
different modules only in the early training stages (especially the first 10 epochs) and κ of all modules
quickly converges to the same value. This reveals that the condition number is hard to provide
sustained information for fine-grained optimization analysis, unlike the principal eigenvalue which is
capable of capturing the effective information throughout the training process."
EMPIRICAL ANALYSIS,0.12301587301587301,"The temporal variation of the principal eigenvalue of mNTK indicates the generalization ability.
In addition to the above analysis depicting the asynchronization between different modules, we
examine the temporal variation of the principal eigenvalue given a specific module and establish its
connection with the generalization ability. In detail, we train the same model using masked modeling
on half sequence length, which makes the model prone to overfitting. As shown in Figure 3(a), the
validation loss starts to increase from the 45th epoch while the training loss continually decreases.
Figure 3(b-e) present the temporal variation of the principal eigenvalue for the 3rd, 6th, 9th, and 12th
layers of the model during the training process, respectively. During the first 45th epochs, i.e., the
non-overfitting stage, the principal eigenvalues of these layers continually decrease. However, after
the 45th epoch when the model enters the overfitting stage, the principal eigenvalues converge to
excessively low values. A possible reason for overfitting is that the optimization direction of the
model starts to align with the direction of small eigenvalues, which means that the model is learning
superfluous features that are too specific or even noisy."
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.12698412698412698,"2.3
Analysis of Trainability and Generalization"
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.13095238095238096,"Using the tool of mNTK, we show that the optimization properties of over-parameterized structured
models are closely related to eigenvalues of mNTKs."
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.1349206349206349,"Suppose for any module θl, λmin(Θl) = λl
nk > 0. Let L denote the loss function and ∇θL(θ) the
gradient w.r.t parameter θ. For a step of gradient descent, loss reduction can be characterized by the"
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.1388888888888889,Figure 3: Training dynamics in the overfitting Case of 4-layer BERT trained by 64 token MLM task.
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.14285714285714285,"directional derivative of the loss function (Wang et al., 2020):"
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.14682539682539683,"∆L
(i)
= lim
ϵ→0
L(θ + ϵ∇θL(θ)) −L(θ) ϵ"
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.15079365079365079,"(ii)
≈∇θL(θ)⊤∇θL(θ)"
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.15476190476190477,"(iii)
= ∇ZL(θ)⊤(∇θf(θ)⊤∇θf(θ))∇ZL(θ)"
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.15873015873015872,"(iv)
= ∇ZL(θ)⊤( L
X"
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.1626984126984127,"l=1
Θl)∇ZL(θ)
(v)
= L
X l=1 nk
X"
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.16666666666666666,"i=1
λl
i

ul
i
⊤Y
2
, (2)"
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.17063492063492064,"where (i) follows the definition of directional derivative; (ii) follows the first-order Taylor expansion;
(iii) follows chain rule of derivative; (iv) follows Eq. 1; and (v) follows the eigen-decomposition of
mNTK under the assumption of squared error loss (Arora et al., 2019). Following the work of Arora
et al. (2019), we assume that true labels align well with top eigenvectors, i.e., (ul
i
⊤Y)2 is large for
large λl
i. Thus, the directional derivative of the loss function can be regarded as closely related to the
eigenspectrum of mNTKs."
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.1746031746031746,(1) The relation between trainability and principal eigenvalues of mNTKs.
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.17857142857142858,"Based on the first observation of empirical analysis, i.e., the spectrum of each mNTK is dominated by
the principal eigenvalues, we utilize λmax of mNTKs as the nearly equivalent proxy of the spectrum
of mNTKs. Therefore, Eq. 2 is simplified as: ∆L ≈ L
X"
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.18253968253968253,"l=1
λl
max

ul
1
⊤Y
2
.
(3)"
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.1865079365079365,"The above equation suggests that the loss decreases faster along the eigenspaces that correspond to
larger eigenvalues. Given the fact that the principal eigenvalues of mNTKs are highly asynchronous
across modules, we suggest selectively training the modules that are active with larger λl
max to
achieve efficient learning with limited computational resources."
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.19047619047619047,(2) The relation between generalization ability and eigenvalues of mNTKs.
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.19444444444444445,"Figure 4: The normalized eigen-spectrum distribu-
tion exhibits two distinct regions, termed informa-
tion space and nuisance space."
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.1984126984126984,"Figure 4 shows the eigen-spectrum distribution
of a 4-layer BERT model at the 10th epoch,
by regarding each head as a module and cal-
culating 256 eigenvalues for each head. Among
all 4×12×256 eigenvalues, the distribution of
eigen-spectrum exhibits two distinct regions: the
left region with a large number of small eigen-
values and the right region with a small number
of large eigenvalues. The two regions are widely
separated. According to Eq. 3, the eigenspaces
corresponding to miniature eigenvalues in the
left region have negligible impact on training
loss minimization, while those in the right re-
gion make dominant contributions. Following
the definition of Oymak et al. (2019), we refer
to the right region as the information space where the training error decreases rapidly, and the left
region as the nuisance space where training is slow and may cause overfitting."
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.20238095238095238,"Model training with more parameter updates tends to have longer distance to the initializations
θ0 (Hoffer et al., 2017). Based on Lemma 5.4 in Arora et al. (2019), longer distance results in
relatively larger Rademacher complexity, as R ∝∥θ −θ0∥F , and larger R results in relatively worse
generalization. As a consequence, we suggest selectively stopping the training in some modules so as
to stop the increase of the distance to the initializations in these directions. Specifically, we propose
to stop training the modules with principal eigenvalues falling into the nuisance space so as to enjoy
better generalization and efficient learning simultaneously."
ANALYSIS OF TRAINABILITY AND GENERALIZATION,0.20634920634920634,"Let’s consider the inter-module training dynamics on model generalization. During the training
process, given modules with variant principal eigenvalues, it often occurs that even though a small
number of modules still lie in the information space, the most of others already fall into the nuisance
space. The modules located in the information space, i.e., those with principal eigenvalues exceeding
the threshold, are trained to learn informative features that facilitate loss minimization. Conversely,
the modules falling into the nuisance space, i.e., those with principal eigenvalues below the threshold,
are prone to learn superfluous or even noise features, which significantly increase the Rademacher
complexity, and deteriorate their generalization ability. This analysis indicates that the existing
training scheme overlooks the asynchronization among modules, resulting in inefficient utilization of
the computational resource for loss minimization during model training."
MODULAR ADAPTIVE TRAINING,0.21031746031746032,"3
Modular Adaptive Training"
MODULAR ADAPTIVE TRAINING,0.21428571428571427,"Based on the analysis in the previous section, we advocate dynamically optimizing partial module
parameters in back propagation, termed as Modular Adaptive Training (MAT). The forward propa-
gation is computed as usual; during back propagation, MAT quantifies module parameters in terms
of modular policy and temporal policy such that only subsets of modules are updated. Below, we
present the proposed modular policy and temporal policy. Algorithm 1 summarizes the proposed
MAT algorithm."
MODULAR ADAPTIVE TRAINING,0.21825396825396826,"Modular Policy. Following the analysis that trainability and generalization of an over-parameterized
model are related to the principal eigenvalues of module parameters, we propose to only update
partial module parameters with λmax greater than a certain threshold during the model training; this
policy is termed as modular policy."
MODULAR ADAPTIVE TRAINING,0.2222222222222222,"Specifically, suppose the model parameters consist of L module parameters and each mNTK has nk
positive eigenvalues, and let λl
i denote ith eigenvalue of the mNTK computed over the parameters
θl. We define eΛ = {λl
i}nk,L
i,l=1 as the set of eigenvalues calculated over all mNTKs, and eλmax, eλmin
as the maximum and minimum value of the set eΛ, respectively. To select the appropriate modules,
we introduce a hyperparameter ratio α ∈(0, 1), thus the corresponding eigenvalue threshold λα is
computed as:"
MODULAR ADAPTIVE TRAINING,0.2261904761904762,"λα = eλmin + (eλmax −eλmin) × α.
(4)"
MODULAR ADAPTIVE TRAINING,0.23015873015873015,"According to the eigenvalue threshold λα, all modules are split into information modules and nuisance
modules as:
It = {θl|λmax(Θl
t) ≥λα}, Nt = {θl|λmax(Θl
t) < λα}.
(5)"
MODULAR ADAPTIVE TRAINING,0.23412698412698413,"As a result, the computation cost of back propagation is reduced."
MODULAR ADAPTIVE TRAINING,0.23809523809523808,"We empirically observe that α ∈(0, 1) is relatively stable over the training process for a certain model
and dataset. Therefore, α can be empirically assigned at the early stage of training after warming up,
or automatically learned by a meta network."
MODULAR ADAPTIVE TRAINING,0.24206349206349206,"Temporal Policy. The previous empirical analysis shows that during training, after the eigenvalues
of a module’s mNTK enter the nuisance region, further training this module can lead to poor
generalization. Therefore, to avoid overfitting, we terminate the training of this module according to
the temporal variation of the principal eigenvalue; we call this the temporal policy."
MODULAR ADAPTIVE TRAINING,0.24603174603174602,"For each module θl, its principal eigenvalue after warming up is recorded as the initial value
λmax(Θl
0). During each training episode t, the temporal variation of principal eigenvalue is defined
as ∆l
t = |λmax(Θl
t) −λmax(Θl
0)|. The information modules will be early stopped if the difference
in the temporal variation between two subsequent episodes relative to the initial variation is smaller"
MODULAR ADAPTIVE TRAINING,0.25,"than a pre-defined threshold β:
|∆l
t −∆l
t−1|
∆l
1
< β.
(6)"
MODULAR ADAPTIVE TRAINING,0.25396825396825395,β is empirically bounded by eigenvalue variation within the nuisance region.
RELATED WORK,0.25793650793650796,"4
Related Work"
RELATED WORK,0.2619047619047619,"Algorithm 1 Modular Adaptive Training
Require: training data {xi, yi}n
i=1, sample number S, modules
set M, modular threshold α, temporal threshold β.
1: I, N = M, ∅
2: while not converged and |I| > 0 do
3:
Compute ∇θf(x; θ) for samples {xi, yi}S
i=1
4:
eΛ = ∅
5:
for θl in M do
6:
Θl = ∇θlf(x; θ)∇θlf(x; θ)⊤"
RELATED WORK,0.26587301587301587,"7:
Compute eigen-decomposition Θl = UlΛlUl⊤"
RELATED WORK,0.2698412698412698,"8:
∆l
t = |λmax(Θl) −λmax(Θl
0)|
9:
eΛ = eΛ ∪Λl"
RELATED WORK,0.27380952380952384,"10:
end for
11:
λα = min(eΛ) + (max(eΛ) −min(eΛ)) × α
12:
N = {θl|λmax(Θl
t) < λα} (Modular policy Eq. 4)"
RELATED WORK,0.2777777777777778,"13:
N = N ∪{θl|
|∆l
t−∆l
t−1|
∆l
1
< β} (Temporal policy Eq. 6)"
RELATED WORK,0.28174603174603174,"14:
I = M −N
15:
for each batch x, y in {xi, yi}n
i=1 do
16:
for θl in I do
17:
θl = θl −η∇θlL(f(x; θ), y)
18:
end for
19:
end for
20: end while"
RELATED WORK,0.2857142857142857,"Optimization Analysis using
NTK.
Neural
Tangent
Ker-
nel (NTK) (Jacot et al., 2018),
which
calculates
the
Gram
matrix of Jacobian, is known
as a powerful tool to analyze
convergence and generalization
properties (Arora et al., 2019).
Modern neural networks are gen-
erally over-parameterized with
positive definite NTKs, meaning
that theoretically the gradient
flow always converges to the
equilibrium where the training
loss is zero (Chizat & Bach,
2018; Bombari et al., 2022). A
line of works study the training
dynamics of over-parameterized
models (Li et al., 2020; Liu
et al., 2022) based on NTK,
mainly under the assumption
of two-layer infinite-width net-
works. Many papers (Xiao et al.,
2020) study the spectrum of the
NTK, and find in particular the
largest
eigenvalue
dominates
the training regime (Jacot et al.,
2018; Bowman & Montufar, 2022). Empirical NTK, developed by many practical tools (Novak
et al., 2022; Engel et al., 2022), succeeds in deep learning applications such as neural architecture
search (Xu et al., 2021; Chen et al., 2021) and network pruning (Chen et al., 2023; Wang et al., 2023).
Our work takes a deeper look into the modules of an over-parameterized model, demonstrating
module-level training asynchronization of the components by the spatio-temporal distribution of
the modular NTK. To the best of our knowledge, this is the first work that proposes to analyze the
modular NTK variation for optimization."
RELATED WORK,0.2896825396825397,"Adaptive Training Approaches. Multirate training (Vlaar & Leimkuhler, 2022) is an emerging
related technique that partitions neural network parameters into two parts, in which parameters in
the slow part are updated less frequently. Our work analyzes the fine-grained training dynamics of
modules and proposes a theoretical-inspired method to dynamically update parameters. Besides,
there is a wide range of methods that can save computational resources, such as network pruning (Lee
et al., 2018a; Rachwan et al., 2022) and dynamic sparse training (Liu et al., 2020; Jiang et al., 2022).
These works tend to disable parameters during both the forward and backward propagation processes,
while our work only sparsifies the gradients during the backward propagation process. Hence, these
techniques are orthogonal to our method, and the proposed mNTK can be employed for evaluating
the importance of parameters in pruning-based methods. We also integrate our method with pruning,
and the results are presented in Appendix."
EXPERIMENTS,0.29365079365079366,"5
Experiments"
EXPERIMENTS,0.2976190476190476,"This section presents experimental results on various model architectures. The models under con-
sideration are both over-parameterized and highly structured , including BERT (Devlin et al., 2018)"
EXPERIMENTS,0.30158730158730157,"with multi-head self-attention (MHSA), Switch-Transformer (Fedus et al., 2022) with both MHSA
and mixture-of-experts (MoE) (Shazeer et al., 2017b) and VGG (Simonyan & Zisserman, 2014) with
convolutional filters."
EXPERIMENTS,0.3055555555555556,"Evaluation Measure. We evaluate the effectiveness of our algorithm for model optimization in terms
of training efficiency and generalization. The training efficiency is measured by the validation or test
error of the models after a certain number of floating-point operations (FLOPs), which serves as a
lower bound for the execution time (Justus et al., 2018). The generalization performance is measured
by the converged test error. Additionally, we calculate the total number of FLOPs used by the models
until they achieve the converged test error."
EXPERIMENTS,0.30952380952380953,"Approximation of NTK. It is hard to directly compute the empirical NTK for an over-parameterized
model, due to the computational and memory cost when calculating the Jacobian matrix J ∈Rnk×m.
Therefore, two alternatives are widely used to approximate the empirical NTK: (1) sampling a subset
of training data instead of using the entire set; (2) computing the gradient by sum-of-logits instead of
all output units, as shown in (Mohamadi & Sutherland, 2022)."
EXPERIMENTS,0.3134920634920635,"Implementation Details. In practice, we evaluate the empirical NTK by sampling N training data (N
is dynamically adjusted based on the available GPU memory in the experimental setup). To expedite
the NTK calculation, we leverage parallelism by utilizing the implementation of Engel et al. (2022)2,
which enables us to collect different data samples across multiple GPUs. The FLOPs is computed
as the total sum of the forward and backward passes, where the backward FLOPs is approximately
estimated as twice the forward pass (Rasley et al., 2020), and we measure it using the DeepSpeed
Profiler3. All experiments are conducted on 8 × NVIDIA GeForce RTX 3090 GPUs. For further
experimental details, please refer to the Appendix."
MAIN RESULT,0.31746031746031744,"5.1
Main Result"
MAIN RESULT,0.32142857142857145,"Results of BERT. Firstly, we experiment on BERT (Devlin et al., 2018), which stacks 12 Transformer
layers with 12 attention heads in each layer. Following the basic setup of Liu et al. (2019), we train
BERT from scratch by masked language modeling (MLM) task on WikiText-2 (Merity et al., 2016).
We compare the proposed MAT method with directly training a BERT model (BERT), using multiple
learning rates (Vlaar & Leimkuhler, 2022) to train BERT (Multirate), and randomly selecting some
heads for training (BERT-Rand). In this experiment, MAT applies α = 0.1, β = 10−3."
MAIN RESULT,0.3253968253968254,"Table 1: Results of BERT on WikiText-2. FLOPs is measured per GPU without embedding. Compu-
tation refers to the FLOPs model used until achieving best test loss. Best results are in boldface."
MAIN RESULT,0.32936507936507936,"Method
Validation Loss
(Log PPL)"
MAIN RESULT,0.3333333333333333,"Test Loss
(Log PPL)
Computation
(PFLOPs)
@ 10 PFLOPs
@ 15 PFLOPs
@ 20 PFLOPs
@ Convergence
BERT
5.39 ± 0.15
4.75 ± 0.06
4.48 ± 0.03
4.41 ± 0.05
28.70
BERT-Rand
5.65 ± 0.18
5.11 ± 0.16
4.96 ± 0.11
4.82 ± 0.06
21.53
Multirate
4.93 ± 0.10
4.57 ± 0.03
4.52 ± 0.02
4.48 ± 0.03
19.46
MAT
4.46 ± 0.04
4.41 ± 0.02
4.35 ± 0.02
4.27 ± 0.03
16.50"
MAIN RESULT,0.3373015873015873,"Table 1 shows the performance of all methods used in training BERT on WikiText-2. MAT out-
performs all the baselines in training efficiency. In particular, MAT achieves the almost identical
validation loss of 4.46 using 10 PFLOPs (1 PFLOPs = 1015 FLOPs) of computation whereas the
vanilla BERT achieves 4.48 using 20 PFLOPs. Our method saves half of the computation by per-
forming a lighter back propagation pass only for appropriate subsets of the modules. Compared to
MAT, the performance degradation of BERT-Rand confirms the effectiveness of using λmax as the
criterion for selecting important modules. While Multirate shows the potential for efficient training,
it comes at the cost of sacrificing performance. In contrast, MAT achieves better test performance
with a reduction of about 15% in test perplexity, and also saves 41% of computational resources."
MAIN RESULT,0.3412698412698413,"Results of Switch-Transformer. To further evaluate the scalability of our method, we apply MAT
to the feed-forward network (FFN) of transformer layer. Switch-Transformer (Fedus et al., 2021)
is a representative implementation of Mixture-of-Expert (MoE) architecture (Shazeer et al., 2017a),"
MAIN RESULT,0.34523809523809523,"2https://github.com/pnnl/torchntk
3https://github.com/microsoft/DeepSpeed"
MAIN RESULT,0.3492063492063492,"which has shown excellent performance in NLP tasks recently. MoE (Shazeer et al., 2017b) architects
the FFN into a structured one, which replicates the FFN as experts and dynamically assigns tokens to
each expert. In this experiment, we compare the performance of Switch-Transformers using vanilla,
Multirate and Switch-Rand training methods on WikiText-103 (Merity et al., 2016). Specifically, MAT
regards both experts and attention heads as the modules with αh = 0.1, αe = 0.2, βh = βe = 10−3."
MAIN RESULT,0.3531746031746032,"Table 2: Results of Switch-Transformer on WikiText-103. FLOPs is measured per GPU without
embedding. Best results are in boldface."
MAIN RESULT,0.35714285714285715,"Method
Validation Loss
(Log PPL)"
MAIN RESULT,0.3611111111111111,"Test Loss
(Log PPL)
Computation
(PFLOPs)
@ 200 PFLOPs
@ 400 PFLOPs
@ 600 PFLOPs
@ Convergence
Switch
3.16 ± 0.12
2.31 ± 0.05
1.93 ± 0.03
1.74 ± 0.02
1155.2
Switch-Rand
2.92 ± 0.15
2.15 ± 0.08
2.01 ± 0.07
1.93 ± 0.05
837.5
Multirate
2.67 ± 0.09
2.02 ± 0.08
1.83 ± 0.05
1.77 ± 0.04
740.4
MAT
2.34 ± 0.06
1.92 ± 0.03
1.69 ± 0.02
1.68 ± 0.02
614.8"
MAIN RESULT,0.36507936507936506,"Table 2 shows that the performance of Switch-Transformer trained on a large dataset with a large
number of parameters. It demonstrates that MAT significantly reduces the computation required for
training. Switch-Rand and Multirate also improve training efficiency to some extent. Compared to
them, our method precisely identifies the appropriate heads and experts trained with features that are
easy to learn and generalize, reducing validation perplexity by 52.3%, 25.2%, and 18.1% (25.9%,
16.9% and 12.4% for log perplexity) within 200, 400, and 600 PFLOPs of computation, respectively.
Compared to the vanilla model, our method achieves 10.4% improved test perplexity. These results
demonstrate the effectiveness of MAT in larger models and datasets."
MAIN RESULT,0.36904761904761907,"Results of VGG. To further validate the versatility of MAT, we deploy it on computer vision tasks.
We take classic convolutional network VGG16 as an example, which is over-parameterized for the
CIFAR-10 dataset. MAT splits convolutional filters into two parts of modules by α = 0.2, β = 10−6.
Experimental results listed in Table 3 show that our method helps VGG16 converge faster (47.3%)
with better test accuracy compared with the vanilla model."
MAIN RESULT,0.373015873015873,Table 3: Results of VGG16 on CIFAR-10. Best results are in boldface.
MAIN RESULT,0.376984126984127,"Method
Computation until Train Acc = n
(PFLOPs)"
MAIN RESULT,0.38095238095238093,"Test Accuracy
(Top-1, %)
Computation
(PFLOPs)
@ n = 95%
@ n = 99%
@ Convergence
VGG16
8.85
12.06
93.77 ± 0.08
17.14
VGG16-Rand
9.96
13.35
92.71 ± 0.08
18.84
Multirate
7.19
9.75
93.43 ± 0.14
13.35
MAT
5.31
7.21
93.86 ± 0.05
9.03"
MODULE-LEVEL TRAINING ANALYSIS,0.38492063492063494,"5.2
Module-level Training Analysis"
MODULE-LEVEL TRAINING ANALYSIS,0.3888888888888889,"Figure 5: Histogram of epochs where the heads
are trained using back-propagation."
MODULE-LEVEL TRAINING ANALYSIS,0.39285714285714285,"This experiment analyzes the histogram of the
number of training epochs of all the attention
heads in BERT. As shown in Figure 5, more than
half of the heads were trained with backpropaga-
tion in only 20% of the epochs, and the average
training epoch per head was 34% of the entire
training process. This verifies that the sparse back-
propagation induced by MAT enhances training ef-
ficiency. Furthermore, approximately 20% of the
heads were never updated, indicating the sparse ac-
tivation of over-parameterized structured models.
This finding suggests the possibility of further re-
ducing the computation of stable modules during
the forward pass."
CONCLUSION,0.3968253968253968,"6
Conclusion"
CONCLUSION,0.4007936507936508,"We have analyzed the modular-level and temporal training characteristic of structured over-
parameterized models through a new measure of modular neural tangent kernel (mNTK). Empirical
and theoretical analysis demonstrates the relationship between optimization effectiveness and mNTK
principal eigenvalues. Based on this finding, we designed a novel training strategy termed Modular
Adaptive Training (MAT), which uses a modularly and temporally adaptive dynamic threshold to
select partial modules for gradient back propagation. Experimental results show that MAT reduces
training cost and increases test accuracy compared to existing training scheme. This work seeks to
improve training efficiency by sparsifying the gradient update. Besides pruning, this work can be
combined with other techniques to further improve training efficiency. We leave it for future work."
CONCLUSION,0.40476190476190477,Acknowledgements
CONCLUSION,0.4087301587301587,"This work was supported by National Natural Science Foundation of China under Grant No.
62090025, National Key R&D Program of China under Grant No. 2022YFB4400400 and China
Postdoctoral Science Foundation No. 2022M720767."
REFERENCES,0.4126984126984127,References
REFERENCES,0.4166666666666667,"Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322–332. PMLR, 2019."
REFERENCES,0.42063492063492064,"Simone Bombari, Mohammad Hossein Amani, and Marco Mondelli. Memorization and optimization
in deep neural networks with minimum over-parameterization. In Advances in Neural Information
Processing Systems, 2022."
REFERENCES,0.4246031746031746,"Benjamin Bowman and Guido F Montufar. Spectral bias outside the training set for deep networks in
the kernel regime. Advances in Neural Information Processing Systems, 35:30362–30377, 2022."
REFERENCES,0.42857142857142855,"Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020."
REFERENCES,0.43253968253968256,"Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture search on imagenet in
four gpu hours: A theoretically inspired perspective. In International Conference on Learning
Representations, 2021."
REFERENCES,0.4365079365079365,"Yixuan Chen, Yubin Shi, Mingzhi Dong, Xiaochen Yang, Dongsheng Li, Yujiang Wang, Robert Dick,
Qin Lv, Yingying Zhao, Fan Yang, et al. Over-parameterized model optimization with polyak-{\L}
ojasiewicz condition. In The Eleventh International Conference on Learning Representations,
2023."
REFERENCES,0.44047619047619047,"Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable program-
ming.(2018). arXiv preprint arXiv:1812.07956, 2018."
REFERENCES,0.4444444444444444,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.44841269841269843,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image
is worth 16x16 words: Transformers for image recognition at scale. In International Conference
on Learning Representations, 2020."
REFERENCES,0.4523809523809524,"Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2018."
REFERENCES,0.45634920634920634,"Andrew Engel, Zhichao Wang, Anand D Sarwate, Sutanay Choudhury, and Tony Chiang. Torch-
ntk: A library for calculation of neural tangent kernels of pytorch models.
arXiv preprint
arXiv:2205.12372, 2022."
REFERENCES,0.4603174603174603,"William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity, 2021."
REFERENCES,0.4642857142857143,"William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):
5232–5270, 2022."
REFERENCES,0.46825396825396826,"Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy,
and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape
geometry and the time evolution of the neural tangent kernel. Advances in Neural Information
Processing Systems, 33:5850–5861, 2020."
REFERENCES,0.4722222222222222,"Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635, 2018."
REFERENCES,0.47619047619047616,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026–1034, 2015."
REFERENCES,0.4801587301587302,"Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gap in large batch training of neural networks. Advances in neural information processing
systems, 30, 2017."
REFERENCES,0.48412698412698413,"Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. Advances in neural information processing systems, 31, 2018."
REFERENCES,0.4880952380952381,"Peng Jiang, Lihan Hu, and Shihui Song. Exposing and exploiting fine-grained block structures
for fast and accurate sparse training. Advances in Neural Information Processing Systems, 35:
38345–38357, 2022."
REFERENCES,0.49206349206349204,"Daniel Justus, John Brennan, Stephen Bonner, and Andrew Stephen McGough. Predicting the
computational cost of deep learning models. In 2018 IEEE international conference on big data
(Big Data), pp. 3873–3882. IEEE, 2018."
REFERENCES,0.49603174603174605,"Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. Snip: Single-shot network pruning based
on connection sensitivity. In International Conference on Learning Representations, 2018a."
REFERENCES,0.5,"Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning
based on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018b."
REFERENCES,0.503968253968254,"Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is
provably robust to label noise for overparameterized neural networks. In International conference
on artificial intelligence and statistics, pp. 4313–4324. PMLR, 2020."
REFERENCES,0.5079365079365079,"Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-parameterized
non-linear systems and neural networks. Applied and Computational Harmonic Analysis, 59:
85–116, 2022."
REFERENCES,0.5119047619047619,"Junjie Liu, Zhe Xu, Runbin Shi, Ray CC Cheung, and Hayden KH So. Dynamic sparse train-
ing: Find efficient sparse network from scratch with trainable masked layers. arXiv preprint
arXiv:2005.06870, 2020."
REFERENCES,0.5158730158730159,"Liyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun Zhou, Jing-Hao Xue, Xinjiang Wang, Yimin
Chen, Wenming Yang, Qingmin Liao, and Wayne Zhang. Group fisher pruning for practical
network compression. In International Conference on Machine Learning, pp. 7021–7032. PMLR,
2021."
REFERENCES,0.5198412698412699,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019."
REFERENCES,0.5238095238095238,"Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843, 2016."
REFERENCES,0.5277777777777778,"Mohamad Amin Mohamadi and Danica J Sutherland. A fast, well-founded approximation to the
empirical neural tangent kernel. arXiv preprint arXiv:2206.12543, 2022."
REFERENCES,0.5317460317460317,"Roman Novak, Jascha Sohl-Dickstein, and Samuel S Schoenholz. Fast finite width neural tangent
kernel. In International Conference on Machine Learning, pp. 17018–17044. PMLR, 2022."
REFERENCES,0.5357142857142857,"Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guaran-
tees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprint
arXiv:1906.05392, 2019."
REFERENCES,0.5396825396825397,"John Rachwan, Daniel Zügner, Bertrand Charpentier, Simon Geisler, Morgane Ayle, and Stephan
Günnemann. Winning the lottery ahead of time: Efficient early network pruning. In International
Conference on Machine Learning, pp. 18293–18309. PMLR, 2022."
REFERENCES,0.5436507936507936,"Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimiza-
tions enable training deep learning models with over 100 billion parameters. In Proceedings of
the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
3505–3506, 2020."
REFERENCES,0.5476190476190477,"Olivier Roy and Martin Vetterli. The effective rank: A measure of effective dimensionality. In 2007
15th European signal processing conference, pp. 606–610. IEEE, 2007."
REFERENCES,0.5515873015873016,"Victor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity by fine-tuning.
Advances in Neural Information Processing Systems, 33:20378–20389, 2020."
REFERENCES,0.5555555555555556,"Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and
Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv
preprint arXiv:1701.06538, 2017a."
REFERENCES,0.5595238095238095,"Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and
Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In
International Conference on Learning Representations, 2017b."
REFERENCES,0.5634920634920635,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.5674603174603174,"Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better:
On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962, 2019."
REFERENCES,0.5714285714285714,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing
systems, 30, 2017."
REFERENCES,0.5753968253968254,"Tiffany J Vlaar and Benedict Leimkuhler. Multirate training of neural networks. In International
Conference on Machine Learning, pp. 22342–22360. PMLR, 2022."
REFERENCES,0.5793650793650794,"Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by
preserving gradient flow. In International Conference on Learning Representations, 2020."
REFERENCES,0.5833333333333334,"Yite Wang, Dawei Li, and Ruoyu Sun. Ntk-sap: Improving neural network pruning by aligning
training dynamics. arXiv preprint arXiv:2304.02840, 2023."
REFERENCES,0.5873015873015873,"Lechao Xiao, Jeffrey Pennington, and Samuel Schoenholz. Disentangling trainability and generaliza-
tion in deep neural networks. In International Conference on Machine Learning, pp. 10462–10472.
PMLR, 2020."
REFERENCES,0.5912698412698413,"Jingjing Xu, Liang Zhao, Junyang Lin, Rundong Gao, Xu Sun, and Hongxia Yang. Knas: green
neural architecture search. In International Conference on Machine Learning, pp. 11613–11625.
PMLR, 2021."
REFERENCES,0.5952380952380952,"Greg Yang and Etai Littwin. Tensor programs iib: Architectural universality of neural tangent kernel
training dynamics. In International Conference on Machine Learning, pp. 11762–11772. PMLR,
2021."
REFERENCES,0.5992063492063492,"Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? Advances in neural information processing systems, 27, 2014."
REFERENCES,0.6031746031746031,"Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for
longer sequences. Advances in neural information processing systems, 33:17283–17297, 2020."
REFERENCES,0.6071428571428571,"Chiyuan Zhang, Samy Bengio, and Yoram Singer. Are all layers created equal? arXiv preprint
arXiv:1902.01996, 2019."
REFERENCES,0.6111111111111112,Appendix
REFERENCES,0.6150793650793651,"A
Trainability and Generalization"
REFERENCES,0.6190476190476191,"A.1
Trainability"
REFERENCES,0.623015873015873,"Following the previous work (Jacot et al., 2018) training neural networks in function space instead
of parameter space, we analyze the trainability of an over-parameterized model by investigating the
evolution of its predictions. Specifically, after a step of gradient descent"
REFERENCES,0.626984126984127,"θt+1 = θt −η∇θtL,
(7)"
REFERENCES,0.6309523809523809,the updated predictions can be approximated using the first-order Taylor expansion as
REFERENCES,0.6349206349206349,f(X; θt+1) = f(X; θt −η∇θtL)
REFERENCES,0.6388888888888888,≈f(X; θt) −η∇θtf(X; θt)⊤∇θtL
REFERENCES,0.6428571428571429,"= f(X; θt) −η∇θtf(X; θt)⊤∇θtf(X; θt)∇ZL
= f(X; θt) −ηΘt(X, X)∇ZL. (8)"
REFERENCES,0.6468253968253969,"Motivated by Arora et al. (2019), we rewrite the equation into the form of norm and eigenpairs:"
REFERENCES,0.6507936507936508,"∥f(X; θt+1) −f(X; θt)∥2
2 ≈∥ηΘt(X, X)∇ZL∥2
2
= η2(Θt(X, X)∇ZL)⊤(Θt(X, X)∇ZL)"
REFERENCES,0.6547619047619048,"= η2∇ZL⊤Θt(X, X)Θt(X, X)∇ZL"
REFERENCES,0.6587301587301587,"= η2∇ZL⊤( nk
X"
REFERENCES,0.6626984126984127,"i
λiuiui
⊤)( nk
X"
REFERENCES,0.6666666666666666,"i
λiuiui
⊤)∇ZL"
REFERENCES,0.6706349206349206,"(i)
= η2∇ZL⊤( nk
X"
REFERENCES,0.6746031746031746,"i
λ2
i uiui
⊤uiui
⊤)∇ZL"
REFERENCES,0.6785714285714286,"(ii)
= η2∇ZL⊤
nk
X"
REFERENCES,0.6825396825396826,"i
λ2
i uiui
⊤∇ZL"
REFERENCES,0.6865079365079365,"= η2
nk
X"
REFERENCES,0.6904761904761905,"i
λ2
i (ui
⊤∇ZL)⊤ui
⊤∇ZL"
REFERENCES,0.6944444444444444,"= η2
nk
X"
REFERENCES,0.6984126984126984,"i
λ2
i (ui
⊤∇ZL)2, (9)"
REFERENCES,0.7023809523809523,"where (i) follows ui⊤uj = 0, ∀i ̸= j and (ii) follows ui⊤ui = ∥ui∥2 = 1. Then we can derive
Eq. 9 into:"
REFERENCES,0.7063492063492064,∥f(X; θt+1) −f(X; θt)∥2 =
REFERENCES,0.7103174603174603,"v
u
u
t nk
X"
REFERENCES,0.7142857142857143,"i=1
(ηλiui⊤∇ZL)2.
(10)"
REFERENCES,0.7182539682539683,"As we can see, at every step of the gradient descent, the model learns the target function faster along
the eigen-directions corresponding to the larger eigenvalues."
REFERENCES,0.7222222222222222,"Further, for the loss function assumed by squared error loss, we characterize the loss reduction by the
following directional derivative (Wang et al., 2020):"
REFERENCES,0.7261904761904762,"∆L(θ) = lim
ϵ→0
L(θ + ϵ∇θL(θ)) −L(θ)"
REFERENCES,0.7301587301587301,"ϵ
,
(11)"
REFERENCES,0.7341269841269841,"using Taylor’s first-order approximation L(θ + ϵ∇θL(θ)) ≈L(θ) + ϵ∇θL(θ)⊤∇θL(θ),"
REFERENCES,0.7380952380952381,"∆L(θ) ≈∇θL(θ)⊤∇θL(θ),
(12)"
REFERENCES,0.7420634920634921,expanded by chain rule ∇θL(θ) = ∇θf(θ)∇ZL(θ) into
REFERENCES,0.746031746031746,∆L(θ) = (∇θf(θ)∇ZL(θ))⊤∇θf(θ)∇ZL(θ)
REFERENCES,0.75,"= ∇ZL(θ)⊤(∇θf(θ)⊤∇θf(θ))∇ZL(θ),
(13)"
REFERENCES,0.753968253968254,"then following Eq. 1 and matrix decomposition of mNTK,"
REFERENCES,0.7579365079365079,∆L(θ) = ∇ZL(θ)⊤(Θ)∇ZL(θ)
REFERENCES,0.7619047619047619,"= ∇ZL(θ)⊤( L
X"
REFERENCES,0.7658730158730159,"l=1
Θl)∇ZL(θ) = Y⊤( L
X l=1 nk
X"
REFERENCES,0.7698412698412699,"i=1
λl
iul
iul
i
⊤)Y = L
X l=1 nk
X"
REFERENCES,0.7738095238095238,"i=1
λl
i(ul
i
⊤Y)⊤(ul
i
⊤Y) = L
X l=1 nk
X"
REFERENCES,0.7777777777777778,"i=1
λl
i

ul
i
⊤Y
2
. (14)"
REFERENCES,0.7817460317460317,The directional derivative of the loss function is closely related to the eigenspectrum of mNTKs. As
REFERENCES,0.7857142857142857,"for the latter projection item

ul
i
⊤Y
2
, Arora et al. (2019) have studied the relationship between the
projection norm and labels, and they demonstrate that true labels generate better alignment with top"
REFERENCES,0.7896825396825397,"eigenvectors, especially for the maximum one. Therefore, we can assume that

ul
i
⊤Y
2
∝λl
i in a
real dataset."
REFERENCES,0.7936507936507936,"A.2
Generalization"
REFERENCES,0.7976190476190477,"We denote the set of n training instances as X = (x, . . . , xn) and the target set as Y = (y1, . . . , yn)⊤.
A two-layer neural network with the ReLU activation function can be written as"
REFERENCES,0.8015873015873016,"fθ,a(X) =
1
√m m
X"
REFERENCES,0.8055555555555556,"r=1
arσ

θr
⊤X

(15)"
REFERENCES,0.8095238095238095,"where θ1, θ2, ..., θm are the weight vectors in the first layer, a1, a2, ..., am are weights in the second
layer. For simplicity, we denote θ = (θ1, . . . , θm) and a = (a1, . . . , am)⊤. Following (Arora
et al., 2019), we assume the network is over-parameterized and is trained by gradient descent on
the quadratic loss over dataset (X, Y). We freeze the second layer a and optimize the first layer
θ through gradient descent. Let θ(0) and θ(t) denote the weights initialized from scratch and the
weights after t iterations, respectively."
REFERENCES,0.8134920634920635,"Under these settings, according to the Lemma 5.3 of (Arora et al., 2019), the embedding function
fθ,a learned from GD is in a restricted class of neural networks with weights close to initialization
θ(0). Therefore, the Rademacher complexity of the two-layer function can be bounded as follows:"
REFERENCES,0.8174603174603174,"Lemma 2 Given R > 0, with probability at least 1 −δ over the random initialization (θ(0), a),
simultaneously for every B > 0, the following function class"
REFERENCES,0.8214285714285714,"Fθ(0),a
R,B
= {fθ,a : ∥θr −θr(0)∥2 ≤R (∀r ∈[m]),"
REFERENCES,0.8253968253968254,"∥θ −θ(0)∥F ≤B}
(16)"
REFERENCES,0.8293650793650794,has empirical Rademacher complexity bounded as:
REFERENCES,0.8333333333333334,"RS

Fθ(0),a
R,B

= 1"
REFERENCES,0.8373015873015873,"nEε∈{±1}n  
sup"
REFERENCES,0.8412698412698413,"f∈Fθ(0),a
R,B n
X"
REFERENCES,0.8452380952380952,"i=1
εif (xi)   ≤B
√"
N,0.8492063492063492,2n 
N,0.8531746031746031,"1 +
2 log 2 δ
m 1/4!"
N,0.8571428571428571,"+ 2R2√m κ
+ R r"
N,0.8611111111111112,2 log 2 δ . (17)
N,0.8650793650793651,"Lemma 2 indicates that the Rademacher complexity is proportional to the weight distance from its
initialization. Specifically, a greater weight distance implies a more significant amount of Rademacher
complexity and is thus associated with weaker generalization ability. Following those intuitions, we
extend the connection between weight distance and Rademacher complexity, initially portrayed in a
two-layer network, into more generalized deep neural networks."
N,0.8690476190476191,"For deep models, as mentioned in (Hoffer et al., 2017), the weight distance from its initialization
grows logarithmically with the number of iterations, which can be described as"
N,0.873015873015873,"∥θ(t) −θ(0)∥F ∝log t.
(18)"
N,0.876984126984127,"Combining Lemma 2 and Eq. 18, we can discover that as training iterations increase, the model’s
Rademacher complexity also grows with its weights more deviated from initializations, which
subsequently impairs the model’s generalization ability. However, solutions remain. As trainability
varies across various modules, we can prevent the significant growth of Rademacher complexity
and thus retain a satisfying model generalization by ignoring the updates of those modules with
undesirable mNTK eigenvalues."
N,0.8809523809523809,"B
Experiments"
N,0.8849206349206349,"B.1
Experimental Settings"
N,0.8888888888888888,"BERT and Switch-Transformer. We generally follow the settings of Liu et al. (2019) to train BERT
and Switch-Transformer from scratch using self-supervised Masked Language Modeling (MLM). In
detail, we uniformly sample approximately 15% input tokens to replace them with a mask, and the
learning objective is to predict those masked tokens accurately with a cross-entropy loss."
N,0.8928571428571429,"All experiments are conducted on 8 × NVIDIA GeForce RTX 3090 GPUs, and we list those key
hyperparameters in Table 4."
N,0.8968253968253969,Table 4: Hyperparameters configuration in BERT and Switch-Transformer
N,0.9007936507936508,"Hyperparameter
BERT
Switch-Transformer
Number of layers
12
12
Attention heads
12
12
Hidden dimension size
768
768
Dropout
0.1
0.1
Attention dropout
0.1
0.1
Sequence length
512
512
Batch size
8
8
Warmup steps
0
12k
Max steps
12k
300k
Weight decay
0
0.01
Peak learning rate
2e-4
1e-4
Learning rate decay
Linear
Cosine
Adam [ϵ, β1, β2]
[1e-6, 0, 0]
[1e-6, 0.9, 0.99]
Number of experts
4
Capacity factor
1.5"
N,0.9047619047619048,"The baseline methods consist of: 1) the vanilla model, 2) the Multirate (Vlaar & Leimkuhler, 2022)
algorithm, and 3) the adaptive training with a random selection of the updated modules (abbreviated
as Rand)."
N,0.9087301587301587,"For BERT, Multirate randomly partitions the heads into fast and slow groups into a 50%/50% split,
with fast heads updated every step of stepsize η and slow ones updated every k = 5 step of stepsize
kη."
N,0.9126984126984127,"BERT-Rand randomly selects and updates 50% heads in every epoch. MAT first randomly samples
64 data samples to approximate 64 eigenvalues for each head-based mNTK. The modular policy and
temporal policy are set with α = 0.1, β = 10−3. The adaptive training algorithm starts from the 5th
epoch and performs every 8 epochs."
N,0.9166666666666666,"For Switch-Transformer, Multirate considers each head/expert as a single module and partitions
heads/experts into fast and slow sets at 50%/50% ratios. The fast heads and experts are updated
every step with current stepsize η, and the slow ones are updated every k = 5 steps with stepsize kη.
Switch-Rand randomly selects 50% heads or experts for updating at the beginning of each epoch.
MAT first randomly samples 64 data samples and separately approximate 64 eigenvalues for each
head-based mNTK and expert-based mNTK. The modular policy and temporal policy are set with
αh = 0.1, βh = 10−3 and αe = 0.2, βe = 10−3 for head and expert, respectively. The adaptive
training algorithm performs every 2 epochs after warming up."
N,0.9206349206349206,"To avoid the case that no heads or experts are updated in any layer, we utilize a protecting strategy
that maintains the gradient of at least one head or expert in each layer for all baselines and our model."
N,0.9246031746031746,"VGG. All baselines of VGG are initialized with Kaiming initialization (He et al., 2015) and are
trained with SGD for 200 epochs with an initial learning rate of 0.1 and batch size of 128. We
decay the learning rate by 0.1 at 1/2 and 3/4 of the total number of epochs. Specifically, Multirate
randomly partitions the filters into fast and slow parts by 50%/50% in each layer. VGG-Rand
randomly selects 50% filters to update filters in each epoch. MAT approximates eigenvalues for each
filter-based mNTK using random 64 data samples. The modular policy and temporal policy are set
with α = 0.2, β = 10−6. All experiments are repeated by three runs and the final computation costs
are calculated on average."
N,0.9285714285714286,"B.2
MAT and Network Pruning"
N,0.9325396825396826,"Network pruning (Frankle & Carbin, 2018; Sanh et al., 2020; Liu et al., 2021) applies various criteria
to determine the importance of different components and prunes those that are most redundant
to compress model size, which often results in slight performance drops. The proposed MAT
distinguishes from network pruning in that MAT only sparsifies modules during backward propagation,
while pruning methods eliminate the forward and backward propagations of pruned modules. Besides,
MAT is the first work to employ the principal eigenvalue of mNTK as the module selection criterion."
N,0.9365079365079365,"Our empirical study reveals some modules that are never or rarely selected by the proposed adaptive
training method (MAT), showing potential for being pruned to achieve further computation savings.
However, due to the complete removal of modules, existing network pruning methods may negatively
impact the model’s performance and generalizations. Based on those observations, we have explored
whether we can apply MAT to network pruning methods to accelerate the training process or to
improve performance."
N,0.9404761904761905,"We employ a BERT model for this experiment, and we prune 50% attention heads according to the
ranking of λmax of head-based mNTKs at the 15th epoch. Across the training session, we use MAT
with α = 0.2 to introduce sparsity in the backward pass, resulting in approximately 25% sparsity of
weight gradients."
N,0.9444444444444444,Table 5: Results of BERT on WikiText-2 by pruning methods.
N,0.9484126984126984,"Method
Sparsity (Pruning Ratio)
Test Loss (Log PPL)
Computation (PFLOPs)
Forward Pass
Backward Pass
@ Convergence
Vanilla
0%
0%
4.41
27.80
SNIP (50%)
50%
50%
4.39
19.02
SNIP (75%)
75%
75%
4.70
15.22
MAT
50%
∼75%
4.32
12.03"
N,0.9523809523809523,"Table 5 compares the extended MAT, the vanilla BERT model, and SNIP (Lee et al., 2018b) in terms
of forward and backward sparsity."
N,0.9563492063492064,"SNIP is a widely used pruning method that operates based on connection sensitivity, enabling sparsity
in over-parameterized models. In our implementation, we apply SNIP in a modular manner by
calculating the connection sensitivity of each module. As shown in the Table, SNIP achieves a
31.6% reduction in computation when pruning 50% of the attention heads without any performance
degradation. However, when the pruning ratio (sparsity) is increased to 75%, SNIP fails to achieve
comparable performance with the vanilla model. This suggests that a large ratio of sparsity can have
a negative impact on model performance."
N,0.9603174603174603,"In contrast, using the criteria of MAT, we prune 50% of the attention heads while training the
remaining ones by MAT. This approach leads to a further acceleration of computations by 56.7%
while slightly improving the overall performance. This experiment serves as an example highlighting
the potential of MAT in network pruning, showcasing the trade-off between computation savings and
performance maintenance."
N,0.9642857142857143,"B.3
Computational Complexity and Overhead of MAT"
N,0.9682539682539683,"The proposed MAT can introduce additional computational and memory overheads as it involves the
calculation of mNTKs and their principal eigenvalues. However, we demonstrate in this subsection
that MAT only yields a negligible proportion of extra computations, and we also report the numeric
overhead results from experiments to support this claim."
N,0.9722222222222222,"To clarify, we have employed two strategies to accelerate the computation of mNTKs significantly: (1)
sampling a subset of size S(≪n) for the NTK approximation instead of using the entire set, and (2)
computing the gradient using the sum-of-logits approach instead of considering all output units. With
those strategies, we approximate the Jacobian matrix J ∈Rnk×m with the approximated Jacobian
matrix eJ ∈RS×m, and we only need to perform S additional gradient propagations and concatenate
the gradients together. mNTKs are then computed with the approximated Jacobian matrix, and we
perform the eigen-decomposition to mNTKs to obtain the principal eigenvalue."
N,0.9761904761904762,"Apart from those techniques mentioned above, the module division strategy also accelerates the matrix
multiplication of the Jacobian. Unlike the integral NTK, MAT applies a lightweight NTK estimation
by modular NTK that significantly reduces the computation time required, and this estimation can
be scalable to deeper structured networks. The complexity of computing integral NTK is O(Sm2);
however, the overall time complexity reduces to O(LS(m/L)2) = O(Sm2/L) in MAT, assuming
we are computing L mNTKs. As for the singular value decomposition (SVD), since S ≪n ≪m, its
complexity, O(LS3), can be far lower than others. Table 6 illustrates the comparison of computational
complexities, showcasing MAT’s significant computational advantages for NTK approximation. In
short, the overhead produced by MAT is negligible."
N,0.9801587301587301,"Table 6: Complexity comparison, where n denotes the number of training data, m the number of
parameters, k the output dimension, L the number of components, and S the sample number."
N,0.9841269841269841,"Complexity
Full
Our Approximation
NTK computaion
O(nkm2)
O(Sm2/L)
SVD computaion
O(n3k3)
O(LS3)"
N,0.9880952380952381,"We also measure the actual computation overhead by MAT. Following the experimental setting of
Turc et al. (2019), we apply the proposed MAT to BERT models with different network scales, namely
BERT-Mini (L=4, H=256), BERT-Small (L=4, H=512), BERT-Medium (L=8, H=512), BERT-Base
(L=12, H=768), and BERT-Large (L=24, H=1024). Table 7 demonstrates the computational costs
across varying multi-scale BERT models, and we can see that MAT can save 26.4%, 33.4%, 39.4%,
40.6%, and 50.9% computations for BERT-Mini, BERT-Small, BERT-Medium, BERT-Base, and
BERT-Large, respectively. These observations indicate that applying MAT to larger models can
better improve training efficiency. Notably, when compared to the overall computational costs, the
overheads introduced by MAT are extremely small and can be arguably ignored."
N,0.9920634920634921,"Table 7: The computation (PFLOPs) required for training to convergence of models of different scale
on WikiText-2."
N,0.996031746031746,"Model
BERT-Mini
BERT-Small
BERT-Meduim
BERT-Base
BERT-Large
Vanilla computation
1.44
4.28
9.52
27.80
61.25
MAT computation
1.06
2.85
5.77
16.50
30.09
MAT overhead
0.01
0.04
0.08
0.24
0.45"
