Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004347826086956522,"Group equivariant Convolutional Neural Networks (G-CNNs) constrain features to
respect the chosen symmetries, and lead to better generalization when these symme-
tries appear in the data. However, if the chosen symmetries are not present, group
equivariant architectures lead to overly constrained models and worse performance.
Frequently, the distribution of the data can be better represented by a subset of a
group than by the group as a whole, e.g., rotations in [−90○,90○]. In such cases,
a model that respects equivariance partially is better suited to represent the data.
Moreover, relevant symmetries may differ for low and high-level features, e.g.,
edge orientations in a face, and face poses relative to the camera. As a result, the
optimal level of equivariance may differ per layer. In this work, we introduce
Partial G-CNNs: a family of equivariant networks able to learn partial and full
equivariances from data at every layer end-to-end. Partial G-CNNs retain full
equivariance whenever beneﬁcial, e.g., for rotated MNIST, but are able to restrict
it whenever it becomes harmful, e.g., for 6 / 9 or natural image classiﬁcation.
Partial G-CNNs perform on par with G-CNNs when full equivariance is necessary,
and outperform them otherwise. Our method is applicable to discrete groups,
continuous groups and combinations thereof."
INTRODUCTION,0.008695652173913044,"1
INTRODUCTION"
INTRODUCTION,0.013043478260869565,"The translation equivariance of Convolutional Neural Networks (CNNs) (LeCun et al., 1998) has
proven to be an important inductive bias for good generalization on vision tasks. This is achieved by
restricting learned representations to respect the translation symmetry observed in visual data, such
that if an input is translated, its feature representation is also translated, but not modiﬁed in any other
way. Group equivariant Convolutional Neural Networks (G-CNNs) (Cohen & Welling, 2016) extend
equivariance to other symmetry groups. Analogously, they restrict the learned representations to
respect the symmetries in the group considered such that if an input is transformed by an element in
the group, e.g., a rotation, its feature representation is also transformed, e.g., rotated, but not modiﬁed."
INTRODUCTION,0.017391304347826087,"However, the group to which G-CNNs are equivariant must be ﬁxed prior to training, and imposing
equivariance to a group larger than the symmetries present in the data leads to over-constraining and
worse performance (Chen et al., 2020). The latter results from a difference in the distribution of
the data, and the family of distributions the model can describe. Consequently, the group must be
selected carefully, and it should correspond to the transformations that appear naturally in the data."
INTRODUCTION,0.021739130434782608,"Interestingly, the distribution of the data can frequently be better represented by a subset of the group
than by the group itself, e.g., rotations in [−90○,90○]. For instance, natural images are much more
likely to show an elephant standing straight or slightly rotated than an elephant upside-down. In
some cases, group transformations can even change the desired model response. A typical example is
the classiﬁcation of the numbers 6 and 9, whose deﬁning factor is their pose. In both examples, the
distribution of the data is better represented by a model that respects rotation equivariance partially.
That is, a model equivariant to some, but not all rotations."
INTRODUCTION,0.02608695652173913,"In addition, the optimal family of full / partial equivariances may change per layer. This results from
changes in pose likelihoods for low and high-level features in the data, e.g., the orientations of edges
in an human face, and the pose of human faces relative to the camera. Whereas detecting a face may
beneﬁt from partial rotation equivariance, detecting edges beneﬁts from full rotation equivariance."
INTRODUCTION,0.030434782608695653,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.034782608695652174,"These observations indicate that constructing a model with different levels of partial / full equivari-
ances at each layer may be advantageous. Weiler & Cesa (2019) empirically show that manually
selecting the level of equivariance at different layers can lead to accuracy improvements. However,
manually deﬁning layer-wise levels of equivariance is at best a difﬁcult and time-consuming task."
INTRODUCTION,0.0391304347826087,"In this work, we introduce Partial Group equivariant Convolutional Neural Networks: a new family
of equivariant methods able to learn partial and full equivariances directly from data at every layer.
This is achieved by deﬁning a learnable probability distribution over group elements at each group
convolutional layer in the network. Instead of sampling group elements uniformly from the group
during the group convolution – as normally done –, Partial G-CNNs sample them from the learned
distribution. This allows them to adjust their level of equivariance during training. We note that our
partial equivariance framework allows for both equivariance forgetting, and full equivariance. The
former is obtained by collapsing the distribution along a certain group dimension, e.g., the rotation
axis, and the later results from keeping the learned distribution uniform over the entire group."
INTRODUCTION,0.043478260869565216,"We evaluate Partial G-CNNs on illustrative toy tasks and on challenging benchmark datasets. We show
that whenever full equivariance is beneﬁcial, e.g., for rotated MNIST, Partial G-CNNs remain fully
equivariant. However, whenever harmful, our models restrict equivariance to a subset of the group
transformations, e.g., to make 6 / 9 classiﬁcation possible or to improve on natural image classiﬁcation.
Our method improves upon conventional group equivariant networks when equivariance reductions
are advantageous, and matches their performance whenever their design is optimal. To the best of our
knowledge, this work is the ﬁrst approach to explore partial group equivariances, and to provide a
practical implementation to learn them directly from data in an end-to-end manner."
INTRODUCTION,0.04782608695652174,"In summary, our contributions are:"
INTRODUCTION,0.05217391304347826,"• We present a novel architectural design for equivariant networks, with which group convolutional
networks are able to learn layer-wise partial or full equivariances from data."
INTRODUCTION,0.05652173913043478,"• We empirically show that partial equivariance allows our networks to improve upon conventional
G-CNNs for tasks for which full equivariance is harmful. However, whenever beneﬁcial, our
models learn to stay fully equivariant and match the performance of G-CNNs."
RELATED WORK,0.06086956521739131,"2
RELATED WORK"
RELATED WORK,0.06521739130434782,"Equivariance learning. Learning equivariant mappings from data has been explored by (i) meta-
learning as a means to iterate between the learning of free weights and a weight-tying matrix that
encodes the symmetry equivariances of the network (Zhou et al., 2020), or by (ii) learning the Lie
algebra generators of the group jointly with the parameters of the network (Dehmamy et al., 2021).
However, both approaches utilize the same learned symmetries across layers. In addition, MSR (Zhou
et al., 2020) is only applicable to (small) discrete groups, and requires longer training times. L-Conv
(Dehmamy et al., 2021), on the other hand, is only applicable to continuous groups and is not fully
compatible with current deep learning components, e.g., pooling, normalization."
RELATED WORK,0.06956521739130435,"Contrary to these approaches, our method learns different levels of full/partial equivariance at every
layer, is fully compatible with current deep learning components, and is applicable for discrete groups,
continuous groups and combinations thereof. It is important to mention, however, that Zhou et al.
(2020); Dehmamy et al. (2021) aim to learn the structure of the group from scratch. Our method, on
the other hand, starts from a (very) large group and allows layers in the network to constrain their
equivariance levels whenever necessary in order to ﬁt the distribution of the data better."
RELATED WORK,0.07391304347826087,"Invariance learning. Learning the right amount of invariance from data has been explored by (i)
learning a probability distribution over data augmentations, e.g., rotation, scaling, and passing the
responses of the network to the augmented inputs through a voting mechanism (Benton et al., 2020),
or by (ii) minimizing the lower bound on the marginal likelihood of the problem to infer the right
amount of rotation invariance in the weights of a network (van der Ouderaa & van der Wilk, 2021)."
RELATED WORK,0.0782608695652174,"Contrary to these approaches, our method learns full / partial equivariant representations, and is able
to modify the extent of equivariance at every layer. Most similar to our approach is Augerino (Benton
et al., 2020), whose main goal is to approximate invariance to global input transformations. However,
Augerino requires passing several augmented versions of an input during inference, and only works
with continuous transformations. Partial G-CNNs on the other hand, are also capable of deﬁning
distributions on discrete sets of transformations."
RELATED WORK,0.08260869565217391,Under review as a conference paper at ICLR 2022
RELATED WORK,0.08695652173913043,"To the best of our knowledge, our work is the ﬁrst approach to explore partial group equivariances,
and to provide a practical implementation to learn them from data end-to-end."
RELATED WORK,0.09130434782608696,"Group equivariant neural networks. The seminal work of Cohen & Welling (2016) has inspired
several methods equivariant to many different groups. Existing convolutional methods show equivari-
ance to planar rotations (Dieleman et al., 2016; Worrall et al., 2017; Weiler et al., 2018b), spherical
rotations (Weiler et al., 2018a; Cohen et al., 2018b; Esteves et al., 2019a;b; 2020), scaling (Worrall
& Welling, 2019; Sosnovik et al., 2019; Romero et al., 2020b), and more general symmetry groups
(Romero et al., 2020a; Bogatskiy et al., 2020; Finzi et al., 2021). Group equivariant self-attention has
also been proposed (Fuchs et al., 2020; Romero & Cordonnier, 2020; Hutchinson et al., 2021)."
RELATED WORK,0.09565217391304348,"Common to all these approaches is that they require ﬁxing the symmetry group to which the model will
be fully equivariant prior to training. In contrast, our method allows us to deﬁne layers approximately
equivariant to subsets of the group, and to learn these subsets during training."
RELATED WORK,0.1,"3
PRELIMINARIES: GROUP EQUIVARIANCE AND THE GROUP CONVOLUTION"
RELATED WORK,0.10434782608695652,"Group equivariance: Group equivariance is the property of a map to respect the transformations in
a group. We say that a map is equivariant to a group if whenever the input is transformed by elements
of the group, the output of the map is equally transformed but not modiﬁed. Formally, for a group G
with elements g ∈G acting on a set X, and a map φ ∶X →X, we say that φ is equivariant to G if:
φ(g ⋅x) = g ⋅φ(x), ∀x ∈X, ∀g ∈G.
(1)
For example, the convolution of a signal f ∶R →R and a kernel ψ ∶R →R is translation equivariant
because Lt(ψ ∗f)=ψ ∗Ltf, where Lt translates the function by t: Ltf(x)=f(x −t). That is, if the
input is translated, its numerical descriptors via the convolution are also translated but not modiﬁed."
RELATED WORK,0.10869565217391304,"The group convolution. In order to construct neural networks equivariant to a given group G, it
is crucial to deﬁne an operation that respects the symmetries in the group. The group convolution
generalizes the convolution to be equivariant to general symmetry groups. Formally, for any u ∈G,
the group convolution of a signal f ∶G →R and a kernel ψ ∶G →R is given by:"
RELATED WORK,0.11304347826086956,"h(u) = (ψ ∗f)(u) =∫G ψ(v−1u)f(v)dµG(v).
(2)"
RELATED WORK,0.11739130434782609,"where µG(⋅) is the (invariant) Haar measure of the group. The group convolution is G-equivariant.
That is, it holds that, for all u,v,w ∈G:"
RELATED WORK,0.12173913043478261,"(ψ ∗Lwf)(u) = Lw(ψ ∗f)(u), with Lwf(u)=f(w−1u)
The lifting convolution. Regularly, the input of a neural network is not readily deﬁned on the group
of interest G, but on a sub-domain X, i.e., f ∶X →R. As a result, in order to use group convolutions,
we ﬁrst require an operation that lifts the input signal from its domain X to the group G . This
operation is called a lifting convolution. For any u ∈G, the lifting group convolution is deﬁned as:"
RELATED WORK,0.12608695652173912,"h(u) = (ψ ∗lift f)(u) = ∫X ψ(v−1u)f(v)dµG(v), v ∈X,u ∈G.
(3)"
RELATED WORK,0.13043478260869565,"Practical implementation of the group convolution. The group convolution is deﬁned on a continu-
ous domain. Consequently, it cannot be computed in ﬁnite time, and thus must be approximated. Two
main strategies exist to approximate group convolutions with regular group representations: (i) group
discretization (Cohen & Welling, 2016), and (ii) Monte Carlo approximations (Finzi et al., 2020)."
RELATED WORK,0.13478260869565217,"Group discretization approximates the group convolution by using a ﬁxed discretization of the group,
e.g., the rotation group as the set of rotations by 90 degrees. Unfortunately, this approximation is only
equivariant to the transformations in the discretization, and not to the underlying continuous group.
A better solution is proposed by Finzi et al. (2020). Finzi et al. (2020) propose a Monte Carlo
approximation of the group convolution by sampling transformations uniformly from the entire group
during each forward pass. Formally, this approximation is given by:"
RELATED WORK,0.1391304347826087,"h(ui) = (ψˆ∗f)(ui) = ∑j ψ(v−1
j ui)f(vj)¯µG(vj),
(4)"
RELATED WORK,0.14347826086956522,"where {ui}, {vj} are the discretizations of the input and output domain of the operation, respectively."
RELATED WORK,0.14782608695652175,"Importantly, this approximation requires the convolutional kernel ψ to be deﬁned on the continuous
group. As the domain cannot be enumerated, ψ cannot be parameterized by independent weights.
Instead, Finzi et al. (2020) parameterize convolutional kernels via small neural networks. With this
parameterization, all elements v−1
j ui can be mapped to a well-deﬁned kernel value."
RELATED WORK,0.15217391304347827,Under review as a conference paper at ICLR 2022
PARTIAL GROUP EQUIVARIANT NETWORKS,0.1565217391304348,"4
PARTIAL GROUP EQUIVARIANT NETWORKS"
PARTIAL GROUP EQUIVARIANT NETWORKS,0.1608695652173913,"In this section, we introduce our approach. First, we deﬁne the concept of partial group equivariance,
and generalize the group convolution to the partial equivariant case in a constructive manner. Next,
we illustrate how group subsets can be learned with learnable probability distributions on the group,
and provide concrete examples for discrete groups, continuous groups and combinations thereof. We
conclude by presenting the partial group convolution and the architecture of Partial G-CNNs."
PARTIAL GROUP EQUIVARIANCE,0.16521739130434782,"4.1
PARTIAL GROUP EQUIVARIANCE"
PARTIAL GROUP EQUIVARIANCE,0.16956521739130434,"Contrary to group equivariance, we say that a map φ is partially equivariant to G, if the map is
equivariant to transformations in a subset of the group S ⊂G, but not to the whole group G:
φ(g ⋅x) = g ⋅φ(x),
∀x ∈X, ∀g ∈S.
(5)
Different from equivariance to a subgroup of G: a subset of the group which also fulﬁll the group
axioms, we do not restrict the subset S to be itself a group."
PARTIAL GROUP EQUIVARIANCE,0.17391304347826086,"As will we show in Sec. 4.2, partial equivariance can only be obtained in an approximate manner.
This is because the set S is not necessarily closed under group actions. Partial equivariance is related
to soft invariance (van der Wilk et al., 2018): the property of a map to be approximately invariant. We
opt for the word partial in the equivariant setting, to make clear that it deﬁnes a subset of the group."
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.1782608695652174,"4.2
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS"
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.1826086956521739,"In this section, we generalize the group convolution to describe partial equivariances. Vital to our
analysis is the equivariance proof of the group convolution (Cohen et al., 2018a). In addition, we
must distinguish between the domains of the input and output signals of the group convolution, i.e.,
the domains of f and h in Eq. 2. This distinction is important because they might differ for partial
group convolutions. From here on, we refer to these as input domain and output domain."
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.18695652173913044,"Proposition 4.1. The group convolution is G-equivariant in the sense that for all u,v,w ∈G:"
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.19130434782608696,"(ψ ∗Lwf)(u) = Lw(ψ ∗f)(u), with Lwf(u)=f(w−1u).
(6)"
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.1956521739130435,"Proof.
(ψ ∗Lwf)(u) = ∫G ψ(v−1u)f(w−1v)dµG(v) = ∫G ψ(¯v−1w−1u)f(¯v)dµG(¯v)"
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.2,= (ψ ∗f)(w−1u) = Lw(ψ ∗f)(u).
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.20434782608695654,"From the ﬁrst to the second equality, we use the change of variables ¯v∶=w−1v. We can do this because
the group convolution is a map from the group to itself, and thus if w,v ∈G, so does w−1v. Since the
Haar measure is an invariant measure on the group, we have that µG(v)=µG(¯v), for all v, ¯v ∈G."
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.20869565217391303,"Going from the group G to a subset S. Crucial to the proof of Proposition 4.1 is the fact the group
convolution is an operation from input signals to output signals on the group. As a result, w−1u is
a member of the output domain for any w ∈G applied to the input domain. Consequently, a group
transformation applied to the input can be reﬂected by an equivalent transformation on the output."
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.21304347826086956,"Now, consider the case in which the output domain is not the group G, but instead an arbitrary subset
S ⊂G, e.g., rotations in [−π 2 , π"
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.21739130434782608,"2 ]. Following the proof of Proposition 4.1 with u ∈S, and v ∈G, we
observe that the operation is equivariant to transformations w ∈G as long as w−1u is a member of S.
However, if w−1u does not belong to the output domain S, the output of the operation cannot reﬂect
an equivalent transformation to that of the input, and thus equivariance is not guaranteed."
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.2217391304347826,"Note, however, that equivariance is only obtained if Eq. 6 holds for all elements in the output domain.
That is, if w−1u is a member of S, for all elements u ∈S. This is not the case in general, as the output
domain S is not necessarily closed under group transformations. It is this difference, in fact, what
Partial G-CNNs make use of to disrupt full equivariance whenever necessary (Fig. 1)."
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.22608695652173913,"Quantifying the equivariance difference. Importantly, we can precisely quantify how much the
output response will change for any input transformation given an output domain S. Intuitively, this
difference is given by the difference in the parts of the output feature representation that go in and out
of S by the action of the input transformation. Naturally, the larger the input transformation and the
smaller the size of S, the larger the equivariance difference will be. Fig. 1 illustrates this effect. A
formal discussion and the derivation of the equivariance difference are provided in Appx. B.1."
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.23043478260869565,Under review as a conference paper at ICLR 2022
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.23478260869565218,"Figure 1: Partial group convolution. In a group convolution, the domain of the output is closed under
group transformations of the input. Consequently, all response components are part of the output
regardless of the transformation applied to the input. Differently, the output domain S of a partial
group convolution is not necessarily closed under input transformations. As a result, the feature
representation captured within S will change for different transformations of the input. In Fig. 1, for
instance, the output feature representation of the input (outlined in red) gradually leaves S for different
transformations of the input. For large transformations (180○here), the responses within S change
entirely. This difference allows partial group convolutions to distinguish among input transformations."
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.2391304347826087,"Figure 2: Partial group convolution on a group subset. Contrary to the group convolution, partial group
convolutions can receive an input whose domain is not the entire group G, but a subset thereof S(1).
Consequently, the input feature representation falling within S(1) may change for different input
transformations (top). As a result, the output of the partial group convolution may show variations for
different input transformations even if the output domain of the group convolution is the group G
(bottom row). Similar to the case in Fig. 1, this difference grows proportionally to the strength of the
input transformation. This can be seen in the red-outlined feature representations at the bottom."
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.24347826086956523,"Going from a subset S(1) to another subset S(2). Now, consider the case in which the domain of
the input and the output of the group convolution are both subsets of the group, i.e., v ∈S(1) and
u ∈S(2). Analogous to the previous case, equivariance to input transformations w ∈G holds at
positions u ∈S(2) for which w−1u are also members of S(2). Now however, the input domain is not
restricted to be closed. Consequently, the feature representation of the input will differ for different
group transformations. This induces an additional difference in the output response."
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.24782608695652175,"To see this, consider the partial group convolution in Fig. 2. For different input transformations,
the feature representations within S(1) change. As a result, the output feature representation might
change for different input transformations, even if the output domain is the group itself, i.e., S(2)=G."
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.25217391304347825,"In summary, considering a group convolution from a subset S(1) to a subset S(2) has two sources of
equivariance difference. The ﬁrst, comes from considering a subset S(2) as the output domain, and is
equivalent to the difference illustrated in the previous case. The second, comes from considering a
subset S(1) in the input domain. This allows the input features to change as a function of the input
transformation, and the size of the input domain S(1). This in turn induces an additional difference
in the output response. A formal treatment of the second difference term is provided Appx. B.2."
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.2565217391304348,"Going from a subset S to the group G. Finally, we can also consider the case in which the input
domain is a subset S of the group and the output domain is the group itself G. Following the previous
analysis, we can see that, this operation is equivariant to all input transformations w ∈G, as w−1u is"
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.2608695652173913,Under review as a conference paper at ICLR 2022
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.26521739130434785,"always member of the output domain for all values of u ∈G. Nevertheless, the input features can
change for different input transformations, and thus the difference illustrated in the previous case can
also emerge here. This case can be seen as an special case of the previous section, where S(2)=G,
and thus the equivariance difference induced by considering a subset in the output domain is zero."
FROM GROUP CONVOLUTIONS TO PARTIAL GROUP CONVOLUTIONS,0.26956521739130435,"Interestingly, the lifting convolution is an special case of this instance with S=X. Note however, that
the lifting convolution is exactly equivariant. This is because the input domain of the operation is X.
Consequently, the input domain remains equal for all input transformations."
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS,0.27391304347826084,"4.3
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS"
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS,0.2782608695652174,"So far, we have discussed the properties of the partial group convolution without providing details on
how to learn group subsets during training. In this section, we describe our approach and provide
speciﬁc examples for discrete groups, continuous groups, and combinations thereof."
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS,0.2826086956521739,Vital to our approach is the Monte Carlo approximation to the group convolution presented in Sec. 3:
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS,0.28695652173913044,"(ψˆ∗f)(ui) = ∑j ψ(v−1
j ui)f(vj)¯µG(vj)."
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS,0.29130434782608694,"As shown in Appx. C, this approximation is equivariant to G in expectation if the elements in the input
and output domain, {ui},{vj} are uniformly sampled from the Haar measure, i.e., ui,vj ∼µG(⋅).1"
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS,0.2956521739130435,"Approach. Our main observation is that we can prioritize sampling speciﬁc group elements during
the group convolution by learning a probability distribution p(g) over the elements of the group."
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS,0.3,"When group convolutions draw elements uniformly from the entire group, each group element is
drawn with equal probability and thus, the resulting approximation is fully equivariant in expectation.
However, we can also deﬁne a different probability distribution with which some samples are drawn
with larger probability. For instance, we can sample from a predeﬁned region, e.g., rotations in
[−π 2 , π"
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS,0.30434782608695654,"2 ], by deﬁning a probability distribution p(g) which is uniform in this range, but zero otherwise.
In the limit, we can forget a certain equivariance by letting this distribution collapse to a single point,
e.g., the identity, along the corresponding group dimension."
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS,0.30869565217391304,"In other words, learning a proper probability distribution p(g) on the group can be used to effectively
learn a subset of the group. Based on this intuition, we deﬁne a probability distribution p(g) on the
output domain of the operation, i.e., on {ui}, in order to learn a subset of the group S(2) upon which
partial equivariance is deﬁned. Note that we only need to learn a distribution on the output domain for
each layer. This is because group convolutional layers are applied sequentially, and thus the subset of
the input domain is learned by a distribution on the output domain of the previous layer."
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS,0.3130434782608696,"4.3.1
DEFINING p(g) FOR DISCRETE, CONTINUOUS AND MULTI-DIMENSIONAL GROUPS"
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS,0.3173913043478261,"Probability distributions for one-dimensional continuous groups. We take inspiration from
Augerino (Benton et al., 2020), an use the reparameterization trick (Kingma & Welling, 2013)
to parameterize continuous distributions. In particular, we use the reparameterization trick on the Lie
algebra of the group (Falorsi et al., 2019) to deﬁne a distribution which is uniform over a connected
set of group elements [u−1,...,e,...,u], but zero otherwise. To this end, we deﬁne a distribution
u ⋅[−1,1] with learnable u on the Lie algebra g, and map it to the group via the pushforward of the
exponential map exp ∶g →G. This give us a distribution which is uniform over a connected set of
elements [u−1,...,e,...,u], but zero otherwise.2"
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS,0.3217391304347826,"For instance, we can learn a distribution on the rotation group SO(2), which is uniform between
[−θ,θ] and zero otherwise by deﬁning a probability distribution θ ⋅[−1,1] with learnable θ on the
Lie algebra, and mapping it to the group. If we parameterize group elements as scalars g ∈[−π,π),
the exponential map is the identity, and thus p(g)=U(θ ⋅[−1,1)). If we sample group elements from
this distribution during the calculation of the group convolution, the output domain will only contain
elements in [−θ,θ) and the output feature map will be partially equivariant."
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS,0.32608695652173914,"Probability distributions for one-dimensional discrete groups. We can deﬁne a probability distri-
bution on a discrete group as the probability of sampling from all possible element combinations."
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS,0.33043478260869563,"1Finzi et al. (2020) show a similar result where ui and vj are the same points and thus vj ∼µG(⋅) sufﬁces.
2Note that an exp-pushforwarded local uniform distribution is locally equivalent to the Haar measure, and
thus we can still use the Haar measurement for integration on group subsets."
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS,0.3347826086956522,Under review as a conference paper at ICLR 2022
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS,0.3391304347826087,"For instance, for the mirroring group {1,−1}, this distribution assigns a probability to each of the
combinations {0,0}, {0,1}, {1,0}, {1,1} indicating whether the corresponding element is sampled
or not. For a group with elements {e,g1,...,gn}, however, this oblige us to sample from 2n+1
elements, which is computationally expensive and potentially unstable to train. Instead, we deﬁne
element-wise Bernoulli distributions over the elements {g1,...,gn}, and learn the probabilities {pi}
of sampling the elements {gi}. The probability distribution on the group can be then formulated as
the joint probability of the element-wise Bernoulli distributions p(e,g1,...,gn) = ∏n
i=1 p(gi)."
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS,0.34347826086956523,"To learn the element-wise distributions, we make use of the Gumbel-Softmax trick (Jang et al., 2016;
Maddison et al., 2016), and use the Straight-Through Gumbel-Softmax to back-propagate through
sampling. If all the probabilities {pi=1}, the group convolution will be fully equivariant. However,
whenever some probabilities start declining, group equivariance becomes partial. In the limit, if all
probabilities {pi=0}, the operation effectively forgets this equivariance."
LEARNING GROUP SUBSETS VIA PROBABILITY DISTRIBUTIONS ON GROUP ELEMENTS,0.34782608695652173,"Probability distributions for multi-dimensional groups. Multi-dimensional groups are important
for the applications we consider. For instance, the orthogonal group O(2) is parameterized by
rotations and mirroring, or the dilation-rotation group by scaling and rotations. In such cases,
we construct the probability distribution over the entire group as a combination of independent
probability distributions along each of these axes. For a group G with elements g decomposable
along n dimensions g=(d1,...,dn), we decompose the probability distribution as: p(g)=∏n
i=1 p(di),
where the probability p(di) is deﬁned given the type of space, i.e., continuous or discrete. For
instance, for the orthogonal group O(2) deﬁned by rotations r and mirroring m, i.e., g = (r,m),
r ∈SO(2),m ∈{±1}, we deﬁne the probability distribution on the group as p(g)=p(r)⋅p(m), where
p(r) is a continuous distribution, and p(m) is a discrete one."
THE PARTIAL GROUP CONVOLUTION,0.3521739130434783,"4.4
THE PARTIAL GROUP CONVOLUTION"
THE PARTIAL GROUP CONVOLUTION,0.3565217391304348,"Let S(1),S(2) be subsets of a group G and p(u) be uniform in S(2) and 0 otherwise. The partial
group convolution from a function f ∶S(1) →R to a function h ∶S(2) →R is given by:"
THE PARTIAL GROUP CONVOLUTION,0.36086956521739133,"h(u) = (ψ ∗f)(u) = ∫S(1) p(u)ψ(v−1u)f(v)dµG(v); u ∈S(2),v ∈S(1).
(7)"
THE PARTIAL GROUP CONVOLUTION,0.3652173913043478,"The cases described in Sec. 4.2 can be written as special cases of Eq. 7. In practice, Eq. 7 is computed
with a Monte Carlo approximation as described in Algorithm 1 (Appx. D)."
PARTIAL GROUP EQUIVARIANT NETWORKS,0.3695652173913043,"4.5
PARTIAL GROUP EQUIVARIANT NETWORKS"
PARTIAL GROUP EQUIVARIANT NETWORKS,0.3739130434782609,Figure 3: Partial G-CNN architecture.
PARTIAL GROUP EQUIVARIANT NETWORKS,0.3782608695652174,"We built upon the work of Finzi et al. (2020) and construct
base G-CNNs equivariant to continuous and discrete groups.
This is achieved by combining kernel parameterizations on
Lie groups, and actions of discrete groups on the group repre-
sentation of the kernels. Moreover, we replace the isotropic
lifting layer of Finzi et al. (2020) by a convolutional layer
with lifting convolutions (Eq. 3). Inspired by Romero et al.
(2021), we parameterize convolutional kernels as implicit
neural representations with SIRENs (Sitzmann et al., 2020).
The proposed parameterization shows better expressivity and
convergence speed, and outperforms ReLU-, LeakyReLU
and Swish-MLP parameterizations used so far for continuous
G-CNNs, e.g., Sch¨utt et al. (2017); Finzi et al. (2020), Tab. 4.
Our network structure is shown in Fig. 3, and varies only in the number of blocks and channels."
EXPERIMENTS,0.3826086956521739,"5
EXPERIMENTS"
EXPERIMENTS,0.3869565217391304,"Experimental details. We parameterize all our convolutional kernels as a 3-layer SIREN (Sitzmann
et al., 2020) with 32 hidden units. All our networks are constructed with two residual blocks of 32
channels each, batch normalization (Ioffe & Szegedy, 2015) and follow the structure shown in Fig. 3.
Our networks are intentionally selected to be simple to better assess the effect of partial equivariance.
Importantly, we avoid learning probability distributions along the translation group, and assume all
spatial positions to be sampled. This allows us to use fast PyTorch primitives in our implementation."
EXPERIMENTS,0.391304347826087,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.39565217391304347,Figure 4: Learned full/partial equivariances for MNIST6-180 with a Partial SE(2)-ResNet.
EXPERIMENTS,0.4,Table 1: Test accuracy on MNIST6-180 and MNIST6-M.
EXPERIMENTS,0.4043478260869565,"MODEL
BASE
MNIST6-180
BASE
MNIST6-M
BASE
MNIST6-M
GROUP
ACC. (%)
GROUP
ACC. (%)
GROUP
ACC. (%)"
EXPERIMENTS,0.40869565217391307,"G-CNN
SE(2)
50.0
Mirroring
50.0
E(2)
50.0
Partial G-CNN
100.0
100.0
100.0"
EXPERIMENTS,0.41304347826086957,"Toy tasks: MNIST6-180 and MNIST6-M. First, we validate the ability of our approach to solve
situations in which full equivariance impedes solving a task. To this end, we construct two datasets:
MNIST6-180, and MNIST6-M, by extracting 6 numbers from the MNIST dataset (LeCun et al., 1998),
and modifying them either by 180○rotations, or mirroring. The original numbers get the label
“original”, and the transformed data the label “transformed”. The goal is to separate these two classes."
EXPERIMENTS,0.41739130434782606,"G-CNNs are unable to solve tasks for which discrimination among group transformations are required.
As a result, SE(2)-CNNs are unable to separate MNIST6-180 classes, and E(2)-CNNs are unable to
separate MNIST6-M classes (Tab 1). Partial G-CNNs with base groups SE(2), E(2), on the other
hand, seamlessly solve the tasks. Interestingly, we observe that the original MNIST6 class already
contains some mirrored numbers, e.g., the input used in Fig. 1, and thus some class overlap exists in
MNIST6-M. Nevertheless, Partial G-CNNs are able to solve the task to perfection."
EXPERIMENTS,0.4217391304347826,"We can also visualize the learned full/partial equivariances at every layer of a network. Fig. 4
illustrates the learned equivariances for a Partial SE(2)-CNN with 2 residual blocks trained on
MNIST6-180. In order to solve the task, the network learns to progressively reduce its equivariance
as a function of depth, up to a ﬁnal range of [−0.25π,0.25π]. Interestingly however, the network
does not disrupt equivariance directly at the beginning of the network, but instead decides to stay fully
equivariant in the ﬁrst four layers. We hypothesize that this is because these layers extract low- and
mid-level features, which might appear at arbitrary poses. As a consequence, it is advantageous to
preserve full equivariance at this hierarchical level. The same behaviour is also present in E(2)-CNNs.
However, we observe that discrete distributions are more unstable to train, and other patterns also
emerge that, for example, interrupt full equivariance at the second or third layer of the network."
EXPERIMENTS,0.4260869565217391,"Benchmark image datasets. Next, we validate our approach on the following benchmark image
recognition datasets: rotated MNIST (Larochelle et al., 2007), CIFAR-10 and CIFAR-100 (Krizhevsky
et al., 2009). Additional results on PatchCamelyon (Veeling et al., 2018) can be found in Appx. F.
We construct Partial G-CNNs with base groups SE(2) and E(2), and varying number of elements
used in the Monte Carlo approximation, and compare them to equivalent ResNets and G-CNNs."
EXPERIMENTS,0.43043478260869567,"Our results are shown in Table 2. Partial G-CNNs are competitive with G-CNNs where full-
equivariance is needed, i.e., RotMNIST and PatchCamelyon. However, for tasks in which the
data does not naturally exhibit full rotation equivariance, i.e., CIFAR-10 and CIFAR-100, Partial
G-CNNs consistently outperform their full equivariant counterparts. Our results support the usage of
Partial G-CNNs for settings with both partial and full equivariance."
EXPERIMENTS,0.43478260869565216,"In addition, we validate our claim that SIRENs are better suited to parameterize group convolutional
kernels than existing alternatives. As shown in Tab. 4, SE(2)-CNNs with SIREN kernels consis-
tently outperform SE(2)-CNNs with kernel parameterizations via ReLU, LeakyReLU and Swish
nonlinearities on all the image benchmarks considered. Our results support the usage of SIRENs as
parameterization for continuous group convolutional kernels."
DISCUSSION,0.4391304347826087,"6
DISCUSSION"
DISCUSSION,0.4434782608695652,"Memory consumption in partial equivariant settings. G-CNNs ﬁx the number of samples with
which the group convolution is approximated prior to training. G-CNNs use the same number of
samples for all group convolutional layers regardless of the symmetries present in the data. Partial"
DISCUSSION,0.44782608695652176,Under review as a conference paper at ICLR 2022
DISCUSSION,0.45217391304347826,Table 2: Test accuracy on vision benchmark datasets.
DISCUSSION,0.45652173913043476,"MODEL
BASE
GROUP
NO.
ELEMENTS
PARTIAL
EQUIV.
CLASSIFICATION ACCURACY (%)"
DISCUSSION,0.4608695652173913,"ROTMNIST
CIFAR-10
CIFAR-100
ResNet
T(2)
1
-
97.23
83.11
47.99 G-CNN SE(2)"
DISCUSSION,0.4652173913043478,"4

99.10
83.73
52.35

99.13
86.15
53.91"
DISCUSSION,0.46956521739130436,"8

99.17
86.08
55.55

99.23
88.59
57.26"
DISCUSSION,0.47391304347826085,"16

99.24
86.59
51.55

99.18
87.45
57.31"
DISCUSSION,0.4782608695652174,"E(2)
8

98.14
85.55
54.29

97.78
89.00
55.22"
DISCUSSION,0.4826086956521739,"16

98.35
88.95
57.78

98.58
90.12
61.46"
DISCUSSION,0.48695652173913045,"G-CNNs on the other hand, are able to dynamically adjust the number of samples used in order to
approximate partial group convolutions. In particular, we ﬁx a maximum number of samples prior
to training, and restrict the amount of samples used at every layer based on the subset of the group
learned. For instance, a Partial SE(2)-CNN with a learned distribution p(u)=U( π"
DISCUSSION,0.49130434782608695,"2 [−1,1]) only uses
half of the elements used in the corresponding G-CNN. This efﬁciency in memory leads to important
reductions in training time for Partial G-CNNs on partial equivariant settings."
DISCUSSION,0.4956521739130435,"Better kernel parameterization. Inspired by Romero et al. (2021), we replace the Swish kernel
parameterization proposed by Finzi et al. (2020) by a SIREN Sitzmann et al. (2020). We observe that
this parameterization consistently leads to better results and faster convergence."
DISCUSSION,0.5,"Sampling per batch element. In our experiments, we sample once from the learned distribution
p(u) at every layer, and use this sample for all elements in the batch. A better estimation of the
probability distribution could be obtained by drawing a different sample for each element in the batch
at every layer. Although this method may beneﬁt the learned distributions, it comes at a large memory
cost. Consequently, all our reported results are obtained using our original formulation."
DISCUSSION,0.5043478260869565,"Partial equivariances for other group representations. Our theory of learnable partial equivari-
ances is directly applicable to methods using regular group representations, e.g., Cohen & Welling
(2016); Romero & Cordonnier (2020); Hutchinson et al. (2021). An extension for other kind of
group representations such as irreducible representations in steerable CNNs, e.g., (Worrall et al.,
2017; Weiler et al., 2018b; Weiler & Cesa, 2019) is an interesting research direction, as irreducible
representations allow for exact equivariance to continuous groups."
DISCUSSION,0.508695652173913,"Unstable training on discrete groups. Although discrete groups can be successfully modelled in
partial equvariance settings, we observe that the discrete distribution used here is not very stable
to train. Finding good parameterizations to learn discrete distributions is an active ﬁeld of research
(Hoogeboom et al., 2019; 2021), and advances in this ﬁeld could lead to more stable training of
partial equivariances on discrete groups."
CONCLUSION AND FUTURE WORK,0.5130434782608696,"7
CONCLUSION AND FUTURE WORK"
CONCLUSION AND FUTURE WORK,0.5173913043478261,"We presented Partial Group equivariant Convolutional Neural Networks (Partial G-CNNs): a new
family of equivariant methods able to learn partial and full equivariances directly from data at
every layer. Partial G-CNNs match the performance of G-CNNs in full equivariant settings, and
outperform G-CNNs in settings in which partial group equivariance better represents the data. By
learning different levels of equivariance at every layer, Partial G-CNNs are able to maintain full
group equivariance in early layers to efﬁciently learn low-level features appearing at arbitrary poses.
However, they are able to restrict it whenever necessary to discriminate among high-level features."
CONCLUSION AND FUTURE WORK,0.5217391304347826,"Our method entirely relies on a good parameterization of the probability distribution deﬁned over
group elements. Finding better representations and learning methods for these distributions, specially
for distributions on discrete variables, is a promising direction for future research. In addition,
an equally interesting future research direction is the extension of partial equivariances to neural
architectures handling other kind of group representations such as steerable CNNs."
CONCLUSION AND FUTURE WORK,0.5260869565217391,Under review as a conference paper at ICLR 2022
REFERENCES,0.5304347826086957,REFERENCES
REFERENCES,0.5347826086956522,"Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew Gordon Wilson. Learning invariances in
neural networks. arXiv preprint arXiv:2010.11882, 2020."
REFERENCES,0.5391304347826087,"Alexander Bogatskiy, Brandon Anderson, Jan Offermann, Marwah Roussi, David Miller, and Risi
Kondor. Lorentz group equivariant neural network for particle physics. In International Conference
on Machine Learning, pp. 992–1002. PMLR, 2020."
REFERENCES,0.5434782608695652,"Shuxiao Chen, Edgar Dobriban, and Jane H Lee. A group-theoretic framework for data augmentation.
Journal of Machine Learning Research, 21(245):1–71, 2020."
REFERENCES,0.5478260869565217,"Taco Cohen and Max Welling. Group equivariant convolutional networks. In International conference
on machine learning, pp. 2990–2999. PMLR, 2016."
REFERENCES,0.5521739130434783,"Taco Cohen, Mario Geiger, and Maurice Weiler. A general theory of equivariant cnns on homogeneous
spaces. arXiv preprint arXiv:1811.02017, 2018a."
REFERENCES,0.5565217391304348,"Taco S Cohen, Mario Geiger, Jonas K¨ohler, and Max Welling. Spherical cnns. arXiv preprint
arXiv:1801.10130, 2018b."
REFERENCES,0.5608695652173913,"Nima Dehmamy, Yanchen Liu, Robin Walters, and Rose Yu. Lie algebra convolutional neural
networks with automatic symmetry extraction, 2021. URL https://openreview.net/
forum?id=cTQnZPLIohy."
REFERENCES,0.5652173913043478,"Sander Dieleman, Jeffrey De Fauw, and Koray Kavukcuoglu. Exploiting cyclic symmetry in convolu-
tional neural networks. In International conference on machine learning, pp. 1889–1898. PMLR,
2016."
REFERENCES,0.5695652173913044,"Carlos Esteves, Avneesh Sud, Zhengyi Luo, Kostas Daniilidis, and Ameesh Makadia. Cross-domain
3d equivariant image embeddings. In International Conference on Machine Learning, pp. 1812–
1822. PMLR, 2019a."
REFERENCES,0.5739130434782609,"Carlos Esteves, Yinshuang Xu, Christine Allen-Blanchette, and Kostas Daniilidis. Equivariant multi-
view networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision,
pp. 1568–1577, 2019b."
REFERENCES,0.5782608695652174,"Carlos Esteves, Ameesh Makadia, and Kostas Daniilidis. Spin-weighted spherical cnns. arXiv
preprint arXiv:2006.10731, 2020."
REFERENCES,0.5826086956521739,"Luca Falorsi, Pim de Haan, Tim R Davidson, and Patrick Forr´e. Reparameterizing distributions
on lie groups. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pp.
3244–3253. PMLR, 2019."
REFERENCES,0.5869565217391305,"Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convolutional
neural networks for equivariance to lie groups on arbitrary continuous data. In International
Conference on Machine Learning, pp. 3165–3176. PMLR, 2020."
REFERENCES,0.591304347826087,"Marc Finzi, Max Welling, and Andrew Gordon Wilson. A practical method for constructing equivari-
ant multilayer perceptrons for arbitrary matrix groups. arXiv preprint arXiv:2104.09459, 2021."
REFERENCES,0.5956521739130435,"Fabian B Fuchs, Daniel E Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d
roto-translation equivariant attention networks. arXiv preprint arXiv:2006.10503, 2020."
REFERENCES,0.6,"Emiel Hoogeboom, Jorn WT Peters, Rianne van den Berg, and Max Welling. Integer discrete ﬂows
and lossless compression. arXiv preprint arXiv:1905.07376, 2019."
REFERENCES,0.6043478260869565,"Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr´e, and Max Welling.
Argmax
ﬂows and multinomial diffusion: Towards non-autoregressive language models. arXiv preprint
arXiv:2102.05379, 2021."
REFERENCES,0.6086956521739131,"Michael J Hutchinson, Charline Le Lan, Sheheryar Zaidi, Emilien Dupont, Yee Whye Teh, and
Hyunjik Kim. Lietransformer: equivariant self-attention for lie groups. In International Conference
on Machine Learning, pp. 4533–4543. PMLR, 2021."
REFERENCES,0.6130434782608696,Under review as a conference paper at ICLR 2022
REFERENCES,0.6173913043478261,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448–456.
PMLR, 2015."
REFERENCES,0.6217391304347826,"Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016."
REFERENCES,0.6260869565217392,"Diederik P Kingma and Max Welling.
Auto-encoding variational bayes.
arXiv preprint
arXiv:1312.6114, 2013."
REFERENCES,0.6304347826086957,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009."
REFERENCES,0.6347826086956522,"Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical
evaluation of deep architectures on problems with many factors of variation. In Proceedings of the
24th international conference on Machine learning, pp. 473–480, 2007."
REFERENCES,0.6391304347826087,"Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.6434782608695652,"Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016."
REFERENCES,0.6478260869565218,"David Romero, Erik Bekkers, Jakub Tomczak, and Mark Hoogendoorn. Attentive group equivariant
convolutional networks. In International Conference on Machine Learning, pp. 8188–8199. PMLR,
2020a."
REFERENCES,0.6521739130434783,"David W Romero and Jean-Baptiste Cordonnier. Group equivariant stand-alone self-attention for
vision. arXiv preprint arXiv:2010.00977, 2020."
REFERENCES,0.6565217391304348,"David W Romero, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. Wavelet networks:
Scale equivariant learning from raw waveforms. arXiv preprint arXiv:2006.05259, 2020b."
REFERENCES,0.6608695652173913,"David W Romero, Anna Kuzina, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. Ckconv:
Continuous kernel convolution for sequential data. arXiv preprint arXiv:2102.02611, 2021."
REFERENCES,0.6652173913043479,"Kristof T Sch¨utt, Pieter-Jan Kindermans, Huziel E Sauceda, Stefan Chmiela, Alexandre Tkatchenko,
and Klaus-Robert M¨uller. Schnet: A continuous-ﬁlter convolutional neural network for modeling
quantum interactions. arXiv preprint arXiv:1706.08566, 2017."
REFERENCES,0.6695652173913044,"Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Im-
plicit neural representations with periodic activation functions. Advances in Neural Information
Processing Systems, 33, 2020."
REFERENCES,0.6739130434782609,"Ivan Sosnovik, Michał Szmaja, and Arnold Smeulders. Scale-equivariant steerable networks. arXiv
preprint arXiv:1910.11093, 2019."
REFERENCES,0.6782608695652174,"Tycho F.A van der Ouderaa and Mark van der Wilk. Learning invariant weights in neural networks.
Workshop in Uncertainty & Robustness in Deep Learning, ICML, 2021."
REFERENCES,0.6826086956521739,"Mark van der Wilk, Matthias Bauer, ST John, and James Hensman. Learning invariances using the
marginal likelihood. arXiv preprint arXiv:1808.05563, 2018."
REFERENCES,0.6869565217391305,"Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equiv-
ariant cnns for digital pathology. In International Conference on Medical image computing and
computer-assisted intervention, pp. 210–218. Springer, 2018."
REFERENCES,0.691304347826087,"Maurice Weiler and Gabriele Cesa.
General e(2)-equivariant steerable cnns.
arXiv preprint
arXiv:1911.08251, 2019."
REFERENCES,0.6956521739130435,"Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen. 3d steerable cnns:
Learning rotationally equivariant features in volumetric data. arXiv preprint arXiv:1807.02547,
2018a."
REFERENCES,0.7,Under review as a conference paper at ICLR 2022
REFERENCES,0.7043478260869566,"Maurice Weiler, Fred A Hamprecht, and Martin Storath. Learning steerable ﬁlters for rotation
equivariant cnns.
In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 849–858, 2018b."
REFERENCES,0.7086956521739131,"Daniel E Worrall and Max Welling. Deep scale-spaces: Equivariance over scale. arXiv preprint
arXiv:1905.11697, 2019."
REFERENCES,0.7130434782608696,"Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic
networks: Deep translation and rotation equivariance. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5028–5037, 2017."
REFERENCES,0.717391304347826,"Allan Zhou, Tom Knowles, and Chelsea Finn. Meta-learning symmetries by reparameterization.
arXiv preprint arXiv:2007.02933, 2020."
REFERENCES,0.7217391304347827,Under review as a conference paper at ICLR 2022
REFERENCES,0.7260869565217392,APPENDIX
REFERENCES,0.7304347826086957,"A
GROUPS AND GROUP ACTION"
REFERENCES,0.7347826086956522,"Groups: Group theory is the mathematical language that describes symmetries. The core mathe-
matical object is that of a group, and deﬁnes what it means for something to exhibit symmetries.
Speciﬁcally, a group consists of a tuple (G,⋅) – set of transformations G, and a binary operation ⋅
with the following properties: (i) closure, i.e., g1 × g2 = g3 ∈G,∀g1,g2 ∈G (ii) associativity, i.e.,
g1 ⋅(g2 ⋅g3)=(g1 ⋅g2) ⋅g3 for all g1,g2,g3 ∈G, (iii) the existence of an identity element e ∈G, such
that g ⋅e=e ⋅g = g, and (iv) the existence of an inverse g−1 ∈G for all g ∈G."
REFERENCES,0.7391304347826086,"Group action: One can deﬁne the action of the group G on a set X. This action describes how
group elements g ∈G modify the set X when the transformation is applied. For instance, the action
of elements in the group of planar rotations θ ∈SO(2) on an image x ∈X –written θx–, depicts how
the image x changes when the rotation θ is applied."
REFERENCES,0.7434782608695653,"B
FORMAL TREATMENT OF THE EQUIVARIANCE DIFFERENCE IN
PARTIAL GROUP CONVOLUTIONS"
REFERENCES,0.7478260869565218,"B.1
PARTIAL GROUP CONVOLUTIONS FROM THE GROUP G TO A SUBSET S"
REFERENCES,0.7521739130434782,"The partial group convolution from signals on G to signals on a subset S can be interpreted as a group
convolution for which the output signal outside of S is set to zero. Consequently, we can calculate
the equivariance difference ∆equiv in the feature representation, by calculating the difference on the
subset S of a group convolution with a group-transformed input (Lwf ∗ψ) and a group convolution
with a canonical input proceeded by the same transformation on S, i.e., Lw(f ∗ψ)."
REFERENCES,0.7565217391304347,"The equivariance difference given by the effect of a subset in the output domain ∆out
equiv is given by:"
REFERENCES,0.7608695652173914,"∆out
equiv = ∥∫S Lw(ψ ∗f)(u)dµG(u) −∫S(ψ ∗Lwf)(u)dµG(u)∥ 2 2"
REFERENCES,0.7652173913043478,= ∥∫w−1S(ψ ∗f)(w−1u)dµG(u) −∫S(ψ ∗f)(w−1u)dµG(u)∥ 2 2
REFERENCES,0.7695652173913043,= ∥∫S(ψ ∗f)(u)dµG(u) −∫wS(ψ ∗f)(u)dµG(u)∥ 2 2 = ∥∫ smax
REFERENCES,0.7739130434782608,"smin
(ψ ∗f)(u)dµG(u) −∫ wsmax"
REFERENCES,0.7782608695652173,"wsmin
(ψ ∗f)(u)dµG(u)∥ 2 2 = ∥(∫ smax"
REFERENCES,0.782608695652174,"wsmax
(ψ ∗f)(u)dµG(u) + ∫ wsmax"
REFERENCES,0.7869565217391304,"smin
(ψ ∗f)(u)dµG(u))− (∫ wsmax"
REFERENCES,0.7913043478260869,"smin
(ψ ∗f)(u)dµG(u) + ∫ smin"
REFERENCES,0.7956521739130434,"wsmin
(ψ ∗f)(u)dµG(u))∥ 2 2 = ∥∫ smax"
REFERENCES,0.8,"wsmax
(ψ ∗f)(u)dµG(u) −∫ smin"
REFERENCES,0.8043478260869565,"wsmin
(ψ ∗f)(u)dµG(u)∥ 2 2"
REFERENCES,0.808695652173913,"From the ﬁrst line to the second we take advantage of the equivariance property of the group
convolution: (Lwf ∗ψ)(u) = Lw(f ∗ψ)(u), and account for the fact that only the region within
S is visible at the output. We use the change of variables u = w−1u from the second to third line.
We specify the boundaries of S from the third to the fourth line. In the ﬁfth line we separate the
integration over S as a sum of two integrals which depict the same range. In the last line, we cancel
out the overlapping parts of the two integrals to come to the ﬁnal result."
REFERENCES,0.8130434782608695,"In conclusion, the equivariance difference induced by a subset S(2) on the domain of the output
∆out
equiv is given by the difference between the part of the representation that leaves the subset S, and
the part that comes to replace it instead. This behaviour is illustrated in Figure 1."
REFERENCES,0.8173913043478261,Under review as a conference paper at ICLR 2022
REFERENCES,0.8217391304347826,"B.2
PARTIAL GROUP CONVOLUTIONS FROM A SUBSET S(1) TO A SUBSET S(2)"
REFERENCES,0.8260869565217391,"To isolate the effect of having a group subset as domain of the input signal f, we ﬁrst consider the
domain of the output to be the group, i.e., S(2)=G. The equivariance difference is given by the
difference across the entire output representation of the group convolution calculated on an input
subset S(1) with a canonical input f, and with a group transformed input Lwf."
REFERENCES,0.8304347826086956,"The equivariance difference ∆in
equiv given by the effect of a subset in the input domain is given by:"
REFERENCES,0.8347826086956521,∆equiv = ∥∫G ∫S ψ(v−1u)f(v)dµG(v)dµG(u) −∫G ∫S ψ(v−1u)f(w−1v)dµG(v)dµG(u)∥ 2 2
REFERENCES,0.8391304347826087,= ∥∫G [∫S ψ(v−1u)f(v)dµG(v) −∫S ψ(v−1u)f(w−1v)dµG(v)]dµG(u)∥ 2 2
REFERENCES,0.8434782608695652,= ∥∫G ∫S ψ(v−1u)[f(v) −f(w−1v)] dµG(v)dµG(u)∥ 2 2
REFERENCES,0.8478260869565217,"In other words, the equivariance difference induced by a subset S(1) on the domain of the input
∆in
equiv is given by the difference in S(1) of the canonical input f, and the part that comes to replace
it when the input is modiﬁed by a group transformation w."
REFERENCES,0.8521739130434782,"C
EQUIVARIANCE PROPERTY OF MONTE-CARLO APPROXIMATIONS"
REFERENCES,0.8565217391304348,Consider the Monte-Carlo approximation depicted in Eq. 4:
REFERENCES,0.8608695652173913,"(ψˆ∗f)(ui) = ∑j ψ(v−1
j ui)f(vj)¯µG(vj)."
REFERENCES,0.8652173913043478,"For a transformed version of the Lwf, we can show that the Monte-Carlo approximation of the group
convolution is equivariant in expectation. The proof follows the same steps than Finzi et al. (2020).
However, the last step of the proof has a different reason, resulting from the fact that input and output
elements can be sampled from different distributions."
REFERENCES,0.8695652173913043,"For a transformed version of the Lwf, we have that:"
REFERENCES,0.8739130434782608,"(ψˆ∗Lwf)(ui) = ∑j ψ(v−1
j ui)f(w−1vj)¯µG(vj)"
REFERENCES,0.8782608695652174,"= ∑j ψ(˜v−1
j w−1ui)f(˜vj)¯µG(˜vj)"
REFERENCES,0.8826086956521739,d= (ψˆ∗f)(w−1ui) = Lw(ψˆ∗f)(ui)
REFERENCES,0.8869565217391304,"From the ﬁrst to the second line, we use the change of variables ˜vj = wvj and the fact that, group ele-
ments in the input domain are sampled from the Haar measure for which it holds that ¯µG(vj)=¯µG(˜vj).
However, from the second to the third line, we must also assume that this holds for the output domain.
That is, that the probability of drawing w−1uj is equal to that of drawing uj. This is of particular
importance for the partial equivariance setting, in which this might not be the case."
REFERENCES,0.8913043478260869,"D
ALGORITHM FOR MONTE-CARLO APPROXIMATION OF THE PARTIAL
GROUP CONVOLUTION"
REFERENCES,0.8956521739130435,"Algorithm 1: The Partial Group Convolution Layer
Inputs: position, function-value tuples on the group or a subset thereof {vj,f(vj)}.
Outputs: convolved position, function-value tuples on the output group subset {ui,(fˆ∗ψ)(ui)}.
{ui} ∼p(u) ; /* Sample elements from p(u)
for ui ∈{ui} do"
REFERENCES,0.9,"h(ui) = ∑j ψ(v−1
j ui)f(vj)¯µG(vj) ; /* Compute group convolution (Eq. 4)
end
return {ui,h(ui)}"
REFERENCES,0.9043478260869565,Under review as a conference paper at ICLR 2022
REFERENCES,0.908695652173913,Table 3: Image recognition accuracy on PatchCam dataset.
REFERENCES,0.9130434782608695,"MODEL
BASE
GROUP
NO.
ELEMENTS
PARTIAL
EQUIV.
CLASSIFICATION ACCURACY
ON PATCHCAM (%)
ResNet
T(2)
1
-
67.59"
REFERENCES,0.9173913043478261,"G-CNN
SE(2)
8

89.87

89.07"
REFERENCES,0.9217391304347826,"16

89.71

90.31"
REFERENCES,0.9260869565217391,"E(2)
16

89.77

88.13"
REFERENCES,0.9304347826086956,"E
ARCHITECTURE AND HYPERPARAMETER DETAILS"
REFERENCES,0.9347826086956522,"We note that all the hyperparameters were chosen based on the best performance for the fully-
equivariant networks and the same parameters were used for training the proposed Partial G-CNNs."
REFERENCES,0.9391304347826087,"For all the experiments shown in this paper, we use residual networks (shown in the main paper in
Figure 3) with an initial lifting convolutional layer followed by 2 ResBlocks with full/partial group
convolutional layers (or regular convolutional layers for the non-equivariant ResNet baselines). For
all the networks for all the datasets, we use 32 feature maps in the hidden layers. We employ Batch
Normalization and ReLU non-linearities as shown in the ﬁgure. For MNIST6-M and MNIST6-180,
max-pooling is performed after each of the ResBlock partial/full group convolutional layers. In the
case of rotMNIST, max-pooling is performed after the lifting convolutional layer and the ﬁrst group
convolutional layers. For CIFAR-10 and CIFAR-100, we use max-pooling layers after each of the
group convolutional layers. Finally, for PatchCamelyon, we apply the max-pooling operation after
the lifting convolution as well as the both the group convolutional layers. At the end of the network,
a global max-pooling layer is used to create the invariant before classiﬁcation."
REFERENCES,0.9434782608695652,"The networks for MNIST6-180, MNIST6-M, rotMNIST, CIFAR-10 and CIFAR-100 are trained for
300 epochs and the networks for PatchCamelyon are trained for 30 epochs. We use a batch size of 64
for all networks. We use an initial learning rate of 1e −3 and Adam optimizer with cosine annealing
with 5 epochs of linear warm-up. In the case of CIFAR-10, CIFAR-100 and PatchCamelyon datasets,
we also use a weight decay of 0.0001. For roMNIST, CIFAR-10, CIFAR-100 and PatchCamelyon,
we also use a different learning rate of 1e −4 for only the parameters of the probability distributions
over the group elements."
REFERENCES,0.9478260869565217,"F
ADDITIONAL EXPERIMENTAL RESULTS"
REFERENCES,0.9521739130434783,"Classiﬁcation results on PatchCamelyon. Table 3 shows the results obtained for ResNet, G-CNNs
and Partial G-CNNs on the PatchCamelyon dataset (Veeling et al., 2018). Partial G-CNNs match the
performance of G-CNNs in this full equivariant setting, as expected."
REFERENCES,0.9565217391304348,"We observe that the learned distributions over the group elements for rotMNIST and PatchCamelyon
are exactly consistent with full-equivariance, and for CIFAR-10 and CIFAR-100, they clearly show
partial equivariance"
REFERENCES,0.9608695652173913,"Evaluation of existing kernel parameterizations. We observe that SIREN kernels consistently
lead to better accuracy than other existing kernel parameterizations. Our results are shown in Tab. 4."
REFERENCES,0.9652173913043478,Under review as a conference paper at ICLR 2022
REFERENCES,0.9695652173913043,Table 4: Comparison of kernel parameterizations.
REFERENCES,0.9739130434782609,"MODEL
NO.
ELEMENTS
KERNEL
TYPE
CLASSIFICATION ACCURACY (%)"
REFERENCES,0.9782608695652174,"ROTMNIST
CIFAR-10
CIFAR-100"
REFERENCES,0.9826086956521739,SE(2)-CNN 4
REFERENCES,0.9869565217391304,"ReLU
96.49
59.95
28.01
LeakyReLU
94.47
56.19
27.36
Swish
94.41
66.12
34.20
SIREN
99.10
83.73
52.35 8"
REFERENCES,0.991304347826087,"ReLU
97.73
68.29
37.81
LeakyReLU
97.65
68.94
36.30
Swish
97.72
69.20
34.10
SIREN
99.17
86.08
55.55 16"
REFERENCES,0.9956521739130435,"ReLU
98.49
66.84
37.72
LeakyReLU
98.53
68.01
38.29
Swish
98.55
65.99
37.72
SIREN
99.24
86.68
51.51"
