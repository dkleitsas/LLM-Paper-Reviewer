Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0016806722689075631,"Stochastic Rising Bandits (SRBs) model sequential decision-making problems in
1"
ABSTRACT,0.0033613445378151263,"which the expected reward of the available options increases every time they are
2"
ABSTRACT,0.005042016806722689,"selected. This setting captures a wide range of scenarios in which the available
3"
ABSTRACT,0.0067226890756302525,"options are learning entities whose performance improves (in expectation) over
4"
ABSTRACT,0.008403361344537815,"time. While previous works addressed the regret minimization problem, this paper
5"
ABSTRACT,0.010084033613445379,"focuses on the ﬁxed-budget Best Arm Identiﬁcation (BAI) problem for SRBs. In this
6"
ABSTRACT,0.011764705882352941,"scenario, given a ﬁxed budget of rounds, we are asked to provide a recommendation
7"
ABSTRACT,0.013445378151260505,"about the best option at the end of the identiﬁcation process. We propose two
8"
ABSTRACT,0.015126050420168067,"algorithms to tackle the above-mentioned setting, namely R-UCBE, which resorts
9"
ABSTRACT,0.01680672268907563,"to a UCB-like approach, and R-SR, which employs a successive reject procedure.
10"
ABSTRACT,0.018487394957983194,"Then, we prove that, with a sufﬁciently large budget, they provide guarantees on
11"
ABSTRACT,0.020168067226890758,"the probability of properly identifying the optimal option at the end of the learning
12"
ABSTRACT,0.021848739495798318,"process. Furthermore, we derive a lower bound on the error probability, matched by
13"
ABSTRACT,0.023529411764705882,"our R-SR (up to logarithmic factors), and illustrate how the need for a sufﬁciently
14"
ABSTRACT,0.025210084033613446,"large budget is unavoidable in the SRB setting. Finally, we numerically validate
15"
ABSTRACT,0.02689075630252101,"the proposed algorithms in synthetic and real-world environments and compare
16"
ABSTRACT,0.02857142857142857,"them with the currently available BAI strategies.
17"
INTRODUCTION,0.030252100840336135,"1
Introduction
18"
INTRODUCTION,0.031932773109243695,"Multi-Armed Bandits (MAB, Lattimore and Szepesvári, 2020) are a well-known framework that
19"
INTRODUCTION,0.03361344537815126,"effectively solves learning problems requiring sequential decisions. Given a time horizon, the learner
20"
INTRODUCTION,0.03529411764705882,"chooses, at each round, a single option (a.k.a. arm) and observes the corresponding noisy reward,
21"
INTRODUCTION,0.03697478991596639,"which is a realization of an unknown distribution. The MAB problem is commonly studied in two
22"
INTRODUCTION,0.03865546218487395,"ﬂavours: regret minimization (Auer et al., 2002) and best arm identiﬁcation (Bubeck et al., 2009).
23"
INTRODUCTION,0.040336134453781515,"In regret minimization, the goal is to control the cumulative loss w.r.t. the optimal arm over a time
24"
INTRODUCTION,0.04201680672268908,"horizon. Conversely, in best arm identiﬁcation, the goal is to provide a recommendation about the
25"
INTRODUCTION,0.043697478991596636,"best arm at the end of the time horizon. Speciﬁcally, we are interested in the ﬁxed-budget scenario,
26"
INTRODUCTION,0.0453781512605042,"where we seek to minimize the error probability of recommending the wrong arm at the end of the
27"
INTRODUCTION,0.047058823529411764,"time budget, no matter the loss incurred during learning.
28"
INTRODUCTION,0.04873949579831933,"This work focuses on the Stochastic Rising Bandits (SRB), a speciﬁc instance of the rested ban-
29"
INTRODUCTION,0.05042016806722689,"dit (Tekin and Liu, 2012) setting in which the expected reward of an arm increases according to the
30"
INTRODUCTION,0.052100840336134456,"number of times it has been pulled. Online learning in such a scenario has been recently addressed
31"
INTRODUCTION,0.05378151260504202,"from a regret minimization perspective by Metelli et al. (2022), in which the authors provide no-
32"
INTRODUCTION,0.05546218487394958,"regret algorithms for the SRB setting in both the rested and restless cases. The SRB setting models
33"
INTRODUCTION,0.05714285714285714,"several real-world scenarios where arms improve their performance over time. A classic example is
34"
INTRODUCTION,0.058823529411764705,"the so-called Combined Algorithm Selection and Hyperparameter optimization (CASH, Thornton
35"
INTRODUCTION,0.06050420168067227,"et al., 2013; Kotthoff et al., 2017; Erickson et al., 2020; Li et al., 2020; Zöller and Huber, 2021), a
36"
INTRODUCTION,0.06218487394957983,"problem of paramount importance in Automated Machine Learning (AutoML, Feurer et al., 2015;
37"
INTRODUCTION,0.06386554621848739,"Yao et al., 2018; Hutter et al., 2019; Mussi et al., 2023). In CASH, the goal is to identify the best
38"
INTRODUCTION,0.06554621848739496,"learning algorithm together with the best hyperparameter conﬁguration for a given ML task (e.g.,
39"
INTRODUCTION,0.06722689075630252,"classiﬁcation or regression). In this problem, every arm represents a hyperparameter tuner acting
40"
INTRODUCTION,0.06890756302521009,"on a speciﬁc learning algorithm. A pull corresponds to a unit of time/computation in which we
41"
INTRODUCTION,0.07058823529411765,"improve (on average) the hyperparameter conﬁguration (via the tuner) for the corresponding learning
42"
INTRODUCTION,0.07226890756302522,"algorithm. CASH was handled in a bandit Best Arm Identiﬁcation (BAI) fashion in Li et al. (2020)
43"
INTRODUCTION,0.07394957983193277,"and Cella et al. (2021). The former handles the problem by considering rising rested bandits with
44"
INTRODUCTION,0.07563025210084033,"deterministic rewards, failing to represent the intrinsic uncertain nature of such processes. Instead,
45"
INTRODUCTION,0.0773109243697479,"the latter, while allowing stochastic rewards, assumes that the expected rewards evolve according to a
46"
INTRODUCTION,0.07899159663865546,"known parametric functional class, whose parameters have to be learned.1
47"
INTRODUCTION,0.08067226890756303,"Original Contributions In this paper, we address the design of algorithms to solve the BAI task
48"
INTRODUCTION,0.08235294117647059,"in the rested SRB setting when a ﬁxed budget is provided.2 More speciﬁcally, we are interested in
49"
INTRODUCTION,0.08403361344537816,"algorithms guaranteeing a sufﬁciently large probability of recommending the arm with the largest
50"
INTRODUCTION,0.08571428571428572,"expected reward at the end of the time budget (as if only this arm were pulled from the beginning).
51"
INTRODUCTION,0.08739495798319327,"The main contributions of the paper are summarized as follows:3
52"
INTRODUCTION,0.08907563025210084,"• We propose two algorithms to solve the BAI problem in the SRB setting: R-UCBE (an optimistic
53"
INTRODUCTION,0.0907563025210084,"approach, Section 4) and R-SR (a phases-based rejection algorithm, Section 5). First, we intro-
54"
INTRODUCTION,0.09243697478991597,"duce speciﬁcally designed estimators required by the algorithms (Section 3). Then, we provide
55"
INTRODUCTION,0.09411764705882353,"guarantees on the error probability of the misidentiﬁcation of the best arm.
56"
INTRODUCTION,0.0957983193277311,"• We derive the ﬁrst error probability lower bound for the SRB setting, matched by our R-SR
57"
INTRODUCTION,0.09747899159663866,"algorithm up to logarithmic factors, which highlights the complexity of the problem and the need
58"
INTRODUCTION,0.09915966386554621,"for a sufﬁciently large time budget (Section 6).
59"
INTRODUCTION,0.10084033613445378,"• Finally, we conduct numerical simulations on synthetically generated data and a real-world online
60"
INTRODUCTION,0.10252100840336134,"best model selection problem. We compare the proposed algorithms with the ones available in the
61"
INTRODUCTION,0.10420168067226891,"bandit literature to tackle the SRB problem (Section 7).
62"
PROBLEM FORMULATION,0.10588235294117647,"2
Problem Formulation
63"
PROBLEM FORMULATION,0.10756302521008404,"In this section, we revise the Stochastic Rising Bandits (SRB) setting (Heidari et al., 2016; Metelli
64"
PROBLEM FORMULATION,0.1092436974789916,"et al., 2022). Then, we formulate our best arm identiﬁcation problem, introduce the deﬁnition of error
65"
PROBLEM FORMULATION,0.11092436974789915,"probability, and provide a preliminary characterization of the problem.
66"
PROBLEM FORMULATION,0.11260504201680673,"Setting We consider a rested Multi-Armed Bandit problem ⌫“ p⌫iqiPJKK with a ﬁnite number
67"
PROBLEM FORMULATION,0.11428571428571428,"of arms K.4 Let T P N be the time budget of the learning process. At every round t P JTK, the
68"
PROBLEM FORMULATION,0.11596638655462185,"agent selects an arm It P JKK, plays it, and observes a reward xt „ ⌫ItpNIt,tq, where ⌫ItpNIt,tq
69"
PROBLEM FORMULATION,0.11764705882352941,"is the reward distribution of the chosen arm It at round t and depends on the number of pulls
70"
PROBLEM FORMULATION,0.11932773109243698,"performed so far Ni,t :“ ∞t"
PROBLEM FORMULATION,0.12100840336134454,"⌧“1 1tI⌧“ iu (i.e., rested). The rewards are stochastic, formally
71"
PROBLEM FORMULATION,0.1226890756302521,"xt :“ µItpNIt,tq ` ⌘t, where µItp¨q is the expected reward of arm It and ⌘t is a zero-mean σ2-
72"
PROBLEM FORMULATION,0.12436974789915967,"subgaussian noise, conditioned to the past.5 As customary in the bandit literature, we assume that
73"
PROBLEM FORMULATION,0.12605042016806722,"the rewards are bounded in expectation, formally µipnq P r0, 1s, @i P JKK, n P JTK. As in (Metelli
74"
PROBLEM FORMULATION,0.12773109243697478,"et al., 2022), we focus on a particular family of rested bandits in which the expected rewards are
75"
PROBLEM FORMULATION,0.12941176470588237,"monotonically non-decreasing and concave in expectation.
76"
PROBLEM FORMULATION,0.13109243697478992,"Assumption 2.1 (Non-decreasing and concave expected rewards). Let ⌫be a rested MAB, deﬁning
77"
PROBLEM FORMULATION,0.13277310924369748,"γipnq :“ µipn ` 1q ´ µipnq, for every n P N and every arm i P JKK the rewards are non-decreasing
78"
PROBLEM FORMULATION,0.13445378151260504,"and concave, formally:
79"
PROBLEM FORMULATION,0.1361344537815126,"Non-decreasing:
γipnq • 0,
Concave:
γipn ` 1q § γipnq."
PROBLEM FORMULATION,0.13781512605042018,"Intuitively, the γipnq represents the increment of the real process µip¨q evaluated at the nth pull.
80"
PROBLEM FORMULATION,0.13949579831932774,"Notice that concavity emerges in several settings, such as the best model selection and economics,
81"
PROBLEM FORMULATION,0.1411764705882353,"representing the decreasing marginal returns (Lehmann et al., 2001; Heidari et al., 2016).
82"
PROBLEM FORMULATION,0.14285714285714285,"1A complete discussion of the related works is available in Appendix A. Additional motivating examples are
discussed in Appendix B."
PROBLEM FORMULATION,0.14453781512605043,"2We focus on the rested setting only and, thus, from now on, we will omit “rested” in the setting name.
3The proofs of all the statements in this work are provided in Appendix D.
4Let y, z P N, we denote with JzK :“ t1, . . . , zu, and with Jy, zK :“ ty, . . . , zu."
PROBLEM FORMULATION,0.146218487394958,5A zero-mean random variable x is σ2-subgaussian if it holds Exre⇠xs § e σ2⇠2
PROBLEM FORMULATION,0.14789915966386555,"2
for every ⇠P R."
PROBLEM FORMULATION,0.1495798319327731,"Learning Problem The goal of BAI in the SRB setting is to select the arm providing the largest
83"
PROBLEM FORMULATION,0.15126050420168066,"expected reward with a large enough probability given a ﬁxed budget T P N. Unlike the stationary
84"
PROBLEM FORMULATION,0.15294117647058825,"BAI problem (Audibert et al., 2010), in which the optimal arm is not changing, in this setting, we
85"
PROBLEM FORMULATION,0.1546218487394958,"need to decide when to evaluate the optimality of an arm. We deﬁne optimality by considering the
86"
PROBLEM FORMULATION,0.15630252100840336,"largest expected reward at time T. Formally, given a time budget T, the optimal arm i˚pTq P JKK,
87"
PROBLEM FORMULATION,0.15798319327731092,"which we assume unique, satisﬁes:
88"
PROBLEM FORMULATION,0.15966386554621848,i˚pTq :“ arg max iPJKK
PROBLEM FORMULATION,0.16134453781512606,"µipTq,"
PROBLEM FORMULATION,0.16302521008403362,"where we highlighted the dependence on T as, with different values of the budget, i˚pTq may
change. Let i P JKKzti˚pTqu be a suboptimal arm, we deﬁne the suboptimality gap as ∆ipTq :“
µi˚pT qpTq´µipTq. We employ the notation piq P JKK to denote the ith best arm at time T (arbitrarily
breaking ties), i.e., we have ∆p2qpTq § ¨ ¨ ¨ § ∆pKqpTq. Given an algorithm A that recommends"
PROBLEM FORMULATION,0.16470588235294117,"ˆI˚pTq P JKK at the end of the learning process, we measure its performance with the error probability,
i.e., the probability of recommending a suboptimal arm at the end of the time budget T:"
PROBLEM FORMULATION,0.16638655462184873,eT pAq :“ PApˆI˚pTq ‰ i˚pTqq.
PROBLEM FORMULATION,0.16806722689075632,"Problem Characterization We now provide a characterization of a speciﬁc class of polynomial
89"
PROBLEM FORMULATION,0.16974789915966387,"functions to upper bound the increments γipnq.
90"
PROBLEM FORMULATION,0.17142857142857143,"Assumption 2.2 (Bounded γipnq). Let ⌫be a rested MAB, there exist c ° 0 and β ° 1 such that for
91"
PROBLEM FORMULATION,0.173109243697479,"every arm i P JKK and number of pulls n P J0, TK it holds that γipnq § cn´β.
92"
PROBLEM FORMULATION,0.17478991596638654,"We anticipate that, even if our algorithms will not require such an assumption, it will be used
93"
PROBLEM FORMULATION,0.17647058823529413,"for deriving the lower bound and for providing more human-readable error probability guarantees.
94"
PROBLEM FORMULATION,0.1781512605042017,"Furthermore, we observe that our Assumption 2.2 is fulﬁlled by a strict superset of the functions
95"
PROBLEM FORMULATION,0.17983193277310924,"employed in Cella et al. (2021).
96"
ESTIMATORS,0.1815126050420168,"3
Estimators
97"
ESTIMATORS,0.18319327731092436,"In this section, we introduce the estimators of the arm expected reward employed by the proposed
98"
ESTIMATORS,0.18487394957983194,"algorithms.6 A visual representation of such estimators is provided in Figure 1.
99"
ESTIMATORS,0.1865546218487395,"Let "" P p0, 1{2q be the fraction of samples collected up to the current time t we use to build estimators
of the expected reward. We employ an adaptive arm-dependent window size hpNi,t´1q :“ t""Ni,t´1u
to include the most recent samples collected only, avoiding the use of samples that are no longer
representative. We deﬁne the set of the last hpNi,t´1q rounds in which the ith arm was pulled as:"
ESTIMATORS,0.18823529411764706,"Ti,t :“ t⌧P JTK : I⌧“ i ^ Ni,⌧“ Ni,t´1 ´ l, l P J0, hpNi,t´1q ´ 1Ku .
Furthermore, the set of the pairs of rounds ⌧and ⌧1 belonging to the sets of the last and second-last
100"
ESTIMATORS,0.1899159663865546,"hpNi,t´1q-wide windows of the ith arm is deﬁned as:
101"
ESTIMATORS,0.1915966386554622,"Si,t :“ "
ESTIMATORS,0.19327731092436976,"p⌧, ⌧1q P JTK ˆ JTK : I⌧“ I⌧1 “ i ^ Ni,⌧“ Ni,t´1 ´ l,"
ESTIMATORS,0.1949579831932773,"Ni,⌧1 “ Ni,⌧´ hpNi,t´1q, l P J0, hpNi,t´1q ´ 1K ( ."
ESTIMATORS,0.19663865546218487,"In the following, we design a pessimistic estimator and an optimistic estimator of the expected reward
102"
ESTIMATORS,0.19831932773109243,"of each arm at the end of the budget time T, i.e., µipTq.7
103"
ESTIMATORS,0.2,"Pessimistic Estimator The pessimistic estimator ˆµipNi,t´1q is a negatively biased estimate of µipTq
104"
ESTIMATORS,0.20168067226890757,"obtained assuming that the function µip¨q remains constant up to time T. This corresponds to the
105"
ESTIMATORS,0.20336134453781513,"minimum admissible value under Assumption 2.1 (due to the Non-decreasing constraint). This
106"
ESTIMATORS,0.20504201680672268,"estimator is an average of the last hpNi,t´1q observed rewards collected from the ith arm, formally:
107"
ESTIMATORS,0.20672268907563024,"ˆµipNi,t´1q :“
1
hpNi,t´1q ÿ"
ESTIMATORS,0.20840336134453782,"⌧PTi,t"
ESTIMATORS,0.21008403361344538,"x⌧.
(1)"
ESTIMATORS,0.21176470588235294,"The estimator enjoys the following concentration property.
108"
ESTIMATORS,0.2134453781512605,"6The estimators are adaptations of those presented by Metelli et al. (2022) to handle a ﬁxed time budget T.
7Naïvely computing the estimators from their deﬁnition requires OphpNi,t´1qq number of operations. An
efﬁcient way to incrementally update them, using Op1q operations, is provided in Appendix C."
ESTIMATORS,0.21512605042016808,"Lemma 3.1 (Concentration of ˆµi). Under Assumption 2.1, for every a ° 0, simultaneously for every
109"
ESTIMATORS,0.21680672268907564,"arm i P JKK and number of pulls n P J0, TK, with probability at least 1 ´ 2TKe´a{2 it holds that:
110"
ESTIMATORS,0.2184873949579832,"ˆβipnq ´ ˆ⇣ipnq § ˆµipnq ´ µipnq § ˆβipnq,
(2)"
ESTIMATORS,0.22016806722689075,where ˆβipnq :“ σ b
ESTIMATORS,0.2218487394957983,"a
hpnq and ˆ⇣ipnq :“ 1"
ESTIMATORS,0.2235294117647059,"2p2T ´ n ` hpnq ´ 1q γipn ´ hpnq ` 1q.
111"
ESTIMATORS,0.22521008403361345,"t
T
⌧
⌧1 ˇµT"
ESTIMATORS,0.226890756302521,"i pNi,t´1q"
ESTIMATORS,0.22857142857142856,"ˆµipNi,t´1q"
ESTIMATORS,0.23025210084033612,"Ni,t´1 ˇβT"
ESTIMATORS,0.2319327731092437,"i pNi,t´1q BT"
ESTIMATORS,0.23361344537815126,"i pNi,t´1q"
ESTIMATORS,0.23529411764705882,"hpNi,t´1q"
ESTIMATORS,0.23697478991596638,"Figure 1: Graphical representation of the
pessimistic ˆµipNi,t´1q and the optimistic ˇµT"
ESTIMATORS,0.23865546218487396,"i pNi,t´1q estimators."
ESTIMATORS,0.24033613445378152,"As supported by intuition, we observe that the estima-
112"
ESTIMATORS,0.24201680672268908,"tor is affected by a negative bias that is represented by
113"
ESTIMATORS,0.24369747899159663,"ˆ⇣ipnq that vanishes as n Ñ 8 under Assumption 2.1
114"
ESTIMATORS,0.2453781512605042,"with a rate that depends on the increment functions
115"
ESTIMATORS,0.24705882352941178,"γip¨q. Considering also the term ˆβipnq and recalling
116"
ESTIMATORS,0.24873949579831933,"that hpnq “ Opnq, under Assumption 2.2, the overall
117"
ESTIMATORS,0.2504201680672269,"concentration rate is Opn´1{2 ` cTn´βq.
118"
ESTIMATORS,0.25210084033613445,"Optimistic Estimator
The optimistic estimator
119 ˇµT"
ESTIMATORS,0.253781512605042,"i pNi,t´1q is a positively biased estimation of µipTq
120"
ESTIMATORS,0.25546218487394956,"obtained assuming that function µip¨q linearly in-
121"
ESTIMATORS,0.2571428571428571,"creases up to time T.
This corresponds to the
122"
ESTIMATORS,0.25882352941176473,"maximum value admissible under Assumption 2.1
123"
ESTIMATORS,0.2605042016806723,"(due to the Concavity constraint). The estimator is
124"
ESTIMATORS,0.26218487394957984,"constructed by adding to the pessimistic estimator
125"
ESTIMATORS,0.2638655462184874,"ˆµipNi,t´1q an estimate of the increment occurring
126"
ESTIMATORS,0.26554621848739496,"in the next step up to T. The latter uses the last
127"
ESTIMATORS,0.2672268907563025,"2hpNi,t´1q samples to obtain an upper bound of such
128"
ESTIMATORS,0.2689075630252101,"growth thanks to the concavity assumption, formally:
129 ˇµT"
ESTIMATORS,0.27058823529411763,"i pNi,t´1q :“ ˆµipNi,t´1q ` ÿ"
ESTIMATORS,0.2722689075630252,"pj,kqPSi,t"
ESTIMATORS,0.2739495798319328,pT ´ jq xj ´ xk
ESTIMATORS,0.27563025210084036,"hpNi,t´1q2 .
(3)"
ESTIMATORS,0.2773109243697479,"The estimator displays the following concentration guarantee.
130"
ESTIMATORS,0.27899159663865547,Lemma 3.2 (Concentration of ˇµT
ESTIMATORS,0.280672268907563,"i ). Under Assumption 2.1, for every a ° 0, simultaneously for every
131"
ESTIMATORS,0.2823529411764706,"arm i P JKK and number of pulls n P J0, TK, with probability at least 1 ´ 2TKe´a{10 it holds that:
132 ˇβT"
ESTIMATORS,0.28403361344537814,i pnq § ˇµT
ESTIMATORS,0.2857142857142857,i pnq ´ µipnq § ˇβT
ESTIMATORS,0.28739495798319326,i pnq ` ˇ⇣T
ESTIMATORS,0.28907563025210087,"i pnq,
(4)"
ESTIMATORS,0.2907563025210084,where ˇβT
ESTIMATORS,0.292436974789916,i pnq :“ σ¨pT ´n`hpnq´1q b
ESTIMATORS,0.29411764705882354,"a
hpnq3 and ˇ⇣T"
ESTIMATORS,0.2957983193277311,i pnq :“ 1
ESTIMATORS,0.29747899159663865,"2p2T ´n`hpnq´1q γipn´2hpnq`1q.
133"
ESTIMATORS,0.2991596638655462,"Differently from the pessimistic estimation, the optimistic one displays a positive vanishing bias
134 ˇ⇣T"
ESTIMATORS,0.30084033613445377,"i pnq. Under Assumption 2.2, we observe that the overall concentration rate is OpTn´3{2`cTn´βq.
135"
ESTIMATORS,0.3025210084033613,"4
Optimistic Algorithm: Rising Upper Confidence Bound Exploration
136"
ESTIMATORS,0.3042016806722689,"In this section, we introduce and analyze Rising Upper Confidence Bound Exploration
137"
ESTIMATORS,0.3058823529411765,"(R-UCBE) an optimistic error probability minimization algorithm for the SRB setting with a ﬁxed
138"
ESTIMATORS,0.30756302521008405,"budget. The algorithm explores by means of a UCB-like approach and, for this reason, makes use of
139"
ESTIMATORS,0.3092436974789916,the optimistic estimator ˇµT
ESTIMATORS,0.31092436974789917,"i plus a bound to account for the uncertainty of the estimation.8
140"
ESTIMATORS,0.3126050420168067,"Algorithm The algorithm, whose pseudo-code is reported in Algorithm 1, requires as input an
141"
ESTIMATORS,0.3142857142857143,"exploration parameter a • 0, the window size "" P p0, 1{2q, the time budget T, and the number of
142"
ESTIMATORS,0.31596638655462184,"arms K. At ﬁrst, it initializes to zero the counters Ni,0, and sets to `8 the upper bounds BT"
ESTIMATORS,0.3176470588235294,"i pNi,0q
143"
ESTIMATORS,0.31932773109243695,"of all the arms (Line 2). Subsequently, at each time t P JTK, the algorithm selects the arm It with the
144"
ESTIMATORS,0.32100840336134456,"largest upper conﬁdence bound (Line 4):
145"
ESTIMATORS,0.3226890756302521,It P arg max iPJKK BT
ESTIMATORS,0.3243697478991597,"i pNi,t´1q :“ ˇµT"
ESTIMATORS,0.32605042016806723,"i pNi,t´1q ` ˇβT"
ESTIMATORS,0.3277310924369748,"i pNi,t´1q,
(5)"
ESTIMATORS,0.32941176470588235,"with:
ˇβT"
ESTIMATORS,0.3310924369747899,"i pNi,t´1q :“ σ ¨ pT ´ Ni,t´1 ` hpNi,t´1q ´ 1q"
ESTIMATORS,0.33277310924369746,"c
a
hpNi,t´1q3 ,
(6)"
ESTIMATORS,0.334453781512605,"8In R-UCBE, the choice of considering the optimistic estimator is natural and obliged since the pessimistic
estimator is affected by negative bias and cannot be used to deliver optimistic estimates."
ESTIMATORS,0.33613445378151263,where ˇβT
ESTIMATORS,0.3378151260504202,"i pNi,t´1q represents the exploration bonus (a graphical representation is reported in Figure 1).
146"
ESTIMATORS,0.33949579831932775,"Once the arm is chosen, the algorithm plays it and observes the feedback xt (Line 5). Then, the
147"
ESTIMATORS,0.3411764705882353,optimistic estimate ˇµT
ESTIMATORS,0.34285714285714286,"ItpNIt,tq and the exploration bonus ˇβT"
ESTIMATORS,0.3445378151260504,"ItpNIt,tq of the selected arm It are updated
148"
ESTIMATORS,0.346218487394958,"(Lines 8-9). The procedure is repeated until the algorithm reaches the time budget T. The ﬁnal
149"
ESTIMATORS,0.34789915966386553,recommendation of the best arm is performed using the last computed values of the bounds BT
ESTIMATORS,0.3495798319327731,"i pNi,T q,
150"
ESTIMATORS,0.35126050420168065,"returning the arm ˆI˚pTq corresponding to the largest upper conﬁdence bound (Line 12).
151"
ESTIMATORS,0.35294117647058826,"Bound on the Error Probability of R-UCBE We now provide bounds on the error probability for
152"
ESTIMATORS,0.3546218487394958,"R-UCBE. We start with a general analysis that makes no assumption on the increments γip¨q and, then,
153"
ESTIMATORS,0.3563025210084034,"we provide a more explicit result under Assumption 2.2. The general result is formalized as follows.
154"
ESTIMATORS,0.35798319327731093,"Theorem 4.1. Under Assumption 2.1, let a˚ be the largest positive value of a satisfying:
155 T ´ ÿ"
ESTIMATORS,0.3596638655462185,i‰i˚pT q
ESTIMATORS,0.36134453781512604,"yipaq • 1,
(7)"
ESTIMATORS,0.3630252100840336,"where for every i P JKK, yipaq is the largest integer for which it holds:
156"
ESTIMATORS,0.36470588235294116,"Tγiptp1 ´ 2""qyuq
looooooooomooooooooon pAq ` 2Tσ"
ESTIMATORS,0.3663865546218487,"c
a
t""yu3
loooooomoooooon pBq"
ESTIMATORS,0.3680672268907563,"• ∆ipTq.
(8)"
ESTIMATORS,0.3697478991596639,"If a˚ exists, then for every a P r0, a˚s the error probability of R-UCBE is bounded by:
157"
ESTIMATORS,0.37142857142857144,"eT pR-UCBEq § 2TK exp ´ ´ a 10 ¯ .
(9)"
ESTIMATORS,0.373109243697479,"Some comments are in order. First, a˚ is deﬁned implicitly, depending on the constants σ, T, the
158"
ESTIMATORS,0.37478991596638656,"increments γip¨q, and the suboptimality gaps ∆ipTq. In principle, there might exist no a˚ ° 0
159"
ESTIMATORS,0.3764705882352941,"fulﬁlling condition in Equation (7) (this can happen, for instance, when the budget T is not large
160"
ESTIMATORS,0.37815126050420167,"enough), and, in such a case, we are unable to provide theoretical guarantees on the error probability
161"
ESTIMATORS,0.3798319327731092,"of R-UCBE. Second, the result presented in Theorem 4.1 holds for generic increasing and concave
162"
ESTIMATORS,0.3815126050420168,"expected reward functions. This result shows that, as expected, the error probability decreases when
163"
ESTIMATORS,0.3831932773109244,"the exploration parameter a increases. However, this behavior stops when we reach the threshold a˚.
164"
ESTIMATORS,0.38487394957983195,"Intuitively, the value of a˚ sets the maximum amount of exploration we should use for learning.
165"
ESTIMATORS,0.3865546218487395,"Under Assumption 2.2, i.e., using the knowledge on the increment γip¨q upper bound, we derive a
166"
ESTIMATORS,0.38823529411764707,"result providing conditions on the time budget T under which a˚ exists and an explicit value for a˚.
167"
ESTIMATORS,0.3899159663865546,"Corollary 4.2. Under Assumptions 2.1 and 2.2, if the time budget T satisﬁes:
168 T • $
’
& ’
% ´ c"
ESTIMATORS,0.3915966386554622,"1
β p1 ´ 2""q´1 pH1,1{βpTqq ` pK ´ 1q"
ESTIMATORS,0.39327731092436974,"¯
β
β´1
if β P p1, 3{2q
´ c"
ESTIMATORS,0.3949579831932773,"2
3 p1 ´ 2""q´ 2"
ESTIMATORS,0.39663865546218485,"3 β pH1,2{3pTqq ` pK ´ 1q ¯3"
ESTIMATORS,0.3983193277310924,"if β P r3{2, `8q"
ESTIMATORS,0.4,",
(10)"
ESTIMATORS,0.4016806722689076,"there exists a˚ ° 0 deﬁned as:
169 a˚ “"
ESTIMATORS,0.40336134453781514,"$
’
’
& ’
’
%"
ESTIMATORS,0.4050420168067227,"✏3
4σ2 ˆ´"
ESTIMATORS,0.40672268907563025,T 1´1{β´pK´1q
ESTIMATORS,0.4084033613445378,"H1,1{βpT q ¯β"
ESTIMATORS,0.41008403361344536,"´ cp1 ´ 2""q´β ˙2"
ESTIMATORS,0.4117647058823529,"if β P p1, 3{2q"
ESTIMATORS,0.4134453781512605,"✏3
4σ2 ˆ´"
ESTIMATORS,0.4151260504201681,T 1{3´pK´1q
ESTIMATORS,0.41680672268907565,"H1,2{3pT q ¯3{2"
ESTIMATORS,0.4184873949579832,"´ cp1 ´ 2""q´β ˙2"
ESTIMATORS,0.42016806722689076,"if β P r3{2, `8q ,"
ESTIMATORS,0.4218487394957983,"where H1,⌘pTq :“ ∞"
ESTIMATORS,0.4235294117647059,"i‰i˚pT q 1
∆⌘"
ESTIMATORS,0.42521008403361343,"i pT q for ⌘° 0. Then, for every a P r0, a˚s, the error probability of
170"
ESTIMATORS,0.426890756302521,"R-UCBE is bounded by:
171"
ESTIMATORS,0.42857142857142855,eT pR-UCBEq § 2TK exp ´ ´ a 10 ¯ .
ESTIMATORS,0.43025210084033616,"First of all, we notice that the error probability eT pR-UCBEq presented in Theorem 4.2 holds under
172"
ESTIMATORS,0.4319327731092437,"the condition that the time budget T fulﬁlls Equation (10). We defer a more detailed discussion
173"
ESTIMATORS,0.4336134453781513,"on this condition to Remark 5.1, where we show that the existence of a ﬁnite value of T fulﬁlling
174"
ESTIMATORS,0.43529411764705883,"Equation (10) is ensured under mild conditions.
175"
ESTIMATORS,0.4369747899159664,"Let us remark that term H1,⌘pTq characterizes the complexity of the SRB setting, corresponding to
176"
ESTIMATORS,0.43865546218487395,"term H1 of Audibert et al. (2010) for the classical BAI problem when ⌘“ 2. As expected, in the
177"
ESTIMATORS,0.4403361344537815,"small-β regime (i.e., β P p1, 3{2s), looking at the dependence of H1,1{βpTq on β, we realize that
178"
ESTIMATORS,0.44201680672268906,"Algorithm 1: R-UCBE.
Input :Time budget T, Number of arms K,"
ESTIMATORS,0.4436974789915966,"Window size "", Exploration parameter a"
ESTIMATORS,0.44537815126050423,"1 Initialize Ni,0 “ 0,"
BT,0.4470588235294118,"2
BT"
BT,0.44873949579831934,"i p0q “ `8, @i P JKK"
FOR T P JTK DO,0.4504201680672269,3 for t P JTK do
COMPUTE IT P ARG MAXIPJKK BT,0.45210084033613446,"4
Compute It P arg maxiPJKK BT"
COMPUTE IT P ARG MAXIPJKK BT,0.453781512605042,"i pNi,t´1q"
PULL ARM IT AND OBSERVE XT,0.45546218487394957,"5
Pull arm It and observe xt
6
NIt,t – NIt,t´1 ` 1"
PULL ARM IT AND OBSERVE XT,0.45714285714285713,"7
Ni,t – Ni,t´1,
@i ‰ It"
PULL ARM IT AND OBSERVE XT,0.4588235294117647,"8
Update ˇµT"
PULL ARM IT AND OBSERVE XT,0.46050420168067224,"ItpNIt,tq"
PULL ARM IT AND OBSERVE XT,0.46218487394957986,"9
Update ˇβT"
PULL ARM IT AND OBSERVE XT,0.4638655462184874,"ItpNIt,tq"
COMPUTE BT,0.46554621848739497,"10
Compute BT"
COMPUTE BT,0.4672268907563025,"ItpNIt,tq “ ˇµT"
COMPUTE BT,0.4689075630252101,"ItpNIt,tq` ˇβT"
COMPUTE BT,0.47058823529411764,"ItpNIt,tq"
END,0.4722689075630252,11 end
END,0.47394957983193275,12 Recommend pI˚pTq P arg maxiPJKK BT
END,0.4756302521008403,"i pNi,T q"
END,0.4773109243697479,"Algorithm 2: R-SR.
Input :Time budget T, Number of arms K,"
END,0.4789915966386555,"Window size """
END,0.48067226890756304,"1 Initialize t – 1, N0 “ 0, X0 “ JKK"
END,0.4823529411764706,2 for j P JK ´ 1K do
END,0.48403361344537815,"3
for i P Xj´1 do"
END,0.4857142857142857,"4
for l P JNj´1 ` 1, NjK do"
PULL ARM I AND OBSERVE XT,0.48739495798319327,"5
Pull arm i and observe xt
6
t – t ` 1"
END,0.4890756302521008,"7
end"
END,0.4907563025210084,"8
Update ˆµipNjq"
END,0.492436974789916,"9
end"
END,0.49411764705882355,"10
Deﬁne Ij P arg miniPXj´1 ˆµipNjq"
END,0.4957983193277311,"11
Update Xj “ Xj´1 ztIju"
END,0.49747899159663866,12 end
END,0.4991596638655462,13 Recommend pI˚pTq P XK´1 (unique)
END,0.5008403361344538,"the complexity of a problem decreases as the parameter β increases. Indeed, the larger β, the faster
179"
END,0.5025210084033613,"the expected reward reaches a stationary behavior. Nevertheless, even in the large-β regime (i.e.,
180"
END,0.5042016806722689,"β ° 3{2), the complexity of the problem is governed by H1,2{3pTq, leading to an error probability
181"
END,0.5058823529411764,"larger than the corresponding one for BAI in standard bandits (Audibert et al., 2010). This can be
182"
END,0.507563025210084,"explained by the fact that R-UCBE uses the optimistic estimator that, as shown in Section 3, enjoys a
183"
END,0.5092436974789916,"slower concentration rate compared to the standard sample mean, even for stationary bandits.
184"
END,0.5109243697478991,"This two-regime behavior has an interesting interpretation when comparing Corollary 4.2 with
185"
END,0.5126050420168067,"Theorem 4.1. Indeed, β “ 3{2 is the break-even threshold in which the two terms of the l.h.s. of
186"
END,0.5142857142857142,"Equation (8) have the same convergence rate. Speciﬁcally, the term pAq takes into account the
187"
END,0.5159663865546219,"expected rewards growth (i.e., the bias in the estimators), while pBq considers the uncertainty in
188"
END,0.5176470588235295,"the estimations of the R-UCBE algorithm (i.e., the variance). Intuitively, when the expected reward
189"
END,0.519327731092437,"function displays a slow growth (i.e., γipnq § cn´β with β † 3{2), the bias term pAq dominates
190"
END,0.5210084033613446,"the variance term pBq and the value of a˚ changes accordingly. Conversely, when the variance term
191"
END,0.5226890756302521,"pBq is the dominant one (i.e., γipnq § cn´β with β ° 3{2), the threshold a˚ is governed by the
192"
END,0.5243697478991597,"estimation uncertainty, being the bias negligible.
193"
END,0.5260504201680672,"As common in optimistic algorithms for BAI (Audibert et al., 2010), setting a theoretically sound
194"
END,0.5277310924369748,"value of exploration parameter a (i.e., computing a˚), requires additional knowledge of the setting,
195"
END,0.5294117647058824,"namely the complexity index H1,⌘pTq.9 In the next section, we propose an algorithm that relaxes this
196"
END,0.5310924369747899,"requirement.
197"
END,0.5327731092436975,"5
Phase-Based Algorithm: Rising Successive Rejects
198"
END,0.534453781512605,"In this section, we introduce the Rising Successive Rejects (R-SR), a phase-based solution
199"
END,0.5361344537815126,"inspired by the one proposed by Audibert et al. (2010), which overcomes the drawback of R-UCBE of
200"
END,0.5378151260504201,"requiring knowledge of H1,⌘pTq.
201"
END,0.5394957983193277,"Algorithm R-SR, whose pseudo-code is reported in Algorithm 2, takes as input the time budget T
202"
END,0.5411764705882353,"and the number of arms K. At ﬁrst, it initializes the set of the active arms X0 with all the available
203"
END,0.5428571428571428,"arms (Line 1). This set will contain the arms that are still eligible candidates to be recommended.
204"
END,0.5445378151260504,"The entire process proceeds through K ´ 1 phases. More speciﬁcally, during the jth phase, the arms
205"
END,0.5462184873949579,"still remaining in the active arms set Xj´1 are played (Line 5) for Nj ´ Nj´1 times each, where:
206 Nj :“"
END,0.5478991596638656,"R
1
logpKq"
END,0.5495798319327732,"T ´ K
K ` 1 ´ j V"
END,0.5512605042016807,",
(11)"
END,0.5529411764705883,"and logpKq :“
1
2 ` ∞K i“2 1"
END,0.5546218487394958,"i . At the end of each phase, the arm with the smallest value of the
207"
END,0.5563025210084034,"pessimistic estimator ˆµipNjq is discarded from the set of active arms (Line 11). At the end of the
208"
END,0.5579831932773109,"pK ´ 1qth phase, the algorithm recommends the (unique) arm left in XK´1 (Line 13).
209"
END,0.5596638655462185,9We defer the empirical study of the sensitivity of a to Section 7.
END,0.561344537815126,"It is worth noting that R-SR makes use of the pessimistic estimator ˆµipnq. Even if both estimators
210"
END,0.5630252100840336,"deﬁned in Section 3 are viable for R-SR, the choice of using the pessimistic estimator is justiﬁed
211"
END,0.5647058823529412,"by its better concentration rate Opn´1{2q compared to that of the optimistic estimator OpTn´3{2q,
212"
END,0.5663865546218487,"being n § T (see Section 3).
213"
END,0.5680672268907563,"Note that the phase lengths are the ones adopted by Audibert et al. (2010). This choice allows
214"
END,0.5697478991596638,"us to provide theoretical results without requiring domain knowledge (still under a large enough
215"
END,0.5714285714285714,"budget). An optimized version of Nj may be derived assuming full knowledge of the gaps ∆ipTq,
216"
END,0.573109243697479,"but, unfortunately, such a hypothetical approach would have similar drawbacks as R-UCBE.
217"
END,0.5747899159663865,"Bound on the Error Probability of R-SR The following theorem provides the guarantee on the
218"
END,0.5764705882352941,"error probability for the R-SR algorithm.
219"
END,0.5781512605042017,"Theorem 5.1. Under Assumptions 2.1 and 2.2, if the time budget T satisﬁes:
220 T • 2"
END,0.5798319327731093,"β`1
β´1 c"
END,0.5815126050420169,"1
β´1 logpKq"
END,0.5831932773109244,"β
β´1 max"
END,0.584873949579832,"iPJ2,KK ! i"
END,0.5865546218487395,"β
β´1 ∆piqpTq´
1
β´1 )"
END,0.5882352941176471,",
(12)"
END,0.5899159663865546,"then, the error probability of R-SR is bounded by:"
END,0.5915966386554622,eT pR-SRq § KpK ´ 1q
EXP,0.5932773109243697,"2
exp ˆ ´ """
EXP,0.5949579831932773,"8σ2 ¨
T ´ K
logpKqH2pTq ˙ ,"
EXP,0.5966386554621849,where H2pTq :“ maxiPJKK 
EXP,0.5983193277310924,i∆piqpTq´2(
EXP,0.6,and logpKq “ 1
EXP,0.6016806722689075,2 ` ∞K i“2 1
EXP,0.6033613445378151,"i .
221"
EXP,0.6050420168067226,"Similar to the R-UCBE, the complexity of the problem is characterized by term H2pTq that, for the
222"
EXP,0.6067226890756302,"standard MAB setting, reduces to the H2 term of Audibert et al. (2010). Furthermore, when the
223"
EXP,0.6084033613445378,"condition of Equation (12) on the time budget T is satisﬁed, the error probability coincides with that
224"
EXP,0.6100840336134454,"of the SR algorithm for standard MABs (apart for constant terms). The following remark elaborates
225"
EXP,0.611764705882353,"on the conditions of Equations (10) and (12) about the minimum requested time budget.
226"
EXP,0.6134453781512605,"Remark 5.1 (About the minimum time budget T). To satisfy the eT bounds presented in Corollary 4.2
227"
EXP,0.6151260504201681,"and Theorem 5.1, R-UCBE and R-SR require the conditions provided by Equations (10) and (12)
228"
EXP,0.6168067226890757,"about the time budget T, respectively. First, let us notice that if the suboptimal arms converge to
229"
EXP,0.6184873949579832,"an expected reward different from that of the optimal arm as T Ñ `8, it is always possible to
230"
EXP,0.6201680672268908,"ﬁnd a ﬁnite value of T † `8 such that these conditions are fulﬁlled. Formally, assume that there
231"
EXP,0.6218487394957983,"exists T0 † `8 and that for every T • T0 we have that for all suboptimal arms i ‰ i˚pTq it holds
232"
EXP,0.6235294117647059,"that ∆ipTq • ∆8 ° 0. In such a case, the l.h.s. of Equations (10) and (12) are upper bounded by
233"
EXP,0.6252100840336134,"a function of ∆8 and are independent on T. Instead, if a suboptimal arm converges to the same
234"
EXP,0.626890756302521,"expected reward as the optimal arm when T Ñ `8, the identiﬁcation problem is more challenging
235"
EXP,0.6285714285714286,"and, depending on the speed at which the two arms converge as a function of T, might slow down the
236"
EXP,0.6302521008403361,"learning process arbitrarily. This should not surprise as the BAI problem becomes non-learnable
237"
EXP,0.6319327731092437,"even in standard (stationary) MABs when multiple optimal arms are present (Heide et al., 2021).
238"
LOWER BOUND,0.6336134453781512,"6
Lower Bound
239"
LOWER BOUND,0.6352941176470588,"In this section, we investigate the complexity of the BAI problem for SRBs with a ﬁxed budget.
240"
LOWER BOUND,0.6369747899159663,"Minimum time budget T We show that, under Assumptions 2.1 and 2.2, any algorithm requires a
241"
LOWER BOUND,0.6386554621848739,"minimum time budget T to be guaranteed to identify the optimal arm, even in a deterministic setting.
242"
LOWER BOUND,0.6403361344537815,"Theorem 6.1. For every algorithm A, there exists a deterministic SRB satisfying Assumptions 2.1
243"
LOWER BOUND,0.6420168067226891,"and 2.2 such that the optimal arm i˚pTq cannot be identiﬁed for some time budgets T unless:
244"
LOWER BOUND,0.6436974789915967,"T • H1,1{pβ´1qpTq “ ÿ"
LOWER BOUND,0.6453781512605042,i‰i˚pT q 1 ∆ipTq
LOWER BOUND,0.6470588235294118,"1
β´1 .
(13)"
LOWER BOUND,0.6487394957983194,"Theorem 6.1 formalizes the intuition that any of the suboptimal arms must be pulled a sufﬁcient
245"
LOWER BOUND,0.6504201680672269,"number of times to ensure that, if pulled further, it cannot become the optimal arm. It is worth
246"
LOWER BOUND,0.6521008403361345,"comparing this bound on the time budget with the corresponding conditions on the minimum
247"
LOWER BOUND,0.653781512605042,"time budget requested by Equations (10) and (12) for R-UCBE and R-SR, respectively. Regarding
248"
LOWER BOUND,0.6554621848739496,"R-UCBE, we notice that the minimum admissible time budget in the small-β regime is of order
249"
LOWER BOUND,0.6571428571428571,"H1,1{βpTqβ{pβ´1q which is larger than term H1,1{pβ´1qpTq of Equation (13).10 Similarly, in the
250"
LOWER BOUND,0.6588235294117647,10See Lemma D.12.
LOWER BOUND,0.6605042016806723,"Error Probability eT p¨q
Time Budget T"
LOWER BOUND,0.6621848739495798,"SRB
1
4 exp ˜"
LOWER BOUND,0.6638655462184874,"´
8T
σ2 ∞"
LOWER BOUND,0.6655462184873949,"i‰i˚pT q 1
∆2"
LOWER BOUND,0.6672268907563025,"i pT q ¸
ÿ"
LOWER BOUND,0.66890756302521,"i‰i˚pT q 1 ∆ipTq 1
β´1"
LOWER BOUND,0.6705882352941176,"R-UCBE
2 T K exp ´ ´ a 10 ¯"
LOWER BOUND,0.6722689075630253,"$
’
’
’
’
&"
LOWER BOUND,0.6739495798319328,"’
’
’
’
% ˆ c"
LOWER BOUND,0.6756302521008404,"1
β p1 ´ 2""q´1 ˆ
ÿ"
LOWER BOUND,0.6773109243697479,"i‰i˚pT q 1 ∆1{β i
pTq ˙"
LOWER BOUND,0.6789915966386555,` pK ´ 1q
LOWER BOUND,0.680672268907563,"˙
β
β´1"
LOWER BOUND,0.6823529411764706,"if β P p1, 3{2q ˆ c"
LOWER BOUND,0.6840336134453782,"2
3 p1 ´ 2""q´ 2 3 β ˆ
ÿ"
LOWER BOUND,0.6857142857142857,"i‰i˚pT q 1 ∆2{3 i
pTq ˙"
LOWER BOUND,0.6873949579831933,` pK ´ 1q ˙3
LOWER BOUND,0.6890756302521008,"if β P r3{2, `8q"
LOWER BOUND,0.6907563025210084,"R-SR
KpK ´ 1q"
EXP,0.692436974789916,"2
exp ¨"
EXP,0.6941176470588235,"˚
˝´ "" 8σ2 T ´ K"
EXP,0.6957983193277311,logpKq max iPJKK ! i∆´2
EXP,0.6974789915966386,"piq pTq ) ˛ ‹‚
2"
EXP,0.6991596638655462,"1`β
β´1 c"
EXP,0.7008403361344537,"1
β´1 logpKq"
EXP,0.7025210084033613,"β
β´1 max"
EXP,0.704201680672269,"iPJ2,KK ! i"
EXP,0.7058823529411765,"β
β´1 ∆piqpTq´
1
β´1 )"
EXP,0.7075630252100841,"Table 1: Bounds on the time budget and error probability: lower for the setting and upper for the
algorithms."
EXP,0.7092436974789916,"large-β regime (i.e., β ° 3{2), the R-UCBE requirement is of order H1,2{3pTq3 • H1,2pTq which
251"
EXP,0.7109243697478992,"is larger than the term of Theorem 6.1 since 1{pβ ´ 1q † 2. Concerning R-SR, it is easy to show
252"
EXP,0.7126050420168067,"that H1,1{pβ´1qpTq « maxiPJ2,KK i∆piqpTq´1{pβ´1q, apart from logarithmic terms, by means of
253"
EXP,0.7142857142857143,"the argument provided by (Audibert et al., 2010, Section 6.1). Thus, up to logarithmic terms,
254"
EXP,0.7159663865546219,"Equation (12) provides a tight condition on the minimum budget.
255"
EXP,0.7176470588235294,"Error Probability Lower Bound We now present a lower bound on the error probability.
256"
EXP,0.719327731092437,"Theorem 6.2. For every algorithm A run with a time budget T fulﬁlling Equation (13), there exists a
257"
EXP,0.7210084033613445,"SRB satisfying Assumptions 2.1 and 2.2 such that the error probability is lower bounded by:
258"
EXP,0.7226890756302521,eT pAq • 1
EXP,0.7243697478991596,4 exp ˆ
EXP,0.7260504201680672,"´
8T
σ2H1,2pTq ˙"
EXP,0.7277310924369748,",
where H1,2pTq :“ ÿ"
EXP,0.7294117647058823,"i‰i˚pT q 1
∆2"
EXP,0.7310924369747899,i pTq.
EXP,0.7327731092436974,"Some comments are in order. First, we stated the lower bound for the case in which the minimum
259"
EXP,0.7344537815126051,"time budget satisﬁes the inequality of Theorem 6.1, which is a necessary condition for identifying the
260"
EXP,0.7361344537815127,"optimal arm. Second, the lower bound on the error probability matches, up to logarithmic factors,
261"
EXP,0.7378151260504202,"that of our R-SR, suggesting the superiority of this algorithm compared to R-UCBE. Finally, provided
262"
EXP,0.7394957983193278,"that the identiﬁability condition of Equation (13), such a result corresponds to that of the standard
263"
EXP,0.7411764705882353,"(stationary) MABs (Audibert et al., 2010; Kaufmann et al., 2016). A summary of all the bounds
264"
EXP,0.7428571428571429,"provided in the paper is presented in Table 1.
265"
NUMERICAL VALIDATION,0.7445378151260504,"7
Numerical Validation
266"
NUMERICAL VALIDATION,0.746218487394958,"In this section, we provide a numerical validation of R-UCBE and R-SR. We compare them with
267"
NUMERICAL VALIDATION,0.7478991596638656,"state-of-the-art bandit baselines designed for stationary and non-stationary BAI in a synthetic setting,
268"
NUMERICAL VALIDATION,0.7495798319327731,"and we evaluate the sensitivity of R-UCBE to its exploration parameter a. Additional details about the
269"
NUMERICAL VALIDATION,0.7512605042016807,"experiments presented in this section are available in Appendix G. Additional experimental results on
270"
NUMERICAL VALIDATION,0.7529411764705882,"both synthetic settings and in a real-world experiment are available in Appendix H.11
271"
NUMERICAL VALIDATION,0.7546218487394958,"Baselines We compare our algorithms against a wide range of solutions for BAI:
272"
NUMERICAL VALIDATION,0.7563025210084033,"• RR: uniformly pulls all the arms until the budget ends in a round-robin fashion and, in the end,
273"
NUMERICAL VALIDATION,0.7579831932773109,"makes a recommendation based on the empirical mean of their reward over the collected samples;
274"
NUMERICAL VALIDATION,0.7596638655462185,"• RR-SW: makes use of the same exploration strategy as RR to pull arms but makes a recommendation
275"
NUMERICAL VALIDATION,0.761344537815126,"based on the empirical mean over the last ""T"
NUMERICAL VALIDATION,0.7630252100840336,"K collected samples from an arm.12
276"
NUMERICAL VALIDATION,0.7647058823529411,"• UCB-E and SR (Audibert et al., 2010): algorithms for the stationary BAI problem;
277"
NUMERICAL VALIDATION,0.7663865546218488,"• Prob-1 (Abbasi-Yadkori et al., 2018): an algorithm dealing with the adversarial BAI setting;
278"
NUMERICAL VALIDATION,0.7680672268907563,"• ETC and Rest-Sure (Cella et al., 2021): algorithms developed for the decreasing loss BAI setting.13
279"
NUMERICAL VALIDATION,0.7697478991596639,"The hyperparameters required by the above methods have been set as prescribed in the original papers.
280"
NUMERICAL VALIDATION,0.7714285714285715,"For both our algorithms and RR-SW, we set "" “ 0.25.
281"
NUMERICAL VALIDATION,0.773109243697479,"11The code to run the experiments is available in the supplementary material. It will be published in a public
repository conditionally to the acceptance of the paper."
NUMERICAL VALIDATION,0.7747899159663866,"12The formal description of this baseline, as well as its theoretical analysis, is provided in Appendix E.
13This problem is equivalent to ours, given a linear transformation of the reward."
NUMERICAL VALIDATION,0.7764705882352941,"0
50
100
150
200
250
300
0 0.2 0.4 0.6 0.8 1"
NUMERICAL VALIDATION,0.7781512605042017,Number of pulls n
NUMERICAL VALIDATION,0.7798319327731092,Expected reward µipnq
NUMERICAL VALIDATION,0.7815126050420168,"1
2
3
4
5"
NUMERICAL VALIDATION,0.7831932773109244,"Figure 2: Expected values µipnq
for the arms of the synthetic set-
ting."
NUMERICAL VALIDATION,0.7848739495798319,"102
102.5
103
103.5 0 0.2 0.4 0.6 0.8 1"
NUMERICAL VALIDATION,0.7865546218487395,Time Budgets T
NUMERICAL VALIDATION,0.788235294117647,Empirical Error eT
NUMERICAL VALIDATION,0.7899159663865546,"R-UCBE
R-SR"
NUMERICAL VALIDATION,0.7915966386554621,"RR
RR-SW
UCB-E
SR
Prob-1
ETC
Rest-Sure"
NUMERICAL VALIDATION,0.7932773109243697,"Figure 3: Empirical error rate for
the synthetically generated set-
ting (100 runs, mean ˘ 95% c.i.)."
NUMERICAL VALIDATION,0.7949579831932773,"100
120
140
160
180
200
220
240 0 0.2 0.4 0.6 0.8 1"
NUMERICAL VALIDATION,0.7966386554621848,Time Budgets T
NUMERICAL VALIDATION,0.7983193277310925,Empirical Error eT
NUMERICAL VALIDATION,0.8,"a “ a˚{50
a “ a˚{10
a “ a˚"
NUMERICAL VALIDATION,0.8016806722689076,a “ 10a˚
NUMERICAL VALIDATION,0.8033613445378152,a “ 50a˚
NUMERICAL VALIDATION,0.8050420168067227,"Figure 4: Empirical error rate
for the R-UCBE at different a
(1000 runs, mean ˘ 95% c.i.)."
NUMERICAL VALIDATION,0.8067226890756303,"Setting To assess the quality of the recommendation ˆI˚pTq provided by our algorithms, we consider
282"
NUMERICAL VALIDATION,0.8084033613445378,"a synthetic SRB setting with K “ 5 and σ “ 0.01. Figure 2 shows the evolution of the expected
283"
NUMERICAL VALIDATION,0.8100840336134454,"values of the arms w.r.t. the number of pulls. In this setting, the optimal arm changes depending
284"
NUMERICAL VALIDATION,0.8117647058823529,"on whether T P r1, 185s or T P p185, `8q. Thus, when the time budget is close to that value, the
285"
NUMERICAL VALIDATION,0.8134453781512605,"problem is more challenging since the optimal and second-best arms expected rewards are close to
286"
NUMERICAL VALIDATION,0.8151260504201681,"each other. For this reason, the BAI algorithms are less likely to provide a correct recommendation
287"
NUMERICAL VALIDATION,0.8168067226890756,"than for time budgets for which the two expected rewards are well separated. We compare the
288"
NUMERICAL VALIDATION,0.8184873949579832,"analyzed algorithms A in terms of empirical error eT pAq (the smaller, the better), i.e., the empirical
289"
NUMERICAL VALIDATION,0.8201680672268907,"counterpart of eT pAq averaged over 100 runs, considering time budgets T P r100, 3200s.
290"
NUMERICAL VALIDATION,0.8218487394957983,"Results The empirical error probability provided by the analyzed algorithms in the synthetically
291"
NUMERICAL VALIDATION,0.8235294117647058,"generated setting is presented in Figure 3. We report with a dashed vertical blue line at T “ 185, i.e.,
292"
NUMERICAL VALIDATION,0.8252100840336134,"the budgets after which the optimal arm no longer changes. Before such a budget, all the algorithms
293"
NUMERICAL VALIDATION,0.826890756302521,"provide large errors (i.e., ¯eT pAq ° 0.2). However, R-UCBE outperforms the others by a large margin,
294"
NUMERICAL VALIDATION,0.8285714285714286,"suggesting that an optimistic estimator might be advantageous when the time budget is small. Shortly
295"
NUMERICAL VALIDATION,0.8302521008403362,"after T “ 185, R-UCBE starts providing the correct suggestion consistently. R-SR begins to identify
296"
NUMERICAL VALIDATION,0.8319327731092437,"the optimal arm (i.e., with ¯eT pR-SRq † 0.05) for time budgets T ° 1000. Nonetheless, both
297"
NUMERICAL VALIDATION,0.8336134453781513,"algorithms perform signiﬁcantly better than the baseline algorithms used for comparison.
298"
NUMERICAL VALIDATION,0.8352941176470589,"Sensitivity Analysis for the Exploration Parameter of R-UCBE We perform a sensitivity analysis
299"
NUMERICAL VALIDATION,0.8369747899159664,"on the exploration parameter a of R-UCBE. Such a parameter should be set to a value less or equal
300"
NUMERICAL VALIDATION,0.838655462184874,"to a˚, and the computation of the latter is challenging. We tested the sensitivity of R-UCBE to this
301"
NUMERICAL VALIDATION,0.8403361344537815,"hyperparameter by looking at the error probability for a P ta˚{50, a˚{10, a˚, 10a˚, 50a˚u. Figure 4
302"
NUMERICAL VALIDATION,0.8420168067226891,"shows the empirical errors of R-UCBE with different parameters a, where the blue dashed vertical
303"
NUMERICAL VALIDATION,0.8436974789915966,"line denotes the last time the optimal arm changes over the time budget. It is worth noting how, even
304"
NUMERICAL VALIDATION,0.8453781512605042,"in this case, we have two signiﬁcantly different behaviors before and after such a time. Indeed, if
305"
NUMERICAL VALIDATION,0.8470588235294118,"T § 185, we have that a misspeciﬁcation with larger values than a˚ does not signiﬁcantly impact
306"
NUMERICAL VALIDATION,0.8487394957983193,"the performance of R-UCBE, while smaller values slightly decrease the performance. Conversely,
307"
NUMERICAL VALIDATION,0.8504201680672269,"for T ° 185 learning with different values of a seems not to impact the algorithm performance
308"
NUMERICAL VALIDATION,0.8521008403361344,"signiﬁcantly. This corroborates the previous results about the competitive performance of R-UCBE.
309"
DISCUSSION AND CONCLUSIONS,0.853781512605042,"8
Discussion and Conclusions
310"
DISCUSSION AND CONCLUSIONS,0.8554621848739495,"This paper introduces the BAI problem with a ﬁxed budget for the Stochastic Rising Bandits setting.
311"
DISCUSSION AND CONCLUSIONS,0.8571428571428571,"Notably, such setting models many real-world scenarios in which the reward of the available options
312"
DISCUSSION AND CONCLUSIONS,0.8588235294117647,"increases over time, and the interest is on the recommendation of the one having the largest expected
313"
DISCUSSION AND CONCLUSIONS,0.8605042016806723,"rewards after the time budget has elapsed. In this setting, we presented two algorithms, namely
314"
DISCUSSION AND CONCLUSIONS,0.8621848739495799,"R-UCBE and R-SR providing theoretical guarantees on the error probability. R-UCBE is an optimistic
315"
DISCUSSION AND CONCLUSIONS,0.8638655462184874,"algorithm requiring an exploration parameter whose optimal value requires prior information on the
316"
DISCUSSION AND CONCLUSIONS,0.865546218487395,"setting. Conversely, R-SR is a phase-based solution that only requires the time budget to run. We
317"
DISCUSSION AND CONCLUSIONS,0.8672268907563025,"established lower bounds for the error probability an algorithm suffers in such a setting, which is
318"
DISCUSSION AND CONCLUSIONS,0.8689075630252101,"matched by our R-SR, up to logarithmic factors. Furthermore, we showed how a requirement on the
319"
DISCUSSION AND CONCLUSIONS,0.8705882352941177,"minimum time budget is unavoidable to ensure the identiﬁability of the optimal arm. Finally, we
320"
DISCUSSION AND CONCLUSIONS,0.8722689075630252,"validate the performance of the two algorithms in both synthetically generated and real-world settings.
321"
DISCUSSION AND CONCLUSIONS,0.8739495798319328,"A possible future line of research is to derive an algorithm balancing the tradeoff between theoretical
322"
DISCUSSION AND CONCLUSIONS,0.8756302521008403,"guarantees on the eT and the chance of providing such guarantees with lower time budgets.
323"
REFERENCES,0.8773109243697479,"References
324"
REFERENCES,0.8789915966386554,"Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020.
325"
REFERENCES,0.880672268907563,"Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
326"
REFERENCES,0.8823529411764706,"problem. Machine Learning, 47(2):235–256, 2002.
327"
REFERENCES,0.8840336134453781,"Sébastien Bubeck, Rémi Munos, and Gilles Stoltz. Pure exploration in multi-armed bandits problems.
328"
REFERENCES,0.8857142857142857,"In Proceedings of the Algorithmic Learning Theory (ALT), volume 5809, pages 23–37, 2009.
329"
REFERENCES,0.8873949579831932,"Cem Tekin and Mingyan Liu. Online learning of rested and restless bandits. IEEE Transaction on
330"
REFERENCES,0.8890756302521008,"Information Theory, 58(8):5588–5611, 2012.
331"
REFERENCES,0.8907563025210085,"Alberto Maria Metelli, Francesco Trovò, Matteo Pirola, and Marcello Restelli. Stochastic rising
332"
REFERENCES,0.892436974789916,"bandits. In Proceedings of the International Conference on Machine Learning (ICML), pages
333"
REFERENCES,0.8941176470588236,"15421–15457, 2022.
334"
REFERENCES,0.8957983193277311,"Chris Thornton, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Auto-weka: Combined
335"
REFERENCES,0.8974789915966387,"selection and hyperparameter optimization of classiﬁcation algorithms. In Proceedings of the ACM
336"
REFERENCES,0.8991596638655462,"(SIGKDD), pages 847–855, 2013.
337"
REFERENCES,0.9008403361344538,"Lars Kotthoff, Chris Thornton, Holger H Hoos, Frank Hutter, and Kevin Leyton-Brown. Auto-weka
338"
REFERENCES,0.9025210084033614,"2.0: Automatic model selection and hyperparameter optimization in weka. Journal of Machine
339"
REFERENCES,0.9042016806722689,"Learning Research, 18:1–5, 2017.
340"
REFERENCES,0.9058823529411765,"Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander
341"
REFERENCES,0.907563025210084,"Smola. Autogluon-tabular: Robust and accurate automl for structured data. arXiv preprint
342"
REFERENCES,0.9092436974789916,"arXiv:2003.06505, 2020.
343"
REFERENCES,0.9109243697478991,"Yang Li, Jiawei Jiang, Jinyang Gao, Yingxia Shao, Ce Zhang, and Bin Cui. Efﬁcient automatic
344"
REFERENCES,0.9126050420168067,"CASH via rising bandits. In Proceedings of the Conference on Artiﬁcial Intelligence (AAAI), pages
345"
REFERENCES,0.9142857142857143,"4763–4771, 2020.
346"
REFERENCES,0.9159663865546218,"Marc-André Zöller and Marco F Huber. Benchmark and survey of automated machine learning
347"
REFERENCES,0.9176470588235294,"frameworks. Journal of Artiﬁcial Intelligence Research, 70:409–472, 2021.
348"
REFERENCES,0.9193277310924369,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, and
349"
REFERENCES,0.9210084033613445,"Frank Hutter. Efﬁcient and robust automated machine learning. In Advances in Neural Information
350"
REFERENCES,0.9226890756302522,"Processing Systems (NeurIPS), pages 2962–2970, 2015.
351"
REFERENCES,0.9243697478991597,"Quanming Yao, Mengshuo Wang, Yuqiang Chen, Wenyuan Dai, Yu-Feng Li, Wei-Wei Tu, Qiang
352"
REFERENCES,0.9260504201680673,"Yang, and Yang Yu. Taking human out of learning applications: A survey on automated machine
353"
REFERENCES,0.9277310924369748,"learning. arXiv preprint arXiv:1810.13306, 2018.
354"
REFERENCES,0.9294117647058824,"Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren. Automated machine learning: methods, systems,
355"
REFERENCES,0.9310924369747899,"challenges. Springer Nature, 2019.
356"
REFERENCES,0.9327731092436975,"Marco Mussi, Davide Lombarda, Alberto Maria Metelli, Francesco Trovó, and Marcello Restelli.
357"
REFERENCES,0.934453781512605,"Arlo: A framework for automated reinforcement learning. Expert Systems with Applications, 224:
358"
REFERENCES,0.9361344537815126,"119883, 2023.
359"
REFERENCES,0.9378151260504202,"Leonardo Cella, Massimiliano Pontil, and Claudio Gentile. Best model identiﬁcation: A rested
360"
REFERENCES,0.9394957983193277,"bandit formulation. In Proceedings of the International Conference on Machine Learning (ICML),
361"
REFERENCES,0.9411764705882353,"volume 139, pages 1362–1372, 2021.
362"
REFERENCES,0.9428571428571428,"Hoda Heidari, Michael J. Kearns, and Aaron Roth. Tight policy regret bounds for improving and
363"
REFERENCES,0.9445378151260504,"decaying bandits. In Proceeding of the International Joint Conference on Artiﬁcial Intelligence
364"
REFERENCES,0.946218487394958,"(AISTATS), pages 1562–1570, 2016.
365"
REFERENCES,0.9478991596638655,"Benny Lehmann, Daniel Lehmann, and Noam Nisan. Combinatorial auctions with decreasing
366"
REFERENCES,0.9495798319327731,"marginal utilities. In ACM Proceedings of the Conference on Electronic Commerce (EC), pages
367"
REFERENCES,0.9512605042016806,"18–28, 2001.
368"
REFERENCES,0.9529411764705882,"Jean-Yves Audibert, Sébastien Bubeck, and Rémi Munos. Best arm identiﬁcation in multi-armed
369"
REFERENCES,0.9546218487394958,"bandits. In Proceedings of the Conference on Learning Theory (COLT), pages 41–53, 2010.
370"
REFERENCES,0.9563025210084034,"Rianne De Heide, James Cheshire, Pierre Ménard, and Alexandra Carpentier. Bandits with many
371"
REFERENCES,0.957983193277311,"optimal arms. In Advances in Neural Information Processing Systems (NeurIPS), pages 22457–
372"
REFERENCES,0.9596638655462185,"22469, 2021.
373"
REFERENCES,0.9613445378151261,"Emilie Kaufmann, Olivier Cappé, and Aurélien Garivier. On the complexity of best-arm identiﬁcation
374"
REFERENCES,0.9630252100840336,"in multi-armed bandit models. Journal of Machine Learning Research, 17:1:1–1:42, 2016.
375"
REFERENCES,0.9647058823529412,"Yasin Abbasi-Yadkori, Peter L. Bartlett, Victor Gabillon, Alan Malek, and Michal Valko. Best of
376"
REFERENCES,0.9663865546218487,"both worlds: Stochastic & adversarial best-arm identiﬁcation. In Proceedings of the Conference on
377"
REFERENCES,0.9680672268907563,"Learning Theory (COLT), volume 75, pages 918–949, 2018.
378"
REFERENCES,0.9697478991596639,"Victor Gabillon, Mohammad Ghavamzadeh, and Alessandro Lazaric. Best arm identiﬁcation: A
379"
REFERENCES,0.9714285714285714,"uniﬁed approach to ﬁxed budget and ﬁxed conﬁdence.
In Advances in Neural Information
380"
REFERENCES,0.973109243697479,"Processing Systems (NeurIPS), pages 3221–3229, 2012.
381"
REFERENCES,0.9747899159663865,"Aurélien Garivier and Emilie Kaufmann. Optimal best arm identiﬁcation with ﬁxed conﬁdence. In
382"
REFERENCES,0.9764705882352941,"Proceedings of the Conference on Learning Theory (COLT), volume 49, pages 998–1027, 2016.
383"
REFERENCES,0.9781512605042016,"Alexandra Carpentier and Andrea Locatelli. Tight (lower) bounds for the ﬁxed budget best arm iden-
384"
REFERENCES,0.9798319327731092,"tiﬁcation bandit problem. In Proceedings of the 29th Conference on Learning Theory, volume 49,
385"
REFERENCES,0.9815126050420168,"pages 590–604, 2016.
386"
REFERENCES,0.9831932773109243,"Yonatan Mintz, Anil Aswani, Philip Kaminsky, Elena Flowers, and Yoshimi Fukuoka. Nonstationary
387"
REFERENCES,0.984873949579832,"bandits with habituation and recovery dynamics. Operations Research, 68(5):1493–1516, 2020.
388"
REFERENCES,0.9865546218487395,"Julien Seznec, Pierre Ménard, Alessandro Lazaric, and Michal Valko. A single algorithm for both
389"
REFERENCES,0.9882352941176471,"restless and rested rotting bandits. In Proceedings of the International Conference on Artiﬁcial
390"
REFERENCES,0.9899159663865547,"Intelligence and Statistics (AISTATS), volume 108, pages 3784–3794, 2020.
391"
REFERENCES,0.9915966386554622,"Nir Levine, Koby Crammer, and Shie Mannor. Rotting bandits. In Advances in Neural Information
392"
REFERENCES,0.9932773109243698,"Processing Systems (NeurIPS), pages 3074–3083, 2017.
393"
REFERENCES,0.9949579831932773,"Julien Seznec, Andrea Locatelli, Alexandra Carpentier, Alessandro Lazaric, and Michal Valko.
394"
REFERENCES,0.9966386554621849,"Rotting bandits are no harder than stochastic ones. In Proceedings of the International Conference
395"
REFERENCES,0.9983193277310924,"on Artiﬁcial Intelligence and Statistics, volume 89, pages 2564–2572, 2019.
396"
