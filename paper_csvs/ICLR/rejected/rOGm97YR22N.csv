Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0024509803921568627,"Recurrent neural networks (RNNs) with continuous-time hidden states are a natural
ﬁt for modeling irregularly-sampled time series. These models, however, face
difﬁculties when the input data possess long-term dependencies. We prove that
similar to standard RNNs, the underlying reason for this issue is the vanishing
or exploding of the gradient during training. This phenomenon is expressed
by the ordinary differential equation (ODE) representation of the hidden state,
regardless of the ODE solver’s choice. We provide a solution by equipping arbitrary
continuous-time networks with a memory compartment separated from its time-
continuous state. This way, we encode a continuous-time dynamical ﬂow within the
RNN, allowing it to respond to inputs arriving at arbitrary time-lags while ensuring
a constant error propagation through the memory path. We call these models
Mixed-Memory-RNNs (mmRNNs). We experimentally show that Mixed-Memory-
RNNs outperform recently proposed RNN-based counterparts on non-uniformly
sampled data with long-term dependencies."
INTRODUCTION,0.004901960784313725,"1
INTRODUCTION"
INTRODUCTION,0.007352941176470588,"Irregularly-sampled time series, routine data streams in medical and business settings, can be modeled
effectively by a time-continuous version of recurrent neural networks (RNNs). These class of RNNs
whose hidden states are identiﬁed by ordinary differential equations, termed an ODE-RNN (Rubanova
et al., 2019), provably suffer from the vanishing and exploding gradient problem (see Figure 1, the
ﬁrst two models), when trained by reverse-mode automatic differentiation (Rumelhart et al., 1986;
Pontryagin, 2018)."
INTRODUCTION,0.00980392156862745,"An elegant solution to the vanishing gradient phenomenon (Hochreiter, 1991; Bengio et al., 1994),
which results in difﬁculties in learning long-term dependencies in RNNs, is memory gating. This
technique, ﬁrst published in form of the long short term memory networks (LSTM) (Hochreiter &
Schmidhuber, 1997), enforce a constant error propagation through the hidden states, learn to forget,
and disentangle the hidden states (memory) from their output states. Despite becoming the standard
choice in modeling regularly-sampled temporal dynamics, memory gated RNNs similar to other
discretized RNN models, face difﬁculties when the time-gap between the observations are irregular."
INTRODUCTION,0.012254901960784314,"In this paper, we propose a compromise to design a novel recurrent neural network algorithm
that simultaneously enjoys the approximation capability of ODE-RNNs in modeling irregularly-
sampled time series and capability of learning long-term dependencies of the memory gated RNN’s
computational graph. To perform this, we let a memory cell compute its implicit memory mechanism
by their typical (input, forget, and output) gates while receiving their feedback inputs from a time-
continuous output state representation. This way, we incorporate a continuous-time dynamical ﬂow
within a gated recurrent module (e.g., an LSTM), enabling cells to respond to data arriving at arbitrary
time-lags, while avoiding the vanishing gradient problem. Since these models interweave continuous
ﬂows with discrete memory mechanisms, we call them mixed-memory RNNs a (See Figure 1, the
last model)."
INTRODUCTION,0.014705882352941176,"We compare mmRNNs to standard and advanced continuous-time RNN variants, on a set of synthetic
and real-world sparse time-series tasks, and discover consistently better performance."
INTRODUCTION,0.01715686274509804,Under review as a conference paper at ICLR 2021 !
INTRODUCTION,0.0196078431372549,"""#!
""ℎ(!"")"
INTRODUCTION,0.022058823529411766,"#!
#""
##$%
##
'(!)"
INTRODUCTION,0.024509803921568627,"((! + *!)
ℎ!"
INTRODUCTION,0.02696078431372549,"ℎ""
ℎ#$% ℎ# 1 0"
INTRODUCTION,0.029411764705882353,"""#!
""ℎ(!"")"
INTRODUCTION,0.031862745098039214,"((! + *!)
ℎ!"
INTRODUCTION,0.03431372549019608,"ℎ""
ℎ#$%
ℎ# 1 0"
INTRODUCTION,0.03676470588235294,ODE RNN
INTRODUCTION,0.0392156862745098,RNN Decay
INTRODUCTION,0.041666666666666664,"""#!
""+(!"")"
INTRODUCTION,0.04411764705882353,"((! + *!)
ℎ! ℎ"" ℎ#$% ℎ# 1 0"
INTRODUCTION,0.04656862745098039,ODE LSTM
INTRODUCTION,0.049019607843137254,"""""
""#$%"
INTRODUCTION,0.051470588235294115,"""#
""!
,(! + *!)"
INTRODUCTION,0.05392156862745098,Exploding gradient region
INTRODUCTION,0.056372549019607844,Exploding gradient region
INTRODUCTION,0.058823529411764705,Exploding gradient region
INTRODUCTION,0.061274509803921566,"Vanish 
region"
INTRODUCTION,0.06372549019607843,"Vanish 
region"
INTRODUCTION,0.0661764705882353,"Vanish 
region mmRNN"
INTRODUCTION,0.06862745098039216,"Figure 1: Magnitude of the states’ er-
ror propagation in time-continuous re-
current neural networks gives rise to the
vanishing or exploding of the gradient
(ﬁrst two models). mmRNNs are a so-
lution to keep a constant gradient ﬂow
to avoid these phenomena in modeling
irregularly sampled data."
INTRODUCTION,0.07107843137254902,"To put this in context, we ﬁrst theoretically prove that
the class of ODE-RNNs suffers from the exploding and
vanishing gradient problem, making them unable to learn
long-term dependencies efﬁciently. We show that learning
ODE-RNNs by the adjoint method (Chen et al., 2018)
does not help with this problem. As a solution, we propose
mmRNNs, a continuous-time recurrent model capable of
learning long-term dependencies of irregularly-sampled
time-series."
BACKGROUND,0.07352941176470588,"2
BACKGROUND"
BACKGROUND,0.07598039215686274,"ODE-RNNs Instead of explicitly deﬁning a state update
function, ODE-RNNs identify an ordinary differential
equations in the following form (Funahashi & Nakamura,
1993):
∂h"
BACKGROUND,0.0784313725490196,"∂t = fθ(xt+T , ht, T) −τh,
(1)"
BACKGROUND,0.08088235294117647,"where xt is the input sequence, ht is an RNN’s hidden
state, and τ is a dampening factor. The time-lag T speciﬁes
at what times the inputs xt have been sampled."
BACKGROUND,0.08333333333333333,"ODE-RNNs were recently rediscovered (Rubanova et al.,
2019) and have shown promise in approximating
irregularly-sampled data, thanks to the implicit deﬁnition
of time in their resulting dynamical systems. ODE-RNNs
can be trained by backpropagation through time (BPTT)
(Rumelhart et al., 1986; Werbos, 1988; 1990) through
ODE solvers, or by treating the solver as a black-box and
apply the adjoint method (Pontryagin, 2018) to gain mem-
ory efﬁciency (Chen et al., 2018). In Section 3, we show
this family of recurrent networks faces difﬁculty to learn
long-term dependencies."
BACKGROUND,0.0857843137254902,"Long Short-term Memory
LSTMs (Hochreiter & Schmidhuber, 1997) express their dis-
cretized hidden states as a pair (ct, ht) and its update function realizes a mapping as follows:
fθ(xt+1, (ct, ht), 1) 7→(ct+1, ht+1)."
BACKGROUND,0.08823529411764706,"LSTMs demonstrate great performance on learning equidistant streams of data (Greff et al., 2016),
however similar to other discrete-state RNNs, they are puzzled with the events arriving in-between
observations. In Section 4, we introduce a continuous-time long short-term memory algorithm to
tackle this."
BACKGROUND,0.09068627450980392,"3
ODE-RNNS SUFFER FROM VANISHING OR EXPLODING GRADIENT."
BACKGROUND,0.09313725490196079,"In this section, we show that ODE-RNNs trained via backpropagation through time (BPTT) are
susceptible to vanishing and exploding gradients. We also illustrate that the adjoint method is not
immune to these gradient issues. We ﬁrst formally deﬁne the gradient problems of the RNNs, and
progressively construct Theorem 1."
BACKGROUND,0.09558823529411764,Gradient propagation in recurrent networks.
BACKGROUND,0.09803921568627451,"Hochreiter (Hochreiter, 1991) discovered that the error-ﬂow in the BPTT algorithm realizes a power
series that determines the effectiveness of the learning process (Hochreiter, 1991; Hochreiter &
Schmidhuber, 1997; Bengio et al., 1994; Pascanu et al., 2013). In particular, the state-previous state
Jacobian of an RNN:
∂ht+T (xt+T , ht, T)"
BACKGROUND,0.10049019607843138,"∂ht
,
(2)"
BACKGROUND,0.10294117647058823,"governs whether the propagated error exponentially grows (explodes), exponentially vanishes, or
stays constant. Formally:"
BACKGROUND,0.1053921568627451,Under review as a conference paper at ICLR 2021
BACKGROUND,0.10784313725490197,"Deﬁnition 1 (Vanishing or exploding gradient). Let ht+T = f(xt+T , ht, T) be a recurrent neural
network, then we say unit i of the network f suffers from a vanishing gradient if for some small ε > 0"
BACKGROUND,0.11029411764705882,"it hold that

PN
j=1
∂hi
t+T
∂hj
t"
BACKGROUND,0.11274509803921569,"< 1 −ε, where N is the dimension of the hidden state ht and super-script"
BACKGROUND,0.11519607843137254,vi denotes the i-th entry of the vector v. We say unit i of the network f suffers from an exploding
BACKGROUND,0.11764705882352941,"gradient if it holds that
 PN
j=1
∂hi
t+T
∂hj
t"
BACKGROUND,0.12009803921568628,"> 1. We say the whole network f suffers from a vanishing or
respectively exploding gradient problem if the above condition hold for some of its units."
BACKGROUND,0.12254901960784313,"The factor ε in Deﬁnition 1 is essential as Gers et al. (Gers et al., 2000) observed that a learnable
vanishing factor in the form of a forget-gate signiﬁcantly beneﬁts the learning capabilities of RNNs,
i.e., the network can learn to forget. Note that a RNN can simultaneously suffer from a vanishing and
an exploding gradient by the deﬁnition above."
BACKGROUND,0.125,"Now, consider an ODE-RNN given by Eq. 1 is implemented either by an Explicit Euler discretization
or by a Runge-Kutta method (Runge, 1895; Dormand & Prince, 1980). We can formulate their
state-previous state Jacobian in the following two lemmas:"
BACKGROUND,0.12745098039215685,"Lemma 1. Let ˙h = fθ(x, h, T) −hτ be an ODE-RNN. Then state-previous state Jacobian of the"
BACKGROUND,0.12990196078431374,explicit Euler is given by the following equation: ∂ht+T
BACKGROUND,0.1323529411764706,"∂ht
= I + T ∂f"
BACKGROUND,0.13480392156862744,"∂h

h=ht −τTI."
BACKGROUND,0.13725490196078433,"Lemma 2. Let ˙h = fθ(x, h, T) −hτ be an ODE-RNN. Then state-previous state Jacobian of the
Runge-Kutta method is given by ∂ht+T"
BACKGROUND,0.13970588235294118,"∂ht
= I + T PM
j=1 bi
∂f
∂h

h=Ki
−τTI., , where PM
j=1 bi = 1"
BACKGROUND,0.14215686274509803,and some Ki.
BACKGROUND,0.14460784313725492,"The proofs for Lemma 1 and Lemma 2 is provided in the supplements. Consequently, we have:"
BACKGROUND,0.14705882352941177,"Theorem 1. (ODE-RNNs suffer from a vanish or exploding gradient) Let ˙h = fθ(x, h, T) −hτ,
and ht the RNN obtained by simulating the ODE by a solver based on the explicit Euler or Runge-
Kutta method. Then the RNN suffers from a vanishing and exploding gradient problem, except for
parameter conﬁgurations which give the non-trainable constant dynamics fθ(h, x) = 0, and cases
where fθ(h, x) is constant, for a particular input sequence x and θ."
BACKGROUND,0.14950980392156862,"The proof is given in full in the supplementary materials. A brief outline of the proof is as follow:
First, we look a the special cases of ∂f"
BACKGROUND,0.15196078431372548,"∂h −τ = 0. While such f would enforce a constant error
propagation by making the Jacobians equal to the identity, it also removes all dynamics from the ODE
state. In other words, it would operate the ODE as a memory element. Intuitively, any interesting
function fθ pushes the Jacobians away from the identity matrix, creating a vanishing or exploding
gradient depending on fθ."
BACKGROUND,0.15441176470588236,Our proof sketch implies a minor but nonetheless interesting statement:
BACKGROUND,0.1568627450980392,Corollary 1. A ODE-RNN with identity Jacobian matrix expresses the trivial dynamics ∂f
BACKGROUND,0.15931372549019607,∂h −τ = 0.
BACKGROUND,0.16176470588235295,The proof follows directly by Lemma 1 and 2.
BACKGROUND,0.1642156862745098,"Corollary 1 suggests that well-conditioned gradients of an ODE-RNN can negatively impact the
modeling capacity of these models and prevent them from learning the dynamics of the training data.
In particular, this result indicates that strong long-term learning results of ODE-based architectures
from the literature may be attributed to a better initialization, i.e., with stable gradients being only
present at the beginning of the training.
Theorem 2. (ODE-RNNs suffer from vanishing/exploding gradients regardless of their choice
of ODE-solver) Let ˙h = fθ(x, h, T) −hτ, with fθ being uniformly Lipschitz continuous. Moreover,
let h(t) be the solution of the initial value problem with initial state h0. Then, the gradients ∂h(T )"
BACKGROUND,0.16666666666666666,"∂h0 ,
i.e, the Jacobian of the ODE state at time T w.r.t. the initial state h0, can vanish and explode, except
for parameter conﬁgurations which give rise to the non-trainable constant dynamics fθ(h, x) = 0,
and cases where fθ(h, x) constant, for a particular input sequence x and parameters θ."
BACKGROUND,0.16911764705882354,"The proof is given in full in the supplementary materials. A brief outline of the proof is as follow:
We start by approximating the initial-value problem by an explicit Euler method with a uniform
step-size. We then let the step-size approach zero which due to the Picard–Lindelöf theorem, makes"
BACKGROUND,0.1715686274509804,Under review as a conference paper at ICLR 2021
BACKGROUND,0.17401960784313725,the series converge to the true solution of the ODE. Based on bounds on ∂f
BACKGROUND,0.17647058823529413,"∂h, we can obtain bounds of
the gradients in the limit, which can vanish or explode depending on fθ."
BACKGROUND,0.17892156862745098,"Note that the Theorems 1 and 2 hold independently of the used RNN architecture. In particular, we
can entail the following statement:
Corollary 2. ODE-combined-with-GRU and ODE-combined-with-LSTM as presented in (Rubanova
et al., 2019), also suffer from vanish or exploding gradients."
BACKGROUND,0.18137254901960784,The proof follows by applying the Theorems 1 and 2 with a GRU and LSTM.
BACKGROUND,0.18382352941176472,"Does the adjoint method solve the vanishing gradient problem? Adjoint sensitivity method
(Pontryagin, 2018) allows for performing memory-efﬁcient reverse-mode automatic differentia-
tion for training neural networks with their hidden states deﬁned by ODEs (Chen et al., 2018). x1 x2 x1 x2 x1 x2 x1 x2"
BACKGROUND,0.18627450980392157,"a) Continuous vector ﬁeld
b) Discrete ODE-solver trajectory"
BACKGROUND,0.18872549019607843,"c) Adjoint time-reversed vector ﬁeld
d) Adjoint ODE-solver trajectory"
BACKGROUND,0.19117647058823528,"Figure 2: The adjoint method makes numerical
error when computing the gradients. a) Continu-
ous vector ﬁeld implied by an ODE. b) Numerical
ODE-solvers realize a discrete trajectory on the
vector ﬁeld. c) The adjoint ODE creates a time-
reversed vector ﬁeld. d) Discrete trajectory of the
adjoint ODE-solver diverges from the trajectory of
the forward simulation due to discretization and
numerical imprecision."
BACKGROUND,0.19362745098039216,"The method, however, possesses lossy reverse-
mode integration steps, as it forgets the com-
puted steps during the forward-pass (Zhuang
et al., 2020). Consequently, at each reverse-
mode step, the backward gradient pass diverges
from the true forward pass (Zhuang et al., 2020;
Gholami et al., 2019)."
BACKGROUND,0.19607843137254902,"This is because the auxiliary differential equa-
tion in the adjoint sensitivity method, a(t), still
contains state-dependent components at each
reverse-step, which depends on the historical
values of the hidden states’ gradient. In the
extreme case, reverse-steps completely diverge
from the hidden states of the forward-time solu-
tion, resulting in incorrect gradients. Therefore,
both vanilla BPTT and the adjoint method face
difﬁculties for learning long-term dependencies.
n the next section, we propose a solution."
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.19852941176470587,"4
MIXED-MEMORY
RECURRENT ARCHITECTURES"
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.20098039215686275,"Instead of having a single state vector ht that
is processed by the discrete RNN and the time-
continuous ODE, our Mixed-Memory architec-
ture represents its hidden state by a pair (ct, ht). An update of the form
ct+1 = ct ⊙σ(gθ(ht) + bf) + zθ(ht),
(3)
governs the memory cell component ct of the mmRNN, where gθ and zθ are learnable gating functions
and bf a bias term. Although , the update in Eq. (3) appears similar to that of a vanilla LSTM and
GRU, there is one key difference: The gates of a LSTM/GRU is controlled by its previous hidden
state, whereas the gating of a mmRNN is deﬁned by a second process ht."
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.2034313725490196,"The second part of the hidden state ht is controlled by a continuous-time process, e.g. a Neural ODE,
initialized by the memory cell vector ct. In particular, the update function for ht is of the form
ht+1 = ODE-Solve(fθ, ct, T).
(4)"
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.20588235294117646,"The fundamental distinction of mmRNN to other continuous-time RNNs is the strict separation of
memory and time-continuous hidden states. In particular, the memory update in Eq. (4) ensures
a constant error propagation through ct, while arbitrary Neural ODEs process the state ht in a
time-countinuous fashion."
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.20833333333333334,"On the contrary, recurrent network variants such as continuous-time gated recurrent units (CT-GRU)
(Mozer et al., 2017), and GRU-D (Che et al., 2018) incorporate a time-dependent decay apparatus on
the state, while preserving the rest of the RNN architecture. This decaying memory originates the
vanishing factor during backward error-propagation, which results in difﬁculties in learning long-term
dependencies."
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.2107843137254902,Under review as a conference paper at ICLR 2021
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.21323529411764705,Algorithm 1 The mixed-memory RNN
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.21568627450980393,"Input: Datapoints and their timestamps {(xt, ti)}i=1...N
Parameters: Weights θ, output weight and bias Woutput, boutput
h0 = 0 {ODE state}
c0 = 0 {Memory cell}
for i = 1 . . . N do"
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.2181372549019608,"ci = ci−1 ⊙σ(gθ(hi−1, xi) + bf) + zθ(hi−1, xi) {Memory cell update}
hi = ODESolve(fθ, ci, ti −ti−1) {Time-continuous state update}
oi = hiWoutput + boutput
end for
Return {oi}i=1...N Time"
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.22058823529411764,Input events
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.22303921568627452,"Forward phase
prediction: 0"
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.22549019607843138,label: 1
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.22794117647058823,"Backpropagation phase
error: 1"
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.23039215686274508,Reversed time
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.23284313725490197,Teaching signal
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.23529411764705882,ODE-RNN
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.23774509803921567,ODE-LSTM
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.24019607843137256,"ODE-RNN
With vanishing gradient"
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.2426470588235294,"mmRNN
Immune to vanishing gradient 𝜃!""#$"
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.24509803921568626,Loss Surface
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.24754901960784315,"Loss Surface 𝜃%
𝜃% 𝜃!""#$ mmRNN"
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.25,"Figure 3: Left: Illustration of how vanishing gradients make RNN training difﬁcult when the data
express long-term dependencies. The prediction error can be thought of as a teaching signal indicating
how the dynamics should be changed to minimize the loss. The vanishing gradient of the ODE-RNN
makes the teaching signal weaker when propagating it back in time. Conversely, the teaching signal
stays near-constant in mmRNNs. Right: The resulting loss surfaces of the ODE-RNN is much steeper
than mmRNN, making the training difﬁcult."
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.25245098039215685,"Table 1: Change to the hidden states of an
RNN between two observations t and t + T"
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.2549019607843137,"Model
State between observation"
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.25735294117647056,"Standard RNN
ht
GRU-D
hte−T τ"
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.25980392156862747,"ODE-RNN
ODE-Solve(fθ, ht, T)
mmRNN
 
ct, ODE-Solve(fθ, ct, T)
"
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.2622549019607843,"Our mmRNNs are immune to this shortcoming. More
precisely, Table 1 lists how the transition of the hid-
den states between two observations of the mmRNN
differs from other architectures. Similar to the vanilla
LSTM and GRU for regularly sampled time-series,
we can ensure a near-constant error propagation at
the beginning of the training process with a proper
weight initialization."
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.2647058823529412,"Theorem 3. Let f with (ct+T , ht+T ) = f(xt+T , (ct, ht), T) be an mmRNN described by Algorithm
1. Moreover, we assume the weights of gθ and zθ to be initialized close to 0. Then, the units ct of
the state pair (ct, ht) do not suffer from a vanishing or exploding gradient at the beginning of the
training process."
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.26715686274509803,"The proof is given in full in the supplements. A brief outline: We assume that gθ and zθ are initialized
close to 0 and we are at the beginning of the training, thus these values have not changed much yet."
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.2696078431372549,"Consequently, we can neglect them and get
 PN
j=1
∂ci
t+T (xt+T ,(ct,ht),T ) ∂cj
t"
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.27205882352941174,"= σ(bf) ≈0.9943, which
is less than 1 (no exploding) but much greater than 0 (no vanishing). Note that exact value of the
Jacobian at the beginning of the training can be controlled by the bias term bf. If the underlying data
express very long-term dependencies, we can increase bf and bring the error ﬂow factor even closer
to 1."
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.27450980392156865,"The mmRNN can be viewed as a memory cell with gates controlled by a time-continuous process
realized by ordinary differential equations. Next, we evaluate the performance of mmRNNs in
multiple time-series prediction tasks."
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.2769607843137255,Under review as a conference paper at ICLR 2021
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.27941176470588236,"Empirical measurement of gradient norms. To emphasize the impact of our theoretical results,
we performed an experiment comparing the hidden state gradient norms of an ODE-RNN instance
compared to another ODE-RNN equipped with mmRNN for different sequence lengths (mean/std
over 3 initialization seeds). The results show (as proven in Theorem 1 and 2) that the gradients of
a standard ODE-RNN tend to exponentially increase with the length of the input sequence. This
gradient issue is signiﬁcantly improved when mmRNNs are used (See Table 2)."
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.2818627450980392,Table 2: Gradient Norms ODE-RNN vs mmRNN. (n =3)
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.28431372549019607,"Sequence length
5
10
25
50"
"MIXED-MEMORY
RECURRENT ARCHITECTURES",0.2867647058823529,"ODE-RNN
10.50 ± 5.47
38.75 ± 33.20
235.76 ± 383.04
1214.94 ± 3750.62
mmRNN
0.97 ± 0.01
0.96 ± 0.03
1.01 ± 0.10
1.25 ± 0.25"
EXPERIMENTAL EVALUATION,0.28921568627450983,"5
EXPERIMENTAL EVALUATION"
EXPERIMENTAL EVALUATION,0.2916666666666667,"We constructed quantitative settings with synthetic and real-world benchmarks. We assessed the
generalization performance of time-continuous RNN architectures on datasets that are deliberately
created to express long-term dependencies and are of irregularly-sampled nature. All code and data
are submitted as a supplement to our manuscript."
EXPERIMENTAL EVALUATION,0.29411764705882354,"Baselines. We compare mmRNN to a large variety of continuous-time RNNs introduced to model
irregularly-sampled data. This set includes RNNs with continuous-state dynamics such as ODE-RNN
(Rubanova et al., 2019) and CT-RNNs (Funahashi & Nakamura, 1993), state-decay mechanisms
such as CT-GRU (Mozer et al., 2017), RNN Decay (Rubanova et al., 2019), CT-LSTM (Mei &
Eisner, 2017), and GRU-D (Che et al., 2018), in addition to oscillatory models such as Phased-LSTM
(Neil et al., 2016), CoRNNs (Rusch & Mishra, 2021), iRNNs (Kag et al., 2019), and Lipschitz
RNNs (Erichson et al., 2021). Furthermore, we tested mmRNNs against intuitive time-gap modeling
approaches we built here, termed an augmented LSTM topology as well as reciprocal RNNs (Babaei
et al., 2010). Experimental settings are given Appendix."
SYNTHETIC BENCHMARK - BIT-STREAM SEQUENCE CLASSIFICATION,0.2965686274509804,"5.1
SYNTHETIC BENCHMARK - BIT-STREAM SEQUENCE CLASSIFICATION"
SYNTHETIC BENCHMARK - BIT-STREAM SEQUENCE CLASSIFICATION,0.29901960784313725,"Table 3: Bit-stream sequence classiﬁcation. Note: Test ac-
curacy (mean ± std, N = 5). While all RNNs can represent
the correct function, training is difﬁcult due to long-term
dependencies."
SYNTHETIC BENCHMARK - BIT-STREAM SEQUENCE CLASSIFICATION,0.3014705882352941,"Model
Dense
Event-based
encoding
encoding
ODE-RNN
50.47% ± 0.06
51.21% ± 0.37
CT-RNN
50.42% ± 0.12
50.79% ± 0.34
Augmented LSTM
100.00% ± 0.00
89.71% ± 3.48
CT-GRU
100.00% ± 0.00
61.36% ± 4.87
RNN Decay
60.28% ± 19.87
75.53% ± 5.28
Reciprocal RNN
100.00% ± 0.00
90.17% ± 0.69
GRU-D
100.00% ± 0.00
97.90% ± 1.71
PhasedLSTM
50.99% ± 0.76
80.29% ± 0.99
GRU-ODE
50.41% ± 0.40
52.52% ± 0.35
CT-LSTM
97.73% ± 0.08
95.09% ± 0.30
iRNN
49.99% ± 1.20
50.54% ± 0.94
coRNN
100.00% ± 0.00
52.89% ± 1.25
Lipschitz RNN
100.00% ± 0.00
52.84% ± 3.25"
SYNTHETIC BENCHMARK - BIT-STREAM SEQUENCE CLASSIFICATION,0.30392156862745096,"mmRNN (ours)
100.00% ± 0.00
98.89% ± 0.26"
SYNTHETIC BENCHMARK - BIT-STREAM SEQUENCE CLASSIFICATION,0.30637254901960786,"We formulated a modiﬁed time-series
variant of the XOR problem (Mar-
vin & Seymour, 1969). In particu-
lar, the model observes a block of bi-
nary data in the form of a bit-after-bit
time-series. The objective is to learn
an XOR function of the incoming bit-
stream. This setup is equivalent to the
binary-classiﬁcation of the input se-
quence, where the labels are obtained
by applying an XOR function to the
inputs."
SYNTHETIC BENCHMARK - BIT-STREAM SEQUENCE CLASSIFICATION,0.3088235294117647,"While any non-linear recurrent neural
network architecture can learn the cor-
rect function, training the network to
do so is non-trivial. For the model to
make an accurate prediction, all bits
in an upcoming chunk are required to
be taken into account. However, the
error signal is only provided after the
last bit is observed. Consequently, during learning, the prediction error needs to be propagated to the
ﬁrst input time-step to precisely capture the dependencies."
SYNTHETIC BENCHMARK - BIT-STREAM SEQUENCE CLASSIFICATION,0.3112745098039216,"We designed two modes, a dense encoding mode in which the input sequence is represented as a
regular, periodically sampled time-series, and an event-based mode which compresses the data into
irregularly sampled bit-streams, e.g., 1, 1, 1, 1 is encoded as (1, t = 4). (See Table 3). We observed
that a considerable number of RNNs face difﬁculties in modeling these tasks, even in dense-encoding"
SYNTHETIC BENCHMARK - BIT-STREAM SEQUENCE CLASSIFICATION,0.3137254901960784,Under review as a conference paper at ICLR 2021
SYNTHETIC BENCHMARK - BIT-STREAM SEQUENCE CLASSIFICATION,0.3161764705882353,"mode. In particular, ODE-RNNs, CT-RNNs, RNN-Decay, Phased-LSTM, and GRU-ODE could not
solve the XOR problem in the ﬁrst mode. Phased-LSTM and RNN-Decay improved their performance
in the second modality, whereas ODE-RNNs, CT-RNNs, and GRU-ODE still could not solve the task.
The core reason for their mediocre performance is the exploitation of the vanishing gradient problem
during training. The rest of the RNN variants (except CT-GRU) were successful in solving the task in
both modes, with mmRNN outperforming others in an event-based encoding scenario."
PERSON ACTIVITY RECOGNITION WITH IRREGULARLY SAMPLED TIME-SERIES,0.31862745098039214,"5.2
PERSON ACTIVITY RECOGNITION WITH IRREGULARLY SAMPLED TIME-SERIES"
PERSON ACTIVITY RECOGNITION WITH IRREGULARLY SAMPLED TIME-SERIES,0.32107843137254904,"Table 4: Per time-step classiﬁcation.
Person activity recognition. Test accu-
racy (mean ± std, N = 5)"
PERSON ACTIVITY RECOGNITION WITH IRREGULARLY SAMPLED TIME-SERIES,0.3235294117647059,"Model
Accuracy
ODE-RNN
80.43% ± 1.55
CT-RNN
83.65% ± 1.55
Augmented LSTM
84.11% ± 0.68
CT-GRU
79.48% ± 2.12
RNN Decay
62.89% ± 3.87
Reciprocal RNN
83.85% ± 0.45
GRU-D
83.57% ± 0.40
PhasedLSTM
83.33% ± 0.69
GRU-ODE
82.56% ± 2.63
CT-LSTM
84.13% ± 0.11
iRNN
74.56% ± 1.29
coRNN
78.89% ± 0.62
Lipschitz RNN
81.35% ± 0.60"
PERSON ACTIVITY RECOGNITION WITH IRREGULARLY SAMPLED TIME-SERIES,0.32598039215686275,"mmRNN (ours)
84.15% ± 0.33"
PERSON ACTIVITY RECOGNITION WITH IRREGULARLY SAMPLED TIME-SERIES,0.3284313725490196,"We consider the person activity recognition dataset from
the UCI repository (Dua & Graff, 2017). This task’s ob-
jective is to classify the current activity of a person, from
four inertial measurement sensors worn on the person’s
arms and feet. Even though the four sensors are mea-
sured at a ﬁxed period of 211ms, the random phase-shifts
between them creates an irregularly sampled time-series.
Rubanova et al. (Rubanova et al., 2019) showed that ODE-
based RNN architectures perform remarkably well on this
dataset. Here, we benchmarked the performance of the
mmRNN model against other variants."
PERSON ACTIVITY RECOGNITION WITH IRREGULARLY SAMPLED TIME-SERIES,0.33088235294117646,"This setting realizes a per-time-step classiﬁcation problem.
That is a new error signal is presented to the network at
every time-step which makes the vanishing gradient less
of an issue here. The results in Table 4 shows that the
mmRNN outperforms other RNN models on this dataset.
While the signiﬁcance of an evaluation on a single dataset
is limited, it demonstrates that the supreme generalization
ability of mmRNN architecture."
EVENT-BASED SEQUENTIAL MNIST,0.3333333333333333,"5.3
EVENT-BASED SEQUENTIAL MNIST"
EVENT-BASED SEQUENTIAL MNIST,0.33578431372549017,"Table 5: Event sequence classiﬁcation.
Irregular sequential MNIST. Test accu-
racy (mean ± std, N = 5)"
EVENT-BASED SEQUENTIAL MNIST,0.3382352941176471,"Model
Accuracy
ODE-RNN
72.41% ± 1.69
CT-RNN
72.05% ± 0.71
Augmented LSTM
82.10% ± 4.36
CT-GRU
87.51% ± 1.57
RNN Decay
88.93% ± 4.06
Reciprocal RNN
94.43% ± 0.23
GRU-D
95.44% ± 0.34
PhasedLSTM
86.79% ± 1.57
GRU-ODE
80.95% ± 1.52
CT-LSTM
94.84% ± 0.17
iRNN
95.51% ± 1.95
coRNN
94.44% ± 0.24
Lipschitz RNN
95.92% ± 0.16"
EVENT-BASED SEQUENTIAL MNIST,0.34068627450980393,"mmRNN (ours)
97.83% ± 0.37"
EVENT-BASED SEQUENTIAL MNIST,0.3431372549019608,"We determined a challenging sequence classiﬁcation task
by designing an event-based version for the sequential-
MNIST dataset. For doing this we followed the procedure
described below:"
EVENT-BASED SEQUENTIAL MNIST,0.34558823529411764,"1. Sequentialization + encoding long-term de-
pendencies
transform the 28-by-28 image into
a time-series of length 784
2. Compression + non-uniform sampling
en-
code binary time-series in a event-based format,
to get rid of consecutive occurrences of the same
binary value, e.g., 1, 1, 1, 1 is transformed to
(1, t = 4). (Read more about this experiment
in supplements)"
EVENT-BASED SEQUENTIAL MNIST,0.3480392156862745,"Using this sequentialization mechansim, we compress
the sequences from 784 to padded sequences of 256
irregularly-sampled datapoints. To perform well on this
task, RNNs must learn to store some information up to 256
time-steps, while taking the time-lags between them into account. Since an error signal is issued at
the end of the sequence, only an RNN model immune to vanishing gradients can achieve high-degrees
of accuracy."
EVENT-BASED SEQUENTIAL MNIST,0.35049019607843135,"Table 5 demonstrates that ODE-based RNN architectures, such as the ODE-RNN, CT-RNN, and the
GRU-ODE (De Brouwer et al., 2019) struggle to learn a high-ﬁdelity model of this dataset. On the
other hand, RNNs built based on a memory mechanism, such as the reciprocal RNN and GRU-D
(Che et al., 2018) perform reasonably well, while the performance of mmRNN surpasses others."
EVENT-BASED SEQUENTIAL MNIST,0.35294117647058826,Under review as a conference paper at ICLR 2021
EVENT-BASED SEQUENTIAL MNIST,0.3553921568627451,"5.4
WALKER2D KINEMATIC SIMULATION"
EVENT-BASED SEQUENTIAL MNIST,0.35784313725490197,"Table 6:
Per time-step regression.
Walker2d kinematic dataset. (mean ±
std, N = 5)"
EVENT-BASED SEQUENTIAL MNIST,0.3602941176470588,"Model
Square-error"
EVENT-BASED SEQUENTIAL MNIST,0.3627450980392157,"ODE-RNN
1.904 ± 0.061
CT-RNN
1.198 ± 0.004
Augmented LSTM
1.065 ± 0.006
CT-GRU
1.172 ± 0.011
RNN-Decay
1.406 ± 0.005
Reciprocal RNN
1.071 ± 0.009
GRU-D
1.090 ± 0.034
PhasedLSTM
1.063 ± 0.010
GRU-ODE
1.051 ± 0.018
CT-LSTM
1.014 ± 0.014
iRNN
1.732 ± 0.025
coRNN
3.241 ± 0.215
Lipschitz RNN
1.781 ± 0.013"
EVENT-BASED SEQUENTIAL MNIST,0.36519607843137253,"mmRNN (ours)
0.883 ± 0.014"
EVENT-BASED SEQUENTIAL MNIST,0.36764705882352944,"In this experiment, we evaluated how well mmRNNs can
model a physical dynamical system. To this end, we col-
lected simulation data of the Walker2d-v2 OpenAI
gym (Brockman et al., 2016) environment using a pre-
trained policy The objective of the model was to learn
the kinematic simulation of the MuJoCo physics engine
(Todorov et al., 2012) in an auto-regressive fashion and a
supervised learning modality. We increased the complex-
ity of this task by using the pre-trained policy at different
training stages (between 500 to 1200 Proximal Policy Op-
timization (PPO) iterations (Schulman et al., 2017)) and
overwrote 1% of all actions by random actions. Moreover,
we simulated frame-skips by removing 10% of the time-
steps. Consequently, the dataset is irregularly-sampled.
The results, shown in Table 6, indicate that mmRNNs
can capture the kinematic dynamics of the physics engine
better than other algorithms with a high margin."
RELATED WORKS,0.3700980392156863,"6
RELATED WORKS"
RELATED WORKS,0.37254901960784315,"Time-continuous RNNs
The notion of CT-RNNs (Funahashi & Nakamura, 1993) was introduced
around three decades ago. It is identical to the ODE-RNN architecture (Rubanova et al., 2019)
with an additional dampening factor τ. In our experiments, however, we observed a competitive
performance to our mmRNNs achieved by the GRU-D architecture (Che et al., 2018). GRU-D
encodes the dependence on the time-lags by a trainable decaying mechanism, similar to RNN-decay
(Rubanova et al., 2019). While this mechanism enables modeling irregularly sampled time-series, it
also introduces a vanishing gradient factor to the backpropagation path."
RELATED WORKS,0.375,"Similarly, CT-GRU (Mozer et al., 2017) adds multiple decay factors in the form of extra dimensions
to the RNN state. An attention mechanism inside the CT-GRU then selects which entry along the
decay dimension to use for computing the next state update. The CT-GRU aims to avoid vanishing
gradients by including a decay rate of 0, i.e., no decay at all. This mechanism nevertheless, fails as
illustrated in Table 3."
RELATED WORKS,0.37745098039215685,"Phased-LSTM (Neil et al., 2016) adds a learnable oscillator to LSTM. The oscillator modulates LSTM
to create dependencies on the elapsed-time, but also introduces a vanishing factor in its gradients."
RELATED WORKS,0.3799019607843137,"GRU-ODE (De Brouwer et al., 2019) modiﬁes the GRU (Chung et al., 2014) topology by incorporating
a continuous dynamical system. First, GRU is expressed as a discrete difference equation and then
transformed into a continuous ODE. This process makes the error-propagation time-dependent, i.e.,
the near-constant error propagation property of GRU is abolished."
RELATED WORKS,0.38235294117647056,"Lipschitz RNN (Erichson et al., 2021) constraints the hidden-to-hidden weight matrix of a continuous-
time RNN, such that the underlying dynamic system globally converges to a stable equilibrium. As
a result, Lipschitz RNNs cannot suffer from a exploding gradient problem. The constraint of the
weight matrix is realized efﬁciently using a symmetric skew decomposition (Wisdom et al., 2016)."
RELATED WORKS,0.38480392156862747,"Log-ODE method (Morrill et al., 2020) compresses the input time-series by time-continuous path
signatures (Friz & Victoir, 2010) before feeding them into ODE-RNNs. As the signatures are much
shorter than the original input sequence, the ODE-RNNs can learn long-term dependencies in the
original input sequence despite expressing a vanishing gradient."
RELATED WORKS,0.3872549019607843,"CT-LSTM (Mei & Eisner, 2017) combines the LSTM architecture with continuous-time neural
Hawkes processes. At each time-step, the RNN computes two alternative next state options of its
hidden state. The actual hidden state is then computed by interpolating between these two hidden
states depending on the elapsed time."
RELATED WORKS,0.3897058823529412,Under review as a conference paper at ICLR 2021
RELATED WORKS,0.39215686274509803,"coRNN (Rusch & Mishra, 2021) uses an implicit-explicit Euler discretization of a second-order ODE
modeling a controlled non-linear oscillator. The state-to-next state gradients of the resulting RNN are
bounded in both directions, mitigating explosion and vanishing effects."
RELATED WORKS,0.3946078431372549,"iRNN (Kag et al., 2019) parametrize a RNN by applying incremental updates to the steady-state of an
ODE. In the limit with inﬁnitely many updates between the state and the next state, iRNN express a
constant error propagation."
RELATED WORKS,0.39705882352941174,"Learning Irregularly-Sampled Data
Statistical (Pearson et al., 2003; Li & Marlin, 2016; Belletti
et al., 2016; Roy & Yan, 2020) and functional analysis (Foster, 1996; Amigó et al., 2012; Kowal
et al., 2019) tools have long been studying non-uniformly-spaced data. A natural ﬁt for this problem
is the use of time-continuous RNNs (Rubanova et al., 2019). We showed that although ODE-RNNs
are performant models in these domains, their performance tremendously drops when the incoming
samples have long-range dependencies. We solved this shortcoming by introducing mmRNNs."
RELATED WORKS,0.39950980392156865,"Learning Long-term Dependencies
The notorious question of vanishing/exploding gradient
(Hochreiter, 1991; Bengio et al., 1994) was identiﬁed as the core reason for RNNs’ lack of gener-
alizability when trained by gradient descent (Allen-Zhu & Li, 2019; Sherstinsky, 2020). Recent
studies used state-regularization (Wang & Niepert, 2019) and long memory stochastic processes
(Greaves-Tunnell & Harchaoui, 2019) to analyze long-range dependencies. Apart from the original
LSTM model (Hochreiter & Schmidhuber, 1997) and its variants (Greff et al., 2016) that solve the
problem in the context of RNNs, very few alternative researches exist (Chen et al., 2019)."
RELATED WORKS,0.4019607843137255,"As the class of CT RNNs become steadily popularized (Hasani et al., 2020a; Lechner et al., 2020a),
it is important to characterize them better (Lechner et al., 2019; Dupont et al., 2019; Durkan et al.,
2019) and understand their applicability and limitations (Jia & Benson, 2019; Lechner et al., 2020b;
Hanshu et al., 2020; Holl et al., 2020; Quaglino et al., 2020; Kidger et al., 2020; Hasani et al., 2020b).
We proposed a method to enable ODE-based RNNs to learn long-term dependencies."
RELATED WORKS,0.40441176470588236,"7
DISCUSSIONS, SCOPE AND LIMITATIONS"
RELATED WORKS,0.4068627450980392,"We proposed a solution to learn long-term dependencies in irregularly-sampled input data streams. To
perform this, we designed a novel long short term memory network, that possesses a continuous-time
output state, and consequently modiﬁes its internal dynamical ﬂow to a continuous-time model.
mmRNNs resolve the vanishing and exploding of the gradient problem of the class of ODE-RNNs
while demonstrating an attractive performance in learning long-term dependencies on data arriving at
non-uniform intervals."
RELATED WORKS,0.40931372549019607,"What if we feed in samples’ time-lag as an additional input feature to network? The Augmented
LSTM architecture we benchmarked against realizes this concept, which is a simplistic approach to
making LSTMs compatible with irregularly sampled data. The RNN could then learn to make sense of
the time input, for instance, by making its change proportional to the elapsed-time. Nonetheless, the
time characteristic of an augmented RNN depends purely on its learning process. Consequently, we
can only hope that the augmented RNN generalize to unseen time-lags. Our experiments showed that
an augmented LSTM performs reasonably well while being outperformed by models that explicitly
declare their state by a continuous-time modality, such as mmRNNs."
RELATED WORKS,0.4117647058823529,"Difference between reciprocal RNNs and mmRNN?
A reciprocal architecture consists of two
different types of RNNs reciprocally linked together in an auto-regressive fashion (Babaei et al.,
2010). In our context, the ﬁrst RNN could be designed to handle irregularly-sample time series while
the second one is capable of learning long-term dependencies. For example, an LSTM bidirectionally
coupled with an ODE-RNN could, in principle, overcome both challenges. However, the use of
heterogeneous RNN architectures might limit the learning process. In particular, due to different
training convergence rates, the LSTM could already be overﬁtting long before the ODE-RNN has
learned useful dynamics."
RELATED WORKS,0.41421568627450983,"Contrarily, our mmRNN interlinks LSTMs and ODE-RNNs not in an autoregressive fashion, but at
an architectural level, avoiding the problem of learning at different speeds. Our experiments showed
that mmRNNs consistently outperform a reciprocal LSTM-ODE-RNN architecture."
RELATED WORKS,0.4166666666666667,Under review as a conference paper at ICLR 2021
REFERENCES,0.41911764705882354,REFERENCES
REFERENCES,0.4215686274509804,"Zeyuan Allen-Zhu and Yuanzhi Li. Can sgd learn recurrent neural networks with provable general-
ization? In Advances in Neural Information Processing Systems, pp. 10331–10341, 2019."
REFERENCES,0.42401960784313725,"José M Amigó, Roberto Monetti, Thomas Aschenbrenner, and Wolfram Bunk. Transcripts: An
algebraic approach to coupled time series. Chaos: An Interdisciplinary Journal of Nonlinear
Science, 22(1):013105, 2012."
REFERENCES,0.4264705882352941,"Sepideh Babaei, Amir Geranmayeh, and Seyyed Ali Seyyedsalehi. Protein secondary structure
prediction using modular reciprocal bidirectional recurrent neural networks. Computer methods
and programs in biomedicine, 100(3):237–247, 2010."
REFERENCES,0.42892156862745096,"Francois W Belletti, Evan R Sparks, Michael J Franklin, Alexandre M Bayen, and Joseph E Gonzalez.
Scalable linear causal inference for irregularly sampled time series with long range dependencies.
arXiv preprint arXiv:1603.03336, 2016."
REFERENCES,0.43137254901960786,"Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient
descent is difﬁcult. IEEE transactions on neural networks, 5(2):157–166, 1994."
REFERENCES,0.4338235294117647,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016."
REFERENCES,0.4362745098039216,"Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recurrent neural
networks for multivariate time series with missing values. Scientiﬁc reports, 8(1):1–12, 2018."
REFERENCES,0.4387254901960784,"Dexiong Chen, Laurent Jacob, and Julien Mairal. Recurrent kernel networks. In Advances in Neural
Information Processing Systems, pp. 13431–13442, 2019."
REFERENCES,0.4411764705882353,"Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differen-
tial equations. In Advances in neural information processing systems, pp. 6571–6583, 2018."
REFERENCES,0.44362745098039214,"Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014."
REFERENCES,0.44607843137254904,"Edward De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau. Gru-ode-bayes: Continuous
modeling of sporadically-observed time series. In Advances in Neural Information Processing
Systems, pp. 7377–7388, 2019."
REFERENCES,0.4485294117647059,"John R Dormand and Peter J Prince. A family of embedded runge-kutta formulae. Journal of
computational and applied mathematics, 6(1):19–26, 1980."
REFERENCES,0.45098039215686275,"Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.4534313725490196,"Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. In Advances in Neural
Information Processing Systems, pp. 3134–3144, 2019."
REFERENCES,0.45588235294117646,"Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline ﬂows. In
Advances in Neural Information Processing Systems, pp. 7509–7520, 2019."
REFERENCES,0.4583333333333333,"N. Benjamin Erichson, Omri Azencot, Alejandro Queiruga, Liam Hodgkinson, and Michael W.
Mahoney. Lipschitz recurrent neural networks. In International Conference on Learning Represen-
tations, 2021. URL https://openreview.net/forum?id=-N7PBXqOUJZ."
REFERENCES,0.46078431372549017,"Grant Foster. Wavelets for period analysis of unevenly sampled time series. The Astronomical
Journal, 112:1709, 1996."
REFERENCES,0.4632352941176471,"Peter K Friz and Nicolas B Victoir. Multidimensional stochastic processes as rough paths: theory
and applications, volume 120. Cambridge University Press, 2010."
REFERENCES,0.46568627450980393,"Ken-ichi Funahashi and Yuichi Nakamura. Approximation of dynamical systems by continuous time
recurrent neural networks. Neural networks, 6(6):801–806, 1993."
REFERENCES,0.4681372549019608,Under review as a conference paper at ICLR 2021
REFERENCES,0.47058823529411764,"Felix A Gers, Jürgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction
with lstm. Neural Computation, 12(10):2451–2471, 2000."
REFERENCES,0.4730392156862745,"Amir Gholami, Kurt Keutzer, and George Biros. Anode: Unconditionally accurate memory-efﬁcient
gradients for neural odes. arXiv preprint arXiv:1902.10298, 2019."
REFERENCES,0.47549019607843135,"Alexander Greaves-Tunnell and Zaid Harchaoui. A statistical investigation of long memory in
language and music. In International Conference on Machine Learning, pp. 2394–2403, 2019."
REFERENCES,0.47794117647058826,"Klaus Greff, Rupesh K Srivastava, Jan Koutník, Bas R Steunebrink, and Jürgen Schmidhuber. Lstm:
A search space odyssey. IEEE transactions on neural networks and learning systems, 28(10):
2222–2232, 2016."
REFERENCES,0.4803921568627451,"YAN Hanshu, DU Jiawei, TAN Vincent, and FENG Jiashi. On robustness of neural ordinary
differential equations. In International Conference on Learning Representations, 2020."
REFERENCES,0.48284313725490197,"Ramin Hasani, Mathias Lechner, Alexander Amini, Daniela Rus, and Radu Grosu. Liquid time-
constant networks. arXiv preprint arXiv:2006.04439, 2020a."
REFERENCES,0.4852941176470588,"Ramin Hasani, Mathias Lechner, Alexander Amini, Daniela Rus, and Radu Grosu. The natural lottery
ticket winner: Reinforcement learning with ordinary neural circuits. In Proceedings of the 2020
International Conference on Machine Learning. JMLR. org, 2020b."
REFERENCES,0.4877450980392157,"Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen netzen [in german] diploma thesis.
TU Münich, 1991."
REFERENCES,0.49019607843137253,"Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997."
REFERENCES,0.49264705882352944,"Philipp Holl, Vladlen Koltun, and Nils Thuerey. Learning to control pdes with differentiable physics.
arXiv preprint arXiv:2001.07457, 2020."
REFERENCES,0.4950980392156863,"Junteng Jia and Austin R Benson. Neural jump stochastic differential equations. In Advances in
Neural Information Processing Systems, pp. 9843–9854, 2019."
REFERENCES,0.49754901960784315,"Anil Kag, Ziming Zhang, and Venkatesh Saligrama. Rnns incrementally evolving on an equilibrium
manifold: A panacea for vanishing and exploding gradients? In International Conference on
Learning Representations, 2019."
REFERENCES,0.5,"Patrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural controlled differential equations
for irregular time series. arXiv preprint arXiv:2005.08926, 2020."
REFERENCES,0.5024509803921569,"Daniel R Kowal, David S Matteson, and David Ruppert. Functional autoregression for sparsely
sampled data. Journal of Business & Economic Statistics, 37(1):97–109, 2019."
REFERENCES,0.5049019607843137,"Mathias Lechner, Ramin Hasani, Manuel Zimmer, Thomas A Henzinger, and Radu Grosu. Designing
worm-inspired neural networks for interpretable robotic control. In 2019 International Conference
on Robotics and Automation (ICRA), pp. 87–94. IEEE, 2019."
REFERENCES,0.5073529411764706,"Mathias Lechner, Ramin Hasani, Alexander Amini, Thomas A Henzinger, Daniela Rus, and Radu
Grosu. Neural circuit policies enabling auditable autonomy. Nature Machine Intelligence, 2(10):
642–652, 2020a."
REFERENCES,0.5098039215686274,"Mathias Lechner, Ramin Hasani, Daniela Rus, and Radu Grosu. Gershgorin loss stabilizes the recur-
rent neural network compartment of an end-to-end robot learning scheme. In 2020 International
Conference on Robotics and Automation (ICRA). IEEE, 2020b."
REFERENCES,0.5122549019607843,"Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.5147058823529411,"Steven Cheng-Xian Li and Benjamin M Marlin. A scalable end-to-end gaussian process adapter
for irregularly sampled time series classiﬁcation. In Advances in neural information processing
systems, pp. 1804–1812, 2016."
REFERENCES,0.5171568627450981,Under review as a conference paper at ICLR 2021
REFERENCES,0.5196078431372549,"Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph
Gonzalez, Michael Jordan, and Ion Stoica. Rllib: Abstractions for distributed reinforcement
learning. In International Conference on Machine Learning, pp. 3053–3062, 2018."
REFERENCES,0.5220588235294118,"Minsky Marvin and A Papert Seymour. Perceptrons. MIT Press, 1969."
REFERENCES,0.5245098039215687,"Hongyuan Mei and Jason M Eisner.
The neural hawkes process: A neurally self-modulating
multivariate point process. In Advances in Neural Information Processing Systems, pp. 6754–6764,
2017."
REFERENCES,0.5269607843137255,"James Morrill, Patrick Kidger, Cristopher Salvi, James Foster, and Terry Lyons. Neural cdes for long
time series via the log-ode method. arXiv preprint arXiv:2009.08295, 2020."
REFERENCES,0.5294117647058824,"Michael C Mozer, Denis Kazakov, and Robert V Lindsey. Discrete event, continuous time rnns.
arXiv preprint arXiv:1710.04110, 2017."
REFERENCES,0.5318627450980392,"Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu. Phased lstm: Accelerating recurrent network
training for long or event-based sequences. In Advances in neural information processing systems,
pp. 3882–3890, 2016."
REFERENCES,0.5343137254901961,"Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural
networks. In International conference on machine learning, pp. 1310–1318, 2013."
REFERENCES,0.5367647058823529,"Ronald Pearson, Gregory Goney, and James Shwaber. Imbalanced clustering for microarray time-
series. In Proceedings of the ICML, volume 3, 2003."
REFERENCES,0.5392156862745098,"Lev Semenovich Pontryagin. Mathematical theory of optimal processes. Routledge, 2018."
REFERENCES,0.5416666666666666,"Alessio Quaglino, Marco Gallieri, Jonathan Masci, and Jan KoutnÃk. Snode: Spectral discretization
of neural odes for system identiﬁcation. In International Conference on Learning Representations,
2020."
REFERENCES,0.5441176470588235,"DP Roy and L Yan. Robust landsat-based crop time series modelling. Remote Sensing of Environment,
238:110810, 2020."
REFERENCES,0.5465686274509803,"Yulia Rubanova, Tian Qi Chen, and David K Duvenaud. Latent ordinary differential equations
for irregularly-sampled time series. In Advances in Neural Information Processing Systems, pp.
5321–5331, 2019."
REFERENCES,0.5490196078431373,"David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by
back-propagating errors. nature, 323(6088):533–536, 1986."
REFERENCES,0.5514705882352942,"Carl Runge. Über die numerische auﬂösung von differentialgleichungen. Mathematische Annalen,
46(2):167–178, 1895."
REFERENCES,0.553921568627451,"T. Konstantin Rusch and Siddhartha Mishra. Coupled oscillatory recurrent neural network (co{rnn}):
An accurate and (gradient) stable architecture for learning long time dependencies. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=F3s69XzWOia."
REFERENCES,0.5563725490196079,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.5588235294117647,"Alex Sherstinsky. Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm)
network. Physica D: Nonlinear Phenomena, 404:132306, 2020."
REFERENCES,0.5612745098039216,"Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31,
2012."
REFERENCES,0.5637254901960784,"Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033.
IEEE, 2012."
REFERENCES,0.5661764705882353,Under review as a conference paper at ICLR 2021
REFERENCES,0.5686274509803921,"Cheng Wang and Mathias Niepert. State-regularized recurrent neural networks. In International
Conference on Machine Learning, pp. 6596–6606, 2019."
REFERENCES,0.571078431372549,"Paul J Werbos. Generalization of backpropagation with application to a recurrent gas market model.
Neural networks, 1(4):339–356, 1988."
REFERENCES,0.5735294117647058,"Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the
IEEE, 78(10):1550–1560, 1990."
REFERENCES,0.5759803921568627,"Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity unitary
recurrent neural networks. Advances in neural information processing systems, 29:4880–4888,
2016."
REFERENCES,0.5784313725490197,"Juntang Zhuang, Nicha Dvornek, Xiaoxiao Li, Sekhar Tatikonda, Xenophon Papademetris, and James
Duncan. Adaptive checkpoint adjoint method for gradient estimation in neural ode. In Proceedings
of the 37th International Conference on Machine Learning. PMLR 119, 2020."
REFERENCES,0.5808823529411765,"A
PROOFS"
REFERENCES,0.5833333333333334,"Derivation of the Euler’s method Jacobian Let ˙h = fθ(x, h, T) −hτ be an ODE-RNN. Then the
explicit Euler’s method with step-size T is deﬁned as the discretization"
REFERENCES,0.5857843137254902,"ht+T = ht + T(fθ(x, h, T) −hτ)

h=ht.
(5)"
REFERENCES,0.5882352941176471,"Therefore, state-previous state Jacobian is given by ∂ht+T"
REFERENCES,0.5906862745098039,"∂ht
= I + T ∂f ∂h"
REFERENCES,0.5931372549019608,"h=ht −τTI.
(6)"
REFERENCES,0.5955882352941176,"Derivation of the Runge-Kutta Jacobian Let ˙h = fθ(x, h, T) −hτ be an ODE-RNN. Then the
Runge-Kutta method with step-size T is deﬁned as the discretization"
REFERENCES,0.5980392156862745,"ht+T = ht + T M
X"
REFERENCES,0.6004901960784313,"j=1
bi(fθ(x, h, T) −hτ)

h=Ki,
(7)"
REFERENCES,0.6029411764705882,"where the coefﬁcients bi and the values Ki are taken according to the Butcher tableau with PM
j=1 bi =
1 and K1 = ht."
REFERENCES,0.6053921568627451,Then state-previous state Jacobian of the Runge-Kutta method is given by the following equation : ∂ht+T
REFERENCES,0.6078431372549019,"∂ht
= I + T M
X"
REFERENCES,0.6102941176470589,"j=1
bi
∂f
∂h"
REFERENCES,0.6127450980392157,"h=Ki −τTI.,
(8)"
REFERENCES,0.6151960784313726,"Note that the explicit Euler method is an instance of the Runge-Kutta method with M = 1 and
b1 = 1."
REFERENCES,0.6176470588235294,"Proof of ODE-RNN suffering from vanishing or exploding gradients Let ˙h = fθ(x, h, T) −hτ
be an ODE-RNN with latent dimension N. Without loss of generality let h0 be the initial state
at t = 0 and hT denote the ODE state which should be computed by a numerical ODE-solver.
Then ODE-solvers, including ﬁxed-step methods (Runge, 1895) and variable-step methods such as
the Dormand-Prince method (Dormand & Prince, 1980), discretize the interval [0, T] by a series
t0, t1, . . . tn, where t0 = 0 and tn = T and each hti is computed by a single-step explicit Euler or
Runge-Kutta method from hti−1."
REFERENCES,0.6200980392156863,"Our proof closely aligns with the analysis in Hochreiter and Schmidhuber (Hochreiter & Schmidhuber,
1997). We refer the reader to (Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2013) for a
rigorous discussion on the vanishing and exploding gradients."
REFERENCES,0.6225490196078431,Under review as a conference paper at ICLR 2021
REFERENCES,0.625,"We ﬁrst prove the theorem for a scalar RNN, i.e., n = 1, and then extend the discussion to the general
case. The error-ﬂow per RNN step between t = 0 and t = T is given by ∂hT ∂h0
= n
Y m=1"
REFERENCES,0.6274509803921569,"
1 + (tm −tm−1) M
X"
REFERENCES,0.6299019607843137,"j=1
bi
∂f
∂h"
REFERENCES,0.6323529411764706,"h=Kmi
−τ(tm −tm−1)

,
(9)"
REFERENCES,0.6348039215686274,which realizes a power series depending on the value
REFERENCES,0.6372549019607843,"|1 + (tm −tm−1) M
X"
REFERENCES,0.6397058823529411,"j=1
bi
∂f
∂h"
REFERENCES,0.6421568627450981,"h=Kmi
−τ(tm −tm−1)|.
(10)"
REFERENCES,0.6446078431372549,"Obviously, the condition that this term is equal to 1 is not enforced during training and violated for
any non-trivial fθ, such as fθ(h, x) = σ(Whh+Wxx+ˆb) with σ being a sigmoidal or rectiﬁed-linear
activation function. The exact magnitude depends on the weights Wh, as"
REFERENCES,0.6470588235294118,"∂fθ(h, x)"
REFERENCES,0.6495098039215687,"∂h
= Whσ′(Whh + Wxx + ˆb).
(11)"
REFERENCES,0.6519607843137255,A non-zero time-constant τ pushes the gradient toward a vanishing region.
REFERENCES,0.6544117647058824,"Note that the Equation (10) only becomes equal to 1, if PM
j=1 bi
∂f
∂h

h=Kmi
= τ. This would imply"
REFERENCES,0.6568627450980392,"that
∂htm
htm−1 = 0, i.e., when the change in ODE-state between two time-points is zero. A variable that
does not change over time is a memory element. Thus the only solution of enforcing a constant-error
propagation is to include an explicit memory element in the architecture (Hochreiter & Schmidhuber,
1997) which does not change its value between two arbitrary time-points tm and tm−1."
REFERENCES,0.6593137254901961,"For the general case n ≥1, the error-ﬂow per RNN step between t = 0 and t = T is given by ∂hT ∂h0
= n
Y m=1"
REFERENCES,0.6617647058823529,"
I + (tm −tm−1) M
X"
REFERENCES,0.6642156862745098,"j=1
bi
∂f
∂h"
REFERENCES,0.6666666666666666,"h=Kmi
−τ(tm −tm−1)I

.
(12)"
REFERENCES,0.6691176470588235,"As h is a vector, we need to consider all possible error-propagation paths. The error-ﬂow from unit u
to unit v is then given by summing all N n−1 possible paths between u to v,"
REFERENCES,0.6715686274509803,"∂hv
T
∂hu
0
= N
X"
REFERENCES,0.6740196078431373,"l1
· · · N
X ln−1 n
Y m=1"
REFERENCES,0.6764705882352942,"
I + (tm −tm−1) M
X"
REFERENCES,0.678921568627451,"j=1
bi
∂f
∂h"
REFERENCES,0.6813725490196079,"h=Kmi
−τ(tm −tm−1)I
"
REFERENCES,0.6838235294117647,"lm,lm−1,
(13)"
REFERENCES,0.6862745098039216,where l0 = u and ln = v.
REFERENCES,0.6887254901960784,"The arguments of the scalar case hold for every individual path in Equation (13). The only difference
between the the scalar case and the individual paths in the vectored version is the non-diagonal
connections in the general case do not include the constant 1 and τ. The error-propagation magnitude
between u and v with u ̸= v is given by"
REFERENCES,0.6911764705882353,"|(tm −tm−1)
 M
X"
REFERENCES,0.6936274509803921,"j=1
bi
∂f
∂h h=Kmi "
REFERENCES,0.696078431372549,"u,v|.
(14)"
REFERENCES,0.6985294117647058,"Again, for fθ(h, x) = σ(Whh + Wxx + ˆb) we obtain an error-ﬂow that depends on the weights Wh
and can be either vanishing or exploding, depending on its magnitude."
REFERENCES,0.7009803921568627,"Proof that even gradients of the ODE solution can vanish or explode Let ˙h = fθ(x, h, T) −hτ
be an ODE-RNN with latent dimension N, with fθ being uniformly Lipschitz continuous. Without
loss of generality let h0 be the initial state at t = 0 and hT denote the ODE state which should be
computed by a numerical ODE-solver. We approximate the interval [0, T] by a uniform discretization
grid, i.e. ti −ti−1 = tj −tj−1 = T/n for all i, j t0, t1, . . . tn, where t0 = 0 and tn = T and each
hti is computed by a single-step explicit Euler from hti−1."
REFERENCES,0.7034313725490197,"Even when making the discretization grid t0, t1, . . . tn ﬁner and ﬁner, the gradient propagation issue
is not resolved. Let hi denote the intermediate values computed by the Picard-iteration, i.e., the
explicit Euler. By the Picard–Lindelöf theorem, we know that hT converges to the true solution h(T)."
REFERENCES,0.7058823529411765,Under review as a conference paper at ICLR 2021
REFERENCES,0.7083333333333334,"First, we assume there exists a ξ > 0 such that ξ ≤∂f"
REFERENCES,0.7107843137254902,"∂h

h=hm −τ for all m. Note that this situation"
REFERENCES,0.7132352941176471,"can naturally occur if we have a fθ(h, x) = σ(Whh + Wxx + ˆb). In the limit n →∞we get"
REFERENCES,0.7156862745098039,"lim
n→∞
∂hT"
REFERENCES,0.7181372549019608,"∂h0
= lim
n→∞ n
Y m=1"
REFERENCES,0.7205882352941176,"
1 + (tm −tm−1)∂f ∂h"
REFERENCES,0.7230392156862745,"h=hm
−τ(tm −tm−1)
"
REFERENCES,0.7254901960784313,"= lim
n→∞ n
Y m=1"
REFERENCES,0.7279411764705882,"
1 + T"
REFERENCES,0.7303921568627451,"n
∂f
∂h"
REFERENCES,0.7328431372549019,"h=hm −τ T n )
"
REFERENCES,0.7352941176470589,"≥lim
n→∞ n
Y m=1"
REFERENCES,0.7377450980392157,"
1 + T"
REFERENCES,0.7401960784313726,"n ξ

, with some 0 < ξ ≤∂f ∂h"
REFERENCES,0.7426470588235294,h=hm −τ for all m
REFERENCES,0.7450980392156863,"= lim
n→∞"
REFERENCES,0.7475490196078431,"
1 + T"
REFERENCES,0.75,"n ξ
n"
REFERENCES,0.7524509803921569,"= eT ξ > 1,"
REFERENCES,0.7549019607843137,"i.e., we have an exploding gradient."
REFERENCES,0.7573529411764706,"Conversely, lets assume there exists a ξ < 0 such that ξ ≥∂f"
REFERENCES,0.7598039215686274,"∂h

h=hm −τ for all m. Note that this"
REFERENCES,0.7622549019607843,"situation can also naturally occur, for instance if τ > 0 and regions where f ′ is small. In the limit
n →∞we get"
REFERENCES,0.7647058823529411,"lim
n→∞
∂hT"
REFERENCES,0.7671568627450981,"∂h0
= lim
n→∞ n
Y m=1"
REFERENCES,0.7696078431372549,"
1 + (tm −tm−1)∂f ∂h"
REFERENCES,0.7720588235294118,"h=hm −τ(tm −tm−1)
"
REFERENCES,0.7745098039215687,"= lim
n→∞ n
Y m=1"
REFERENCES,0.7769607843137255,"
1 + T"
REFERENCES,0.7794117647058824,"n
∂f
∂h"
REFERENCES,0.7818627450980392,"h=hm −τ T n )
"
REFERENCES,0.7843137254901961,"≤lim
n→∞ n
Y m=1"
REFERENCES,0.7867647058823529,"
1 + T"
REFERENCES,0.7892156862745098,"n ξ

, with some 0 > ξ ≥∂f ∂h"
REFERENCES,0.7916666666666666,h=hm −τ for all m
REFERENCES,0.7941176470588235,"= lim
n→∞"
REFERENCES,0.7965686274509803,"
1 + T"
REFERENCES,0.7990196078431373,"n ξ
n"
REFERENCES,0.8014705882352942,"= eT ξ < 1,"
REFERENCES,0.803921568627451,"i.e., we have a vanishing gradient."
REFERENCES,0.8063725490196079,"Similar to the argument in the proof above, we can extend the scalar case to the general case. However,
summing over all possible path might not be trivial, as the number of possible path also growths to
inﬁnity."
REFERENCES,0.8088235294117647,"lim
n→∞
∂hv
T
∂hu
0
= lim
n→∞ N
X"
REFERENCES,0.8112745098039216,"l1
· · · N
X ln−1 n
Y m=1"
REFERENCES,0.8137254901960784,"
I +(tm −tm−1)∂f ∂h"
REFERENCES,0.8161764705882353,"h=hm
−τ(tm −tm−1)I
"
REFERENCES,0.8186274509803921,"lm,lm−1
. (15)"
REFERENCES,0.821078431372549,"Instead, we assume u = v = l1 = . . . ln −1, i.e., we only look at the error-propagation through the
diagonal element u."
REFERENCES,0.8235294117647058,"lim
n→∞
∂hv
T
∂hu
0
= lim
n→∞ n
Y m=1"
REFERENCES,0.8259803921568627,"
I + (tm −tm−1)∂f ∂h"
REFERENCES,0.8284313725490197,"h=hm −τ(tm −tm−1)I
 u,u"
REFERENCES,0.8308823529411765,"= lim
n→∞ n
Y m=1"
REFERENCES,0.8333333333333334,"
1 + (tm −tm−1)∂f u ∂hu"
REFERENCES,0.8357843137254902,"hu=hu
m
−τ u(tm −tm−1)
"
REFERENCES,0.8382352941176471,"= lim
n→∞ n
Y m=1"
REFERENCES,0.8406862745098039,"
1 + T"
REFERENCES,0.8431372549019608,"n
∂f u ∂hu"
REFERENCES,0.8455882352941176,"hu=hu
m
−τ u T"
REFERENCES,0.8480392156862745,"n )

,"
REFERENCES,0.8504901960784313,Under review as a conference paper at ICLR 2021
REFERENCES,0.8529411764705882,"which is equivalent to the scalar case. For an interesting f such as fθ(h, x) = σ(Whh + Wxx + ˆb),
the term f"
REFERENCES,0.8553921568627451,"h depends on the value W u,u
h
. By assuming W w,z for any (w, z) ̸= (u, u) is neglectable
small, we can infer that the effects of the gradient by any other path in Equation (15) is neglectable
small. Thus the global error ﬂow depends on W u,u
h
, which can make the error-ﬂow either explode or
vanish depending on its value."
REFERENCES,0.8578431372549019,"Note that this argument is similar to arguing that as the multi-dimensional case properly contains the
scalar case, the multi-dimensional case can express an exploding or vanishing gradient too."
REFERENCES,0.8602941176470589,"Proof that the mmRNNs does not suffer from a vanishing or exploding gradient Recall that we
assume that weights of gθ and zθ are initialized close to 0 and that we are at the beginning of the
training process, i.e., we assume the weights do not differ signiﬁcantly from their initialized values.
Moreover, for the proof we assume that gθ and zθ are standard multi-layer perceptron modules."
REFERENCES,0.8627450980392157,"We have
∂ct+1"
REFERENCES,0.8651960784313726,"∂ct
= ∂ct ⊙σ(gθ(ht) + bf) + zθ(ht) ∂ct"
REFERENCES,0.8676470588235294,= ∂σ(gθ(ht) + bf)
REFERENCES,0.8700980392156863,"∂ct
diag(ct) + diag(σ(gθ(ht) + bf)) + ∂zθ(ht) ∂ct
."
REFERENCES,0.8725490196078431,For the derivatives of the ﬁrst term we can simply apply the chain-rule and get
REFERENCES,0.875,∂σ(gθ(ht) + bf)v
REFERENCES,0.8774509803921569,"∂cu
t
= σ′(gθ(ht) + bf)vgθ(ht)v ∂gθ(ht)v"
REFERENCES,0.8799019607843137,"∂cu
t
≈0,"
REFERENCES,0.8823529411764706,because we assumed gθ(ht) ≈0 due to its initialization.
REFERENCES,0.8848039215686274,"Similar argument holds for the derivative of the last term, where we assumed that zθ is initialized
closet to 0.
∂zθ(ht)v"
REFERENCES,0.8872549019607843,"∂cu
t
= f′
act(Wθˆzt + bθ)Wθ
∂ˆzv
t
∂cu
t
≈0,"
REFERENCES,0.8897058823529411,"where fact is the ﬁnal activation function of zθ, Wθ and bθ the weights and bias parametrizing the last
layer of zθ and ˆzt the last hidden vector of zθ. Note that we assumed Wθ ≈0."
REFERENCES,0.8921568627450981,"Consequently, with a proper weight initialization, the Jacobian simpliﬁes to ∂ct+1"
REFERENCES,0.8946078431372549,"∂ct
≈diag(σ(gθ(ht) + bf))."
REFERENCES,0.8970588235294118,"We assumed that gθ is initialized close to 0. Hence,"
REFERENCES,0.8995098039215687,"diag(σ(gθ(ht) + bf))v =≈σ(bf)
= σ(3)
≈0.994,"
REFERENCES,0.9019607843137255,as we initialized bf to 3.
REFERENCES,0.9044117647058824,"Hence, we have  N
X j=1"
REFERENCES,0.9068627450980392,"∂ci
t+1
∂cj
t"
REFERENCES,0.9093137254901961,"≈0.994,"
REFERENCES,0.9117647058823529,", which is less than 1 (no exploding) but much greater than 0 (no vanishing) and ensures a near-
constant error propagation at the beginning of the training process."
REFERENCES,0.9142156862745098,"As already mentioned in the paper, the exact value of the error ﬂow can be controlled by changing the
bias term from its default value of 1. If the underlying data distribution contains dependencies with a
very long time-lag, we can bring the error ﬂow factor closer to 1 by increasing forget gate bias; Thus
enabling the mmRNN to learn even very long-term dependencies in the data."
REFERENCES,0.9166666666666666,Under review as a conference paper at ICLR 2021 Time
REFERENCES,0.9191176470588235,"a
a
a
a b
b
b a"
REFERENCES,0.9215686274509803,Figure 4: Dense coding Time
REFERENCES,0.9240196078431373,a:∆t = 4
REFERENCES,0.9264705882352942,b:∆t = 3
REFERENCES,0.928921568627451,a:∆t = . . .
REFERENCES,0.9313725490196079,Figure 5: Event-based coding
REFERENCES,0.9338235294117647,"Figure 6: Dense and event-based coding of the same time-series. An event-based coding is more
efﬁcient than a dense coding at encoding sequences where the transmitted symbol changes only
sparsely."
REFERENCES,0.9362745098039216,"B
EXPERIMENTAL EVALUATION"
REFERENCES,0.9387254901960784,"For models containing differential equations, we used the ODE-solvers as listed in Table 7. Hyperpa-
rameter settings used for our evaluation is shown in Table 8."
REFERENCES,0.9411764705882353,"Batching Sequences of our event-based bit-stream classiﬁcation task and event-based seqMNIST
can have different lengths. To allow an arbitrary batching of several sequences, we pad all sequences
to equal length and apply a binary mask during training and evaluation."
REFERENCES,0.9436274509803921,"B.1
DATASET DESCRIPTION"
REFERENCES,0.946078431372549,The individual datasets are created as follows:
REFERENCES,0.9485294117647058,"Bit-stream XOR dataset Every data point is a block of 32 random bits. The binary labels are created
by applying an XOR function on the bit block, i.e., class A if the number of 1s in the bit-stream are
even, class B if the number of 1s in the bit-stream is odd. For training, a cross-entropy loss on these
two classes is used. The training set consists of 100,000 samples, which are less than 0.0024% of all
possible bit-streams that can occur. The test set consists of 10,000 samples."
REFERENCES,0.9509803921568627,"For the event-based encoding, we introduce a time-dimension. The time is normalized such that the
complete sequence equals 1 unit of time, i.e., 32 bits corresponds to exactly 1 second. An illustration
of the two different encodings is shown in Figure 6."
REFERENCES,0.9534313725490197,"Person Activity We consider a variation of the ""Human activity"" dataset described in (Rubanova
et al., 2019) form the UCI machine learning repository (Dua & Graff, 2017). The dataset is comprised
of 25 recordings of human participants performing different physical activities. The eleven possible
activities are ”lying down”, ”lying”, ”sitting down”, ”sitting”, ”standing up from lying”, ”standing up
from”, ”sitting”, ”standing up from sitting on the ground”, ”walking”, ”falling”, ”on all fours”, and
”sitting on the ground”. The objective of this task is to recognize the activity from inertial sensors
worn by the participant, i.e., a per-time-step classiﬁcation problem. We group the eleven activities
listed above into seven different classes, as proposed by (Rubanova et al., 2019)."
REFERENCES,0.9558823529411765,"The input data consists of sensor readings from four inertial measurement units placed on the
participant’s arms and feet. The sensors are read at a ﬁxed period of 211 ms but have different
phase-shifts in the 25 recordings. Therefore, we treat the data as irregularly sampled time-series."
REFERENCES,0.9583333333333334,"The 25 recordings are split into partially overlapping sequences of length 32, to allow an efﬁcient
training of the machine learning models."
REFERENCES,0.9607843137254902,Under review as a conference paper at ICLR 2021
REFERENCES,0.9632352941176471,"Table 7: ODE-solvers used for the different RNN architectures involving ordinary differential
equations"
REFERENCES,0.9656862745098039,"Model
ODE-solver
Time-step ratio"
REFERENCES,0.9681372549019608,"CT-RNN
4-th order Runge-Kutta
1/3
ODE-RNN
4-th order Runge-Kutta
1/3
GRU-ODE
Explicit Euler
1/4
mmRNN
Explicit Euler
1/4"
REFERENCES,0.9705882352941176,"Our results are not directly comparable to the experiments in (Rubanova et al., 2019), as we use
a different representation of the input features. While (Rubanova et al., 2019) represents each
input feature as a value-mask pair, i.e., 24 input features, we represent the data in the form of a
7-dimensional feature vector. The ﬁrst four entries of the input indicate the senor ID, i.e., which arm
or foot, whereas the remaining three entries contain the sensor reading."
REFERENCES,0.9730392156862745,"Event-based seqMNIST The MNIST dataset consists of 70,000 data points split into 60,000 training
and 10,000 test samples (LeCun et al., 1998). Each sample is a 28-by-28 grayscale image, quantized
with 8-bits and represents one out of 10 possible digits, i.e., a number from 0 to 10."
REFERENCES,0.9754901960784313,"We pre-process each sample as follows: We ﬁrst apply a threshold to transform the 8-bits pixel values
into binary values. The threshold is 128, on a scale where 0 represents the lowest possible and 255
the larges possible pixel value. We further transform the 28-by-28 image into a time-series of length
784. Next, we encode binary time-series in a event-based format. Essentially, the encoding step gets
rid of consecutive occurrences of the same binary value, i.e., 1, 1, 1, 1 is transformed into (1, t = 4).
By introducing a time dimension, we can compress the sequences from 784 to an average of 53
time-steps."
REFERENCES,0.9779411764705882,"To allow an efﬁcient batching and training, we pad each sequence to a length of 256. Note that no
information was lost during this process. We normalize the added time dimension such that 256
symbols correspond to 1 second or unit of time. The resulting task is a per-sequence classiﬁcation
problem of irregularly sampled time-series."
REFERENCES,0.9803921568627451,"Walker2d kinematic modeling Here we create a dataset based on the Walker2d-v2 OpenAI gym
(Brockman et al., 2016) environment and the MuJoCo physics engine (Todorov et al., 2012). Our
objective is to benchmark how well the RNN architecture can model kinematic dynamical systems in
an irregularly sampled fashion. The learning setup is based on an auto-regressive supervised learning,
i.e., the model predicts the next state of the Walker2d environment based on the current state."
REFERENCES,0.9828431372549019,"In order to obtain interesting simulation rollouts, we trained a non-recurrent policy by Proximal
Policy Optimization (PPO) (Schulman et al., 2017) using the Rllib (Liang et al., 2018) reinforcement
learning framework. We then collect the training data for our benchmark by performing rollouts
on the Walker2d-v2 environment using our pre-trained policy. Note that because the policy is
deterministic, there is no need to include the actions produced by the policy in the training data."
REFERENCES,0.9852941176470589,"We introduce three sources of uncertainty to make this task more challenging. First of all, for
each rollout we uniformly sample a checkpoint of policy at 562, 822, 923, or 1104 PPO iterations.
Secondly, we overwrite 1% of all actions by random actions. Thirdly, we exclude 10% of the
time-steps, i.e., we simulate frame-skips/frame-drops. Note that the last step transforms the rollouts
into irregularly sampled time-series and introduces a time dimension."
REFERENCES,0.9877450980392157,"In total, we collected 400 rollouts, i.e., 300 used for training, 40 for validation, and 60 for testing. For
an efﬁcient training, we align the rollouts into sequences of length 64. We use the mean-square-error
as training loss and evaluation metric. We train each RNN for 200 epochs and log the validation
error after each training epochs. At the end, we restore the weights that achieved the best (lowest)
validation error and evaluate them on the test set."
REPRODUCIBILITY STATEMENT,0.9901960784313726,Reproducibility statement All code and data are submitted for review as a supplement.
REPRODUCIBILITY STATEMENT,0.9926470588235294,Under review as a conference paper at ICLR 2021
REPRODUCIBILITY STATEMENT,0.9950980392156863,"Table 8: Hyperparameters
Parameter
Value
Description"
REPRODUCIBILITY STATEMENT,0.9975490196078431,"RNN latent dimension
64
number of neurons in the RNN
Minibatch size
256
Optimizer
RMSprop (Tieleman & Hinton, 2012)
Learning rate
5e-3
Training epochs
500/200
Synthetic/real datasets"
