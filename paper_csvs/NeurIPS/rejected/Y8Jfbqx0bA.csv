Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002028397565922921,"Generating synthetic data, with or without differential privacy, has attracted signiﬁ-
1"
ABSTRACT,0.004056795131845842,"cant attention as a potential solution to the dilemma between making data easily
2"
ABSTRACT,0.006085192697768763,"available, and the privacy of data subjects. Several works have shown that consis-
3"
ABSTRACT,0.008113590263691683,"tency of downstream analyses from synthetic data, including accurate uncertainty
4"
ABSTRACT,0.010141987829614604,"estimation, requires accounting for the synthetic data generation. There are very
5"
ABSTRACT,0.012170385395537525,"few methods of doing so, most of them for frequentist analysis. In this paper, we
6"
ABSTRACT,0.014198782961460446,"study how to perform consistent Bayesian inference from synthetic data. We prove
7"
ABSTRACT,0.016227180527383367,"that mixing posterior samples obtained separately from multiple large synthetic
8"
ABSTRACT,0.018255578093306288,"datasets converges to the posterior of the downstream analysis under standard regu-
9"
ABSTRACT,0.02028397565922921,"larity conditions when the analyst’s model is compatible with the data provider’s
10"
ABSTRACT,0.02231237322515213,"model. We show experimentally that this works in practice, unlocking consistent
11"
ABSTRACT,0.02434077079107505,"Bayesian inference from synthetic data while reusing existing downstream analysis
12"
ABSTRACT,0.02636916835699797,"methods.
13"
INTRODUCTION,0.028397565922920892,"1
Introduction
14"
INTRODUCTION,0.030425963488843813,"Synthetic data has the potential of opening privacy-sensitive datasets for widespread analysis. The
15"
INTRODUCTION,0.032454361054766734,"idea is to train a generative model with real data, and release synthetic data that has been generated
16"
INTRODUCTION,0.034482758620689655,"from the model. The synthetic data does not contain records from real people, and ideally it preserves
17"
INTRODUCTION,0.036511156186612576,"the population-level properties of the real data, making it useful for analysis. Privacy preservation can
18"
INTRODUCTION,0.038539553752535496,"be guaranteed with differential privacy (DP) (Dwork et al. 2006b), which offers provable protection
19"
INTRODUCTION,0.04056795131845842,"of privacy.
20"
INTRODUCTION,0.04259634888438134,"The most convenient and straightforward way for downstream analysts to analyse synthetic data
21"
INTRODUCTION,0.04462474645030426,"is using the same method that would be used with real data. However, ignoring the additional
22"
INTRODUCTION,0.04665314401622718,"stochasticity arising from the synthetic data generation will yield biased results and overconﬁdent
23"
INTRODUCTION,0.0486815415821501,"uncertainty estimates (Raghunathan et al. 2003; Räisä et al. 2023; Wilde et al. 2021). This is especially
24"
INTRODUCTION,0.05070993914807302,"problematic under DP, which requires adding extra noise, which will be ignored if the synthetic data
25"
INTRODUCTION,0.05273833671399594,"is treated like real data. This problem creates the need for noise-aware analyses that account for the
26"
INTRODUCTION,0.05476673427991886,"synthetic data generation.
27"
INTRODUCTION,0.056795131845841784,"When the downstream analysis is frequentist, it is possible to account for the synthetic data generation
28"
INTRODUCTION,0.058823529411764705,"when multiple synthetic datasets are generated and analysed (Raghunathan et al. 2003). Recent work
29"
INTRODUCTION,0.060851926977687626,"has extended this to DP synthetic data (Räisä et al. 2023), which allows generating multiple synthetic
30"
INTRODUCTION,0.06288032454361055,"datasets without compromising on privacy. These methods reuse the analysis method for the real
31"
INTRODUCTION,0.06490872210953347,"data, and only require using simple combining rules to combine the results from the analyses on each
32"
INTRODUCTION,0.06693711967545639,"synthetic dataset, making them simple to apply.
33"
INTRODUCTION,0.06896551724137931,"For Bayesian downstream analyses, Wilde et al. (2021) have shown that the analyst can use additional
34"
INTRODUCTION,0.07099391480730223,"samples of public real data to correct their analysis. However, their method requires targeting a
35"
INTRODUCTION,0.07302231237322515,"generalised notion of the posterior (Bissiri et al. 2016) and needs the additional public data for
36"
INTRODUCTION,0.07505070993914807,"calibration. Ghalebikesabi et al. (2022) propose a correction using importance sampling to avoid the
37"
INTRODUCTION,0.07707910750507099,"need of public data, but only prove convergence to a generalised posterior and do not clearly address
38"
INTRODUCTION,0.07910750507099391,"the noise-awareness of the method.
39"
INTRODUCTION,0.08113590263691683,"In the context of missing data, Gelman et al. (2014) have proposed inferring the downstream posterior
40"
INTRODUCTION,0.08316430020283976,"of a Bayesian analysis by imputing multiple completed datasets, inferring the analysis posterior for
41"
INTRODUCTION,0.08519269776876268,"each completed dataset separately, and mixing the posteriors together. We study the applicability
42"
INTRODUCTION,0.0872210953346856,"of this method to synthetic data, aiming the bring the simplicity of the frequentist methods using
43"
INTRODUCTION,0.08924949290060852,"multiple synthetic datasets to Bayesian downstream analysis.
44"
INTRODUCTION,0.09127789046653144,"Contributions
45"
WE STUDY INFERRING THE DOWNSTREAM ANALYSIS POSTERIOR BY GENERATING MULTIPLE SYNTHETIC,0.09330628803245436,"1. We study inferring the downstream analysis posterior by generating multiple synthetic
46"
WE STUDY INFERRING THE DOWNSTREAM ANALYSIS POSTERIOR BY GENERATING MULTIPLE SYNTHETIC,0.09533468559837728,"datasets, inferring the analysis posterior for each synthetic dataset as if it were the real
47"
WE STUDY INFERRING THE DOWNSTREAM ANALYSIS POSTERIOR BY GENERATING MULTIPLE SYNTHETIC,0.0973630831643002,"dataset, and mixing the posteriors together. We ﬁnd that in this setting, the synthetic datasets
48"
WE STUDY INFERRING THE DOWNSTREAM ANALYSIS POSTERIOR BY GENERATING MULTIPLE SYNTHETIC,0.09939148073022312,"also need to be larger than the original dataset.
49"
WE STUDY INFERRING THE DOWNSTREAM ANALYSIS POSTERIOR BY GENERATING MULTIPLE SYNTHETIC,0.10141987829614604,"2. We prove that when the Bernstein–von Mises, or a similar theorem, applies, this method
50"
WE STUDY INFERRING THE DOWNSTREAM ANALYSIS POSTERIOR BY GENERATING MULTIPLE SYNTHETIC,0.10344827586206896,"converges to the true posterior as the number of synthetic datasets and the size of the
51"
WE STUDY INFERRING THE DOWNSTREAM ANALYSIS POSTERIOR BY GENERATING MULTIPLE SYNTHETIC,0.10547667342799188,"synthetic datasets grow. Under stronger assumptions, we prove a convergence rate for this
52"
WE STUDY INFERRING THE DOWNSTREAM ANALYSIS POSTERIOR BY GENERATING MULTIPLE SYNTHETIC,0.1075050709939148,"method in the synthetic dataset size, which we expect to match the rate that usually applies in
53"
WE STUDY INFERRING THE DOWNSTREAM ANALYSIS POSTERIOR BY GENERATING MULTIPLE SYNTHETIC,0.10953346855983773,"the Bernstein–von Mises theorem (Hipp and Michel 1976). These are presented in Section 3.
54"
WE STUDY INFERRING THE DOWNSTREAM ANALYSIS POSTERIOR BY GENERATING MULTIPLE SYNTHETIC,0.11156186612576065,"3. We evaluate this method with two examples in Section 4: non-private univariate Gaussian
55"
WE STUDY INFERRING THE DOWNSTREAM ANALYSIS POSTERIOR BY GENERATING MULTIPLE SYNTHETIC,0.11359026369168357,"mean estimation, and differentially private Bayesian logistic regression. In the ﬁrst example,
56"
WE STUDY INFERRING THE DOWNSTREAM ANALYSIS POSTERIOR BY GENERATING MULTIPLE SYNTHETIC,0.11561866125760649,"we use the tractability of the model to derive further theoretical properties of the method,
57"
WE STUDY INFERRING THE DOWNSTREAM ANALYSIS POSTERIOR BY GENERATING MULTIPLE SYNTHETIC,0.11764705882352941,"and in both examples, we verify that the method works in practice through experiments.
58"
RELATED WORK,0.11967545638945233,"1.1
Related Work
59"
RELATED WORK,0.12170385395537525,"Generating synthetic data to preserve privacy was, as far as we know, originally proposed by Liew
60"
RELATED WORK,0.12373225152129817,"et al. (1985). Rubin (1993) proposed accounting for the synthetic data generation in frequentist
61"
RELATED WORK,0.1257606490872211,"downstream analyses by adapting multiple imputation (Rubin 1987), which involves generating
62"
RELATED WORK,0.12778904665314403,"multiple synthetic datasets, analysing each of them, and combining the results with so called Rubin’s
63"
RELATED WORK,0.12981744421906694,"rules (Raghunathan et al. 2003; Reiter 2002). Recently, Räisä et al. (2023) have shown that multiple
64"
RELATED WORK,0.13184584178498987,"imputation also works when the synthetic data is generated under DP when the data generation
65"
RELATED WORK,0.13387423935091278,"algorithm is noise-aware in a certain sense.
66"
RELATED WORK,0.1359026369168357,"Wilde et al. (2021) study downstream Bayesian inference from DP synthetic data by considering
67"
RELATED WORK,0.13793103448275862,"the analyst’s model to be misspeciﬁed, and targeting a generalised notion of the posterior (Bissiri
68"
RELATED WORK,0.13995943204868155,"et al. 2016) to deal with the misspeciﬁcation, which makes method their more difﬁcult to apply than
69"
RELATED WORK,0.14198782961460446,"standard Bayesian inference. They also assume that the analyst has additional public data available to
70"
RELATED WORK,0.1440162271805274,"calibrate their method.
71"
RELATED WORK,0.1460446247464503,"Ghalebikesabi et al. (2022) use importance sampling to correct for bias with DP synthetic data,
72"
RELATED WORK,0.14807302231237324,"and have Bayesian inference as an example application. However, they also target a generalised
73"
RELATED WORK,0.15010141987829614,"variant (Bissiri et al. 2016) of the posterior instead of the noise-aware posterior we target, and they do
74"
RELATED WORK,0.15212981744421908,"not evaluate uncertainty estimation, so the noise-awareness of their method is not clear.
75"
RELATED WORK,0.15415821501014199,"We are not aware of any existing work adapting multiple imputation for Bayesian downstream analysis
76"
RELATED WORK,0.15618661257606492,"in the synthetic data setting. In the missing data setting without DP, where multiple imputation was
77"
RELATED WORK,0.15821501014198783,"originally developed (Rubin 1987), Gelman et al. (2014) have proposed sampling the downstream
78"
RELATED WORK,0.16024340770791076,"posterior by mixing samples of the downstream posteriors from each of the multiple synthetic datasets.
79"
RELATED WORK,0.16227180527383367,"We ﬁnd that this is not sufﬁcient in the synthetic data setting, and add one extra component: our
80"
RELATED WORK,0.1643002028397566,"synthetic datasets are larger than the original dataset. We compare the two cases in more detail in
81"
RELATED WORK,0.1663286004056795,"Supplemental Section F, and in particular explain why large synthetic datasets are not needed in the
82"
RELATED WORK,0.16835699797160245,"missing data setting.
83"
RELATED WORK,0.17038539553752535,"Noise-aware DP Bayesian inference is critical for taking into account the DP noise in synthetic data,
84"
RELATED WORK,0.1724137931034483,"but only a few works address this even without synthetic data. Bernstein and Sheldon (2018) present
85"
RELATED WORK,0.1744421906693712,"an inference method for simple exponential family models. Their approach was extended to linear
86"
RELATED WORK,0.17647058823529413,"models (Bernstein and Sheldon 2019) and generalised linear models (Kulkarni et al. 2021). Recently,
87"
RELATED WORK,0.17849898580121704,"Ju et al. (2022) developed an MCMC sampler that can sample the noise-aware posterior using a noisy
88"
RELATED WORK,0.18052738336713997,"summary statistic.
89"
BACKGROUND ON BAYESIAN INFERENCE,0.18255578093306288,"2
Background on Bayesian Inference
90"
BACKGROUND ON BAYESIAN INFERENCE,0.1845841784989858,"Bayesian inference is a paradigm of statistical inference where the data analyst’s uncertainty in a
91"
BACKGROUND ON BAYESIAN INFERENCE,0.18661257606490872,"quantity Q after observing data X is represented using the posterior distribution p(Q|X) (Gelman
92"
BACKGROUND ON BAYESIAN INFERENCE,0.18864097363083165,"et al. 2014). The posterior is given by Bayes’ rule:
93"
BACKGROUND ON BAYESIAN INFERENCE,0.19066937119675456,"p(Q|X) =
p(X|Q)p(Q)
R"
BACKGROUND ON BAYESIAN INFERENCE,0.1926977687626775,"p(X|Q0)p(Q0) dQ0 ,
(1)"
BACKGROUND ON BAYESIAN INFERENCE,0.1947261663286004,"where p(X|Q) is the likelihood of observing the data X for a given value of Q, and p(Q) is the
94"
BACKGROUND ON BAYESIAN INFERENCE,0.19675456389452334,"analyst’s prior of Q. Computing the denominator is typically intractable, so analysts often use
95"
BACKGROUND ON BAYESIAN INFERENCE,0.19878296146044624,"numerical methods to sample p(Q|X) (Gelman et al. 2014).
96"
BACKGROUND ON BAYESIAN INFERENCE,0.20081135902636918,"Bernstein–von Mises Theorem
It turns out that in many typical settings, the prior’s inﬂuence on
97"
BACKGROUND ON BAYESIAN INFERENCE,0.2028397565922921,"the posterior vanishes when the dataset X is large. A basic example of this is the Bernstein–von
98"
BACKGROUND ON BAYESIAN INFERENCE,0.20486815415821502,"Mises theorem (van der Vaart 1998), which informally states that under some regularity conditions,
99"
BACKGROUND ON BAYESIAN INFERENCE,0.20689655172413793,"the posterior approaches a Gaussian that does not depend on the prior as the size of the dataset
100"
BACKGROUND ON BAYESIAN INFERENCE,0.20892494929006086,"increases.
101"
BACKGROUND ON BAYESIAN INFERENCE,0.21095334685598377,"A crucial component of the theorem, and also our theory, is the notion of total variation distance
102"
BACKGROUND ON BAYESIAN INFERENCE,0.2129817444219067,"between random variables, which is used to measure the difference between two random variables or
103"
BACKGROUND ON BAYESIAN INFERENCE,0.2150101419878296,"probability distributions.
104"
BACKGROUND ON BAYESIAN INFERENCE,0.21703853955375255,"Deﬁnition 2.1. The total variation distance between random variables (or distributions) P1 and P2
105"
BACKGROUND ON BAYESIAN INFERENCE,0.21906693711967545,"is
106"
BACKGROUND ON BAYESIAN INFERENCE,0.2210953346855984,"TV(P1, P2) = sup A"
BACKGROUND ON BAYESIAN INFERENCE,0.2231237322515213,"| Pr(P1 2 A) −Pr(P2 2 A)|,
(2)"
BACKGROUND ON BAYESIAN INFERENCE,0.22515212981744423,"where A is any measurable set.
107"
BACKGROUND ON BAYESIAN INFERENCE,0.22718052738336714,"As a slight abuse of notation, we allow the arguments of TV(·, ·) to be random variables, probability
108"
BACKGROUND ON BAYESIAN INFERENCE,0.22920892494929007,"distributions, or probability density functions interchangeably. We list some properties of total
109"
BACKGROUND ON BAYESIAN INFERENCE,0.23123732251521298,"variation distance that we use in Lemma A.1 in the Supplement.
110"
BACKGROUND ON BAYESIAN INFERENCE,0.2332657200811359,"Now we can state the theorem.
111"
BACKGROUND ON BAYESIAN INFERENCE,0.23529411764705882,"Theorem 2.2 (Bernstein–von Mises (van der Vaart 1998)). Let n denote the size of the dataset Xn.
112"
BACKGROUND ON BAYESIAN INFERENCE,0.23732251521298176,"Under regularity conditions stated in Condition A.4 in Supplemental Section A.2, for true parameter
113"
BACKGROUND ON BAYESIAN INFERENCE,0.23935091277890466,"value Q0, the posterior ¯Q(Xn) ⇠p(Q|Xn) satisﬁes
114 TV"
BACKGROUND ON BAYESIAN INFERENCE,0.2413793103448276,"""pn( ¯Q(Xn) −Q0), N(µ(Xn), ⌃)"
BACKGROUND ON BAYESIAN INFERENCE,0.2434077079107505,"# P−! 0
(3)"
BACKGROUND ON BAYESIAN INFERENCE,0.24543610547667344,"as n ! 1 for some µ(Xn) and ⌃, that do not depend on the prior, where the convergence in
115"
BACKGROUND ON BAYESIAN INFERENCE,0.24746450304259635,"probability is over sampling Xn ⇠p(Xn|Q0).
116"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.24949290060851928,"3
Bayesian Inference from Synthetic Data
117"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.2515212981744422,"When the downstream analysis is Bayesian, and the analyst has access to non-DP synthetic data,
118"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.2535496957403651,"they would ultimately want to obtain the posterior p(Q|X, IA) of some quantity Q given real data
119"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.25557809330628806,"X, where IA denotes the background knowledge such as priors of the analyst. In the DP case, the
120"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.25760649087221094,"exact posterior is unobtainable, so we assume that X is only available through a noisy summary ˜s (Ju
121"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.25963488843813387,"et al. 2022; Räisä et al. 2023), so the posterior is p(Q|˜s, IA). To unify these notations, we use Z to
122"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.2616632860040568,"denote the observed values, so Z = X in the non-DP case, Z = ˜s in the DP case, and the posterior
123"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.26369168356997974,"of interest is p(Q|Z, IA). We summarise these random variables and their dependencies in Figure 1,
124"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.2657200811359026,"and give an introduction to DP in Supplemental Section A.3.
125"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.26774847870182555,"In order to introduce the synthetic data into the posterior of interest, we can decompose the posterior
126"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.2697768762677485,"as
127"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.2718052738336714,"p(Q|Z, IA) = Z"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.2738336713995943,"p(Q|Z, X⇤, IA)p(X⇤|Z, IA) dX⇤,
(4)"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.27586206896551724,"where we abuse notation by using X⇤as the variable to integrate over, so inside the integral X⇤is
128"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.2778904665314402,"not a random variable. The decomposition in (4) means that we could sample p(Q|Z, IA) by ﬁrst
129"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.2799188640973631,"• ✓: data generating model parameters
• X: real data
• X⇤: hypothetical data
• Z: observed summary of X (Z = X without DP)
• XSyn: synthetic data, XSyn ⇠p(X⇤|Z, IS)
• Q: estimated quantity in downstream analysis
• IS: synthetic data generator’s background infor-"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.281947261663286,"mation
• IA: analyst’s background information ✓
Q X
X⇤ Z XSyn"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.2839756592292089,"Figure 1: Left: random variables in noise-aware uncertainty estimation from synthetic data. Right: a
Bayesian network describing the dependencies of the random variables."
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.28600405679513186,"sampling the synthetic data from the posterior predictive XSyn ⇠p(X⇤|Z, IA), and then sampling
130"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.2880324543610548,"Q ⇠p(Q|Z, X⇤= XSyn, IA).
131"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.29006085192697767,"Note that the random variable X⇤represents a hypothetical real dataset that could be obtained if more
132"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.2920892494929006,"data was collected, as seen in Figure 1, and it is not the synthetic dataset. The synthetic dataset XSyn
133"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.29411764705882354,"is a sample from the conditional distribution of X⇤given Z. For this reason, p(Q|Z, X⇤, IA) 6=
134"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.2961460446247465,"p(Q|Z, IA). To make our notation less cluttered, we write p( · |X⇤, · ) in place of p( · |X⇤=
135"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.29817444219066935,"XSyn, · ) in probabilities when the meaning is clear.
136"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.3002028397565923,"There are still two major issues with the decomposition in (4):
137"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.3022312373225152,"1. Sampling p(Q|Z, X⇤, IA) requires access to Z, which defeats the purpose of using synthetic
138"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.30425963488843816,"data.
139"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.30628803245436104,"2. X⇤needs to be sampled conditionally on the analyst’s background information IA, while
140"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.30831643002028397,"the synthetic data provider could have different background information IS.
141"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.3103448275862069,"To solve the ﬁrst issue, in Section 3.2 we show that if we replace p(Q|Z, X⇤, IA) inside the integral
142"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.31237322515212984,"of (4) with p(Q|X⇤, IA), the resulting distribution converges to the desired posterior,
143 Z"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.3144016227180527,"p(Q|X⇤, IA)p(X⇤|Z, IA) dX⇤! p(Q|Z, IA)
(5)"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.31643002028397565,"in total variation distance as the size of each synthetic data set X⇤grows. It should be noted that
144"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.3184584178498986,"many such synthetic data sets will be needed to account for the integral over X⇤.
145"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.3204868154158215,"The second issue is known as congeniality in the multiple imputation literature (Meng 1994; Xie and
146"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.3225152129817444,"Meng 2016). We look at congeniality in the context of Bayesian inference from synthetic data in
147"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.32454361054766734,"Section 3.1, and ﬁnd that we can obtain p(Q|Z, IA) under appropriate assumptions on the relationship
148"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.3265720081135903,"between IA and IS.
149"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.3286004056795132,"Exactly sampling the LHS of (5) requires generating a synthetic dataset for each sample of p(Q|Z, IA),
150"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.3306288032454361,"which is not practical. However, we can perform a Monte-Carlo approximation for p(Q|Z, IA) by
151"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.332657200811359,generating m synthetic datasets XSyn
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.33468559837728196,"1
, . . . , XSyn"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.3367139959432049,"m
⇠p(X⇤|Z, IA), drawing multiple samples from
152"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.33874239350912777,each of the p(Q|X⇤= XSyn
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.3407707910750507,"i
, IA), and mixing these samples, which allows us to obtain more than
153"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.34279918864097364,"one sample of p(Q|Z, IA) per synthetic dataset. We look at some properties of this in Supplemental
154"
BAYESIAN INFERENCE FROM SYNTHETIC DATA,0.3448275862068966,"Section E, but we use the integral form in (5) in the rest of our theory.
155"
CONGENIALITY,0.34685598377281945,"3.1
Congeniality
156"
CONGENIALITY,0.3488843813387424,"In the decomposition (4) of the analyst’s posterior, X⇤should be sampled conditionally on the
157"
CONGENIALITY,0.3509127789046653,"analyst’s background information IA, while in reality the synthetic data provider could have different
158"
CONGENIALITY,0.35294117647058826,"background information IS.
159"
CONGENIALITY,0.35496957403651114,"A similar distinction has been studied in the context of missing data (Meng 1994; Xie and Meng
160"
CONGENIALITY,0.35699797160243407,"2016), where the imputer of missing data has a similar role as the synthetic data generator. Meng
161"
CONGENIALITY,0.359026369168357,"(1994) found that Rubin’s rules implicitly assume that the probability models of both parties are
162"
CONGENIALITY,0.36105476673427994,"compatible in a certain sense, which Meng (1994) deﬁned as congeniality.
163"
CONGENIALITY,0.3630831643002028,"As our examples with Gaussian distributions in Section 4.1 and Supplemental Section C.2 show,
164"
CONGENIALITY,0.36511156186612576,"some notion of congeniality is also required in our setting. However, because we study synthetic data
165"
CONGENIALITY,0.3671399594320487,"instead of imputation, and Bayesian instead of frequentist downstream analysis, we need a different
166"
CONGENIALITY,0.3691683569979716,"formal deﬁnition. As the analyst only makes inferences on Q, it sufﬁces that both the analyst and
167"
CONGENIALITY,0.3711967545638945,"synthetic data generator make the same inferences of Q:
168"
CONGENIALITY,0.37322515212981744,"Deﬁnition 3.1. The background information sets IS and IA are congenial for observation Z if
169"
CONGENIALITY,0.3752535496957404,"p(Q|X⇤, IS) = p(Q|X⇤, IA)
(6)"
CONGENIALITY,0.3772819472616633,"for all X⇤and
170"
CONGENIALITY,0.3793103448275862,"p(Q|Z, IS) = p(Q|Z, IA).
(7)"
CONGENIALITY,0.3813387423935091,"In the non-DP case, (7) is redundant, as it is implied by (6), but in the DP case, both are needed, as
171"
CONGENIALITY,0.38336713995943206,"the parties may draw different conclusions on X given Z = ˜s.
172"
CONGENIALITY,0.385395537525355,"Combining congeniality and (5),
173 Z"
CONGENIALITY,0.38742393509127787,"p(Q|X⇤, IA)p(X⇤|Z, IS) dX⇤= Z"
CONGENIALITY,0.3894523326572008,"p(Q|X⇤, IS)p(X⇤|Z, IS) dX⇤"
CONGENIALITY,0.39148073022312374,"! p(Q|Z, IS) = p(Q|Z, IA), (8)"
CONGENIALITY,0.3935091277890467,"where the convergence is in total variation distance as the size of X⇤grows. In the following, we
174"
CONGENIALITY,0.39553752535496955,"assume congeniality, and drop IA and IS from our notation.
175"
CONSISTENCY PROOF,0.3975659229208925,"3.2
Consistency Proof
176"
CONSISTENCY PROOF,0.3995943204868154,"To recap, we want to prove that the posterior from synthetic data,
177"
CONSISTENCY PROOF,0.40162271805273836,¯pn(Q) = Z
CONSISTENCY PROOF,0.40365111561866124,p(Q|X⇤
CONSISTENCY PROOF,0.4056795131845842,n)p(X⇤
CONSISTENCY PROOF,0.4077079107505071,n|Z) dX⇤
CONSISTENCY PROOF,0.40973630831643004,"n,
(9)"
CONSISTENCY PROOF,0.4117647058823529,converges in total variation distance to p(Q|Z) as the size n of X⇤
CONSISTENCY PROOF,0.41379310344827586,"n grows. We prove this in
178"
CONSISTENCY PROOF,0.4158215010141988,"Theorem 3.4, which requires that both p(Q|Z, X⇤"
CONSISTENCY PROOF,0.4178498985801217,n) and p(Q|X⇤
CONSISTENCY PROOF,0.4198782961460446,"n) approach the same distribution as
179"
CONSISTENCY PROOF,0.42190669371196754,"n grows. We formally state this in Condition 3.2. In Lemma 3.3, we show that Condition 3.2 is a
180"
CONSISTENCY PROOF,0.4239350912778905,"consequence of the Bernstein–von Mises theorem (Theorem 2.2) under some additional assumptions,
181"
CONSISTENCY PROOF,0.4259634888438134,"so we expect it to hold in typical settings.
182"
CONSISTENCY PROOF,0.4279918864097363,"To make the notation more compact, let ¯Q+"
CONSISTENCY PROOF,0.4300202839756592,"n ⇠p(Q|Z, X⇤"
CONSISTENCY PROOF,0.43204868154158216,"n), and let ¯Qn ⇠p(Q|X⇤"
CONSISTENCY PROOF,0.4340770791075051,"n).
183"
CONSISTENCY PROOF,0.43610547667342797,"Condition 3.2. For all Q there exist distributions Dn such that
184 TV "" ¯Q+"
CONSISTENCY PROOF,0.4381338742393509,"n , Dn"
CONSISTENCY PROOF,0.44016227180527384,"# P−! 0
and
TV"
CONSISTENCY PROOF,0.4421906693711968,""" ¯Qn, Dn"
CONSISTENCY PROOF,0.44421906693711966,"# P−! 0
(10)"
CONSISTENCY PROOF,0.4462474645030426,"as n ! 1, where the convergence in probability is over sampling X⇤"
CONSISTENCY PROOF,0.4482758620689655,n ⇠p(X⇤
CONSISTENCY PROOF,0.45030425963488846,"n|Z, Q).
185"
CONSISTENCY PROOF,0.45233265720081134,"Theorem 2.2 implies Condition 3.2 with some additional assumptions:
186"
CONSISTENCY PROOF,0.4543610547667343,"Lemma 3.3. If the assumptions of Theorem 2.2 (Condition A.4) and the following assumptions:
187"
CONSISTENCY PROOF,0.4563894523326572,"(1) Z and X⇤are conditionally independent given Q; and
188"
CONSISTENCY PROOF,0.45841784989858014,"(2) p(Z|Q) > 0 for all Q,
189"
CONSISTENCY PROOF,0.460446247464503,"hold for the downstream analysis for all Q0, then Condition 3.2 holds.
190"
CONSISTENCY PROOF,0.46247464503042596,"Proof. The full proof is in Supplemental Section B.1. Proof idea: when Z and X⇤are conditionally
191"
CONSISTENCY PROOF,0.4645030425963489,"independent given Q,
192"
CONSISTENCY PROOF,0.4665314401622718,"p(Q|Z, X⇤) / p(X⇤|Q)p(Z|Q)p(Q)
(11)
so p(Q|Z, X⇤) can be equivalently seen as the result of Bayesian inference with observed data
193"
CONSISTENCY PROOF,0.4685598377281947,"X⇤and prior p(Q|Z). As the only difference to p(Q|X⇤) is the prior, the Bernstein–von Mises
194"
CONSISTENCY PROOF,0.47058823529411764,"theorem implies that both p(Q|Z, X⇤) and p(Q|X⇤) converge in total variation distance to the same
195"
CONSISTENCY PROOF,0.4726166328600406,"distribution.
196"
CONSISTENCY PROOF,0.4746450304259635,"Assumption (1) of Lemma 3.3 will hold if the downstream analysis treats its input data as an i.i.d.
197"
CONSISTENCY PROOF,0.4766734279918864,"sample from some distribution. Assumption (2) holds when the likelihood is always positive, and in
198"
CONSISTENCY PROOF,0.4787018255578093,"the DP case when the density of the privacy mechanism is positive everywhere, which is the case for
199"
CONSISTENCY PROOF,0.48073022312373226,"common DP mechanisms like the Gaussian and Laplace mechanisms (Dwork and Roth 2014).
200"
CONSISTENCY PROOF,0.4827586206896552,"Next is the main theorem of this work: (5) holds under Condition 3.2.
201"
CONSISTENCY PROOF,0.4847870182555781,"Theorem 3.4. Under congeniality and Condition 3.2, TV (p(Q|Z), ¯pn(Q)) ! 0 as n ! 1.
202"
CONSISTENCY PROOF,0.486815415821501,"Proof. The full proof is in Supplemental Section B.1. Proof idea: the proof consists of three steps.
203"
CONSISTENCY PROOF,0.48884381338742394,"The ﬁrst two are in Lemma B.1 and the third is in Lemma B.2 in the Supplement. The ﬁrst step
204"
CONSISTENCY PROOF,0.4908722109533469,"is showing that TV( ¯Qn, ¯Q+ n )"
CONSISTENCY PROOF,0.49290060851926976,P−! 0 when X⇤
CONSISTENCY PROOF,0.4949290060851927,n ⇠p(X⇤
CONSISTENCY PROOF,0.4969574036511156,"n|Z, Q) for ﬁxed Z and Q. This is a simple
205"
CONSISTENCY PROOF,0.49898580121703856,"consequence of the triangle inequality and Condition 3.2, as total variation distance is a metric. In the
206"
CONSISTENCY PROOF,0.5010141987829615,"second step, we show that TV( ¯Qn, ¯Q+ n )"
CONSISTENCY PROOF,0.5030425963488844,P−! 0 also holds when X⇤
CONSISTENCY PROOF,0.5050709939148073,n ⇠p(X⇤
CONSISTENCY PROOF,0.5070993914807302,"n|Z). In the ﬁnal step, we
207"
CONSISTENCY PROOF,0.5091277890466531,"show that this implies the claim.
208"
CONVERGENCE RATE,0.5111561866125761,"3.3
Convergence Rate
209"
CONVERGENCE RATE,0.513184584178499,"Under stronger regularity conditions, we can get a convergence rate for Theorem 3.4. The regularity
210"
CONVERGENCE RATE,0.5152129817444219,"conditions depend on uniform integrability:
211"
CONVERGENCE RATE,0.5172413793103449,"Deﬁnition 3.5. A sequence of random variables Xn is uniformly integrable if
212"
CONVERGENCE RATE,0.5192697768762677,"lim
M!1 sup"
CONVERGENCE RATE,0.5212981744421906,"n E(|Xn|I|Xn|>M) = 0
(12)"
CONVERGENCE RATE,0.5233265720081136,"Now we can state the regularity conditions for a convergence rate O(Rn):
213"
CONVERGENCE RATE,0.5253549695740365,"Condition 3.6. There exist distributions Dn such that for a sequence R1, R2, · · · > 0, Rn ! 0 as
214"
CONVERGENCE RATE,0.5273833671399595,"n ! 1,
215"
RN,0.5294117647058824,"1
Rn TV "" ¯Q+"
RN,0.5314401622718052,"n , Dn #"
RN,0.5334685598377282,"and
1
Rn TV"
RN,0.5354969574036511,""" ¯Qn, Dn # (13)"
RN,0.537525354969574,are uniformly integrable when X⇤
RN,0.539553752535497,n ⇠p(X⇤
RN,0.5415821501014199,"n|Z).
216"
RN,0.5436105476673428,Note that X⇤
RN,0.5456389452332657,n ⇠p(X⇤
RN,0.5476673427991886,"n|Z) conditions on Z, not Q and Z like in Condition 3.2. We prove that
217"
RN,0.5496957403651116,"Condition 3.6 is met in univariate Gaussian mean estimation for Rn =
1
pn in Theorem D.1 in the
218"
RN,0.5517241379310345,"Supplement. This is the same rate that commonly applies in the Bernstein–von Mises theorem (Hipp
219"
RN,0.5537525354969574,"and Michel 1976).
220"
RN,0.5557809330628803,"Condition 3.6 implies an O(Rn) convergence rate:
221"
RN,0.5578093306288032,"Theorem 3.7. Under congeniality and Condition 3.6, TV (p(Q|Z), ¯pn(Q)) = O(Rn).
222"
RN,0.5598377281947262,"Proof. The full proof is in Supplemental Section B.2. Proof idea: ﬁrst, we prove the uniform
223"
RN,0.5618661257606491,"integrability of
1
Rn TV( ¯Qn, ¯Q+"
RN,0.563894523326572,n ) when X⇤
RN,0.565922920892495,n ⇠p(X⇤
RN,0.5679513184584178,"n|Z) by using the triangle inequality and properties
224"
RN,0.5699797160243407,"of uniform integrability. Second, we prove that this implies the claimed convergence rate.
225"
EXAMPLES,0.5720081135902637,"4
Examples
226"
EXAMPLES,0.5740365111561866,"In this section, we present two examples of downstream inference from synthetic data at a high level.
227"
EXAMPLES,0.5760649087221096,"First, we demonstrate univariate Gaussian mean estimation. Second, we have logistic regression on
228"
EXAMPLES,0.5780933062880325,"a toy dataset, with DP synthetic data. In the ﬁrst example, we use the tractability of the model to
229"
EXAMPLES,0.5801217038539553,"derive additional theoretical properties, and in both examples, we experimentally verify our theory.
230"
EXAMPLES,0.5821501014198783,"	
	

	


     "
EXAMPLES,0.5841784989858012,"  ""$!""  "
EXAMPLES,0.5862068965517241,"	
	

	


     "
EXAMPLES,0.5882352941176471,"  ""$!""  "
EXAMPLES,0.59026369168357,"%!""!	& 
"" # !	& 
""!%"" )	"
EXAMPLES,0.592292089249493,"Figure 2: Simulation results for the Gaussian mean estimation example, showing that the mixture of
posteriors from synthetic data in green converges. In the left panel, both the analyst and data provider
have the correct known variance. The blue and orange lines overlap, as both parties have the same
p(µ|X). On the right, the analyst’s known variance is too small (ˆσ2 k = 1 4 ¯σ2"
EXAMPLES,0.5943204868154158,"k), so congeniality is not
met, but the mixture of posteriors from synthetic data, ¯pn(µ), still converges to the data provider’s
posterior. In both panels, m = 400 and nX⇤"
EXAMPLES,0.5963488843813387,nX = 20.    
EXAMPLES,0.5983772819472617," 
         "
EXAMPLES,0.6004056795131846," 
         "
EXAMPLES,0.6024340770791075," 
     "
EXAMPLES,0.6044624746450304,"	 
	 	"
EXAMPLES,0.6064908722109533,"Figure 3: Convergence of the mixture of posteriors from synthetic data with different sizes of the
synthetic dataset on Gaussian mean estimation with known variance. nX⇤= nX is clearly not
enough, but nX⇤= 20nX is already relatively good."
EXAMPLES,0.6085192697768763,"Supplemental Section C, contains more detailed descriptions of the examples, and some additional
231"
EXAMPLES,0.6105476673427992,"results. Supplemental Section D proves an O( 1
pn) convergence rate for Theorem 3.4 in the Gaussian
232"
EXAMPLES,0.6125760649087221,"mean estimation case. Our code is in the supplementary material.
233"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6146044624746451,"4.1
Non-private Gaussian Mean Estimation
234"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6166328600405679,"Our ﬁrst example is very simple: the analyst infers the mean µ of a Gaussian distribution with known
235"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6186612576064908,"variance from synthetic data that has been generated from the same model. The posteriors for this
236"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6206896551724138,"setting can be found in Supplemental Section A.4. To differentiate the variables for the analyst and
237"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6227180527383367,"data provider, we use bars for the data provider (like ¯σ2"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6247464503042597,0) and hats for the analyst (like ˆσ2
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6267748478701826,"0).
238"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6288032454361054,When the synthetic data is generated from the known variance model with known variance ¯σ2
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6308316430020284,"k, we
239"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6328600405679513,"sample from the posterior predictive p(X⇤|X) as
240"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6348884381338742,"¯µ|X ⇠N(¯µnX, ¯σ2"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6369168356997972,"nX),
X⇤|¯µ ⇠N nX⇤(¯µ, ¯σ2"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6389452332657201,"k)
(14)"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.640973630831643,¯µnX =
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6430020283975659,"1
¯
σ02 ¯µ0 + nX ¯σ2 k ¯X 1
¯σ2"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6450304259634888,0 + nX ¯σ2 k
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6470588235294118,",
1
¯σ2nX = 1 ¯σ2 0 + nX ¯σ2 k"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6490872210953347,".
(15)"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6511156186612576,"N nX⇤denotes a Gaussian distribution over nX⇤i.i.d. samples.
241"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6531440162271805,When downstream analysis is the model with known variance ˆσ2
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6551724137931034,"k, we have
242"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6572008113590264,"ˆµ|X⇤⇠N(ˆµnX⇤, ˆσ2"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6592292089249493,"nX⇤),
ˆµnX⇤="
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6612576064908722,"1
ˆ
σ02 ˆµ0 + nX⇤ ˆσ2 k ¯X⇤ 1
ˆσ2"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6632860040567952,0 + nX⇤ ˆσ2 k
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.665314401622718,",
1
ˆσ2nX⇤ = 1 ˆσ2 0 + nX⇤ ˆσ2 k"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6673427991886409,".
(16)"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6693711967545639,"Now, using µ⇤to denote a sample from the mixture of posteriors from synthetic data ¯pn(µ) in (9),
243"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6713995943204868,"we show in Supplemental Section C.1 that
244"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6734279918864098,"E(µ⇤) ! ¯µnX,
Var(µ⇤) ! ¯σ2"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6754563894523327,"nX
(17)
as nX⇤! 1, so µ⇤asymptotically has the same mean and variance as the downstream posterior
245"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6774847870182555,"distribution p(µ|X) on the real data.
246"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6795131845841785,"We test the theory with a numerical simulation in Figure 2. We generated the real data X of size
247"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6815415821501014,"nX = 100 by i.i.d. sampling from N(1, 4). Both the analyst and data provider use N(0, 102) as the
248"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6835699797160243,prior. The data provider uses the correct known variance (¯σ2
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6855983772819473,"k = 4), and the analyst either uses the
249"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6876267748478702,correct known variance (ˆσ2
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6896551724137931,"k = 4), or a too small known variance (ˆσ2"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.691683569979716,"k = 1), which is an example of
250"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6937119675456389,"uncongeniality.
251"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6957403651115619,"In the congenial case in the left panel of Figure 2, both parties have the same posterior given the real
252"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6977687626774848,"data X, and the mixture of posteriors from synthetic data is very close to that. In the uncongenial case
253"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.6997971602434077,"in the right panel, where the analyst underestimates the variance, the parties have different posteriors
254"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.7018255578093306,"given X, but the mixture of synthetic data posteriors is still close to the data provider’s posterior.
255"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.7038539553752535,"In Figure 3, we examine the convergence of the mixture of posteriors from synthetic data under
256"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.7058823529411765,"congeniality. We see that setting nX⇤= nX is not enough, as the mixture of posteriors is signiﬁcantly
257"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.7079107505070994,"wider than the analyst’s posterior. The synthetic dataset needs to be larger than the original, with
258"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.7099391480730223,"nX⇤= 5nX already giving a decent approximation and nX⇤= 20nX a rather good one. In Figure S1
259"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.7119675456389453,"in the Supplement, we also examine the effect of m on the mixture of synthetic data posteriors, and
260"
NON-PRIVATE GAUSSIAN MEAN ESTIMATION,0.7139959432048681,"see that m must also be sufﬁciently large, otherwise the method produces very jagged posteriors.
261"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.716024340770791,"4.2
Differentially Private Logistic Regression
262"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.718052738336714,")


	
	




!   "
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7200811359026369," !"""
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7221095334685599,"!	"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7241379310344828,")	
)	
)


	
	
!   "
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7261663286004056," !"""
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7281947261663286,"!"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7302231237322515,"""! (
""!# '

"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7322515212981744,"Figure 4: Posteriors in the DP logistic regression experiment, where Q are the regression coefﬁcients.
The mixture of posteriors from synthetic data, ¯pn(Q), (with nX⇤/nX = 20, m = 400) is very close
the to the private posterior p(Q|˜s) computed using (4). Computing the posterior without synthetic data
with DP-GLM gives a somewhat wider posterior. The true parameter values are highlighted by the
grey dashed lines and shown in the panel titles. The privacy bounds are ✏= 1, δ = n−2"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7342799188640974,X = 2.5 · 10−7.
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7363083164300203,"Our second example is logistic regression on a simple 3-d binary toy dataset, (nX = 2000), with DP
263"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7383367139959433,"synthetic data, under the same setting as used by Räisä et al. (2023) for frequentist logistic regression.
264"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7403651115618661,"We change the downstream task to Bayesian logistic regression to evaluate our theory.
265"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.742393509127789,"Under DP, Z is a noisy summary ˜s of the real data. We need synthetic data sampled from the posterior
266"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.744421906693712,"predictive p(X⇤|˜s), which is exactly what the NAPSU-MQ algorithm of Räisä et al. (2023) provides.
267"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7464503042596349,"In NAPSU-MQ, ˜s is the values of user-selected marginal queries with added Gaussian noise. We used
268"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7484787018255578,"the open-source implementation of NAPSU-MQ1 by Räisä et al. (2023), and describe NAPSU-MQ
269"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7505070993914807,"in Supplemental Section A.3.
270"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7525354969574036,1https://github.com/DPBayes/NAPSU-MQ-experiments
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7545638945233266,"Because of the simplicity of this model, it is possible to use the exact posterior decomposition (4)
271"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7565922920892495,"as a baseline, by using p(X|˜s) instead of p(X⇤|˜s) to generate synthetic data. We give a detailed
272"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7586206896551724,"description of this process in Supplemental Section C.5. We have also included the DP-GLM
273"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7606490872210954,"algorithm (Kulkarni et al. 2021) that does not use synthetic data, and the non-DP posterior from the
274"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7626774847870182,"real data as baselines. We obtained the code for DP-GLM from Kulkarni et al. (2021) upon request.
275"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7647058823529411,"Figure 4 compares the mixture of posteriors from synthetic data from (9) that uses p(Q|X⇤), with
276"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7667342799188641,"nX⇤/nX = 20 and m = 400 synthetic datasets, to the baselines. The posterior from (9) is very close
277"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.768762677484787,"to the posterior from (4). The DP-GLM posterior that does not use synthetic data is somewhat wider.
278"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.77079107505071,"The privacy bounds are ✏= 1, δ = n−2"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7728194726166329,"X = 2.5 · 10−7.
279"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7748478701825557,"We ran the experiment 100 times and also with ✏= 0.1 and ✏= 0.5, and plot coverages and widths
280"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7768762677484787,"of credible intervals in Figure S4 in the Supplement. With ✏= 1 and ✏= 0.5, the coverages are
281"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7789046653144016,"accurate and DP-GLM consistently produces wider intervals. With ✏= 0.1, the mixture of synthetic
282"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7809330628803245,"data posteriors likely needs more and larger synthetic datasets to converge, as it produced wider and
283"
DIFFERENTIALLY PRIVATE LOGISTIC REGRESSION,0.7829614604462475,"slightly overconﬁdent intervals for one coefﬁcient.
284"
DISCUSSION,0.7849898580121704,"5
Discussion
285"
DISCUSSION,0.7870182555780934,"Synthetic data are often considered as a substitute for real data that are sensitive. Since the data
286"
DISCUSSION,0.7890466531440162,"generation process is based on having access to the Z, one might ask why is the synthetic data needed
287"
DISCUSSION,0.7910750507099391,"in ﬁrst place. Why cannot we simply perform the downstream posterior analysis directly using Z?
288"
DISCUSSION,0.7931034482758621,"Our analysis allows Z to be an arbitrary, even noisy, representation of the data, and it might be
289"
DISCUSSION,0.795131845841785,"difﬁcult for the analyst to place a model for such generative process for Q. In most applications, the
290"
DISCUSSION,0.7971602434077079,"analyst does have a model for Q arising from the data. Therefore using the synthetic data as a proxy
291"
DISCUSSION,0.7991886409736308,"for the Z allows the analyst to use existing models and inference methods to perform the analysis.
292"
DISCUSSION,0.8012170385395537,"Limitations
A clear limitation of mixing posteriors from multiple synthetic datasets is the compu-
293"
DISCUSSION,0.8032454361054767,"tational cost of analysing many large synthetic datasets, which may be substantial for more complex
294"
DISCUSSION,0.8052738336713996,"Bayesian downstream models, where even a single analysis can be computationally expensive. How-
295"
DISCUSSION,0.8073022312373225,"ever, the separate analyses can be run in parallel. We also expect the downstream posteriors of
296"
DISCUSSION,0.8093306288032455,"different synthetic datasets to be similar to each other, so it should be possible to use the information
297"
DISCUSSION,0.8113590263691683,"gained from sampling a few of them to speed up sampling the others.
298"
DISCUSSION,0.8133874239350912,"Under DP, we need noise-aware synthetic data generation, which limits the settings in which the
299"
DISCUSSION,0.8154158215010142,"method can currently be applied. However, if new noise-aware methods are developed in the future,
300"
DISCUSSION,0.8174442190669371,"the method can immediately be used with them.
301"
DISCUSSION,0.8194726166328601,"To recover the analyst’s posterior, the method requires congeniality, which basically requires the
302"
DISCUSSION,0.821501014198783,"analyst’s prior to be compatible with the data provider’s. However, the method was still able to
303"
DISCUSSION,0.8235294117647058,"recover the data provider’s posterior in the Gaussian example, suggesting that the data provider’s
304"
DISCUSSION,0.8255578093306288,"prior information overrides the analyst’s prior information. This suggests an interesting area of future
305"
DISCUSSION,0.8275862068965517,"research: analysis methods that override the data provider’s prior. An importance sampling approach
306"
DISCUSSION,0.8296146044624746,"similar to that of Ghalebikesabi et al. (2022) could provide one approach. This observation also raises
307"
DISCUSSION,0.8316430020283976,"interesting questions on whether truly general and objective synthetic data generation is possible.
308"
DISCUSSION,0.8336713995943205,"Conclusion
We considered the problem of consistent Bayesian inference of downstream analyses
309"
DISCUSSION,0.8356997971602435,"using multiple, potentially DP, synthetic datasets, and studied an inference method that mixes the
310"
DISCUSSION,0.8377281947261663,"posteriors from multiple large synthetic datasets. We proved, under general and well-understood
311"
DISCUSSION,0.8397565922920892,"regularity conditions of the Bernstein–von Mises theorem, that the method is asymptotically exact as
312"
DISCUSSION,0.8417849898580122,"the sizes of the synthetic datasets grow. We also derived a convergence rate under stricter regularity
313"
DISCUSSION,0.8438133874239351,"conditions. We studied the method in two examples: non-private Gaussian mean estimation and
314"
DISCUSSION,0.845841784989858,"DP logistic regression. In the former, we were able to use the analytically tractable structure of the
315"
DISCUSSION,0.847870182555781,"setting to derive additional properties of the method, including a convergence rate without additional
316"
DISCUSSION,0.8498985801217038,"assumptions. In both settings, we experimentally validated our theory, and showed that the method
317"
DISCUSSION,0.8519269776876268,"works in practice. This ﬁlls a major gap in the synthetic data analysis literature, unlocking consistent
318"
DISCUSSION,0.8539553752535497,"Bayesian inference while reusing existing downstream analysis methods.
319"
REFERENCES,0.8559837728194726,"References
320"
REFERENCES,0.8580121703853956,"Balle, B. and Y.-X. Wang (2018). “Improving the Gaussian Mechanism for Differential Privacy: Ana-
321"
REFERENCES,0.8600405679513184,"lytical Calibration and Optimal Denoising”. In: Proceedings of the 35th International Conference
322"
REFERENCES,0.8620689655172413,"on Machine Learning. Vol. 80. Proceedings of Machine Learning Research. PMLR, pp. 394–403.
323"
REFERENCES,0.8640973630831643,"Bernstein, G. and D. Sheldon (2018). “Differentially Private Bayesian Inference for Exponential
324"
REFERENCES,0.8661257606490872,"Families”. In: Advances in Neural Information Processing Systems. Vol. 31, pp. 2924–2934.
325"
REFERENCES,0.8681541582150102,"Bernstein, G. and D. Sheldon (2019). “Differentially Private Bayesian Linear Regression”. In:
326"
REFERENCES,0.8701825557809331,"Advances in Neural Information Processing Systems. Vol. 32, pp. 523–533.
327"
REFERENCES,0.8722109533468559,"Billingsley, Patrick. (1995). Probability and Measure. 3rd ed. Wiley Series in Probability and
328"
REFERENCES,0.8742393509127789,"Mathematical Statistics. New York, NY: Wiley.
329"
REFERENCES,0.8762677484787018,"Bissiri, P. G., C. C. Holmes, and S. G. Walker (2016). “A General Framework for Updating Belief
330"
REFERENCES,0.8782961460446247,"Distributions”. In: Journal of the Royal Statistical Society. Series B, Statistical Methodology 78.5,
331"
REFERENCES,0.8803245436105477,"pp. 1103–1130.
332"
REFERENCES,0.8823529411764706,"Duane, S., A. D. Kennedy, B. J. Pendleton, and D. Roweth (1987). “Hybrid Monte Carlo”. In: Physics
333"
REFERENCES,0.8843813387423936,"letters B 195.2, pp. 216–222.
334"
REFERENCES,0.8864097363083164,"Dwork, C. (2008). “Differential Privacy: A Survey of Results”. In: International Conference on
335"
REFERENCES,0.8884381338742393,"Theory and Applications of Models of Computation. Springer, pp. 1–19.
336"
REFERENCES,0.8904665314401623,"Dwork, C., K. Kenthapadi, F. McSherry, I. Mironov, and M. Naor (2006a). “Our Data, Ourselves:
337"
REFERENCES,0.8924949290060852,"Privacy Via Distributed Noise Generation”. In: Advances in Cryptology - EUROCRYPT. Vol. 4004.
338"
REFERENCES,0.8945233265720081,"Lecture Notes in Computer Science. Springer, pp. 486–503.
339"
REFERENCES,0.896551724137931,"Dwork, C., F. McSherry, K. Nissim, and A. D. Smith (2006b). “Calibrating Noise to Sensitivity in
340"
REFERENCES,0.8985801217038539,"Private Data Analysis”. In: Third Theory of Cryptography Conference. Vol. 3876. Lecture Notes in
341"
REFERENCES,0.9006085192697769,"Computer Science. Springer, pp. 265–284.
342"
REFERENCES,0.9026369168356998,"Dwork, C. and A. Roth (2014). “The Algorithmic Foundations of Differential Privacy”. In: Founda-
343"
REFERENCES,0.9046653144016227,"tions and Trends in Theoretical Computer Science 9.3-4, pp. 211–407.
344"
REFERENCES,0.9066937119675457,"Gelman, A., J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin (2014). Bayesian
345"
REFERENCES,0.9087221095334685,"Data Analysis. Third edition. Chapman & Hall/CRC Texts in Statistical Science Series. Boca
346"
REFERENCES,0.9107505070993914,"Raton: CRC Press.
347"
REFERENCES,0.9127789046653144,"Ghalebikesabi, S., H. Wilde, J. Jewson, A. Doucet, S. Vollmer, and C. Holmes (2022). “Mitigating
348"
REFERENCES,0.9148073022312373,"Statistical Bias within Differentially Private Synthetic Data”. In: Proceedings of the Thirty-Eighth
349"
REFERENCES,0.9168356997971603,"Conference on Uncertainty in Artiﬁcial Intelligence. PMLR, pp. 696–705.
350"
REFERENCES,0.9188640973630832,"Gilks, W. R., N. G. Best, and K. K. C. Tan (1995). “Adaptive Rejection Metropolis Sampling Within
351"
REFERENCES,0.920892494929006,"Gibbs Sampling”. In: Journal of the Royal Statistical Society Series C: Applied Statistics 44.4,
352"
REFERENCES,0.922920892494929,"pp. 455–472.
353"
REFERENCES,0.9249492900608519,"Hipp, C. and R. Michel (1976). “On the Bernstein-v. Mises Approximation of Posterior Distributions”.
354"
REFERENCES,0.9269776876267748,"In: The Annals of Statistics 4.5, pp. 972–980.
355"
REFERENCES,0.9290060851926978,"Hoffman, M. D. and A. Gelman (2014). “The No-U-Turn Sampler: Adaptively Setting Path Lengths
356"
REFERENCES,0.9310344827586207,"in Hamiltonian Monte Carlo.” In: Journal of Machine Learning Research 15.1, pp. 1593–1623.
357"
REFERENCES,0.9330628803245437,"Ju, N., J. Awan, R. Gong, and V. Rao (2022). “Data Augmentation MCMC for Bayesian Inference
358"
REFERENCES,0.9350912778904665,"from Privatized Data”. In: Advances in Neural Information Processing Systems. Vol. 35, pp. 12732–
359"
REFERENCES,0.9371196754563894,"12743.
360"
REFERENCES,0.9391480730223124,"Kelbert, M. (2023). “Survey of Distances between the Most Popular Distributions”. In: Analytics 2.1,
361"
REFERENCES,0.9411764705882353,"pp. 225–245.
362"
REFERENCES,0.9432048681541582,"Kulkarni, T., J. Jälkö, A. Koskela, S. Kaski, and A. Honkela (2021). “Differentially Private Bayesian
363"
REFERENCES,0.9452332657200812,"Inference for Generalized Linear Models”. In: Proceedings of the 38th International Conference on
364"
REFERENCES,0.947261663286004,"Machine Learning. Vol. 139. Proceedings of Machine Learning Research. PMLR, pp. 5838–5849.
365"
REFERENCES,0.949290060851927,"Liew, C. K., U. J. Choi, and C. J. Liew (1985). “A Data Distortion by Probability Distribution”. In:
366"
REFERENCES,0.9513184584178499,"ACM Transactions on Database Systems 10.3, pp. 395–411.
367"
REFERENCES,0.9533468559837728,"Meng, X.-L. (1994). “Multiple-Imputation Inferences with Uncongenial Sources of Input”. In:
368"
REFERENCES,0.9553752535496958,"Statistical Science 9.4.
369"
REFERENCES,0.9574036511156186,"Neal, R. M. (2011). “MCMC Using Hamiltonian Dynamics”. In: Handbook of Markov Chain Monte
370"
REFERENCES,0.9594320486815415,"Carlo. Chapman & Hall / CRC Press.
371"
REFERENCES,0.9614604462474645,"Raghunathan, T. E., J. P. Reiter, and D. B. Rubin (2003). “Multiple Imputation for Statistical
372"
REFERENCES,0.9634888438133874,"Disclosure Limitation”. In: Journal of Ofﬁcial Statistics 19.1, p. 1.
373"
REFERENCES,0.9655172413793104,"Räisä, O., J. Jälkö, S. Kaski, and A. Honkela (2023). “Noise-Aware Statistical Inference with
374"
REFERENCES,0.9675456389452333,"Differentially Private Synthetic Data”. In: Proceedings of The 26th International Conference on
375"
REFERENCES,0.9695740365111561,"Artiﬁcial Intelligence and Statistics. PMLR, pp. 3620–3643.
376"
REFERENCES,0.9716024340770791,"Reiter, J. P. (2002). “Satisfying Disclosure Restrictions with Synthetic Data Sets”. In: Journal of
377"
REFERENCES,0.973630831643002,"Ofﬁcial Statistics 18.4, p. 531.
378"
REFERENCES,0.9756592292089249,"Rubin, D. B. (1987). Multiple Imputation for Nonresponse in Surveys. New York: John Wiley \&
379"
REFERENCES,0.9776876267748479,"Sons.
380"
REFERENCES,0.9797160243407708,"Rubin, D. B. (1993). “Discussion: Statistical Disclosure Limitation”. In: Journal of Ofﬁcial Statistics
381"
REFERENCES,0.9817444219066938,"9.2, pp. 461–468.
382"
REFERENCES,0.9837728194726166,"van der Vaart, A. W. (1998). Asymptotic Statistics. Repr. 2000. Cambridge Series in Statistical and
383"
REFERENCES,0.9858012170385395,"Probabilistic Mathematics. Cambridge: Cambridge University Press.
384"
REFERENCES,0.9878296146044625,"Wilde, H., J. Jewson, S. J. Vollmer, and C. Holmes (2021). “Foundations of Bayesian Learning from
385"
REFERENCES,0.9898580121703854,"Synthetic Data”. In: The 24th International Conference on Artiﬁcial Intelligence and Statistics.
386"
REFERENCES,0.9918864097363083,"Vol. 130. Proceedings of Machine Learning Research. PMLR, pp. 541–549.
387"
REFERENCES,0.9939148073022313,"Xie, X. and X.-L. Meng (2016). “Dissecting Multiple Imputation from a Multi-Phase Inference
388"
REFERENCES,0.9959432048681541,"Perspective: What Happens When God’s, Imputer’s and Analyst’s Models Are Uncongenial?” In:
389"
REFERENCES,0.9979716024340771,"Statistica Sinica.
390"
