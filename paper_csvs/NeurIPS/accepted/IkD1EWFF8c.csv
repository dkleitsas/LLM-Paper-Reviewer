Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0016638935108153079,"The transferability of adversarial perturbations provides an effective shortcut for
black-box attacks. Targeted perturbations have greater practicality but are more dif-
ﬁcult to transfer between models. In this paper, we experimentally and theoretically
demonstrated that neural networks trained on the same dataset have more consistent
performance in High-Sample-Density-Regions (HSDR) of each class instead of
low sample density regions. Therefore, in the target setting, adding perturbations
towards HSDR of the target class is more effective in improving transferability.
However, density estimation is challenging in high-dimensional scenarios. Further
theoretical and experimental veriﬁcation demonstrates that easy samples with low
loss are more likely to be located in HSDR. Perturbations towards such easy sam-
ples in the target class can avoid density estimation for HSDR location. Based on
the above facts, we veriﬁed that adding perturbations to easy samples in the target
class improves targeted adversarial transferability of existing attack methods. A
generative targeted attack strategy named Easy Sample Matching Attack (ESMA)
is proposed, which has a higher success rate for targeted attacks and outperforms
the SOTA generative method. Moreover, ESMA requires only 5% of the storage
space and much less computation time comparing to the current SOTA, as ESMA
attacks all classes with only one model instead of seperate models for each class.
Our code is available at https://github.com/gjq100/ESMA"
INTRODUCTION,0.0033277870216306157,"1
Introduction"
INTRODUCTION,0.004991680532445923,"Deep learning models exhibits substantial computational capacity in many downstream tasks, but
are vulnerable to adversarial attacks [1, 2]. Such attacks tend to be transferable [3, 4] as well as
real-world achievable [5], which makes it implementable in black-box scenarios. Targeted attacks are
known to be more difﬁcult to transfer [4, 6] compared with non-targeted attacks."
INTRODUCTION,0.0066555740432612314,"Intuitively, directions and transferability of adversarial attacks are closely related, but their relationship
is rarely discussed. [7] found that samples in low density regions of ground truth distribution are more
susceptible to adversarial attacks. They also veriﬁed that adversarial perturbations that aligned with
the low density direction of ground truth distribution can lead to better transferability. For targeted
attack scenarios, the direction that can bring more transferability has not been fully researched yet is
not clear."
INTRODUCTION,0.008319467554076539,"Findings and arguments.
In non-targeted scenarios, directing towards low-density regions
of the ground-truth distribution can improve adversarial transferability [7]. However, this ap-
proach is not direct enough for targeted attacks.
As shown in Figure 1, perturbations point-
ing at High-Sample-Density-Regions (HSDR) of the target domain are more effective than that
pointing at low-density regions of the ground-truth distribution, it perturbates samples more di-
rectly to the target discrimination region.
Moreover, we demonstrate and theoretically prove
that neural networks tend to have more consistent outputs in the HSDR of each class, which
means that perturbations pointing to the HSDR of the target class are also transferable between
different models.
However, density estimation for samples in high dimension is challenging."
INTRODUCTION,0.009983361064891847,"Figure 1: A schematic example of our
motivation, plotting the probability den-
sity (darker the color represents larger
the density) and samples for three pop-
ulations (orange, cyan, and green). The
black line indicates the Bayesian dis-
criminant boundary."
INTRODUCTION,0.011647254575707155,"Fortunately, by the deﬁnitions of hard samples and easy
samples in [8], we found a fact that helps, in each cate-
gory, easy samples with smaller losses are more likely to
locate in HSDR. We provide theoretical and experimental
assurance for this fact. Based on this fact, we can directly
perturbate towards such easy samples of the target domain
to improve transferability without density estimation."
INTRODUCTION,0.013311148086522463,"Related works.
Adversarial attacks can be divided into
white-box attacks and black-box attacks. In the white-box
setting, attackers have access to the model’s structure and
parameters, while in the black-box setting, they have no
such information but only access to the input and output
of the model. This is typically the scenario encountered
in real-world situations. Black-box attacks include query-
based attacks [9, 10, 11] and transfer-based attacks [12,
13, 14]. Conducting too many queries is impractical in
real-world applications. In contrast, transfer-based attacks
are more feasible as they do not require queries. Transfer-
based attacks often require the use of a white-box surrogate
model to produce adversarial perturbations. In terms of
the way adversarial perturbations are generated, there are
two approaches: iterative instance-speciﬁc methods and generative methods. Iterative instance-
speciﬁc methods utilize model gradients to iteratively add perturbations to speciﬁed samples (e.g.,
FGSM [3], C&W [5], PGD [15]). To enhance the transferability of adversarial samples, subsequent
work combines these methods with various techniques, such as introducing momentum [12, 13] and
considering input transformations [13, 14, 16, 17] during iterations, training auxiliary classiﬁers
[18, 19], or substituting different loss functions [20, 21, 22]. However, instance-speciﬁc methods
are primarily designed for non-targeted scenarios, often lacking effectiveness in targeted settings.
Although relatively good results have been achieved for targeted attacks [20, 21, 23], instance-speciﬁc
methods still require iteratively creating perturbations for each speciﬁed sample, while generators
trained for generating adversarial perturbations can be generalized on more samples after training
[24, 25, 26, 27]."
INTRODUCTION,0.014975041597337771,"The leading perturbation generative method currently is TTP [24], which employs a target-speciﬁc
GAN to align clean and augmented data from the source domain with the target domain data. However,
TTP necessitate training a dedicated generator for each class. This signiﬁcantly increases storage
requirements and training time. In our work, we design a generator for simultaneous attacks on
all target classes, which uses class embedding information for each target class. To construct these
embeddings, we adopt techniques similar to SNE [28] to align the surrogate model’s output logits for
target classes with the generator’s embeddings. This enables latent features learned by the surrogate
model to guide the construction of well-structured class embeddings. Building on our ﬁndings,
we train a multi-class perturbation generator to simultaneously perturb the source domain samples
towards easy samples with low loss of each target class. Experiments on the ImageNet dataset shown
that our method can obtain a better target transfer success rate than TTP, while requiring much less
storage. Our method also has certain advantages in training time."
INTRODUCTION,0.016638935108153077,"Therefore, our contribution can be summarized as follows:"
INTRODUCTION,0.018302828618968387,"• We found that deep learning models have more consistent outputs in HSDR of each class, as
demonstrated theoretically and through experiments (Section 2.1). This implies that adding"
INTRODUCTION,0.019966722129783693,"adversarial perturbations pointing to the HSDR of the target class results in better targeted
adversarial transferability."
INTRODUCTION,0.021630615640599003,"• We experimentally and theoretically veriﬁed that easy samples with low loss in early-
stopping models are likely to be located in HSDR, which allows us to directly add per-
turbations pointing to such samples of the target class to improve targeted adversarial
transferability (Section 2.2). This avoids density estimation of high-dimensional samples,
which is challenging and computationally expensive."
INTRODUCTION,0.02329450915141431,"• We introduced the Easy Sample Matching Attack (ESMA) (Section 3). ESMA achieves a
higher targeted transfer success rate compared to SOTA generative attacks TTP. Furthermore,
it only needs one model to perform attacks for all target classes (Section 4), which requires
much less storage space than TTP (only about 1/20 of the storage space in 10 targets case),
and requires less training time."
INTRODUCTION,0.024958402662229616,"Our ﬁndings and conclusions can not only provide guidance for target transfer attacks, but more
importantly, they reveal the consistency of deep learning models in the HSDR, and the correlation
between sample difﬁculty and local sample density. Not only that, the design of our multi-target
perturbation generative model can provide a new reference for subsequent related research."
MAIN CONCLUSIONS,0.026622296173044926,"2
Main conclusions"
MAIN CONCLUSIONS,0.028286189683860232,"In this section, we combine illustrative experiments and theoretical proofs to illustrate our two
conclusions in turn, and we use the following notations and deﬁnitions.
Z = X × Y ∈
Rd × R is the sample space. The i.i.d. dataset S = {xi, yi}n
i=1 consists of n sample pairs
(xi, yi), 1 ≤i ≤n.
Given y, the conditional distribution of x is Dx|y.
Feature mapping
f : X →RK, where K is the number of class, the class of feature mapping f ∈F. Specif-
ically, the parametrized class Fw :=

fw : X →RK, w ∈W
	
, where W is the parameter
space. Deﬁne FS
w :=

f S
w = S ◦fw : fw ∈Fw, S ◦fw(x) = S(fw(x))
	
, S denotes Softmax-
transformation. The Softmax-Cross-Entropy loss is denoted as ℓsce(·, ·) : RK × R →R. Let
Sj := {(x, y) ∈S : y = j}, Ij :=

i : (xi, yi) ∈Sj	
, Cj := {x ∈X : (x, y) ∈Z, y = j}."
MAIN CONCLUSIONS,0.029950083194675542,"Deﬁnition 1 ((j, x0, r)-Local sample density) Given a class j ∈[K], (x0, y0) ∈Sj, the (j, x0, r)-
Local sample density:"
MAIN CONCLUSIONS,0.03161397670549085,"ρ(j,x0,r) = P"
MAIN CONCLUSIONS,0.033277870216306155,"i∈Ij 1(xi ∈B(x0, r))"
MAIN CONCLUSIONS,0.03494176372712146,"volB(x0, r)"
MAIN CONCLUSIONS,0.036605657237936774,"where B(x0, r) :=

x ∈Rd : ∥x −x0∥≤r
	
, volB(x0, r) denotes the volume of B(x0, r)."
MAIN CONCLUSIONS,0.03826955074875208,"Now we use the notation I(j,x0,r) = {i : (xi, yi) ∈Sj, xi ∈B(x0, r), (x0, y0) ∈Sj)}, C(j,x0,r) =
{x : x ∈Cj, xi ∈B(x0, r), (x0, y0) ∈Sj)}. For simplicity, we denote ℓsce(fw(x), y) as ℓ(w, x)
and deﬁne the local empirical risk R(j,x0,r)(w) =
1
|I(j,x0,r)|
P
i∈I(j,x0,r) ℓ(w, xi)."
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.03993344425956739,"2.1
The Output Consistency of Different Deep Learning Models in HSDR"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.04159733777038269,"We conduct an experiment to explain why samples in low-density regions of ground-truth distribution
are vulnerable to attacks, and verify the consistency of the output of the deep learning model in HSDR.
A dataset consist of 200 samples is constructed by sampling from two 2-d Gaussian distributions with
equal probability, which is then used to train a neural network for classiﬁcation. Subsequently, we
plot the discriminant region of the Bayes’ criterion (which has the minimum error rate) for the known
ground truth prior distribution and compared it with the discriminant region of the trained classiﬁer.
In addition, we train two other neural networks with different structures and parameter quantities,
and plot the output differences of the three neural networks (Figure 2). The discriminant region in
Figure 2(a) can reach the Bayesian error rate. The intersection of the two population distributions
in the middle of the probability density curve (as shown in Figure 2(b)) belongs to the low-density
region of the ground-truth distribution and also to the misclassiﬁed region. In such a region, even
with minimal expected error, the Bayesian discriminant criterion will classify samples with relatively
small probability density of a single population into another class. These samples can be thought of
as ""outliers"". The trained NN classiﬁer discriminates samples into their original categories, causing a"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.04326123128119801,"(a)
(b)
(c)
(d)"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.04492512479201331,"Figure 2: (a): Bayesian discriminant region, darker the color indicate higher the conﬁdence probability.
(b): Classiﬁer discriminant region, the probability density curves of the two population distributions
are plotted, the white part represents the low-density region of ground truth joint distribution, and
we boxed out the small pits in the Bayesian misclassiﬁed region. (c): Classiﬁer discriminant region
with samples. We boxed an outlier. (d): Output differences between three different classiﬁers, darker
purple indicates greater difference in output between different classiﬁers"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.04658901830282862,"difference in decision boundaries from the Bayesian prior classiﬁer and creating small pits as shown
in Figure 2(a). Most of the samples around the pit (Figure 2(c)) belong to another class due to its
lower population density compared to the other class. Hence, perturbing the samples from this pit
(i.e. outliers) towards the discriminant region of the other class becomes easier. If other trained
neural networks can accurately classify such outliers, they will also generate similar pits. By adding
perturbations pointing these pits to samples from another class, they will be perturbed into such
pits, enabling the transfer of adversarial samples between different classiﬁers, which explained the
ﬁndings of [7]."
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.048252911813643926,"However, as we stated in Figure 1, perturbations towards the low-density regions of the ground-truth
distribution are not direct enough for targeted attacks, while perturbations towards the HSDR of the
target class are more direct. Combining this with Figure 2(d), we observe that different classiﬁers
have more consistent outputs in the HSDR. With Theorem 1, we theoretically veriﬁed this, which also
implies that perturbing samples to the HSDR of the target class leads to better targeted adversarial
transferability.
Theorem 1 (Local output consistency) For a target class j
∈
[K],
and two different
parametrized class Fw1
:=
{fw1 : w1 ∈W1}, Fw2
:=
{fw2 : w2 ∈W2}, assume that"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.04991680532445923,"1
|I(j,x0,r)|"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.051580698835274545,"P
i∈I(j,x0,r)
 
f Sk
w1(xi) −f Sk
w2(xi)
 ≤γ, then for any sample (x0, y0) ∈Sj, in the neigh-"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.05324459234608985,"borhood B(x0, r), with probability at least 1 −δ, the following holds:
Ex∼Dx|y

f S
w1(x) −f S
w2(x) | x ∈C(j,x0,r)

∞ ≤O s Kdd/2"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.05490848585690516,"ρ(j,x0,r)2drd log2  
rd√ρ(j,x0,r)

+ s"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.056572379367720464,dd/2 log(2K/δ)
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.05823627287853577,"ρ(j,x0,r)2d+1rd ! + γ."
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.059900166389351084,"Remark.
Theorem 1 suggests that different models have a more consistent output near the samples
in HSDR, speciﬁcally, the difference between the outputs has a bound of order ˜O
q"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.06156405990016639,"Kdd/2
ρ(j,x0,r)2drd

.
When the number of classes and dimensions are ﬁxed, a larger sample density results in more
consistent performance, a weaker consistency within smaller neighborhoods. Note that this local
consistency is weakened as the dimension increases, but the relative output consistency between
HSDR and LSDR does not change."
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.0632279534109817,"However, a very practical problem is that sample points in high-dimensional space tend to be very
discrete due to the curse of dimentionality [29], which makes it difﬁcult to ﬁnd a suitable local
neighborhood size to calculate the local density of samples, moreover, in the case of large datasets,
calculating local density can be computationally expensive. But the conclusion of the next section can
help us ﬁnd the sample located in the HSDR using early-stopping models without directly calculating
the local density of the sample."
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.064891846921797,"2.2
Correlation between Local Sample Density and Sample Difﬁculty"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.06655574043261231,"In this section we illustrate the correlation between sample density and sample difﬁculty. Hard
samples have attracted much attention in various tasks because of their signiﬁcance for training"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.06821963394342762,"convergence and model generalization [8, 30, 31, 32]. [8] proposed that for the convergent model, the
difﬁculty of the sample can be measured by the loss gradient norm of the sample, easy samples has
a relatively small loss gradient norm, while the loss gradient norm of difﬁcult samples is relatively
large, especially, those with too large gradient norm may be outliers."
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.06988352745424292,"We use the following theoretical analysis to illustrate that for a trained classiﬁer, the samples in the
HSDR tend to have a smaller local empirical risk. At the same time, smaller loss gradient norms
ensure that losses are more consistent in small neighborhoods, which implies those samples with both
smaller loss and loss gradient norms guarantee less local empirical risk in the neighborhood where
the sample is located. To illustrate the following conclusions, we make several mild assumptions:"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.07154742096505824,"Assumption 1 (Smoothness assumption) ℓ(w, x) satisﬁes the following conditions of Lipschitz
continuous gradient:"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.07321131447587355,"∥∇wℓ(w1, x) −∇wℓ(w2, x)∥≤L1 ∥w1 −w2∥, ∀w1, w2 ∈W,
∥∇xℓ(w, x1) −∇xℓ(w, x2)∥≤L2 ∥x1 −x2∥, ∀x1, x2 ∈X."
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.07487520798668885,"Assumption 2 ∥∇wℓ(w, x)∥≤G for all w ∈W."
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.07653910149750416,"Assumption 3 (Polyak-Łojasiewicz Condition) R(j,x0,r)(w) satisﬁes the PL-condition: 1
2"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.07820299500831947,"∇wR(j,x0,r)(w)
2 ≥µ

R(j,x0,r)(w) −R∗
(j,x0,r)

,"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.07986688851913477,"where R(j,x0,r)(w) =
1
|I(j,x0,r)|
P"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.08153078202995008,"i∈I(j,x0,r) ℓ(w, xi) is the local empirical risk, R∗
(j,x0,r) ="
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.08319467554076539,"inf
w∈WR(j,x0,r)(w)."
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.08485856905158069,"Assumption 4 For ∀i ∈I(j,x0,r), the following holds:

∇wR(j,x0,r)(w), ∇wℓ(w, xi)

≥β
∇wR(j,x0,r)(w)
2"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.08652246256239601,"Assumption 1 and 2 were made in [33], [34] and [35], where Assumption 2 can actually be derived
from Assumption 1 when the input space is bounded, which is usually satisﬁed. Even a non-convex
function can still satisfy Assumption 3 [34], the inequality in Assumption 3 implies that all stationary
points are global minimum [36], which is proved in recently works [37, 38] for over-parameterized
DNNs. Assumption 4 is reasonable when local neighborhood radius r is small."
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.08818635607321132,"Algorithm 1 Mini-batch SGD
Require: Initialized weights w1, total steps T, sample set S, batch size M and step size ηt."
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.08985024958402663,"1: for t = 1 ←T do
2:
Randomly sample M different samples Bt from S, where batch size |Bt| = M, corresponding
indicator set denote as IBt.
3:
wt+1 = wt −
1
M
P"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.09151414309484193,"i∈IBt ∇wℓ(wt, xi)
4: end for
Return: wT +1"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.09317803660565724,"Consider the local empirical risk under Algorithm 1, we use Theorem 2 to illustrate the convergence
rate relies on local density."
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.09484193011647254,"Theorem 2 (Optimization relies on local density) Given a learnable parametrized class Fw :=

fw : X →RK, w ∈W
	
, let wt updated by Algorithm 1, under Assumption 1, 2, 3 and 4, set
ηt =
1
βµt and T ≤
L1
2βµ, then with probability at least 1 −δ, holds"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.09650582362728785,"R(j,x0,r)(wt+1) −R∗
(j,x0,r) ≤

1 −2"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.09816971713810316,"t τ
 
ρ(j,x0,r)
 
R(j,x0,r)(wt) −R∗
(j,x0,r)

+ o
 1 t2 
,"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.09983361064891846,"where τ
 
ρ(j,x0,r)

= max

ρ(j,x0,r)πd/2rd Γ( d"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.10149750415973377,"2 +1)M
−
q"
THE OUTPUT CONSISTENCY OF DIFFERENT DEEP LEARNING MODELS IN HSDR,0.10316139767054909,ln(T/δ)
M,0.1048252911813644,2M
M,0.1064891846921797,"
, 0

, which is a non-descending function"
M,0.10815307820299501,"of ρ(j,x0,r)."
M,0.10981697171381032,"According to theorem 2, the local empirical risk R(j,x0,r)(w) will reach a more faster convergence
rate in HSDR and, relatively, a relatively slower convergence rate in LSDR, thus for early-stopping
models, they has a lower local empirical risk in HSDR. Combined with the following Proposition
1, we show that a smaller gradient norm guarantees a smaller local empirical risk. For overﬁtting
models, the local empirical risk of each neighborhood where samples are located may be very small,
but for early-stopping models, this relativity of local empirical risk with respect to local sample
density can be maintained."
M,0.11148086522462562,"Proposition 1 Under Assumption 1, given any (xi, yi) ∈S, for any x ∈X that satisﬁes ∥xi −x∥≤
r, the following holds:"
M,0.11314475873544093,"|ℓ(w, xi) −ℓ(w, x)|"
M,0.11480865224625623,"r
−3L2r"
M,0.11647254575707154,"2
≤∥∇xℓ(w, xi)∥≤ℓ(w, xi)"
M,0.11813643926788686,"r
+ 3L2r 2
."
M,0.11980033277870217,"Remark.
Proposition 1 suggests that minimizing the loss leads to a smaller norm of the loss gradient.
However, this constraint becomes less tight as the neighborhood radius r decreases. Compared with
the loss itself, the loss gradient norm further guarantees the proximity of local loss, especially when r
is small. Therefore, samples with smaller loss and smaller loss gradient norm are more likely to be in
a neighborhood with smaller local empirical risk."
M,0.12146422628951747,"Using the above conclusion, for an early-stopping model, samples with smaller losses and loss
gradient norms tend to be located in HSDR. Still in the example in the previous section, we train
three models with early-stopping, and the learning rate adjusted with the number of steps, and then
plotted Figure 3. The model has more consistent outputs in HSDR. When the loss and gradient
norms of a sample are small, the region where the sample is located has smaller local empirical risk.
Samples in HSDR have smaller local empirical risk, which is consistent with our theoretical analysis.
Additionally, the rightmost graph of Figure 3 shows that samples with smaller loss and gradient
norms often locate in HSDR, validating our conclusions."
M,0.12312811980033278,"Therefore, we can determine whether a sample is more likely to located in HSDR or LSDR by
evaluating whether it has smaller loss and gradient norms simultaneously. This eliminates the need to
calculate local sample densities to ﬁnd samples in HSDR."
M,0.12479201331114809,"Figure 3: The ﬁrst ﬁgure depicts the difference in output of three models under different local
sample densities ρ(yi,xi,r) divided into different bins. The second ﬁgure shows the local empirical
risk R(yi,xi,r) of samples under different sum of loss and gradient norms (Loss+Gradnorm). For
Loss+Gradnorm, we ﬁrst normalize both variables separately and then add them up to eliminate
magnitude differences. The third ﬁgure represents the local empirical risk of local sample densities in
different values. The fourth ﬁgure displays the local density under different Loss+Gradnorms. The
neighborhood radius r is taken as 0.4."
M,0.1264559068219634,"We conduct transfer attack experiments on three baselines on the CIFAR10 dataset, three different
models are chosen as victims. Combining our perspectives above, we use algorithm 2 to select the
anchor of each target class for guiding the addition of adversarial perturbations. We implement our
strategy by simply using squared loss to match anchor points, i.e. minimizing
f(xadv
i
) −atargeti
2"
M,0.1281198003327787,"(choosing q = 10 and ϵ = 16 pixels). For comparison, we also use cross-entropy (CE) loss to
calculate adversarial examples in the vanilla way, as well as using squared error without a screening
mechanism (i.e., randomly selecting the target anchor in the target class) to exclude the inﬂuence
of different losses. The results are shown in Table 1, our strategy indeed helps to enhance the
transferability of target attacks, which further conﬁrms our viewpoints."
M,0.129783693843594,"Algorithm 2 Target Anchor Screening
Require: Early-stopping classiﬁer fw, screening parameter q and sample set S."
M,0.1314475873544093,"1: for i = 1 ←n do
2:
lossi = ℓsce(f(xi), yi), gradnormi = ∥∇xℓsce(f(xi), yi)∥.
3: end for
4: For each class k ∈[K], select the q-th smallest loss and the gradient norm among the samples in
that class as thresholds thrloss
k
and thrgradnorm
k
5: for k = 1 ←K do
6:
Ak :=
n
i ∈Ik : lossi < thrloss
k , gradnormi < thrgradnorm
k
o
,"
M,0.13311148086522462,"ak =
1
|Ak|
P"
M,0.13477537437603992,"j∈Ak fw(xj):
7: end for"
M,0.13643926788685523,"Table 1: Targeted transfer success rates. ""left/middle/right"" represent attacks using regular CE loss,
square loss with randomly selected target anchors, and square loss with target anchor screening,
respectively. In parentheses, the clean accuracy is indicated."
M,0.13810316139767054,"Attack
Src:Res34(95.44%)
Src:VGG16(94.27%)
Src:Dense121(95.47%)
→VGG16
→Dense121
→Res34
→Dense121
→Res34
→VGG16"
M,0.13976705490848584,"MIM 14.32%/12.94%/14.44% 19.81%/19.00%/20.09% 14.80%/13.78%/15.13% 13.41%/12.11%/13.83% 17.13%/14.17%/17.17% 11.23%/10.27%/11.25%
TIM 13.53%/12.50%/13.88% 15.23%/14.56%/15.71% 12.49%/11.78% /12.88% 11.14%/ 10.17% /11.40% 15.74%/14.61%/15.89% 11.93%/10.94%/12.09%
DIM 15.46%/14.39%/15.97% 18.10%/ 17.61%/18.96% 14.56%/13.28%/15.12% 12.97%/12.11%/13.47% 18.11%/16.83%/18.33% 13.18%/11.72%/13.26%"
TRAINING STRATEGY OF ESMA,0.14143094841930118,"3
Training Strategy of ESMA"
TRAINING STRATEGY OF ESMA,0.14309484193011648,"We present our training strategy in this section, as shown in Figure 4. Our training strategy is carried
out in two steps, the ﬁrst step we pre-train the generator’s embedding representations, and the second
step is to ﬁnd easy samples of each target class based on our previous conclusions, and then use these
samples to guide the generator to generate perturbations from the source domain to the target domain."
TRAINING STRATEGY OF ESMA,0.1447587354409318,Figure 4: Training strategy of ESMA.
TRAINING STRATEGY OF ESMA,0.1464226289517471,"Pre-trained Embeddings Guided by
Latent Features
To obtain better
embeddings that more effectively rep-
resents inter-class information, since
the latent space of deep learning mod-
els often extracts enough class infor-
mation [39], we treat the output fea-
tures of the local model as a set of a
priori embedding. In order to make
the generator embedding have a sim-
ilar structure to such a priori embed-
ding, we refer to the idea of manifold
learning, using a strategy similar to
the SNE algorithm [28]. Let µj =
1
|Ij|
P"
TRAINING STRATEGY OF ESMA,0.1480865224625624,"i∈Ij li, where li = f(xi), and
then we design the following manifold
matching loss. Generator embeddings of various class were pulled to the manifold that output features
in to obtain embeddings with a better structure."
TRAINING STRATEGY OF ESMA,0.1497504159733777,"Next, we denote the generator embedding of class j as ej. The four matrices M Seuc, M Eeuc, M Scos,
M Ecos satisfy M Seuc
i,j
= ∥µi −µj∥, M Eeuc
i,j
= ∥ei −ej∥, M Scos
i,j
=
µi·µj
∥µi∥∥µj∥, M Ecos
i,j
=
ei·ej
∥ei∥∥ej∥,
respectively. Let"
TRAINING STRATEGY OF ESMA,0.15141430948419302,"M
Seuc
i,j
=
exp(M Seuc
i,j )
PK
k=1 exp(M Seuc
i,k ), M
Eeuc
i,j
=
exp(M Eeuc
i,j )
PK
k=1 exp(M Eeuc
i,k ),"
TRAINING STRATEGY OF ESMA,0.15307820299500832,"M
Scos
i,j
=
exp(M Scos
i,j )
PK
k=1 exp(M Scos
i,k ), M
Ecos
i,j
=
exp(M Ecos
i,j )
PK
k=1 exp(M Ecos
i,k ),"
TRAINING STRATEGY OF ESMA,0.15474209650582363,then our manifold matching loss is as follows:
TRAINING STRATEGY OF ESMA,0.15640599001663893,LM = P
TRAINING STRATEGY OF ESMA,0.15806988352745424,"i,j M
Seuc
i,j log"
TRAINING STRATEGY OF ESMA,0.15973377703826955,"M
Seuc
i,j
M
Eeuc
i,j
+ P"
TRAINING STRATEGY OF ESMA,0.16139767054908485,"i,j M
Eeuc
i,j
log"
TRAINING STRATEGY OF ESMA,0.16306156405990016,"M
Eeuc
i,j
M
Seuc
i,j +λ1 P"
TRAINING STRATEGY OF ESMA,0.16472545757071547,"i,j M
Scos
i,j
log"
TRAINING STRATEGY OF ESMA,0.16638935108153077,"M
Scos
i,j
M
Ecos
i,j
+ P"
TRAINING STRATEGY OF ESMA,0.16805324459234608,"i,j M
Ecos
i,j
log M i,j M i,j"
TRAINING STRATEGY OF ESMA,0.16971713810316139,"
+ λ2
PK
i=1
ei ."
TRAINING STRATEGY OF ESMA,0.1713810316139767,"The last regular term is to prevent losses from collapsing. λ1 and λ2 are hyperparameters. The pre-
trained embedding with our strategy has a larger Euclidean distance and a smaller cosine similarity,
which greatly alleviates the previous clustering phenomenon. More detailed analysis and discussion
are provided in Appendix B."
TRAINING STRATEGY OF ESMA,0.17304492512479203,"Training of Multi-target Adversarial Perturbation Generators
After pre-training embedding,
we freeze the parameters of the embedding layer, combined with our previous conclusions, we select
several easy samples in each class to form target anchor sets Ai, and match them with the output
of our generator in feature space, the generator we use is a Unet with Resblocks. We propose the
following objective easy sample feature matching loss to train the multi-class adversarial generator. LEM = K
X j=1"
P,0.17470881863560733,"1
P"
P,0.17637271214642264,i∈[n] 1 (i /∈Ij) X
P,0.17803660565723795,"i/∈Ij
d (f (aj) , f (clipϵ (W ∗Gθ (xi)))) ,"
P,0.17970049916805325,"where clipϵ(x) = clip (min (x + ϵ, max (x, x −ϵ))), W is a differentiable Gaussian kernel with
size 3 ∗3, such smoothing operations have been demonstrated to further improve transferability[24].
The measure of the distance between the two features d(·, ·) is Smooth L1 loss, which has a unique
optimal solution that is not sensitive to exceptional values [40]. Then, our ﬁnal training strategy can
be represented by algorithm 3."
P,0.18136439267886856,"Algorithm 3 Training Strategy of ESMA
Require: Generator Gθ with pre-trained embeddings, anchors ak, k ∈[K] and Total epochs N."
P,0.18302828618968386,"1: for t = 1 ←N do
2:
for i = 1 ←n do
3:
targeti ∼Uniform({1, . . . , K}) .
4:
if yi ̸= targeti then
5:
Random choice an anchor aj from Aj,
6:
Gradient descent step on LEM.
7:
end if
8:
end for
9: end for"
EXPERIMENTS,0.18469217970049917,"4
Experiments"
EXPERIMENTS,0.18635607321131448,"In this section, we verify the effectiveness of our method through experiments on ILSVRC2012
dataset [41]. To evaluate the effectiveness of different components in our strategy, we conduct
ablation experiments in Section 4.3. Other ablation experiments can be found in Appendix C."
EXPERIMENT SETUP,0.18801996672212978,"4.1
Experiment Setup"
EXPERIMENT SETUP,0.1896838602329451,"Dataset
The dataset we used comes from the ILSVRC2012 dataset [41], in which we selected ten
classes as the training set, which refer to the ten classes used for TTP training in [24], they are 24, 99,
198, 245, 344, 471, 661, 701, 802, 919. We train generators using images of these ten classes in the
training set (1300 images per class), and use the images in the validation set (50 images per class) as
the validation dataset for the targeted attack."
EXPERIMENT SETUP,0.1913477537437604,"Models
We use the four networks used in [24] as source models —ResNet-50 [42] (Res50), VGG-
19-bn [43] (VGG19bn), DenseNet-121 [44] (Dense121), ResNet-152 [42] (Res152). Except the
above four models, we also select three models from the Inception series: Inception-v3 [45] (Inc-v3),
Inception-v4 [46] (Inc-v4), Inception-ResNet-v2 [46] (IncRes-v2) and a transformer vision model,
VIT [47]. As the analysis and fundamental assumptions of this paper are based on the condition
of training data from the same distribution, in order to explore the transferability between models
trained on training data with different distribution, we conduct additional transfer attack on two"
EXPERIMENT SETUP,0.1930116472545757,"adversarially trained models, Inc-v3-adv [48] and IncRes-v2-ens [49]. Results are shown in E.2. In
addition, we also test the adversarial transferability of ESMA on the scenario where the source model
is an ensemble of different models, corresponded results are reported in E.1."
EXPERIMENT SETUP,0.194675540765391,"Baselines
We select many iterative instance-speciﬁc attack benchmarks, MIM [12], SI-NIM [13],
TIM [14], DIM [16], and advanced iterative instance-speciﬁc attacks that are competitive in target
setting, Po-TI-Trip [20], Logit [21], Rap-LS [23], FGS2M [50], DMTI-Logit-SU [51], S2I-SI-TI-
DIM [52]. Generative adversarial attacks HGN [53] and TTP [24] also included in our comparision,
where TTP is the current SOTA generative method. As a generative attack, TTP requires training a
class-dependent generator speciﬁcally for each class."
EXPERIMENT SETUP,0.19633943427620631,"Parameter Setting
For the parameter settings of different attack methods, we refer to the default
settings in [20], total iteration number T = 20, step size α = ϵ/T, where the ℓ∞perturbation
restriction ϵ is set to 16. Momentum factors µ is set to 1, for the stochastic input diversity in DIM, we
set the probability of applying input diversity as 0.7. For TIM, the kernel-length is set to 7, which is
more suitable for targeted attacks. For Po-TI-Trip, the weight of triplet loss λ is set to 0.01, while the
margin γ is set to 0.007. Referring to [21], the number of iteration steps of Logit is set to 300, and
for RAP-LS, we choose 400 iteration steps, KLS is set to 100, and ϵn is set to 12/255 [23]. Then
for TTP, since we used a relatively small training set, we added 10 epochs to the original paper [24]
settings to ensure the performance of the model, and the learning rate of Adam optimizer is 1e −4
(β1 = .5, β2 = .999). Finally, for our model, we used the AdamW optimizer to train 300 epochs
with a learning rate of 1e −4 (350 for cases where the source model is VGG19bn or Dense121), the
value of q used for sample screening is set to 2."
EXPERIMENT SETUP,0.19800332778702162,"Evaluation Setting
We train the generator on the training set of the selected ten classes, verify
the targeted transferality on the validation set, for each target class, we use the 450 images of the
remaining classes as the source data and perturb them to the target class. For the iterative instance-
speciﬁc attacks, we directly attack these instances and test their targeted transfer success rate. All
methods (including training) were implemented on a single NVIDIA RTX A5000 GPU."
RESULTS,0.19966722129783693,"4.2
Results"
RESULTS,0.20133111480865223,"The results of our experiments, reported in Table 3, ESMA outperforms current SOTA gen-
erative attack TTP in targeted transfer success rates.
Our approach demonstrates signiﬁcant
advantages in terms of efﬁciency and effectiveness, with TTP having a parameter count of
7.84M per model compared to ESMA’s 4.40M. Additionally, ESMA achieves an average train-
ing time that is 78.4% of TTP. The experimental results also strongly supports our viewpoint."
RESULTS,0.20299500831946754,"Figure 5:
Comparison of
targeted attack transfer suc-
cess
rates
with
(w)
pre-
trained embeddings and with-
out (w/o) pre-trained embed-
dings at different training
epochs. Src:Res50."
ABLATION STUDIES,0.20465890183028287,"4.3
Ablation Studies"
ABLATION STUDIES,0.20632279534109818,"To validate the effectiveness of the two components in our strategy
design, we compared the cases with (w/) and without (w/o) pre-
trained embeddings and target anchor screening. The results are
shown in Figure 5. In the case without pre-trained embeddings, the
performance is signiﬁcantly weaker than the case with pre-trained
embeddings across different training durations. Moreover, the case
with target anchor screening shows a noticeable improvement com-
pared to randomly selecting target anchors."
ABLATION STUDIES,0.2079866888519135,"To verify that ESMA can indeed increase the sample density of the
target class for the original samples, we use the samples used for
adversarial testing in table 3. First, we calculate the sample density
of the target class for the clean samples ρ(target,xi,r) (here r is set to 600). Then, we apply ESMA
to generate adversarial samples xadv
i
, and calculate the sample density of the target class for these
adversarial samples ρ(target,xadv
i
,r). In both cases, the density calculation results were averaged across
all target classes for each sample. We normalize the results by dividing them by the maximum value
among all results in both cases. Additionally, we further bin and count the number of test samples in
different intervals under different perturbation constraints. The results are shown in Table 2. The"
ABLATION STUDIES,0.2096505823627288,"target class sample density of the samples perturbated using ESMA has increased in bins with larger
values, which conﬁrms that ESMA can indeed enhance the target class sample density."
ABLATION STUDIES,0.2113144758735441,Table 2: Comparison of sample density (binned).
ABLATION STUDIES,0.2129783693843594,Counts 0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0
ABLATION STUDIES,0.2146422628951747,Case for ϵ = 8/255:
ABLATION STUDIES,0.21630615640599002,"Clean
32
35
31
45
39
51
57
51
61
48
ESMA
27
30
32
37
43
49
62
51
70
49"
ABLATION STUDIES,0.21797004991680533,Case for ϵ = 16/255:
ABLATION STUDIES,0.21963394342762063,"Clean
32
35
31
45
39
51
57
51
61
48
ESMA
25
27
36
30
49
50
59
54
71
49"
ABLATION STUDIES,0.22129783693843594,"Table 3: Targeted transfer success rates. ""Src"" indicates the source model. For TTP and ESMA, the
numbers in parentheses indicate training duration in seconds."
ABLATION STUDIES,0.22296173044925124,"Src
Attack
→VGG19bn →Dense121
→Res152
→Inc-v3 →Inc-v4 →IncRes-v2
→ViT
AVG Res50"
ABLATION STUDIES,0.22462562396006655,"MIM
1.33%
3.44%
3.67%
0.51%
0.31%
0.22%
0.16%
1.38%
TIM
13.07%
26.16%
23.82%
5.51%
3.84%
2.96%
0.60%
10.85%
DIM
13.36%
24.96%
23.44%
5.11%
3.80%
2.29%
0.47%
10.49%
SI-NIM
1.07%
1.91%
2.00%
0.40%
0.44%
0.29%
0.07%
0.88%
Po-TI-Trip
17.93%
33.62%
32.20%
7.73%
5.69%
3.38%
1.07%
14.52%
SI-FGS2M
20.75%
32.68%
30.55%
6.30%
4.42%
2.92%
0.91%
14.08%
S2I-SI-TI-DIM
26.49%
36.36%
36.49%
14.87%
12.67%
10.02%
1.18%
19.72%
Logit
51.60%
77.22%
76.98%
17.62%
12.67%
8.40%
3.09%
35.37%
DTMI-Logit-SU
53.36%
78.91%
79.00%
18.56%
13.36%
8.71%
3.32%
36.46%
RAP-LS
60.04%
83.40%
80.38%
22.16%
17.24%
10.82%
3.80%
39.69%
HGN
63.92%
70.64%
68.43%
22.41%
13.76%
7.31%
12.12%
36.94%
TTP(95339.7)
67.11%
73.67%
72.64%
33.49%
25.27%
11.27%
18.07%
43.07%
ESMA(82060.1)
81.29%
83.89%
81.78%
37.53%
39.29%
17.29%
21.25% 51.76%"
ABLATION STUDIES,0.22628951747088186,"Src
Attack
→VGG19bn
→Res50
→Res152
→Inc-v3 →Inc-v4 →IncRes-v2
→ViT
AVG"
ABLATION STUDIES,0.22795341098169716,Dense121
ABLATION STUDIES,0.22961730449251247,"MIM
1.98%
2.33%
1.51%
0.56%
0.40%
0.27%
0.27%
1.05%
TIM
8.98%
12.82%
8.87%
3.44%
3.31%
1.91%
0.71%
5.72%
DIM
10.20%
13.80%
9.18%
3.98%
3.27%
2.29%
0.60%
6.19%
SI-NIM
8.29%
11.42%
6.96%
1.78%
1.71%
1.13%
0.29%
4.51%
Po-TI-Trip
11.29%
16.80%
11.78%
5.49%
4.84%
2.96%
0.96%
7.73%
SI-FGS2M
12.55%
16.67%
11.70%
5.16%
4.89%
3.58%
0.85%
7.91%
S2I-SI-TI-DIM
25.60%
30.33%
23.98%
13.11%
12.04%
7.93%
1.38%
16.33%
Logit
31.98%
43.49%
31.71%
12.91%
11.51%
7.16%
3.27%
20.29%
DTMI-Logit-SU
33.38%
44.93%
34.16%
13.44%
12.36%
7.75%
3.62%
21.38%
RAP-LS
38.56%
50.67%
38.24%
15.78%
13.96%
9.64%
3.82%
24.38%
HGN
47.81%
56.96%
44.21%
22.75%
19.56%
9.42%
10.62%
30.19%
TTP(109644.6)
52.00%
58.02%
49.24%
29.69%
23.00%
13.24%
17.71%
34.70%
ESMA(96335.6)
56.83%
63.71%
54.70%
33.23%
29.28%
15.97%
18.50%
38.89%"
ABLATION STUDIES,0.23128119800332778,"Src
Attack
→Res50
→Dense121
→Res152
→Inc-v3 →Inc-v4 →IncRes-v2
→ViT
AVG"
ABLATION STUDIES,0.23294509151414308,VGG19bn
ABLATION STUDIES,0.23460898502495842,"MIM
0.56%
0.58%
0.29%
0.27%
0.20%
0.04%
0.07%
0.29%
TIM
3.80%
4.58%
1.84%
1.47%
1.09%
0.56%
0.16%
1.93%
DIM
2.98%
4.33%
1.76%
1.09%
1.16%
0.47%
0.16%
1.71%
SI-NIM
0.42%
0.42%
0.29%
0.11%
0.20%
0.18%
0.07%
0.24%
Po-TI-Trip
4.56%
6.27%
2.42%
1.69%
1.47%
0.87%
0.27%
2.51%
SI-FGS2M
4.12%
5.71%
1.97%
1.56%
1.25%
0.68%
0.25%
2.22%
S2I-SI-TI-DIM
12.09%
14.84%
6.49%
4.87%
5.80%
2.93%
0.31%
6.76%
Logit
22.16%
30.47%
11.51%
5.51%
6.71%
1.91%
0.98%
11.32%
DTMI-Logit-SU
23.47%
31.47%
12.60%
6.13%
7.31%
2.07%
1.11%
12.02%
RAP-LS
24.31%
33.33%
13.56%
6.58%
8.51%
2.78%
1.11%
12.88%
HGN
34.18%
31.00%
19.36%
11.16%
8.87%
1.71%
2.26%
15.51%
TTP(131519.2)
36.76%
34.44%
22.11%
11.82%
10.40%
2.20%
2.58%
17.19%
ESMA(110769.2)
39.61%
47.25%
22.85%
12.98%
11.73%
4.09%
3.35%
20.27%"
ABLATION STUDIES,0.23627287853577372,"Src
Attack
→VGG19bn
→Res50
→Dense121 →Inc-v3 →Inc-v4 →IncRes-v2
→ViT
AVG"
ABLATION STUDIES,0.23793677204658903,Res152
ABLATION STUDIES,0.23960066555740434,"MIM
1.11%
5.69%
3.29%
0.69%
0.31%
0.18%
0.18%
1.64%
TIM
9.76%
26.64%
21.56%
5.69%
4.22%
3.04%
1.07%
10.28%
DIM
9.89%
26.27%
20.98%
5.69%
4.11%
2.44%
0.71%
10.01%
SI-NIM
0.82%
1.76%
1.16%
0.36%
0.20%
0.24%
0.11%
0.66%
Po-TI-Trip
14.53%
35.36%
29.87%
8.93%
6.29%
4.71%
1.36%
14.44%
SI-FGS2M
13.98%
39.22%
24.80%
7.98%
6.11%
4.31%
0.95%
13.91%
S2I-SI-TI-DIM
19.29%
36.64%
30.78%
14.36%
11.42%
10.84%
1.29%
17.80%
Logit
35.44%
75.22%
61.31%
15.33%
10.69%
8.11%
2.76%
29.84%
DTMI-Logit-SU
37.76%
77.47%
62.91%
16.02%
11.51%
8.58%
3.02%
31.04%
RAP-LS
42.36%
83.20%
69.67%
18.82%
14.33%
10.36%
3.20%
34.56%
HGN
61.40%
73.31%
67.89%
33.47%
26.87%
11.31%
13.96%
41.17%
TTP(132223.9)
65.31%
79.73%
74.93%
36.73%
30.11%
13.44%
15.62%
45.12%
ESMA(93977.6)
78.67%
88.18%
79.93%
41.68%
34.38%
14.58%
18.72%
50.88%"
CONCLUSION,0.24126455906821964,"5
Conclusion"
CONCLUSION,0.24292845257903495,"In this work, we provided a novel perspective on the transferability of targeted adversarial attacks.
The study theoretically and experimentally demonstrated that adding perturbations towards HSDR
of the target class can further enhance the transferability of targeted attacks. We also proposed a
method for identifying samples within HSDR, which avoided the impracticality of density estimation
in high-dimensional data. Building upon these insights, we introduced an improved generative
multi-target attack strategy ESMA, which surpassed previous generative targeted attacks in both
effectiveness and efﬁciency. We believe that our insights not only provide guidance for targeted
attacks but also offer insights for dataset selection. Further discussion on these points would be
included in our future work."
REFERENCES,0.24459234608985025,References
REFERENCES,0.24625623960066556,"[1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and
Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014."
REFERENCES,0.24792013311148087,"[2] Biqing Qi, Bowen Zhou, Weinan Zhang, Jianxing Liu, and Ligang Wu. Improving robustness of intent
detection under adversarial attacks: A geometric constraint perspective. IEEE transactions on neural
networks and learning systems, PP, 2023."
REFERENCES,0.24958402662229617,"[3] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In ICLR, 2015."
REFERENCES,0.2512479201331115,"[4] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and
black-box attacks. In ICLR. OpenReview.net, 2017."
REFERENCES,0.2529118136439268,"[5] Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In IEEE
S&P, 2017."
REFERENCES,0.2545757071547421,"[6] Nathan Inkawhich, Wei Wen, Hai (Helen) Li, and Yiran Chen. Feature space perturbations yield more
transferable adversarial examples. In CVPR, 2019."
REFERENCES,0.2562396006655574,"[7] Yao Zhu, Jiacheng Sun, and Zhenguo Li. Rethinking adversarial transferability from a data distribution
perspective. In ICLR, 2022."
REFERENCES,0.2579034941763727,"[8] Buyu Li, Yu Liu, and Xiaogang Wang. Gradient harmonized single-stage detector. In AAAI, 2019."
REFERENCES,0.259567387687188,"[9] Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In ACM, 2017."
REFERENCES,0.2612312811980033,"[10] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited
queries and information. In ICML, 2018."
REFERENCES,0.2628951747088186,"[11] Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep neural
networks. IEEE Trans. Evol. Comput., 23(5):828–841, 2019."
REFERENCES,0.26455906821963393,"[12] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting
adversarial attacks with momentum. In CVPR, 2018."
REFERENCES,0.26622296173044924,"[13] Jiadong Lin, Chuanbiao Song, Kun He, Liwei Wang, and John E. Hopcroft. Nesterov accelerated gradient
and scale invariance for adversarial attacks. In ICLR, 2020."
REFERENCES,0.26788685524126454,"[14] Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading defenses to transferable adversarial examples
by translation-invariant attacks. In CVPR, 2019."
REFERENCES,0.26955074875207985,"[15] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards
deep learning models resistant to adversarial attacks. In ICLR, 2018."
REFERENCES,0.27121464226289516,"[16] Junhua Zou, Zhisong Pan, Junyang Qiu, Xin Liu, Ting Rui, and Wei Li. Improving the transferability of
adversarial examples with resized-diverse-inputs, diversity-ensemble and region ﬁtting. In ECCV, 2020."
REFERENCES,0.27287853577371046,"[17] Junyoung Byun, Seungju Cho, Myung-Joon Kwon, Heeseon Kim, and Changick Kim. Improving the
transferability of targeted adversarial examples through object-based diverse input. In CVPR, 2022."
REFERENCES,0.27454242928452577,"[18] Nathan Inkawhich, Kevin J. Liang, Lawrence Carin, and Yiran Chen. Transferable perturbations of deep
feature distributions. In ICLR, 2020."
REFERENCES,0.2762063227953411,"[19] Nathan Inkawhich, Kevin J. Liang, Binghui Wang, Matthew Inkawhich, Lawrence Carin, and Yiran Chen.
Perturbing across the feature hierarchy to improve standard and strict blackbox attack transferability. In
NeurIPS, 2020."
REFERENCES,0.2778702163061564,"[20] Maosen Li, Cheng Deng, Tengjiao Li, Junchi Yan, Xinbo Gao, and Heng Huang. Towards transferable
targeted attack. In CVPR, 2020."
REFERENCES,0.2795341098169717,"[21] Zhengyu Zhao, Zhuoran Liu, and Martha A. Larson. On success and simplicity: A second look at
transferable targeted attacks. In NeurIPS, 2021."
REFERENCES,0.281198003327787,"[22] Chaoning Zhang, Philipp Benz, Adil Karjauv, Jae-Won Cho, Kang Zhang, and In So Kweon. Investigating
top-k white-box and transferable black-box attack. In CVPR, 2022."
REFERENCES,0.28286189683860236,"[23] Zeyu Qin, Yanbo Fan, Yi Liu, Li Shen, Yong Zhang, Jue Wang, and Baoyuan Wu. Boosting the transfer-
ability of adversarial attacks with reverse adversarial perturbation. In NeurIPS, 2022."
REFERENCES,0.28452579034941766,"[24] Muzammal Naseer, Salman H. Khan, Munawar Hayat, Fahad Shahbaz Khan, and Fatih Porikli. On
generating transferable targeted perturbations. In ICCV, 2021."
REFERENCES,0.28618968386023297,"[25] Omid Poursaeed, Isay Katsman, Bicheng Gao, and Serge J. Belongie. Generative adversarial perturbations.
In CVPR, 2018."
REFERENCES,0.2878535773710483,"[26] Konda Reddy Mopuri, Utkarsh Ojha, Utsav Garg, and R. Venkatesh Babu. NAG: network for adversary
generation. In CVPR, 2018."
REFERENCES,0.2895174708818636,"[27] Muzammal Naseer, Salman H. Khan, Muhammad Haris Khan, Fahad Shahbaz Khan, and Fatih Porikli.
Cross-domain transferability of adversarial perturbations. In NeurIPS, 2019."
REFERENCES,0.2911813643926789,"[28] Geoffrey E. Hinton and Sam T. Roweis. Stochastic neighbor embedding. In Suzanna Becker, Sebastian
Thrun, and Klaus Obermayer, editors, NeurIPS, 2002."
REFERENCES,0.2928452579034942,"[29] Trevor Hastie, Robert Tibshirani, and Jerome H. Friedman. The Elements of Statistical Learning: Data
Mining, Inference, and Prediction, 2nd Edition. 2009."
REFERENCES,0.2945091514143095,"[30] Abhinav Shrivastava, Abhinav Gupta, and Ross B. Girshick. Training region-based object detectors with
online hard example mining. In CVPR, 2016."
REFERENCES,0.2961730449251248,"[31] Xiaolong Wang, Abhinav Shrivastava, and Abhinav Gupta. A-fast-rcnn: Hard positive generation via
adversary for object detection. In CVPR, 2017."
REFERENCES,0.2978369384359401,"[32] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object
detection. In ICCV, 2017."
REFERENCES,0.2995008319467554,"[33] Aman Sinha, Hongseok Namkoong, and John C. Duchi. Certifying some distributional robustness with
principled adversarial training. In ICLR, 2018."
REFERENCES,0.3011647254575707,"[34] Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D. Lee, and Meisam Razaviyayn. Solving a class
of non-convex min-max games using iterative ﬁrst order methods. In NeurIPS, 2019."
REFERENCES,0.30282861896838603,"[35] Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the convergence
and robustness of adversarial training. In ICML, 2019."
REFERENCES,0.30449251247920134,"[36] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient
methods under the polyak-łojasiewicz condition. In ECML/PKDD, 2016."
REFERENCES,0.30615640599001664,"[37] Simon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds global
minima of deep neural networks. In ICML, 2019."
REFERENCES,0.30782029950083195,"[38] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In ICML, 2019."
REFERENCES,0.30948419301164726,"[39] Bolei Zhou, Aditya Khosla, Àgata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features
for discriminative localization. In CVPR, 2016."
REFERENCES,0.31114808652246256,"[40] Ross B. Girshick. Fast R-CNN. In ICCV, 2015."
REFERENCES,0.31281198003327787,"[41] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In CVPR, pages 248–255, 2009."
REFERENCES,0.3144758735440932,"[42] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In CVPR, 2016."
REFERENCES,0.3161397670549085,"[43] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-
tion. In ICLR, 2015."
REFERENCES,0.3178036605657238,"[44] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger.
Densely connected
convolutional networks. In CVPR, 2017."
REFERENCES,0.3194675540765391,"[45] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In CVPR, 2016."
REFERENCES,0.3211314475873544,"[46] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi. Inception-v4, inception-
resnet and the impact of residual connections on learning. In AAAI, 2017."
REFERENCES,0.3227953410981697,"[47] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and
Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR,
2021."
REFERENCES,0.324459234608985,"[48] Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian J. Goodfellow, Dan Boneh, and Patrick D. McDaniel.
Ensemble adversarial training: Attacks and defenses. In ICLR, 2018."
REFERENCES,0.3261231281198003,"[49] Alexey Kurakin, Ian J. Goodfellow, Samy Bengio, Yinpeng Dong, Fangzhou Liao, Ming Liang, Tianyu
Pang, Jun Zhu, Xiaolin Hu, Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, Alan L. Yuille, Sangxia
Huang, Yao Zhao, Yuzhe Zhao, Zhonglin Han, Junjiajia Long, Yerkebulan Berdibekov, Takuya Akiba,
Seiya Tokui, and Motoki Abe. Adversarial attacks and defences competition. CoRR, abs/1804.00097,
2018."
REFERENCES,0.3277870216306156,"[50] Lianli Gao, Qilong Zhang, Xiaosu Zhu, Jingkuan Song, and Heng Tao Shen. Staircase sign method for
boosting adversarial attacks. CoRR, abs/2104.09722, 2021."
REFERENCES,0.32945091514143093,"[51] Zhipeng Wei, Jingjing Chen, Zuxuan Wu, and Yu-Gang Jiang.
Enhancing the self-universality for
transferable targeted attacks. In CVPR, 2023."
REFERENCES,0.33111480865224624,"[52] Yuyang Long, Qilong Zhang, Boheng Zeng, Lianli Gao, Xianglong Liu, Jian Zhang, and Jingkuan Song.
Frequency domain model augmentation for adversarial attack. In ECCV, 2022."
REFERENCES,0.33277870216306155,"[53] Xiao Yang, Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Boosting transferability of targeted
adversarial examples via hierarchical generative networks. In Shai Avidan, Gabriel J. Brostow, Moustapha
Cissé, Giovanni Maria Farinella, and Tal Hassner, editors, ECCV, 2022."
REFERENCES,0.33444259567387685,"[54] Dylan J. Foster and Alexander Rakhlin.
ℓ∞vector contraction for rademacher complexity.
CoRR,
abs/1911.06468, 2019."
REFERENCES,0.33610648918469216,"[55] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning. MIT
Press, 2012."
REFERENCES,0.33777038269550747,"[56] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning - From Theory to Algorithms.
Cambridge University Press, 2014."
REFERENCES,0.33943427620632277,"[57] Yunwen Lei, Ürün Dogan, Ding-Xuan Zhou, and Marius Kloft. Data-dependent generalization bounds for
multi-class classiﬁcation. IEEE Trans. Inf. Theory, 65(5):2995–3021, 2019."
REFERENCES,0.3410981697171381,"[58] Yong Liu. Reﬁned learning bounds for kernel and approximate k-means. In Marc’Aurelio Ranzato, Alina
Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural
Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021,
NeurIPS 2021, December 6-14, 2021, virtual, 2021."
REFERENCES,0.3427620632279534,"[59] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American
Statistical Association, 58(301):13–30, 1963."
REFERENCES,0.34442595673876875,"Supplementary Material:
Perturbation Towards Easy Samples Improves Targeted
Adversarial Transferability"
REFERENCES,0.34608985024958405,"A
Proofs"
REFERENCES,0.34775374376039936,"A.1
Proof of Theorem 1"
REFERENCES,0.34941763727121466,"First, we need the deﬁnition below:"
REFERENCES,0.35108153078202997,"Deﬁnition 2 ((j, k, x0, r)-Local Output Rademacher Complexity) Let f S
w = (f S1
w , . . . , f SK
w ) ∈
[0, 1]K, FSk
w :=

f Sk
w : w ∈W
	
, given B(x0, r) = {x : ∥x −x0∥≤r}, (x0, y0) ∈Sj, then the
empirical (j, k, x0, r)-local output Rademacher Complexity is deﬁned by:"
REFERENCES,0.3527454242928453,"R(j,x0,r)
n
(FSk
w ) := Eσ  
sup"
REFERENCES,0.3544093178036606,"f
Sk
w ∈F
Sk
w  X"
REFERENCES,0.3560732113144759,"i∈I(j,x0,r)
σif Sk
w (xi)   ,"
REFERENCES,0.3577371048252912,"where σi, i
∈
I(j,x0,r) are i.i.d random variables satisﬁes P (σi = 1)
=
P (σi = −1)
=
1
2.
Correspondingly, the expected (j, k, x0, r)-local output Rademacher complexity is de-"
REFERENCES,0.3594009983361065,"ﬁned as R(j,x0,r)(FSk
w )
=
ES(j,x0,r)
h
R(j,x0,r)
n
(FSk
w ) | xi ∈C(j,x0,r)
i
, where S(j,x0,r)
="
REFERENCES,0.3610648918469218,"{(x, y) ∈Sj : x ∈B(x0, r), (x0, y0) ∈S}."
REFERENCES,0.3627287853577371,"We ﬁrst prove the lemma 1 and use it to prove theorem 1, and we need lemma 2 and 3 before proving
the lemma 1."
REFERENCES,0.3643926788685524,"Lemma 1 Given B(x0, r), (x0, y0) ∈Sj, S(j,x0,r), and f Sk
w ∈FSk
w , then with probability at least
1 −δ, the following holds for any f Sk
w ∈FSk
w and k ∈[K]:
Ex∼Dx|y

f Sk
w (x) | x ∈C(j,x0,r)

−ˆES(j,x0,r)

f Sk
w
"
REFERENCES,0.36605657237936773,"≤4Cw
s"
REFERENCES,0.36772046589018303,"K
I(j,x0,r)
 log2   q"
REFERENCES,0.36938435940099834,"2
I(j,x0,r) bw  + 3 s"
REFERENCES,0.37104825291181365,"log(2/δ)
2
I(j,x0,r)
,"
REFERENCES,0.37271214642262895,"where K is the number of classes, Cw and bw are constants."
REFERENCES,0.37437603993344426,"Lemma 2 (l∞Contraction Inequality [54] ) Let F ⊆

f : X →RK	
, and let φ : RK →R be
L-Lipschitz with respect to the l∞norm, i.e. ∥φ(v) −φ (v′)∥∞≤L · ∥v −v′∥∞, ∀v, v′ ∈RK . For
any a > 0 , there exists a constant C > 0 such that if |φ(f(x))| ∨∥f(x)∥∞≤ζ , then"
REFERENCES,0.37603993344425957,"Rn(φ ◦F) ≤C · L
√"
REFERENCES,0.3777038269550749,"K max
i
˜Rn (Fi) log
3
2 +a

ζn
maxi ˜Rn (Fi) 
,"
REFERENCES,0.3793677204658902,"where Rn(φ ◦F) = Eσ

supf∈F |Pn
i=1 σiφ (f (xi))|

, ˜Rn (Fi) = supS∈X n Rn (Fi) ."
REFERENCES,0.3810316139767055,"Lemma 3 Given B(x0, r), (x0, y0) ∈Sj, S(j,x0,r) and FSk
w , with probability at least 1 −δ, holds:"
REFERENCES,0.3826955074875208,"R(j,x0,r)(FSk
w ) ≤R(j,x0,r)
n
(FSk
w ) +"
REFERENCES,0.3843594009983361,"sI(j,x0,r)
 log(1/δ)
2
."
REFERENCES,0.3860232945091514,"Proof of Lemma 3
Let S′
(j,x0,r) be a dataset that has at most one element different with S(j,x0,r), its
elements are x′
i, i ∈I(j,x0,r), then Eσ  
sup"
REFERENCES,0.3876871880199667,"f
Sk
w ∈F
Sk
w  X"
REFERENCES,0.389351081530782,"i∈I(j,x0,r)
σif Sk
w (xi)   −Eσ  
sup"
REFERENCES,0.3910149750415973,"f
Sk
w ∈F
Sk
w  X"
REFERENCES,0.39267886855241263,"i∈I(j,x0,r)
σif Sk
w (x′
i)    ≤Eσ  
sup"
REFERENCES,0.39434276206322794,"f
Sk
w ∈F
Sk
w    X"
REFERENCES,0.39600665557404324,"i∈I(j,x0,r)
σif Sk
w (xi) −  X"
REFERENCES,0.39767054908485855,"i∈I(j,x0,r)
σif Sk
w (x′
i)      ≤Eσ  
sup"
REFERENCES,0.39933444259567386,"f
Sk
w ∈F
Sk
w  X"
REFERENCES,0.40099833610648916,"i∈I(j,x0,r)
σi
 
f Sk
w (xi) −f Sk
w (x′
i)
  ≤1,"
REFERENCES,0.40266222961730447,"the last inequality is because S′
(j,x0,r) differs from S(j,x0,r) with at most one element, and f Sk
w ∈[0, 1].
Then use McDiarmid’s inequality ([55], Theorem D.3), we have"
REFERENCES,0.4043261231281198,"R(j,x0,r)(FSk
w ) ≤R(j,x0,r)
n
(FSk
w ) +"
REFERENCES,0.4059900166389351,"sI(j,x0,r)
 log(1/δ)
2"
REFERENCES,0.40765391014975044,holds with probability at least 1 −δ.
REFERENCES,0.40931780366056575,"In the following we use the notation: ˆES(j,x0,r)

f Sk
w

=
1
|I(j,x0,r)|
P"
REFERENCES,0.41098169717138106,"i∈I(j,x0,r) f Sk
w (xi), then deﬁne:"
REFERENCES,0.41264559068219636,"Φ(S(j,x0,r)) =
sup"
REFERENCES,0.41430948419301167,"f
Sk
w ∈F
Sk
w"
REFERENCES,0.415973377703827,"Ex∼Dx|y

f Sk
w (x) | x ∈C(j,x0,r)

−ˆES(j,x0,r)

f Sk
w
 ."
REFERENCES,0.4176372712146423,"Lemma 4 Given B(x0, r), (x0, y0) ∈Sj and f Sk
w
∈FSk
w , with probability at least 1 −δ, the
following holds for any f Sk
w ∈FSk
w :"
REFERENCES,0.4193011647254576,"Ex∼Dx|y

f Sk
w (x) | x ∈C(j,x0,r)

−ˆES(j,x0,r)

f Sk
w
 ≤
2
I(j,x0,r)
R(j,x0,r)(FSk
w )+ s"
REFERENCES,0.4209650582362729,"log(1/δ)
2
I(j,x0,r)"
REFERENCES,0.4226289517470882,"Proof of Lemma 4
Similarly, let S′
(j,x0,r) be a dataset that has at most one element different with
S(j,x0,r), then we have"
REFERENCES,0.4242928452579035,"Φ(S′
(j,x0,r)) −Φ(S(j,x0,r)) =
sup"
REFERENCES,0.4259567387687188,"f
Sk
w ∈F
Sk
w"
REFERENCES,0.4276206322795341,"Ex∼Dx|y

f Sk
w (x) | x ∈C(j,x0,r)

−ˆES′
(j,x0,r)

f Sk
w
 −
sup"
REFERENCES,0.4292845257903494,"f
Sk
w ∈F
Sk
w"
REFERENCES,0.43094841930116473,"Ex∼Dx|y

f Sk
w (x) | x ∈C(j,x0,r)

−ˆES(j,x0,r)

f Sk
w
 ≤
sup"
REFERENCES,0.43261231281198004,"f
Sk
w ∈F
Sk
w"
REFERENCES,0.43427620632279534,"ˆES′
(j,x0,r)

f Sk
w

−ˆES(j,x0,r)

f Sk
w
 =
sup"
REFERENCES,0.43594009983361065,"f
Sk
w ∈F
Sk
w "
REFERENCES,0.43760399334442596,"1
I(j,x0,r)

X"
REFERENCES,0.43926788685524126,"i∈I(j,x0,r)"
REFERENCES,0.44093178036605657," 
f Sk
w (x′
i) −f Sk
w (xi)
"
REFERENCES,0.4425956738768719,"≤
1
I(j,x0,r)
,"
REFERENCES,0.4442595673876872,"then use McDiarmid’s inequality, with probability at least 1 −δ, the following holds:"
REFERENCES,0.4459234608985025,"Φ(S(j,x0,r)) ≤ES(j,x0,r)

Φ(S(j,x0,r)) | xi ∈C(j,x0,r)

+ s"
REFERENCES,0.4475873544093178,"log(1/δ)
2
I(j,x0,r)
,
(1)"
REFERENCES,0.4492512479201331,"then we could use the standard symmetrization technique ([56], Theorem 3.1) to derive the following:"
REFERENCES,0.4509151414309484,"ES(j,x0,r)

Φ(S(j,x0,r)) | xi ∈C(j,x0,r)
"
REFERENCES,0.4525790349417637,"= ES(j,x0,r) "" sup"
REFERENCES,0.454242928452579,"f
Sk
w ∈F
Sk
w"
REFERENCES,0.4559068219633943,"Ex∼Dx|y

f Sk
w (x) | x ∈C(j,x0,r)

−ˆES(j,x0,r)

f Sk
w
 | xi ∈C(j,x0,r) #"
REFERENCES,0.45757071547420963,"= ES(j,x0,r) "" sup"
REFERENCES,0.45923460898502494,"f
Sk
w ∈F
Sk
w"
REFERENCES,0.46089850249584025,"ES′
(j,x0,r)"
REFERENCES,0.46256239600665555,"h
ˆES′
(j,x0,r)

f Sk
w

−ˆES(j,x0,r)

f Sk
w

| x′
i ∈C(j,x0,r)
i | xi ∈C(j,x0,r) #"
REFERENCES,0.46422628951747086,"≤
Jensen ES(j,x0,r),S′
(j,x0,r) "" sup"
REFERENCES,0.46589018302828616,"f
Sk
w ∈F
Sk
w"
REFERENCES,0.46755407653910147,"ˆES′
(j,x0,r)

f Sk
w

−ˆES(j,x0,r)

f Sk
w
 | xi, x′
i ∈C(j,x0,r) #"
REFERENCES,0.46921797004991683,"= ES(j,x0,r),S′
(j,x0,r)  
sup"
REFERENCES,0.47088186356073214,"f
Sk
w ∈F
Sk
w"
REFERENCES,0.47254575707154745,"1
I(j,x0,r)  X"
REFERENCES,0.47420965058236275,"i∈I(j,x0,r)"
REFERENCES,0.47587354409317806," 
f Sk
w (x′
i) −f Sk
w (xi)


| xi, x′
i ∈C(j,x0,r)  "
REFERENCES,0.47753743760399336,"= ES(j,x0,r),S′
(j,x0,r)  
sup"
REFERENCES,0.47920133111480867,"f
Sk
w ∈F
Sk
w"
REFERENCES,0.480865224625624,"1
I(j,x0,r) Eσ  
X"
REFERENCES,0.4825291181364393,"i∈I(j,x0,r)
σi
 
f Sk
w (x′
i) −f Sk
w (xi)

 "
REFERENCES,0.4841930116472546,"| xi, x′
i ∈C(j,x0,r)  "
REFERENCES,0.4858569051580699,(Since S and S′ have same distribution)
REFERENCES,0.4875207986688852,"≤
Jensen ES(j,x0,r),S′
(j,x0,r),σ  
sup"
REFERENCES,0.4891846921797005,"f
Sk
w ∈F
Sk
w"
REFERENCES,0.4908485856905158,"1
I(j,x0,r)  X"
REFERENCES,0.4925124792013311,"i∈I(j,x0,r)
σi
 
f Sk
w (x′
i) −f Sk
w (xi)


| xi, x′
i ∈C(j,x0,r)  "
REFERENCES,0.49417637271214643,"≤2ES(j,x0,r),σ  
sup"
REFERENCES,0.49584026622296173,"f
Sk
w ∈F
Sk
w"
REFERENCES,0.49750415973377704,"1
I(j,x0,r)  X"
REFERENCES,0.49916805324459235,"i∈I(j,x0,r)
σif Sk
w (xi)"
REFERENCES,0.5008319467554077,"| xi ∈C(j,x0,r)  "
REFERENCES,0.502495840266223,"=
2
I(j,x0,r)
R(j,x0,r)(FSk
w ),"
REFERENCES,0.5041597337770383,"combine with (1), we can get:"
REFERENCES,0.5058236272878536,"Ex∼Dx|y

f Sk
w (x) | x ∈C(j,x0,r)

−ˆES(j,x0,r)

f Sk
w
 ≤
2
I(j,x0,r)
R(j,x0,r)(FSk
w )+ s"
REFERENCES,0.5074875207986689,"log(1/δ)
2
I(j,x0,r)"
REFERENCES,0.5091514143094842,holds with probability at least 1 −δ.
REFERENCES,0.5108153078202995,"Proof of Lemma 1
Let φk : RK →R satisﬁes φk(v) = vk, where v = (v1, . . . , vk) ∈RK. Then"
REFERENCES,0.5124792013311148,"R(j,x0,r)
n
(FSk
w ) = R(j,x0,r)
n
(φk ◦FS
w),"
REFERENCES,0.5141430948419301,"since |φk(v) −φk(v′)| = |vk −v′
k| ≤∥v −v′∥∞, ∀v, v′ ∈RK, i.e. φk is 1-Lipschitz. For f S
w ∈
FS
w, we have
f S
w

∞≤1, therefore,
φk
 
f S
w (x)
 ∨
f S
w (x)

∞≤1, then use Lemma 2, choose
L = 1, ζ = 1, a = 1"
REFERENCES,0.5158069883527454,"2, there exists a constant Cw
k > 0 such that"
REFERENCES,0.5174708818635607,"R(j,x0,r)
n
(φk ◦FS
w) ≤Cw
k ·
√"
REFERENCES,0.519134775374376,"K max
k
˜R(j,x0,r)
n
 
FSk
w

log2 "
REFERENCES,0.5207986688851913,"
I(j,x0,r)"
REFERENCES,0.5224625623960066,"maxk ˜R(j,x0,r)
n

FSk
w
 "
REFERENCES,0.5241264559068219,",
(2)"
REFERENCES,0.5257903494176372,"where
˜R(j,x0,r)
n
 
FSk
w

=
sup
S(j,x0,r)∈Z|I(j,x0,r)|"
REFERENCES,0.5274542429284526,"(j,x0,r)
R(j,x0,r)
n
 
FSk
w

,
and
we
denote

(x, y) ∈Z : x ∈C(j,x0,r)
	
as Z(j,x0,r)."
REFERENCES,0.5291181364392679,"For ∀S(j,x0,r) ∈Z|I(j,x0,r)|
(j,x0,r)
, ∀f Sk
w ∈FSk
w , we have Eσ    X"
REFERENCES,0.5307820299500832,"i∈I(j,x0,r)
σif Sk
w (xi)   = Eσ  "
REFERENCES,0.5324459234608985,"v
u
u
u
t  X"
REFERENCES,0.5341098169717138,"i∈I(j,x0,r)
σif Sk
w (xi)  2 "
REFERENCES,0.5357737104825291,"≤
Jensen"
REFERENCES,0.5374376039933444,"v
u
u
u
u
tEσ    X"
REFERENCES,0.5391014975041597,"i∈I(j,x0,r)
σif Sk
w (xi)  2  ≤"
REFERENCES,0.540765391014975,"v
u
u
t X"
REFERENCES,0.5424292845257903,"i∈I(j,x0,r)"
REFERENCES,0.5440931780366056,"f Sk
w (xi)

2
≤√n,"
REFERENCES,0.5457570715474209,"the penultimate inequality uses the Khintchine-Kahane Inequality [57]. Therefore,"
REFERENCES,0.5474209650582362,"R(j,x0,r)
n
 
FSk
w

= Eσ  
sup"
REFERENCES,0.5490848585690515,"f
Sk
w ∈F
Sk
w  X"
REFERENCES,0.5507487520798668,"i∈I(j,x0,r)
σif Sk
w (xi)   ≤√n,"
REFERENCES,0.5524126455906821,"which means ˜R(j,x0,r)
n
 
FSk
w

≤√n."
REFERENCES,0.5540765391014975,"Choose (˜x, ˜y) ∈Z(j,x0,r), ˜f Sk
w ∈˜FSk
w satisﬁes Eσ    X"
REFERENCES,0.5557404326123128,"i∈I(j,x0,r)
σi ˜f Sk
w (˜x)  "
REFERENCES,0.5574043261231281,"=
sup"
REFERENCES,0.5590682196339434,"z∈Z(j,x0,r),f
Sk
w ∈F
Sk
w
Eσ    X"
REFERENCES,0.5607321131447587,"i∈I(j,x0,r)
σif Sk
w (x)   ,"
REFERENCES,0.562396006655574,"then, similar to Lemma 3 in [58], we have sup"
REFERENCES,0.5640599001663894,"S(j,x0,r)∈Z|I(j,x0,r)|"
REFERENCES,0.5657237936772047,"(j,x0,r)"
REFERENCES,0.56738768718802,"R(j,x0,r)
n
 
FSk
w

=
sup"
REFERENCES,0.5690515806988353,"S(j,x0,r)∈Z|I(j,x0,r)|"
REFERENCES,0.5707154742096506,"(j,x0,r) Eσ  
sup"
REFERENCES,0.5723793677204659,"f
Sk
w ∈F
Sk
w  X"
REFERENCES,0.5740432612312812,"i∈I(j,x0,r)
σif Sk
w (xi)   "
REFERENCES,0.5757071547420965,"≥
sup
z∈Z(j,x0,r)
Eσ  
sup"
REFERENCES,0.5773710482529119,"f
Sk
w ∈F
Sk
w  X"
REFERENCES,0.5790349417637272,"i∈I(j,x0,r)
σif Sk
w (x)   "
REFERENCES,0.5806988352745425,"≥
Jensen
sup"
REFERENCES,0.5823627287853578,"z∈Z(j,x0,r),f
Sk
w ∈F
Sk
w
Eσ    X"
REFERENCES,0.5840266222961731,"i∈I(j,x0,r)
σif Sk
w (x)    = Eσ    X"
REFERENCES,0.5856905158069884,"i∈I(j,x0,r)
σi ˜f Sk
w (˜x)    ≥ √ 2
2"
REFERENCES,0.5873544093178037,"v
u
u
t X"
REFERENCES,0.589018302828619,"i∈I(j,x0,r)"
REFERENCES,0.5906821963394343,"˜f Sk
w (˜x)

2 = q"
REFERENCES,0.5923460898502496,"2
I(j,x0,r) 2"
REFERENCES,0.5940099833610649,"˜f Sk
w (˜x)
 ,"
REFERENCES,0.5956738768718802,"similarly, the penultimate inequality also uses the Khintchine-Kahane Inequality [57]. Below we de-"
REFERENCES,0.5973377703826955,"note
 ˜f Sk
w (˜x)
 as bw,k. Choose bw = max
1≤k≤Kbw,k, which satisﬁes maxk ˜R(j,x0,r)
n
≥ q"
REFERENCES,0.5990016638935108,"2|I(j,x0,r)|bw 2
,"
REFERENCES,0.6006655574043261,"and Cw = max
1≤k≤KCw
k , combine with (2), we have"
REFERENCES,0.6023294509151415,"R(j,x0,r)
n
(φk ◦FS
w) ≤Cw ·
q"
REFERENCES,0.6039933444259568,"K
I(j,x0,r)
 log2   q"
REFERENCES,0.6056572379367721,"2
I(j,x0,r) bw  
(3)"
REFERENCES,0.6073211314475874,"holds for all k ∈[K]. Then use Lemma 3 and Lemma 4, with probability at least 1 −δ, holds
Ex∼Dx|y

f Sk
w (x) | x ∈C(j,x0,r)

−ˆES(j,x0,r)

f Sk
w
"
REFERENCES,0.6089850249584027,"≤
2
I(j,x0,r)
R(j,x0,r)
n
(FSk
w ) + 3 s"
REFERENCES,0.610648918469218,"log(2/δ)
2
I(j,x0,r)"
REFERENCES,0.6123128119800333,"then combine with (3), we have
Ex∼Dx|y

f Sk
w (x) | x ∈C(j,x0,r)

−ˆES(j,x0,r)

f Sk
w
"
REFERENCES,0.6139767054908486,"≤4Cw
s"
REFERENCES,0.6156405990016639,"K
I(j,x0,r)
 log2   q"
REFERENCES,0.6173044925124792,"2
I(j,x0,r) bw  + 3 s"
REFERENCES,0.6189683860232945,"log(2/δ)
2
I(j,x0,r)"
REFERENCES,0.6206322795341098,holds with probability at least 1 −δ.
REFERENCES,0.6222961730449251,"Proof of Theroem 1
For the inequality in Lemma 1, by taking δ′ =
δ
K for each k ∈[K], we can
guarantee with probability at least 1 −δ, the inequality
Ex∼Dx|y

f Sk
w (x) | x ∈C(j,x0,r)

−ˆES(j,x0,r)

f Sk
w
"
REFERENCES,0.6239600665557404,"≤4Cw
s"
REFERENCES,0.6256239600665557,"K
I(j,x0,r)
 log2   q"
REFERENCES,0.627287853577371,"2
I(j,x0,r) bw  + 3 s"
REFERENCES,0.6289517470881864,"log(2K/δ)
2
I(j,x0,r)"
REFERENCES,0.6306156405990017,"simultaneously holds for each k ∈[K].
Then for two different parametrized class FSk
w1 :=

f Sk
w1 : w1 ∈W1
	
and FSk
w
:=

f Sk
w : w ∈W
	
, just take C = max {Cw1, Cw} and b =
min {bw1, bw}, the following simultaneously holds for any i ∈{1, 2} and k ∈[K]:"
REFERENCES,0.632279534109817,"Ex∼Dx|y

f Sk
wi(x) | x ∈C(j,x0,r)

−ˆES(j,x0,r)

f Sk
wi
 ≤4C s"
REFERENCES,0.6339434276206323,"K
I(j,x0,r)
 log2   q"
REFERENCES,0.6356073211314476,"2
I(j,x0,r) b  + 3 s"
REFERENCES,0.6372712146422629,"log(2K/δ)
2
I(j,x0,r)
."
REFERENCES,0.6389351081530782,"According to the assumption
1
|I(j,x0,r)| P"
REFERENCES,0.6405990016638935,"i∈I(j,x0,r)
 
f Sk
w1(xi) −f Sk
w (xi)
 ≤γ, which means
ˆES(j,x0,r)

f Sk
w1

−ˆES(j,x0,r)

f Sk
w
 ≤γ, then we have
Ex∼Dx|y

f Sk
w1(x) | x ∈C(j,x0,r)

−Ex∼Dx|y

f Sk
w (x) | x ∈C(j,x0,r)
"
REFERENCES,0.6422628951747088,"≤
Ex∼Dx|y

f Sk
w1(x) | x ∈C(j,x0,r)

−ˆES(j,x0,r)

f Sk
w1
"
REFERENCES,0.6439267886855241,"+
ˆES(j,x0,r)

f Sk
w1

−ˆES(j,x0,r)

f Sk
w
"
REFERENCES,0.6455906821963394,"+
Ex∼Dx|y

f Sk
w (x) | x ∈C(j,x0,r)

−ˆES(j,x0,r)

f Sk
w
 ≤8C s"
REFERENCES,0.6472545757071547,"K
I(j,x0,r)
 log2   q"
REFERENCES,0.64891846921797,"2
I(j,x0,r) b  + 6 s"
REFERENCES,0.6505823627287853,"log(2K/δ)
2
I(j,x0,r)
 + γ,"
REFERENCES,0.6522462562396006,"thus,
Ex∼Dx|y

f S
w1(x) | x ∈C(j,x0,r)

−Ex∼Dx|y

f S
w(x) | x ∈C(j,x0,r)

∞
= max
k∈[K]"
REFERENCES,0.653910149750416,"Ex∼Dx|y

f Sk
w1(x) | x ∈C(j,x0,r)

−Ex∼Dx|y

f Sk
w (x) | x ∈C(j,x0,r)
 ≤8C s"
REFERENCES,0.6555740432612313,"K
I(j,x0,r)
 log2   q"
REFERENCES,0.6572379367720466,"2
I(j,x0,r) b  + 6 s"
REFERENCES,0.6589018302828619,"log(2K/δ)
2
I(j,x0,r)
 + γ."
REFERENCES,0.6605657237936772,"By the deﬁnition, we have
I(j,x0,r)
 = ρ(j,x0,r) · volB(x0, r), and note that"
REFERENCES,0.6622296173044925,"
2r
√ d"
REFERENCES,0.6638935108153078,"d
≤volB(x0, r) =
π
d
2
Γ( d"
REFERENCES,0.6655574043261231,"2 +1)rd ≤6rd,"
REFERENCES,0.6672212978369384,"therefore we can derive that with probability at least 1 −δ, the following holds:
Ex∼Dx|y

f S
w1(x) | x ∈C(j,x0,r)

−Ex∼Dx|y

f S
w(x) | x ∈C(j,x0,r)

∞ ≤8C s Kdd/2"
REFERENCES,0.6688851913477537,"ρ(j,x0,r)2drd log2
 
2rdp"
REFERENCES,0.670549084858569,"3ρj(x0, r) b ! + 6 s"
REFERENCES,0.6722129783693843,dd/2 log(2K/δ)
REFERENCES,0.6738768718801996,"ρ(j,x0,r)2d+1rd + γ,"
REFERENCES,0.6755407653910149,which completes the proof.
REFERENCES,0.6772046589018302,"A.2
Proof of Theorem 2"
REFERENCES,0.6788685524126455,"First, note that"
REFERENCES,0.6805324459234608,"R(j,x0,r)(w1) −R(j,x0,r)(w2)
 = "
REFERENCES,0.6821963394342762,"1
I(j,x0,r)

X"
REFERENCES,0.6838602329450915,"i∈I(j,x0,r)
(ℓ(w1, xi) −ℓ(w2, xi)) "
REFERENCES,0.6855241264559068,"≤
1
I(j,x0,r)

X"
REFERENCES,0.6871880199667221,"i∈I(j,x0,r)
∥ℓ(w1, xi) −ℓ(w2, xi)∥"
REFERENCES,0.6888519134775375,"≤L1 ∥w1 −w2∥,"
REFERENCES,0.6905158069883528,"the second inequality holds because of Assumption 1. Then due to the Lipschitz continuous gradient
of R(j,x0,r)(w), we have"
REFERENCES,0.6921797004991681,"R(j,x0,r)(wt+1) −R(j,x0,r)(wt) ≤

∇wR(j,x0,r)(wt), wt+1 −wt
+ L1 2"
REFERENCES,0.6938435940099834,"wt+1 −wt2 = −ηt *"
REFERENCES,0.6955074875207987,"∇wR(j,x0,r)(wt), 1 M X"
REFERENCES,0.697171381031614,"i∈IBt
∇wℓ(wt, xi) +"
REFERENCES,0.6988352745424293,"+ L1η2
t
2 "
M,0.7004991680532446,"1
M X"
M,0.7021630615640599,"i∈IBt
∇wℓ(wt, xi)  2 ≤−ηt *"
M,0.7038269550748752,"∇wR(j,x0,r)(wt), 1 M X"
M,0.7054908485856906,"i∈IBt
∇wℓ(wt, xi) +"
M,0.7071547420965059,"+ L1η2
t G2 2
,"
M,0.7088186356073212,"the inequality holds because of Assumption 2, prescribe notation ξt =
IBt ∩I(j,x0,r)
, combined
with Assumption 3, 4, we have −ηt *"
M,0.7104825291181365,"∇wR(j,x0,r)(wt), 1 M X"
M,0.7121464226289518,"i∈IBt
∇wℓ(wt, xi) + = −ηt M *"
M,0.7138103161397671,"∇wR(j,x0,r)(wt),
X"
M,0.7154742096505824,"i∈IBt∩I(j,x0,r)
∇wℓ(wt, xi) + −ηt M *"
M,0.7171381031613977,"∇wR(j,x0,r)(wt),
X"
M,0.718801996672213,"i∈IBt\I(j,x0,r)
∇wℓ(wt, xi) +"
M,0.7204658901830283,≤−βηtξt M
M,0.7221297836938436,"∇wR(j,x0,r)(wt)
2 + (M −ξt) ηtG2 M"
M,0.7237936772046589,≤−2βµηtξt M
M,0.7254575707154742,"
R(j,x0,r)(wt) −R∗
(j,x0,r)

+ ηtG2,"
M,0.7271214642262895,"choose ηt =
1
βµt, due to T ≤
L1
2βµ, ηtG2 ≤L1η2
t G2 2
."
M,0.7287853577371048,"Note that ξt ∼Hypergeometric
 
n,
I(j,x0,r)
 , M

, by Hoeffding’s inequality [59], we have"
M,0.7304492512479202,"P
 
ξt −
I(j,x0,r)
 ≤−Mε

≤e−2Mε2,"
M,0.7321131447587355,take δ
M,0.7337770382695508,"T = e−2Mε2, we have"
M,0.7354409317803661,"ξt
M ≥"
M,0.7371048252911814,"I(j,x0,r) M
− r"
M,0.7387687188019967,ln (T/δ)
M,0.740432612312812,2M
M,0.7420965058236273,"holds simultaneously for all t ∈{1, 2, . . . , T} with probability at least 1 −δ, which means"
M,0.7437603993344426,−2βµηtξt M
M,0.7454242928452579,"
R(j,x0,r)(wt) −R∗
(j,x0,r)
"
M,0.7470881863560732,≤−2βµηt max
M,0.7487520798668885,"( I(j,x0,r) M
− r"
M,0.7504159733777038,ln (T/δ)
M,0.7520798668885191,"2M ! , 0"
M,0.7537437603993344,") 
R(j,x0,r)(wt) −R∗
(j,x0,r)

."
M,0.7554076539101497,"Therefore,"
M,0.757071547420965,"R(j,x0,r)(wt+1) −R∗
(j,x0,r)
≤R(j,x0,r)(wt) −R∗
(j,x0,r) ≤  1 −2 t max"
M,0.7587354409317804,"( I(j,x0,r) M
− r"
M,0.7603993344425957,ln (T/δ)
M,0.762063227953411,"2M ! , 0"
M,0.7637271214642263,")! 
R(j,x0,r)(wt) −R∗
(j,x0,r)

+
L1G2"
M,0.7653910149750416,2β2µ2t2 =  1 −2 t max
M,0.7670549084858569,"( 
ρ(j,x0,r)πd/2rd Γ
  d"
M,0.7687188019966722,"2 + 1

M
− r"
M,0.7703826955074875,ln (T/δ)
M,0.7720465890183028,"2M ! , 0"
M,0.7737104825291181,")! 
R(j,x0,r)(wt) −R∗
(j,x0,r)

+
L1G2"
M,0.7753743760399334,2β2µ2t2
M,0.7770382695507487,"holds simultaneously for all t ∈{1, 2, . . . , T} with probability at least 1 −δ."
M,0.778702163061564,"A.3
Proof of Proposition 1"
M,0.7803660565723793,"Under Assumption 1, due to the Lipschitz continuous gradient, for ∀(xi, yi) ∈S, we have"
M,0.7820299500831946,"ℓ(w, xi) ≥ℓ(w, x) + ⟨∇xℓ(w, x) , xi −x⟩−L2"
M,0.78369384359401,"2 ∥xi −x∥2 ,
(4)"
M,0.7853577371048253,"ℓ(w, xi) ≤ℓ(w, x) + ⟨∇xℓ(w, x) , xi −x⟩+ L2"
M,0.7870216306156406,"2 ∥xi −x∥2
(5)"
M,0.7886855241264559,"ℓ(w, x) ≤ℓ(w, xi) + ⟨∇xℓ(w, xi) , x −xi⟩+ L2"
M,0.7903494176372712,"2 ∥xi −x∥2
(6)"
M,0.7920133111480865,"holds for ∀x ∈X. By (4), choose x = xi −r ∇xℓ(w,x)"
M,0.7936772046589018,"∥∇xℓ(w,x)∥, which satisﬁes ∥xi −x∥≤r, then we
have"
M,0.7953410981697171,"ℓ(w, xi) ≥ℓ

w, xi −r ∇xℓ(w, x)"
M,0.7970049916805324,"∥∇xℓ(w, x)∥"
M,0.7986688851913477,"
+ r ∥∇xℓ(w, x)∥−L2r2 2"
M,0.800332778702163,"≥r ∥∇xℓ(w, x)∥−L2r2 2"
M,0.8019966722129783,"= r ∥∇xℓ(w, x) −∇xℓ(w, xi) + ∇xℓ(w, xi)∥−L2r2 2"
M,0.8036605657237936,"≥r (∥∇xℓ(w, xi)∥−∥∇xℓ(w, xi) −∇xℓ(w, x)∥) −L2r2 2"
M,0.8053244592346089,"≥r (∥∇xℓ(w, xi)∥−L2 ∥xi −x∥) −L2r2 2"
M,0.8069883527454242,"≥r ∥∇xℓ(w, xi)∥−3L2r2 2
,"
M,0.8086522462562395,which means
M,0.8103161397670549,"∥∇xℓ(w, xi)∥≤ℓ(w, xi)"
M,0.8119800332778702,"r
+ 3L2r"
M,0.8136439267886856,"2
.
(7)"
M,0.8153078202995009,"Use (5), we have"
M,0.8169717138103162,"ℓ(w, xi) −ℓ(w, x) ≤⟨∇xℓ(w, x) , xi −x⟩+ L2"
M,0.8186356073211315,2 ∥xi −x∥2
M,0.8202995008319468,"≤r ∥∇xℓ(w, x)∥+ L2r2 2"
M,0.8219633943427621,"≤r (∥∇xℓ(w, xi)∥+ ∥∇xℓ(w, xi) −∇xℓ(w, x)∥) + L2r2 2"
M,0.8236272878535774,"≤r ∥∇xℓ(w, xi)∥+ 3L2r2 2
,"
M,0.8252911813643927,"then use (6), we have"
M,0.826955074875208,"ℓ(w, x) −ℓ(w, xi) ≤⟨∇xℓ(w, xi) , x −xi⟩+ L2"
M,0.8286189683860233,2 ∥xi −x∥2
M,0.8302828618968386,"≤r ∥∇xℓ(w, xi)∥+ L2r2"
THUS,0.831946755407654,"2
Thus
|ℓ(w, xi) −ℓ(w, x)|"
THUS,0.8336106489184693,"r
−3L2r"
THUS,0.8352745424292846,"2
≤∥∇xℓ(w, xi)∥.
(8)"
THUS,0.8369384359400999,"Combining (7) and (8), we get
|ℓ(w, xi) −ℓ(w, x)|"
THUS,0.8386023294509152,"r
−3L2r"
THUS,0.8402662229617305,"2
≤∥∇xℓ(w, xi)∥≤ℓ(w, xi)"
THUS,0.8419301164725458,"r
+ 3L2r 2
."
THUS,0.8435940099833611,"B
Details of Pre-training Embeddings"
THUS,0.8452579034941764,"In actual experiments, it was found that if the embedding is not pre-trained ﬁrst, the model convergence
speed will be quite slow. We tried to freeze the parameters of the embedding layer after pre-training
multiple epochs, but found that even with the same number of epochs each pre-training, the internal
structures between the embedding vectors are very different, and there may be clustering between
different class embedding (as shown in Figure6), the distance between multiple classes is very close,
which leads to redundant information from many other classes in embedding."
THUS,0.8469217970049917,"To address this issue and enable the generator’s embeddings to better reﬂect class information, we
employ the following loss, utilizing the feature outputs of the source model as guidance for embedding
pretraining:"
THUS,0.848585690515807,LM = P
THUS,0.8502495840266223,"i,j M
Seuc
i,j log"
THUS,0.8519134775374376,"M
Seuc
i,j
M
Eeuc
i,j
+ P"
THUS,0.8535773710482529,"i,j M
Eeuc
i,j
log"
THUS,0.8552412645590682,"M
Eeuc
i,j
M
Seuc
i,j
+ P
i,j M
Scos
i,j
log"
THUS,0.8569051580698835,"M
Scos
i,j
M
Ecos
i,j
+ P
i,j M
Ecos
i,j
log M i,j"
THUS,0.8585690515806988,"M i,j ."
THUS,0.8602329450915142,"However, directly optimizing the LM leads to the issue of loss collapse. We analyse each component
of the loss. First, note that for ∀i, j ∈[K], −1 ≤M Ecos
i,j
≤1, −1 ≤M Scos
i,j
≤1. Then"
THUS,0.8618968386023295,"M
Ecos
i,j
=
exp(M Ecos
i,j
)
PK
k=1 exp(M Ecos
i,k ) =
1
1+P"
THUS,0.8635607321131448,"k̸=j exp(M Ecos
i,k
−M Ecos
i,j
) ≥
1
1+(K−1) exp(2),"
THUS,0.8652246256239601,"M
Scos
i,j
=
exp(M Scos
i,j )
PK
k=1 exp(M Scos
i,k ) =
1
1+P"
THUS,0.8668885191347754,"k̸=j exp(M Scos
i,k
−M Scos
i,j ) ≥
1
1+(K−1) exp(2)."
THUS,0.8685524126455907,"Under the settings in this paper, K = 10, so both terms analyse above are positive with lower bounds.
In the optimization process, their ratio will not approach zero. As for M
Seuc
i,j , it is a deterministic
positive constant. Therefore, when it serves as the numerator, it will not lead to a ratio close to zero
inside the logarithm. However, for M
Eeuc
i,j , if it becomes close to zero when used as the numerator, it
will cause loss collapse. Note that"
THUS,0.870216306156406,"M
Eeuc
i,j
=
exp

M Eeuc
i,j
"
THUS,0.8718801996672213,"PK
k=1 exp

M Eeuc
i,k
 =
1 1 + P"
THUS,0.8735440931780366,"k̸=j exp

M Eeuc
i,k
−M Eeuc
i,j
 ≥
1"
THUS,0.8752079866888519,"K max
1≤k≤K exp

M Eeuc
i,k
−M Eeuc
i,j
."
THUS,0.8768718801996672,"Figure 6: The top row displays the result of directly trained for 9 epochs, while the bottom row shows
the result of directly trained for 49 epochs. The numbers 0 −9 represent different class labels in
sequential order. The two heatmaps on the left depict the Euclidean distance and cosine similarity
between the embeddings of each class. The two images on the right display the distances after PCA
projection onto a 2-dimensional plane (normalized and unnormalized, representing cosine similarity
and Euclidean distance, respectively)."
THUS,0.8785357737104825,"To prevent M
Eeuc
i,j from becoming too small, we need to ensure: 1"
THUS,0.8801996672212978,"K max
1≤k≤K exp

M Eeuc
i,k −M Eeuc
i,j
 > 0,"
THUS,0.8818635607321131,which means we need to ensure that
THUS,0.8835274542429284,"max
1≤k≤K exp

M Eeuc
i,k

̸= +∞, ∀1 ≤i ≤K,"
THUS,0.8851913477537438,"since M Eeuc
i,k , M Eeuc
i,j
are non-negative. Also, note that M Eeuc
i,k
=
ei −ek ≤
ei +
ek. Therefore, by
imposing the following constraint in the optimization:"
THUS,0.8868552412645591,"min LM
s.t.
ei ≤U, 1 ≤i ≤K"
THUS,0.8885191347753744,"We can ensure that M Eeuc
i,k
≤2U, which in turn guarantees
max
1≤k≤K exp M Eeuc
i,k
̸= +∞, ∀1 ≤i ≤K,"
THUS,0.8901830282861897,"thus avoiding the issue of M
Eeuc
i,j becoming too small and causing loss collapse."
THUS,0.891846921797005,"By explicitly incorporating the above constraint as a regularizer into the optimization objective, we
obtain the ﬁnal manifold matching loss:"
THUS,0.8935108153078203,LM = P
THUS,0.8951747088186356,"i,j M
Seuc
i,j log"
THUS,0.8968386023294509,"M
Seuc
i,j
M
Eeuc
i,j
+ P"
THUS,0.8985024958402662,"i,j M
Eeuc
i,j
log"
THUS,0.9001663893510815,"M
Eeuc
i,j
M
Seuc
i,j +λ1 P"
THUS,0.9018302828618968,"i,j M
Scos
i,j
log"
THUS,0.9034941763727121,"M
Scos
i,j
M
Ecos
i,j
+ P"
THUS,0.9051580698835274,"i,j M
Ecos
i,j
log M i,j M i,j"
THUS,0.9068219633943427,"
+ λ2
PK
i=1
ei ."
THUS,0.908485856905158,"In our experiments, we use the AdamW optimizer with a learning rate of 1.5e −5 for 15000 epochs
to optimize the embeddings, while setting λ1 to 5 and λ2 to 0.01. The results, as shown in Figure 7,
demonstrate a signiﬁcant improvement in the clustering of embeddings between different classes."
THUS,0.9101497504159733,"The trained embedding is denoted as ej ∈Rd1, j ∈[K], where ej ∈Rd1, j ∈[K]. For the utilized
Resblock, let the input and output channels be Cin and Cout respectively. The fully connected layer"
THUS,0.9118136439267887,Figure 7: The results obtained by optimizing LM.
THUS,0.913477537437604,"MLP(·) ∈Rd1 →RCout. Firstly, we employ the initial convolution kernel Conv1 to map the raw
data from the input h of the current layer, transitioning it from [Cin, H, W] to [Cout, H, W]. Then
we perform an addition operation between this mapped result and the embedding transformation
obtained through the MLP. Formally expressed as:"
THUS,0.9151414309484193,h′ = Conv1(h) + MLP(ey)
THUS,0.9168053244592346,"where y is the label of the input. Subsequently, we apply a second convolution to this output, followed
by a LayerNorm operation, and ﬁnally, pass it through an attention operation before adding it to the
residual. Formally stated as:"
THUS,0.9184692179700499,output = Attention(LN(Conv2(h′)) + h)
THUS,0.9201331114808652,"C
Additional Ablation Studies"
THUS,0.9217970049916805,"To investigate the impact of different settings during the training phase, we conduct several experi-
ments. First, we investigate the output consistency of the model for different values of q used for
sample screening, as shown in Figure 8. A smaller q implies a stricter selection, which leads to an
increase in output consistency (in terms of mean), but also results in a larger standard deviation. In
this case, q = 5 is a relatively suitable choice."
THUS,0.9234608985024958,"Then we validate the impact of different choices of d(·, ·) on targeted transferability. The results are
shown in Figure 9, where d(·, ·) was chosen as the Smooth L1 loss, which signiﬁcantly improves the
targeted transferability compared to L1 and L2 losses."
THUS,0.9251247920133111,"D
Implementation Details"
THUS,0.9267886855241264,"In the exploratory experiments conducted in Section 2, the three different network architectures we
employed are depicted in Table 4."
THUS,0.9284525790349417,Table 4: Architectures of NN Classiﬁers
THUS,0.930116472545757,"Model
Hidden Layer 1
Hidden Layer 2
Hidden Layer 3"
THUS,0.9317803660565723,"Model 1
Linear(2, 500)
Linear(500, 500)
Linear(500, 2)
ReLU
ReLU"
THUS,0.9334442595673876,"Model 2
Linear(2, 50)
Linear(50, 100)
Linear(100, 150)
ReLU
ReLU
ReLU
Linear(150, 2)"
THUS,0.9351081530782029,"Model 3
Linear(2, 20)
Linear(20, 20)
Linear(20, 20)
ReLU
ReLU
ReLU
Linear(20, 2)"
THUS,0.9367720465890182,"For the experiments on CIFAR10 mentioned in Section 2, we used the PyTorch framework. The
optimizer is SGD, with an initial learning rate of 0.1, weight decay of 5e −4, and momentum of 0.9.
We apply cosine annealing learning rate decay, with a maximum number of epochs (Tmax) set to 200.
Tolerance of Early-stopping is set to 30. Additionally, we used a batch size of 128 during training
and applied the following data augmentations:"
THUS,0.9384359400998337,• Randomly crops the input image to a size of 32 × 32 pixels with padding of 4 pixels.
THUS,0.940099833610649,"Figure 8: Density plots of output similarity between models under different q selections. For the
screened data, we calculate the cosine similarity between the output of the source model and the
output of all target models, and take the average to represent the output similarity of the data across
different models. ""q=all"" indicates no screening, and the dashed line represents the average for
different values of q."
THUS,0.9417637271214643,"Figure 9: Targeted transferability (Src:Res50) with different chosen of d(·, ·)."
THUS,0.9434276206322796,• Randomly ﬂips the input image horizontally with a probability of 0.5.
THUS,0.9450915141430949,"After data augmentation, the input is then transformed into a tensor and normalized with a mean of
(0.4914, 0.4822, 0.4465) and a standard deviation of (0.2023, 0.1994, 0.2010)."
THUS,0.9467554076539102,"For the experiments in Section 4, we used the structure of following models: ResNet-50, VGG19bn,
DenseNet-121, ResNet-152 from the torchvision 1 library, and others from the timm 2 library. The
torchattacks 3 library is utilized for MIM, SI-NIM, TIM, and DIM."
THUS,0.9484193011647255,"E
Additional Experimental Results"
THUS,0.9500831946755408,"E.1
Ensemble Model as Source"
THUS,0.9517470881863561,"In Table 5, we report the results of the case of an ensemble model as the source model. The selected
ensemble model are consist of three models: Res-50, Inc-v3, and Inc-v4, while the other models"
THUS,0.9534109816971714,"1https://github.com/pytorch/vision
2https://github.com/huggingface/pytorch-image-models
3https://github.com/Harry24k/adversarial-attacks-pytorch"
THUS,0.9550748752079867,"not involved in the ensemble are treated as target models. ESMA still demonstrates outstanding
adversarial transferability."
THUS,0.956738768718802,"Table 5: Targeted transfer success rates. The source model is an ensemble of Res-50, Inc-v3, and
Inc-v4."
THUS,0.9584026622296173,"Attack
Src:Ensemble
→VGG19bn →Dense121 →Res152 →IncRes-v2 →ViT
AVG"
THUS,0.9600665557404326,"MIM
1.76%
3.40%
3.29%
1.07%
0.22%
1.95%
TIM
9.96%
16.13%
14.76%
8.27%
1.11%
10.05%
DIM
10.27%
16.31%
15.22%
8.20%
0.87%
10.17%
SI-NIM
1.16%
1.67%
1.49%
0.93%
0.13%
1.08%
Po-TI-Trip
14.22%
21.76%
18.69%
12.09%
1.62%
13.68%
SI-FGS2M
15.72%
24.84%
20.79%
12.20%
1.53%
15.02%
S2I-SI-TI-DIM
17.82%
21.07%
21.36%
16.38%
2.38%
15.80%
Logit
16.09%
27.56%
17.51%
36.96%
2.20%
20.06%
DTMI-Logit-SU
18.11%
30.16%
18.73%
38.64%
2.64%
21.71%
RAP-LS
48.98%
72.91%
70.67%
26.53%
4.22%
44.66%
HGN
55.10%
67.37%
63.54%
49.85%
26.78% 52.53%
TTP(129257.2)
60.02%
71.33%
67.44%
55.42%
30.04% 56.85%
ESMA(112303.5)
62.51%
74.11%
72.61%
62.33%
32.70% 60.85%"
THUS,0.961730449251248,"E.2
Models Trained on Different Distributions as Victim"
THUS,0.9633943427620633,"Given that our discussion is based on the condition that the source and target models are trained on
the same distribution data, to explore the transferability between models trained on training data
with different distribution, we conducted additional transfer attacks on two adversarially trained
models, Inc-v3-adv and IncRes-v2-ens. The results are reported in Table 6. ESMA’s performance is
slightly weakened but still competitive. Considering that this validation is not directly related to the
viewpoints, conclusions, and statements of our paper, we will further discuss the transferability on
target models trained on data with distribution shift in our future work."
THUS,0.9650582362728786,"Table 6: Targeted transfer success rates. ""Src"" indicates the source model."
THUS,0.9667221297836939,"Src
Target
MIM
TIM
DIM SI-NIM Po-TI-Trip SI-FGS2M S2I-SI-TI-DIM Logit DTMI-Logit-SU RAP-LS HGN
TTP
ESMA(Ours)"
THUS,0.9683860232945092,"Res50
→Inc-v3adv
0.11% 0.36% 0.18% 0.11%
0.22%
0.24%
0.76%
0.20%
0.27%
0.38%
2.12%
3.53%
3.23%
→IncRes-v2ens 0.11% 0.22% 0.18% 0.02%
0.38%
0.30%
0.89%
0.18%
0.22%
0.27%
2.70%
4.53%
3.98%"
THUS,0.9700499168053245,"Src
Target
MIM
TIM
DIM SI-NIM Po-TI-Trip SI-FGS2M S2I-SI-TI-DIM Logit DTMI-Logit-SU RAP-LS HGN
TTP
ESMA(Ours)"
THUS,0.9717138103161398,"Dense121
→Inc-v3adv
0.13% 0.13% 0.18% 0.11%
0.18%
0.22%
0.56%
0.20%
0.20%
0.22%
1.88%
3.18%
1.73%
→IncRes-v2ens 0.09% 0.18% 0.13% 0.11%
0.31%
0.20%
0.84%
0.22%
0.20%
0.18%
2.39%
3.56%
1.83%"
THUS,0.9733777038269551,"Src
Target
MIM
TIM
DIM SI-NIM Po-TI-Trip SI-FGS2M S2I-SI-TI-DIM Logit DTMI-Logit-SU RAP-LS HGN
TTP
ESMA(Ours)"
THUS,0.9750415973377704,"VGG19bn
→Inc-v3adv
0.13% 0.11% 0.16% 0.11%
0.11%
0.22%
0.16%
0.13%
0.13%
0.18%
0.20%
0.49%
0.31%
→IncRes-v2ens 0.04% 0.07% 0.09% 0.01%
0.11%
0.13%
0.22%
0.07%
0.07%
0.09%
0.53%
0.60%
0.22%"
THUS,0.9767054908485857,"Src
Target
MIM
TIM
DIM SI-NIM Po-TI-Trip SI-FGS2M S2I-SI-TI-DIM Logit DTMI-Logit-SU RAP-LS HGN
TTP
ESMA(Ours)"
THUS,0.978369384359401,"Res152
→Inc-v3adv
0.13% 0.20% 0.20% 0.04%
0.40%
0.27%
0.93%
0.24%
0.26%
0.24%
4.26%
5.96%
2.31%
→IncRes-v2ens 0.07% 0.36% 0.27% 0.07%
0.56%
0.33%
1.02%
0.27%
0.31%
0.38%
4.55%
5.96%
3.68%"
THUS,0.9800332778702163,"Src
Target
MIM
TIM
DIM SI-NIM Po-TI-Trip SI-FGS2M S2I-SI-TI-DIM Logit DTMI-Logit-SU RAP-LS HGN
TTP
ESMA(Ours)"
THUS,0.9816971713810316,"Ensemble
→Inc-v3adv
0.20% 0.60% 0.40% 0.13%
0.69%
0.60%
2.51%
0.53%
0.53%
0.58% 22.67% 24.44%
17.00%
→IncRes-v2ens 0.11% 0.96% 0.42% 0.09%
0.87%
0.66%
2.51%
0.20%
0.24%
0.58% 21.25% 23.33%
20.82%"
THUS,0.9833610648918469,"F
Fooling Google Lens"
THUS,0.9850249584026622,"We show that adversarial examples perturbated using ESMA can deceive Google Lens in image
search, as shown in Figure 10 and 11."
THUS,0.9866888519134775,"(a) French Bulldog
(b) Google Lens: French Bulldog"
THUS,0.9883527454242929,"(c) ESMA (Target:Goose)
(d) Google Lens: Goose"
THUS,0.9900166389351082,"(e) ESMA (Target:Hippo)
(f) Google Lens: Hippo"
THUS,0.9916805324459235,"Figure 10: The original image is of a French Bulldog, which can be intentionally perturbed to yield
different search results, such as a goose in the second row and a hippopotamus in the third row."
THUS,0.9933444259567388,"(a) Model T
(b) Google Lens: Model T"
THUS,0.9950083194675541,"(c) ESMA (Target:Cannon)
(d) Google Lens: Cannon"
THUS,0.9966722129783694,"(e) ESMA (Target:Grey Owl)
(f) Google Lens: Grey Owl"
THUS,0.9983361064891847,"Figure 11: The original image is of a Ford Model T, which can be intentionally perturbed to yield
different search results, such as a cannon in the second row and a gray owl in the third row."
