Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017605633802816902,"Recent works have shown that physics-inspired architectures allow the train-
ing of deep graph neural networks (GNNs) without oversmoothing. The role
of these physics is unclear, however, with successful examples of both re-
versible (e.g., Hamiltonian) and irreversible (e.g., diffusion) phenomena pro-
ducing comparable results despite diametrically opposed mechanisms, and fur-
ther complications arising due to empirical departures from mathematical theory.
This work presents a series of novel GNN architectures based upon structure-
preserving bracket-based dynamical systems, which are provably guaranteed to
either conserve energy or generate positive dissipation with increasing depth. It
is shown that the theoretically principled framework employed here allows for
inherently explainable constructions, which contextualize departures from theory
in current architectures and better elucidate the roles of reversibility and irre-
versibility in network performance. Code is available at the Github repository
https://github.com/natrask/BracketGraphs."
ABSTRACT,0.0035211267605633804,"∗K. Lee acknowledges the support from the U.S. National Science Foundation under grant CNS2210137.
†Sandia National Laboratories is a multimission laboratory managed and operated by National Technology &
Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S.
Department of Energy’s National Nuclear Security Administration under contract DE-NA0003525. This paper
describes objective technical results and analysis. Any subjective views or opinions that might be expressed
in the paper do not necessarily represent the views of the U.S. Department of Energy or the United States
Government. This article has been co-authored by an employee of National Technology & Engineering Solutions
of Sandia, LLC under Contract No. DE-NA0003525 with the U.S. Department of Energy (DOE). The employee
owns all right, title and interest in and to the article and is solely responsible for its contents. The United States
Government retains and the publisher, by accepting the article for publication, acknowledges that the United
States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce
the published form of this article or allow others to do so, for United States Government purposes. The DOE
will provide public access to these results of federally sponsored research in accordance with the DOE Public
Access Plan https://www.energy.gov/downloads/doe-public-access-plan. The work of N. Trask and A. Gruber is
supported by the U.S. Department of Energy, Office of Advanced Computing Research under the ""Scalable and
Efficient Algorithms - Causal Reasoning, Operators and Graphs"" (SEA-CROGS) project, the DoE Early Career
Research Program, and the John von Neumann fellowship at Sandia."
INTRODUCTION,0.00528169014084507,"1
Introduction"
INTRODUCTION,0.007042253521126761,"Graph neural networks (GNNs) have emerged as a powerful learning paradigm able to treat un-
structured data and extract “object-relation”/causal relationships while imparting inductive biases
which preserve invariances through the underlying graph topology [1, 2, 3, 4]. This framework
has proven effective for a wide range of both graph analytics and data-driven physics modeling
problems. Despite successes, GNNs have generally struggle to achieve the improved performance
with increasing depth typical of other architectures. Well-known pathologies, such as oversmoothing,
oversquashing, bottlenecks, and exploding/vanishing gradients yield deep GNNs which are either
unstable or lose performance as the number of layers increase [5, 6, 7, 8]."
INTRODUCTION,0.008802816901408451,"To combat this, a number of works build architectures which mimic physical processes to impart
desirable numerical properties. For example, some works claim that posing message passing as either
a diffusion process or reversible flow may promote stability or help retain information, respectively.
These present opposite ends of a spectrum between irreversible and reversible processes, which
either dissipate or retain information. It is unclear, however, what role (ir)reversibility plays [9]. One
could argue that dissipation entropically destroys information and could promote oversmoothing,
so should be avoided. Alternatively, in dynamical systems theory, dissipation is crucial to realize a
low-dimensional attractor, and thus dissipation may play an important role in realizing dimensionality
reduction. Moreover, recent work has shown that dissipative phenomena can actually sharpen
information as well as smooth it [10], although this is not often noticed in practice since typical
empirical tricks (batch norm, etc.) lead to a departure from the governing mathematical theory."
INTRODUCTION,0.01056338028169014,"In physics, Poisson brackets and their metriplectic/port-Hamiltonian generalization to dissipative
systems provide an abstract framework for studying conservation and entropy production in dynamical
systems. In this work, we construct four novel architectures which span the (ir)reversibility spectrum,
using geometric brackets as a means of parameterizing dynamics abstractly without empirically
assuming a physical model. This relies on an application of the data-driven exterior calculus (DDEC)
[11], which allows a reinterpretation of the message-passing and aggregation of graph attention
networks [12] as the fluxes and conservation balances of physics simulators [13], providing a simple
but powerful framework for mathematical analysis. In this context, we recast graph attention as an
inner-product on graph features, inducing graph derivative “building-blocks” which may be used to
build geometric brackets. In the process, we generalize classical graph attention [12] to higher-order
clique cochains (e.g., labels on edges and loops). The four architectures proposed here scale with
identical complexity to classical graph attention networks, and possess desirable properties that
have proven elusive in current architectures. On the reversible and irreversible end of the spectrum
we have Hamiltonian and Gradient networks. In the middle of the spectrum, Double Bracket and
Metriplectic architectures combine both reversibility and irreversibility, dissipating energy to either
the environment or an entropic variable, respectively, in a manner consistent with the second law of
thermodynamics. We summarize these brackets in Table 1, providing a diagram of their architecture
in Figure 1."
INTRODUCTION,0.01232394366197183,Primary contributions:
INTRODUCTION,0.014084507042253521,"Theoretical analysis of GAT in terms of exterior calculus. Using DDEC we establish a unified
framework for construction and analysis of message-passing graph attention networks, and provide an
extensive introductory primer to the theory in the appendices. In this setting, we show that with our
modified attention mechanism, GATs amount to a diffusion process for a special choice of activation
and weights."
INTRODUCTION,0.01584507042253521,"Generalized attention mechanism. Within this framework, we obtain a natural and flexible extension
of graph attention from nodal features to higher order cliques (e.g. edge features). We show attention
must have a symmetric numerator to be formally structure-preserving, and introduce a novel and
flexible graph attention mechanism parameterized in terms of learnable inner products on nodes and
edges."
INTRODUCTION,0.017605633802816902,"Novel structure-preserving extensions. We develop four GNN architectures based upon bracket-
based dynamical systems. In the metriplectic case, we obtain the first architecture with linear
complexity in the size of the graph while previous works are O(N 3)."
INTRODUCTION,0.01936619718309859,"Unified evaluation of dissipation. We use these architectures to systematically evaluate the role
of (ir)reversibility in the performance of deep GNNs. We observe best performance for partially"
INTRODUCTION,0.02112676056338028,"Formalism
Equation
Requirements
Completeness
Character
Hamiltonian
˙x = {x, E}
L∗= −L,
complete
conservative
Jacobi’s identity
Gradient
˙x = −[x, E]
M∗= M
incomplete
totally dissipative
Double Bracket
˙x = {x, E} + {{x, E}}
L∗= −L
incomplete
partially dissipative
Metriplectic
˙x = {x, E} + [x, S]
L∗= −L, M∗= M,
complete
partially dissipative
L∇S = M∇E = 0"
INTRODUCTION,0.022887323943661973,"Table 1: The abstract bracket formulations employed in this work. Here x represents a state variable,
while E = E(x), S = S(x) are energy and entropy functions. “Conservative” indicates purely
reversible motion, “totally dissipative” indicates purely irreversible motion, and “partially dissipative”
indicates motion which either dissipates E (in the double bracket case) or generates S (in the
metriplectic case)."
INTRODUCTION,0.02464788732394366,"dissipative systems, indicating that a combination of both reversibility and irreversibility are important.
Pure diffusion is the least performant across all benchmarks. For physics-based problems including
optimal control, there is a distinct improvement. All models provide near state-of-the-art performance
and marked improvements over black-box GAT/NODE networks."
PREVIOUS WORKS,0.02640845070422535,"2
Previous works"
PREVIOUS WORKS,0.028169014084507043,"Neural ODEs: Many works use neural networks to fit dynamics of the form ˙x = f(x, θ) to time
series data. Model calibration (e.g., UDE [14]), dictionary-based learning (e.g., SINDy [15]), and
neural ordinary differential equations (e.g., NODE [16]) pose a spectrum of inductive biases requiring
progressively less domain expertise. Structure-preservation provides a means of obtaining stable
training without requiring domain knowledge, ideally achieving the flexibility of NODE with the
robustness of UDE/SINDy. The current work learns dynamics on a graph while using a modern
NODE library to exploit the improved accuracy of high-order integrators [17, 18, 19]."
PREVIOUS WORKS,0.02992957746478873,"Structure-preserving dense networks: For dense networks, it is relatively straightforward to
parameterize reversible dynamics, see for example: Hamiltonian neural networks [20, 21, 22, 23],
Hamiltonian generative networks [24], Hamiltonian with Control (SymODEN) [25], Deep Lagrangian
networks [26] and Lagrangian neural networks [27]. Structure-preserving extensions to dissipative
systems are more challenging, particularly for metriplectic dynamics [28] which require a delicate
degeneracy condition to preserve discrete notions of the first and second laws of thermodynamics. For
dense networks such constructions are intensive, suffering from O(N 3) complexity in the number of
features [29, 30, 31]. In the graph setting we avoid this and achieve linear complexity by exploiting
exact sequence structure. Alternative dissipative frameworks include Dissipative SymODEN [32]
and port-Hamiltonian [33]. We choose to focus on metriplectic parameterizations due to their
broad potential impact in data-driven physics modeling, and ability to naturally treat fluctuations in
multiscale systems [34]."
PREVIOUS WORKS,0.03169014084507042,"Physics-informed vs structure-preserving: ""Physics-informed"" learning imposes physics by penalty,
adding a regularizer corresponding to a physics residual. The technique is simple to implement and has
been successfully applied to solve a range of PDEs [35], discover data-driven models to complement
first-principles simulators [36, 37, 38], learn metriplectic dynamics [39], and perform uncertainty
quantification [40, 41]. Penalization poses a multiobjective optimization problem, however, with
parameters weighting competing objectives inducing pathologies during training, often resulting in
physics being imposed to a coarse tolerance and qualitatively poor predictions [42, 43]. In contrast,
structure-preserving architectures exactly impose physics by construction via carefully designed
networks. Several works have shown that penalty-based approaches suffer in comparison, with
structure-preservation providing improved long term stability, extrapolation and physical realizability."
PREVIOUS WORKS,0.03345070422535211,"Structure-preserving graph networks: Several works use discretizations of specific PDEs to
combat oversmoothing or exploding/vanishing gradients, e.g. telegraph equations [44] or various
reaction-diffusion systems [45]. Several works develop Hamiltonian flows on graphs [46, 47]. For
metriplectic dynamics, [48] poses a penalty based formulation on graphs. We particularly focus on
GRAND, which poses graph learning as a diffusive process [49], using a similar exterior calculus
framework and interpreting attention as a diffusion coefficient. We show in Appendix A.5 that their"
PREVIOUS WORKS,0.035211267605633804,"Attention
Encoder"
PREVIOUS WORKS,0.03697183098591549,"Energy/Entropy
Gradients"
PREVIOUS WORKS,0.03873239436619718,Graph derivatives
PREVIOUS WORKS,0.040492957746478875,"Metric adjoints
Bracket matrices"
PREVIOUS WORKS,0.04225352112676056,Decoder
PREVIOUS WORKS,0.04401408450704225,"Input/Output
Trainable
Calculated"
PREVIOUS WORKS,0.045774647887323945,Integrator
PREVIOUS WORKS,0.04753521126760563,Figure 1: A diagrammatic illustration of the bracket-based architectures introduced in Section 4.
PREVIOUS WORKS,0.04929577464788732,"analysis fails to account for the asymmetry in the attention mechanism, leading to a departure from
the governing theory. To account for this, we introduce a modified attention mechanism which retains
interpretation as a part of diffusion PDE. In this purely irreversible case, it is of interest whether
adherence to the theory provides improved results, or GRAND’s success is driven by something other
than structure-preservation."
THEORY AND FUNDAMENTALS,0.051056338028169015,"3
Theory and fundamentals"
THEORY AND FUNDAMENTALS,0.0528169014084507,"Here we introduce the two essential ingredients to our approach: bracket-based dynamical systems for
neural differential equations, and the data-driven exterior calculus which enables their construction.
A thorough introduction to this material is provided in Appendices A.1, A.3, and A.2."
THEORY AND FUNDAMENTALS,0.05457746478873239,"Bracket-based dynamics: Originally introduced as an extension of Hamiltonian/Lagrangian dy-
namics to include dissipation [50], bracket formulations are used to inform a dynamical system
with certain structural properties, e.g., time-reversibility, invariant differential forms, or property
preservation. Even without dissipation, bracket formulations may compactly describe dynamics while
preserving core mathematical properties, making them ideal for designing neural architectures."
THEORY AND FUNDAMENTALS,0.056338028169014086,"Bracket formulations are usually specified via some combination of reversible brackets {F, G} =
⟨∇F, L∇G⟩and irreversible brackets [F, G] = ⟨∇F, M∇G⟩, {{F, G}} =

∇F, L2∇G

for poten-
tially state-dependent operators L∗= −L and M∗= M. The particular brackets which are used
in the present network architectures are summarized in Table 1. Note that complete systems are
the dynamical extensions of isolated thermodynamical systems: they conserve energy and produce
entropy, with nothing lost to the ambient environment. Conversely, incomplete systems do not account
for any lost energy: they only require that it vanish in a prescribed way. The choice of completeness
is an application-dependent modeling assumption."
THEORY AND FUNDAMENTALS,0.058098591549295774,"Exterior calculus: In the combinatorial Hodge theory [51], an oriented graph G = {V, E} carries
sets of k-cliques, denoted Gk, which are collections of ordered subgraphs generated by (k + 1) nodes.
This induces natural exterior derivative operators dk : Ωk →Ωk+1, acting on the spaces of functions
on Gk, which are the signed incidence matrices between k-cliques and (k + 1)-cliques. An explicit
representation of these derivatives is given in Appendix A.1, from which it is easy to check the exact
sequence property dk+1 ◦dk = 0 for any k. This yields a discrete de Rham complex on the graph G
(Figure 2). Moreover, given a choice of inner product (say, ℓ2) on Ωk, there is an obvious dual de
Rham complex which comes directly from adjointness. In particular, one can define dual derivatives
d∗
k : Ωk+1 →Ωk via the equality"
THEORY AND FUNDAMENTALS,0.05985915492957746,"⟨dkf, g⟩k+1 = ⟨f, d∗
kg⟩k ,"
THEORY AND FUNDAMENTALS,0.061619718309859156,"from which nontrivial results such as the Hodge decomposition, Poincaré inequality, and coerciv-
ity/invertibility of the Hodge Laplacian ∆k = d∗
kdk + dk−1d∗
k−1 follow (see e.g. [11]). Using
the derivatives dk, d∗
k, it is possible to build compatible discretizations of PDEs on G which are
guaranteed to preserve exactness properties such as, e.g., d1 ◦d0 = curl ◦grad = 0."
THEORY AND FUNDAMENTALS,0.06338028169014084,"The choice of inner product ⟨·, ·⟩k thus induces a definition of the dual derivatives d∗
k. In the graph
setting [52], one typically selects the ℓ2 inner product, obtaining the adjoints of the signed incidence
matrices as d∗
k = d⊺
k. By instead working with the modified inner product (v, w) = v⊺Akw for a
machine-learnable Ak, we obtain d∗
k = A−1
k d⊺
kAk+1 (see Appendix A.3). This parameterization"
THEORY AND FUNDAMENTALS,0.06514084507042253,"Ω0
Ω1
Ω2
· · ·
Ωk"
THEORY AND FUNDAMENTALS,0.06690140845070422,"Ω0
Ω1
Ω2
· · ·
Ωk d0 d⊺
0 d1 d⊺
1 d2 d⊺
2 dk−1"
THEORY AND FUNDAMENTALS,0.06866197183098592,"d⊺
k−1
A0
A1 d∗
0 A2"
THEORY AND FUNDAMENTALS,0.07042253521126761,"d∗
1
d∗
2
d∗
k−1 Ak"
THEORY AND FUNDAMENTALS,0.0721830985915493,"Figure 2: A commutative diagram illustrating the relationship between the graph derivatives dk, their
ℓ2 adjoints d⊺
k, and the learnable adjoints d∗
k. These operators form a de Rham complex due to the
exact sequence property di+1 ◦di = d⊺
i ◦d⊺
i+1 = d∗
i ◦d∗
i+1 = 0. We show that the learnable Ak may
encode attention mechanisms, without impacting the preservation of exact sequence structure."
THEORY AND FUNDAMENTALS,0.07394366197183098,"inherits the exact sequence property from the graph topology encoded in dk while allowing for
incorporation of geometric information from data. This leads directly to the following result, which
holds for any (potentially feature-dependent) symmetric positive definite matrix Ak."
THEORY AND FUNDAMENTALS,0.07570422535211267,"Theorem 3.1. The dual derivatives d∗
k : Ωk+1 →Ωk adjoint to dk : Ωk →Ωk+1 with respect to the
learnable inner products Ak : Ωk →Ωk satisfy an exact sequence property."
THEORY AND FUNDAMENTALS,0.07746478873239436,"Proof.
d∗
k−1d∗
k = A−1
k−1d⊺
k−1AkA−1
k d⊺
kAk+1 = A−1
k−1 (dkdk−1)⊺Ak+1 = 0."
THEORY AND FUNDAMENTALS,0.07922535211267606,"As will be shown in Section 4, by encoding graph attention into the Ak, we may exploit the exact
sequence property to obtain symmetric positive definite diffusion operators, as well as conduct the
cancellations necessary to enforce degeneracy conditions necessary for metriplectic dynamics."
THEORY AND FUNDAMENTALS,0.08098591549295775,"For a thorough review of DDEC, we direct readers to Appendix A.1 and [11]. For exterior calculus in
topological data analysis see [52], and an overview in the context of PDEs see [53, 54]."
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.08274647887323944,"4
Structure-preserving bracket parameterizations"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.08450704225352113,"We next summarize properties of the bracket dynamics introduced in Section 3 and displayed in
Table 1, postponing details and rigorous discussion to Appendices A.3 and A.6. Letting x = (q, p)
denote node-edge feature pairs, the following operators will be used to generate our brackets."
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.08626760563380281,"L =

0
−d∗
0
d0
0"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.0880281690140845,"
,
G =

∆0
0
0
∆1"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.0897887323943662,"
=

d∗
0d0
0
0
d∗
1d1 + d0d∗
0"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.09154929577464789,"
,
M =

0
0
0
A1d∗
1d1A1 
."
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.09330985915492958,"As mentioned before, the inner products A0, A1, A2 on Ωk which induce the dual derivatives d∗
0, d∗
1,
are chosen in such a way that their combination generalizes a graph attention mechanism. The precise
details of this construction are given below, and its relationship to the standard GAT network from
[12] is shown in Appendix A.5. Notice that L∗= −L, while G∗= G, M∗= M are positive semi-
definite with respect to the block-diagonal inner product (·, ·) defined by A = diag (A0, A1) (details
are provided in Appendix A.6). Therefore, L generates purely reversible (Hamiltonian) dynamics and
G, M generate irreversible (dissipative) ones. Additionally, note that state-dependence in L, M, G
enters only through the adjoint differential operators, meaning that any structural properties induced
by the topology of the graph G (such as the exact sequence property mentioned in Theorem 3.1) are
automatically preserved.
Remark 4.1. Strictly speaking, L is guaranteed to be a truly Hamiltonian system only when d∗
0 is
state-independent, since it may otherwise fail to satisfy Jacobi’s identity. On the other hand, energy
conservation is always guaranteed due to the fact that L is skew-adjoint."
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.09507042253521127,"In addition to the bracket matrices L, M, G, it is necessary to have access to energy and entropy
functions E, S and their associated functional derivatives with respect to the inner product on Ω0 ⊕Ω1
defined by A. For the Hamiltonian, gradient, and double brackets, E is chosen simply as the “total
kinetic energy”"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.09683098591549295,"E(q, p) = 1 2"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.09859154929577464,"
|q|2 + |p|2
= 1 2 X"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.10035211267605634,"i∈V
|qi|2 + 1 2 X"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.10211267605633803,"α∈E
|pα|2 ,"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.10387323943661972,"whose A-gradient (computed in Appendix A.6) is just ∇E(q, p) =
 
A−1
0 q
A−1
1 p
⊺. Since the
metriplectic bracket uses parameterizations of E, S which are more involved, discussion of this case
is deferred to later in this Section."
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.1056338028169014,"Attention as learnable inner product: Before describing the dynamics, it remains to discuss
how the matrices Ai, 0 ≤i ≤2, are computed in practice, and how they relate to the idea of
graph attention. Recall that if nV > 0 denotes the nodal feature dimension, a graph attention
mechanism takes the form a(qi, qj) = f (˜aij) / P
j f (˜aij) for some differentiable pre-attention
function ˜a : nV × nV →R (e.g., for scaled dot product [55]) one typically represents a(qi, qj)
as a softmax, so that f = exp(q)). This suggests a decomposition a(qi, qj) = A−1
0 A1 where
A0 = (a0,ii) is diagonal on nodes and A1 = (a1,ij) is diagonal on edges,"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.1073943661971831,"a0,ii =
X"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.10915492957746478,"j∈N(i)
f (˜a (qi, qj)) ,
a1,ij = f (˜a (qi, qj)) ."
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.11091549295774648,"Treating the numerator and denominator of the standard attention mechanism separately in A0, A1
allows for a flexible and theoretically sound incorporation of graph attention directly into the adjoint
differential operators on G. In particular, if A1 is symmetric with respect to edge-orientation and p is
an edge feature which is antisymmetric, it follows that"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.11267605633802817,"(d∗
0p)i =
 
A−1
0 d⊺
0A1p
 i =
X"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.11443661971830986,"j∈N(i)
a (qi, qj) pji,"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.11619718309859155,"which is just graph attention combined with edge aggregation. This makes it possible to give the
following informal statement regarding graph attention networks which is explained and proven in
Appendix A.5.
Remark 4.2. The GAT layer from [12] is almost the forward Euler discretization of a metric heat
equation."
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.11795774647887323,"The “almost” appearing here has to do with the fact that (1) the attentional numerator f (˜a(qi, qj))
is generally asymmetric in i, j, and is therefore symmetrized by the divergence operator d⊺
0, (2) the
activation function between layers is not included, and (3) learnable weight matrices Wk in GAT are
set to the identity.
Remark 4.3. The interpretation of graph attention as a combination of learnable inner products
admits a direct generalization to higher-order cliques, which is discussed in Appendix A.4."
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.11971830985915492,"Hamiltonian case: A purely conservative system is generated by solving ˙x = L(x)∇E(x), or

˙q
˙p"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.12147887323943662,"
=

0
−d∗
0
d0
0"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.12323943661971831," 
A−1
0
0
0
A−1
1"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.125," 
q
p"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.1267605633802817,"
=

−d∗
0A−1
1 p
d0A−1
0 q 
."
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.12852112676056338,"This is a noncanonical Hamiltonian system which generates a purely reversible flow. In particular, it
can be shown that"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.13028169014084506,"˙E(x) = ( ˙x, ∇E(x)) = (L(x)∇E(x), ∇E(x)) = −(∇E(x), L(x)∇E(x)) = 0,"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.13204225352112675,so that energy is conserved due to the skew-adjointness of L.
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.13380281690140844,"Gradient case: On the opposite end of the spectrum are generalized gradient flows, which are totally
dissipative. Consider solving ˙x = −G(x)∇E(x), or

˙q
˙p"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.13556338028169015,"
= −

∆0
0
0
∆1"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.13732394366197184," 
A−1
0
0
0
A−1
1"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.13908450704225353," 
q
p"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.14084507042253522,"
= −

∆0A−1
0 q
∆1A−1
1 p 
."
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.1426056338028169,"This system is a metric diffusion process on nodes and edges separately. Moreover, it corresponds to
a generalized gradient flow, since"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.1443661971830986,"˙E(x) = ( ˙x, ∇E(x)) = −(G(x)∇E(x), ∇E(x)) = −|∇E(x)|2
G ≤0,"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.14612676056338028,"due to the self-adjoint and positive semi-definite nature of G.
Remark 4.4. The architecture in GRAND [49] is almost a gradient flow, however the pre-attention
mechanism lacks the requisite symmetry to formally induce a valid inner product."
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.14788732394366197,"Double bracket case: Another useful formulation for incomplete systems is the so-called double-
bracket formalism. Consider solving ˙x = L∇E + L2∇E, or

˙q
˙p"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.14964788732394366,"
=

0
−d∗
0
d0
0"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.15140845070422534," 
A−1
0 q
A−1
1 p"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.15316901408450703,"
+

−d∗
0d0
0
0
−d0d∗
0"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.15492957746478872," 
A−1
0 q
A−1
1 p"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.15669014084507044,"
=

−∆0A−1
0 q −d∗
0A−1
1 p
d0A−1
0 q −d0d∗
0A−1
1 p 
."
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.15845070422535212,"This provides a dissipative relationship which preserves the Casimirs of the Poisson bracket generated
by L, since L∇C = 0 implies L2∇C = 0. In particular, it follows that"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.1602112676056338,"˙E(x) = ( ˙x, ∇E(x)) =
 
L(x)∇E(x) + L2(x)∇E(x), ∇E(x)

= 0 −|L(x)∇E(x)|2 ≤0,"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.1619718309859155,"since L is skew-adjoint and therefore L2 is self-adjoint.
Remark 4.5. It is interesting to note that the matrix L is essentially a Dirac operator (square root
of the Hodge Laplacian ∆= (d + d∗)2) restricted to cliques of degree at most 1. However, here
L2 = −∆, so that L is in some sense “pure imaginary”."
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.1637323943661972,"Metriplectic case: Metriplectic systems are expressible as ˙x = L∇E + M∇S where E, S are
energy resp. entropy functions which satisfy the degeneracy conditions L∇S = M∇E = 0. One
way of setting this up in the present case is to define the energy and entropy functions"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.16549295774647887,"E(q, p) = fE (s(q)) + gE (s (d0d⊺
0p)) ,"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.16725352112676056,"S(q, p) = gS (s (d⊺
1d1p)) ,"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.16901408450704225,"where s is sum aggregation over nodes resp. edges, fE : RnV →R acts on node features, and
gE, gS : RnE →R act on edge features. Denoting the “all ones” vector (of variable length) by 1, it
is shown in Appendix A.6 that the A-gradients of energy and entropy can be computed as"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.17077464788732394,"∇E(x) =

A−1
0 1 ⊗∇fE (h(q))
A−1
1 d0d⊺
01 ⊗∇gE (h (d0d⊺
0p))"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.17253521126760563,"
,
∇S(x) =

0
A−1
1 d⊺
1d11 ⊗∇gS (h (d⊺
1d1p)) 
."
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.1742957746478873,"Similarly, it is shown in Appendix A.6 that the degeneracy conditions L∇S = M∇E = 0 are
satisfied by construction. Therefore, the governing dynamical system becomes

˙q
˙p"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.176056338028169,"
= L∇E + M∇S =

−A−1
0 d⊺
0d0d⊺
01 ⊗∇gE (s (d0d⊺
0p))
d0A−1
0 1 ⊗∇fE (s(q)) + A1d∗
1d1d⊺
1d11 ⊗∇gS (s (d⊺
1d1p)) 
."
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.17781690140845072,"With this, it follows that the system obeys a version of the first and second laws of thermodynamics,"
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.1795774647887324,"˙E(x) = ( ˙x, ∇E(x)) = (L∇E(x), ∇E(x)) + (M∇S(x), ∇E(x)) = (∇S(x), M∇E(x)) = 0,
˙S(x) = ( ˙x, ∇S(x)) = (L∇E(x), ∇S(x)) + (M∇S(x), ∇S(x)) = 0 + |∇S(x)|2
M ≥0."
STRUCTURE-PRESERVING BRACKET PARAMETERIZATIONS,0.1813380281690141,"Remark 4.6. As seen in the increased complexity of this formulation, enforcing the degeneracy
conditions necessary for metriplectic structure is nontrivial. This is accomplished presently via an
application of the exact sequence property in Theorem 3.1, which we derive in Appendix A.6.
Remark 4.7. It is worth mentioning that, similar to the other architectures presented in this Section,
the metriplectic network proposed here exhibits linear O(N) scaling in the graph size. This is in
notable contrast to [29, 31] which scale as O(N 3)."
EXPERIMENTS,0.18309859154929578,"5
Experiments"
EXPERIMENTS,0.18485915492957747,"This section reports results on experiments designed to probe the influence of bracket structure on
trajectory prediction and nodal feature classification. Additional experimental details can be found in
Appendix B. In each Table, orange indicates the best result by our models, and blue indicates the best
of those compared. We consider both physical systems, where the role of structure preservation is
explicit, as well as graph-analytic problems."
DAMPED DOUBLE PENDULUM,0.18661971830985916,"5.1
Damped double pendulum"
DAMPED DOUBLE PENDULUM,0.18838028169014084,"As a first experiment, consider applying one of these architectures to reproduce the trajectory of a
double pendulum with a damping force proportional to the angular momenta of the pendulum masses
(see Appendix B.1 for details)."
DAMPED DOUBLE PENDULUM,0.19014084507042253,"Double pendulum
MAE q
MAE p
Total MAE
NODE
0.0240 ± 0.015
0.0299 ± 0.0091
0.0269 ± 0.012
NODE+AE
0.0532 ± 0.029
0.0671 ± 0.043
0.0602 ± 0.035
Hamiltonian
0.00368 ± 0.0015
0.00402 ± 0.0015
0.00369 ± 0.0013
Gradient
0.00762 ± 0.0023
0.0339 ± 0.012
0.0208 ± 0.0067
Double Bracket
0.00584 ± 0.0013
0.0183 ± 0.0071
0.0120 ± 0.0037
Metriplectic
0.00364 ± 0.00064
0.00553 ± 0.00029
0.00459 ± 0.00020"
DAMPED DOUBLE PENDULUM,0.19190140845070422,"Table 2: Mean absolute errors (MAEs) of the network predictions in the damped double pendulum
case, reported as avg±stdev over 5 runs."
DAMPED DOUBLE PENDULUM,0.1936619718309859,"Since this system is metriplectic when expressed in position-momentum-entropy coordinates (c.f.
[56]), it is useful to see if any of the brackets from Section 4 can adequately capture these dynamics
without an entropic variable. The results of applying the architectures of Section 4 to reproduce a
trajectory of five periods are displayed in Table 2, alongside comparisons with a black-box NODE
network and a latent NODE with feature encoder/decoder. While each network is capable of producing
a small mean absolute error, it is clear that the metriplectic and Hamiltonian networks produce the
most accurate trajectories. It is remarkable both that the Hamiltonian bracket does so well here and
that the gradient bracket does so poorly, being that the damped double pendulum system is quite
dissipative. On the other hand, it is unlikely to be only the feature encoder/decoder leading to good
performance here, as both the NODE and NODE+AE architectures perform worse on this task by
about one order of magnitude."
MUJOCO DYNAMICS,0.1954225352112676,"5.2
MuJoCo Dynamics"
MUJOCO DYNAMICS,0.19718309859154928,"Next we test the proposed models on more complex physical systems that are generated by the
Multi-Joint dynamics with Contact (MuJoCo) physics simulator [57]. We consider the modified
versions of Open AI Gym environments [23]: HalfCheetah, Hopper, and Swimmer."
MUJOCO DYNAMICS,0.198943661971831,"We represent an object in an environment as a fully-connected graph, where a node corresponds to a
body part of the object and, thus, the nodal feature qi corresponds to a position of a body part or an
angle of a joint.3 As the edge features, a pair of nodal velocities pα = (vsrc(α), vdst(α)) are provided,
where vsrc(α) and vdst(α) denote velocities of the source and destination nodes connected to the edge."
MUJOCO DYNAMICS,0.2007042253521127,"Since the MuJoCo environments contain an actor applying controls, additional control input is
accounted for with an additive forcing term which is parameterized by a multi-layer perceptron and
introduced into the bracket-based dynamics models. See Appendix B.2 for additional experimental
details. The problem therefore consists of finding an optimal control MLP, and we evaluate the
improvement which comes from representing the physics surrogate with bracket dynamics over
NODE."
MUJOCO DYNAMICS,0.20246478873239437,"All models are trained via minimizing the MSE between the predicted positions ˜q and the ground truth
positions q and are tested on an unseen test set. Table 3 reports the errors of network predictions on
the test set measured in the relative ℓ2 norm, ∥q −˜q∥2/∥q∥2∥˜q∥2. Similar to the double pendulum
experiments, all models are able to produce accurate predictions with around or less than 10% errors.
While the gradient bracket makes little to no improvements over NODEs, the Hamiltonian, double,
and metriplectic brackets produce more accurate predictions. Interestingly, the Hamiltonian bracket
performs the best in this case as well, meaning that any dissipation present is effectively compensated
for by the autoencoder which transforms the features."
NODE CLASSIFICATION,0.20422535211267606,"5.3
Node classification"
NODE CLASSIFICATION,0.20598591549295775,"Moving beyond physics-based examples, it remains to see how bracket-based architectures perform on
“black-box” node classification problems. Table 4 and Table 5 present results on common benchmark
problems including the citation networks Cora [58], Citeseer [59], and Pubmed [60], as well as
the coauthor graph, CoauthorCS [61], and the Amazon co-purchasing graphs, Computer and Photo
[62]. For comparison, we report results on a standard GAT [12], a neural graph differential equation"
NODE CLASSIFICATION,0.20774647887323944,"3Results of an experiment with an alternative embedding (i.e., qi = (qi, vi)) are reported in Appendix B.2.2."
NODE CLASSIFICATION,0.20950704225352113,"Dataset
HalfCheetah
Hopper
Swimmer
NODE+AE
0.106 ± 0.0011
0.0780 ± 0.0021
0.0297 ± 0.0036
Hamiltonian
0.0566 ± 0.013
0.0279 ± 0.0019
0.0122 ± 0.00044
Gradient
0.105 ± 0.0076
0.0848 ± 0.0011
0.0290 ± 0.0011
Double Bracket
0.0621 ± 0.0096
0.0297 ± 0.0048
0.0128 ± 0.00070
Metriplectic
0.105 ± 0.0091
0.0398 ± 0.0057
0.0179 ± 0.00059"
NODE CLASSIFICATION,0.2112676056338028,"Table 3: Relative error of network predictions for the MuJoCo environment on the test set, reported
as avg±stdev over 4 runs."
NODE CLASSIFICATION,0.2130281690140845,"Planetoid splits
CORA
CiteSeer
PubMed
GAT
82.8 ± 0.5
69.5 ± 0.9
79.0 ± 0.5
GDE
83.8 ± 0.5
72.5 ± 0.5
79.9 ± 0.3
GRAND-nl
83.6 ± 0.5
70.8 ± 1.1
79.7 ± 0.3"
NODE CLASSIFICATION,0.2147887323943662,"Hamiltonian
77.2 ± 0.7
73.0 ± 1.2
78.5 ± 0.3
Gradient
79.9 ± 0.7
71.8 ± 1.4
78.6 ± 0.7
Double Bracket
82.6 ± 0.9
74.2 ± 1.4
79.6 ± 0.6
Metriplectic
57.4 ± 1.0
60.5 ± 1.1
69.8 ± 0.7"
NODE CLASSIFICATION,0.21654929577464788,"Table 4: Test accuracy and standard deviations (averaged over 20 randomly initialized runs) using the
original Planetoid train-valid-test splits. Comparisons use the numbers reported in [49]."
NODE CLASSIFICATION,0.21830985915492956,"architecture (GDE) [63], and the nonlinear GRAND architecture (GRAND-nl) from [49] which is
closest to ours. Since our experimental setting is similar to that of [49], the numbers reported for
GAT, GDE, and GRAND-nl are taken directly from this paper. Note that, despite the similar O(N)
scaling in the metriplectic architecture, the high dimension of the node and edge features on the latter
three datasets led to trainable E, S functions which exhausted the memory on our available machines,
and therefore results are not reported for these cases. A full description of experimental details is
provided in Appendix B.3."
NODE CLASSIFICATION,0.22007042253521128,"Remark 5.1. To highlight the effect of bracket structure on network performance, only minimal
modifications are employed during network training. In particular, we do not include any additional
regularization, positional encoding, graph rewiring, extraction of connected components, extra terms
on the right-hand side, or early stopping. While it is likely that better classification performance
could be achieved with some of these modifications included, it becomes very difficult to isolate the
effect of structure-preservation. A complete list of tunable hyperparameters is given in Appendix B.3."
NODE CLASSIFICATION,0.22183098591549297,"The results show different behavior produced by each bracket architecture. It is empirically clear
that there is some value in full or partial reversibility, since the Hamiltonian and double bracket
architectures both perform better than the corresponding gradient architecture on datasets such as
Computer and Photo. Moreover, it appears that the partially reversible double bracket performs the
best of the bracket architectures in every case, which is consistent with the idea that both reversible
and irreversible dynamics are critical for capturing the behavior of general dynamical systems.
Interestingly, the metriplectic bracket performs worse on these tasks by a large margin. We conjecture
this architecture may be harder to train for larger problems despite its O(N) complexity in the graph
size, suggesting that more sophisticated training strategies may be required for large problems."
CONCLUSION,0.22359154929577466,"6
Conclusion"
CONCLUSION,0.22535211267605634,"This work presents a unified theoretical framework for analysis and construction of graph attention
networks. The exact sequence property of graph derivatives and coercivity of Hodge Laplacians
which follow from the theory allow the construction of four structure-preserving brackets, which we
use to evaluate the role of irreversibility in both data-driven physics simulators and graph analytics
problems. In all contexts, the pure diffusion bracket performed most poorly, with mixed results
between purely reversible and partially dissipative brackets."
CONCLUSION,0.22711267605633803,"Random splits
CORA
CiteSeer
PubMed
Coauthor CS
Computer
Photo
GAT
81.8 ± 1.3
71.4 ± 1.9
78.7 ± 2.3
90.5 ± 0.6
78.0 ± 19.0
85.7 ± 20.3
GDE
78.7 ± 2.2
71.8 ± 1.1
73.9 ± 3.7
91.6 ± 0.1
82.9 ± 0.6
92.4 ± 2.0
GRAND-nl
82.3 ± 1.6
70.9 ± 1.0
77.5 ± 1.8
92.4 ± 0.3
82.4 ± 2.1
92.4 ± 0.8"
CONCLUSION,0.22887323943661972,"Hamiltonian
76.2 ± 2.1
72.2 ± 1.9
76.8 ± 1.1
92.0 ± 0.2
84.0 ± 1.0
91.8 ± 0.2
Gradient
81.3 ± 1.2
72.1 ± 1.7
77.2 ± 2.1
92.2 ± 0.3
78.1 ± 1.2
88.2 ± 0.6
Double Bracket
83.0 ± 1.1
74.2 ± 2.5
78.2 ± 2.0
92.5 ± 0.2
84.8 ± 0.5
92.4 ± 0.3
Metriplectic
59.6 ± 2.0
63.1 ± 2.4
69.8 ± 2.1
-
-
-"
CONCLUSION,0.2306338028169014,"Table 5: Test accuracy and standard deviations averaged over 20 runs with random 80/10/10
train/val/test splits. Comparisons use the numbers reported in [49]."
CONCLUSION,0.2323943661971831,"The linear scaling achieved by the metriplectic brackets has a potential major impact for data-driven
physics modeling. Metriplectic systems emerge naturally when coarse-graining multiscale systems.
With increasing interest in using ML to construct digital twins, fast data-driven surrogates for complex
multi-physics acting over multiple scales will become crucial. In this setting the stability encoded by
metriplectic dynamics translates to robust surrogates, with linear complexity suggesting the possibility
of scaling up to millions of degrees of freedom."
CONCLUSION,0.23415492957746478,"Limitations:
All analysis holds under the assumption of modified attention mechanisms which
allow interpretation of GAT networks as diffusion processes; readers should take care that the
analysis is for a non-standard attention. Secondly, for all brackets we did not introduce empirical
modifications (e.g. regularization, forcing, etc) to optimize performance so that we could study the
role of (ir)reversibility in isolation. With this in mind, one may be able to add “tricks” to e.g. obtain a
diffusion architecture which outperforms those presented here. Finally, note that the use of a feature
autoencoder in the bracket architectures means that structure is enforced in the transformed space.
This allows for applicability to more general systems, and can be easily removed when appropriate
features are known."
CONCLUSION,0.23591549295774647,"Broader impacts:
The work performed here is strictly foundational mathematics and is intended to
improve the performance of GNNs in the context of graph analysis and data-driven physics modeling.
Subsequent application of the theory may have societal impact, but the current work anticipated to
improve the performance of machine learning in graph settings only at a foundational level."
REFERENCES,0.23767605633802816,References
REFERENCES,0.23943661971830985,"[1] Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains.
In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2, pages
729–734. IEEE, 2005."
REFERENCES,0.24119718309859156,"[2] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,
Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications.
AI open, 1:57–81, 2020."
REFERENCES,0.24295774647887325,"[3] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
Advances in neural information processing systems, 30, 2017."
REFERENCES,0.24471830985915494,"[4] William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods and
applications. arXiv preprint arXiv:1709.05584, 2017."
REFERENCES,0.24647887323943662,"[5] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-smoothing
problem for graph neural networks from the topological view. In Proceedings of the AAAI conference on
artificial intelligence, volume 34, pages 3438–3445, 2020."
REFERENCES,0.2482394366197183,"[6] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veliˇckovi´c. Geometric deep learning: Grids,
groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021."
REFERENCES,0.25,"[7] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graph
neural networks with differentiable group normalization. Advances in neural information processing
systems, 33:4917–4928, 2020."
REFERENCES,0.2517605633802817,"[8] Chen Cai and Yusu Wang.
A note on over-smoothing for graph neural networks.
arXiv preprint
arXiv:2006.13318, 2020."
REFERENCES,0.2535211267605634,"[9] Yifei Wang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. Dissecting the diffusion process in linear
graph convolutional networks. Advances in Neural Information Processing Systems, 34:5758–5769, 2021."
REFERENCES,0.25528169014084506,"[10] Francesco Di Giovanni, James Rowbottom, Benjamin P. Chamberlain, Thomas Markovich, and Michael M.
Bronstein. Understanding convolution on graphs via energies, 2023."
REFERENCES,0.25704225352112675,"[11] Nathaniel Trask, Andy Huang, and Xiaozhe Hu. Enforcing exact physics in scientific machine learning: a
data-driven exterior calculus on graphs. arXiv preprint arXiv:2012.11799, 2020."
REFERENCES,0.25880281690140844,"[12] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio.
Graph attention networks. arXiv preprint arXiv:1710.10903, 2017."
REFERENCES,0.2605633802816901,"[13] Khemraj Shukla, Mengjia Xu, Nathaniel Trask, and George E Karniadakis. Scalable algorithms for
physics-informed neural and graph networks. Data-Centric Engineering, 3:e24, 2022."
REFERENCES,0.2623239436619718,"[14] Christopher Rackauckas, Yingbo Ma, Julius Martensen, Collin Warner, Kirill Zubov, Rohit Supekar,
Dominic Skinner, Ali Ramadhan, and Alan Edelman. Universal differential equations for scientific
machine learning. arXiv preprint arXiv:2001.04385, 2020."
REFERENCES,0.2640845070422535,"[15] Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from data by
sparse identification of nonlinear dynamical systems. Proceedings of the national academy of sciences,
113(15):3932–3937, 2016."
REFERENCES,0.2658450704225352,"[16] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential
equations. In Proceedings of the 32nd International Conference on Neural Information Processing Systems,
pages 6572–6583, 2018."
REFERENCES,0.2676056338028169,"[17] Michael Poli, Stefano Massaroli, Atsushi Yamashita, Hajime Asama, Jinkyoo Park, and Stefano Ermon.
Torchdyn: Implicit models and neural numerical methods in pytorch."
REFERENCES,0.26936619718309857,"[18] Louis-Pascal Xhonneux, Meng Qu, and Jian Tang. Continuous graph neural networks. In International
Conference on Machine Learning, pages 10432–10441. PMLR, 2020."
REFERENCES,0.2711267605633803,"[19] Fangda Gu, Heng Chang, Wenwu Zhu, Somayeh Sojoudi, and Laurent El Ghaoui. Implicit graph neural
networks. Advances in Neural Information Processing Systems, 33:11984–11995, 2020."
REFERENCES,0.272887323943662,"[20] Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural
Information Processing Systems, volume 32. Curran Associates, Inc., 2019."
REFERENCES,0.2746478873239437,"[21] Marc Finzi, Ke Alexander Wang, and Andrew G Wilson. Simplifying hamiltonian and lagrangian neural
networks via explicit constraints. Advances in neural information processing systems, 33:13880–13889,
2020."
REFERENCES,0.2764084507042254,"[22] Renyi Chen and Molei Tao. Data-driven prediction of general hamiltonian dynamics via learning exactly-
symplectic maps. In International Conference on Machine Learning, pages 1717–1727. PMLR, 2021."
REFERENCES,0.27816901408450706,"[23] Nate Gruver, Marc Anton Finzi, Samuel Don Stanton, and Andrew Gordon Wilson. Deconstructing the
inductive biases of hamiltonian neural networks. In International Conference on Learning Representations."
REFERENCES,0.27992957746478875,"[24] Peter Toth, Danilo J Rezende, Andrew Jaegle, Sébastien Racanière, Aleksandar Botev, and Irina Higgins.
Hamiltonian generative networks. In International Conference on Learning Representations, 2019."
REFERENCES,0.28169014084507044,"[25] Yaofeng Desmond Zhong, Biswadip Dey, and Amit Chakraborty.
Symplectic ODE-Net: Learning
Hamiltonian dynamics with control. In International Conference on Learning Representations."
REFERENCES,0.2834507042253521,"[26] Michael Lutter, Christian Ritter, and Jan Peters. Deep lagrangian networks: Using physics as model prior
for deep learning. In International Conference on Learning Representations, 2018."
REFERENCES,0.2852112676056338,"[27] Miles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel, and Shirley Ho. La-
grangian neural networks. In ICLR 2020 Workshop on Integration of Deep Neural Models and Differential
Equations, 2020."
REFERENCES,0.2869718309859155,"[28] Partha Guha. Metriplectic structure, leibniz dynamics and dissipative systems. Journal of Mathematical
Analysis and Applications, 326(1):121–136, 2007."
REFERENCES,0.2887323943661972,"[29] Kookjin Lee, Nathaniel Trask, and Panos Stinis. Machine learning structure preserving brackets for
forecasting irreversible processes. Advances in Neural Information Processing Systems, 34:5696–5707,
2021."
REFERENCES,0.2904929577464789,"[30] Kookjin Lee, Nathaniel Trask, and Panos Stinis. Structure-preserving sparse identification of nonlinear
dynamics for data-driven modeling. In Mathematical and Scientific Machine Learning, pages 65–80.
PMLR, 2022."
REFERENCES,0.29225352112676056,"[31] Zhen Zhang, Yeonjong Shin, and George Em Karniadakis. Gfinns: Generic formalism informed neural
networks for deterministic and stochastic dynamical systems. Philosophical Transactions of the Royal
Society A, 380(2229):20210207, 2022."
REFERENCES,0.29401408450704225,"[32] Yaofeng Desmond Zhong, Biswadip Dey, and Amit Chakraborty.
Dissipative symoden: Encoding
hamiltonian dynamics with dissipation and control into deep learning. In ICLR 2020 Workshop on
Integration of Deep Neural Models and Differential Equations."
REFERENCES,0.29577464788732394,"[33] Shaan A Desai, Marios Mattheakis, David Sondak, Pavlos Protopapas, and Stephen J Roberts. Port-
hamiltonian neural networks for learning explicit time-dependent dynamical systems. Physical Review E,
104(3):034312, 2021."
REFERENCES,0.2975352112676056,"[34] Miroslav Grmela. Generic guide to the multiscale dynamics and thermodynamics. Journal of Physics
Communications, 2(3):032001, 2018."
REFERENCES,0.2992957746478873,"[35] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep
learning framework for solving forward and inverse problems involving nonlinear partial differential
equations. Journal of Computational physics, 378:686–707, 2019."
REFERENCES,0.301056338028169,"[36] Maziar Raissi and George Em Karniadakis. Hidden physics models: Machine learning of nonlinear partial
differential equations. Journal of Computational Physics, 357:125–141, 2018."
REFERENCES,0.3028169014084507,"[37] Filippo Masi and Ioannis Stefanou. Multiscale modeling of inelastic materials with thermodynamics-based
artificial neural networks (tann). Computer Methods in Applied Mechanics and Engineering, 398:115190,
2022."
REFERENCES,0.3045774647887324,"[38] Ravi G Patel, Indu Manickam, Nathaniel A Trask, Mitchell A Wood, Myoungkyu Lee, Ignacio Tomas,
and Eric C Cyr. Thermodynamically consistent physics-informed neural networks for hyperbolic systems.
Journal of Computational Physics, 449:110754, 2022."
REFERENCES,0.30633802816901406,"[39] Quercus Hernández, Alberto Badías, David González, Francisco Chinesta, and Elías Cueto. Structure-
preserving neural networks. Journal of Computational Physics, 426:109950, 2021."
REFERENCES,0.30809859154929575,"[40] Yibo Yang and Paris Perdikaris. Adversarial uncertainty quantification in physics-informed neural networks.
Journal of Computational Physics, 394:136–152, 2019."
REFERENCES,0.30985915492957744,"[41] Dongkun Zhang, Lu Lu, Ling Guo, and George Em Karniadakis. Quantifying total uncertainty in physics-
informed neural networks for solving forward and inverse stochastic problems. Journal of Computational
Physics, 397:108850, 2019."
REFERENCES,0.31161971830985913,"[42] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient flow pathologies in
physics-informed neural networks. SIAM Journal on Scientific Computing, 43(5):A3055–A3081, 2021."
REFERENCES,0.31338028169014087,"[43] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural tangent kernel
perspective. Journal of Computational Physics, 449:110768, 2022."
REFERENCES,0.31514084507042256,"[44] T Konstantin Rusch, Ben Chamberlain, James Rowbottom, Siddhartha Mishra, and Michael Bronstein.
Graph-coupled oscillator networks. In International Conference on Machine Learning, pages 18888–18909.
PMLR, 2022."
REFERENCES,0.31690140845070425,"[45] Jeongwhan Choi, Seoyoung Hong, Noseong Park, and Sung-Bae Cho. Gread: Graph neural reaction-
diffusion equations. arXiv preprint arXiv:2211.14208, 2022."
REFERENCES,0.31866197183098594,"[46] Alvaro Sanchez-Gonzalez, Victor Bapst, Kyle Cranmer, and Peter Battaglia. Hamiltonian graph networks
with ode integrators. arXiv preprint arXiv:1909.12790, 2019."
REFERENCES,0.3204225352112676,"[47] Suresh Bishnoi, Ravinder Bhattoo, Jayadeva Jayadeva, Sayan Ranu, and NM Anoop Krishnan. Enhancing
the inductive biases of graph neural ode for modeling physical systems. In The Eleventh International
Conference on Learning Representations."
REFERENCES,0.3221830985915493,"[48] Quercus Hernandez, Alberto Badias, Francisco Chinesta, and Elias Cueto. Thermodynamics-informed
graph neural networks. IEEE Transactions on Artificial Intelligence, 1(01):1–1, 2022."
REFERENCES,0.323943661971831,"[49] Ben Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb, and Emanuele
Rossi. Grand: Graph neural diffusion. In International Conference on Machine Learning, pages 1407–1418.
PMLR, 2021."
REFERENCES,0.3257042253521127,"[50] PJ Morrison. Thoughts on brackets and dissipation: old and new. In Journal of Physics: Conference Series,
volume 169, page 012006. IOP Publishing, 2009."
REFERENCES,0.3274647887323944,"[51] Oliver Knill. The dirac operator of a graph, 2013."
REFERENCES,0.32922535211267606,"[52] Xiaoye Jiang, Lek-Heng Lim, Yuan Yao, and Yinyu Ye. Statistical ranking and combinatorial hodge theory.
Mathematical Programming, 127(1):203–244, 2011."
REFERENCES,0.33098591549295775,"[53] Pavel B Bochev and James M Hyman. Principles of mimetic discretizations of differential operators. In
Compatible spatial discretizations, pages 89–119. Springer, 2006."
REFERENCES,0.33274647887323944,"[54] Douglas N Arnold. Finite element exterior calculus. SIAM, 2018."
REFERENCES,0.3345070422535211,"[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems,
volume 30. Curran Associates, Inc., 2017."
REFERENCES,0.3362676056338028,"[56] Ignacio Romero. Thermodynamically consistent time-stepping algorithms for non-linear thermomechanical
systems. International Journal for Numerical Methods in Engineering, 79(6):706–732, 2023/05/14 2009."
REFERENCES,0.3380281690140845,"[57] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In
2012 IEEE/RSJ international conference on intelligent robots and systems, pages 5026–5033. IEEE, 2012."
REFERENCES,0.3397887323943662,"[58] Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the construc-
tion of internet portals with machine learning. Information Retrieval, 3:127–163, 2000."
REFERENCES,0.3415492957746479,"[59] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93–93, 2008."
REFERENCES,0.34330985915492956,"[60] Galileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. Query-driven active surveying for
collective classification. In 10th international workshop on mining and learning with graphs, volume 8,
page 1, 2012."
REFERENCES,0.34507042253521125,"[61] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. Pitfalls of
graph neural network evaluation. ArXiv, abs/1811.05868, 2018."
REFERENCES,0.34683098591549294,"[62] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based recommen-
dations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR conference on
research and development in information retrieval, pages 43–52, 2015."
REFERENCES,0.3485915492957746,"[63] Michael Poli, Stefano Massaroli, Junyoung Park, Atsushi Yamashita, Hajime Asama, and Jinkyoo Park.
Graph neural ordinary differential equations. arXiv preprint arXiv:1911.07532, 2019."
REFERENCES,0.3503521126760563,"[64] Xiaoye Jiang, Lek-Heng Lim, Yuan Yao, and Yinyu Ye. Statistical ranking and combinatorial hodge theory.
Mathematical Programming, 127(1):203–244, nov 2010."
REFERENCES,0.352112676056338,"[65] Anthony Bloch, P. S. Krishnaprasad, Jerrold E. Marsden, and Tudor S. Ratiu. The euler-poincaré equations
and double bracket dissipation. Communications in Mathematical Physics, 175(1):1–42, 1996."
REFERENCES,0.3538732394366197,"[66] Darryl D Holm, Jerrold E Marsden, and Tudor S Ratiu. The euler–poincaré equations and semidirect
products with applications to continuum theories. Advances in Mathematics, 137(1):1–81, 1998."
REFERENCES,0.35563380281690143,"[67] Hans Christian Oettinger. Irreversible dynamics, onsager-casimir symmetry, and an application to turbu-
lence. Physical Review E, 90(4):042121, 2014."
REFERENCES,0.3573943661971831,"[68] Anthony Gruber, Max Gunzburger, Lili Ju, and Zhu Wang. Energetically consistent model reduction for
metriplectic systems. Computer Methods in Applied Mechanics and Engineering, 404:115709, 2023."
REFERENCES,0.3591549295774648,[69] Robert J. Renka. A simple explanation of the sobolev gradient method. 2006.
REFERENCES,0.3609154929577465,"[70] Chris Yu, Henrik Schumacher, and Keenan Crane. Repulsive curves. ACM Trans. Graph., 40(2), may
2021."
REFERENCES,0.3626760563380282,"[71] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. Advances in neural information processing systems, 32, 2019."
REFERENCES,0.3644366197183099,"[72] Joe Chen. Chaos from simplicity: an introduction to the double pendulum. Technical report, University of
Canterbury, 2008."
REFERENCES,0.36619718309859156,"[73] Diederik P Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.36795774647887325,"[74] Romeo Ortega, Arjan Van Der Schaft, Bernhard Maschke, and Gerardo Escobar. Interconnection and
damping assignment passivity-based control of port-controlled hamiltonian systems. Automatica, 38(4):585–
596, 2002."
REFERENCES,0.36971830985915494,"[75] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.3714788732394366,"[76] Lukas Biewald. Experiment tracking with weights and biases, 2020. Software available from wandb.com."
REFERENCES,0.3732394366197183,Glossary of Notation and Symbols
REFERENCES,0.375,The next list describes several symbols that will be later used within the body of the document
REFERENCES,0.3767605633802817,"{{·, ·}} (Irreversible) double bracket on functions with generator L2"
REFERENCES,0.3785211267605634,"[·, ·]
Degenerate (irreversible) metric bracket on functions with generator M⊺= M"
REFERENCES,0.38028169014084506,"{·, ·}
Poisson (reversible) bracket on functions with generator L⊺= −L"
REFERENCES,0.38204225352112675,"N(i), N(i) Neighbors of node i ∈V, neighbors of node i ∈V including i"
REFERENCES,0.38380281690140844,"[S]
Indicator function of the statement S"
REFERENCES,0.3855633802816901,"δf, ∇f Adjoint of df with respect to ⟨·, ·⟩, adjoint of df with respect to (·, ·)"
REFERENCES,0.3873239436619718,"∆k
Hodge Laplacian dkd∗
k + d∗
kdk"
REFERENCES,0.3890845070422535,"δij
Kronecker delta"
REFERENCES,0.3908450704225352,"˙f
Derivative of f with respect to time"
REFERENCES,0.3926056338028169,"(·, ·)k
Learnable metric inner product on k-cliques with matrix representation Ak"
REFERENCES,0.39436619718309857,"⟨·, ·⟩k
Euclidean ℓ2 inner product on k-cliques"
REFERENCES,0.3961267605633803,"G, V, E Oriented graph, set of nodes, set of edges"
REFERENCES,0.397887323943662,"Gk, Ωk Set of k-cliques, vector space of real-valued functions on k-cliques"
REFERENCES,0.3996478873239437,"d, dk
Exterior derivative operator on functions, exterior derivative operator on k-cliques"
REFERENCES,0.4014084507042254,"d⊺
k, d∗
k
Adjoint of dk with respect to ⟨·, ·⟩k, adjoint of dk with respect to (·, ·)k"
REFERENCES,0.40316901408450706,"A
Mathematical foundations"
REFERENCES,0.40492957746478875,"This Appendix provides the following: (1) an introduction to the ideas of graph exterior calculus,
A.1, and bracket-based dynamical systems, A.2, necessary for understanding the results in the body,
(2) additional explanation regarding adjoints with respect to generic inner products and associated
computations, A.3, (3) a mechanism for higher-order attention expressed in terms of learnable inner
products, A.4, (4) a discussion of GATs in the context of exterior calculus, A.5, and (5) proofs which
are deferred from Section 4, A.6."
REFERENCES,0.40669014084507044,"A.1
Graph exterior calculus"
REFERENCES,0.4084507042253521,"Here some basic notions from the graph exterior calculus are recalled. More details can be found in,
e.g., [11, 51, 64]. 1 2 3 4
5 6
1 2 3 4 5 6
1"
REFERENCES,0.4102112676056338,"Figure 3: A toy graph with six 0-cliques (nodes), six 1-cliques (edges), and one 2-clique."
REFERENCES,0.4119718309859155,"As mentioned in Section 3, an oriented graph G = {V, E} carries sets of k-cliques, denoted Gk, which
are collections of ordered subgraphs generated by (k + 1) nodes. For example, the graph in Figure 3
contains six 0-cliques (nodes), six 1-cliques (edges), and one 2-clique. A notion of combinatorial
derivative is then given by the signed incidence matrices dk : Ωk →Ωk+1, operating on the space
Ωk of differentiable functions on k-cliques, whose entries (dk)ij are 1 or -1 if the jth k-clique is"
REFERENCES,0.4137323943661972,"incident on the ith (k + 1)-clique, and zero otherwise. For the example in Figure 3, these are: d0 = "
REFERENCES,0.4154929577464789,"




"
REFERENCES,0.41725352112676056,"−1
1
0
0
0
0
0
−1
1
0
0
0
0
−1
0
1
0
0
0
0
0
−1
1
0
0
1
0
0
−1
0
0
0
0
0
−1
1 "
REFERENCES,0.41901408450704225,"





,
d1 = (0
0
1
1
1
0) ."
REFERENCES,0.42077464788732394,"Remark A.1. While the one-hop neighborhood of node i in G, denoted N(i), does not include node
i itself, many machine learning algorithms employ the extended neighborhood N(i) = N(i) ∪{i}.
Since this is equivalent to considering the one-hop neighborhood of node i in the self-looped graph
G, this modification does not change the analysis of functions on graphs."
REFERENCES,0.4225352112676056,"It can be shown that the action of these matrices can be conveniently expressed in terms of totally
antisymmetric functions f ∈Ωk, via the expression"
REFERENCES,0.4242957746478873,"(dkf) (i0, i1, ..., ik+1) = k+1
X"
REFERENCES,0.426056338028169,"j=0
(−1)jf

i0, ..., bij, ..., ik+1

,"
REFERENCES,0.4278169014084507,"where (i0, ..., ik+1) denotes a (k + 1)-clique of vertices v ∈V. As convenient shorthand, we often
write subscripts, e.g., (dkf)i0i1...ik+1, instead of explicit function arguments. Using [S] to denote the
indicator function of the statement S, it is straightforward to check that d ◦d = 0,"
REFERENCES,0.4295774647887324,"(dkdk−1f)i0,...,ik+1 = k+1
X"
REFERENCES,0.43133802816901406,"j=0
(−1)j (dk−1f)i0,...,bij,...,ik+1 = k+1
X j=0 k+1
X"
REFERENCES,0.43309859154929575,"l=0
[l < j] (−1)j+l fi0...bil...bij...ik+1 + k+1
X j=0 k+1
X"
REFERENCES,0.43485915492957744,"l=0
[l > j] (−1)j+l−1 fi0...bij...bil...ik+1 =
X"
REFERENCES,0.43661971830985913,"l<j
(−1)j+l fi0...bil...bij...ik+1 −
X"
REFERENCES,0.43838028169014087,"l<j
(−1)j+l fi0...bil...bij...ik+1 = 0,"
REFERENCES,0.44014084507042256,"since (−1)j+l−1 = (−1)−1(−1)j+l = (−1)(−1)j+l and the final sum follows from swapping the
labels j, l. This shows that the k-cliques on G form a de Rham complex [53]: a collection of function
spaces Ωk equipped with mappings dk satisfying Im dk−1 ⊂Ker dk as shown in Figure 4. When"
REFERENCES,0.44190140845070425,"Ω0
Ω1
Ω2
· · ·
ΩK
d0
d1
d2
dK−1"
REFERENCES,0.44366197183098594,"Figure 4: Illustration of the de Rham complex on G induced by the combinatorial derivatives, where
K > 0 is the maximal clique degree."
REFERENCES,0.4454225352112676,"K = 3, this is precisely the graph calculus analogue of the de Rham complex on R3 formed by the
Sobolev spaces H1, H(curl), H(div), L2 which satisfies div ◦curl = curl ◦grad = 0."
REFERENCES,0.4471830985915493,"While the construction of the graph derivatives and their associated de Rham complex is purely
topological, building elliptic differential operators such as the Laplacian relies on a dual de Rham
complex, which is specified by an inner product on Ωk. In the case of ℓ2, this leads to dual derivatives
which are the matrix transposes of the dk having the following explicit expression.
Proposition A.1. The dual derivatives d⊺
k : Ωk+1 →Ωk adjoint to dk through the ℓ2 inner product
are given by"
REFERENCES,0.448943661971831,"(d⊺
kf) (i0, i1, ..., ik) =
1
k + 2 X ik+1 k+1
X"
REFERENCES,0.4507042253521127,"j=0
f (i0, ..., [ij, ..., ik+1]) ,"
REFERENCES,0.4524647887323944,"where [ij, ..., ik+1] = ik+1, ij, ..., ik indicates a cyclic permutation forward by one index."
REFERENCES,0.45422535211267606,"Proof. This is a direct calculation using the representation of dk in terms of antisymmetric functions.
More precisely, let an empty sum Σ denote summation over all unspecified indices. Then, for any
g ∈Ωk,"
REFERENCES,0.45598591549295775,"⟨dkf, g⟩=
X"
REFERENCES,0.45774647887323944,"i0...ik+1∈Gk+1
(dkf)i0...ik+1 gi0...,ik+1"
REFERENCES,0.4595070422535211,"=
1
(k + 2)! X
"
REFERENCES,0.4612676056338028,"
k+1
X"
REFERENCES,0.4630281690140845,"j=0
(−1)jfi0...bij...ik+1 "
REFERENCES,0.4647887323943662,gi0...ik+1
REFERENCES,0.4665492957746479,"=
1
(k + 2)!"
REFERENCES,0.46830985915492956,"X
fi0...ik  X ik+1 k+1
X"
REFERENCES,0.47007042253521125,"j=0
(−1)jgi0...[ij...ik+1]  "
REFERENCES,0.47183098591549294,"=
1
k + 2 X"
REFERENCES,0.4735915492957746,"i0i1...ik∈Gk
fi0...ik  X ik+1 k+1
X"
REFERENCES,0.4753521126760563,"j=0
(−1)jgi0...[ij...ik+1]   =
X"
REFERENCES,0.477112676056338,"i0i1...ik∈Gk
fi0...ik (d⊺
kg)i0...ik = ⟨f, d⊺
kg⟩,"
REFERENCES,0.4788732394366197,which establishes the result.
REFERENCES,0.48063380281690143,"Proposition A.1 is perhaps best illustrated with a concrete example. Consider the graph gradient,
defined for edge α = (i, j) as (d0f)α = (d0f)ij = fj −fi. Notice that this object is antisymmetric
with respect to edge orientation, and measures the outflow of information from source to target nodes.
From this, it is easy to compute the ℓ2-adjoint of d0, known as the graph divergence, via"
REFERENCES,0.4823943661971831,"⟨d0f, g⟩=
X"
REFERENCES,0.4841549295774648,"α=(i,j)
(fj −fi) gij =
X i X"
REFERENCES,0.4859154929577465,"(j>i)∈N(i)
gijfj −gijfi = 1 2 X i X"
REFERENCES,0.4876760563380282,"j∈N(i)
fi (gji −gij) = ⟨f, d⊺
0g⟩,"
REFERENCES,0.4894366197183099,"where we have re-indexed under the double sum, used that i ∈N(j) if and only if j ∈N(i), and
used that there are no self-edges in E. Therefore, it follows that the graph divergence at node i is
given by"
REFERENCES,0.49119718309859156,"(d⊺
0g)i =
X"
REFERENCES,0.49295774647887325,"α∋i
g−α −gα = 1 2 X"
REFERENCES,0.49471830985915494,"j∈N(i)
gji −gij,"
REFERENCES,0.4964788732394366,"which reduces to the common form (d⊺
0g)i = −P"
REFERENCES,0.4982394366197183,"j gij if and only if the edge feature gij is
antisymmetric.
Remark A.2. When the inner product on edges E is not L2, but defined in terms of a nonnegative,
orientation-invariant, and (edge-wise) diagonal weight matrix W = (wij), a similar computation
shows that the divergence becomes"
REFERENCES,0.5,"(d∗
0f)i = 1 2 X"
REFERENCES,0.5017605633802817,"j∈N(i)
wij (fji −fij) ."
REFERENCES,0.5035211267605634,"The more general case of arbitrary inner products on V, E is discussed in section A.3."
REFERENCES,0.5052816901408451,"The differential operators d⊺
k induce a dual de Rham complex since d⊺
k−1d⊺
k = (dkdk−1)⊺= 0, which
enables both the construction of Laplace operators on k-cliques, ∆k = d⊺
kdk + dk−1d⊺
k−1, as well as
the celebrated Hodge decomposition theorem, stated below. For a proof, see, e.g., [11, Theorem 3.3].
Theorem A.3. (Hodge Decomposition Theorem) The de Rham complexes formed by dk, d⊺
k induce
the following direct sum decomposition of the function space Ωk,
Ωk = Im dk−1 ⊕Ker ∆k ⊕Im d⊺
k."
REFERENCES,0.5070422535211268,"In the case where the dual derivatives d∗
k are adjoint with respect to a learnable inner product which
does not depend on graph features, the conclusion of Theorem A.3 continues to hold, leading to an
interesting well-posedness result proved in [11] involving nonlinear perturbations of a Hodge-Laplace
problem in mixed form."
REFERENCES,0.5088028169014085,"Theorem A.4. ([11, Theorem 3.6]) Suppose fk ∈Ωk, and g (x; ξ) is a neural network with parame-
ters ξ which is Lipschitz continuous and satisfies g(0) = 0. Then, the problem"
REFERENCES,0.5105633802816901,"wk−1 = d∗
k−1uk + ϵg
 
d∗
k−1uk; ξ

,"
REFERENCES,0.5123239436619719,"fk = dk−1wk−1 + d∗
kdkuk,"
REFERENCES,0.5140845070422535,has a unique solution on Ωk/Ker ∆k.
REFERENCES,0.5158450704225352,"This result shows that initial-value problems involving the Hodge-Laplacian are stable under nonlinear
perturbations. Moreover, when ∆0 is the Hodge Laplacian on nodes, there is a useful connection
between ∆0 and the degree and adjacency matrices of the graph G. Recall that the degree matrix
D = (dij) is diagonal with entries dii = P
j∈N(i) 1, while the adjacency matrix A = (aij) satisfies
aij = 1 when j ∈N(i) and aij = 0 otherwise.
Proposition A.2. The combinatorial Laplacian on V, denoted ∆0 = d⊺
0d0, satisfies ∆0 = D −A."
REFERENCES,0.5176056338028169,Proof. Notice that
REFERENCES,0.5193661971830986,"(d⊺
0d0)ij =
X"
REFERENCES,0.5211267605633803,"α∈E
(d0)αi (d0)αj = [i = j]
X"
REFERENCES,0.522887323943662,"α∈E
((d0)αi)2 + [i ̸= j]
X"
REFERENCES,0.5246478873239436,"α=(i,j)
(d0)αi (d0)αj"
REFERENCES,0.5264084507042254,"= [i = j] dii −[i ̸= j] aij = dij −aij = D −A,"
REFERENCES,0.528169014084507,"where we used that D is diagonal, A is diagonal-free, and (d0)αi (d0)αj = −1 whenever α = (i, j)
is an edge in E, since one of (d0)αi , (d0)αj is 1 and the other is -1."
REFERENCES,0.5299295774647887,"A.2
Bracket-based dynamical systems"
REFERENCES,0.5316901408450704,"Here we mention some additional facts regarding bracket-based dynamical systems. More information
can be found in, e.g., [50, 65, 66, 67]."
REFERENCES,0.5334507042253521,"As mentioned before, the goal of bracket formalisms is to extend the Hamiltonian formalism to
systems with dissipation. To understand where this originates, consider an action functional A(q) =
R b
a L (q, ˙q) dt on the space of curves q(t), defined in terms of a Lagrangian L on the tangent bundle to
some Riemannian manifold. Using Lq, L ˙q to denote partial derivatives with respect to the subscripted
variable, it is straightforward to show that, for any compactly supported variation δq of q, we have"
REFERENCES,0.5352112676056338,"dA(q)δq =
Z b"
REFERENCES,0.5369718309859155,"a
dL (q, ˙q) δq =
Z b"
REFERENCES,0.5387323943661971,"a
Lqδq + L ˙qδ ˙q =
Z b"
REFERENCES,0.5404929577464789,"a
(Lq −∂tL ˙q) δq,"
REFERENCES,0.5422535211267606,"where the final equality follows from integration-by-parts and the fact that variational and temporal
derivatives commute in this setting. It follows that A is stationary (i.e., dA = 0) for all variations
only when ∂tL ˙q = Lq. These are the classical Euler-Lagrange equations which are (under some
regularity conditions) transformed to Hamiltonian form via a Legendre transformation,"
REFERENCES,0.5440140845070423,"H(q, p) = sup
˙q
(⟨p, ˙q⟩−L(q, ˙q)) ,"
REFERENCES,0.545774647887324,"which defines the Hamiltonian functional H on phase space, and yields the conjugate momentum
vector p = L ˙q. Substituting L = ⟨p, ˙q⟩−H into the previously derived Euler-Lagrange equations
leads immediately to Hamilton’s equations for the state x = (q
p)⊺,"
REFERENCES,0.5475352112676056,"˙x =

˙q
˙p"
REFERENCES,0.5492957746478874,"
=

0
1
−1
0"
REFERENCES,0.551056338028169," 
Hq
Hp"
REFERENCES,0.5528169014084507,"
= J∇H,"
REFERENCES,0.5545774647887324,"which are an equivalent description of the system in question in terms of the anti-involution J and the
functional gradient ∇H."
REFERENCES,0.5563380281690141,"An advantage of the Hamiltonian description is its compact bracket-based formulation, ˙x = J∇H =
{x, H}, which requires only the specification of an antisymmetric Poisson bracket {·, ·} and a
Hamiltonian functional H. Besides admitting a direct generalization to more complex systems such
as Korteweg-de Vries or incompressible Euler, where the involved bracket is state-dependent, this
formulation makes the energy conservation property of the system obvious. In particular, it follows
immediately from the antisymmetry of {·, ·} that
˙H = ⟨˙x, ∇H⟩= {H, H} = 0,"
REFERENCES,0.5580985915492958,"while it is more difficult to see immediately that the Euler-Lagrange system obeys this same property.
The utility and ease-of-use of bracket formulations is what inspired their extension to other systems
of interest which do not conserve energy. On the opposite end of this spectrum are the generalized
gradient flows, which can be written in terms of a bracket which is purely dissipative. An example
of this is heat flow ˙q = ∆q := −[q, D], which is the L2-gradient flow of Dirichlet energy D(q) =
(1/2)
R b
a |q′|2 dt (c.f. Appendix A.3). In this case, the functional gradient ∇D = −∂tt is the negative
of the usual Laplace operator, so that the positive-definite bracket [·, ·] is generated by the identity
operator M = id. It is interesting to note that the same system could be expressed using the usual
kinetic energy E(q) = (1/2)
R b
a |q|2 dt instead, provided that the corresponding bracket is generated
by M = −∆. This is a good illustration of the flexibility afforded by bracket-based dynamical
systems."
REFERENCES,0.5598591549295775,"Since physical systems are not always purely reversible or irreversible, other useful bracket formalisms
have been introduced to capture dynamics which are a mix of these two. The double bracket
˙x = {x, E}+{{x, E}} = L∇E +L2∇E is a nice extension of the Hamiltonian bracket particularly
because it is Casimir preserving, i.e., those quantities which annihilate the Poisson bracket {·, ·}
also annihilate the double bracket. This allows for the incorporation of dissipative phenomena into
idealized Hamiltonian systems without affecting desirable properties such as mass conservation, and
has been used to model, e.g., the Landau-Lifschitz dissipative mechanism, as well as a mechanism
for fluids where energy decays but entrophy is preserved (see [65] for additional discussion). A
complementary but alternative point of view is taken by the metriplectic bracket formalism, which
requires that any dissipation generated by the system is accounted for within the system itself
through the generation of entropy. In the metriplectic formalism, the equations of motion are
˙x = {x, E} + [x, S] = L∇E + M∇S, along with important and nontrivial compatibility conditions
L∇S = M∇E = 0, also called degeneracy conditions, which ensure that the reversible and
irreversible mechanisms do not cross-contaminate. As shown in the body of the paper, this guarantees
that metriplectic systems obey a form of the first and second thermodynamical laws. Practically,
the degeneracy conditions enforce a good deal of structure on the operators L, M which has been
exploited to generate surrogate models [29, 68, 31]. In particular, it can be shown that the reversible
and irreversible brackets can be parameterized in terms of a totally antisymmetric order-3 tensor
ξ = (ξijk) and a partially symmetric order-4 tensor ζ = (ζik,jl) through the relations (Einstein
summation assumed)"
REFERENCES,0.5616197183098591,"{A, B} = ξijk ∂iA ∂jB ∂kS,"
REFERENCES,0.5633802816901409,"[A, B] = ζik,jl ∂iA ∂kE ∂jB ∂lE."
REFERENCES,0.5651408450704225,"Moreover, using the symmetries of ζ, it follows (see [67]) that this tensor decomposes into the product
ζik,jl = Λm
ikDmnΛn
jl of a symmetric matrix D and an order-3 tensor Λ which is skew-symmetric in its
lower indices. Thus, by applying symmetry relationships, it is easy to check that {·, S} = [·, E] = 0."
REFERENCES,0.5669014084507042,"Remark A.5. In [29], trainable 4- and 3- tensors ξijk and ζik,jl are constructed to achieve the
degeneracy conditions, mandating a costly O(N 3) computational complexity. In the current work we
overcome this by instead achieving degeneracy through the exact sequence property."
REFERENCES,0.5686619718309859,"A.3
Adjoints and gradients"
REFERENCES,0.5704225352112676,"Beyond the basic calculus operations discussed in section A.1 which depend only on graph topology,
the network architectures discussed in the body also make extensive use of learnable metric infor-
mation coming from the nodal features. To understand this, it is useful to recall some information
about general inner products and the derivative operators that they induce. First, recall that the usual
ℓ2 inner product on node features a, b ∈R|V|, ⟨a, b⟩= a⊺b, is (in this context) a discretization
of the standard L2 inner product
R"
REFERENCES,0.5721830985915493,"V ab dµ which aggregates information from across the vertex set
V. While this construction is clearly dependent only on the graph structure (i.e., topology), any
symmetric positive definite (SPD) matrix A0 : Ω0 →Ω0 also defines an inner product on functions
a ∈Ω0 through the equality
(a, b)0 := ⟨a, A0b⟩= a⊺A0b,"
REFERENCES,0.573943661971831,"which gives a different way of measuring the distance between a and b. The advantage of this
construction is that A0 can be chosen in a way that incorporates geometric information which
implicitly regularizes systems obeying a variational principle. This follows from the following"
REFERENCES,0.5757042253521126,"intuitive fact: the Taylor series of a function does not change, regardless of the inner product on its
domain. For any differentiable function(al) E : Ω0 →R, using d to denote the exterior derivative,
this means that the following equality holds"
REFERENCES,0.5774647887323944,"dE(a)b := lim
ε→0
E(a + ϵb) −E(a)"
REFERENCES,0.579225352112676,"ε
= ⟨δE(a), b⟩= (∇E(a), b)0 ,"
REFERENCES,0.5809859154929577,"where δE denotes the ℓ2-gradient of E and ∇E denotes its A0-gradient, i.e., its gradient with
respect to the derivative operator induced by the inner product involving A0. From this, it is clear
that δE = A0∇E, so that the A0-gradient is just an anisotropic rescaling of the ℓ2 version. The
advantage of working with ∇over δ in the present case of graph networks is that A0 can be learned
based on the features of the graph. This means that learnable feature information (i.e., graph attention)
can be directly incorporated into the differential operators governing our bracket-based dynamical
systems by construction."
REFERENCES,0.5827464788732394,"The prototypical example of where this technique is useful is seen in the gradient flow of Dirichlet
energy. Recall that the Dirichlet energy of a differentiable function u : Rn →R is given by
D(u) = (1/2)
R
|∇u|2 dµ, where ∇now denotes the usual ℓ2-gradient of the function u on Rn.
Using integration-by-parts, it is easy to see that dD(u)v = −
R
v∆u for any test function v with
compact support, implying that the L2-gradient of D is −∆and ˙u = ∆u is the L2-gradient flow of
Dirichlet energy: the motion which decreases the quantity D(u) the fastest as measured by the L2
norm. It can be shown that high-frequency modes decay quickly under this flow, while low-frequency
information takes much longer to dissipate. On the other hand, we could alternatively run the
H1-gradient flow of D, which is motion of fastest decrease with respect to the H1 inner product
(u, v) =
R
⟨∇u, ∇v⟩dµ. This motion is prescribed in terms of the H1-gradient of D, which by the
discussion above with A0 = −∆is easily seen to be the identity. This means that the H1-gradient
flow is given by ˙u = −u, which retains the minimizers of the L2-flow but with quite different
intermediate character, since it functions by simultaneously flattening all spatial frequencies. The
process of preconditioning a gradient flow by matching derivatives is known as a Sobolev gradient
method (c.f. [69]), and these methods often exhibit faster convergence and better numerical behavior
than their L2 counterparts [70]."
REFERENCES,0.5845070422535211,"Returning to the graph setting, our learnable matrices Ak on k-cliques will lead to inner products
(·, ·)k on functions in Ωk, and this will induce dual derivatives as described in Appendix A.1. However,
in this case we will not have d∗
0 = d⊺
0, but instead the expression given by the following result:
Proposition A.3. The Ak-adjoints d∗
k to the graph derivative operators dk are given by d∗
k =
A−1
k d⊺
kAk+1. Similarly, for any linear operator B : Ωk →Ωk, the Ak-adjoint B∗= A−1
k B⊺A."
REFERENCES,0.5862676056338029,"Proof. Let q, p denote vectors of k-clique resp. (k + 1)-clique features. It follows that
(dkq, p)k+1 = ⟨dkq, Ak+1p⟩= ⟨q, d⊺
kAk+1p⟩= ⟨q, Akd∗
kp⟩= (q, d∗
kp)k ."
REFERENCES,0.5880281690140845,"Therefore, we see that d⊺
kAk+1 = Akd∗
k and hence d∗
k = A−1
k d⊺
kAk+1. Similarly, if q, q′ denote
vectors of k-clique features, it follows from the ℓ2-self-adjointness of Ak that
 
q, Bq′"
REFERENCES,0.5897887323943662,"k =

q, AkBq′
= ⟨B⊺Akq, q′⟩=

A−1
k B⊺Akq, Akq′
= (B∗q, q′)k ,"
REFERENCES,0.5915492957746479,"establishing that B∗= A−1
k B⊺Ak."
REFERENCES,0.5933098591549296,"Remark A.6. It is common in graph theory to encounter the case where ai > 0 are nodal weights
and wij > 0 are edge weights. These are nothing more than the (diagonal) inner products A0, A1 in
disguise, and so Proposition A.3 immediately yields the familiar formula for the induced divergence"
REFERENCES,0.5950704225352113,"(d∗
0p)i = 1 ai X"
REFERENCES,0.596830985915493,"j:(i,j)∈E
wij (pji −pij) ."
REFERENCES,0.5985915492957746,"Note that all of these notions extend to the case of block inner products in the obvious way. For
example, if q, p are node resp. edge features, it follows that A = diag (A0, A1) is an inner product
on node-edge feature pairs, and the adjoints of node-edge operators with respect to A are computed
as according to Proposition A.3.
Remark A.7. For convenience, this work restricts to diagonal matrices A0, A1. However, note that
a matrix which is diagonal in “edge space” G2 is generally full in a nodal representation. This is
because an (undirected) edge is uniquely specified by the two nodes which it connects, meaning that
a purely local quantity on edges is necessarily nonlocal on nodes."
REFERENCES,0.6003521126760564,"A.4
Higher order attention"
REFERENCES,0.602112676056338,"As mentioned in the body, when f = exp and ˜a(qi, qj) = (1/d) ⟨WKqi, WQqj⟩, defining the
learnable inner products A0 = (a0,ii) , A1 = (a1,ij) as"
REFERENCES,0.6038732394366197,"a0,ii =
X"
REFERENCES,0.6056338028169014,"j∈N(i)
f (˜a (qi, qj)) ,
a1,ij = f (˜a (qi, qj)) ,"
REFERENCES,0.6073943661971831,"recovers scaled dot product attention as A−1
0 A1.
Remark A.8. Technically, A1 is an inner product only with respect to a predefined ordering of
the edges α = (i, j), since we do not require A1 be orientation-invariant. On the other hand, it is
both unnecessary and distracting to enforce symmetry on A1 in this context, since any necessary
symmetrization will be handled automatically by the differential operator d∗
0."
REFERENCES,0.6091549295774648,"Similarly, other common attention mechanisms are produced by modifying the pre-attention function
˜a. While A−1
0 A1 never appears in the brackets of Section 4, letting α = (i, j) denote a global edge
with endpoints i, j, it is straightforward to calculate the divergence of an antisymmetric edge feature
p at node i,"
REFERENCES,0.6109154929577465,"(d∗
0p)i =
 
A−1
0 d⊺
0A1p
"
REFERENCES,0.6126760563380281,"i = a−1
0,ii
X"
REFERENCES,0.6144366197183099,"α
(d⊺
0)iα (A1p)α"
REFERENCES,0.6161971830985915,"= a−1
0,ii
X"
REFERENCES,0.6179577464788732,"α∋i
(A1p)−α −(A1p)α = −
X"
REFERENCES,0.6197183098591549,j∈N(i)
REFERENCES,0.6214788732394366,"a1,ji + ai,ij"
REFERENCES,0.6232394366197183,"a0,ii
pij."
REFERENCES,0.625,"This shows that b(qi, qj) = (a1,ij + a1,ji) /a0,ii appears under the divergence in d∗
0 = A−1
0 d⊺
0A1,
which is the usual graph attention up to a symmetrization in A1.
Remark A.9. While A1 is diagonal on global edges α = (i, j), it appears sparse nondiagonal in
its nodal representation. Similarly, any diagonal extension A2 to 2-cliques will appear as a sparse
3-tensor A2 = (a2,ijk) when specified by its nodes."
REFERENCES,0.6267605633802817,"This inspires a straightforward extension of graph attention to higher-order cliques. In particular, de-
note by K > 0 the highest degree of clique under consideration, and define AK−1 = (aK−1,i1i2...iK)
by
aK−1,i1i2...iK = f (W (qi1, qi2, ..., qiK)) ,
where W ∈R⊗KnV is a learnable K-tensor. Then, for any 0 ≤k ≤K −2 define Ak =
 
ak,i1i2...ik+1

by"
REFERENCES,0.6285211267605634,"ak,i1i2...ik+1 =
X"
REFERENCES,0.6302816901408451,"iK,...,iK−k−1
aK−1,i1i2...iK."
REFERENCES,0.6320422535211268,"This recovers the matrices A0, A1 from before when K = 2, and otherwise extends the same core
idea to higher-order cliques. It’s attractive that the attention mechanism captured by d∗
k remains
asymmetric, meaning that the attention of any one node to the others in a k-clique need not equal the
attention of the others to that particular node.
Remark A.10. A more obvious but less expressive option for higher-order attention is to let"
REFERENCES,0.6338028169014085,"ak,i1i2...ik+1 =
aK−1,i1i2...iK
P"
REFERENCES,0.6355633802816901,"iK,...,iK−k−1 aK−1,i1i2...iK
,"
REFERENCES,0.6373239436619719,"for any 0 ≤k ≤K −2. However, application of the combinatorial codifferential d⊺
k−1 appearing in
d∗
k−1 will necessarily symmetrize this quantity, so that the asymmetry behind the attention mechanism
is lost in this formulation."
REFERENCES,0.6390845070422535,"To illustrate how this works more concretely, consider the extension K = 3 to 2-cliques, and let
N(i, j) = N(i)∩N(j). We have the tensors A2 = (a2,ijk), A1 = (a1,ij), and A0 = (a0,i) defined
by"
REFERENCES,0.6408450704225352,"a2,ijk = f (W (qi, qj, qk)) ,
a1,ij =
X"
REFERENCES,0.6426056338028169,"k∈N(i,j)
a2,ijk,
a0,i =
X"
REFERENCES,0.6443661971830986,j∈N(i) X
REFERENCES,0.6461267605633803,"k∈N(i,j)
a2,ijk."
REFERENCES,0.647887323943662,"This provides a way for (features on) 3-node subgraphs of G to attend to each other, and can be
similarly built-in to the differential operator d∗
1 = A−1
1 d⊺
0A2."
REFERENCES,0.6496478873239436,"A.5
Exterior calculus interpretation of GATs"
REFERENCES,0.6514084507042254,"Let N(i) denote the one-hop neighborhood of node i, and let N(i) = N(i)∪{i}. Recall the standard
(single-headed) graph attention network (GAT) described in [12], described layer-wise as"
REFERENCES,0.653169014084507,"qk+1
i
= σ  X"
REFERENCES,0.6549295774647887,"j∈N(i)
a
 
qk
i , qk
j

Wkqk
j "
REFERENCES,0.6566901408450704,",
(1)"
REFERENCES,0.6584507042253521,"where σ is an element-wise nonlinearity, Wk is a layer-dependent embedding matrix, and a (qi, qj)
denotes the attention node i pays to node j. Traditionally, the attention mechanism is computed
through"
REFERENCES,0.6602112676056338,"a (qi, qj) = Softmaxj ˜a (qi, qj) = e˜a(qi,qj) σi
,"
REFERENCES,0.6619718309859155,"where the pre-attention coefficients ˜a (qi, qj) and nodal weights σi are defined as"
REFERENCES,0.6637323943661971,"˜a (qi, qj) = LeakyReLU (a⊺(W⊺qi || W⊺qj)) ,
σi =
X"
REFERENCES,0.6654929577464789,"j∈N(i)
e˜a(qi,qj)."
REFERENCES,0.6672535211267606,"However, the exponentials in the outer Softmax are often replaced with other nonlinear functions, e.g.
Squareplus, and the pre-attention coefficients ˜a appear as variable (but learnable) functions of the
nodal features. First, notice that (1) the attention coefficients a (qi, qj) depend on the node features
q and not simply the topology of the graph, and (2) the attention coefficients are not symmetric,
reflecting the fact that the attention paid by node i to node j need not equal the attention paid by node j
to node i. A direct consequence of this is that GATs are not purely diffusive under any circumstances,
since it was shown in Appendix A.1 that the combinatorial divergence d⊺
0 will antisymmetrize the
edge features it acts on. In particular, it is clear that the product a (qi, qj) (qi −qj) is asymmetric
in i, j under the standard attention mechanism, since even the pre-attention coefficients ˜a (qi, qj)
are not symmetric, meaning that there will be two distinct terms after application of the divergence.
More precisely, there is the following subtle result."
REFERENCES,0.6690140845070423,"Proposition A.4. Let q ∈R|V|×nV denote an array of nodal features. The expression
X"
REFERENCES,0.670774647887324,"j∈N(i)
a (qi, qj) (qi −qj) ,"
REFERENCES,0.6725352112676056,"where a = A−1
0 A1 is not the action of a Laplace operator whenever A1 is not symmetric."
REFERENCES,0.6742957746478874,"Proof. From Appendix A.3, we know that any Laplace operator on nodes is expressible as d∗
0d0 =
A−1
0 d⊺
0A1d0 for some positive definite A0, A1. So, we compute the action of the Laplacian at node
i,"
REFERENCES,0.676056338028169,"(∆0q)i = (d∗
0d0q)i =
 
A−1
0 d⊺
0A1d0q
"
REFERENCES,0.6778169014084507,"i = a−1
0,ii
X"
REFERENCES,0.6795774647887324,"α
(d⊺
0)iα (A1d0q)α"
REFERENCES,0.6813380281690141,"= a−1
0,ii
X"
REFERENCES,0.6830985915492958,"α∋i
(A1d0q)−α −(A1d0q)α = −1 2 X"
REFERENCES,0.6848591549295775,j∈N(i)
REFERENCES,0.6866197183098591,"a1,ji + ai,ij"
REFERENCES,0.6883802816901409,"a0,ii
(qj −qi) , =
X"
REFERENCES,0.6901408450704225,"j∈N(i)
a (qi, qj) (qj −qi) ,"
REFERENCES,0.6919014084507042,"which shows that a (qi, qj) = (1/2) (a1,ji + a1,ij) /a0,ii must have symmetric numerator."
REFERENCES,0.6936619718309859,"While this result shows that GATs (and their derivatives, e.g., GRAND) are not purely diffusive, it
also shows that it is possible to get close to GAT (at least syntactically) with a learnable diffusion
mechanism. In fact, setting σ = Wk = I in (1) yields precisely a single-step diffusion equation
provided that a
 
qk
i , qk
j

is right-stochastic (i.e., P"
REFERENCES,0.6954225352112676,"j a (qi, qj) 1j = 1i) and built as dictated by
Proposition A.4.
Theorem A.11. The GAT layer (1) is a single-step diffusion equation provided that σ = Wk = I,
and the attention mechanism a (qi, qj) = (1/2) (a1,ji + a1,ij) /a0,ii is right-stochastic."
REFERENCES,0.6971830985915493,"Proof. First, notice that the Laplacian with respect to an edge set which contains self-loops is
computable via"
REFERENCES,0.698943661971831,"(∆0q)i = −
X"
REFERENCES,0.7007042253521126,"j∈N(i)
a (qi, qj) (qj −qi) = qi −
X"
REFERENCES,0.7024647887323944,"j∈N(i)
a (qi, qj) qj."
REFERENCES,0.704225352112676,"Therefore, taking a single step of heat flow ˙q = −∆0q with forward Euler discretization and time
step τ = 1 is equivalent to"
REFERENCES,0.7059859154929577,"qk+1
i
= qk
i −τ
 
∆0qk i =
X"
REFERENCES,0.7077464788732394,"j∈N(i)
a
 
qk
i , qk
j

qk
j ,"
REFERENCES,0.7095070422535211,which is just a modified and non-activated GAT layer with Wk = I and attention mechanism a.
REFERENCES,0.7112676056338029,"Remark A.12. Since Softmax and its variants are right-stochastic, Theorem A.11 is what establishes
equivalence between the non-divergence equation"
REFERENCES,0.7130281690140845,"˙qi =
X"
REFERENCES,0.7147887323943662,"j∈N(i)
a (qi, qj) (qj −qi) ,"
REFERENCES,0.7165492957746479,"and the standard GAT layer seen in, e.g., [49], when a(qi, qj) is the usual attention mechanism."
REFERENCES,0.7183098591549296,"Remark A.13. In the literature, there is an important (and often overlooked) distinction between
the positive graph/Hodge Laplacian ∆0 and the negative “geometer’s Laplacian” ∆which is worth
noting here. Particularly, we have from integration-by-parts that the gradient ∇= d0 is L2-adjoint to
minus the divergence −∇· = d⊺
0, so that the two Laplace operators ∆0 = d⊺
0d0 and ∆= ∇·∇differ
by a sign. This is why the same ℓ2-gradient flow of Dirichlet energy can be equivalently expressed as
˙q = ∆q = −∆0q, but not by, e.g., ˙q = ∆0q."
REFERENCES,0.7200704225352113,"This shows that, while they are not equivalent, there is a close relationship between attention and
diffusion mechanisms on graphs. The closest analogue to the standard attention expressible in this
format is perhaps the choice a1,ij = f (˜a (qi, qj)), a0,ii = P"
REFERENCES,0.721830985915493,"j∈¯
N(i) a1,ij, discussed in Section 4
and Appendix A.4, where f is any scalar-valued positive function. For example, when f(x) = ex, it
follows that"
REFERENCES,0.7235915492957746,(∆0q)i = −1 2 X
REFERENCES,0.7253521126760564,j∈N(i)
REFERENCES,0.727112676056338,"e˜a(qi,qj) + e˜a(qj,qi)"
REFERENCES,0.7288732394366197,"σi
(qj −qi) = −1 2 X"
REFERENCES,0.7306338028169014,j∈N(i)
REFERENCES,0.7323943661971831,"
a (qi, qj) + e˜a(qj,qi) σi"
REFERENCES,0.7341549295774648,"
(qj −qi) ,"
REFERENCES,0.7359154929577465,"which leads to the standard GAT propagation mechanism plus an extra term arising from the fact that
the attention a is not symmetric."
REFERENCES,0.7376760563380281,"Remark A.14. Practically, GATs and their variants typically make use of multi-head attention,
defined in terms of an attention mechanism which is averaged over some number |h| of independent
“heads”,"
REFERENCES,0.7394366197183099,"a (qi, qj) = 1 |h| X"
REFERENCES,0.7411971830985915,"h
ah (qi, qj) ,"
REFERENCES,0.7429577464788732,"which are distinct only in their learnable parameters. While the results of this section were presented
in terms of |h| = 1, the reader can check that multiple attention heads can be used in this framework
provided it is the pre-attention ˜a that is averaged instead."
REFERENCES,0.7447183098591549,"A.6
Bracket derivations and properties"
REFERENCES,0.7464788732394366,"Here the architectures in the body are derived in greater detail. First, it will be shown that L∗= −L,
G∗= G, and M∗= M, as required for structure-preservation."
REFERENCES,0.7482394366197183,"Proposition A.5. For L, G, M defined in Section 4, we have L∗= −L, G∗= G, and M∗= M."
REFERENCES,0.75,"Proof. First, denoting A = diag (A0, A1), it was shown in section A.3 that B∗= A−1B⊺A for
any linear operator B of appropriate dimensions. So, applying this to L, it follows that"
REFERENCES,0.7517605633802817,"L∗=

A−1
0
0
0
A−1
1"
REFERENCES,0.7535211267605634," 
0
−d∗
0
d0
0"
REFERENCES,0.7552816901408451,"⊺
A0
0
0
A1 "
REFERENCES,0.7570422535211268,"=

0
A−1
0 d⊺
0A1
−A−1
1
(d∗
0)⊺A0
0"
REFERENCES,0.7588028169014085,"
=

0
d∗
0
−d0
0"
REFERENCES,0.7605633802816901,"
= −L."
REFERENCES,0.7623239436619719,"Similarly, it follows that"
REFERENCES,0.7640845070422535,"G∗=

A−1
0
0
0
A−1
1"
REFERENCES,0.7658450704225352," 
d∗
0d0
0
0
d∗
1d1"
REFERENCES,0.7676056338028169,"⊺
A0
0
0
A1 "
REFERENCES,0.7693661971830986,"=

A−1
0 d⊺
0 (d∗
0)⊺A0
0
0
A−1
1 d⊺
1 (d∗
1)⊺A1"
REFERENCES,0.7711267605633803,"
=

d∗
0d0
0
0
d∗
1d1"
REFERENCES,0.772887323943662,"
= G,"
REFERENCES,0.7746478873239436,"M∗=

A−1
0
0
0
A−1
1"
REFERENCES,0.7764084507042254," 
0
0
0
A1d∗
1d1A1"
REFERENCES,0.778169014084507,"⊺
A0
0
0
A1 "
REFERENCES,0.7799295774647887,"=

0
0
0
d⊺
1 (d∗
1)⊺A2
1"
REFERENCES,0.7816901408450704,"
=

0
0
0
A1d∗
1d1A1"
REFERENCES,0.7834507042253521,"
= M,"
REFERENCES,0.7852112676056338,"where the second-to-last equality used that A1A−1
1
= I."
REFERENCES,0.7869718309859155,"Remark A.15. Note that the choice of zero blocks in L, G is sufficient but not necessary for these
adjointness relationships to hold. For example, one could alternatively choose the diagonal blocks of
L to contain terms like B −B∗for an appropriate message-passing network B."
REFERENCES,0.7887323943661971,"Next, we compute the gradients of energy and entropy with respect to (·, ·).
Proposition A.6. The A-gradient of the energy"
REFERENCES,0.7904929577464789,"E(q, p) = 1 2"
REFERENCES,0.7922535211267606,"
|q|2 + |p|2
= 1 2 X"
REFERENCES,0.7940140845070423,"i∈V
|qi|2 + 1 2 X"
REFERENCES,0.795774647887324,"α∈E
|pα|2 ,"
REFERENCES,0.7975352112676056,satisfies
REFERENCES,0.7992957746478874,"∇E(q, p) =

A−1
0
0
0
A−1
1"
REFERENCES,0.801056338028169," 
q
p"
REFERENCES,0.8028169014084507,"
=

A−1
0 q
A−1
1 p 
."
REFERENCES,0.8045774647887324,"Moreover, given the energy and entropy defined as"
REFERENCES,0.8063380281690141,"E(q, p) = fE (s(q)) + gE (s (d0d⊺
0p)) ,"
REFERENCES,0.8080985915492958,"S(q, p) = gS (s (d⊺
1d1p)) ,"
REFERENCES,0.8098591549295775,"where fE : RnV →R acts on node features, gE, gS : RnE →R act on edge features, and s denotes
sum aggregation over nodes or edges, the A-gradients are"
REFERENCES,0.8116197183098591,"∇E(q, p) =

A−1
0 1 ⊗∇fE (s(q))
A−1
1 d0d⊺
01 ⊗∇gE (s (d0d⊺
0p))"
REFERENCES,0.8133802816901409,"
,
∇S(q, p) =

0
A−1
1 d⊺
1d11 ⊗∇gS (s (d⊺
1d1p)) 
,"
REFERENCES,0.8151408450704225,"Proof. Since the theory of A-gradients in section A.3 establishes that ∇E = A−1δE, it is only
necessary to compute the L2-gradients. First, letting x = (q
p)⊺, it follows for the first definition
of energy that"
REFERENCES,0.8169014084507042,"dE(x) =
X"
REFERENCES,0.8186619718309859,"i∈V
⟨qi, dqi⟩+
X"
REFERENCES,0.8204225352112676,"α∈E
⟨pα, dpα⟩= ⟨q, dq⟩+ ⟨p, dp⟩= ⟨x, dx⟩,"
REFERENCES,0.8221830985915493,"showing that δE(q, p) = (q
p)⊺, as desired. Moving to the metriplectic definitions, since each
term of E, S has the same functional form, it suffices to compute the gradient of f (s (Bq)) for some
function f : Rnf →R and matrix B : R|V| →R|V|. To that end, adopting the Einstein summation
convention where repeated indices appearing up-and-down in an expression are implicitly summed,
if 1 ≤a, b ≤nf and 1 ≤i, j ≤|V|, we have"
REFERENCES,0.823943661971831,"d (s(q)) =
X"
REFERENCES,0.8257042253521126,"i∈|V|
dqi =
X"
REFERENCES,0.8274647887323944,"i∈|V|
δj
i dqj = 1jdqa
j ea = (1 ⊗I) : dq = ∇(s(q)) : dq,"
REFERENCES,0.829225352112676,"implying that ∇s(q) = 1 ⊗I. Continuing, it follows that"
REFERENCES,0.8309859154929577,"d (f ◦s ◦Bq) = f ′ (s (Bq))a s′ (Bq)a
i Bijdqa
j = f ′ (s (Bq))a ea (B⊺)ij 1j dqa
i
= ⟨∇f (s (Bq)) ⊗B⊺1, dq⟩= ⟨∇(f ◦s ◦Bq) , dq⟩,
showing that ∇(f ◦s ◦B) decomposes into an outer product across modalities. Applying this
formula to the each term of E, S then yields the L2-gradients,"
REFERENCES,0.8327464788732394,"δE(q, p) =

1 ⊗∇fE (s(q))
d0d⊺
01 ⊗∇gE (s (d0d⊺
0p))"
REFERENCES,0.8345070422535211,"
,
δS(q, p) =

0
d⊺
1d11 ⊗∇gS (s (d⊺
1d1p)) 
,"
REFERENCES,0.8362676056338029,from which the desired A-gradients follow directly.
REFERENCES,0.8380281690140845,"Finally, we can show that the degeneracy conditions for metriplectic structure are satisfied by the
network in Section 4.
Theorem A.16. The degeneracy conditions L∇S = M∇E = 0 are satisfied by the metriplectic
bracket in Section A.6."
REFERENCES,0.8397887323943662,"Proof. This is a direct calculation using Theorem 3.1 and Proposition A.6. In particular, it follows
that"
REFERENCES,0.8415492957746479,"L∇S =

0
−d∗
0
d0
0"
REFERENCES,0.8433098591549296," 
0
A−1
1 d⊺
1d11 ⊗∇gS (s (d⊺
1d1p)) "
REFERENCES,0.8450704225352113,"=

−A−1
0
(d1d0)⊺d11 ⊗∇gS (s (d⊺
1d1p))
0"
REFERENCES,0.846830985915493,"
=

0
0 
,"
REFERENCES,0.8485915492957746,"M∇E =

0
0
0
A1d∗
1d1A1"
REFERENCES,0.8503521126760564," 
A−1
0 1 ⊗∇fE (s(q))
A−1
1 d0d⊺
01 ⊗∇gE (s (d0d⊺
0p)) "
REFERENCES,0.852112676056338,"=

0
A1d∗
1(d1d0)d⊺
01 ⊗∇gE (s (d0d⊺
0p))"
REFERENCES,0.8538732394366197,"
=

0
0 
,"
REFERENCES,0.8556338028169014,"since d1d0 = 0 as a consequence of the graph calculus. These calculations establish the validity of the
energy conservation and entropy generation properties seen previously in the manuscript body."
REFERENCES,0.8573943661971831,"Remark A.17. Clearly, this is not the only possible metriplectic formulation for GNNs. On the other
hand, this choice is in some sense maximally general with respect to the chosen operators L, G, since
only constants are in the kernel of d0 (hence there is no reason to include a nodal term in S), and
only elements in the image of d⊺
1 (which do not exist in our setting) are guaranteed to be in the kernel
of d⊺
0 for any graph. Therefore, M is chosen to be essentially G without the ∆0 term, whose kernel
is graph-dependent and hence difficult to design."
REFERENCES,0.8591549295774648,"B
Experimental details and more results"
REFERENCES,0.8609154929577465,"This Appendix provides details regarding the experiments in Section 5, as well as any additional
information necessary for reproducing them. We implement the proposed algorithms with PYTHON
and PYTORCH [71] that supports CUDA. The experiments are conducted on systems that are equipped
with NVIDIA RTX A100 and V100 GPUs. For NODEs capabilities, we use the TORCHDIFFEQ
library [16]."
REFERENCES,0.8626760563380281,"B.1
Damped double pendulum"
REFERENCES,0.8644366197183099,"The governing equations for the damped double pendulum can be written in terms of four coupled
first-order ODEs for the angles that the two pendula make with the vertical axis θ1, θ2 and their
associated angular momenta ω1, ω2 (see [72]),
˙θi = ωi,
1 ≤i ≤2,
(2)"
REFERENCES,0.8661971830985915,"˙ω1 = m2l1ω2
1 sin (2∆θ) + 2m2l2ω2
2 sin (∆θ) + 2gm2 cos θ2 sin ∆θ + 2gm1 sin θ1 + γ1
−2l1
 
m1 + m2 sin2 ∆θ

,
(3)"
REFERENCES,0.8679577464788732,"˙ω2 = m2l2ω2
2 sin (2∆θ) + 2 (m1 + m2) l1ω2
1 sin ∆θ + 2g (m1 + m2) cos θ1 sin ∆θ + γ2
2l2
 
m1 + m2 sin2 ∆θ

,
(4)"
REFERENCES,0.8697183098591549,"where m1, m2, l1, l2 are the masses resp. lengths of the pendula, ∆θ = θ1 −θ2 is the (signed)
difference in vertical angle, g is the acceleration due to gravity, and"
REFERENCES,0.8714788732394366,γ1 = 2k1 ˙θ1 −2k2 ˙θ2 cos ∆θ.
REFERENCES,0.8732394366197183,γ2 = 2k1 ˙θ1 cos ∆θ −2 (m1 + m2)
REFERENCES,0.875,"m2
k2 ˙θ2,"
REFERENCES,0.8767605633802817,"for damping constants k1, k2."
REFERENCES,0.8785211267605634,"Dataset.
A trajectory of the damped double pendulum by solving an initial value problem associated
with the ODE 2. The initial condition used is (1.0, π/2, 0.0, 0.0), and the parameters are m1 = m2 =
1, g = 1, l1 = 1, l2 = 0.9, k1 = k2 = 0.1. For time integrator, we use the TorchDiffeq library [16]
with Dormand–Prince 5 (DOPRI5) as the numerical solver. The total simulation time is 50 (long
enough for significant dissipation to occur), and solution snapshots are collected at 500 evenly-spaced
temporal indices."
REFERENCES,0.8802816901408451,"To simulate the practical case where only positional data for the system is available, the double
pendulum solution is integrated to time T = 50 (long enough for significant dissipation to occur)
and post-processed once the angles and angular momenta are determined from the equations above,
yielding the (x, y)-coordinates of each pendulum mass at intervals of 0.1s. This is accomplished
using the relationships"
REFERENCES,0.8820422535211268,"x1 = l1 sin θ1
y1 = −l1 cos θ1
x2 = x1 + l2 sin θ2 = l1 sin θ1 + l2 sin θ2
y2 = y1 −l2 cos θ2 = −l1 cos θ1 −l2 cos θ2."
REFERENCES,0.8838028169014085,"The double pendulum is then treated as a fully connected three-node graph with positional coordinates
qi = (xi, yi) as nodal features, and relative velocities pα = (d0q)α as edge features. Note that the
positional coordinates (x0, y0) = (0, 0) of the anchor node are held constant during training. To
allow for the necessary flexibility of coordinate changes, each architecture from Section 4 makes
use of a message-passing feature encoder before time integration, acting on node features and edge
features separately, with corresponding decoder returning the original features after time integration."
REFERENCES,0.8855633802816901,"To elicit a fair comparison, both the NODE and NODE+AE architectures are chosen to contain
comparable numbers of parameters to the bracket architectures (∼30k), and all networks are trained
for 100,000 epochs. For each network, the configuration of weights producing the lowest overall
error during training is used for prediction."
REFERENCES,0.8873239436619719,"Hyperparameters.
The networks are trained to reconstruct the node/edge features in mean absolute
error (MAE) using the Adam optimizer [73]. The NODEs and metriplectic bracket use an initial
learning rate of 10−4, while the other models use an initial learning rate of 10−3. The width of the
hidden layers in the message passing encoder/decoder is 64, and the number of hidden features for
nodes/edges is 32. The time integrator used is simple forward Euler."
REFERENCES,0.8890845070422535,"Network architectures.
The message passing encoders/decoders are 3-layer MLPs mapping, in
the node case, nodal features and their graph derivatives, and in the edge case, edge features and
their graph coderivatives, to a hidden representation. For the bracket architectures, the attention
mechanism used in the learnable coderivatives is scaled dot product. The metriplectic network uses
2-layer MLPs fE, gE, gS with scalar output and hidden width 64. For the basic NODE, node and
edge features are concatenated, flattened, and passed through a 4-layer fully connected network of
width 128 in each hidden layer, before being reshaped at the end. The NODE+AE architecture uses a
3-layer fully connected network which operates on the concatenated and flattened latent embedding
of size 32 ∗6 = 192, with constant width throughout all layers."
REFERENCES,0.8908450704225352,"B.2
Mujoco"
REFERENCES,0.8926056338028169,"We represent an object as a fully-connected graph, where a node corresponds to a body part of the
object and, thus, the nodal feature corresponds to a position of a body part or joint. To learn the
dynamics of an object, we again follow the encoder-decoder-type architecture considered in the"
REFERENCES,0.8943661971830986,"(a) q
(b) p
(c) E and S"
REFERENCES,0.8961267605633803,"Figure 5: [Double pendulum] Trajectories of q and p: ground-truth (solid lines) and predictions of
the metriplectic bracket model (dashed lines). The evolution of the energy E and the entropy S over
the simulation time. Note that slight fluctuations appear in E due to the fact that forward Euler is not
a symplectic integrator."
REFERENCES,0.897887323943662,"double-pendulum experiments. First we employ a node-wise linear layer to embed the nodal feature
into node-wise hidden representations (i.e., the nodal feature qi corresponds to a position of a body
part or an angle of a joint.). As an alternative encoding scheme for the nodal feature, in addition to the
position or the angle, nodal velocities are considered as additional nodal features, i.e., qi = (qi, vi).
The experimental results of the alternative scheme is represented in the following section B.2.2."
REFERENCES,0.8996478873239436,"The proposed dynamics models also require edge features (e.g., edge velocity), which are not
presented in the dataset. Thus, to extract a hidden representation for an edge, we employ a linear
layer, which takes velocities of the source and destination nodes of the edge as an input and outputs
edge-wise hidden representations, i.e., the edge feature correspond to a pair of nodal velocities
pα = (vsrc(α), vdst(α)), where vsrc(α) and vdst(α) denote velocities of the source and destination nodes
connected to the edge."
REFERENCES,0.9014084507042254,"The MuJoCo trajectories are generated in the presence of an actor applying controls. To handle the
changes in dynamics due to the control input, we introduce an additive forcing term, parameterized
by an MLP, to the dynamics models, which is a similar approach considered in dissipative SymODEN
[32]. In dissipative SymODEN, the forcing term is designed to affect only the change of the
generalized momenta (also known as the port-Hamiltonian dynamics [74]). As opposed to this
approach, our proposed forcing term affects the evolution of both the generalized coordinates that are
defined in the latent space. Once the latent states are computed at specified time indices, a node-wise
linear decoder is applied to reconstruct the position of body parts of the object. Then the models are
trained based on the data matching loss measured in mean-square errors between the reconstructed
and the ground-truth positions."
REFERENCES,0.903169014084507,"B.2.1
Experiment details"
REFERENCES,0.9049295774647887,We largely follow the experimental settings considered in [23].
REFERENCES,0.9066901408450704,"Dataset.
As elaborated in [23], the standard Open AI Gym [75] environments preprocess observa-
tions in ad-hoc ways, e.g., Hopper clips the velocity observations to [−10, 10]d. Thus, the authors in
[23] modified the environments to simply return the position and the velocity (q, v) as the observation
and we use the same dataset, which is made publicly available by the authors. The dataset consists of
training and test data, which are constructed by randomly splitting the episodes in the replay buffer
into training and test data. Training and test data consist of ∼40K and ∼300 or ∼85 trajectories,
respectively. For both training and test data, we include the first 20 measurements (i.e., 19 transitions)
in each trajectory."
REFERENCES,0.9084507042253521,"Hyperparameters.
For training, we use the Adam optimizer [73] with the initial learning rate 5e-3
and weight decay 1e-4. With the batch size of 200 trajectories, we train the models for 256 epochs.
We also employ a cosine annealing learning rate scheduler with the minimum learning rate 1e-6. For
time integrator, we use the Torchdiffeq library with the Euler method."
REFERENCES,0.9102112676056338,"Network architectures.
The encoder and decoder networks are parameterized as a linear layer
and the dimension of the hidden representations is set to 80. For attention, the scaled dot-product
attention is used with 8 heads and the embedding dimension is set to 16. The MLP for handling the"
REFERENCES,0.9119718309859155,"forcing term consists of three fully-connected layers (i.e., input, output layers and one hidden layer
with 128 neurons). The MLP used for parameterizing the “black-box” NODEs also consists of three
fully-connected layers with 128 neurons in each layer."
REFERENCES,0.9137323943661971,"B.2.2
Additional results"
REFERENCES,0.9154929577464789,"Figure 6 reports the loss trajectories for all considered dynamics models. For the given number of
maximum epochs (i.e., 256), the Hamiltonian and double bracket models tend to reach much lower
training losses (an order of magnitude smaller) errors than the NODE and Gradient models do. The
metriplectic model produces smaller training losses compared to the NODE and gradient models
after a certain number of epochs (e.g., 100 epochs for Hopper)."
REFERENCES,0.9172535211267606,"(a) HalfCheetah
(b) Hopper
(c) Swimmer"
REFERENCES,0.9190140845070423,"Figure 6: [Mujoco] Train MSE over epoch for all considered dynamics models. For the nodal feature,
only the position or the angle of the body part/joint is considered."
REFERENCES,0.920774647887324,"In the next set of experiments, we provide not only positions/angles of body parts as nodal features,
but also velocities of the body parts as nodal features (i.e., qi = (qi, vi)). Table 6 reports the
relative errors measured in L2-norm; again, the Hamiltonian, double bracket, and metriplectic
outperform other dynamics models. In particular, the metriplectic bracket produces the most accurate
predictions in the Hopper and Swimmer environments. Figure 7 reports the loss trajectories for all
considered models. Similar to the previous experiments with the position as the only nodal feature,
the Hamiltonian and Double bracket produces the lower training losses than the NODE and Gradient
models do. For the Hopper and Swimmer environments, however, among all considered models, the
metriplectic model produces the lowest training MSEs after 256 training epochs."
REFERENCES,0.9225352112676056,"Dataset
HalfCheetah
Hopper
Swimmer
NODE+AE
0.0848 ± 0.0011
0.0421 ± 0.0041
0.0135 ± 0.00082
Hamiltonian
0.0403 ± 0.0052
0.0294 ± 0.0028
0.0120 ± 0.00022
Gradient
0.0846 ± 0.00358
0.0490 ± 0.0013
0.0158 ± 0.00030
Double Bracket
0.0653 ± 0.010
0.0274 ± 0.00090
0.0120 ± 0.00060
Metriplectic
0.0757 ± 0.0021
0.0269 ± 0.00035
0.0114 ± 0.00067"
REFERENCES,0.9242957746478874,"Table 6: Relative errors of the network predictions of the MuJoCo environments on the test set,
reported as avg±stdev over 4 runs."
REFERENCES,0.926056338028169,"B.3
Node classification"
REFERENCES,0.9278169014084507,"To facilitate comparison with previous work, we follow the experimental methodology of [61]."
REFERENCES,0.9295774647887324,"(a) HalfCheetah
(b) Hopper
(c) Swimmer"
REFERENCES,0.9313380281690141,"Figure 7: [Mujoco] Train MSE over epoch for all considered dynamics models. For the nodal feature,
along with the position or the angle of the body part/joint, the node velocities are also considered."
REFERENCES,0.9330985915492958,"B.3.1
Experiment details"
REFERENCES,0.9348591549295775,"Datasets.
We consider the three well-known citation networks, Cora [58], Citeseer [59], and
Pubmed [60]; the proposed models are tested on the datasets with the original fixed Planetoid
traing/test splits, as well as random train/test splits. In addition, we also consider the coauthor graph,
CoauthorCS [61] and the Amazon co-purchasing graphs, Computer and Photo [62]. Table 7 provides
some basic statistics about each dataset."
REFERENCES,0.9366197183098591,"Dataset
Cora
Citeseer
PubMed
CoauthorCS
Computer
Photo
Classes
7
6
3
15
10
8
Features
1433
3703
500
6805
767
745
Nodes
2485
2120
19717
18333
13381
7487
Edges
5069
3679
44324
81894
245778
119043"
REFERENCES,0.9383802816901409,Table 7: Dataset statistics.
REFERENCES,0.9401408450704225,"Hyperparameters
The bracket architectures employed for this task are identical to those in
Section 4 except for that the right-hand side of the Hamiltonian, gradient, and double bracket
networks is scaled by a learnable parameter Sigmoid (α) > 0, and the matrix A2 = I is used as the
inner product on 3-cliques. It is easy to verify that this does not affect the structural properties or
conservation character of the networks. Nodal features qi are specified by the datasets, and edge
features pα = (d0q)α are taken as the combinatorial gradient of the nodal features. In order to
determine good hyperparameter configurations for each bracket, a Bayesian search is conducted using
Weights and Biases [76] for each bracket and each dataset using a random 80/10/10 train/valid/test
split with random seed 123. The number of runs per bracket was 500 for CORA, CiteSeer, and
PubMed, and 250 for CoauthorCS, Computer, and Photo. The hyperparameter configurations leading
to the best validation accuracy are used when carrying out the experiments in Table 4 and Table 5."
REFERENCES,0.9419014084507042,"Specifically, the hyperparameters that are optimized are as follows: initial learning rate (from 0.0005
to 0.05), number of training epochs (from 25 to 150), method of integration (rk4 or dopri5), integration
time (from 1 to 5), latent dimension (from 10 to 150 in increments of 10), pre-attention mechanism ˜a
(see below), positive function f (either exp or Squareplus), number of pre-attention heads (from 1 to
15, c.f. Remark A.14), attention embedding dimension (from 1× to 15× the number of heads), weight
decay rate (from 0 to 0.05), dropout/input dropout rates (from 0 to 0.8), and the MLP activation
function for the metriplectic bracket (either relu, tanh, or squareplus). The pre-attention is chosen"
REFERENCES,0.9436619718309859,"from one of four choices, defined as follows:"
REFERENCES,0.9454225352112676,"˜a (qi, qj) = (WKqi)⊺WQqj"
REFERENCES,0.9471830985915493,"d
scaled dot product,"
REFERENCES,0.948943661971831,"˜a (qi, qj) = (WKqi)⊺WQqj"
REFERENCES,0.9507042253521126,"|WKqi| |WQqj|
cosine similarity,"
REFERENCES,0.9524647887323944,"˜a (qi, qj) ="
REFERENCES,0.954225352112676," 
WKqi −WKqi
⊺ 
WQqj −WQqj

WKqi −WKqi
 WQqj −WQqj
 ,
Pearson correlation"
REFERENCES,0.9559859154929577,"˜a (qi, qj) = (σuσx)2 exp "
REFERENCES,0.9577464788732394,−|WKui −WQuj|2 2ℓ2u ! exp 
REFERENCES,0.9595070422535211,−|WKxi −WQxj|2 2ℓ2x !
REFERENCES,0.9612676056338029,",exponential kernel"
REFERENCES,0.9630281690140845,"Network architectures.
The architectures used for this experiment follow that of GRAND [49],
consisting of the learnable affine encoder/decoder networks ϕ, ψ and learnable bracket-based dynam-
ics in the latent space. However, recall that the bracket-based dynamics require edge features, which
are manufactured as pα = (d0q)α. In summary, the inference procedure is as follows:"
REFERENCES,0.9647887323943662,"q(0) = ϕ(q)
(nodal feature encoding),
p(0) = d0q(0)
(edge feature manufacturing),"
REFERENCES,0.9665492957746479,"(q(T), p(T)) = (q(0), p(0)) +
Z T"
REFERENCES,0.9683098591549296,"0
( ˙q, ˙p) dt,
(latent dynamics)"
REFERENCES,0.9700704225352113,"˜q = ψ (q(T)) ,
(nodal feature decoding)
y = c(˜q).
(class prediction)"
REFERENCES,0.971830985915493,Training is accomplished using the standard cross entropy
REFERENCES,0.9735915492957746,"H (t, y) = |V|
X"
REFERENCES,0.9753521126760564,"i=1
t⊺
i log yi,"
REFERENCES,0.977112676056338,"where ti is the one-hot truth vector corresponding to the ith node. In the case of the metriplectic
network, the networks fE, gE, gS are 2-layer MLPs with hidden dimension equal to the latent feature
dimension and output dimension 1."
REFERENCES,0.9788732394366197,"B.3.2
Additional depth study"
REFERENCES,0.9806338028169014,"Here we report the results of a depth study on Cora with the Planetoid train/val/test split. Table 9
shows the train/test accuracy of the different bracket architectures under two different increased depth
conditions, labeled Task 1 and Task 2, respectively. Task 1 refers to fixing the integration step-size at
∆t = 1 and integrating to a variable final time T, while Task 2 instead fixes the final time T to the
value identified by the hyperparameter search (see Table 8) and instead varies the step-size ∆t. Notice
that both tasks involve repeatedly composing the trained network and hence simulate increasing
depth, so that any negative effects of network construction such as oversmoothing, oversquashing, or
vanishing/exploding gradients should appear in both cases. For more information, Table 10 provides
a runtime comparison corresonding to the depth studies in Table 9."
REFERENCES,0.9823943661971831,"Observe that every bracket-based architecture exhibits very stable performance in Task 2, where the
final time is held fixed while the depth is increased. This suggests that our proposed networks are
dynamically stable and effectively mitigate negative effects like oversmoothing which are brought by
repeated composition and often seen in more standard GNNs. Interestingly, despite success on Task
2, only the gradient and metriplectic architectures perfectly maintain or improve their performance
during the more adversarial Task 1 where the final time is increased with a fixed step-size. This
suggests that, without strong diffusion, the advection experienced during conservative dynamics has
the potential to radically change label classification over time, as information is moved through the
feature domain in a loss-less fashion.
Remark B.1. It is interesting that the architecture most known for oversmoothing (i.e., gradient)
exhibits the most improved classification performance with increasing depth on Task 1. This is
perhaps due to the fact that the gradient system decouples over nodes and edges, while the others"
REFERENCES,0.9841549295774648,"do not, meaning that the gradient network does not have the added challenge of learning a useful
association between the manufactured edge feature information and the nodal labels. It remains to
be seen if purely node-based bracket dynamics exhibit the same characteristics as the node-edge
formulations presented here."
REFERENCES,0.9859154929577465,"CORA networks
Trainable Parameters
Integration Time
Hamiltonian
60723
1.49625
Gradient
160772
14.82404
Double Bracket
30718
5.36151
Metriplectic
104088
7.53107"
REFERENCES,0.9876760563380281,"Table 8: The integration time and number of trainable parameters corresponding to the best networks
trained on CORA. Note that integration time can be considered as a surrogate for depth, since the
temporal step-size of each network is fixed to 1."
REFERENCES,0.9894366197183099,"Depth study (acc)
1 layer
2 layers
4 layers
8 layers
16 layers
32 layers
64 layers
Hamiltonian
75.0 ± 1.4
77.9 ± 1.0
62.0 ± 0.6
38.8 ± 1.0
32.0 ± 0.8
25.4 ± 0.5
17.1 ± 1.3
Gradient
69.6 ± 1.1
72.7 ± 1.0
74.5 ± 1.1
77.7 ± 1.0
80.2 ± 1.2
81.4 ± 1.0
82.0 ± 1.2
Double Bracket
77.8 ± 1.1
81.2 ± 0.8
83.9 ± 1.1
83.8 ± 1.0
80.1 ± 1.3
58.9 ± 1.0
19.3 ± 1.4
Metriplectic
61.0 ± 1.6
62.4 ± 1.9
62.0 ± 0.8
61.9 ± 1.3
61.6 ± 1.0
60.6 ± 1.3
61.2 ± 1.0"
REFERENCES,0.9911971830985915,"Hamiltonian
77.2 ± 0.8
77.5 ± 1.2
77.4 ± 1.2
77.0 ± 0.6
78.0 ± 0.7
77.8 ± 1.2
77.4 ± 0.7
Gradient
79.9 ± 1.4
79.9 ± 0.6
79.6 ± 0.6
79.6 ± 1.1
80.4 ± 1.1
80.5 ± 0.9
79.8 ± 1.1
Double Bracket
84.2 ± 0.9
84.5 ± 1.3
84.1 ± 0.5
84.2 ± 1.3
84.1 ± 0.8
83.8 ± 0.9
84.2 ± 1.0
Metriplectic
61.7 ± 1.4
61.0 ± 0.9
61.0 ± 1.0
61.4 ± 1.3
61.6 ± 1.4
61.7 ± 1.2
61.9 ± 1.1"
REFERENCES,0.9929577464788732,"Table 9: Accuracy results of a depth study on CORA. Test accuracies reported as mean±stdev over
10 runs with random train/valid/test splits. Top/bottom groups correspond to tasks 1 and 2 of the
study, respectively, where Task 1 uses a fixed step-size while Task 2 uses a fixed integration domain."
REFERENCES,0.9947183098591549,"Depth study (time)
1 layer
2 layers
4 layers
8 layers
16 layers
32 layers
64 layers
Hamiltonian
0.038 ± 0.003
0.060 ± 0.006
0.106 ± 0.011
0.191 ± 0.035
0.308 ± 0.035
0.497 ± 0.045
0.874 ± 0.035
Gradient
0.053 ± 0.005
0.091 ± 0.011
0.159 ± 0.015
0.273 ± 0.023
0.470 ± 0.031
0.809 ± 0.037
1.44 ± 0.038
Double Bracket
0.068 ± 0.007
0.125 ± 0.010
0.232 ± 0.023
0.434 ± 0.039
0.770 ± 0.068
1.36 ± 0.098
2.52 ± 0.103
Metriplectic
0.161 ± 0.010
0.305 ± 0.011
0.574 ± 0.014
1.10 ± 0.012
2.13 ± 0.040
4.11 ± 0.075
7.86 ± 0.129"
REFERENCES,0.9964788732394366,"Hamiltonian
0.052 ± 0.009
0.067 ± 0.013
0.114 ± 0.021
0.200 ± 0.040
0.350 ± 0.054
0.613 ± 0.056
1.17 ± 0.084
Gradient
0.365 ± 0.020
0.680 ± 0.012
1.26 ± 0.022
2.26 ± 0.058
4.22 ± 0.069
8.38 ± 0.451
19.4 ± 0.152
Double Bracket
0.241 ± 0.036
0.394 ± 0.046
0.731 ± 0.057
1.44 ± 0.132
2.98 ± 0.286
5.42 ± 0.609
9.44 ± 0.487
Metriplectic
1.05 ± 0.051
2.05 ± 0.097
3.87 ± 0.100
7.35 ± 0.149
14.1 ± 0.332
27.2 ± 0.378
53.8 ± 0.370"
REFERENCES,0.9982394366197183,"Table 10: Runtime results of a depth study on CORA. Wall clock times reported as mean±stdev over
10 runs with random train/valid/test splits. Top/bottom groups correspond to tasks 1 and 2 of the
study, respectively, where Task 1 uses a fixed step-size while Task 2 uses a fixed integration domain."
