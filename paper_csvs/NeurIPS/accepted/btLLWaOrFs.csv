Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0020491803278688526,"Deep learning-based feature matching has shown great superiority for point cloud
registration in the absence of pose priors. Although coarse-to-fine matching ap-
proaches are prevalent, the coarse matching of existing methods is typically sparse
and loose without consideration of geometric consistency, which makes the sub-
sequent fine matching rely on ineffective optimal transport and hypothesis-and-
selection methods for consistency. Therefore, these methods are neither efficient
nor scalable for real-time applications such as odometry in robotics. To address
these issues, we design a consistency-aware spot-guided Transformer (CAST),
which incorporates a spot-guided cross-attention module to avoid interfering with
irrelevant areas, and a consistency-aware self-attention module to enhance match-
ing capabilities with geometrically consistent correspondences. Furthermore, a
lightweight fine matching module for both sparse keypoints and dense features can
estimate the transformation accurately. Extensive experiments on both outdoor
LiDAR point cloud datasets and indoor RGBD point cloud datasets demonstrate
that our method achieves state-of-the-art accuracy, efficiency, and robustness. Our
code is available at https://github.com/RenlangHuang/CAST."
INTRODUCTION,0.004098360655737705,"1
Introduction"
INTRODUCTION,0.006147540983606557,"Point cloud registration is a fundamental yet crucial task for a variety of 3D vision and robotic
applications, such as simultaneous localization and mapping (SLAM) [1], object pose estimation [2]
and structure from motion (SfM) [3]. Aiming at aligning two partially overlapped point clouds,
the typical approach involves a two-stage pipeline: data association which establishes reliable
point correspondences, and pose estimation. However, establishing these correspondences has been
challenging due to the noisy, irregular, non-uniform, and textureless nature of 3D point clouds."
INTRODUCTION,0.00819672131147541,"Feature matching has long been the mainstream of data association without pose priors. Extensive
research has made advances in distinctive local feature representations, ranging from hand-crafted
descriptors [4, 5, 6] to recent learning-based descriptors [7, 8, 9]. Although the emerging learning-
based descriptors significantly improve the reliability of correspondences, the inlier ratio still falls
short of what is required for robust and efficient pose estimation. Recently, coarse-to-fine matching is
a thriving framework for 2D-2D [10], 3D-3D [11, 12, 13], and even 2D-3D [14] data association. It
has been a consensus that Transformers stacked by alternate self-attention and cross-attention modules
are effective for coarse matching, which are inspired by human visual processes. Typically, humans
may first scan through the point clouds to identify and match salient landmarks across different point
clouds reliably. For less salient points, the geometric relationships between them and those salient
landmarks would be utilized to revisit their potential correspondences. The correspondences will
eventually be established for the entire point cloud after several iterations of this process."
INTRODUCTION,0.010245901639344262,âˆ—Corresponding author
INTRODUCTION,0.012295081967213115,"Unfortunately, existing coarse matching approaches tend to be sparse and loose without consideration
of geometric consistency. An important reason for looseness is that global cross-attention inevitably
attends to similar yet irrelevant areas, resulting in misleading feature aggregation and consequent
inconsistent correspondences that undermine both robustness and accuracy. As a result, the hypothesis-
and-selection pipeline such as RANSAC [15] is commonly used for outlier rejection, which is typically
inaccurate and inefficient, especially for numerous samples with low inlier ratio. Furthermore, the
sparsity necessitates the use of complicated fine matching such as optimal transport-based algorithms
to establish reliable dense correspondences. Due to iterative dense matrix operations for patch-to-
patch correspondences established by coarse matching, these fine matching methods are neither
efficient nor scalable for real-time large-scale applications such as odometry."
INTRODUCTION,0.014344262295081968,"To this end, we attempt to design an efficient and scalable coarse-to-fine matching network based
on consistency-aware semi-dense coarse correspondences. Inspired by ASTR [10] for 2D feature
matching, we leverage local consistency to direct the cross-attention of each point exclusively
to corresponding patches of its confident neighbors, which is referred to as spot-guided cross-
attention. Unlike [10], we propose a novel consistency-aware matching confidence criterion to sample
reliable neighbors based on both feature similarity and geometric compatibility. Additionally, we
design a consistency-aware self-attention module to enhance the distinctiveness of coarse feature
representations via aggregation with salient nodes from the compatibility graph. Notably, both spot-
guided cross-attention and consistency-aware self-attention are efficient sparse attention mechanisms."
INTRODUCTION,0.01639344262295082,"For scalability to real-time applications such as odometry with pose priors, we propose a lightweight
fine matching module allowing independent deployment without coarse matching. The scalability
is credited to flexible point-to-patch local matching instead of optimal transport heavily relying on
patch-to-patch correspondences. In addition, our fine matching adopts a sparse-to-dense registration
pipeline, benefiting from the efficiency of sparse keypoint matching and the accuracy of dense
registration. Furthermore, an efficient compatibility graph embedding module is leveraged for outlier
rejection as a substitute for inefficient hypothesis-and-selection pipelines."
INTRODUCTION,0.018442622950819672,"In summary, our main contributions are as follows:"
INTRODUCTION,0.020491803278688523,"â€¢ A consistency-aware spot-guided Transformer (CAST) with multi-scale feature fusion for
much tighter coarse matching with a focus on geometric consistency."
INTRODUCTION,0.022540983606557378,"â€¢ A spot-guided cross-attention module with a consistency-aware matching confidence crite-
rion that can maintain local consistency without interfering with irrelevant areas."
INTRODUCTION,0.02459016393442623,"â€¢ A consistency-aware self-attention module based on sparse sampling from the compatibility
graph to enhance global consistency during feature aggregation."
INTRODUCTION,0.02663934426229508,"â€¢ A lightweight and scalable sparse-to-dense matching module involving both sparse key-
points and dense features to achieve lower registration errors without optimal transport and
hypothesis-and-selection pipelines."
RELATED WORK,0.028688524590163935,"2
Related Work"
RELATED WORK,0.030737704918032786,"3D Feature Descriptors.
Feature matching plays a crucial role in point cloud registration, enabling
the establishment of reliable correspondences without pose priors. Early methods use hand-crafted
descriptors based on signatures [6] or histograms [4, 5] to represent local geometric features. Recently,
learning-based 3D descriptors have showcased greater performance than hand-crafted ones, which
are usually trained in a self-supervised manner by maximizing the similarity between descriptors of
true correspondences and minimizing the similarity otherwise. 3DMatch [16] and PerfectMatch [7]
leverage 3D CNNs to learn local patch-wise descriptors from 3D patches converted into voxels
of truncated distance function (TDF) values and smoothed density value (SDV) representations,
respectively. PPFNet [17] extracts global context-aware patch-wise descriptors based on PointNet [18].
FCGF [8] employs a sparse 3D convolutional encoder-decoder network for dense descriptor learning.
SpinNet [19] proposes a 3D cylindrical convolution network to extract rotation-invariant patch-wise
descriptors. Predator [20] utilizes graph convolution and cross-attention to enhance the descriptors
and predict the overlapping regions for robust performance in low overlap scenarios."
RELATED WORK,0.03278688524590164,"3D Keypoint Detectors.
Detection-based methods have been widely studied in image matching but
less developed for 3D point clouds. Existing 3D keypoint detectors are mainly hand-crafted, which"
RELATED WORK,0.03483606557377049,"extract salient points based on unique geometric features such as specific curvatures [21] or principal
directions [22]. However, they suffer from noisy, sparse, and non-uniform real-world point clouds
with large-scale transformations. Recent advances include learning-based detectors such as USIP [23]
that predicts repeatable keypoints by minimizing a probabilistic chamfer loss, and HRegNet [24] that
further utilizes weighted farthest point sampling to select sparse keypoints from the predicted ones
for hierarchical registration. 3DFeat-Net [25] extracts patch-wise descriptors with saliency scores
for keypoint selection in a weakly supervised manner by minimizing a weighted feature alignment
triplet loss. D3Feat [9] adopts a fully convolutional network to predict point-wise descriptors with
hand-crafted saliency scores by minimizing a self-supervised detection loss."
RELATED WORK,0.036885245901639344,"3D Correspondence Learning.
DCP [26] predicts soft correspondences from learned features
and estimates the pose by a differential SVD layer. IDAM [27] designs iterative distance-aware
similarity matrix convolution for iterative pairwise matching and pose estimation. Recently, coarse-
to-fine correspondence learning has been regarded as a promising approach. The pioneering work
CoFiNet [11] exploits a group of self-attention and cross-attention for coarse feature matching
and the optimal transport for fine matching. GeoTransformer [12] proposes a geometric structure
embedding for self-attention, and the local-to-global registration (LGR) for consistent pose estimation.
RoITr [28] improves the coarse-to-fine framework with a rotation-invariant point cloud Transformer
based on point pair features, while PEAL [29] and DiffusionPCR [30] use overlap priors and diffusion
models for iterative feature matching, respectively. For outlier rejection, RANSAC [15] remains
popular despite its inefficiency. DGR [31] predicts correspondence-wise confidence scores via a
6D convolutional network, while PointDSC [32] designs a consistency-guided non-local feature
embedding to sample consistent correspondences for neural spectral matching and pose estimation."
METHOD,0.0389344262295082,"3
Method"
METHOD,0.040983606557377046,"In this section, we present the proposed consistency-aware spot-guided Transformer (CAST) with a
lightweight sparse-to-dense fine matching module for accurate and efficient point cloud registration."
OVERVIEW,0.0430327868852459,"3.1
Overview"
OVERVIEW,0.045081967213114756,"Given two partially overlapped point clouds X = {xi âˆˆR3, i = 1, Â· Â· Â· , M} and Y = {yj âˆˆ
R3, j = 1, Â· Â· Â· , N}, the point cloud registration problem can be formulated as solving the optimal
rigid transformation between X, Y by minimizing the weighted sum of point-to-point errors of a
predicted correspondence set C with a confidence weight wk for each correspondence (xk, yk):"
OVERVIEW,0.0471311475409836,"min
R,t X"
OVERVIEW,0.04918032786885246,"(xk,yk)âˆˆC
wkâˆ¥Rxk + t âˆ’ykâˆ¥2
2,
(1)"
OVERVIEW,0.05122950819672131,"where R âˆˆSO(3) and t âˆˆR3 are the rotation and the translation between X and Y, respectively."
OVERVIEW,0.05327868852459016,"As depicted in Figure 1, CAST follows a coarse-to-fine feature matching and registration architec-
ture, including a feature pyramid network, a consistency-aware spot-guided attention-based coarse
matching module, and a sparse-to-dense fine matching module. We first utilize a KPConv-based fully
convolutional network [33] to extract multi-scale features. We denote feature maps of the decoder with
the size of 1/k as F1/k = {F1/k
X , F1/k
Y
}, which correspond to nodes X1/k and Y1/k down-sampled
from X, Y, respectively. For coarse matching, we first adopt an efficient linear cross-attention [34]
module to enhance F1/4. Then both semi-dense features F1/4 and coarse features F1/8 are fed into
a consistency-aware spot-guided attention-based coarse matching module to improve the feature
distinctiveness. The similarity matrix S âˆˆRM â€²Ã—Nâ€² between these enhanced semi-dense features
Ë†FX âˆˆRM â€²Ã—D, Ë†FY âˆˆRNâ€²Ã—D is computed based on inner product: S = Ë†FX Ë†FT
Y . Furthermore, we
fed Ë†FX and Ë†FY into a point-wise MLP to predict the overlap scores, which encode the likelihood of
a node having a correspondence. We perform dual-softmax on S to obtain the final matching scores:"
OVERVIEW,0.055327868852459015,"Pij = Ë†oX
i Ë†oY
j
softmax
kâˆˆ{1,Â·Â·Â· ,M â€²}(Skj)i softmax
kâˆˆ{1,Â·Â·Â· ,N â€²}(Sik)j,
(2)"
OVERVIEW,0.05737704918032787,"where Ë†oX
i and Ë†oY
j are predicted overlap scores of the i-th node of X1/4 and the j-th node of Y1/4,
respectively. We use the mutual nearest neighbor scheme to select confident coarse correspondences."
OVERVIEW,0.05942622950819672,Similarity Matrix MLP
OVERVIEW,0.06147540983606557,Correspondences
OVERVIEW,0.06352459016393443,"à·¡ð‘, Æ¸ð­ ð‘­ðŸ/ðŸ– Ã— ð‘³ ð‘­ðŸ/ðŸ"
OVERVIEW,0.06557377049180328,Self-Attention
OVERVIEW,0.06762295081967214,Cross-Attention
OVERVIEW,0.06967213114754098,Linear
OVERVIEW,0.07172131147540983,Cross-Attention
OVERVIEW,0.07377049180327869,Consistency-Aware
OVERVIEW,0.07581967213114754,Self-Attention
OVERVIEW,0.0778688524590164,Spot-Guided
OVERVIEW,0.07991803278688525,Cross-Attention â€¦
OVERVIEW,0.08196721311475409,Overlap Scores
OVERVIEW,0.08401639344262295,Matching Scores Ã—
OVERVIEW,0.0860655737704918,"Ã—
ð‘­ðŸ/ðŸ’
à·¡ð‘­ð‘¿ ð‘º à·¡ð‘­ð’€ ð‘º à·ð’ð’€ à·ð’ð‘¿ à·¡ð‘­ð‘¿ (ð’) à·¡ð‘­ð’€ (ð’) à·©ð‘­ð’€ (ð’) à·©ð‘­ð‘¿ (ð’)"
OVERVIEW,0.08811475409836066,Upsample&Fuse
OVERVIEW,0.09016393442622951,Downsample&Fuse
OVERVIEW,0.09221311475409837,Graph Sampling
OVERVIEW,0.0942622950819672,Spot Selection
OVERVIEW,0.09631147540983606,Signs:
OVERVIEW,0.09836065573770492,"Transformer-based Coarse Matching ð‘­ð‘¿ ðŸ/ðŸ– ð‘­ð’€ ðŸ/ðŸ– ð‘¿
ð’€ ð‘­ð‘¿ ðŸ/ðŸ’ ð‘­ð’€"
OVERVIEW,0.10040983606557377,"ðŸ/ðŸ’
Keypoint Detector"
OVERVIEW,0.10245901639344263,and Descriptor
OVERVIEW,0.10450819672131148,Nearest
OVERVIEW,0.10655737704918032,Assignment kNN
OVERVIEW,0.10860655737704918,Grouping â€¦
OVERVIEW,0.11065573770491803,"ð±ð’Š
ð²ð’Š
Keypoints"
OVERVIEW,0.11270491803278689,"ð‘¿ðŸ/ðŸ’, ð‘­ð‘¿"
OVERVIEW,0.11475409836065574,"ðŸ/ðŸ’
ð’€ðŸ/ðŸ’, ð‘­ð’€ ðŸ/ðŸ’"
OVERVIEW,0.1168032786885246,Compatibility
OVERVIEW,0.11885245901639344,Graph Embedding
OVERVIEW,0.12090163934426229,Single-Head Attention
OVERVIEW,0.12295081967213115,Keypoint-to-Patch
OVERVIEW,0.125,Correspondences â€¦
OVERVIEW,0.12704918032786885,"ð±ð’Š
à·ð’šð’Šà·¡ð’…ð’Š"
OVERVIEW,0.1290983606557377,Confidence Weights
OVERVIEW,0.13114754098360656,Sparse-to-Dense Fine Matching
OVERVIEW,0.13319672131147542,Sparse Matching
OVERVIEW,0.13524590163934427,Dense Matching
OVERVIEW,0.13729508196721313,Weighted SVD
OVERVIEW,0.13934426229508196,"à·¡ð‘ðŸŽ, Æ¸ð­ðŸŽ
Weighted SVD"
OVERVIEW,0.1413934426229508,"Local 
Attention"
OVERVIEW,0.14344262295081966,"Alignment â€¦
â€¦"
OVERVIEW,0.14549180327868852,"Figure 1: Overview of CAST. The feature pyramid network down-samples the point clouds and learns
features in multiple resolutions. The coarse matching module extracts consistency-aware semi-dense
correspondences via a group of alternate consistency-aware self-attention modules and spot-guided
cross-attention modules with multi-scale feature fusion. Finally, the fine matching module predicts
correspondences for both sparse keypoints and dense features and estimates the transformation."
OVERVIEW,0.14754098360655737,"For efficient fine matching, we extract a keypoint from the neighborhood of each semi-dense node in
X1/4, and predict its virtual correspondence in Y1/4 based on the lightweight single-head attention.
Then we utilize compatibility graph embedding to predict the confidence of these keypoint correspon-
dences as weights in Eq. 1 for initial pose estimation. Finally, a lightweight local attention module
for dense points X1/2 and Y1/2 predicts dense correspondences to refine the pose."
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.14959016393442623,"3.2
Consistency-Aware Spot-Guided Attention"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.15163934426229508,"To tackle the sparsity and looseness of coarse matching, we focus on feature aggregation among
semi-dense features F1/4 leveraging both local and global geometric consistency. To be specific,
the self-attention only attends to salient nodes sampled from a global compatibility graph, while the
cross-attention only attends to nodes sampled based on local consistency, which are referred to as
consistency-aware self-attention and spot-guided cross-attention, respectively."
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.15368852459016394,"Preliminaries.
Transformers stacked by alternate self-attention and cross-attention have showcased
advanced performance in coarse feature matching. When D-dimensional features FA attends to FB,
the output of vanilla attention is formulated as:"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.1557377049180328,"Ë†FA = softmax
 1
âˆš"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.15778688524590165,"D
FAWQ(FBWK)T

FBWV ,
(3)"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.1598360655737705,"where WQ, WK, WV are learnable linear transformations to generate queries, keys, and values.
When FA, FB related to coordinates PA, PB are from the same point cloud, it becomes self-attention
that requires positional encoding to embed spatial information. To encode the 3D relative positions,
we equip the rotary positional embedding [35] ËœR(Â·) with learnable weights b1, Â· Â· Â· , bD/2 âˆˆR1Ã—3:"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.16188524590163936,ËœR(p) = ï£® ï£¯ï£°
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.16393442622950818,"R(b1p)
0
...
0
R(bD/2p) ï£¹"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.16598360655737704,"ï£ºï£», R(Î¸) =

cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.1680327868852459,"
, âˆ€p âˆˆR3.
(4)"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.17008196721311475,"When applying ËœR(Â·) to vanilla self-attention, the output is formulated as:"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.1721311475409836,"Ë†FA = softmax
 1
âˆš"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.17418032786885246,"D
FAWQ ËœR(PA)(FBWK ËœR(PB))T

FBWV .
(5)"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.1762295081967213,"Architecture.
As both spot-guided cross-attention and consistency-aware self-attention are sparse
attention lacking of abundant global context, we propose to enhance the semi-dense features via
multi-scale feature fusion with coarse features. Hence, the architecture of our coarse matching module
is designed as a sequence of blocks for attention-based multi-scale feature aggregation. For each"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.17827868852459017,Vanilla Global Attention     Spot-Guided Attention
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.18032786885245902,Consistency-Aware
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.18237704918032788,Self-Attention
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.18442622950819673,Spot-Guided
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.1864754098360656,Cross-Attention
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.1885245901639344,Matching Matrix
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.19057377049180327,Compatibility Graph
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.19262295081967212,Generalized
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.19467213114754098,"Degrees
Matching"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.19672131147540983,Scores
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.1987704918032787,"Consistency-Aware 
Matching Confidence
Graph Sampling"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.20081967213114754,Spot Selection Spot
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.2028688524590164,"Figure 2: Illustration of consistency-aware self-attention and spot-guided cross-attention (Left),
as well as visualization of the global cross-attention and spot-guided cross-attention (Right). For
the left part, the green nodes are query nodes, while the red ones with correct correspondences
(green dot lines) are reliable neighbors, and the blue one with a false correspondence (red dot line)
is an unreliable neighbor. The self-attention (black lines) only attends to salient nodes while the
cross-attention (black lines) only attends to spots (nodes within black circles)."
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.20491803278688525,"block with both semi-dense features F1/4 and coarse features F1/8 as inputs, we first feed F1/8 into
a self-attention module (Eq. 5) and a cross-attention module (Eq. 3). Then F1/4 and F1/8 are fused
into each other based on nearest up-sampling and distance-based interpolated down-sampling [18]:
Ë†F1/4 = F1/4 + MLP(Nearest Up-sampling(F1/8)),
Ë†F1/8 = F1/8 + MLP(Interpolated Down-sampling(F1/4)).
(6)"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.2069672131147541,"Finally, Ë†F1/4 is fed into a consistency-aware self-attention module and a spot-guided cross-attention
module at the end of each block. Before these sparse attention modules, we need to match the
semi-dense features and evaluate the geometric consistency as a clue to select sparse yet instructive
tokens. Given semi-dense features Ë†F(l)
X , Ë†F(l)
Y in the l-th block, the matching score is formulated as:"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.20901639344262296,"P(l)
ij =
softmax
kâˆˆ{1,Â·Â·Â· ,M â€²}(S(l)
kj )i softmax
kâˆˆ{1,Â·Â·Â· ,N â€²}(S(l)
ik )j, S(l) = Ë†F(l)
X (Ë†F(l)
Y )T.
(7)"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.21106557377049182,"Then the correspondence of each node can be obtained as the node from another point cloud with the
highest matching score, forming a correspondence set C(l) = {(xS
i , yS
i ) : xS
i âˆˆX1/4, yS
i âˆˆY1/4}.
An insight about the consistency among correspondences is that the distance between two points is
invariant after transformation. Hence, geometric compatibility is adopted as a simple yet effective
measure of consistency [32], which is based on the length difference between pairwise line segments.
Given a pre-defined threshold Ïƒc, the pair-wise geometric compatibility of C(l) is formulated as:"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.21311475409836064,"Î²ij =

1 âˆ’d2
ij/Ïƒ2
c
+ , dij =
âˆ¥xS
i âˆ’xS
j âˆ¥2 âˆ’âˆ¥yS
i âˆ’yS
j âˆ¥2
 .
(8)"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.2151639344262295,"The compatibility matrix Bc = [Î²ij]M â€²Ã—Nâ€² is also considered as the adjacency matrix of a weighted
undirected graph known as the compatibility graph, where each vertex is a pair of correspondence
and the edge connectivity corresponds to the compatibility between two correspondences. Intuitively,
we adopt the generalized degree of a pair of correspondence in the graph as a measure of global
consistency, which quantifies the connectivity of a vertex as the sum of edge weights connected to it."
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.21721311475409835,"Consistency-Aware Self-Attention.
Intuitively, the correspondences of less salient nodes can
be effectively located based on the geometric relationships between them and the salient ones.
Hence, compared with global self-attention that attends to all nodes, attending to only salient nodes
is more efficient and effective to encode the geometric context for matching. We propose the
consistency-aware self-attention that samples sparse salient nodes to be attended to based on both
geometric consistency and feature similarity. Given the correspondence set C(l) with a compatibility
graph, we perform two-stage sampling by ranking the generalized degrees and matching scores,
respectively. The first-stage graph sampling using generalized degrees can obtain sufficient consistent
correspondences as proposals. The second-stage sampling based on matching scores can further
obtain sparse salient nodes from these proposals. Finally, semi-dense features Ë†F(l)
X , Ë†F(l)
Y only attend
to features of salient nodes from the same point cloud for feature aggregation according to Eq. 5."
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.2192622950819672,"Spot-Guided Cross-Attention.
As shown in Figure 2, global cross-attention tends to aggregate
features from many irrelevant regions with similar patterns, leading to false correspondences. Inspired
by local consistency that the correspondences of adjacent 3D points remain close to each other,
we design the spot-guided cross-attention as depicted in Figure 2. For each node xS
i such that
(xS
i , yS
i ) âˆˆC(l), we select a subset Ns(xS
i ) from its neighborhood N(xS
i ) as seeds, and construct
a region of interest for it as S(xS
i ) = S"
CONSISTENCY-AWARE SPOT-GUIDED ATTENTION,0.22131147540983606,"xS
k âˆˆNs(xS
i ) N(yS
k ), namely its spot. Ns(xS
i ) selects xS
i
and only its neighbors with reliable correspondences. We propose a consistency-aware matching
confidence criterion to rank the neighbors, which is formulated as the product of the matching score
and the normalized generalized degree in the compatibility graph. This criterion incorporates feature
similarity and geometric consistency to properly measure the reliability of correspondences for seed
selection. Finally, semi-dense features attend to their spots for feature aggregation according to Eq. 3.
Under the guarantee of local consistency, the spots are likely to cover the true correspondences,
providing guidance for feature aggregation without interfering with irrelevant areas."
SPARSE-TO-DENSE FINE MATCHING,0.22336065573770492,"3.3
Sparse-to-Dense Fine Matching"
SPARSE-TO-DENSE FINE MATCHING,0.22540983606557377,"Given a coarse correspondence set Ë†C = {(xS
j , yS
j ) : xS
j âˆˆX1/4, yS
j âˆˆY1/4} selected as mutual
nearest neighbors from the final coarse matching scores (Eq. 2), we propose a lightweight sparse-to-
dense fine matching module for hierarchical pose estimation without optimal transport, maintaining
scalability and efficiency. For sparse matching, we first search k-nearest neighbors (kNN) of semi-
dense nodes X1/4 among dense points X1/2 to group patches, then we use an attentive keypoint
detector [36] to predict a repeatable keypoint with a descriptor from each patch. Each keypoint of
point cloud X is assigned to its nearest node, and each node yS
j âˆˆY1/4 with a correspondence in Ë†C
groups a patch P(yS
j ) of keypoints via kNN. Then, a keypoint xi assigned to xS
j will correspond to the
patch P(yS
j ), forming a pair of keypoint-to-patch correspondence. Finally, we utilize a shared single-
head attention layer for each keypoint-to-patch correspondence to predict virtual correspondences
for keypoints. Denote the descriptor of xi as dX
i , the virtual correspondence Ë†yi with feature Ë†dY
i is
predicted from keypoints yi1, Â· Â· Â· , yik with top-k descriptor similarity in P(yS
j ) as: Ë†yi = k
X"
SPARSE-TO-DENSE FINE MATCHING,0.22745901639344263,"j=1
softmax(dX
i WQ(dY
ijWK)T)yik, Ë†dY
i = k
X"
SPARSE-TO-DENSE FINE MATCHING,0.22950819672131148,"j=1
softmax(dX
i WQ(dY
ijWK)T)dY
ik,
(9)"
SPARSE-TO-DENSE FINE MATCHING,0.23155737704918034,"where dY
i1, Â· Â· Â· , dY
ik are descriptors of yi1, Â· Â· Â· , yik, and WQ and WK are learnable weights. Inspired
by PointDSC [32], we construct a compatibility graph B (Eq. 8) of sparse keypoint correspondences
{(xi, Ë†yi)} for spatial consistency filtering via compatibility graph embedding:"
SPARSE-TO-DENSE FINE MATCHING,0.2336065573770492,"E(l+1) = softmax

1
âˆšDe
E(l)W(l)
Q (E(l)W(l)
K )T âŠ™B

E(l)W(l)
V , E(0)
i
= MLP([xi, dX
i , Ë†yi, Ë†dY
i ]),"
SPARSE-TO-DENSE FINE MATCHING,0.23565573770491804,"(10)
where E(l) is the correspondence-wise embedding of the l-th layer with learnable weights W(l)
Q ,"
SPARSE-TO-DENSE FINE MATCHING,0.23770491803278687,"W(l)
K , W(l)
V . Finally, the embedding is fed into an MLP to classify if a correspondence is an inlier.
The predicted inlier confidences serve as the weights of keypoint correspondences for pose estimation
formulated as Eq. 1, which can be analytically solved by weighted Kabsch algorithm [37]. It is
noteworthy that the above process is really lightweight and scalable to large-scale registration tasks."
SPARSE-TO-DENSE FINE MATCHING,0.23975409836065573,"After aligning two point clouds based on sparse matching, we propose to refine the transformation
based on dense matching. We still utilize local attention (Eq. 9) to predict the correspondences of
dense points X1/2 from its neighbors in Y1/2 within a radius Rd, and we simply set the confidence
weight of a correspondence with a distance d as w = [1 âˆ’d/Rd]+. By solving Eq. 1 again with both
sparse and dense correspondences, we can achieve more accurate pose estimation efficiently."
LOSS FUNCTIONS,0.24180327868852458,"3.4
Loss Functions"
LOSS FUNCTIONS,0.24385245901639344,"Our loss function needs to supervise four modules, i.e., keypoint detection, coarse matching, keypoint
matching, and dense registration. For keypoint detection, we utilize the probabilistic chamfer loss [23]
Lp to minimize the distances between the closest keypoints from the source and target point clouds
after alignment under the ground-truth transformation. Please refer to [23] for details."
LOSS FUNCTIONS,0.2459016393442623,"Coarse Matching.
Given the ground-truth coarse correspondence set C with an overlap ratio oij
for each correspondence (i, j) âˆˆC, we propose a spot matching loss Ls and a coarse matching loss
Lc formulated as weighted cross entropy losses to supervise the layer-wise coarse matching scores
P(l) (l = 1, 2, Â· Â· Â· , L) and the final coarse matching scores P, respectively:"
LOSS FUNCTIONS,0.24795081967213115,"Ls = âˆ’1 L L
X l=1"
P,0.25,"1
P"
P,0.2520491803278688,"(i,j)âˆˆC oij X"
P,0.2540983606557377,"(i,j)âˆˆC
oij log P(l)
ij ,
(11)"
P,0.25614754098360654,"Lc = âˆ’
1
P"
P,0.2581967213114754,"(i,j)âˆˆC oij X"
P,0.26024590163934425,"(i,j)âˆˆC
oij log Pij âˆ’
1
|NX| X"
P,0.26229508196721313,"kâˆˆNX
log(1âˆ’Ë†oX
k )âˆ’
1
|NY | X"
P,0.26434426229508196,"kâˆˆNY
log(1âˆ’Ë†oY
k ), (12)"
P,0.26639344262295084,"where NX and NY are sets of semi-dense nodes in point clouds X and Y without correspondences,
respectively. Two nodes are considered as a pair of coarse correspondence only when their ground-
truth overlap ratio is greater than 0. Assuming that the patch centered at a point p âˆˆR3 is a spherical
neighborhood of radius r, the overlapping ratio oij of patches centered at pi âˆˆXS and qj âˆˆYS"
P,0.26844262295081966,with ground-truth translation t âˆˆR3 and rotation R âˆˆSO(3) can be calculated by:
P,0.27049180327868855,"oij =
2Ï€
R r
d/2(r2 âˆ’h2)dh"
P,0.2725409836065574,"4Ï€r3/3
= 1 âˆ’3d"
P,0.27459016393442626,"4r +
d3"
P,0.2766393442622951,"16r3 , d = max{âˆ¥Rpi + t âˆ’qiâˆ¥, 2r}.
(13)"
P,0.2786885245901639,"Keypoint Matching.
As our keypoint matching module follows a three-stage pipeline including
similarity calculation, correspondence prediction, and consistency filtering, it is reasonable to super-
vise these stages with three losses, respectively. Only valid keypoint-to-patch correspondences are
supervised during training, i.e., the distance of the keypoint x and its closest point px in the patch Cx
is less than a pre-defined threshold Rp > 0, and the points whose distances from x are greater than a
pre-defined threshold Rn â‰¥Rp form a non-empty set Nx âŠ‚Cx. We formulate the keypoint matching
loss Lf as an InfoNCE loss [38] with symmetric learnable weights W, which aims at maximizing
the similarity between descriptors dx and dpx of true correspondences (x, px) and minimizing the
similarity between descriptors dx and dnx of false correspondences (x, nx), nx âˆˆNx."
P,0.2807377049180328,"Lf = âˆ’E(x,px,Nx) """
P,0.2827868852459016,"log
exp(dT
xWdpx)
exp(dTxWdpx) + P"
P,0.2848360655737705,nxâˆˆNx exp(dTxWdnx) #
P,0.28688524590163933,".
(14)"
P,0.2889344262295082,"For correspondence prediction, we adopt a L2 loss Lk = E(x,Ë†y)âˆ¥Rx + t âˆ’Ë†yâˆ¥2 for the predicted
correspondences Ë†y of keypoints x from all valid keypoint-to-patch correspondences (x, Cx). For
consistency filtering, we simply utilize a binary entropy loss Li to supervise the confidence scores of
all keypoint correspondences. The binary ground-truth label of a keypoint correspondence (x, Ë†y) is 1
if and only if it is an inlier, i.e., âˆ¥Rx + t âˆ’Ë†yâˆ¥2 is less than a threshold Rf > 0."
P,0.29098360655737704,"Dense Registration.
Given the translation Ë†t and rotation Ë†R estimated by dense registration, we
adopt a translation loss Lt = âˆ¥Ë†t âˆ’tâˆ¥2 and a rotation loss Lr = âˆ¥Ë†RTR âˆ’I3Ã—3âˆ¥F for supervision."
P,0.2930327868852459,"Finally, we formulate our loss as L = Lp + Î»sLs + Î»cLc + Î»fLf + Î»kLk + Î»iLi + Î»tLt + Î»rLr,
where Î»c, Î»s, Î»f, Î»k, Î»i, Î»t, Î»r are balancing weights."
EXPERIMENTS,0.29508196721311475,"4
Experiments"
EXPERIMENTS,0.29713114754098363,"In this section, we evaluate our method on both outdoor LiDAR point cloud datasets KITTI [39],
nuScenes [40], and the indoor RGBD point cloud dataset 3DMatch [16]. Our network is trained
using an AdamW [41] optimizer with a batch size of 1, an initial learning rate of 1e-4, and a weight
decay of 1e-4. The step scheduler decrease the learning rate to 90% every five steps, with gradients
clipped at a norm of 0.5 during back propagation. Despite the complexity of the loss function, only
one stage is needed for training. Our model is trained on an NVIDIA RTX 3090 GPU with an Intel
Xeon CPU @2.90GHZ for 5, 40, and 3 epochs on 3DMatch, KITTI, and nuScenes, respectively, and
we set Î»f = Î»i = 1, Î»r = 20, Î»t = 5, Î»s = 0.1, and Î»c = 0.2, Î»k = 1 for KITTI and nuScenes,
Î»c = 1, Î»k = 10 for 3DMatch."
EXPERIMENTS,0.29918032786885246,"Source Frame 
with Keypoints"
EXPERIMENTS,0.3012295081967213,"Target Frame
with Keypoints"
EXPERIMENTS,0.30327868852459017,"Sparse Keypoint 
Correspondences"
EXPERIMENTS,0.305327868852459,Pose Estimation
EXPERIMENTS,0.3073770491803279,"Figure 3: Qualitative registration results on KITTI dataset. We show three examples in three columns.
The first two rows present the raw point clouds and highlight the 3D keypoints with low uncertainty in
red. Our keypoints are typically located in sharp corners and edges of buildings, pillars, and vehicles.
The third row shows the predicted sparse keypoint correspondences with high scores, while the last
row presents the aligned point clouds after pose estimation. Although a few outliers colored in red
have not been filtered out, their distances are acceptable for accurate registration."
EXPERIMENTS,0.3094262295081967,"4.1
Outdoor Scenarios: KITTI and NuScenes"
EXPERIMENTS,0.3114754098360656,"KITTI [39] is a popular benchmark for autonomous driving. Following [9], we use sequences 0
to 5 for training, 6 to 7 for validation, and 8 to 10 for testing, and select only point cloud pairs
at least 10m away from each other with ICP-refined [42] GPS localization results as ground truth.
NuScenes [40] is another large-scale outdoor autonomous driving benchmark including 850 scenes
for training and validation and 150 for testing. Following [24], we select each LiDAR keyframe
with the second keyframe after it as a pair of point clouds. We use three metrics for evaluation [20]:
relative translation error (RTE), relative rotation error (RRE), and registration recall (RR)."
EXPERIMENTS,0.3135245901639344,Table 1: Registration performance on KITTI odometry dataset.
EXPERIMENTS,0.3155737704918033,"Model
Publication
RTE (cm)
RRE (Â°)
RR (%)"
DFEAT-NET,0.3176229508196721,"3DFeat-Net
ECCV 2018 [25]
25.9
0.25
96.0
FCGF
ICCV 2019 [8]
9.5
0.30
96.6
D3Feat
CVPR 2020 [9]
7.2
0.30
99.8
SpinNet
CVPR 2021 [19]
9.9
0.47
99.1
Predator
CVPR 2021 [20]
6.8
0.27
99.8
CoFiNet
NeurIPS 2021 [11]
8.2
0.41
99.8
GeoTransformer
CVPR 2022 [12]
6.8
0.24
99.8
OIF-Net
NeurIPS 2022 [13]
6.5
0.23
99.8
PEAL
CVPR 2023 [29]
6.8
0.23
99.8
DiffusionPCR
CVPR 2024 [30]
6.3
0.23
99.8
MAC
CVPR 2023 [43]
8.5
0.40
99.5
RegFormer
ICCV 2023 [44]
8.4
0.24
99.8
CAST
2.5
0.27
100.0"
DFEAT-NET,0.319672131147541,"Our results on KITTI are detailed quantitatively in Table 1 and qualitatively in Figure 3. Table 1 shows
that the proposed CAST outperforms various learning-based methods, including descriptor-based
[25, 8, 9, 19, 20], coarse-to-fine correspondence-based [11, 12, 13] including the latest ones with
iterative matching [29, 30], a recent graph-based [43] and an end-to-end [44] baselines. Specifically,
CAST achieves a RR of 100.0% and the lowest RTE of 2.5cm, which is 60.3% improvement over the
state-of-the-art DiffusionPCR [30], highlighting its superior robustness and accuracy."
DFEAT-NET,0.32172131147540983,"As for RRE, CAST slightly underperforms some coarse-to-fine methods [12, 13, 29, 30], primarily
due to numerical errors in SVD-based pose estimation that usually produces non-orthonormal rotation
matrices. RRE is set as 0 when trace( Ë†RTR) > 3, a condition met frequently across all methods.
Consequently, geodesic distance-based RRE may not accurately reflect the actual performance."
DFEAT-NET,0.3237704918032787,Table 3: Evaluation results on indoor RGBD point cloud datasets.
DFEAT-NET,0.32581967213114754,"Dataset
3DMatch
3DLoMatch
Average"
DFEAT-NET,0.32786885245901637,"Registration Recall (%)
Registration Recall (%)
Time (s)
Samples
5000
2500
1000
500
250
5000
2500
1000
500
250
All"
DFEAT-NET,0.32991803278688525,descriptor-based
DFEAT-NET,0.3319672131147541,"PerfectMatch [7]
78.4
76.2
71.4
67.6
50.8
33.0
29.0
23.3
17.0
11.0
-
FCGF [8]
85.1
84.7
83.3
81.6
71.4
40.1
41.7
38.2
35.4
26.8
0.271
D3Feat [9]
81.6
84.5
83.4
82.4
77.9
37.2
42.7
46.9
43.8
39.1
0.289
SpinNet [19]
88.6
86.6
85.5
83.5
70.2
59.8
54.9
48.3
39.8
26.8
90.804
YOHO [50]
90.8
90.3
89.1
88.6
84.5
65.2
65.5
63.2
56.5
48.0
13.529
Predator [20]
89.0
89.9
90.6
88.5
86.6
59.8
61.2
62.4
60.8
58.1
0.759"
DFEAT-NET,0.33401639344262296,correspondence-based
DFEAT-NET,0.3360655737704918,"REGTR [47]
92.0
64.8
0.382
CoFiNet [11]
89.3
88.9
88.4
87.4
87.0
67.5
66.2
64.2
63.1
61.0
0.306
GeoTransformer [12]
92.0
91.8
91.8
91.4
91.2
75.0
74.8
74.2
74.1
73.5
0.192
OIF-Net [13]
92.4
91.9
91.8
92.1
91.2
76.1
75.4
75.1
74.4
73.6
0.555
RoITr [28]
91.9
91.7
91.8
91.4
91.0
74.7
74.8
74.8
74.2
73.6
0.457
PEAL [29]
94.4
94.1
94.1
93.9
93.4
79.2
79.0
78.8
78.5
77.9
2.074
BUFFER [48]
92.9
71.8
0.290
SIRA-PCR [49]
93.6
93.9
93.9
92.7
92.4
73.5
73.9
73.0
73.4
71.1
0.291
DiffusionPCR [30]
94.4
94.3
94.5
94.0
93.9
80.0
80.4
79.2
78.8
78.8
1.964
CAST
95.2
75.1
0.182"
DFEAT-NET,0.33811475409836067,Table 2: Registration performance on nuScenes.
DFEAT-NET,0.3401639344262295,"Method
RTE (m)
RRE (Â°)
RR (%)"
DFEAT-NET,0.3422131147540984,"Point-to-Point ICP [42]
0.25
0.25
18.8
Point-to-Plane ICP [42]
0.15
0.21
36.8
FGR [45]
0.71
1.01
32.2
RANSAC [15]
0.21
0.74
60.9
DCP [26]
1.09
2.07
56.8
IDAM [27]
0.47
0.79
88.0
FMR [46]
0.60
1.61
92.1
DGR [31]
0.21
0.48
98.4
HRegNet [24]
0.18
0.45
99.9
CAST
0.12
0.20
99.9"
DFEAT-NET,0.3442622950819672,"For a more challenging LiDAR benchmark
nuScenes, we compare CAST with both tra-
ditional [42, 45, 15] and learning-based algo-
rithms [26, 27, 46, 31, 24] in Table 2. We do
not include the coarse-to-fine methods since
none have been trained or tested on nuScenes.
Most of the results are borrowed from [24]
while HRegNet [24] is re-evaluated with their
open source codes. Our method achieves the
lowest translation error of 0.12m and the low-
est rotation error of 0.20Â° while maintaining
the best RR of 99.9%, showcasing state-of-
the-art robustness and accuracy."
DFEAT-NET,0.3463114754098361,"4.2
Indoor Scenarios: 3DMatch and 3DLoMatch"
DFEAT-NET,0.3483606557377049,"Our approach is also evaluated on indoor benchmarks 3DMatch [16] and 3DLoMatch [20], which
consist of point cloud pairs with overlaps >30% and 10% âˆ¼30%, respectively. In Table 3, we
use registration recall [16] as our evaluation metric, and test the runtime of all methods in Pytorch
implementation with 5000 points on our device with an Intel CPU i7-12800HX@2.30GHZ and an
NVIDIA RTX 3080Ti GPU for fairness, except [7] in Tensorflow implementation and [13, 29, 30]
using the results reported in their papers [13, 30] due to the absence of source codes. Our method
along with other sparse matching baselines [47, 48] directly uses all points for evaluation. To enhance
the robustness in low overlapping cases, our method is combined with RANSAC estimating an initial
pose from only 250 coarse correspondences to reject the outliers during fine matching."
DFEAT-NET,0.35040983606557374,"On the 3DMatch benchmark, our method achieves state-of-the-art RR of 95.2%. On the more
challenging 3DLoMatch, CAST achieves a high RR of 75.1%, outperforming all descriptors and
non-iterative correspondence-based methods [47, 11, 12, 28, 48, 49] except OIF-Net [13] using
more than 1000 sampled points. As our method typically detects about 1000 sparse keypoints and
establishes less than 250 keypoint correspondences on 3DLoMatch, it is fair to compare CAST
with other methods using only 250 sample points. However, CAST outperforms the state-of-the-art
non-iterative correspondence-based methods OIF-Net [13] using less than 1000 points. Notably, our
method achieves such superior performance only with the lowest runtime, while RANSAC remains
efficient due to our high inlier ratio. Although PEAL [29] and DiffusionPCR [30] show higher RR on
3DLoMatch, their iterative feature matching with overlap priors is extremely time-consuming (10
times of ours), while PEAL even requires extra information from 2D images."
DFEAT-NET,0.3524590163934426,Table 4: Ablation studies of coarse matching modules on indoor datasets.
DFEAT-NET,0.35450819672131145,"MS
SG
CA
OV
3DMatch
3DLoMatch
PIR (%)
PMR (%)
RR (%)
PIR (%)
PMR (%)
RR (%)"
DFEAT-NET,0.35655737704918034,"1
âœ“
âœ“
77.56
95.87
94.45
40.82
70.58
72.07
2
âœ“
âœ“
âœ“
77.95
96.61
94.92
42.55
72.77
74.57
3
âœ“
âœ“
âœ“
69.58
96.67
94.14
32.59
65.02
73.00
4
âœ“
âœ“
âœ“
73.56
97.17
95.07
35.25
68.33
74.91
5
âœ“
âœ“
âœ“
âœ“
79.79
97.17
96.01
44.41
75.24
76.59"
ABLATION STUDIES,0.35860655737704916,"4.3
Ablation Studies"
ABLATION STUDIES,0.36065573770491804,"We select indoor datasets for ablation studies of coarse matching as they are more challenging. Here
we evaluate the RR over the whole dataset rather than the average RR of eight sequences reported
in Table 3, which is more reasonable for a dataset with significant variances of sequence lengths.
Besides, we assess two extra metrics to directly measure the performance of coarse matching: patch
inlier ratio (PIR), the fraction of patch matches with actual overlap; and patch matching recall (PMR),
the fraction of point cloud pairs with PIR above 20%. Results from five experiments in Table 4
demonstrate the effects of the proposed multi-scale feature fusion (MS), spot-guided cross-attention
(SG), consistency-aware self-attention (CA), and the overlap head for overlap score prediction (OV).
The first experiment ablating CA and replacing SG with linear cross-attention, suffers performance
degradation in all metrics due to inconsistency. The second experiment improves all metrics based on
SG, while the last one achieves the best performance via CA, showcasing their effectiveness. Figure 2
visualizes the vanilla global cross-attention and our spot-guided cross-attention. Instead of interacting
with many similar yet irrelevant regions for misleading feature aggregation, SG can effectively select
instructive areas to attend to according to local consistency. Compared to the last experiment, the
third one verifies the effectiveness of multi-scale feature fusion, while the fourth one demonstrates
the necessity of overlap prediction."
ABLATION STUDIES,0.36270491803278687,Table 5: Ablation studies of fine matching on KITTI.
ABLATION STUDIES,0.36475409836065575,"RTE (cm)
RRE (Â°)
RR (%)"
ABLATION STUDIES,0.3668032786885246,"ours
2.51
0.27
100.00
ours w/o dense registration
3.13
0.30
100.00
ours w/o virtual dense corr.
2.85
0.28
100.00
ours w/o keypoint detection
3.58
0.30
100.00
ours w/o virtual sparse corr.
3.25
0.30
100.00
ours w/o graph embedding
5.01
0.30
100.00"
ABLATION STUDIES,0.36885245901639346,"Additionally, we conducted five abla-
tion studies on KITTI for a better un-
derstanding of our fine matching, since
pose errors are better metrics to reflect
accuracy. The second experiment using
only sparse keypoints for registration
highlights the effectiveness of dense
registration, while the third one shows
the effect of learnable dense correspon-
dences compared to nearest neighbors.
The last three experiments report the
performance of sparse registration by
ablating the keypoint detector, the learnable sparse correspondences, and the compatibility graph
embedding, each demonstrating their necessity for accuracy. Despite these variations, all studies
maintain a 100% RR, showing the robustness of coarse matching."
CONCLUSION,0.3709016393442623,"5
Conclusion"
CONCLUSION,0.3729508196721312,"In this paper, we present a novel consistency-aware spot-guided Transformer to achieve compact and
consistent coarse matching for point cloud registration. At the coarse matching stage, our consistency-
aware self-attention enhances the feature representations with sparse sampling from the geometric
compatibility graph. Additionally, our spot-guided cross-attention leverages local consistency to
guide the cross-attention to confident spots without interfering with irrelevant areas. Based on these
semi-dense and consistent coarse correspondences, a lightweight and scalable sparse-to-dense fine
matching module empowered by local attention can achieve accurate pose estimation without optimal
transport or hypothesis-and-selection pipelines. Our method has showcased state-of-the-art accuracy,
robustness, and efficiency for point cloud registration across different 3D sensors and scenarios,
which paves the way for large-scale real-time applications such as SLAM."
CONCLUSION,0.375,Acknowledgments
CONCLUSION,0.3770491803278688,This work is supported by the National Natural Science Foundation of China (62203383/62088101).
REFERENCES,0.3790983606557377,References
REFERENCES,0.38114754098360654,"[1] Renlang Huang, Minglei Zhao, Jiming Chen, and Liang Li. Kdd-loam: Jointly learned keypoint detector
and descriptors assisted lidar odometry and mapping. In 2024 IEEE International Conference on Robotics
and Automation (ICRA), pages 8559â€“8565, 2024."
REFERENCES,0.3831967213114754,"[2] Weitong Hua, Zhongxiang Zhou, Jun Wu, Huang Huang, Yue Wang, and Rong Xiong. Rede: End-to-end
object 6d pose robust estimation using differentiable outliers elimination. IEEE Robotics and Automation
Letters, 6(2):2886â€“2893, 2021."
REFERENCES,0.38524590163934425,"[3] Jianyuan Wang, Yiran Zhong, Yuchao Dai, Stan Birchfield, Kaihao Zhang, Nikolai Smolyanskiy, and
Hongdong Li. Deep two-view structure-from-motion revisited. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 8953â€“8962, 2021."
REFERENCES,0.38729508196721313,"[4] Radu Bogdan Rusu, Nico Blodow, Zoltan Csaba Marton, and Michael Beetz. Aligning point cloud views
using persistent feature histograms. In 2008 IEEE/RSJ International Conference on Intelligent Robots and
Systems, pages 3384â€“3391. IEEE, 2008."
REFERENCES,0.38934426229508196,"[5] Radu Bogdan Rusu, Nico Blodow, and Michael Beetz. Fast point feature histograms (fpfh) for 3d
registration. In 2009 IEEE International Conference on Robotics and Automation, pages 3212â€“3217. IEEE,
2009."
REFERENCES,0.39139344262295084,"[6] Samuele Salti, Federico Tombari, and Luigi Di Stefano. Shot: Unique signatures of histograms for surface
and texture description. Computer Vision and Image Understanding, 125:251â€“264, 2014."
REFERENCES,0.39344262295081966,"[7] Zan Gojcic, Caifa Zhou, Jan D Wegner, and Andreas Wieser. The perfect match: 3d point cloud matching
with smoothed densities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 5545â€“5554, 2019."
REFERENCES,0.39549180327868855,"[8] Christopher Choy, Jaesik Park, and Vladlen Koltun. Fully convolutional geometric features. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, pages 8958â€“8966, 2019."
REFERENCES,0.3975409836065574,"[9] Xuyang Bai, Zixin Luo, Lei Zhou, Hongbo Fu, Long Quan, and Chiew-Lan Tai. D3feat: Joint learning
of dense detection and description of 3d local features. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 6359â€“6367, 2020."
REFERENCES,0.39959016393442626,"[10] Jiahuan Yu, Jiahao Chang, Jianfeng He, Tianzhu Zhang, Jiyang Yu, and Feng Wu. Adaptive spot-guided
transformer for consistent local feature matching. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 21898â€“21908, 2023."
REFERENCES,0.4016393442622951,"[11] Hao Yu, Fu Li, Mahdi Saleh, Benjamin Busam, and Slobodan Ilic. Cofinet: Reliable coarse-to-fine
correspondences for robust point cloud registration. Advances in Neural Information Processing Systems,
34:23872â€“23884, 2021."
REFERENCES,0.4036885245901639,"[12] Zheng Qin, Hao Yu, Changjian Wang, Yulan Guo, Yuxing Peng, Slobodan Ilic, Dewen Hu, and Kai Xu.
Geotransformer: Fast and robust point cloud registration with geometric transformer. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 2023."
REFERENCES,0.4057377049180328,"[13] Fan Yang, Lin Guo, Zhi Chen, and Wenbing Tao. One-inlier is first: Towards efficient position encoding
for point cloud registration. Advances in Neural Information Processing Systems, 35:6982â€“6995, 2022."
REFERENCES,0.4077868852459016,"[14] Minhao Li, Zheng Qin, Zhirui Gao, Renjiao Yi, Chenyang Zhu, Yulan Guo, and Kai Xu. 2d3d-matr: 2d-3d
matching transformer for detection-free registration between images and point clouds. In Proceedings of
the IEEE/CVF International Conference on Computer Vision, pages 14128â€“14138, 2023."
REFERENCES,0.4098360655737705,"[15] Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for model fitting with
applications to image analysis and automated cartography. Communications of the ACM, 24(6):381â€“395,
1981."
REFERENCES,0.41188524590163933,"[16] Andy Zeng, Shuran Song, Matthias NieÃŸner, Matthew Fisher, Jianxiong Xiao, and Thomas Funkhouser.
3dmatch: Learning local geometric descriptors from rgb-d reconstructions. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 1802â€“1811, 2017."
REFERENCES,0.4139344262295082,"[17] Haowen Deng, Tolga Birdal, and Slobodan Ilic. Ppfnet: Global context aware local features for robust
3d point matching. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 195â€“205, 2018."
REFERENCES,0.41598360655737704,"[18] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d
classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 652â€“660, 2017."
REFERENCES,0.4180327868852459,"[19] Sheng Ao, Qingyong Hu, Bo Yang, Andrew Markham, and Yulan Guo. Spinnet: Learning a general surface
descriptor for 3d point cloud registration. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 11753â€“11762, 2021."
REFERENCES,0.42008196721311475,"[20] Shengyu Huang, Zan Gojcic, Mikhail Usvyatsov, Andreas Wieser, and Konrad Schindler. Predator:
Registration of 3d point clouds with low overlap. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 4267â€“4276, 2021."
REFERENCES,0.42213114754098363,"[21] Hui Chen and Bir Bhanu. 3d free-form object recognition in range images using local surface patches.
Pattern Recognition Letters, 28(10):1252â€“1262, 2007."
REFERENCES,0.42418032786885246,"[22] Yu Zhong. Intrinsic shape signatures: A shape descriptor for 3d object recognition. In IEEE International
Conference on Computer Vision Workshops, ICCV workshops, pages 689â€“696. IEEE, 2009."
REFERENCES,0.4262295081967213,"[23] Jiaxin Li and Gim Hee Lee. Usip: Unsupervised stable interest point detection from 3d point clouds. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 361â€“370, 2019."
REFERENCES,0.42827868852459017,"[24] Fan Lu, Guang Chen, Yinlong Liu, Lijun Zhang, Sanqing Qu, Shu Liu, Rongqi Gu, and Changjun Jiang.
Hregnet: A hierarchical network for efficient and accurate outdoor lidar point cloud registration. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 2023."
REFERENCES,0.430327868852459,"[25] Zi Jian Yew and Gim Hee Lee. 3dfeat-net: Weakly supervised local 3d features for point cloud registration.
In Proceedings of the European Conference on Computer Vision (ECCV), pages 607â€“623, 2018."
REFERENCES,0.4323770491803279,"[26] Yue Wang and Justin M Solomon. Deep closest point: Learning representations for point cloud registration.
In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3523â€“3532, 2019."
REFERENCES,0.4344262295081967,"[27] Jiahao Li, Changhao Zhang, Ziyao Xu, Hangning Zhou, and Chi Zhang. Iterative distance-aware similarity
matrix convolution with mutual-supervised point elimination for efficient point cloud registration. In
Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings,
Part XXIV 16, pages 378â€“394. Springer, 2020."
REFERENCES,0.4364754098360656,"[28] Hao Yu, Zheng Qin, Ji Hou, Mahdi Saleh, Dongsheng Li, Benjamin Busam, and Slobodan Ilic. Rotation-
invariant transformer for point cloud matching. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 5384â€“5393, 2023."
REFERENCES,0.4385245901639344,"[29] Junle Yu, Luwei Ren, Yu Zhang, Wenhui Zhou, Lili Lin, and Guojun Dai. Peal: Prior-embedded explicit
attention learning for low-overlap point cloud registration. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 17702â€“17711, 2023."
REFERENCES,0.4405737704918033,"[30] Zhi Chen, Yufan Ren, Tong Zhang, Zheng Dang, Wenbing Tao, Sabine SÃ¼sstrunk, and Mathieu Salz-
mann. Diffusionpcr: Diffusion models for robust multi-step point cloud registration. arXiv preprint
arXiv:2312.03053, 2023."
REFERENCES,0.4426229508196721,"[31] Christopher Choy, Wei Dong, and Vladlen Koltun. Deep global registration. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2514â€“2523, 2020."
REFERENCES,0.444672131147541,"[32] Xuyang Bai, Zixin Luo, Lei Zhou, Hongkai Chen, Lei Li, Zeyu Hu, Hongbo Fu, and Chiew-Lan Tai.
Pointdsc: Robust point cloud registration using deep spatial consistency. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 15859â€“15869, 2021."
REFERENCES,0.44672131147540983,"[33] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, FranÃ§ois Goulette, and
Leonidas J Guibas. Kpconv: Flexible and deformable convolution for point clouds. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pages 6411â€“6420, 2019."
REFERENCES,0.4487704918032787,"[34] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret. Transformers are rnns: Fast
autoregressive transformers with linear attention. In International Conference on Machine Learning, pages
5156â€“5165. PMLR, 2020."
REFERENCES,0.45081967213114754,"[35] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced
transformer with rotary position embedding. Neurocomputing, 568:127063, 2024."
REFERENCES,0.45286885245901637,"[36] Fan Lu, Guang Chen, Yinlong Liu, Zhongnan Qu, and Alois Knoll. Rskdd-net: Random sample-based
keypoint detector and descriptor. Advances in Neural Information Processing Systems, 33:21297â€“21308,
2020."
REFERENCES,0.45491803278688525,"[37] Wolfgang Kabsch. A solution for the best rotation to relate two sets of vectors. Acta Crystallographica
Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography, 32(5):922â€“923, 1976."
REFERENCES,0.4569672131147541,"[38] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018."
REFERENCES,0.45901639344262296,"[39] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision
benchmark suite. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3354â€“3361.
IEEE, 2012."
REFERENCES,0.4610655737704918,"[40] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan,
Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving.
arXiv preprint arXiv:1903.11027, 2019."
REFERENCES,0.46311475409836067,"[41] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on
Learning Representations, 2018."
REFERENCES,0.4651639344262295,"[42] P.J. Besl and Neil D. McKay. A method for registration of 3-d shapes. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 14(2):239â€“256, 1992."
REFERENCES,0.4672131147540984,"[43] Xiyu Zhang, Jiaqi Yang, Shikun Zhang, and Yanning Zhang. 3d registration with maximal cliques. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17745â€“
17754, 2023."
REFERENCES,0.4692622950819672,"[44] Jiuming Liu, Guangming Wang, Zhe Liu, Chaokang Jiang, Marc Pollefeys, and Hesheng Wang. Regformer:
An efficient projection-aware transformer network for large-scale point cloud registration.
In 2023
IEEE/CVF International Conference on Computer Vision (ICCV), pages 8417â€“8426, 2023."
REFERENCES,0.4713114754098361,"[45] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Fast global registration. In Computer Visionâ€“ECCV 2016:
14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14,
pages 766â€“782. Springer, 2016."
REFERENCES,0.4733606557377049,"[46] Xiaoshui Huang, Guofeng Mei, and Jian Zhang. Feature-metric registration: A fast semi-supervised
approach for robust point cloud registration without correspondences. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 11366â€“11374, 2020."
REFERENCES,0.47540983606557374,"[47] Zi Jian Yew and Gim Hee Lee. Regtr: End-to-end point cloud correspondences with transformers. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6677â€“6686,
2022."
REFERENCES,0.4774590163934426,"[48] Sheng Ao, Qingyong Hu, Hanyun Wang, Kai Xu, and Yulan Guo. Buffer: Balancing accuracy, efficiency,
and generalizability in point cloud registration. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 1255â€“1264, 2023."
REFERENCES,0.47950819672131145,"[49] Suyi Chen, Hao Xu, Ru Li, Guanghui Liu, Chi-Wing Fu, and Shuaicheng Liu. Sira-pcr: Sim-to-real
adaptation for 3d point cloud registration. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 14394â€“14405, 2023."
REFERENCES,0.48155737704918034,"[50] Haiping Wang, Yuan Liu, Zhen Dong, and Wenping Wang. You only hypothesize once: Point cloud
registration with rotation-equivariant descriptors. In Proceedings of the 30th ACM International Conference
on Multimedia, pages 1630â€“1641, 2022."
REFERENCES,0.48360655737704916,"[51] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon.
Scene coordinate regression forests for camera relocalization in rgb-d images. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 2930â€“2937, 2013."
REFERENCES,0.48565573770491804,"[52] Kevin Lai, Liefeng Bo, and Dieter Fox. Unsupervised feature learning for 3d scene labeling. In 2014 IEEE
International Conference on Robotics and Automation (ICRA), pages 3050â€“3057. IEEE, 2014."
REFERENCES,0.48770491803278687,"[53] Julien Valentin, Angela Dai, Matthias NieÃŸner, Pushmeet Kohli, Philip Torr, Shahram Izadi, and Cem
Keskin. Learning to navigate the energy landscape. In 2016 Fourth International Conference on 3D Vision
(3DV), pages 323â€“332. IEEE, 2016."
REFERENCES,0.48975409836065575,"[54] Angela Dai, Matthias NieÃŸner, Michael ZollhÃ¶fer, Shahram Izadi, and Christian Theobalt. Bundlefusion:
Real-time globally consistent 3d reconstruction using on-the-fly surface reintegration. ACM Transactions
on Graphics (ToG), 36(4):1, 2017."
REFERENCES,0.4918032786885246,"[55] Jianxiong Xiao, Andrew Owens, and Antonio Torralba. Sun3d: A database of big spaces reconstructed
using sfm and object labels. In Proceedings of the IEEE International Conference on Computer Vision,
pages 1625â€“1632, 2013."
REFERENCES,0.49385245901639346,"[56] Maciej Halber and Thomas Funkhouser. Fine-to-coarse global registration of rgb-d scans. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1755â€“1764, 2017."
REFERENCES,0.4959016393442623,"[57] Brian Curless and Marc Levoy. A volumetric method for building complex models from range images.
In Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, pages
303â€“312, 1996."
REFERENCES,0.4979508196721312,"[58] Heng Yang, Jingnan Shi, and Luca Carlone. Teaser: Fast and certifiable point cloud registration. IEEE
Transactions on Robotics, 37(2):314â€“333, 2021."
REFERENCES,0.5,"[59] Zhi Chen, Kun Sun, Fan Yang, and Wenbing Tao. Sc2-pcr: A second order spatial compatibility for
efficient and robust point cloud registration. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 13221â€“13231, 2022."
REFERENCES,0.5020491803278688,"[60] Yifei Zhang, Hao Zhao, Hongyang Li, and Siheng Chen. Fastmac: Stochastic spectral sampling of
correspondence graph. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 17857â€“17867, 2024."
REFERENCES,0.5040983606557377,"[61] FranÃ§ois Pomerleau, M. Liu, Francis Colas, and Roland Siegwart. Challenging data sets for point cloud
registration algorithms. The International Journal of Robotics Research, 31(14):1705â€“1711, December
2012."
REFERENCES,0.5061475409836066,"[62] M. Leordeanu and M. Hebert. A spectral technique for correspondence problems using pairwise constraints.
In Tenth IEEE International Conference on Computer Vision (ICCVâ€™05) Volume 1, pages 1482â€“1489, 2005."
REFERENCES,0.5081967213114754,"A
Appendix"
REFERENCES,0.5102459016393442,"In this appendix, we will first detail our neural network architecture with hyper-parameters in Sec. A.1.
Then we introduce the related datasets in Sec. A.2 and provide more quantitative experimental results
of our method with detailed explanation of evaluation metrics in Sec. A.3. Additionally, more
qualitative results are illustrated in Sec. A.4. Finally, we will discuss the limitations and broader
impacts in Sec. A.5 and Sec. A.6, respectively."
REFERENCES,0.5122950819672131,"A.1
Neural Network Architectures and Hyper-parameters"
REFERENCES,0.514344262295082,"Feature Pyramid Network.
CAST utilizes a fully convolutional feature pyramid network as the
backbone, which follows an encoder-decoder architecture based on KPConv [33] operations. Details
of our network architecture are illustrated in Figure 4, which remain the same as [12] including five
encoder layers and three decoder layers. Note that the backbone for indoor datasets 3DMatch [16] is
slightly different, which only comprises four encoder layers and two decoder layers."
REFERENCES,0.5163934426229508,"Coarse Matching Module.
Figure 5 illustrates the details of our consistency-aware spot-guided
attention blocks for coarse matching. Both coarse features and semi-dense features extracted from
the backbone are first projected to 128 dimensions and then pass through three consistency-aware
spot-guided attention blocks (Figure 1). Each attention module uses 4 heads with ReLU activation.
Compared to vanilla attention with a quadratic increase in the size of the attention matrix with respect
to the input length, the linear attention [34] is much more efficient in global context aggregation by
replacing the softmax operator with the product of two kernel functions:"
REFERENCES,0.5184426229508197,"Linear attention(Q, K, V ) = Ï•(Q)(Ï•(K)TV ),
(15)"
REFERENCES,0.5204918032786885,"where Ï•(Â·) = elu(Â·) + 1. For spot-guided cross-attention, each node xS
i selects 4 seeds as Ns(xS
i )
from the neighborhood N(xS
i ) with 12 nodes based on consistency-aware matching confidence. For
consistency-aware self-attention, we first scale the generalized degrees to [0, 1], and sample 48 nodes
with highest matching scores from nodes with scaled degrees greater than 0.3 as keys to be attended."
REFERENCES,0.5225409836065574,"Fine Matching Modules.
For keypoint detection and description, we utilize an attentive keypoint
detector [36] to extract a keypoint with a descriptor from each local patch, which contains k = 32
nearest neighbors among dense points xi1, Â· Â· Â· , xik âˆˆX1/2 with features fi1, Â· Â· Â· , fik of a patch"
REFERENCES,0.5245901639344263,"KPConv (1, 64)"
REFERENCES,0.5266393442622951,"ResBlock (64, 128)"
REFERENCES,0.5286885245901639,"StridedResBlock (128, 128)"
REFERENCES,0.5307377049180327,"ResBlock (128, 256)"
REFERENCES,0.5327868852459017,"ResBlock (256, 256)"
REFERENCES,0.5348360655737705,"StridedResBlock (256, 256)"
REFERENCES,0.5368852459016393,"ResBlock (256, 512)"
REFERENCES,0.5389344262295082,"ResBlock (512, 512)"
REFERENCES,0.5409836065573771,"StridedResBlock (512, 512)"
REFERENCES,0.5430327868852459,"ResBlock (512, 1024)"
REFERENCES,0.5450819672131147,"ResBlock (1024, 1024)"
REFERENCES,0.5471311475409836,"StridedResBlock (1024, 1024)"
REFERENCES,0.5491803278688525,"ResBlock (1024, 2048)"
REFERENCES,0.5512295081967213,"ResBlock (2048, 2048)"
REFERENCES,0.5532786885245902,Nearest Up-Sampling
REFERENCES,0.555327868852459,"Linear (3072, 1024) C"
REFERENCES,0.5573770491803278,Nearest Up-Sampling C
REFERENCES,0.5594262295081968,"Linear (1536, 512)"
REFERENCES,0.5614754098360656,Nearest Up-Sampling C
REFERENCES,0.5635245901639344,"Linear (768, 256)"
REFERENCES,0.5655737704918032,"KPConv (in/2, in/2)"
REFERENCES,0.5676229508196722,"Linear (in, in/2)"
REFERENCES,0.569672131147541,"Linear (in/2, in*2) +"
REFERENCES,0.5717213114754098,"Linear (in, in*2)"
REFERENCES,0.5737704918032787,"Linear (in, in/4)"
REFERENCES,0.5758196721311475,"KPConv (in/4, in/4)"
REFERENCES,0.5778688524590164,"Linear (in/4, in) +"
REFERENCES,0.5799180327868853,"Linear (in, in/4)"
REFERENCES,0.5819672131147541,"Max-Pool
KPConv (in/4, in/4)"
REFERENCES,0.5840163934426229,"Linear (in/4, in) +"
REFERENCES,0.5860655737704918,GroupNorm
REFERENCES,0.5881147540983607,"LeakyReLU +
Add"
REFERENCES,0.5901639344262295,"C
Concat"
REFERENCES,0.5922131147540983,"ResBlock (in, in*2)
ResBlock (in, in)
StridedResBlock (in, in)"
REFERENCES,0.5942622950819673,Figure 4: The detailed architecture of the KPConv-based feature pyramid network. FC
REFERENCES,0.5963114754098361,"Multi-Head 
Self-Attention"
REFERENCES,0.5983606557377049,"Add & Norm FC
FC"
REFERENCES,0.6004098360655737,"Multi-Head 
Self-Attention"
REFERENCES,0.6024590163934426,"Multi-Head 
Self-Attention"
REFERENCES,0.6045081967213115,"Add & Norm
Add & Norm"
REFERENCES,0.6065573770491803,Feed Forward
REFERENCES,0.6086065573770492,Add & Norm
REFERENCES,0.610655737704918,Feed Forward
REFERENCES,0.6127049180327869,Add & Norm
REFERENCES,0.6147540983606558,"Multi-Head 
Cross-Attention"
REFERENCES,0.6168032786885246,"Multi-Head 
Cross-Attention FC"
REFERENCES,0.6188524590163934,Add & Norm
REFERENCES,0.6209016393442623,Add & Norm
REFERENCES,0.6229508196721312,"Feed Forward
Feed Forward"
REFERENCES,0.625,Linear Attention
REFERENCES,0.6270491803278688,"ð—ð¶
ð˜ð‘†
ð˜ð¶
ð—ð‘†
ð…ð‘‹ ð¶
ð…ð‘Œ ð¶
ð…ð‘‹ ð‘†
ð…ð‘Œ ð‘† ð
ðŠ
ð• ð
ðŠ
ð•"
REFERENCES,0.6290983606557377,Add & Norm
REFERENCES,0.6311475409836066,"Rotary 
Positional"
REFERENCES,0.6331967213114754,"Encoding ð
ðŠ
ð• ð
ðŠ
ð•"
REFERENCES,0.6352459016393442,Add & Norm
REFERENCES,0.6372950819672131,"Add & Norm
Add & Norm FC"
REFERENCES,0.639344262295082,"Add & Norm FC 3
4 1
2 3"
REFERENCES,0.6413934426229508,Multi-Scale
REFERENCES,0.6434426229508197,"Fusion FC
FC"
REFERENCES,0.6454918032786885,Add & Norm
REFERENCES,0.6475409836065574,Feed Forward
REFERENCES,0.6495901639344263,"Multi-Head 
Cross-Attention"
REFERENCES,0.6516393442622951,Add & Norm
REFERENCES,0.6536885245901639,Feed Forward
REFERENCES,0.6557377049180327,"Add & Norm ð
ðŠ
ð• FC"
REFERENCES,0.6577868852459017,"Multi-Head 
Self-Attention"
REFERENCES,0.6598360655737705,Add & Norm
REFERENCES,0.6618852459016393,Add & Norm FC
REFERENCES,0.6639344262295082,Add & Norm
REFERENCES,0.6659836065573771,Feed Forward
REFERENCES,0.6680327868852459,"Multi-Head 
Cross-Attention"
REFERENCES,0.6700819672131147,Add & Norm
REFERENCES,0.6721311475409836,Feed Forward
REFERENCES,0.6741803278688525,"Add & Norm ð
ðŠ
ð•"
REFERENCES,0.6762295081967213,"ð
ðŠ
ð•
ð
ðŠ
ð• 4"
REFERENCES,0.6782786885245902,Salient Node
REFERENCES,0.680327868852459,Sampling
REFERENCES,0.6823770491803278,"Spot 
Selection"
REFERENCES,0.6844262295081968,Linear Attention
REFERENCES,0.6864754098360656,"FC
FC
x3
2
1"
REFERENCES,0.6885245901639344,"Figure 5: The detailed architecture of the consistency-aware spot-guided Transformer with multi-scale
feature fusion for coarse feature matching."
REFERENCES,0.6905737704918032,"node xS
i âˆˆXS. The details of the keypoint detector and descriptor are illustrated in Figure 6, which
are based on only shared MLPs with

xij, âˆ¥xij âˆ’xS
i âˆ¥2, fij

, j = 1, Â· Â· Â· , k as inputs. For keypoints
xK
1 , Â· Â· Â· , xK
M â€² with uncertainties Ïƒ1, Â· Â· Â· , ÏƒM â€² and yK
1 , Â· Â· Â· , yK
Nâ€² with uncertainties Ïƒ1, Â· Â· Â· , ÏƒN â€² from
point cloud X and Y, respectively, we adopt the probabilistic chamfer loss in [23] for training, please
refer to their paper for details."
REFERENCES,0.6926229508196722,Figure 6: Attentive keypoint detector and descriptor.
REFERENCES,0.694672131147541,"To establish keypoint-to-patch
correspondences based on coarse
correspondences, each keypoint
is assigned to its nearest node
when their distance is below a
threshold Rk. Then the keypoint
corresponds to the correspond-
ing patch of this node containing
kp keypoints. As mentioned in
Sec. 3, we leverage a single-head
attention layer to predict the vir-
tual correspondence of each key-
point based on only ks keypoints
in its corresponding patch with the highest similarity (Eq. 9). Finally, spatial consistency filtering is
performed via three graph compatibility embedding layers with embedding dimension De = 64 in
Eq. 10. After sparse matching and registration, we further refine the transformation based on dense
matching, which searches kd nearest neighbors within a distance threshold Rd for each dense point to
predict its virtual dense correspondence. Specifically, we set ks = 4, kd = 6 in our implementation,
while other related hyper-parameters are listed in Table 6."
REFERENCES,0.6967213114754098,"Weighted Kabsch Algorithm.
The weighted Kabsch algorithm [37], also known as the Proscrutes
algorithm, provides the closed-form solution for the point cloud registration problem (Eq. 1). Given
a predicted correspondence set Ë†C = {(xk, yk) : k = 1, Â· Â· Â· , n}, the optimal rigid transformation
Ë†R âˆˆSO(3),Ë†t âˆˆR3 can be estimated via two steps:"
REFERENCES,0.6987704918032787,Table 6: Hyper-parameter setting for three datasets in our implementation.
REFERENCES,0.7008196721311475,"Hyper-parameters
Explanation
3DMatch
KITTI
nuScenes"
REFERENCES,0.7028688524590164,"r
coarse overlap radius (Eq. 13)
0.075m
1.2m
1.2m
Ïƒc
coarse compatibility threshold (Eq. 8)
0.15m
1.8m
1.8m
Rk
keypoint-to-node distance threshold
0.1m
1.8m
1.8m
Ïƒd
fine compatibility threshold
0.1m
1.0m
1.0m
Rd
dense matching radius
0.15m
0.75m
1.0m
Rp
positive matching threshold (Eq. 14)
0.05m
0.45m
0.45m
Rn
negative matching threshold (Eq. 14)
0.06m
0.6m
0.6m
kp
number of keypoints in a patch
16
24
24"
REFERENCES,0.7049180327868853,Step 1. Centralize the point clouds by subtracting away their weighted centroids:
REFERENCES,0.7069672131147541,"Ëœxk = xk âˆ’x, Ëœyk = yk âˆ’y, x =
Pn
k=1 wkxk
Pn
k=1 wk
, y =
Pn
k=1 wkyk
Pn
k=1 wk
.
(16)"
REFERENCES,0.7090163934426229,"Step 2. Estimate the transformation. A 3 Ã— 3 weighted covariance matrix can be computed as H = n
X"
REFERENCES,0.7110655737704918,"k=1
wkËœxkËœyT
k.
(17)"
REFERENCES,0.7131147540983607,"With its singular value decomposition H = UÎ£VT, the optimal estimate of pose is given by:"
REFERENCES,0.7151639344262295,Ë†R = V ï£®
REFERENCES,0.7172131147540983,"ï£°
1
0
0
0
1
0
0
0
det(VUT) ï£¹"
REFERENCES,0.7192622950819673,"ï£»UT, Ë†t = y âˆ’Ë†Rx.
(18)"
REFERENCES,0.7213114754098361,"A.2
Data and Benchmarks"
REFERENCES,0.7233606557377049,"3DMatch.
3DMatch [16] is a widely used RGBD point cloud dataset for 3D reconstruction,
including 62 scenes from 7-Scenes [51], RGB-D Scenes V2 [52], Analysis-by-Synthesis [53],
BundleFusion [54], SUN3D [55] and Halber et al. [56] with their licenses in Table 7, where 46
scenes are used for training, 8 scenes for validation and 8 scenes for testing. Input point clouds are
generated by fusing 50 consecutive depth frames collected by RGBD cameras using TSDF volumetric
fusion [57]. Different from the original 3DMatch [16] that only consists of point cloud pairs with
>30% overlaps, [20] also includes point cloud pairs with overlaps between 10% and 30% for training,
and it sets two benchmarks for performance evaluation, i.e., 3DMatch consisting of point cloud pairs
with >30% overlaps, and 3DLoMatch consisting of point cloud pairs with low overlap ratios between
10% and 30%. Following [12], we utilize the voxel grid down-sampling of 2.5cm voxel size for data
preprocessing, which picks the centroid as a down-sampled point when multiple points fall into a
common voxel grid. We leverage the data and evaluation protocols in [20] for training and testing."
REFERENCES,0.7254098360655737,Table 7: Raw data in the 3DMatch [16] dataset and their licenses.
REFERENCES,0.7274590163934426,"Datasets
License"
REFERENCES,0.7295081967213115,"7-Scenes [51]
Non-commercial use only
Analysis-by-Synthesis [53]
CC BY-NC-SA 4.0
BundleFusion [54]
CC BY-NC-SA 4.0
RGB-D Scenes v2 [52]
(License not stated)
SUN3D [55]
CC BY-NC-SA 4.0
Halber et al. [56]
CC BY-NC-SA 4.0"
REFERENCES,0.7315573770491803,"KITTI.
KITTI [39] is a classic benchmark under the NonCommercial-ShareAlike 3.0 License for a
variety of computer vision tasks of autonomous driving, ranging from LiDAR-based or vision-based
or multi-sensors based odometry, object detection and tracking, optical flow estimation, point cloud
registration, etc. KITTI comprises of 11 sequences scanned by a Velodyne HDL-64 3D laser scanner
in driving scenarios. Following the data splitting method in [8], we use sequences 0 to 5 for training,
6 to 7 for validation, and 8 to 10 for testing. Besides, we directly leverage the source code of [9] to"
REFERENCES,0.7336065573770492,"select point cloud pairs which are at least 10m away from each other, which leads to 1,358 training
pairs, 180 validation pairs, and 555 testing pairs. Moreover, as the ground truth transformations
provided by GPS are less accurate, we follow [9] to refine them via standard ICP [42] in 500 iterations.
Following [9], we utilize the voxel down-sampling of 0.3m voxel size for data preprocessing."
REFERENCES,0.735655737704918,"NuScenes.
NuScenes [40] is an outdoor autonomous driving datasets under CC BY-NC-SA 4.0
license. It is the first large-scale dataset to provide data from the entire sensor suite of an autonomous
vehicle, consisting of 850 scenes for training and validation and 150 scenes for testing. Following [24],
we select the first 700 scenes from the 850 scenes for training and the others for validation. The
information about the point cloud pairs with ground-truth transformations is downloaded from the
source codes of [24] which selects each LiDAR keyframe with the second keyframe after it as a pair.
For data preprocessing, we apply 0.3m voxel grid down-sampling."
REFERENCES,0.7377049180327869,"A.3
Evaluation Metrics with Extra Quantitative Results"
REFERENCES,0.7397540983606558,"The outdoor datasets KITTI [39] and nuScenes [40] commonly use three metrics for evaluation [20]:
(1) relative translation error (RTE), the Euclidean distance between the estimated and ground-truth
translation vectors; (2) relative rotation error (RRE), the geodesic distance between the estimated and
ground-truth rotation matrices on SO(3); and (3) registration recall (RR), the percentage of point
cloud pairs with RTE<2m and RRE<5Â°. Given the ground-truth rotation R âˆˆSO(3) and translation
t âˆˆR3, as well as the estimated rotation Ë†R and translation Ë†t, the RRE and RTE are formulated as"
REFERENCES,0.7418032786885246,RRE = arccos
REFERENCES,0.7438524590163934,trace( Ë†RTR) âˆ’1 2 !
REFERENCES,0.7459016393442623,", RTE = âˆ¥Ë†t âˆ’tâˆ¥2.
(19)"
REFERENCES,0.7479508196721312,"The indoor benchmarks 3DMatch [16] and 3DLoMatch [20] commonly leverage three metrics for
evaluation: registration recall (RR), inlier ratio (IR), and feature matching recall (FMR). To keep
with existing methods [20], we exclude the consecutive point clouds when evaluating the RR."
REFERENCES,0.75,"Registration Recall
refers to the percentage of point cloud pairs whose root mean square error
(RMSE) is less than a pre-defined threshold Ï„ = 0.2m. Given point clouds X, Y with a ground-
truth correspondence set C = {(xi, yj) : xi âˆˆX, yj âˆˆY}, estimated rotation Ë†R âˆˆSO(3), and
estimated translation Ë†t âˆˆR3, the RMSE of X, Y is formulated as"
REFERENCES,0.7520491803278688,"RMSE(X, Y) ="
REFERENCES,0.7540983606557377,"v
u
u
t 1 |C| X"
REFERENCES,0.7561475409836066,"(xi,yj)âˆˆC
âˆ¥Ë†Rxi + Ë†t âˆ’yjâˆ¥2.
(20)"
REFERENCES,0.7581967213114754,"Inlier Ratio
refers to the percentage of estimated correspondences whose distance is less than a
pre-defined threshold Ï„1 = 0.1m. Given point clouds X, Y with an estimated correspondence set
Ë†C = {(xi, yj) : xi âˆˆX, yj âˆˆY}, ground-truth rotation R âˆˆSO(3), and ground-truth translation
t âˆˆR3, the IR of X, Y is formulated as"
REFERENCES,0.7602459016393442,"IR(X, Y) =
1 | Ë†C| X"
REFERENCES,0.7622950819672131,"(xi,yj)âˆˆË†C
1 (âˆ¥Rxi + t âˆ’yjâˆ¥< Ï„1) .
(21)"
REFERENCES,0.764344262295082,"Feature Matching Recall
refers to the fraction of point cloud pairs with an IR> Ï„2 = 5%."
REFERENCES,0.7663934426229508,"We adopt RR for evaluation as it directly measures the performance on the target task of point cloud
registration, while FMR and IR are not suitable to quantize our matching performance for three
reasons: (1) our method detects only sparse keypoints for fine matching, hence it is not fair to be
compared with other methods based on dense descriptors or correspondences when using the same
number of sampled points for evaluation; (2) our method directly supervises the differential pose
estimator to learn virtual correspondences with confidence scores instead of explicit feature matching
and outlier rejection, hence our method inevitably retains many outliers which are acceptable for
registration but not likely to maintain advanced matching performance; (3) feature matching recall and
inlier ratio only reflect the matching performance which is not decisive for either accuracy (relative
pose errors) or robustness (registration recall) especially for RANSAC-free methods [12, 13, 28, 49]."
REFERENCES,0.7684426229508197,Table 8: Inlier ratios and feature matching recalls on indoor datasets 3DMatch.
REFERENCES,0.7704918032786885,Inlier Ratio (%)
REFERENCES,0.7725409836065574,"Benchmark
3DMatch
3DLoMatch
Samples
5000
2500
1000
500
250
5000
2500
1000
500
250"
REFERENCES,0.7745901639344263,descriptor-based
REFERENCES,0.7766393442622951,"PerfectMatch [7]
36.0
32.5
26.4
21.5
16.4
11.4
10.1
8.0
6.4
4.8
FCGF [8]
56.8
54.1
48.7
42.5
34.1
21.4
20.0
17.2
14.8
11.6
D3Feat [9]
39.0
38.8
40.4
41.5
41.8
13.2
13.1
14.0
14.6
15.0
SpinNet [19]
47.5
44.7
39.4
33.9
27.6
20.5
19.0
16.3
13.8
11.1
YOHO [50]
64.4
60.7
55.7
46.4
41.2
25.9
23.3
22.6
18.2
15.0
Predator [20]
58.0
58.4
57.1
54.1
49.3
26.7
28.1
28.3
27.5
25.8"
REFERENCES,0.7786885245901639,correspondence-based
REFERENCES,0.7807377049180327,"CoFiNet [11]
49.8
51.2
51.9
52.2
52.2
24.4
25.9
26.7
26.8
26.9
GeoTransformer [12]
71.9
75.2
76.0
82.2
85.1
43.5
45.3
46.2
52.9
57.7
OIF-Net [13]
62.3
65.2
66.8
67.1
67.5
27.5
30.0
31.2
32.6
33.1
RoITr [28]
82.6
82.8
83.0
83.0
83.0
54.3
54.6
55.1
55.2
55.3
PEAL [29]
74.8
81.3
86.0
87.9
89.2
49.1
54.1
60.5
63.6
65.0
SIRA-PCR [49]
70.8
78.3
83.7
85.9
87.4
43.3
49.0
55.9
58.8
60.7
DiffusionPCR [30]
75.0
81.6
86.3
88.2
89.4
49.7
55.4
61.8
64.5
66.2
CAST
-
-
91.2
91.5
93.1
-
-
66.3
66.3
66.5"
REFERENCES,0.7827868852459017,Feature Matching Recall (%)
REFERENCES,0.7848360655737705,"Benchmark
3DMatch
3DLoMatch
Samples
5000
2500
1000
500
250
5000
2500
1000
500
250"
REFERENCES,0.7868852459016393,descriptor-based
REFERENCES,0.7889344262295082,"PerfectMatch[7]
95.0
94.3
92.9
90.1
82.9
63.6
61.7
53.6
45.2
34.2
FCGF[8]
97.4
97.3
97.0
96.7
96.6
76.6
75.4
74.2
71.7
67.3
D3Feat[9]
95.6
95.4
94.5
94.1
93.1
67.3
66.7
67.0
66.7
66.5
SpinNet[19]
97.6
97.2
96.8
95.5
94.3
75.3
74.9
72.5
70.0
63.6
YOHO[50]
98.2
97.6
97.5
97.7
96.0
79.4
78.1
76.3
73.8
69.1
Predator[20]
96.6
96.6
96.5
96.3
96.5
78.6
77.4
76.3
75.7
75.3"
REFERENCES,0.7909836065573771,correspondence-based
REFERENCES,0.7930327868852459,"CoFiNet[11]
98.1
98.3
98.1
98.2
98.3
83.1
83.5
83.3
83.1
82.6
GeoTransformer[12]
97.9
97.9
97.9
97.9
97.6
88.3
88.6
88.8
88.6
88.3
OIF-Net [13]
98.1
98.1
97.9
98.4
98.4
84.6
85.2
85.5
86.6
87.0
RoITr [28]
98.0
98.0
97.9
98.0
97.9
89.6
89.6
89.5
89.4
89.3
PEAL [29]
98.5
98.6
98.6
98.7
98.7
89.1
89.2
89.0
89.0
88.8
SIRA-PCR [49]
98.2
98.4
98.4
98.5
98.5
88.8
89.0
88.9
88.6
87.7
DiffusionPCR [30]
98.3
98.3
98.3
98.3
98.3
86.3
85.9
86.0
86.1
85.9
CAST
-
98.3
98.3
98.4
98.3
-
83.1
83.6
85.5
84.7"
REFERENCES,0.7950819672131147,"Feature Matching Performance.
Nevertheless, this appendix presents IR and FMR in Table 8 to
demonstrate the feature matching performance of our method. For IR evaluation, we scale the inlier
confidences predicted by compatibility graph embedding to [0, 1] and discard the correspondences
with confidences < 0.1. As the number of keypoint correspondences after filtering is always less
than 1000, we only report the IR regarding â‰¤1000 correspondences. Even without filtering, the
number of keypoint correspondences is always less than 2500, hence we only report the FMR
regarding â‰¤2500 correspondences. Enjoying the merits of our coarse matching, keypoint detection,
and compatibility graph embedding, our method achieves the highest inlier ratio compared to all
sorts of baselines. As for FMR, our method performs on par with DiffusionPCR [30] on 3DMatch
and better than CoFiNet [11] on 3DLoMatch. However, our method performs worse than other
coarse-to-fine methods [12, 13, 28, 29, 49, 30] on 3DLoMatch, since it remains challenging to extract
enough keypoint correspondences in extremely low overlapping cases due to sparsity."
REFERENCES,0.7971311475409836,"Indoor Registration Performance.
We demonstrate the accuracy of CAST for indoor RGB-D
point cloud registration by comparing it with various point cloud registration methods [15, 58, 59, 31,
32, 43, 60] in Table 9. All of the registration methods leverage the prevalent FCGF descriptor [8],
and FastMAC [60] uses a sampling ratio of 50%. For a fair comparison, we follow the evaluation
strategy of MAC [43] to re-compute the registration recall of our method, which is formulated as
the fraction of point cloud pairs with RTE<30cm and RRE<15Â°. Our method achieves the highest
registration recall and the lowest registration errors, suggesting its robustness and accuracy."
REFERENCES,0.7991803278688525,Table 9: Registration results on indoor RGBD point cloud datasets.
REFERENCES,0.8012295081967213,"Methods
3DMatch
3DLoMatch
RR (%)
RTE (cm)
RRE (Â°)
RR (%)
RTE (cm)
RRE (Â°)"
REFERENCES,0.8032786885245902,"RANSAC-1M [15]
88.42
9.42
3.05
9.77
14.87
7.01
RANSAC-4M [15]
91.44
8.38
2.69
10.44
15.14
6.91
TEASER++ [58]
85.77
8.66
2.73
46.76
12.89
4.12
SC2-PCR [59]
93.16
6.51
2.09
58.73
10.44
3.80
DGR [31]
88.85
7.02
2.28
43.80
10.82
4.17
PointDSC [32]
91.87
6.54
2.10
56.20
10.48
3.87
MAC [43]
93.72
6.54
2.02
59.85
9.75
3.50
FastMAC [60]
92.67
6.47
2.00
58.23
10.81
3.80
CAST
96.48
5.64
1.71
76.13
8.47
2.75"
REFERENCES,0.805327868852459,Table 10: Empirical standard deviations of the evaluation metrics of CAST in repeated experiments.
REFERENCES,0.8073770491803278,"Dataset
3DMatch
KITTI
nuScenes
Metrics
RR
IR
FMR
PIR
PMR
RR
RTE
RRE
RR
RTE
RRE"
REFERENCES,0.8094262295081968,"STD
0.4%
0.4%
0.5%
0.3%
0.2%
0.0%
0.1cm
0.01Â°
0.0%
0.1cm
0.01Â°"
REFERENCES,0.8114754098360656,"Generalization Studies.
To extensively evaluate the generalizability of the proposed CAST in
unseen domains, we conduct a generalization experiment from the outdoor dataset KITTI [39] to
another outdoor dataset ETH [61]. Note that the KITTI and ETH datasets use Velodyne-64 3D LiDAR
and Hokuyo 2D LiDAR, respectively, leading to very different appearances and distributions of point
clouds. Hence, our generalization study is practical in applications and solid to demonstrate the
generalizability of different methods. For fairness, all methods adopt 30cm for voxel down-sampling,
and all methods involving RANSAC set the maximum iterations to be 50000 and the confidence to be
0.999 as the convergence criteria. To enhance the robustness, our method is combined with RANSAC
estimating an initial pose from 250 coarse correspondences to reject the outliers during fine matching,
and utilizes the global registration in GeoTransformer [12] to refine the pose estimate."
REFERENCES,0.8135245901639344,"We present the translation errors, rotation errors, and the registration recalls in Table 11. Our method
achieves satisfying accuracy and robustness, showcasing better generalizability than the coarse-to-fine
baseline GeoTransformer [12] and other point-wise descriptors [8, 1, 20]. We also compare the
our learnable compatibility graph embedding (CGE) with spectral matching (SM) [62] for outlier
rejection in our method. With RANSAC filtering out severe outliers in advance, spectral matching
can lead to better performance than learning-based CGE in unseen domains. Notably, all point-wise
methods including CAST exhibit lower registration recalls in generalization studies than patch-wise
local descriptor SpinNet [19] and BUFFER [48] incorporating patch-wise and point-wise features.
This is mainly because they adopt a feature pyramid network architecture to learn features with
abundant global context, which is detrimental for generalization [19]. Furthermore, we conduct an
unsupervised domain adaptation (UDA) experiment for CAST, which tunes the network by learning
to align a point cloud to itself after random rotation and cropping. The results indicate that our model
can easily adapt to an unseen domain and achieve robust and accurate performance after one epochâ€™s
unsupervised tuning (only 20 minutes on an NVIDIA RTX3090 GPU)."
REFERENCES,0.8155737704918032,"Experiment Statistical Significance.
Finally, Table 10 reports the standard deviations (1-sigma) of
our evaluation metrics, which are assumed to be Gaussian distributed. Despite the randomness from
voxel down-sampling and RANSAC, the performance of our method remains stable. Notably, the
runtime of some methods such as [8, 19, 50] in Table 3 are quite different from results in [48], since
we report the average runtime including data preprocessing, feature extraction, feature matching, and
pose estimation, while the source codes of these methods save some intermediate results such as
descriptors to avoid repeated calculation of the same point cloud in different pairs, which leads to
unfair runtime comparison."
REFERENCES,0.8176229508196722,Table 11: Results of generalization from KITTI to ETH.
REFERENCES,0.819672131147541,"Methods
RTE (cm)
RRE (Â°)
RR (%)"
REFERENCES,0.8217213114754098,"FCGF [8]
9.08
0.94
45.86
Predator [20]
11.72
1.38
65.64
SpinNet [19]
6.05
0.98
99.44
TCKDD [1]
9.61
0.88
92.43
GeoTransformer [12]
5.97
0.73
91.87
BUFFER [48]
6.02
0.71
100.00"
REFERENCES,0.8237704918032787,"CAST (CGE)
6.85
0.65
97.76
CAST (SM)
6.66
0.61
98.04
CAST + UDA
5.25
0.56
99.58"
REFERENCES,0.8258196721311475,"A.4
Qualitative Results"
REFERENCES,0.8278688524590164,"Figure 3, Figure 7, and Figure 8 provide qualitative results about the registration performance on
outdoor datasets KITTI [39], nuScenes [40], and the indoor dataset 3DMatch [16], respectively."
REFERENCES,0.8299180327868853,"A.5
Limitation"
REFERENCES,0.8319672131147541,"The main limitation of the proposed CAST is the sub-optimal performance in low overlapping
scenarios such as the 3DLoMatch benchmark compared to state-of-the-art methods, which may be
ascribed to two aspects. (1) There is no effective outlier rejection for coarse matching as it is difficult
to search inliers from patch correspondences based on geometry consistency due to low resolution.
(2) Due to the sparsity and non-uniformity of keypoints, the inlier ratio of keypoint correspondences
still falls short of what is required for robust pose estimation without a hypothesis-and-selection
pipeline. Nevertheless, this RANSAC-free lightweight fine matching pipeline can achieve satisfying
performance in outdoor scenarios. Considering the superior PIR and PMR of our coarse matching,
we may directly exploit dense feature matching to enhance the robustness in low overlapping point
cloud registration scenarios as a future work."
REFERENCES,0.8340163934426229,"A.6
Broader Impacts"
REFERENCES,0.8360655737704918,"We present a novel consistency-aware spot-guided Transformer based on sparse attention to extract
consistent coarse correspondences from point clouds. In addition, we propose a lightweight fine
matching module for versatile and hierarchical point cloud registration, benefiting from the efficiency
of sparse keypoint matching and the accuracy of dense registration. Different from existing methods,
our fine matching is based on flexible local attention instead of optimal transport heavily relying on
patch-to-patch correspondences, thus allowing independent deployment without coarse matching.
Besides, the sparsity of keypoints ensures the efficiency of spatial consistency filtering."
REFERENCES,0.8381147540983607,"Enjoying these merits, this work not only achieves superior accuracy, efficiency, and robustness in
point cloud registration, but also paves the way to various large-scale real-time applications, such
as SfM, SLAM, autonomous driving, or any other where point cloud registration plays a role. For
examples, the reconstruction of indoor scenes and objects from unlabeled 3D scans could benefit from
our work, which can precisely recover the rigid transformation between different scans. Additionally,
our fine matching may independently construct a real-time LiDAR-based or RGBD camera-based
odometry system for SLAM or SfM, as it is capable of efficient and reliable local data association
and accurate pose estimation between two large-scale point clouds with a strong pose prior, while our
coarse matching could be utilized in place recognition and global re-localization in SLAM."
REFERENCES,0.8401639344262295,"As our work aims at tackling a fundamental problem in 3D computer vision, we do not anticipate
a direct negative impact. Potential negative outcomes might occur in real applications where our
method is involved."
REFERENCES,0.8422131147540983,"Source Frame 
with Keypoints"
REFERENCES,0.8442622950819673,"Target Frame
with Keypoints"
REFERENCES,0.8463114754098361,"Sparse Keypoint 
Correspondences"
REFERENCES,0.8483606557377049,Pose Estimation
REFERENCES,0.8504098360655737,"Figure 7: Qualitative registration results on nuScenes dataset. We show three examples in three
columns to demonstrate the effectness of CAST in keypoint extraction, matching, and pose estimation."
REFERENCES,0.8524590163934426,"RMSE: 3.30m
RMSE: 2.92m
RMSE: 3.71m
RMSE: 0.16m"
REFERENCES,0.8545081967213115,"RMSE: 4.24m
RMSE: 4.87m
RMSE: 4.31m
RMSE: 0.09m"
REFERENCES,0.8565573770491803,"RMSE: 0.21m
RMSE: 0.21m
RMSE: 0.30m
RMSE: 0.16m"
REFERENCES,0.8586065573770492,"RMSE: 2.90m
RMSE: 3.95m
RMSE: 3.94m
RMSE: 0.06m"
REFERENCES,0.860655737704918,"RMSE: 0.29m
RMSE: 2.94m
RMSE: 1.53m
RMSE: 0.07m"
REFERENCES,0.8627049180327869,"CoFiNet
GeoTransformer
RoITr
Ours (CAST)
Ground-Truth"
REFERENCES,0.8647540983606558,"Figure 8: Qualitative registration results of CoFiNet [11], GeoTransformer [12], RoITr [28], and
CAST compared with the ground truth alignment on 3DMatch dataset. We present five examples in
five rows, which demonstrate the robustness and accuracy of our method."
REFERENCES,0.8668032786885246,NeurIPS Paper Checklist
CLAIMS,0.8688524590163934,1. Claims
CLAIMS,0.8709016393442623,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: Our abstract and introduction clearly state the claims made, including the
motivations, contributions, and the performance of our approach.
Guidelines:"
CLAIMS,0.8729508196721312,"â€¢ The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢ The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢ The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.875,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We have detailed the limitations of our work in Sec. A.5 of the appendix,
and we also point out the scope of our claims made, including the benchmarks and sensors
(Sec. A.2) and the computational efficiency (Sec. 4), etc.
Guidelines:"
CLAIMS,0.8770491803278688,"â€¢ The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
â€¢ The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
â€¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢ The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
â€¢ The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢ If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
â€¢ While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.8790983606557377,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
CLAIMS,0.8811475409836066,Answer: [NA]
CLAIMS,0.8831967213114754,Justification: This paper does not include any theoretical results.
CLAIMS,0.8852459016393442,Guidelines:
CLAIMS,0.8872950819672131,"â€¢ The answer NA means that the paper does not include theoretical results.
â€¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢ The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
â€¢ Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.889344262295082,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8913934426229508,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8934426229508197,"Answer: [Yes]
Justification: This paper fully discloses all the information needed to reproduce the main
experimental results, including the detailed system architecture in Sec. 3 and some modular
architectures along with the hyper-parameters in Sec. A.1, the training and evaluation
settings in Sec. 4, and datasets with metrics in Sec. A.2 and Sec. A.3, respectively."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8954918032786885,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8975409836065574,"â€¢ The answer NA means that the paper does not include experiments.
â€¢ If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
â€¢ Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
â€¢ While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.8995901639344263,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9016393442622951,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We have submitted the data and codes in the supplementary material with
detailed instructions on data access and preparation as well as guidelines to reproduce all
experimental results. The paper will provide public access to the codes upon acceptance.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9036885245901639,"â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
â€¢ While we encourage the release of code and data, we understand that this might not be
possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢ The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
â€¢ The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢ The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢ At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
â€¢ Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
OPEN ACCESS TO DATA AND CODE,0.9057377049180327,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: This paper fully specifies all the training and evaluation settings, including the
optimizer and the learning rate scheduler in Sec. 4, hyper-parameters in Sec. A, data splits
and preparation in Sec. A.2.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9077868852459017,"â€¢ The answer NA means that the paper does not include experiments.
â€¢ The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
â€¢ The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
OPEN ACCESS TO DATA AND CODE,0.9098360655737705,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: In this paper, all metrics are assumed to be normally distributed, whose
empirical standard deviations (1-sigma) in repeated experiments are reported in Table 10 to
demonstrate the experiment statistical significance. The randomness mainly comes from
voxel down-sampling and RANSAC.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9118852459016393,â€¢ The answer NA means that the paper does not include experiments.
OPEN ACCESS TO DATA AND CODE,0.9139344262295082,"â€¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
â€¢ The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
â€¢ The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢ It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
â€¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
â€¢ For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢ If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9159836065573771,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9180327868852459,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9200819672131147,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9221311475409836,"Justification: In Sec. 4, we have detailed the type of CPU and GPU of our device, and the
runtime of the proposed method and nearly all baselines on our computer."
EXPERIMENTS COMPUTE RESOURCES,0.9241803278688525,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9262295081967213,"â€¢ The answer NA means that the paper does not include experiments.
â€¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
â€¢ The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢ The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didnâ€™t make it into the paper)."
CODE OF ETHICS,0.9282786885245902,9. Code Of Ethics
CODE OF ETHICS,0.930327868852459,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9323770491803278,Answer: [Yes]
CODE OF ETHICS,0.9344262295081968,Justification: This research conforms with the NeurIPS Code of Ethics in every respect.
CODE OF ETHICS,0.9364754098360656,Guidelines:
CODE OF ETHICS,0.9385245901639344,"â€¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢ If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9405737704918032,10. Broader Impacts
BROADER IMPACTS,0.9426229508196722,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.944672131147541,Answer: [Yes]
BROADER IMPACTS,0.9467213114754098,Justification: The broader impacts are discussed in Sec. A.6 of the appendix.
BROADER IMPACTS,0.9487704918032787,Guidelines:
BROADER IMPACTS,0.9508196721311475,"â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢ If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢ Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
â€¢ The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢ The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢ If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
BROADER IMPACTS,0.9528688524590164,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper poses no such risks for misuse.
Guidelines:"
BROADER IMPACTS,0.9549180327868853,"â€¢ The answer NA means that the paper poses no such risks.
â€¢ Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
â€¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢ We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
BROADER IMPACTS,0.9569672131147541,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: This paper involves existing benchmarks and utilizes existing methods for
evaluation, whose papers are properly cited with licenses. The related URL are included in
our codes.
Guidelines:"
BROADER IMPACTS,0.9590163934426229,"â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢ The authors should state which version of the asset is used and, if possible, include a
URL."
BROADER IMPACTS,0.9610655737704918,"â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢ For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
â€¢ If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
â€¢ For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
â€¢ If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators."
NEW ASSETS,0.9631147540983607,13. New Assets
NEW ASSETS,0.9651639344262295,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9672131147540983,Answer: [Yes]
NEW ASSETS,0.9692622950819673,"Justification: This paper provides details of our model in both main part (Sec. 3) and the
appendix (Sec. A.1). Our source codes have been submitted in the supplementary material
while no new datasets are proposed."
NEW ASSETS,0.9713114754098361,Guidelines:
NEW ASSETS,0.9733606557377049,"â€¢ The answer NA means that the paper does not release new assets.
â€¢ Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢ The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢ At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9754098360655737,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9774590163934426,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9795081967213115,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9815573770491803,Justification: This paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9836065573770492,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985655737704918,"â€¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢ Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
â€¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9877049180327869,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897540983606558,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9918032786885246,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9938524590163934,Justification: This paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9959016393442623,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979508196721312,"â€¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
â€¢ We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢ For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
