Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0016051364365971107,"Many modern machine learning algorithms such as generative adversarial networks
(GANs) and adversarial training can be formulated as minimax optimization. Gradi-
ent descent ascent (GDA) is the most commonly used algorithm due to its simplicity.
However, GDA can converge to non-optimal minimax points. We propose a new
minimax optimization framework, GDA-AM, that views the GDA dynamics as a
ﬁxed-point iteration and solves it using Anderson Mixing to converge to the local
minimax. It addresses the diverging issue of simultaneous GDA and accelerates
the convergence of alternating GDA. We show theoretically that the algorithm can
achieve global convergence for bilinear problems under mild conditions. We also
empirically show that GDA-AM solves a variety of minimax problems and improves
adversarial training on several datasets. Codes are available on Github 1."
INTRODUCTION,0.0032102728731942215,"1
INTRODUCTION"
INTRODUCTION,0.004815409309791332,"Minimax optimization has received a surge of interest due to its wide range of applications in modern
machine learning, such as generative adversarial networks (GAN), adversarial training and multi-
agent reinforcement learning (Goodfellow et al., 2014; Madry et al., 2018; Li et al., 2019). Formally,
given a bivariate function f(x, y), the objective is to ﬁnd a stable solution where the players cannot
improve their objective, i.e., to ﬁnd the Nash equilibrium of the underlying game (von Neumann &
Morgenstern, 1944):
arg min
x∈X
arg max
y∈Y
f(x, y).
(1)"
INTRODUCTION,0.006420545746388443,"It is commonplace to use simple algorithms such as gradient descent ascent (GDA) to solve such
problems, where both players take a gradient update simultaneously or alternatively. Despite its
simplicity, GDA is known to suffer from a generic issue for minimax optimization: it may cycle
around a stable point, exhibit divergent behavior, or converge very slowly since it requires very
small learning rates (Gidel et al., 2019a; Mertikopoulos et al., 2019). Given the widespread usage of
gradient-based methods for solving machine learning problems, ﬁrst-order optimization algorithms to
solve minimax problems have gained considerable popularity in the last few years. Algorithms such
as optimistic Gradient Descent Ascent (OG) (Daskalakis et al., 2018; Mertikopoulos et al., 2019) and
extra-gradient (EG) (Gidel et al., 2019a) can alleviate the issue of GDA for some problems. Yet, it
has been shown that these methods can still diverge or cycle around a stable point (Adolphs et al.;
Mazumdar et al., 2019; Parker-Holder et al., 2020). For example, these algorithms even fail to ﬁnd
a local minimax (the set of local minimax is a superset of local Nash (Jin et al., 2020; Wang et al.,
2020)) as shown in Figure 1. This leads to the following question: Can we design better algorithms
for minimax problems? We answer this in the afﬁrmative, by introducing GDA-AM. We cast the GDA
dynamics as a ﬁxed-point iteration problem and compute the iterates effectively using an advanced
nonlinear extrapolation method. We show that indeed our algorithm has theoretical and empirical
guarantees across a broad range of minimax problems, including GANs."
INTRODUCTION,0.008025682182985553,"∗hhe37, szhao89, yxi26, jho31@emory.edu
†saad@umn.edu
1https://github.com/hehuannb/GDA-AM"
INTRODUCTION,0.009630818619582664,Published as a conference paper at ICLR 2022
INTRODUCTION,0.011235955056179775,"(a) Cycling Behavoir
(b) Diverging Behavoir
(c) Converging to a non-optima"
INTRODUCTION,0.012841091492776886,"Figure 1: Left:f(x, y) = (4x2−(y−3x+0.05x3)2−0.1y4)e−0.01(x2+y2). Middle: −3x2−y2+4xy.
Right: f(x, y) = 2x2 + y2 + 4xy + 4"
INTRODUCTION,0.014446227929373997,3y3 −1
INTRODUCTION,0.016051364365971106,"4y4. We can observe that baseline methods fail to
converge to a local minimax, whereas GDA-AM with table size p = 3 always exhibits desirable
behaviors."
INTRODUCTION,0.01765650080256822,"Our contributions:
In this paper, we propose a different approach to solve minimax optimization.
Our starting point is to cast the GDA dynamics as a ﬁxed-point iteration. We then highlight that the
ﬁxed-point iteration can be solved effectively by using advanced non-linear extrapolation methods
such as Anderson Mixing (Anderson, 1965), which we name as GDA-AM. redAlthough ﬁrst mentioned
in Azizian et al. (2020), to our best knowledge, this is still the ﬁrst work to investigate and improve
the GDA dynamics by tapping into advanced ﬁxed-point algorithms."
INTRODUCTION,0.019261637239165328,"We demonstrate that GDA dynamics can beneﬁt from Anderson Mixing. In particular, we study
bilinear games and give a systematic analysis of GDA-AM for both simultaneous and alternating
versions of GDA. We theoretically show that GDA-AM can achieve global convergence guarantees
under mild conditions."
INTRODUCTION,0.02086677367576244,"We complement our theoretical results with numerical simulations across a variety of minimax
problems. We show that for some convex-concave and non-convex-concave functions, GDA-AM can
converge to the optimal point with little hyper-parameter tuning whereas existing ﬁrst-order methods
are prone to divergence and cycling behaviors."
INTRODUCTION,0.02247191011235955,"We also provide empirical results for GAN training across two different datasets, CIFAR10 and
CelebA. Given the limited computational overhead of our method, the results suggest that an extrapo-
lation add-on to GDA can lead to signiﬁcant performance gains. Moreover, the convergence behavior
across a variety of problems and the ease-of-use demonstrate the potential of GDA-AM to become
the minimax optimization workhorse."
PRELIMINARIES AND BACKGROUND,0.024077046548956663,"2
PRELIMINARIES AND BACKGROUND"
MINIMAX OPTIMIZATION,0.025682182985553772,"2.1
MINIMAX OPTIMIZATION"
MINIMAX OPTIMIZATION,0.027287319422150885,"Deﬁnition 1. Point (x∗, y∗) is a local Nash equilibrium of f if there exists δ > 0 such that for any
(x, y) satisfying ∥x −x∗∥≤δ and ∥y −y∗∥≤δ we have: f (x∗, y) ≤f (x∗, y∗) ≤f (x, y∗) ."
MINIMAX OPTIMIZATION,0.028892455858747994,"To ﬁnd the Nash equilibria, common algorithms including GDA, EG and OG, can be formulated as
follows. For the two variants of GDA, simultaneous GDA (SimGDA) and alternating GDA (AltGDA),
the updates have the following forms:"
MINIMAX OPTIMIZATION,0.030497592295345103,"Simultaneous :
xt+1 = xt −η∇xf(xt, yt),
yt+1 = yt + η∇yf(xt, yt)
Alternating :
xt+1 = xt −η∇xf(xt, yt),
yt+1 = yt + η∇yf(xt+1, yt).
(2)"
MINIMAX OPTIMIZATION,0.03210272873194221,The EG update has the following form: xt+ 1
MINIMAX OPTIMIZATION,0.033707865168539325,"2 = xt −η∇xf(xt, yt),
yt+ 1"
MINIMAX OPTIMIZATION,0.03531300160513644,"2 = yt + η∇yf(xt, yt)"
MINIMAX OPTIMIZATION,0.03691813804173355,xt+1 = xt −η∇xf(xt+ 1
MINIMAX OPTIMIZATION,0.038523274478330656,"2 , yt+ 1"
MINIMAX OPTIMIZATION,0.04012841091492777,"2 ),
yt+1 = yt + η∇yf(xt+ 1"
MINIMAX OPTIMIZATION,0.04173354735152488,"2 , yt+ 1"
MINIMAX OPTIMIZATION,0.04333868378812199,"2 ).
(3)"
MINIMAX OPTIMIZATION,0.0449438202247191,Published as a conference paper at ICLR 2022
MINIMAX OPTIMIZATION,0.04654895666131621,The OG update has the following form:
MINIMAX OPTIMIZATION,0.048154093097913325,"xt+1 = xt−η∇xf(xt, yt)+ η"
MINIMAX OPTIMIZATION,0.04975922953451043,"2∇xf(xt−1, yt−1), yt+1 = yt+η∇yf(xt, yt)−η"
MINIMAX OPTIMIZATION,0.051364365971107544,"2∇yf(xt−1, yt−1).
(4)"
MINIMAX OPTIMIZATION,0.052969502407704656,"2.2
FIXED-POINT ITERATION AND ANDERSON MIXING (AM)"
MINIMAX OPTIMIZATION,0.05457463884430177,Deﬁnition 2. w⋆is a ﬁxed point of the mapping g if w⋆= g (w⋆) .
MINIMAX OPTIMIZATION,0.056179775280898875,"Consider the simple ﬁxed-point iteration wt+1 = g(wt) which produces a sequence of iterates
{w0, w1, · · · , wN}. In most cases, this converges to the ﬁxed-point, w∗= g(w∗). Take gradient
descent as an example, it can be viewed as iteratively applying the operation: wt+1 = g (wt) ≜
wt−αt∇f (wt) , where the limit is the ﬁxed-point w⋆= g (w⋆) (i.e.∇f (wt = 0) . SimGDA updates
can be deﬁned as the repeated application of a nonlinear operator:"
MINIMAX OPTIMIZATION,0.05778491171749599,"wt+1 = G(sim)
η
(wt) ≜wt −ηV (wt) with w =

x
y"
MINIMAX OPTIMIZATION,0.0593900481540931,"
, V (w) =

∇xf(x, y)
−∇yf(x, y) "
MINIMAX OPTIMIZATION,0.060995184590690206,"Similarly, we can write AltGDA updates as wt+1 = G(alt)
η
(wt). An issue with ﬁxed-point iteration
is that it does not always converge, and even in the cases where it does converge, it might do so very
slowly. GDA is one example that it could result in the possibility of the operator converging to a limit
cycle instead of a single point for the GDA dynamic. A way of dealing with these problems is to use
acceleration methods, which can potentially speed up the convergence process and in some cases
even decrease the likelihood for divergence."
MINIMAX OPTIMIZATION,0.06260032102728733,"There are many different acceleration methods, but we will put our focus on an algorithm which we
refer to as Anderson Mixing (or Anderson Acceleration). In short, Anderson Mixing (AM) shares the
same idea as Nesterov’s acceleration. Given a ﬁxed-point iteration wt = g (wt−1), Anderson Mixing
argues that a good approximation to the ﬁnal solution w∗can be obtained as a linear combination
of the previous p iterates wt+1 = Pp
i=0 βig (wt−pt+i). Since obtaining the proper coefﬁcients βi is
a nonlinear procedure, Anderson Mixing is also known as a nonlinear extrapolation method. The
general form of Anderson Mixing is shown in Algorithm 1. For efﬁciency, we prefer a ‘restarted’
version with a small table size p that cleans up the table F every p iterations because it avoids solving
a linear system of increasing size."
MINIMAX OPTIMIZATION,0.06420545746388442,"Algorithm 1: Anderson Mixing Prototype (truncated version)
Input: Initial point w0, Anderson restart dimension p, ﬁxed-point mapping g : Rn →Rn.
Output: wt+1
for t = 0, 1, ... do"
MINIMAX OPTIMIZATION,0.06581059390048154,"Set pt = min{t, p}.
Set Ft = [ft−pt, . . . , ft], where fi = g(wi) −wi for each i ∈[t −pt . . t].
Determine weights β = (β0, . . . , βpt)T that solves minβ ∥Ftβ∥2 , s. t. Ppt
i=0 βi = 1.
Set wt+1 = Ppt
i=0 βig (wt−pt+i).
end"
MINIMAX OPTIMIZATION,0.06741573033707865,"2.3
AM AND GENERALIZED MINIMAL RESIDUAL (GMRES)"
MINIMAX OPTIMIZATION,0.06902086677367576,"Developed by Saad & Schultz (1986), Generalized Minimal Residual method (GMRES) is a Krylov
subspace method for solving linear system equations. The method approximates the solution by the
vector in a Krylov subspace with minimal residual, which is described below."
MINIMAX OPTIMIZATION,0.07062600321027288,"Deﬁnition 3. Assume we have the linear system of equations x = b with ∈Rn×n, b ∈Rn and
an initial guess x0. Then we denote the initial residual by r0 = b −x0 and deﬁne the tth Krylov
subspace as Kt = span{r0, r0, · · · ,t−1 r0}."
MINIMAX OPTIMIZATION,0.07223113964686999,"The tth iterate xt of GMRES minimizes the norm of the residual rt = b −xt in Kt, that is, xt solves"
MINIMAX OPTIMIZATION,0.0738362760834671,"min
xt∈x0+Kt ∥b −xt∥2 ."
MINIMAX OPTIMIZATION,0.0754414125200642,Published as a conference paper at ICLR 2022
MINIMAX OPTIMIZATION,0.07704654895666131,"The following formulation is equivalent to GMRES minimization problem and more convenient for
implementation. It computes bxt such that"
MINIMAX OPTIMIZATION,0.07865168539325842,"bxt = arg min
bxt∈Kt
∥b −(x0 + bxt)∥2 = arg min
bxt∈Kt
∥r0 −bxt∥2 ."
MINIMAX OPTIMIZATION,0.08025682182985554,"Using a larger Krylov dimension will improve the convergence of the method, but will require more
memory. For this reason, a smaller Krylov subspace dimension t and ‘restarted’ versions of the
method are used in practice Saad (2003)."
MINIMAX OPTIMIZATION,0.08186195826645265,The convergence of GMRES can be studied through the magnitude of the residual polynomial.
MINIMAX OPTIMIZATION,0.08346709470304976,"Theorem 2.1 (Lemma 6.31 of Saad (2003)). Let bxt be the approximate solution obtained at the t-th
iteration of GMRES being applied to solve x = b, and denote the residual as rt = b −bxt. Then, rt
is of the form
rt = ft()r0,
(5)"
MINIMAX OPTIMIZATION,0.08507223113964688,"where
∥rt∥2 = ∥ft()r0∥2 = min
ft∈Pt ∥ft()r0∥2,
(6)"
MINIMAX OPTIMIZATION,0.08667736757624397,"where Pp is the family of polynomials with degree p such that fp(0) = 1, ∀fp ∈Pp, which are
usually called residual polynomials."
MINIMAX OPTIMIZATION,0.08828250401284109,"Although GMRES is applied to a system of linear equations not a ﬁxed-point problem, there is a
strong connection between Anderson Mixing and GMRES. In AM we are looking for a ﬁxed-point x
such that Gx −b −x = 0 and by rearranging this equation we get"
MINIMAX OPTIMIZATION,0.0898876404494382,b + (G −I)x = 0 ⇔(I −G)x = b.
MINIMAX OPTIMIZATION,0.09149277688603531,"Theorem 2.2 shows that if GMRES is applied to the system (I −G)x = b and AM is applied to
g(x) = Gx + b with the same initial guess and I −G is non-singular, then these are equivalent in
the sense that the iterates of each algorithm can be obtained directly from the iterates of the other
algorithm."
MINIMAX OPTIMIZATION,0.09309791332263243,"Theorem 2.2 (Equivalence between AM with restart and GMRES (Walker & Ni, 2011a)). Consider
the ﬁxed point iteration x = g(x) where g(x) = Gx + b for G ∈Rn×n and b ∈Rn. If I −G
is non-singular, Algorithm 1 produces exactly the same iterates as GMRES being applied to solve
(I −G)x = b when both algorithms start with the same initial guess."
MINIMAX OPTIMIZATION,0.09470304975922954,Theorem 2.2 can also be generalized to the restart version of AM an GMRES as well.
MINIMAX OPTIMIZATION,0.09630818619582665,"3
GDA-AM : GDA WITH ANDERSON MIXING"
MINIMAX OPTIMIZATION,0.09791332263242375,"We propose a novel minimax optimizer, called GDA-AM, that is inspired by recent advances in
parameter (or weight) averaging (Wu et al., 2020; Yazici et al., 2019). We argue that a nonlinear
adaptive average (combination) is a more appropriate choice for minimax optimization."
MINIMAX OPTIMIZATION,0.09951845906902086,"3.1
GDA WITH NA¨IVE ANDERSON MIXING"
MINIMAX OPTIMIZATION,0.10112359550561797,"We propose to exploit the dynamic information present in the GDA iterates to “smartly” combine the
past iterates. This is in contrast to the classical averaging methods (moving averaging and exponential
moving averaging) (Yang et al., 2019) that “blindly” combine past iterates. A na¨ıve adoption of
Anderson Mixing using the past p GDA iterates for both simGDA and altGDA has the following
form:"
MINIMAX OPTIMIZATION,0.10272873194221509,"Anderson mixing :
xt+1 = p
X"
MINIMAX OPTIMIZATION,0.1043338683788122,"i=0
βixt−p+i, yt+1 = p
X"
MINIMAX OPTIMIZATION,0.10593900481540931,"i=0
βiyt−p+i."
MINIMAX OPTIMIZATION,0.10754414125200643,"Since Zhang et al. (2021); Gidel et al. (2019b) show the AltGDA is superior to SimGDA in many
aspects, we brieﬂy summarized both Simultaneous and Alternating GDA-AM in Algorithms 2 and 3
with the truncated Anderson Mixing Algorithm 1 using a table size p."
MINIMAX OPTIMIZATION,0.10914927768860354,Published as a conference paper at ICLR 2022
MINIMAX OPTIMIZATION,0.11075441412520064,"Algorithm 2: Simultaneous GDA-AM
Input: x0, y0, stepsize η, Anderson table
size p
Output: xt, yt
Set w0 = [x0, y0], sx = length(x0)
for t = 0, 1, ... do"
MINIMAX OPTIMIZATION,0.11235955056179775,"xt, yt = wt[0 : sx −1], wt[sx : end]
xt+1 = xt −η∇xf(xt, yt)
yt+1 = yt −η∇yf(xt, yt)"
MINIMAX OPTIMIZATION,0.11396468699839486,"wt+1 =

xt+1
yt+1 "
MINIMAX OPTIMIZATION,0.11556982343499198,"Use Anderson Mixing with table size p
to extrapolate wt+1
end
xt, yt = wt+1[0 : sx −1], wt+1[sx : end]
return xt, yt"
MINIMAX OPTIMIZATION,0.11717495987158909,"Algorithm 3: Alternating GDA-AM
Input: x0, y0, stepsize η, Anderson table
size p
Output: xt, yt
Set w0 = [x0, y0], sx = length(x0)
for t = 0, 1, ... do"
MINIMAX OPTIMIZATION,0.1187800963081862,"xt, yt = wt[0 : sx −1], wt[sx : end]
xt+1 = xt −η∇xf(xt, yt)
yt+1 = yt −η∇yf(xt+1, yt)"
MINIMAX OPTIMIZATION,0.12038523274478331,"wt+1 =

xt+1
yt+1 "
MINIMAX OPTIMIZATION,0.12199036918138041,"Use Anderson Mixing with table size p
to extrapolate wt+1
end
xt, yt = wt+1[0 : sx −1], wt+1[sx : end]
return xt, yt"
MINIMAX OPTIMIZATION,0.12359550561797752,"It is important to note that the Anderson Mixing form shown in Algorithm 1 is for illustrative purpose
and not computationally efﬁcient. For example, only one column of Ft needs to be updated at each
iteration. In addition, the solution of the least-square problem in Algorithm 1 can also be solved by
a quick QR update scheme which costs (2n + 1)p2 (Walker & Ni, 2011a). Thus, from Algorithms
2 and 3, we can see that the major cost of GDA-AM arises from solving the additional linear least
squares problem compared to regular GDA at each iteration. Additional implementation details are
provided in the Appendix."
CONVERGENCE RESULTS FOR GDA-AM,0.12520064205457465,"4
CONVERGENCE RESULTS FOR GDA-AM"
CONVERGENCE RESULTS FOR GDA-AM,0.12680577849117175,"In this section, we show that both simultaneous and alternating version GDA-AM converge to
the equilibrium for bilinear problems. First, we do not require the learning rate to be sufﬁciently
small. Second, we explicitly provide a linear convergence rate that is faster than EG and OG. More
importantly, we derive nonasymptotic rates from the spectrum analysis perspective because existing
theoretical results can not help us derive a convergent rate (see C.1)."
BILINEAR GAMES,0.12841091492776885,"4.1
BILINEAR GAMES"
BILINEAR GAMES,0.13001605136436598,"Bilinear games are often regarded as an important simple example for theoretically analyzing and
understanding new algorithms and techniques for solving general minimax problems (Gidel et al.,
2019a; Mertikopoulos et al., 2019; Schaefer & Anandkumar, 2019). In this section, we analyze the
convergence property of simultaneous GDA-AM and alternating GDA-AM schemes on the following
zero-sum bilinear games:"
BILINEAR GAMES,0.13162118780096307,"min
x∈Rn max
y∈Rn f(x, y) = xT Ay + bT x + cT y,
A is full rank.
(7)"
BILINEAR GAMES,0.1332263242375602,"The Nash equilibrium to the above problem is given by (x∗, y∗) = (−−T c, −−1b)."
BILINEAR GAMES,0.1348314606741573,"We also investigate bilinear-quadratic games from a spectrum analysis perspective. In addition, we
show that analysis based on the numerical range (Bollapragada et al., 2018) can be also extended to
such games, although it can not help derive a convergent bound for equation 7. Detailed discussion
can be found in Appendix C.1 and C.4.1."
SIMULTANEOUS GDA-AM,0.13643659711075443,"4.2
SIMULTANEOUS GDA-AM"
SIMULTANEOUS GDA-AM,0.13804173354735153,"Suppose x0 and y0 are the initial guesses for x∗and y∗, respectively. Then each iteration of
simultaneous GDA can be written in the following matrix form:

xt+1
yt+1"
SIMULTANEOUS GDA-AM,0.13964686998394862,"
=
 I
−ηA
ηAT
I "
SIMULTANEOUS GDA-AM,0.14125200642054575,"|
{z
}
G(Sim)"
SIMULTANEOUS GDA-AM,0.14285714285714285,"
xt
yt  |{z}"
SIMULTANEOUS GDA-AM,0.14446227929373998,"w(Sim)
t"
SIMULTANEOUS GDA-AM,0.14606741573033707,"−η

b
c "
SIMULTANEOUS GDA-AM,0.1476725521669342,"|{z}
b(Sim) .
(8)"
SIMULTANEOUS GDA-AM,0.1492776886035313,Published as a conference paper at ICLR 2022
SIMULTANEOUS GDA-AM,0.1508828250401284,"It has been shown that the iteration in equation 8 often cycles and fails to converge for the bilinear
problem due to the poor spectrum/numerical range of the ﬁxed point operator G(Sim) (Gidel et al.,
2019a; Azizian et al., 2020; Mokhtari et al., 2020a). Next we show that the convergence can be
improved with Algorithm 2.
Theorem 4.1. [Global convergence for simultaneous GDA-AM on bilinear problem] Denote the
distance between the stationary point w∗and current iterate w(k+1)p of Algorithm 2 with table size
p as N(k+1)p = ∥w∗−w(k+1)p∥. Then we have the following bound for Nt"
SIMULTANEOUS GDA-AM,0.15248796147672553,"N 2
(k+1)p ≤ρ(A)N 2
kp
(9)"
SIMULTANEOUS GDA-AM,0.15409309791332262,"where ρ(A) = (
1
Tp(1+
2
κ(T )−1 ))2. Here, Tp is the Chebyshev polynomial of ﬁrst kind of degree p and"
SIMULTANEOUS GDA-AM,0.15569823434991975,"1
Tp(1+
2
κ(T )−1 ) < 1 since 1 +
2
κ(T )−1 > 1."
SIMULTANEOUS GDA-AM,0.15730337078651685,"It is worthy emphasizing that the convergence rate of Algorithm 2 is independent of learning rate η
while the convergence results of other methods like EG and OG depend on the learning rate.
Remark 4.1.1. Both EG and OG have the following form of convergence rate (Mokhtari et al.,
2020a) for bilinear problem
N 2
t+1 ≤(1 −
c
κ(T ))N 2
t ,"
SIMULTANEOUS GDA-AM,0.15890850722311398,where c is a positive constant independent of the problem parameters.
SIMULTANEOUS GDA-AM,0.16051364365971107,"(a) Eigenvalues of iteration matrix of
SimGDA and GDA-AM"
SIMULTANEOUS GDA-AM,0.16211878009630817,"(b) Different condition number
(c) Different table size, condition
number κ = 100"
SIMULTANEOUS GDA-AM,0.1637239165329053,"Figure 2: Figure 2a: The blue line is the spectrum of matrix G(Sim) while the red line is spectrum
of matrix I −G(Sim). Our method transforms the divergent problem to a convergent problem due
to the transformed spectrum. Figure 2b: Convergence rate comparison between SimGDA-AM and
EG for different condition numbers of and ﬁxed table size p = 10, 20, 50. Figure 2c: Convergence
rate comparison between SimGDA-AM and EG for increasing table size on a matrix with condition
number 100."
ALTERNATING GDA-AM,0.1653290529695024,"4.3
ALTERNATING GDA-AM"
ALTERNATING GDA-AM,0.16693418940609953,"The underlying ﬁxed point iteration in Algorithm 3 can be written in the following matrix form:

xt+1
yt+1"
ALTERNATING GDA-AM,0.16853932584269662,"
=
 I
−ηA
ηAT
I −η2AT "
ALTERNATING GDA-AM,0.17014446227929375,"|
{z
}
G(Alt)"
ALTERNATING GDA-AM,0.17174959871589085,"
xt
yt  |{z}"
ALTERNATING GDA-AM,0.17335473515248795,"w(Alt)
t"
ALTERNATING GDA-AM,0.17495987158908508,"−η

b
c "
ALTERNATING GDA-AM,0.17656500802568217,"|{z}
b(Alt) ."
ALTERNATING GDA-AM,0.1781701444622793,"According to the equivalence between truncated Anderson acceleration and GMRES with restart, we
can analyze the convergence of Algorithm 3 through the convergence analysis of applying GMRES
to solve linear systems associated with G = I −G(Alt):"
ALTERNATING GDA-AM,0.1797752808988764,"G =

0
ηA
−ηAT
η2AT 
."
ALTERNATING GDA-AM,0.18138041733547353,Published as a conference paper at ICLR 2022
ALTERNATING GDA-AM,0.18298555377207062,"Theorem 4.2. [Global convergence for alternating GDA-AM on bi-
linear problem] Denote the distance between the stationary point
w∗and current iterate w(k+1)p of Algorithm 3 with table size p as
N(k+1)p = ∥w∗−w(k+1)p∥. Assume is normalized such that its
largest singular value is equal to 1. Then when the learning rate η is
less than 2, we have the following bound for Nt"
ALTERNATING GDA-AM,0.18459069020866772,"N 2
(k+1)p ≤
r"
ALTERNATING GDA-AM,0.18619582664526485,"1 +
2η
2 −η (r"
ALTERNATING GDA-AM,0.18780096308186195,"c)pN 2
kp"
ALTERNATING GDA-AM,0.18940609951845908,"where c and r are the center and radius of a disk D(c, r) which includes
all the eigenvalues of G. Especially, r"
ALTERNATING GDA-AM,0.19101123595505617,c < 1.
ALTERNATING GDA-AM,0.1926163723916533,"Figure 3: An illustration of the
spectrum of G (red) and the clos-
ing circle (blue) in Theorem 4.2."
ALTERNATING GDA-AM,0.1942215088282504,"Theorem 4.2 shows that when p >
log
q"
ALTERNATING GDA-AM,0.1958266452648475,"2−η
2+η
log r"
ALTERNATING GDA-AM,0.19743178170144463,"c
, alternating GDA-AM will converge globally."
DISCUSSION OF OBTAINED RATES,0.19903691813804172,"4.4
DISCUSSION OF OBTAINED RATES"
DISCUSSION OF OBTAINED RATES,0.20064205457463885,"We would like to ﬁrst explain on why taking Chebyshev polynomial of degree p at the point 1 +
2
κ−1.
We evaluate the Chebyshev polynomial at this speciﬁc point because the reciprocal of this value
gives the minimal value of inﬁnite norm of the all polynomials of degree p deﬁned on the interval
˜I = [η2σ2
min(), η2σ2
max()] based on Theorem 6.25 (page 209) (Saad, 2003). In other words, taking
the function value at this point leads to the tight bound."
DISCUSSION OF OBTAINED RATES,0.20224719101123595,"When comparing between existing bounds, we would like to point our our derived bounds are hard to
compare directly. The numerical experiments in ﬁgure 2b numerically verify that our bound is smaller
than EG. We wanted to numerically compare our rate with EG with positive momentum. However the
bound of EG with positive momentum is asymptotic. Moreover, it does not specify the constants so
we can not numerically compare them. We do provide empirical comparison between GDA-AM and
EG with positive momentum for bilinear problems in Appendix D.1. It shows GDA-AM outperforms
EG with positive momentum. Regarding alternating GDA-AM , we would like to note that the bound
in Theorem 4.2 depends on the eigenvalue distribution of the matrix G. Condition number is not
directly related to the distribution of eigenvalues of a nonsymmetric matrix G. Thus, the condition
number is not a precise metric to characterize the convergence. If these eigenvalues are clustered,
then our bound can be small. On the other hand, if these eigenvalues are evenly distributed in the
complex plane, then the bound can very close to 1."
DISCUSSION OF OBTAINED RATES,0.20385232744783308,"More importantly, we would like to stress several technical contributions."
DISCUSSION OF OBTAINED RATES,0.20545746388443017,"1 : Our obtained Theorem 4.1 and 4.2 provide nonasymptotic guarantees, while most other work are
asymptotic. For example, EG with positive momentum can achieve a asymptotic rate of 1−O(1/√κ)
under strong assumptions (Azizian et al., 2020)."
DISCUSSION OF OBTAINED RATES,0.20706260032102727,"2 : Our contribution is not just about ﬁx the convergence issue of GDA by applying Anderson Mixing;
another contribution is that we arrive at a convergent and tight bound on the original work and not
just adopting existing analyses. We developed Theorem 4.1 and 4.2 from a new perspective because
applying existing theoretical results fail to give us neither convergent nor tight bounds."
DISCUSSION OF OBTAINED RATES,0.2086677367576244,"3 : Theorem 4.1 and 4.2 only requires mild conditions and reﬂects how the table size p controls the
convergence rate. Theorem 4.1 is independent of the learning rate η. However, the convergence
results of other methods like EG and OG depend on the learning rate, which may yield less than
desirable results for ill-speciﬁed learning rates."
EXPERIMENTS,0.2102728731942215,"5
EXPERIMENTS"
EXPERIMENTS,0.21187800963081863,"In this section, we conduct experiments to see whether GDA-AM improves GDA for minimax
optimization from simple to practical problems. We ﬁrst investigate performance of GDA-AM on
bilinear games. In addition, we evaluate the efﬁcacy of our approach on GANs."
EXPERIMENTS,0.21348314606741572,Published as a conference paper at ICLR 2022
BILINEAR PROBLEMS,0.21508828250401285,"5.1
BILINEAR PROBLEMS"
BILINEAR PROBLEMS,0.21669341894060995,"In this section, we answer following questions: Q1: How is GDA-AM perform in terms of iteration
number and running time? Q2: How is the scalability of GDA-AM ? Q3: How is the performance of
GDA-AM using different table size p? Q4: Does GDA-AM converge for large step size η?"
BILINEAR PROBLEMS,0.21829855537720708,"We compare the performance with SimGDA, AltGDA, EG, and OG, and EG with Negative Momen-
tum(Azizian et al. (2020)) on bilinear minimax games shown in equation 7 without any constraint."
BILINEAR PROBLEMS,0.21990369181380418,", b, c, and initial points are generated using normally distributed random number. We set the maximum
iteration number as 1 × 106, stopping criteria 1 × 10−5 and depict convergence by use of the norm
of distance to optima, which is deﬁned as ∥w∗−wt∥. Similar to Azizian et al. (2020); Wei et al.
(2021a), the step size is set as 1 after rescaling to have 2-norm 1. We present results of different
settings in Figures 4, 5, and 6."
BILINEAR PROBLEMS,0.22150882825040127,"We ﬁrst generate different problem size (n = 100, 1000, 5000) and present results of convergence in
terms of iteration number in Figure 4. It can be observed that GDA-AM converges in much fewer
iterations for different problem sizes. Note that EG, EG-NM, and OG converge in the end but requires
many iterations, thus we plot only a portion for illustrative purposes. Figure 5 depicts the convergence
for all methods in terms of time. It can be observed that the running time of GDA-AM is faster
than EG. Although slower than OG, we can observe GDA-AM converges in much less time for all
problems. Figure 4 and Figure 5 answer Q1 and Q2; although there is additional computation for
GDA-AM , it does not hinder the beneﬁts of adopting Anderson Mixing. Even for a large problem
size, GDA-AM still converges in much less time than the baselines."
BILINEAR PROBLEMS,0.2231139646869984,"Next, we run GDA-AM using different table size p and show the results in Figure 6a and Figure
6b. Figure 6a indicates an increasing of table size results in faster convergence in terms of iteration
number, which also veriﬁes our claim in Theorem 4.1. However, we also observe an increased running
time when using a larger table size in Figure 6b. Further, we can see that p = 50 converges in a
comparable time and iterations to p = 100. Similar results are found in repeated experiments as well.
As a result, our answer to Q3 is that although a larger p means less iterations, a medium p is sufﬁcient
and a small p still outperforms the baselines. The optimal choice of p is related to the condition
number and step size, which is another interesting topic in the Anderson Mixing community."
BILINEAR PROBLEMS,0.2247191011235955,"Next, we answer Q4 on convergence under different step sizes. Although GDA-AM usually converges
with suitable step size, our theorem suggests it requires a larger table size when combined with a
extremely aggressive step size. Figure 6c shows the convergence under such circumstance. We can
observe that although a very large step size goes the wrong way in the beginning, Anderson Mixing
can still make it back on track except when η > 1. It answers the question and conﬁrms our claim
that GDA-AM can achieve global convergence for bilinear problems for a large step size η > 0."
BILINEAR PROBLEMS,0.22632423756019263,"(a) n = 100
(b) n = 500
(c) n = 1000"
BILINEAR PROBLEMS,0.22792937399678972,"Figure 4: Comparison in terms of iteration: minx maxy f(x, y) = xT Ay + bT x + cT y. We use
different problem size and ﬁx p = 10, η = 1 for all experiments."
BILINEAR PROBLEMS,0.22953451043338685,"5.2
GAN EXPERIMENTS: IMAGE GENERATION"
BILINEAR PROBLEMS,0.23113964686998395,"We apply our method to the CIFAR10 dataset (Krizhevsky, 2009) and use the ResNet architecture with
WGAN-GP (Gulrajani et al., 2017) and SNGAN (Miyato et al., 2018) objective. We also compared"
BILINEAR PROBLEMS,0.23274478330658105,Published as a conference paper at ICLR 2022
BILINEAR PROBLEMS,0.23434991974317818,"(a) Time comparison for Figure 4a
(b) Time comparison for Figure 4b
(c) Time comparison for Figure 4c"
BILINEAR PROBLEMS,0.23595505617977527,Figure 5: Comparison between methods in terms of time.
BILINEAR PROBLEMS,0.2375601926163724,"(a) Effects of p in terms of iteration
(b) Time compasion for Figure 6a
(c) Effect of step size η, p = 10"
BILINEAR PROBLEMS,0.2391653290529695,"Figure 6: Effects of table size p and step size η, n = 500"
BILINEAR PROBLEMS,0.24077046548956663,"the performance of GDA-AM using cropped CelebA (64×64) (Liu et al., 2015) on WGAN-GP. We
compare with Adam and extra-gradient with Adam (EG) as it offers signiﬁcant improvement over
OG. Models are evaluated using the inception score (IS) (Salimans et al., 2016) and FID (Heusel
et al., 2017) computed on 50,000 samples. For fair comparison, we ﬁxed the same hyperparamters
of Adam for all methods after an extensive search. Experiments were run with 5 random seeds. We
show results in Table 1. Table 1 reports the best IS and FID (averaged over 5 runs) achieved on these
datasets by each method. We see that GDA-AM yields improvements over the baselines in terms of
generation quality."
BILINEAR PROBLEMS,0.24237560192616373,"Table 1: Best inception scores and FID for Cifar10 and FID for CelebA (IS is a less informative
metric for celebA)."
BILINEAR PROBLEMS,0.24398073836276082,"WGAN-GP(ResNet)
SNGAN(ResNet)
CIFAR10
CelebA
CIFAR10
Method
IS ↑
FID ↓
FID
IS
FID
Adam
7.76 ±.11
22.45 ±.65
8.43 ±.05
8.21 ±.05
20.81 ±.16
EG
7.83 ±.08
20.73 ±.22
8.15 ±.06
8.15 ±.07
21.12 ±.19
Ours (GDA-AM )
8.05 ±.06
19.32 ±.16
7.82 ±.06
8.38 ±.04
18.84 ±.13"
CONCLUSION,0.24558587479935795,"6
CONCLUSION"
CONCLUSION,0.24719101123595505,"We prove the convergence property of GDA-AM and obtain a faster convergence rate than EG and
OG on the bilinear problem. Empirically, we verify our claim for such a problem and show the
efﬁcacy of GDA-AM in a deep learning setting as well. We believe our work is different from previous
approaches and takes an important step towards understanding and improving minimax optimization
by exploiting the GDA dynamic and reforming it with numerical techniques."
CONCLUSION,0.24879614767255218,Published as a conference paper at ICLR 2022
CONCLUSION,0.2504012841091493,ACKNOWLEDGMENTS
CONCLUSION,0.2520064205457464,"This work was funded in part by the NSF grant OAC 2003720, IIS 1838200 and NIH grant
5R01LM013323-03,5K01LM012924-03."
REFERENCES,0.2536115569823435,REFERENCES
REFERENCES,0.2552166934189406,"Leonard Adolphs, Hadi Daneshmand, Aurelien Lucchi, and Thomas Hofmann. Local saddle point
optimization: A curvature exploitation approach. Proceedings of Machine Learning Research.
PMLR."
REFERENCES,0.2568218298555377,Donald G. Anderson. Iterative procedures for nonlinear integral equations. 1965.
REFERENCES,0.25842696629213485,"Wa¨ıss Azizian, Damien Scieur, Ioannis Mitliagkas, Simon Lacoste-Julien, and Gauthier Gidel.
Accelerating smooth games by manipulating spectral shapes. In The 23rd International Conference
on Artiﬁcial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily,
Italy], volume 108 of Proceedings of Machine Learning Research, pp. 1705–1715. PMLR, 2020."
REFERENCES,0.26003210272873195,"Raghu Bollapragada, Damien Scieur, and Alexandre d’Aspremont. Nonlinear acceleration of mo-
mentum and primal-dual algorithms. arXiv preprint arXiv:1810.04539, 2018."
REFERENCES,0.26163723916532905,"Ronald E. Bruck. On the weak convergence of an ergodic iteration for the solution of variational
inequalities for monotone operators in hilbert space. Journal of Mathematical Analysis and
Applications, 1977."
REFERENCES,0.26324237560192615,"Daniela Calvetti, Bryan Lewis, and Lothar Reichel. On the regularizing properties of the gmres
method. Numerische Mathematik, 91(4):605–625, 2002."
REFERENCES,0.26484751203852325,"Michel Crouzeix and C´esar Palencia. The numerical range is a (1+2)-spectral set. SIAM Journal on
Matrix Analysis and Applications, 38(2):649–655, 2017."
REFERENCES,0.2664526484751204,"C. Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with optimism.
ArXiv, abs/1711.00141, 2018."
REFERENCES,0.2680577849117175,"Howard C Elman. Iterative methods for large, sparse, nonsymmetric systems of linear equations.
PhD thesis, Yale University New Haven, Conn, 1982."
REFERENCES,0.2696629213483146,"Haw-ren Fang and Yousef Saad. Two classes of multisecant methods for nonlinear acceleration.
Numerical Linear Algebra with Applications, 16(3):197–221, 2009."
REFERENCES,0.2712680577849117,"Bernd Fischer and Roland Freund. Chebyshev polynomials are not always optimal. Journal of
Approximation Theory, 65(3):261–272, 1991."
REFERENCES,0.27287319422150885,"Gauthier Gidel, Hugo Berard, Ga¨etan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A varia-
tional inequality perspective on generative adversarial networks. In 7th International Conference
on Learning Representations, ICLR, 2019a."
REFERENCES,0.27447833065810595,"Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, R´emi Le Priol, Gabriel Huang,
Simon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics.
In Proceedings of the Twenty-Second International Conference on Artiﬁcial Intelligence and
Statistics, Proceedings of Machine Learning Research. PMLR, 2019b."
REFERENCES,0.27608346709470305,"I. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, S. Ozair, Aaron C.
Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014."
REFERENCES,0.27768860353130015,"Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples, 2015."
REFERENCES,0.27929373996789725,"Anne Greenbaum. Iterative methods for solving linear systems. SIAM, 1997."
REFERENCES,0.2808988764044944,"Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of wasserstein gans. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing
Systems, 2017."
REFERENCES,0.2825040128410915,Published as a conference paper at ICLR 2022
REFERENCES,0.2841091492776886,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems, 2017."
REFERENCES,0.2857142857142857,"Yu-Guan Hsieh, F. Iutzeler, J. Malick, and P. Mertikopoulos. On the convergence of single-call
stochastic extra-gradient methods. In NeurIPS, 2019."
REFERENCES,0.28731942215088285,"Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvex-nonconcave
minimax optimization? In Proceedings of the 37th International Conference on Machine Learning,
Proceedings of Machine Learning Research, pp. 4880–4889. PMLR, 2020."
REFERENCES,0.28892455858747995,A. Krizhevsky. Learning multiple layers of features from tiny images. 2009.
REFERENCES,0.29052969502407705,"Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. ArXiv,
abs/1611.01236, 2017."
REFERENCES,0.29213483146067415,"Qi Lei, Sai Ganesh Nagarajan, Ioannis Panageas, and Xiao Wang. Last iterate convergence in
no-regret learning: constrained min-max optimization for convex-concave landscapes. In AISTATS,
2021."
REFERENCES,0.29373996789727125,"S. Li, Yi Wu, Xinyue Cui, Honghua Dong, Fei Fang, and Stuart J. Russell. Robust multi-agent
reinforcement learning via minimax deep deterministic policy gradient. In AAAI, 2019."
REFERENCES,0.2953451043338684,"Tianyi Lin, Chi Jin, and Michael I. Jordan. On gradient descent ascent for nonconvex-concave
minimax problems. In ICML, pp. 6083–6093, 2020. URL http://proceedings.mlr.
press/v119/lin20a.html."
REFERENCES,0.2969502407704655,"Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), December 2015."
REFERENCES,0.2985553772070626,"Luo Luo, Haishan Ye, Zhichao Huang, and Tong Zhang. Stochastic recursive gradient descent ascent
for stochastic nonconvex-strongly-concave minimax problems. In H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems,
2020."
REFERENCES,0.3001605136436597,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In 6th International Conference on
Learning Representations, ICLR 2018,, 2018."
REFERENCES,0.3017656500802568,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks, 2019."
REFERENCES,0.30337078651685395,"Eric V. Mazumdar, Michael I. Jordan, and S. Sastry. On ﬁnding local nash equilibria (and only local
nash equilibria) in zero-sum games. ArXiv, abs/1901.00838, 2019."
REFERENCES,0.30497592295345105,"Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar,
and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra
gradient mile. In 7th International Conference on Learning Representations, ICLR, 2019."
REFERENCES,0.30658105939004815,"Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Proceedings of
the 31st International Conference on Neural Information Processing Systems, NIPS’17, 2017."
REFERENCES,0.30818619582664525,"Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Y. Yoshida. Spectral normalization for
generative adversarial networks. ArXiv, abs/1802.05957, 2018."
REFERENCES,0.3097913322632424,"Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A uniﬁed analysis of extra-gradient and
optimistic gradient methods for saddle point problems: Proximal point approach. In International
Conference on Artiﬁcial Intelligence and Statistics, pp. 1497–1507. PMLR, 2020a."
REFERENCES,0.3113964686998395,"Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A uniﬁed analysis of extra-gradient and
optimistic gradient methods for saddle point problems: Proximal point approach. In Proceedings
of the Twenty Third International Conference on Artiﬁcial Intelligence and Statistics, Proceedings
of Machine Learning Research. PMLR, 2020b."
REFERENCES,0.3130016051364366,Published as a conference paper at ICLR 2022
REFERENCES,0.3146067415730337,"A. Nemirovski. Prox-method with rate of convergence o(1/t) for variational inequalities with lipschitz
continuous monotone operators and smooth convex-concave saddle point problems. SIAM J.
Optim., 2004."
REFERENCES,0.3162118780096308,"Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D. Lee, and Meisam Razaviyayn. Solving a
Class of Non-Convex Min-Max Games Using Iterative First Order Methods. Curran Associates
Inc., Red Hook, NY, USA, 2019."
REFERENCES,0.31781701444622795,"Dmitrii M. Ostrovskii, Andrew Lowy, and Meisam Razaviyayn. Efﬁcient search of ﬁrst-order nash
equilibria in nonconvex-concave smooth min-max problems, 2021."
REFERENCES,0.31942215088282505,"Jack Parker-Holder, Luke Metz, Cinjon Resnick, Hengyuan Hu, Adam Lerer, Alistair Letcher,
Alexander Peysakhovich, Aldo Pacchiano, and Jakob Foerster. Ridge rider: Finding diverse
solutions by following eigenvectors of the hessian. In Advances in Neural Information Processing
Systems, 2020."
REFERENCES,0.32102728731942215,"L. Popov. A modiﬁcation of the arrow-hurwicz method for search of saddle points. Mathematical
notes of the Academy of Sciences of the USSR, 1980."
REFERENCES,0.32263242375601925,"Youcef Saad and Martin H. Schultz. Gmres: A generalized minimal residual algorithm for solving
nonsymmetric linear systems. SIAM Journal on Scientiﬁc and Statistical Computing, 1986."
REFERENCES,0.32423756019261635,"Yousef Saad. Iterative methods for sparse linear systems. SIAM, 2003."
REFERENCES,0.3258426966292135,"Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and
Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing
Systems, 2016."
REFERENCES,0.3274478330658106,"Florian Schaefer and Anima Anandkumar. Competitive gradient descent. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information
Processing Systems, 2019."
REFERENCES,0.3290529695024077,"Kiran K Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh. Efﬁcient algorithms
for smooth minimax optimization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc,
E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.3306581059390048,"John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton
University Press, 1944."
REFERENCES,0.33226324237560195,"Homer F Walker and Peng Ni. Anderson acceleration for ﬁxed-point iterations. SIAM Journal on
Numerical Analysis, 49(4):1715–1735, 2011a."
REFERENCES,0.33386837881219905,Homer F. Walker and Peng Ni. Anderson acceleration for ﬁxed-point iterations. 2011b.
REFERENCES,0.33547351524879615,"Yuanhao Wang, Guodong Zhang, and Jimmy Ba. On solving minimax optimization locally: A
follow-the-ridge approach. In 8th International Conference on Learning Representations, ICLR
2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020."
REFERENCES,0.33707865168539325,"Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate convergence
in constrained saddle-point optimization. In International Conference on Learning Representations,
2021a. URL https://openreview.net/forum?id=dx11_7vm5_r."
REFERENCES,0.33868378812199035,"Fuchao Wei, Chenglong Bao, and Yang Liu. Stochastic anderson mixing for nonconvex stochastic
optimization. arXiv preprint arXiv:2110.01543, 2021b."
REFERENCES,0.3402889245585875,"Yue Wu, Pan Zhou, A. Wilson, E. Xing, and Zhiting Hu. Improving gan training with probability
ratio clipping and sample reweighting. ArXiv, abs/2006.06900, 2020."
REFERENCES,0.3418940609951846,"Zi Xu, Huiling Zhang, Yang Xu, and Guanghui Lan. A uniﬁed single-loop alternating gradient
projection algorithm for nonconvex-concave and convex-nonconcave minimax problems, 2021."
REFERENCES,0.3434991974317817,"Minghan Yang, A. Milzarek, Z. Wen, and T. Zhang. A stochastic extra-step quasi-newton method for
nonsmooth nonconvex optimization. arXiv: Optimization and Control, 2019."
REFERENCES,0.3451043338683788,Published as a conference paper at ICLR 2022
REFERENCES,0.3467094703049759,"Yasin Yazici, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, Georgios Piliouras, and Vijay
Chandrasekhar. The unusual effectiveness of averaging in GAN training. In 7th International
Conference on Learning Representations, ICLR , 2019, 2019."
REFERENCES,0.34831460674157305,"Guodong Zhang and Yuanhao Wang. On the suboptimality of negative momentum for minimax
optimization. In AISTATS, 2021."
REFERENCES,0.34991974317817015,"Guodong Zhang, Yuanhao Wang, Laurent Lessard, and Roger B. Grosse. Don’t ﬁx what ain’t broke:
Near-optimal local convergence of alternating gradient descent-ascent for minimax optimization.
CoRR, 2021."
REFERENCES,0.35152487961476725,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I.
Jordan. Theoretically principled trade-off between robustness and accuracy. CoRR, 2019. URL
http://arxiv.org/abs/1901.08573."
REFERENCES,0.35313001605136435,"Jiawei Zhang, Peijun Xiao, Ruoyu Sun, and Zhiquan Luo. A single-loop smoothed gradient descent-
ascent algorithm for nonconvex-concave min-max problems. In H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems,
2020."
REFERENCES,0.3547351524879615,Published as a conference paper at ICLR 2022
REFERENCES,0.3563402889245586,"A
RELATED WORK"
REFERENCES,0.3579454253611557,"There is a rich literature on different strategies to alleviate the issue of minimax optimization. A useful
add-on technique, Momentum, has been shown to be effective for bilinear games and strongly-convex-
strongly-concave settings (Zhang & Wang, 2021; Gidel et al., 2019b; Azizian et al., 2020). Several
second-order methods (Adolphs et al.; Mescheder et al., 2017; Mazumdar et al., 2019; Parker-Holder
et al., 2020) show that their stable ﬁxed points are exactly either Nash equilibria or local minimax
by incorporating second-order information. However, such methods are computationally expensive
and thus unsuitable for large applications such as image generation. Focusing on variants of GDA,
EG and OG are two widely studied algorithms on improving the GDA dynamics. EG proposed to
apply extra-gradient to overcome the cycling behaviour of GDA. OG, originally proposed in Popov
(1980) and rediscovered in Daskalakis et al. (2018); Mertikopoulos et al. (2019), is more efﬁcient by
storing and re-using the extrapolated gradient for the extrapolation step. Without projection, OG is
equivalent to extrapolation from past. Mokhtari et al. (2020b) shows that both of these algorithms can
be interpreted as approximations of the classical proximal point method and did a uniﬁed analysis
for bilinear games. These approaches mentioned the GDA dynamics can be viewed as a ﬁxed-point
iteration, but none of them further provides a solution to improve it. In this work, we ﬁll this gap
by proposing the application of the extrapolation method directly on the entire GDA dynamics.
Unlike OG, EG and their variants (Hsieh et al., 2019; Lei et al., 2021; Thekumparampil et al.,
2019; Yang et al., 2019), which regard minimax problems as variational inequality problems (Bruck,
1977; Nemirovski, 2004), our work is from a new perspective and thus orthogonal to these previous
approaches."
REFERENCES,0.3595505617977528,"In addition, several recent works consider nonconvex-concave minimax problems. Zhang et al.
(2020) introduced a “smoothing” scheme combined with GDA to stabilize the dynamic of GDA. Luo
et al. (2020) proposed a method called Stochastic Recursive gradiEnt Descent Ascent (SREDA) for
stochastic nonconvex-strongly-concave minimax problems, by estimating gradients recursively and
reducing its variance. Lin et al. (2020) showed that the two-timescale GDA can ﬁnd a stationary point
of nonconvex-concave minimax problems effectively. Ostrovskii et al. (2021) proposed a variant of
Nesterov’s accelerated algorithm to ﬁnd ϵ -ﬁrst-order Nash equilibrium that is a stronger criterion than
the commonly used proximal gradient norm. Nouiehed et al. (2019) proposed a iterative method that
ﬁnds ϵ -ﬁrst-order Nash equilibrium in O(ϵ−2) iterations under Polyak-Lojasiewicz (PL) condition.
Focusing on nonconvex minimax problems, they studied an interesting and difﬁcult problem. Since
our work cast insight on the effectiveness of solving minimax optimization via Anderson Mixing, we
expect the extension of this algorithm to general nonconvex problems can be further investigated in
the future."
REFERENCES,0.3611556982343499,"B
ANDERSON MIXING IMPLEMENTATION DETAILS"
REFERENCES,0.36276083467094705,"In this section, we discuss the efﬁcient implementation of Anderson Mixing. We start with generic
Anderson Mixing prototype (Algorithm 4) and then present the idea of Quick QR-update Anderson
Mixing implementation as described in Walker & Ni (2011b), which is commonly used in practice.
For each iteration t ≥0 , AM prototype solves a least squares problem with a normalization
constraint. The intuition is to minimize the norm of the weighted residuals of the previous m iterates."
REFERENCES,0.36436597110754415,"Algorithm 4: Anderson Mixing Prototype (truncated version)
Input: Initial point w0, Anderson restart dimension p, ﬁxed-point mapping g : Rn →Rn.
Output: wt+1
for t = 0, 1, ... do"
REFERENCES,0.36597110754414125,"Set pt = min{t, p}.
Set Ft = [ft−pt, . . . , ft], where fi = g(wi) −wi for each i ∈[t −pt . . t].
Determine weights β = (β0, . . . , βpt)T that solves minβ ∥Ftβ∥2 , s. t. Ppt
i=0 βi = 1.
Set wt+1 = Ppt
i=0 βig (wt−pt+i).
end"
REFERENCES,0.36757624398073835,"The constrained linear least-squares problem in Algorithm AA can be solved in a number of ways.
Our preference is to recast it in an unconstrained form suggested in Fang & Saad (2009); Walker &
Ni (2011b) that is straightforward to solve and convenient for implementing efﬁcient updating of QR."
REFERENCES,0.36918138041733545,Published as a conference paper at ICLR 2022
REFERENCES,0.3707865168539326,"Deﬁne fi = g(wi) −wi, △fi = fi+1 −fi for each i and set Ft = [ft−pt, . . . , ft], Ft =
[△ft−pt, . . . , △ft]. Then solving the least-squares problem (minβ ∥Ftβ∥2 , s. t. Ppt
i=0 βi = 1)
is equivalent to
min
γ=(γ0,...,γpt−1)
T ∥ft −Ftγ∥2
(10)"
REFERENCES,0.3723916532905297,"where α and γ are related by α0 = γ0, αi = γi −γi−1 for 1 ≤i ≤pt −1, and αpt = 1 −γpt−1."
REFERENCES,0.3739967897271268,"Now the inner minimization subproblem can be efﬁciently solved as an unconstrained least squares
problem by a simple variable elimination. This unconstrained least-squares problem leads to a
modiﬁed form of Anderson Mixing"
REFERENCES,0.3756019261637239,wt+1 = g (wt) −
REFERENCES,0.37720706260032105,"pt−1
X"
REFERENCES,0.37881219903691815,"i=0
γ(t)
i
[g (wt−pt+i+1) −g (wt−pt+i)] = g (wt) −Gtγ(t)"
REFERENCES,0.38041733547351525,"where Gt = [△gt−pt, . . . , △gt−1] with △gi = g(wi+1) −g(wi) for each i."
REFERENCES,0.38202247191011235,"To obtain γ(t) =

γ(t)
0 , . . . , γ(t)
pt−1
T
by solving equation 10 efﬁciently, we show how the successive
least-squares problems can be solved efﬁciently by updating the factors in the QR decomposition
Ft = QtRt as the algorithm proceeds. We assume a think QR decomposition, for which the solution
of the least-squares problem is obtained by solving the pt × pt linear system Rγ = Q′ ∗ft. Each Ft
is n × pt and is obtained from Ft−1 by adding a column on the right and, if the resulting number of
columns is greater than p, also cleaning up (re-initialize) the table. That is,we never need to delete
the left column because cleaning up the table stands for a restarted version of AM. As a result, we
only need to handle two cases; 1 the table is empty(cleaned). 2 the table is not full. When the table
is empty, we initialize F1 = Q1R1 with Q1 = △f0/ ∥△f0∥2 and R = ∥△f0∥2. If the table size is
smaller than p, we add a column on the right of Ft−1. Have Ft−1 = QR, we update Q and R so that
Ft = [Ft−1, ∆ft−1] = QR. It is a single modiﬁed Gram–Schmidt sweep that is described as follows:"
REFERENCES,0.38362760834670945,"Algorithm 5: QR-updating procedures
for i = 1, . . . , pt−1 do"
REFERENCES,0.3852327447833066,"Set R(i, pt) = Q(:, i)′ ∗△ft−1.
Update △ft−1 ←∆ft−1 −R (i, pt) ∗Q(:, i)
end
Set Q (:, pt) = △ft−1/ ∥△ft−1∥2 and R (pt, pt) = ∥∆ft−1∥2"
REFERENCES,0.3868378812199037,"Note that we do not explicitly conduct QR decomposition in each iteration, instead we update the
factors (O(p2n)) and then solve a linear system using back substitution which has a complexity of
O(p2). Based on this complexity analysis, we can ﬁnd Anderson Mixing with QR-updating scheme
has limited computational overhead than GDA (or OG). This explains why GDA-AM is faster than
EG but slower than OG in terms of running time of each iteration."
REFERENCES,0.3884430176565008,"C
THEORETICAL RESULTS"
REFERENCES,0.3900481540930979,"C.1
DIFFICULTY OF ANALYSIS ON GDA WITH ANDERSON MIXING"
REFERENCES,0.391653290529695,"In the analysis, we study the inherent structures of the dynamics of the ﬁxed point iteration and provide
the convergence analysis for both simultaneous and alternating schemes. We want to emphasize that
the direct application of existing convergence results of GMRES can not lead to convergent results.
A recent paper Bollapragada et al. (2018) study the convergence acceleration schemes for multi-step
optimization algorithms using Regularized Nonlinear Acceleration. We also want to point out that a
na¨ıve application of Crouzeix’s bound to the minimax optimization problem can not be used to derive
the convergent result.
Theorem C.1 (Fischer & Freund (1991)). Let n ≥5 be an integer, r > 1, and c ∈R. Consider the
following constrained polynomial minmax problem
min
p∈Pn:p(c)=1 max
z∈Er |p(z)|
(11) where"
REFERENCES,0.39325842696629215,"Er :=

z ∈C
 |z −1| + |z + 1| ≤r + 1 r"
REFERENCES,0.39486356340288925,"
(12)"
REFERENCES,0.39646869983948635,Published as a conference paper at ICLR 2022
REFERENCES,0.39807383627608345,and c ∈C \ Er. Then this problem can be solved uniquely by
REFERENCES,0.3996789727126806,tn(z; c) := Tn(z)
REFERENCES,0.4012841091492777,"Tn(c) ,
(13) where"
REFERENCES,0.4028892455858748,Tn(z) = 1 2
REFERENCES,0.4044943820224719,"
vn + 1 vn"
REFERENCES,0.406099518459069,"
,
z = 1 2"
REFERENCES,0.40770465489566615,"
v + 1 v"
REFERENCES,0.40930979133226325,"
(14)"
REFERENCES,0.41091492776886035,"if
(a) |c| ≥1"
REFERENCES,0.41252006420545745,"2

r
√"
REFERENCES,0.41412520064205455,"2 + r−
√ 2
or"
REFERENCES,0.4157303370786517,"(b) |c| ≥(1/2ar)

2a2
r −1 +
p"
REFERENCES,0.4173354735152488,"2a4r −a2r + 1

, where ar := 1"
REFERENCES,0.4189406099518459,"2
 
r + 1 r

."
REFERENCES,0.420545746388443,"This is because the point 0 where all the residual polynomials take the ﬁxed value of 1 is included
in the numerical range of the iteration matrix, which violates the assumption of Theorem C.1. As
a result, it can not be used to prove that the residual norm is decreasing based on this approach.
Instead, we show that although the coefﬁcient matrix is non-normal, it is diagonalizable. We then give
the convergence results based on the eigenvalues instead of the numerical range. More speciﬁcally,
Anderson mixing is equivalent to GMRES being applied to solve the following linear system:"
REFERENCES,0.42215088282504015,"(I −G(Alt))w = b(Alt),
with w0 = w(Alt)
0
.
(15)"
REFERENCES,0.42375601926163725,"Writing this linear system in the block form:

0
ηA
−ηAT
η2AT"
REFERENCES,0.42536115569823435,"
w = b(Alt).
(16)"
REFERENCES,0.42696629213483145,The residual norm bound for GMRES reads:
REFERENCES,0.42857142857142855,"∥rt∥2 = min
p∈P1
t
∥p(I −G(Alt))r0∥2.
(17)"
REFERENCES,0.4301765650080257,"Notice that the matrix (I −G(Alt)) is non-normal. If we apply Crouzeix’s bound in Crouzeix &
Palencia (2017) to our problem as Bollapragada et al. (2018) did, then we have the following bound"
REFERENCES,0.4317817014446228,"∥rt∥2
∥r0∥2
≤min
p∈P1
t
∥p(I −G(Alt))∥≤(1 +
√"
REFERENCES,0.4333868378812199,"2) min
p∈P1
t
sup
z∈W (I−G(Alt))
∥p(z)∥
(18)"
REFERENCES,0.434991974317817,"where W(I −G(Alt)) = {z∗(I −G(Alt))z, ∀z ∈C2n \ {0}, ∥z∥= 1} is the numerical range for
I −G(Alt). In order to simplify the upper bound in the previous theorem, we study the numerical"
REFERENCES,0.43659711075441415,"range of I −G(Alt) similar to Bollapragada et al. (2018). Writing z =

z1
z2"
REFERENCES,0.43820224719101125,"
and computing the"
REFERENCES,0.43980738362760835,numerical range of I −G(Alt) explicitly yields:
REFERENCES,0.44141252006420545,"[z∗
1, z∗
2]

0
ηA
−ηAT
η2AT"
REFERENCES,0.44301765650080255," 
z1
z2"
REFERENCES,0.4446227929373997,"
= η2z∗
2AT z2 + ηz∗
1z2 −ηz∗
2
T z1.
(19)"
REFERENCES,0.4462279293739968,"For a general matrix A, there is no special structure about the numerical range of I−G(Alt). However,
when is symmetric, we can decompose as = Pn
i=1 λivivT
i where {λi}n
i=1 are eigenvalues of in
decreasing order and {vi}n
i=1 are associated eigenvectors, and write AT = Pn
i=1 λ2
i vivT
i . Then we
can compute the numerical range of G(Alt) as follows: n
X"
REFERENCES,0.4478330658105939,"i
[z∗
1, z∗
2]

0
ηλivivT
i
−ηλivivT
i
η2λ2
i vivT
i"
REFERENCES,0.449438202247191," 
z1
z2 
= n
X"
REFERENCES,0.4510433386837881,"i
[z∗
1vi, z∗
2vi]

0
ηλi
−ηλi
η2λ2
i"
REFERENCES,0.45264847512038525,"
.

vT
i z1
vT
i z2"
REFERENCES,0.45425361155698235,"
(20)"
REFERENCES,0.45585874799357945,"Following the techniques proposed in Bollapragada et al. (2018) to analyze the numerical range of
general 2 × 2 matrices, we can show that the numerical range of I −G(Alt) is equal to the convex
hull of the union of the numerical range of"
REFERENCES,0.45746388443017655,"Gi =

0
ηλi
−ηλi
η2λ2
i"
REFERENCES,0.4590690208667737,"
, i = 1, . . . , n.
(21)"
REFERENCES,0.4606741573033708,Published as a conference paper at ICLR 2022
REFERENCES,0.4622792937399679,"And the boundary of numerical range of Gi is an ellipse whose axes are the line segments joining the
points x to y and w to z, respectively, with"
REFERENCES,0.463884430176565,"x = 0,
y = η2λ2
i ,
, w = η2λ2
i
2
−
√"
REFERENCES,0.4654895666131621,"−1η|λi|,
z = η2λ2
i
2
+
√"
REFERENCES,0.46709470304975925,"−1η|λi|.
(22)"
REFERENCES,0.46869983948635635,"Thus, the numerical range of I −G(Alt) can be spanned by convex hull of the union of the numerical
range of a set of 2-by-2 matrices and the numerical range of each such a 2-by-2 matrix is an ellipse.
We can compute the center o and focal distance d of the ellipse generated by numerical range of
I −G(Alt) explicitly. Then a linear transformation enables us to use Theorem C.1 to show that the
near-best polynomial for the minimax problem on the numerical range of I −G(Alt) is given by"
REFERENCES,0.47030497592295345,tn(z; c) := Tn( z−o
REFERENCES,0.47191011235955055,"d
)
Tn( c−o"
REFERENCES,0.47351524879614765,"d ) if 0 is excluded from the numerical range of I −G(Alt). However, according
to equation 22 the numerical range includes the point 0 where the residual polynomial takes value
1, thus the analysis based on numerical range can not help derive the convergent result as the upper
bound is not guaranteed to be less than 1."
REFERENCES,0.4751203852327448,"C.2
PROOFS OF THEOREM"
REFERENCES,0.4767255216693419,"We ﬁrst provide proof of Theorem 4.1.
Theorem C.2 (Global convergence for simultaneous GDA-AM on bilinear problem). Denote the
distance between the stationary point w∗and current iterate w(k+1)p of Algorithm 2 with Anderson
restart dimension p as N(k+1)p = dist(w∗, w(k+1)p). Then we have the following bound for Nt
Algorithm 2 is unconditionally convergent"
REFERENCES,0.478330658105939,"N(k+1)p ≤
1
Tp(1 +
2
κ(T )−1)Nkp
(23)"
REFERENCES,0.4799357945425361,"where Tp is the Chebyshev polynomial of ﬁrst kind of degree p and
1
Tp(1+
2
κ(T )−1 ) < 1 since 1 +"
REFERENCES,0.48154093097913325,"2
κ(T )−1 > 1."
REFERENCES,0.48314606741573035,"Proof of Theorem 4.1. Note that I −G(Sim) is a normal matrix which will be denoted as G for
notational simplicity. Thus it admits the following eigendecomposition:"
REFERENCES,0.48475120385232745,"G = UΛUT ,
UUT = I,
Λ = diag(λ1, . . . , λ2n).
(24)"
REFERENCES,0.48635634028892455,"Based on the equivalence between GMRES and Anderson Mixing, we know that the convergence
rate of simultaneous GDA-AM can be estimated by the spectrum of G. Especially, it holds that"
REFERENCES,0.48796147672552165,"r(k+1)p = Ufp(Λ)UT rkp. fp ∈Pp
(25)"
REFERENCES,0.4895666131621188,"where Pp is the family of residual polynomials with degree p such that fp(0) = 1, ∀fp ∈Pp.
According to Lemma 2.1, we have the following estimation"
REFERENCES,0.4911717495987159,"∥r(k+1)p∥2 = min
fp∈Pp ∥fp(G)rkp∥2 ≤min
fp∈Pp max
i
|fp(λi)|∥rkp∥2.
(26)"
REFERENCES,0.492776886035313,"Due to the block structure of G, the eigenvalues of G can be computed explicitly as"
REFERENCES,0.4943820224719101,"± ησi
√"
REFERENCES,0.4959871589085072,"−1, i = 1, . . . , n,
(27)"
REFERENCES,0.49759229534510435,"where σi is the ith largest singular value of matrix . This shows that the eigenvalues of G are n pairs
of purely imaginary numbers excluding 0 since has full rank."
REFERENCES,0.49919743178170145,Since the eigenvalues of G are distributed in two intervals excluding the origin
REFERENCES,0.5008025682182986,"I = [−ησmax()
√"
REFERENCES,0.5024077046548957,"−1, −ησmin()
√"
REFERENCES,0.5040128410914928,"−1] ∪[ησmin()
√"
REFERENCES,0.5056179775280899,"−1, ησmax()
√ −1],"
REFERENCES,0.507223113964687,"it can be shown that the following p-th degree polynomial with value 1 at the origin that has the
minimal maximum deviation from 0 on I is given by:"
REFERENCES,0.5088282504012841,fp(z) = Tl(q(√−1z))
REFERENCES,0.5104333868378812,"Tl(q(0))
,
q(
√"
REFERENCES,0.5120385232744783,−1z) = 1 −2(√−1z −ησmin)(√−1z + ησmin)
REFERENCES,0.5136436597110754,"(ησmax())2 −(ησmin())2
(28)"
REFERENCES,0.5152487961476726,Published as a conference paper at ICLR 2022
REFERENCES,0.5168539325842697,where l = [ p
REFERENCES,0.5184590690208668,"2] and Tl is the Chebyshev polynomial of ﬁrst kind of degree l. The function q(√−1z)
maps I to [−1, 1]. Thus the numerator of the polynomial fp is bounded by 1 on I. The size of
denominator can be determined by the method discussed in Chapter 3 of Greenbaum (1997). Assume
q(0) = 1"
REFERENCES,0.5200642054574639,"2(y + y−1), then Tl(q(0)) = 1"
REFERENCES,0.521669341894061,2(yl + y−l). Then y can be determined by solving
REFERENCES,0.5232744783306581,q(0) = (ησmax())2 + (ησmin())2
REFERENCES,0.5248796147672552,"(ησmax())2 −(ησmin())2 .
(29)"
REFERENCES,0.5264847512038523,The solutions to this equation are
REFERENCES,0.5280898876404494,y1 = ησmax() + ησmin()
REFERENCES,0.5296950240770465,"ησmax() −ησmin()
or
y2 = ησmax() −ησmin()"
REFERENCES,0.5313001605136437,"ησmax() + ησmin().
(30)"
REFERENCES,0.5329052969502408,Then plugging the value of q(0) into the polynomial fp yields
REFERENCES,0.5345104333868379,∥r(k+1)p∥
REFERENCES,0.536115569823435,"∥rkp∥
≤2
p"
REFERENCES,0.5377207062600321,"η2σ2max() −
p"
REFERENCES,0.5393258426966292,"η2σ2
min()
p"
REFERENCES,0.5409309791332263,"η2σ2max() +
p"
REFERENCES,0.5425361155698234,"η2σ2
min() l"
REFERENCES,0.5441412520064205,"= 2
σmax() −σmin()"
REFERENCES,0.5457463884430177,σmax() + σmin()
REFERENCES,0.5473515248796148,"l
= 2
κ() −1"
REFERENCES,0.5489566613162119,κ() + 1
REFERENCES,0.550561797752809,"l
(31)"
REFERENCES,0.5521669341894061,"Note that Nt and rt is related through G(wt −w∗) = rt. Therefore,"
REFERENCES,0.5537720706260032,"N(k+1)p = ∥w(k+1)p −w∗∥2 = ∥G−1r(k+1)p∥2 = min
fp∈Pp ∥G−1fp(G)G(wkp −w∗)∥2"
REFERENCES,0.5553772070626003,"≤min
fp∈Pp max
i
|fp(λi)|∥wkp −w∗∥2 ≤2

1 −
2
κ() + 1  p"
REFERENCES,0.5569823434991974,"2 Nkp.
(32)"
REFERENCES,0.5585874799357945,"Actually a tighter bound can be proved after noting that the problem is essentially equivalent to
polynomial minmax problem on the interval:
˜I = [η2σ2
min(), η2σ2
max()],"
REFERENCES,0.5601926163723917,"Then it is well known that,"
REFERENCES,0.5617977528089888,"N(k+1)p ≤min
fp∈Pp
max
λi∈[η2σ2
min(), η2σ2max()] |fp(λi)|∥wkp −w∗∥2 ≤
1"
REFERENCES,0.5634028892455859,"Tp(1 + 2
σ2
min
σ2max−σ2
min )
Nkp"
REFERENCES,0.565008025682183,"≤
1
Tp(1 +
2
κ(T )−1)Nkp (33)"
REFERENCES,0.5666131621187801,"where Tp Chebyshev polynomial of degree p of the ﬁrst kind and
1
Tp(1+
2
κ(T )−1 ) < 1. Explicitly,"
REFERENCES,0.5682182985553772,"Tp(1 +
2
κ(T ) −1) = 1 2"
REFERENCES,0.5698234349919743,"h
1 +
2
κ(T ) −1 + s"
REFERENCES,0.5714285714285714,"(1 +
2
κ(T ) −1)2 −1
p"
REFERENCES,0.5730337078651685,"+

1 +
2
κ(T ) −1 + s"
REFERENCES,0.5746388443017657,"(1 +
2
κ(T ) −1)2 −1
−pi"
REFERENCES,0.5762439807383628,"Next, we give the proof of Theorem 4.2.
Theorem C.3 (Global convergence for alternating GDA-AM on bilinear problem). Denote the
distance between the stationary point w∗and current iterate w(k+1)p of Algorithm 3 with Anderson
restart dimension p as N(k+1)p = dist(w∗, w(k+1)p). Assume is normalized such that its largest
singular value is equal to 1. Then when the learning rate η is less than 2, we have the following
bound for Nt"
REFERENCES,0.5778491171749599,"N 2
(k+1)p ≤
r"
REFERENCES,0.579454253611557,"1 +
2η
2 −η (r"
REFERENCES,0.5810593900481541,"c)pN 2
kp"
REFERENCES,0.5826645264847512,"where c and r are the center and radius of a disk D(c, r) which includes all the eigenvalues of G in
equation 4.3. Especially, r"
REFERENCES,0.5842696629213483,c < 1.
REFERENCES,0.5858747993579454,Published as a conference paper at ICLR 2022
REFERENCES,0.5874799357945425,Proof. Since the residual rp of AA at p-th iteration has the form of
REFERENCES,0.5890850722311396,"rp = (I − p
X"
REFERENCES,0.5906902086677368,"i=1
Gi)r0,"
REFERENCES,0.5922953451043339,"and AA minimizes the residual, we have"
REFERENCES,0.593900481540931,"∥r(k+1)p∥2
2 ≤min
β ∥rkp −βGirkp∥2
2 ≤min
fp∈Pp ∥fp(G)rkp∥2
2,"
REFERENCES,0.5955056179775281,"where Pp is the family of polynomials with degree p such that fp(0) = 1, ∀fp ∈Pp . It’s easy to see
that G is unitarily similar to a block diagonal matrix Λ with 2 × 2 blocks as follows:

0
ησi
−ησi
(ησi)2"
REFERENCES,0.5971107544141252,"
∀i ∈[n]."
REFERENCES,0.5987158908507223,Thus the eigenvalues of G can be easily identiﬁed as
REFERENCES,0.6003210272873194,"λ±i = (ησi(ησi ±
p"
REFERENCES,0.6019261637239165,"(ησi)2 −4))
2
,
i ∈[n]."
REFERENCES,0.6035313001605136,"where σ1 ≥σ2 ≥· · · ≥σn are the singular values of . Furthermore, the eigenvector and eigenvalue
associated with each 2 × 2 diagonal block are

0
ησi
−ησi
(ησi)2"
REFERENCES,0.6051364365971108,"  1
λ±i ησi"
REFERENCES,0.6067415730337079,"
= λ±i"
REFERENCES,0.608346709470305," 1
λ±i ησi "
REFERENCES,0.6099518459069021,"Thus G is diagonalizable and denote the matrix with the columns of eigenvectors of G by X. The
real part of the eigenvalues of G are at least"
REFERENCES,0.6115569823434992,R(λ±i) ≥(ησi)2
REFERENCES,0.6131621187800963,"2
,
i ∈[n].
(34)"
REFERENCES,0.6147672552166934,"And since |ησi| ≥|
p"
REFERENCES,0.6163723916532905,"(ησi)2 −4)|, all the eigenvalues will be included in a disk D(c, r) which is
included in the right half plane. Moreover, both c and r being greater than zero indicates that r"
REFERENCES,0.6179775280898876,"c < 1.
Start from the following inequality:"
REFERENCES,0.6195826645264848,"N(k+1)p =
w(k+1)p −w∗
2 =
G−1r(k+1)p

2 ≤min
fp∈Pp"
REFERENCES,0.6211878009630819,"G−1fp(G)rkp

2"
REFERENCES,0.622792937399679,"= min
fp∈Pp"
REFERENCES,0.6243980738362761,"G−1fp(G)G (wkp −w∗)

2 = min
fp∈Pp"
REFERENCES,0.6260032102728732,"G−1
p (G) (wkp −w∗)

2"
REFERENCES,0.6276083467094703,"= min
fp∈Pp ∥fp(G) (wkp −w∗)∥2 (35)"
REFERENCES,0.6292134831460674,We will use the eigendeomposition of G and the special polynomial ( c−t
REFERENCES,0.6308186195826645,"c )p to derive the inequality
in Theorem 3. Now we know r"
REFERENCES,0.6324237560192616,c < 1. If we choose gp(t) = ( c−t
REFERENCES,0.6340288924558587,"c )p, we can obtain"
REFERENCES,0.6356340288924559,"min
fp∈Pp ∥fp(G)(wkp −w∗)∥2 ≤∥gp(G)(wkp −w∗)∥2"
REFERENCES,0.637239165329053,which implies
REFERENCES,0.6388443017656501,"min
fp∈Pp ∥fp(G)(wkp −w∗)∥2 ≤∥gp(XΛX−1)∥∥(wkp −w∗)∥2"
REFERENCES,0.6404494382022472,"Since G is diagonalizable (which has been shown above), we assume the eigendecomposition of G is
G = XΛX−1. Then"
REFERENCES,0.6420545746388443,"min
fp∈Pp ∥gp(G)(wkp−w∗)∥2 ≤∥X∥∥X−1∥max
{λi}2n
i=1
∥gp(Λ)∥∥(wkp−w∗)∥2 ≤κG(r"
REFERENCES,0.6436597110754414,c)p∥wkp−w∗∥2
REFERENCES,0.6452648475120385,"where κG is the condition number of X. The last inequality comes from Lemma 6.26 and Proposition
6.32 in Saad (2003).. Since G and Λ are unitarily similar, κG is equal to the condition number of the
eigenvector matrix of Λ. The eigenvector matrix of Λ is a block diagonal matrix with the ith block"
REFERENCES,0.6468699839486356,"as
 1
1
λ+i
ησi
λ−i ησi"
REFERENCES,0.6484751203852327,"
. Thus the singluar values of the eigenvector matrix of Λ is equal to the union of the"
REFERENCES,0.6500802568218299,"singular values of these 2-by-2 blocks. Under the assumption that the largest singular value of are
equal to 1 and the learning rate is less than 2, it is easy to ﬁnd the singular values of the eigenvector"
REFERENCES,0.651685393258427,"matrix of Λ are √2 ± ησi. Thus, κG =
√2+ησmax
√2−ησmax =
√2+η
√2−η =
q"
REFERENCES,0.6532905296950241,"1 +
2η
2−η."
REFERENCES,0.6548956661316212,Published as a conference paper at ICLR 2022
REFERENCES,0.6565008025682183,"C.3
DISCUSSION OF OBTAINED RATES"
REFERENCES,0.6581059390048154,"We would like to ﬁrst explain on why taking Chebyshev polynomial of degree p at the point 1 +
2
κ−1.
We evaluate the Chebyshev polynomial at this speciﬁc point because the reciprocal of this value
gives the minimal value of inﬁnite norm of the all polynomials of degree p deﬁned on the interval
˜I = [η2σ2
min(), η2σ2
max()] based on Theorem 6.25 (page 209) (Saad, 2003). In other words, taking
the function value at this point leads to the tight bound."
REFERENCES,0.6597110754414125,"When comparing between existing bounds, we would like to point our our derived bounds are hard to
compare directly. Alternatively, we can derive another bound for comparison with existing bounds
for simultaneous GDA-AM. If we use the inequality that Tp(t) ≥1"
REFERENCES,0.6613162118780096,"2((t +
√"
REFERENCES,0.6629213483146067,"t2 −1)p), we can obtain"
REFERENCES,0.6645264847512039,"the bound ρ(A) = 4(
√"
REFERENCES,0.666131621187801,"κ(AT A)−1
√"
REFERENCES,0.6677367576243981,"κ(AT A)+1)2 = 4(1 −O(
1
√"
REFERENCES,0.6693418940609952,"κ(AT A))), which is in a form that is comparable"
REFERENCES,0.6709470304975923,"with EG and can compete with EG + positive momentum. The numerical experiments in ﬁgure 2b
numerically verify that our bound is smaller than EG. We wanted to numerically compare our rate
with EG with positive momentum. However the bound of EG with positive momentum is asymptotic.
Moreover, it does not specify the constants so we can not numerically compare them. We do provide
empirical comparison between GDA-AM and EG with positive momentum for bilinear problems in
Appendix D.1. It shows GDA-AM outperforms EG with positive momentum. Regarding alternating
GDA-AM , we would like to note that the bound in Theorem 4.2 depends on the eigenvalue distribution
of the matrix G. Condition number is not directly related to the distribution of eigenvalues of a
nonsymmetric matrix G. Thus, the condition number is not a precise metric to characterize the
convergence. If these eigenvalues are clustered, then our bound can be small. On the other hand, if
these eigenvalues are evenly distributed in the complex plane, then the bound can very close to 1."
REFERENCES,0.6725521669341894,"More importantly, we would like to stress several technical contributions."
REFERENCES,0.6741573033707865,"1. Our obtained Theorem 4.1 and 4.2 provide nonasymptotic guarantees, while most other
work are asymptotic. For example, EG with positive momentum can achieve a asymptotic
rate of 1 −O(1/√κ) under strong assumptions (Azizian et al., 2020).
2. Our contribution is not just about ﬁx the convergence issue of GDA by applying Anderson
Mixing; another contribution is that we arrive at a convergent and tight bound on the original
work and not just adopting existing analyses. We developed Theorem 4.1 and 4.2 from a new
perspective because applying existing theoretical results fail to give us neither convergent
nor tight bounds.
3. Theorem 4.1 and 4.2 only requires mild conditions and reﬂects how the table size p controls
the convergence rate. Theorem 4.1 is independent of the learning rate η. However, the
convergence results of other methods like EG and OG depend on the learning rate, which
may yield less than desirable results for ill-speciﬁed learning rates."
REFERENCES,0.6757624398073836,"C.4
CONVEX-CONCAVE AND GENERAL CASE"
REFERENCES,0.6773675762439807,"Given the widespread usage of minimax problems in applications of machine learning, it is natural
to ask about its properties when being applied to general nonconvex-nonconcave settings. If f is
a nonconvex-nonconcave function, the problem of ﬁnding global Nash equilibrium is NP-hard in
general. Recently, Jin et al. (2020) show that local or global Nash equilibrium may not exist in
nonconvex-nonconcave settings and propose a new notation local minimax as deﬁned below:
Deﬁnition 4. A point (x⋆, y⋆) is said to be a local minimax point of f, if there exists δ0 > 0 and
a function h satisfying h(δ) →0 as δ →0, such that for any δ ∈(0, δ0], and any (x, y) satisfying
∥x −x⋆∥≤δ and ∥y −y⋆∥≤δ, we have"
REFERENCES,0.6789727126805778,"f (x⋆, y) ≤f (x⋆, y⋆) ≤
max
y′:∥y′−y⋆∥≤h(δ) f (x, y′) ."
REFERENCES,0.680577849117175,"Jin et al. (2020) also establishes the following ﬁrst- and second-order conditions to characterize local
minimax:
Proposition 1 (First-order Condition). Any local minimax point (x∗, y∗) satisﬁes ∇f(x∗, y∗) = 0.
Proposition 2 (Second-order Necessary Condition). Any local minimax point (x∗, y∗) satisﬁes
∇yyf(x∗, y∗) ≼0 and ∇xxf(x∗, y∗) −∇xyf(x∗, y∗)(∇yyf(x∗, y∗))−1∇yxf(x∗, y∗) ≽0."
REFERENCES,0.6821829855537721,Published as a conference paper at ICLR 2022
REFERENCES,0.6837881219903692,"Proposition 3 (Second-order Sufﬁcient Condition). Any stationary point (x∗, y∗) satisﬁes
∇yyf(x∗, y∗) ≺0 and ∇xxf(x∗, y∗) −∇xyf(x∗, y∗)(∇yyf(x∗, y∗))−1∇yxf(x∗, y∗) ≻0 is a
local minimax point."
REFERENCES,0.6853932584269663,"Given the second-order conditions of local minimax, it turns out that above question is extremely
challenging—GDA-AM is a ﬁrst-order method. But we can prove the following result for GDA-AM:"
REFERENCES,0.6869983948635634,"Theorem C.4 (Local minimax as subset of limiting points of GDA-AM). Consider a general objective
function f(x, y). The set of limiting points of GDA-AM for minimax problem"
REFERENCES,0.6886035313001605,"min
x∈Rn max
y∈Rn f(x, y)"
REFERENCES,0.6902086677367576,includes the local minimax points of this function.
REFERENCES,0.6918138041733547,"The deﬁnition of local minimax is stronger than that of ﬁrst order ϵ point. The convergence analysis
for complexity of ﬁnding ϵ stationary point is included in the next section. The proof of Theorem C.4
needs the result from the following theorem."
REFERENCES,0.6934189406099518,"Theorem C.5 (Calvetti et al. (2002)). Let δ satisfy 0 < δ ≤δ0 for some constant δ0 > 0 (refer to
Calvetti et al. (2002) for details), and let bδ ∈X satisfy
b −bδ ≤δ. Let k ≤ℓand let xδ
k denote
the kth iterate determined by the GMRES method applied to equation Ax = bδ, with initial guess
xδ
0 = 0. Similarly, let xk denote the kth iterate determined by the GMRES method applied to equation
Ax = b with initial guess x0 = 0. Then, there are constants σk independent of δ, such that
xk −xδ
k
 ≤σkδ,
1 ≤k ≤ℓ"
REFERENCES,0.695024077046549,"Then, we give the proof of Theorem C.4."
REFERENCES,0.6966292134831461,"Proof of Theorem C.4. For notational simplicity, we will denote ∇xxf(x∗, y∗), ∇xyf(x∗, y∗) and
∇yyf(x∗, y∗) by Hx∗x∗, Hx∗y∗and Hy∗y∗, respectively. Simultaneous GDA can be written as"
REFERENCES,0.6982343499197432,"wt+1 =

xt+1
yt+1"
REFERENCES,0.6998394863563403,"
=

xt −η∇xf(xt, yt)
yt + η∇yf(xt, yt) 
."
REFERENCES,0.7014446227929374,"Since the function is differentiable, Taylor expansion holds for ∇xf(xt, yt) and ∇yf(xt, yt) at a
local minimx point w∗= (x∗, y∗),"
REFERENCES,0.7030497592295345,"∇xf(xt, yt) = ∇xf(x∗, y∗) + Hx∗x∗(xt −x∗) + Hx∗y∗(yt −y∗) + o(∥wt −w∗∥2)
∇yf(xt, yt) = ∇yf(x∗, y∗) + Hy∗y∗(yt −y∗) + Hy∗x∗(xt −x∗) + o(∥wt −w∗∥2)."
REFERENCES,0.7046548956661316,"Use the fact that ∇f(x∗, y∗) = 0 to simplify the above equations and obtain"
REFERENCES,0.7062600321027287,"∇xf(xt, yt) = Hx∗x∗(xt −x∗) + Hx∗y∗(yt −y∗) + o(∥wt −w∗∥2)
∇yf(xt, yt) = Hy∗y∗(yt −y∗) + Hy∗x∗(xt −x∗) + o(∥wt −w∗∥2)."
REFERENCES,0.7078651685393258,"Inserting the above formulas into the iteration scheme, it yields"
REFERENCES,0.709470304975923,"wt+1 =

xt+1
yt+1"
REFERENCES,0.7110754414125201,"
=

I −ηHx∗x∗
−ηHx∗y∗
ηHy∗x∗
I + ηHy∗y∗"
REFERENCES,0.7126805778491172," 
xt
yt"
REFERENCES,0.7142857142857143,"
+

ηHx∗x∗x∗+ ηHx∗y∗y∗+ ϵ
−ηHy∗y∗y∗−ηHx∗y∗x∗+ ϵ "
REFERENCES,0.7158908507223114,"where ϵ denotes the higher order error o(∥wt −w∗∥2). According to Theorem 2.2, we know that
simultaneous GDA-AM is equivalent to applying GMRES to solve the following linear system"
REFERENCES,0.7174959871589085,"(I−

(1 −α)I −ηHx∗x∗
−ηHx∗y∗
ηHy∗x∗
(1 −α)I + ηHy∗y∗"
REFERENCES,0.7191011235955056,"
)w =

αI + ηHx∗x∗
ηHx∗y∗
−ηHy∗x∗
αI −ηHy∗y∗"
REFERENCES,0.7207062600321027,"
w = b+ϵ"
REFERENCES,0.7223113964686998,"where b =

ηHx∗x∗x∗+ ηHx∗y∗y∗
−ηHy∗y∗y∗−ηHx∗y∗x∗"
REFERENCES,0.723916532905297,"
. We now know that GDA-AM is equivalent to GMRES"
REFERENCES,0.7255216693418941,"being applied to solve the following linear system

αI + ηHx∗x∗
ηHx∗y∗
−ηHy∗x∗
αI −ηHy∗y∗"
REFERENCES,0.7271268057784912,"
˜w = b"
REFERENCES,0.7287319422150883,Published as a conference paper at ICLR 2022
REFERENCES,0.7303370786516854,"The symmetric part of the coefﬁcient matrix of the above linear system is

αI + ηHx∗x∗
0
0
αI −ηHy∗y∗ 
."
REFERENCES,0.7319422150882825,"According to Proposition 2, αI −ηHy∗y∗is positive deﬁnite since Hy∗y∗≼0. If Hx∗x∗is
positive semideﬁnite, then αI + ηHx∗x∗is positive deﬁnite and we’re done. Otherwise, assume
λmin(Hx∗x∗) < 0. Then for ﬁxed α, when η < −
α
λmin(Hx∗x∗), αI+ηHx∗x∗will be positive deﬁnite.
Then according to Theorem 2.2, we know GDA-AM indeed converges. Let’s create a new companion
linear system as follows

αI + ηHx∗x∗
ηHx∗y∗
−ηHy∗x∗
αI −ηHy∗y∗"
REFERENCES,0.7335473515248796,"
ˆw = b + αw∗"
REFERENCES,0.7351524879614767,"Note that ˆw = w∗and GMRES on this companion linear system is convergent under suitable
choice of learning rate η. Let the iterates of GMRES for ˜w, ˆw, w be denoted by ˜wt, ˆwt, wt. Then
∥˜wt −ˆwt∥≤∥˜wt −wt∥+ ∥ˆwt −wt∥. According to Theorem C.5, we also have ∥˜wt −wt∥≤
σkϵ, 1 ≤k ≤t. Further more, again according to Theorem C.5, we know ∥ˆwt−ˆwt∥≤σk(αw∗+ϵ).
Starting from an initial point very close to w∗and let t →∞and α, ϵ →0, ˆwt will converge to
w∗= (x∗, y∗), which means the local minimax w∗= (x∗, y∗) is a limiting point of GDA-RAM."
REFERENCES,0.7367576243980738,"Theorem C.6. For strongly-convex-strongly-concave function f(x, y), GDA-AM will converge to
the Nash equilibrium of this function."
REFERENCES,0.7383627608346709,"Proof: Since strongly-convex-strongly-concave function f(x, y) has unique Nash equilibrium which
is also the unique minimax point, this minimax point must be the limiting point of GDA-AM according
to Theorem C.4."
REFERENCES,0.7399678972712681,"C.4.1
BILINEAR-QUADRATIC GAMES"
REFERENCES,0.7415730337078652,"Moreover, we can further show that the GDA-AM converges on bilinear-quadratic games. Consider a
quadratic problem as follows,"
REFERENCES,0.7431781701444623,"min
x∈Rn max
y∈Rn f(x, y) = xT Ay + xT Bx −yT Cy + bT x + cT y,
(36)"
REFERENCES,0.7447833065810594,"where A is full rank, B and C are both positive deﬁnite."
REFERENCES,0.7463884430176565,"Theorem C.7. [Global convergence for simultaneous GDA-AM on bilinear-quadratic problem] Let
r(Sim)
t
be the residual of Algorithm 2 being applied to problem equation 36. For some constant
ρ < 1,"
REFERENCES,0.7479935794542536,"∥r(Sim)
t
∥2 ≤

1 −(λmin(JT + J))2"
REFERENCES,0.7495987158908507,4λmax (JT J) t/2
REFERENCES,0.7512038523274478,"|
{z
}
ρt/2"
REFERENCES,0.7528089887640449,"∥r0∥2,
(37)"
REFERENCES,0.7544141252006421,"where J =
 ηB
ηA
−ηAT
ηC"
REFERENCES,0.7560192616372392,"
and λmin and λmax denote the smallest and largest eigenvalue, respec-"
REFERENCES,0.7576243980738363,tively.
REFERENCES,0.7592295345104334,"The convergence property of GMRES has been studied in the next theorem. We use this theorem to
show the convergence rate of GDA-AM for bilinear-quadratic games."
REFERENCES,0.7608346709470305,"Theorem C.8 (Elman (1982)). Consider solving a linear system Ex = b using GMRES. Let
rt = b −Ext be the residual at tth iteration. If the Hermitian part of E is positive deﬁnite, then for
some positive constant ρ < 1, it holds that"
REFERENCES,0.7624398073836276,"∥rt∥2 ≤

1 −(λmin(EH + E))2"
REFERENCES,0.7640449438202247,4λmax (EHE) t/2
REFERENCES,0.7656500802568218,"|
{z
}
ρt/2"
REFERENCES,0.7672552166934189,"∥r0∥2.
(38)"
REFERENCES,0.7688603531300161,Published as a conference paper at ICLR 2022
REFERENCES,0.7704654895666132,"Proof of Theorem C.7. Applying simultaneous GDA-AM to solve the above problem is equivalent
to applying Anderson Mixing on the following ﬁxed point iteration:

xt+1
yt+1"
REFERENCES,0.7720706260032103,"
=
I −ηB
−ηA
ηAT
I −ηC "
REFERENCES,0.7736757624398074,"|
{z
}
G(Quad−sim)"
REFERENCES,0.7752808988764045,"
xt
yt  |{z}"
REFERENCES,0.7768860353130016,"w(Quad−sim)
t"
REFERENCES,0.7784911717495987,"+

−ηb
−ηc "
REFERENCES,0.7800963081861958,| {z }
REFERENCES,0.7817014446227929,b(Quad−sim)
REFERENCES,0.78330658105939,".
(39)"
REFERENCES,0.7849117174959872,"We know that we need to study the convergence properties of GMRES for solving the following
linear system
 ηB
ηA
−ηAT
ηC"
REFERENCES,0.7865168539325843,"
w = b.
(40)"
REFERENCES,0.7881219903691814,"For notational simplicity, the superscripts has been dropped.
Denote the coefﬁcient matrix
 ηB
ηA
−ηAT
ηC"
REFERENCES,0.7897271268057785,"
by J. The symmetric part of J is"
REFERENCES,0.7913322632423756,J + JT
REFERENCES,0.7929373996789727,"2
=
 η"
REFERENCES,0.7945425361155698,"2(B + BT )
0
0
η
2(C + CT ) "
REFERENCES,0.7961476725521669,"which is positive deﬁnite. Then immediately by Theorem C.8, the following convergence rate holds
For some constant 0 < ρ < 1,"
REFERENCES,0.797752808988764,"∥rt∥2 = min
p∈P1
t
∥p(J)r0∥2 ≤ "
REFERENCES,0.7993579454253612,"1 −(λmin(J + JT )
2"
REFERENCES,0.8009630818619583,(4λmax (JT J)) !t/2
REFERENCES,0.8025682182985554,"|
{z
}
ρt/2 ∥r0∥2"
REFERENCES,0.8041733547351525,= ρt/2∥r0∥2 (41)
REFERENCES,0.8057784911717496,"Note that the convergence of GDA-AM for bilinear-quadratic games can also be analyzed by numerical
range as shown in (Bollapragada et al., 2018). Although we previously show that analysis based on
the numerical range can not help us derive a convergent bound for bilinear games, we show analysis in
Bollapragada et al. (2018) can be extended to bilinear-quadratic games. When B and C are positive
deﬁnite, 1 is outside of the numerical range of matrix G(Quad−sim) as shown in 7a. When B or C is
not positive deﬁnite, 1 can be included in the numerical range of matrix G(Quad−sim) as shown in 7b.
That is saying analysis based on the numerical range (Crouzeix & Palencia, 2017; Bollapragada et al.,
2018) to the bilinear-quadratic problem can lead to a convergent result when B and C are positive
deﬁnite. And analysis based on the numerical range can not help us derive convergent results when
B or C is not positive deﬁnite."
REFERENCES,0.8073836276083467,"C.5
STOCHASTIC CONVEX-NONCONVACE CASE"
REFERENCES,0.8089887640449438,"In this section, we study the convergence of GDA-AM for convex-noncovace problem in the stochastic
setting with the same assumptions in Wei et al. (2021b); Xu et al. (2021). The recent work Wei et al.
(2021b) proves the convergence of the stochastic gradient descent with Anderson Mixing for min
optimization. The convergence of GDA-AM for minimax optimization builds on top of it with several
modiﬁcations. The minimax problem is equivalent to minimizing a function Φ(·) = maxy∈Y f(·, y)
(Lin et al., 2020). And we are interested in complexity of a pair of ϵ-stationary point (x, y) instead of
analysis of a point x."
REFERENCES,0.8105939004815409,"Deﬁnition 5. (Lin et al., 2020) A pair of points (x, y) is an ϵ-stationary point (ϵ ≥0) of a differen-
tiable function Φ if
∥∇xf(x, y)∥≤ϵ
∥PY (y + (1/ℓ)∇yf(x, y)) −y∥≤ϵ/ℓ"
REFERENCES,0.812199036918138,"Assumption 1. f : Rd 7→R is continuously differentiable. f(x) ≥f low > −∞for any x ∈Rd.
∇f is globally L-Lipschitz continuous; namely ∥∇f(x) −∇f(y)∥2 ≤L∥x −y∥2 for any x, y ∈Rd."
REFERENCES,0.8138041733547352,Published as a conference paper at ICLR 2022
REFERENCES,0.8154093097913323,"(a) Positive deﬁnite B and C
(b) Random generated B and C"
REFERENCES,0.8170144462279294,"Figure 7:
Numerical range of ﬁxed-point operator (Simultaneous GDA-AM ) G
=
I −ηB
−ηA
ηAT
I −ηC"
REFERENCES,0.8186195826645265,"
for bilinear-quadratic games."
REFERENCES,0.8202247191011236,"Assumption 2. For any iteration k, the stochastic gradient ∇fξk (xk) satisﬁes Eξk [∇fξk (xk)] ="
REFERENCES,0.8218298555377207,"∇f (xk) , Eξk
h
∥∇fξk (xk) −∇f (xk)∥2
2
i
≤σ2, where σ > 0, and ξk, k = 0, 1, . . ., are indepen-"
REFERENCES,0.8234349919743178,dent samples that are independent of {xi}k
REFERENCES,0.8250401284109149,"Theorem C.9. For a general convex-nonconcave function f, suppose that Assumptions 1 and 2
hold. Batch size nt = n for t = 0, . . . , N −1. C > 0 is a constant. βt =
µ
4L(1+C−1).δt ≥"
REFERENCES,0.826645264847512,"Cβ−2
t
, 0 ≤αt ≤min
n
1, β"
REFERENCES,0.8282504012841091,"1
2
t
o
and αt is chosen to make sure the positive deﬁniteness of Ht. Let"
REFERENCES,0.8298555377207063,"R be a random variable following PR(t)
def
= Prob{R = t} = 1/N, and ¯N be the total number
of stochastic GDA-AM calls needed to calculate stochastic gradients ˜∇fSt (wt) in our algorithm.
To ensure E
h ˜∇f (wR)

2"
REFERENCES,0.8314606741573034,"i
≤ϵ, total number of stochastic GDA-AM calls needed to calculate"
REFERENCES,0.8330658105939005,stochastic gradients ˜∇fSt (wt) is O(ϵ−4).
REFERENCES,0.8346709470304976,Recall that we can recast GDA scheme as the following ﬁxed point iteration.
REFERENCES,0.8362760834670947,"wt+1 = G(sim)
η
(wt) ≜wt + ηV (wt) with w =

x
y"
REFERENCES,0.8378812199036918,"
, V (w) =

−∇xf(x, y)
∇yf(x, y) "
REFERENCES,0.8394863563402889,"Ignoring the stepsize η and let Wt and Rt record the ﬁrst and second order diffrence of recent m
iterates:"
REFERENCES,0.841091492776886,"Wt = [∆wt−m, ∆wt−m+1, · · · , ∆wt−1] , Rt = [∆Vt−m, ∆Vt−m+1, · · · , ∆Vt−1]"
REFERENCES,0.8426966292134831,"Similarly as Wei et al. (2021b),the Anderson mixing can be decoupled into"
REFERENCES,0.8443017656500803,"¯wt+1 = wt −WtΓt,
(Projection step)"
REFERENCES,0.8459069020866774,"¯wt+1 = wt + βt ¯Vt,
(Mixing step)"
REFERENCES,0.8475120385232745,"where βt is the mixing parameter, and ¯Vt = Vt −WtΓt and Γt is solved by"
REFERENCES,0.8491171749598716,"Γt = arg min
Γ∈Rm ∥Vt −RtΓ∥2 + δt∥Γ∥2"
REFERENCES,0.8507223113964687,"We want to argue that similar arguments in Wei et al. (2021b) can be applied to the problem here. To
see why Anderson mixing works for minimax optimization, we assume function f is smooth. Then
the hessian matrix for G(sim)
η
is"
REFERENCES,0.8523274478330658,"H =
 −∇2
xxf
−∇2
xyf
∇2
yxf
∇2
yyf "
REFERENCES,0.8539325842696629,Published as a conference paper at ICLR 2022
REFERENCES,0.85553772070626,"Notice that in a small neighborhood of wt+1, we have"
REFERENCES,0.8571428571428571,"Rt = −HWt =

∇2
xxf
∇2
xyf
−∇2
yxf
−∇2
yyf 
Wt"
REFERENCES,0.8587479935794543,"Thus ∥Vt −RtΓ∥2 ≈∥Vt + HWtΓ∥2, which is equivalent to solving for a vector pt such that
Hpk = Vt. This is exactly the second order method for the ﬁxed point iteration problem. Also at
each step the AM is minimizing the residual, the reason that AM is equivalent to GMRES for linear
problem is that this quadratic approximation is exact. Finally, we rewrite AM as the quasi-newton
framework as Wei et al. (2021b) did. wt+1 = wt + HtVt where
min
Ht ∥Ht −βtI∥F subject to HtRt = −Xt"
REFERENCES,0.8603531300160514,"Finally, with damping parameter, Anderson mixing has the following form
Wt+1 = Wt + βtVt −αt (Wt + βtRt) Γt
(42)"
REFERENCES,0.8619582664526485,"we can also apply the very similar arguments to prove key results in lemma 1, lemma 2 in Wei
et al. (2021b). There is also a key difference with Wei et al. (2021b). Here we are considering"
REFERENCES,0.8635634028892456,"minimax optimization problem. Thus our gradient is actually V (w) =

−∇xf(x, y)
∇yf(x, y)"
REFERENCES,0.8651685393258427,"
rather than"
REFERENCES,0.8667736757624398,"∇f(w) =

∇xf(x, y)
∇yf(x, y)"
REFERENCES,0.8683788121990369,"
This will introduce some difﬁculty to the dynamics of the ﬁxed pointe"
REFERENCES,0.869983948635634,"iteration. However, noticing that ∥V ∥= ∥∇f(w)∥and"
REFERENCES,0.8715890850722311,f (wt+1) ≤f (wt) + ∇f (wt)T (wt+1 −wt) + L
REFERENCES,0.8731942215088283,"2 ∥wt+1 −wt∥2
2"
REFERENCES,0.8747993579454254,≤f (wt) + ˜∇f (wt)T (wt+1 −wt) + L
REFERENCES,0.8764044943820225,"2 ∥wt+1 −wt∥2
2"
REFERENCES,0.8780096308186196,= f (wt) + ˜∇f (wt)T HtVt + L
REFERENCES,0.8796147672552167,"2 ∥HtVt∥2
2 (43)"
REFERENCES,0.8812199036918138,"where
˜∇f (wt) =

−∇xf(x, y)
∇yf(x, y)"
REFERENCES,0.8828250401284109,"
(44)"
REFERENCES,0.884430176565008,"we call this the ascent-descent gradient (ADG) which is the gradient for minimax optimization
problem
min
x∈Rd max
y∈Rd f(x, y)."
REFERENCES,0.8860353130016051,"To see why ∇f (wt)T (wt+1 −wt) ≤˜∇f (wt)T (wt+1 −wt), we consider their difference"
REFERENCES,0.8876404494382022,"( ˜∇−∇)f (wt)T (wt+1 −wt) = −2∇xf(xt, yt)T (xt+1 −xt).
For ﬁxed yt, f(xt, yt+1) has the Talyor expansion:"
REFERENCES,0.8892455858747994,"f(xt+1, yt) = f(xt, yt)+∇xf(xt, yt)T (xt+1−xt)+(xt+1−xt)T ∇xxf(xt+θ(xt+1−xt), yt)(xt+1−xt)
Assuming f is convex w.r.t x and apply safeguard to ensure f(xt+1, yt) ≤f(xt, yt) can guarantee
( ˜∇−∇)f (wt)T (wt+1 −wt) ≥0. Now applying lemmas in Wei et al. (2021b), we can derive the
convergence of our method for general convex-nonconcave function similarly."
REFERENCES,0.8908507223113965,"D
ADDITIONAL EXPERIMENTS"
REFERENCES,0.8924558587479936,"D.1
COMPARISON WITH EG WITH POSITIVE MOMENTUM"
REFERENCES,0.8940609951845907,"In this section, we include additional comparison between GDA-AM and EG with positive momentum.
GDA-AM has two big theoretical advantages over EG with positive momentum. First, convergence
of GDA-AM does not require strong assumptions on choices of hyperparamters. Second, 4.1 and 4.2
provide nonasymptotic guarantees while convergence of EG with positive mometum is asymptotic.
Experimental results are shown in 8. It indicates GDA-AM outperforms EG with positive momentum.
Finding a good choice of the inner and outer step size of EG and momentum term is hard. For EG
with positive momentum, we set the step size of extrapolation step as 1, the step size of update as 0.5,
and the positive momentum term as 0.3 after grid search as shown in 8b and 8c. On the other hand,
GDA-AM converges fast for different step size without hyperparameter tuning."
REFERENCES,0.8956661316211878,Published as a conference paper at ICLR 2022
REFERENCES,0.8972712680577849,"(a) n = 100, p = 10, η = 1"
REFERENCES,0.898876404494382,"(b) Effects of step size η. β is ﬁxed
as 0.3."
REFERENCES,0.9004815409309791,"(c) Effects of momentum term β. η
is ﬁxed as 1."
REFERENCES,0.9020866773675762,Figure 8: Additional Comparison between GDA-AM and EG with positive momentum
REFERENCES,0.9036918138041734,"D.2
1D MINIMAX FUNCTIONS"
REFERENCES,0.9052969502407705,"We begin with investigating the empirical performance of GDA-AM for 6 non-trivial 1d bivariate
functions. We set initial points as (3, 3) and m as 20 or 5 for all functions. We use optimal learning
rates for all methods on each problem. Results are shown in Figure 9, 10, 11, 12, 13 and 14. We
observe GDA-AM consistently outperforms all baselines and improves convergence. It is worthwhile
to mention that the difference between GDA-AM and traditional averaging is twofold. First, traditional
averaging does not involve an adaptive averaging scheme and thus blindly converge to (0, 0) for all
1d bivariate functions. In contrast, GDA-AM obtains optimal weights by solving a small linear system
on past iterates. Using different weights for each iteration, GDA-AM is able to minimize the residual
of past iterates and thus ﬁnd the solution of a ﬁxed-point iteration. More importantly, averaging
does not change the GDA dynamic because averaging generates a new sequence of parameters based
on GDA iterates. This means averaging is independent with base training algorithm (GDA here).
However, GDA-AM changes the dynamic directly by overwriting the latest iterate. It means Anderson
Mixing interacts with GDA, which is another major difference from averaging."
REFERENCES,0.9069020866773676,"Figure 9: f(x, y) = (x −1"
REFERENCES,0.9085072231139647,2)(y −1
REFERENCES,0.9101123595505618,2) + 1
REFERENCES,0.9117174959871589,"3e−(x−0.25)2−(y−0.75)2). The optima for this function is not
(0, 0). Because averaging blindly converges to (0, 0), it can never ﬁnd the correct solution."
REFERENCES,0.913322632423756,"D.3
DENSITY ESTIMATION"
REFERENCES,0.9149277688603531,"To test our proposed method, we evaluate our method on two low-dimension density estimation
problems, mixture of 25 Gaussians and Swiss roll. For both generator and discriminator, we use fully
connected neural networks with 3 hidden layers and 128 hidden units in each layer. Except for the
output layer of discriminator that uses a sigmoid activation, we use tanh-activation for all other layers.
We run Adam and GDA-AM for 50000 steps. The learning rate is set as 2×10−4 and β1 = 0, β2 = 0.9
after an extensive grid search, which is close to the maximal possible stepsize under which the methods
rarely diverge. Figure 15 and 16 show the output after {1K, 10K, 30K, 50K} iterations. It can be
seen that our method converges faster to the target distribution offers a improvement over Adam. In
addition, we can observe that the generated samples using our method gather around the circle and
are less connected with other circles."
REFERENCES,0.9165329052969502,Published as a conference paper at ICLR 2022
REFERENCES,0.9181380417335474,"Figure 10: f(x, y) = (4x2 −(y −3x + 0.05x3)2 −0.1y4)e−0.01(x2+y2). All baselines except
averaging are cyclying around the optima. Averaging is converging slowly."
REFERENCES,0.9197431781701445,"Figure 11: f(x, y) = −3x2 −y2 + 4xy. Baselines tend to diverge. Averaging is converging slowly
again because averaging can only blindly converge to (0, 0) and the optima for this function is (0, 0)."
REFERENCES,0.9213483146067416,"D.4
ROBUST NEURAL NETWORK TRAINING"
REFERENCES,0.9229534510433387,"In this section, we test the effectiveness of GDA-AM by training a robust neural network on MNIST
data set against adversarial attacks (Madry et al., 2019; Goodfellow et al., 2015; Kurakin et al., 2017)
. The optimization formulation is min
w N
X"
REFERENCES,0.9245585874799358,"i=1
max
δi, s.t. |δi|∞≤ε ℓ(f (xi + δi; w) , yi)
(45)"
REFERENCES,0.9261637239165329,"where w is the parameter of the neural network, the pair (xi, yi) denotes the i-th data point, and δi is
the perturbation added to data point i. The accuracy of our formulation against popular attacks, FGSM
(Goodfellow et al., 2015) and PGD (Kurakin et al., 2017), are summarized in Table 2.. Since solving
such problem is computationally challenging, Nouiehed et al. (2019) proposed an approximation of
the above optimization problem with a new objective function as the following nonconvex-concave
problem: min
w N
X"
REFERENCES,0.92776886035313,"i=1
max
t∈T"
X,0.9293739967897271,"9
X"
X,0.9309791332263242,"j=0
tjℓ
 
f
 
xK
ij ; w

, yi

, T = ("
X,0.9325842696629213,"(t1, · · · , tm) | m
X"
X,0.9341894060995185,"i=1
ti = 1, ti ≥0 ) (46)"
X,0.9357945425361156,"where K is a parameter in the approximation, and xK
ij is an approximated attack on sample xi by
changing the output of the network to label j. We use the public available implementation (Nouiehed
et al., 2019) 2. We apply our algorithm on top of (Nouiehed et al., 2019) and compare our results
(p = 50) with (Madry et al., 2019; Zhang et al., 2019; 2020; Nouiehed et al., 2019). Results are
summarized in table 2. We can observe that GDA-AM leads to a comparable or slightly better
performance to the other methods. In addition, GDA-AM does not exhibit a signiﬁcant drop in
accuracy when ϵ is larger and this suggests the learned model is more robust."
X,0.9373996789727127,2https://github.com/optimization-for-data-driven-science/Robust-NN-Training
X,0.9390048154093098,Published as a conference paper at ICLR 2022
X,0.9406099518459069,"Figure 12: f(x, y) = 1"
X,0.942215088282504,3x3 + y2 + 2xy −6x −3y + 4.
X,0.9438202247191011,"Figure 13: f(x, y) = x3 −y3 −2xy + 6."
X,0.9454253611556982,"D.5
IMAGE GENERATION"
X,0.9470304975922953,"In this section, we provide additional experimental results that are not given in Section 5. Figure
18a and 18b show the Inception Score for CIFAR10 using WGAN-GP and SNGAN. It can be
observed that our method consistently performs better than Adam and EG during training. Further, on
CIFAR-10 using WGAN-GP and SNGAN, GDA-AM is slightly slower than Adam (about 110-115%
computational time), but signiﬁcantly faster than EG (about 65-75% computational time)."
X,0.9486356340288925,"D.6
DETAILS ON THE EXPERIMENTS"
X,0.9502407704654896,"For our experiments, we used the PyTorch 3 deep learning framework. Experiments were run one
NVIDIA V100 GPU. The residual network architecture for generator and discriminator are sum-
marized in Table 3 and 4. We use a WGAN-GP loss, with gradient penalty λ = 10. When using
the gradient penalty (WGAN-GP), we remove the batch normalization layers in the discriminator.
When using SNGAN, we replace the batch normalization layers with spectral normalization. Hyper-
paramters of Adam are selected after grid search. We use a learning rate of 2 × 10−4 and batch size
of 64. For table size of GDA-AM , we set it as 120 for CIFAR10 and 150 for CelebA. We set β1 = 0.0
and β2 = 0.9 as we ﬁnd it gives us better models than default settings."
X,0.9518459069020867,3https://pytorch.org/
X,0.9534510433386838,"Natural
FGSM L∞
PGD40L∞
ε = 0.2
ε = 0.3
ε = 0.4
ε = 0.2
ε = 0.3
ε = 0.4
Madry et al. (2019)
98.58%
96.09%
94.82%
89.84%
94.64%
91.41%
78.67%
Trade: ε = 0.35
97.37%
95.47%
94.86%
79.04%
94.41%
92.69%
85.74%
Trade: ε = 0.40
97.21%
96.19%
96.17%
96.14%
95.01%
94.36%
94.11%
Nouiehed et al. (2019)
98.20%
97.04%
96.66%
96.23%
96.00%
95.17%
94.22%
Zhang et al. (2020)
98.89%
97.87%
97.23%
95.81%
96.71%
95.62%
94.51%
GDA-AM
98.61%
97.75%
97.74%
97.75%
96.47%
95.91%
95.41%"
X,0.9550561797752809,Table 2: Test accuracies under FGSM and PGD attack. Trade refers to Zhang et al. (2019).
X,0.956661316211878,Published as a conference paper at ICLR 2022
X,0.9582664526484751,"Figure 14: f(x, y) = 2x2 + y2 + 4xy + 4"
X,0.9598715890850722,3y3 −1 4y4.
X,0.9614767255216693,"(a) Adam
(b) GDA-AM"
X,0.9630818619582665,"Figure 15: 25 Gaussians: Evolution plot of Adam and GDA-AM . Green dots are observed points
and red dots are generated points."
X,0.9646869983948636,"(a) Adam
(b) GDA-AM"
X,0.9662921348314607,"Figure 16: Swiss roll: Evolution plot of Adam and GDA-AM . Green dots are observed points and
red dots are generated points."
X,0.9678972712680578,Published as a conference paper at ICLR 2022
X,0.9695024077046549,"(a) WGAN-GP (ResNet)
(b) SNGAN (ResNet)"
X,0.971107544141252,Figure 17: FID (lower or ↓is better) for CIFAR 10
X,0.9727126805778491,"(a) IS for CIFAR10
(b) IS for CIFAR10
(c) FID CelebA"
X,0.9743178170144462,"Figure 18: Left: IS for CIFAR10 using WGANGP. Middle: IS for CIFAR10 using SNGAN. Right:
FID for CelebA using WGANGP."
X,0.9759229534510433,"(a) Generated images for CIFAR10
(b) Generated images for CelebA"
X,0.9775280898876404,Figure 19: Generated Images for CIFAR10 and CelebA using WGAN-GP(ResNet)
X,0.9791332263242376,Published as a conference paper at ICLR 2022
X,0.9807383627608347,Table 3: ResNet architecture used for our CIFAR-10 experiments.
X,0.9823434991974318,Generator
X,0.9839486356340289,"Input: z ∈R128 ∼N(0, I)
Linear 128 →256 × 4 × 4
ResBlock 128 →128
ResBlock 256 →256
ResBlock 256 →256
Batch Normalization
ReLu
transposed conv. (256, kernel:3 × 3, stride:1, pad: 1
tanh(·)"
X,0.985553772070626,Discriminator
X,0.9871589085072231,Input: x ∈R3×32×32
X,0.9887640449438202,"Linear 128 →128 × 4 × 4
ResBlock 128 →128
ResBlock 128 →128
ResBlock 128 →128
Linear 128 →1"
X,0.9903691813804173,Table 4: ResNet architecture used for our CelebA (64 × 64) experiments.
X,0.9919743178170144,Generator
X,0.9935794542536116,"Input: z ∈R128 ∼N(0, I)
Linear 128 →512 × 8 × 8
ResBlock 512 →256
ResBlock 256 →128
ResBlock 128 →64
Batch Normalization
ReLu
transposed conv. (64, kernel:3 × 3, stride:1, pad: 1
tanh(·)"
X,0.9951845906902087,Discriminator
X,0.9967897271268058,Input: x ∈R3×64×64
X,0.9983948635634029,"Linear 128 →128 × 4 × 4
ResBlock 128 →128
ResBlock 128 →256
ResBlock 256 →512
Linear 512 →1"
