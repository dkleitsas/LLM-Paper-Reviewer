Section,Section Appearance Order,Paragraph
"DEPARTMENT OF MATHEMATICS
COLLEGE OF INFORMATION SCIENCE AND TECHNOLOGY",0.0,"1Department of Mathematics
College of Information Science and Technology
Jinan University, Guangzhou, China
{linyizun,laizhr}@jnu.edu.cn
licheng@stu2020.jnu.edu.cn"
ABSTRACT,0.0014184397163120568,Abstract
ABSTRACT,0.0028368794326241137,"The Sharpe ratio is an important and widely-used risk-adjusted return in Ô¨Ånancial
engineering. In modern portfolio management, one may require an m-sparse
(no more than m active assets) portfolio to save managerial and Ô¨Ånancial costs.
However, few existing methods can optimize the Sharpe ratio with the m-sparse
constraint, due to the nonconvexity and the complexity of this constraint. We
propose to convert the m-sparse fractional optimization problem into an equivalent
m-sparse quadratic programming problem. The semi-algebraic property of the
resulting objective function allows us to exploit the Kurdyka-≈Åojasiewicz property
to develop an efÔ¨Åcient Proximal Gradient Algorithm (PGA) that leads to a portfolio
which achieves the globally optimal m-sparse Sharpe ratio under certain conditions.
The convergence rates of PGA are also provided. To the best of our knowledge,
this is the Ô¨Årst proposal that achieves a globally optimal m-sparse Sharpe ratio with
a theoretically-sound guarantee."
INTRODUCTION,0.00425531914893617,"1
Introduction"
INTRODUCTION,0.005673758865248227,"The Sharpe ratio (SR) [33] is an important and widely-used performance metric in Ô¨Ånance. Suppose
an investing strategy is represented by a portfolio w ‚ààRN of N assets from a Ô¨Ånancial market.
¬µ ‚ààRN and Œ£ ‚ààRN√óN denote the expected return vector (in excess of the risk-free rate) and its
covariance matrix for the N assets, respectively. It can be seen that w‚ä§¬µ and
‚àö"
INTRODUCTION,0.0070921985815602835,"w‚ä§Œ£w represent
the expected return and its standard deviation (i.e., risk) for the portfolio w. The original deÔ¨Ånition
of SR is given as the follow quotient between return and risk:"
INTRODUCTION,0.00851063829787234,"S0(w) =
w‚ä§¬µ
‚àö"
INTRODUCTION,0.009929078014184398,"w‚ä§Œ£w
.
(1.1)"
INTRODUCTION,0.011347517730496455,"Ever since the proposal of SR, how to maximize it becomes an attractive research topic. Ordinary
portfolio optimization methods based on either the mean-variance approach [5, 10] or the exponential
growth rate approach [22, 24] can reduce the portfolio risk and increase the portfolio return to some
extent [23], and hence improve the SR. On the other hand, direct SR optimization methods are
also proposed. Hung et al. [18], Yu and Xu [35] consider the SR as a differentiable function of
the portfolio, which can be solved via the augmented Lagrangian method. Pang [29] converts the
SR maximization under the self-Ô¨Ånancing and long-only constraints into a linear complementarity
problem, which can be solved via the Parametric Linear Complementarity Technique (PLCT) and the
principle pivoting algorithm [12]. Note that PLCT requires ¬µi > 0 for at least some asset i in order
to be feasible."
INTRODUCTION,0.01276595744680851,‚àóCorrespondence to: Zhao-Rong Lai
INTRODUCTION,0.014184397163120567,"In modern portfolio management, it is widely-recognized that the number of selected assets should
be restricted to a manageable size, in order to keep simplicity and save time and Ô¨Ånancial costs.
Managerial strategies provide an approach to achieve this objective, such as the revenue driven
resource allocation [11], the endowment model [15], and selling stocks after market crashes [27].
However, the managerial approaches still require intensive administration and abundant experience
in management and Ô¨Ånance. Hence researchers turn to sparsity models for solutions via the com-
putational approaches. In [10], a Sparse and Stable Markowitz Portfolio (SSMP) is proposed by
imposing ‚Ñì1-regularization on the portfolio. Ao et al. [2] further propose a mean-variance model with
an ‚Ñì1 constraint based on a maximum-Sharpe-ratio estimation. In [24], the exponential growth rate
(EGR) criterion [1, 13, 19, 23] is exploited to develop a Short-term Sparse Portfolio Optimization
(SSPO). Furthermore, a Short-term Sparse Portfolio Optimization with ‚Ñì0-regularization (SSPO-‚Ñì0)
is developed in [28]. In [25], a nonlinear shrinkage of the covariance matrix is proposed to obtain an
appropriate size of free parameters. Motivated by this strategy, Lai et al. [22] characterize a sparse
structure for covariance estimation to construct a portfolio via the machine learning approach."
INTRODUCTION,0.015602836879432624,"The ‚Ñì1-regularization and the ‚Ñì1 constraint cannot control the exact number of selected assets. One
has to tune the sparsity parameter to roughly adjust this number. On the other hand, suppose we
want to select no more than m active assets out of N assets to construct a portfolio, then this can be
exactly represented by the m-sparse (or ‚Ñì0) constraint ‚à•w‚à•0 ‚©Ωm, where the ‚Ñì0 norm ‚à•¬∑ ‚à•0 denotes
the number of nonzero components of a vector. Although many sparsity models are established for
the Markowitz portfolio, few existing methods can optimize the SR (1.1) with the ‚Ñì0 constraint, due
to the nonconvexity and the complexity of this constraint. In addition to the ‚Ñì0 constraint, other
realistic constraints should also be imposed to ensure feasibility. For example, the self-Ô¨Ånancing
constraint represents full re-investment and no external loans; the long-only constraint represents
no short position. If all these constraints are imposed, the whole model becomes even much more
difÔ¨Åcult to solve."
INTRODUCTION,0.01702127659574468,"To overcome these difÔ¨Åculties, we observe that this optimization is essentially an m-sparse fractional
optimization that can be transformed into an equivalent m-sparse quadratic programming. Then
the resulting objective function is semi-algebraic, so that the Kurdyka-≈Åojasiewicz (KL) property
[3] can be exploited to develop an efÔ¨Åcient Proximal Gradient Algorithm (PGA) [30]. It converges
to a portfolio which achieves the globally optimal m-sparse SR under certain conditions. To the
best of our knowledge, this is the Ô¨Årst proposal that achieves a globally optimal m-sparse SR with a
theoretically-sound guarantee. Our main contributions can be summarized as follows."
INTRODUCTION,0.018439716312056736,"1) We propose to directly maximize the SR with the ‚Ñì0 constraint, the self-Ô¨Ånancing constraint
and the long-only constraint on the portfolio. This model aims to obtain a feasible and realistic
portfolio that optimizes the SR with exact sparsity.
2) SR maximization is essentially a fractional optimization. We convert this m-sparse fractional
optimization problem into an equivalent m-sparse quadratic programming problem, which reduces
the difÔ¨Åculty of solving it.
3) We observe that the resulting objective function is semi-algebraic, thus exploit the KL property
to develop an efÔ¨Åcient PGA that leads to a globally or at least a locally optimal solution of the
m-sparse SR maximization model. The convergence rates of PGA are also provided."
INTRODUCTION,0.019858156028368795,"Besides the above contributions, our approach also has several advantages: (i) It can be extended to a
wide range of optimization problems with semi-algebraic objective functions and constraints. (ii)
The actual sparsity is robust to the choice of m. (iii) It needs very little parameter tuning. (iv) It
does not require any external algorithms or commercial optimizers."
RELATED WORKS AND EXISTING PROBLEMS,0.02127659574468085,"2
Related Works and Existing Problems"
RELATED WORKS AND EXISTING PROBLEMS,0.02269503546099291,"There are some existing works that indirectly or directly optimize the SR to some extent via the
computational approach. We introduce some examples and then analyze some unsolved problems."
ORDINARY PORTFOLIO OPTIMIZATION,0.024113475177304965,"2.1
Ordinary Portfolio Optimization"
ORDINARY PORTFOLIO OPTIMIZATION,0.02553191489361702,"An intuitive approach is to directly optimize the portfolio, so that the expected return is maximized
and/or the risk is minimized. These methods can be categorized into the mean-variance approach
and the exponential growth rate approach [23]. Let R ‚ààRT √óN be the sample asset return matrix
with T trading times and N assets, and 1n denotes the vector of n ones. Brodie et al. [10] propose"
ORDINARY PORTFOLIO OPTIMIZATION,0.02695035460992908,"to impose ‚Ñì1-regularization on the mean-variance model, forming a Sparse and Stable Markowitz
Portfolio (SSMP):"
ORDINARY PORTFOLIO OPTIMIZATION,0.028368794326241134,"ÀÜw = argmin
w‚ààRN  1"
ORDINARY PORTFOLIO OPTIMIZATION,0.029787234042553193,"T ‚à•Rw ‚àíœÅ1T ‚à•2
2 + œÑ‚à•w‚à•1"
ORDINARY PORTFOLIO OPTIMIZATION,0.031205673758865248,"
,
s.t.
w‚ä§ÀÜ¬µ = œÅ, w‚ä§1N = 1,
(2.1)"
ORDINARY PORTFOLIO OPTIMIZATION,0.032624113475177303,where ÀÜ¬µ := 1
ORDINARY PORTFOLIO OPTIMIZATION,0.03404255319148936,"T R‚ä§1T denotes the column vector of sample mean returns, and œÑ ‚©æ0 is the regular-
ization parameter. ‚à•¬∑ ‚à•2 and ‚à•¬∑ ‚à•1 denote the ‚Ñì2-norm and the ‚Ñì1-norm, respectively. The quadratic
form 1"
ORDINARY PORTFOLIO OPTIMIZATION,0.03546099290780142,"T ‚à•Rw ‚àíœÅ1T ‚à•2
2 actually computes the mean squared error between the sample portfolio return
r(t)w (r(t) is the t-th row of R) and the given return level œÅ. Equations w‚ä§ÀÜ¬µ = œÅ and w‚ä§1N = 1
are the expected return constraint and the self-Ô¨Ånancing constraint, respectively. Model (2.1) can be
approximately solved by a surrogate model [10] and the Least Absolute Shrinkage and Selection
Operator (Lasso) [34]. Goto and Xu [17] also exploit the Lasso to solve a mean-variance model
through sparse hedging restrictions."
ORDINARY PORTFOLIO OPTIMIZATION,0.03687943262411347,"Ao et al. [2] propose a maximum-Sharpe-ratio estimated and sparse regression (MAXER) to approach
mean-variance efÔ¨Åciency. Assume there are sufÔ¨Åcient observations T > (N + 2). MAXER Ô¨Årst
computes the maximum-Sharpe-ratio estimated regression response ÀÜrc as follows:"
ORDINARY PORTFOLIO OPTIMIZATION,0.03829787234042553,"ÀÜŒ∏s = ÀÜ¬µ‚ä§ÀÜŒ£‚àí1 ÀÜ¬µ, ÀÜŒ∏ := (T ‚àíN ‚àí2)ÀÜŒ∏s ‚àíN"
ORDINARY PORTFOLIO OPTIMIZATION,0.03971631205673759,"T
, ÀÜrc := œÉ 1 + ÀÜŒ∏
p"
ORDINARY PORTFOLIO OPTIMIZATION,0.04113475177304964,"ÀÜŒ∏
,
(2.2)"
ORDINARY PORTFOLIO OPTIMIZATION,0.0425531914893617,"where ÀÜ¬µ and ÀÜŒ£ denote the sample mean and the sample covariance, respectively, œÉ is the risk
constraint parameter. Then it adopts the Lasso to obtain the portfolio:"
ORDINARY PORTFOLIO OPTIMIZATION,0.04397163120567376,"ÀÜw = argmin
w‚ààRN
1
T ‚à•Rw ‚àíÀÜrc1T ‚à•2
2,
s.t.
‚à•w‚à•1 ‚©ΩœÑ.
(2.3)"
ORDINARY PORTFOLIO OPTIMIZATION,0.04539007092198582,"Instead of the mean-variance approach, our method takes an essentially different objective that
directly maximizes the SR in (1.1). Besides, our method does not require T > (N + 2)."
ORDINARY PORTFOLIO OPTIMIZATION,0.04680851063829787,"Based on the exponential growth rate criterion [1, 13, 19, 23], Lai et al. [24] propose to minimize a
kind of negative potential return w‚ä§œï with ‚Ñì1-regularization but without any risk term, forming a
Short-term Sparse Portfolio Optimization (SSPO) model"
ORDINARY PORTFOLIO OPTIMIZATION,0.04822695035460993,"ÀÜw = argmin
w‚ààRN"
ORDINARY PORTFOLIO OPTIMIZATION,0.04964539007092199,"
w‚ä§œï + œÑ‚à•w‚à•1
	
,
s.t.
w‚ä§1N = 1.
(2.4)"
ORDINARY PORTFOLIO OPTIMIZATION,0.05106382978723404,"It develops an unconstrained augmented Lagrangian with the existence of a saddle point that can be
solved by the alternating direction method of multipliers (ADMM). Luo et al. [28] further propose
the SSPO-‚Ñì0 model"
ORDINARY PORTFOLIO OPTIMIZATION,0.0524822695035461,"ÀÜw = argmin
w‚àà‚àÜ"
ORDINARY PORTFOLIO OPTIMIZATION,0.05390070921985816,"
w‚ä§œï + œÑ‚à•w‚à•0
	
,
‚àÜ:=

w ‚ààRN w ‚©æ0N and w‚ä§1N = 1
	
,
(2.5)"
ORDINARY PORTFOLIO OPTIMIZATION,0.05531914893617021,"where ‚àÜis the N-dimensional simplex. This simplex constraint w ‚àà‚àÜis the combination of the
long-only and the self-Ô¨Ånancing constraints. Under this constraint, the ‚Ñì0-regularization problem
(2.5) has a closed-form solution ÀúImin
œï
:=
n
i ‚ààNN
 œïi ‚©Ωminj‚ààNN œïj + œµ
o
with a tolerance œµ ‚©æ0,"
ORDINARY PORTFOLIO OPTIMIZATION,0.05673758865248227,"where NN := {1, 2, . . . , N}."
ORDINARY PORTFOLIO OPTIMIZATION,0.05815602836879433,"On the other hand, Lai et al. [22] propose a rank-one covariance estimator based on the operator
space decomposition, in order to capture the rapidly-changing risk structure in the Ô¨Ånancial market:"
ORDINARY PORTFOLIO OPTIMIZATION,0.059574468085106386,"X = VJŒûU ‚ä§
J , D = Œû2 ‚àí1"
ORDINARY PORTFOLIO OPTIMIZATION,0.06099290780141844,"T ŒûV ‚ä§
J 1T 1‚ä§
T VJŒû, Œ∂‚àó
1 =

tr(D)
N(T ‚àí1) ‚àí1"
ORDINARY PORTFOLIO OPTIMIZATION,0.062411347517730496,"2
Œ∏1, ÀÜŒ£RO := u1Œ∂‚àó
1u‚ä§
1 ,"
ORDINARY PORTFOLIO OPTIMIZATION,0.06382978723404255,"where X = R + 1T √óN denotes the price relative matrix and VJŒûU ‚ä§
J is its singular value decompo-
sition (SVD), Œ∏1 and u1 are the largest eigenvalue and its eigenvector, respectively."
ORDINARY PORTFOLIO OPTIMIZATION,0.06524822695035461,"Although the above portfolio optimization methods may partly improve the SR, they may not
be competitive to direct SR optimization. Hence direct SR optimization methods should still be
developed and investigated."
SHARPE RATIO OPTIMIZATION,0.06666666666666667,"2.2
Sharpe Ratio Optimization"
SHARPE RATIO OPTIMIZATION,0.06808510638297872,Pang [29] proposes to optimize the following SR model:
SHARPE RATIO OPTIMIZATION,0.06950354609929078,"max
w‚ààRN S0(w),
s.t.
w ‚àà‚àÜ, Cw ‚©Ωd,
(2.6)"
SHARPE RATIO OPTIMIZATION,0.07092198581560284,"where S0(w) is deÔ¨Åned by (1.1), C ‚ààRl√óN and d ‚ààRl form a linear constraint for w. It can be
transformed into the following equivalent parametric linear complementarity problem:
Ô£±
Ô£≤ Ô£≥"
SHARPE RATIO OPTIMIZATION,0.07234042553191489,"u = ‚àí¬µ + Œ£w + (C‚ä§‚àí1Nd‚ä§)y ‚©æ0N,
w ‚©æ0N,
v = ‚àí(C ‚àíd1‚ä§
N)w ‚©æ0l,
y ‚©æ0l,
u‚ä§w = v‚ä§y = 0.
(2.7)"
SHARPE RATIO OPTIMIZATION,0.07375886524822695,"Problem (2.7) can be efÔ¨Åciently solved by the principle pivoting algorithm [12], but it requires ¬µi > 0
for at least some asset i in order to be feasible. Moreover, if we aim to construct an m-sparse w,
this approach becomes invalid. In Section 3, we will convert the m-sparse SR optimization into an
equivalent m-sparse quadratic programming, and the latter is still a nonconvex optimization. We
further elaborate a proximal gradient algorithm to obtain a globally or locally optimal SR."
SHARPE RATIO OPTIMIZATION,0.075177304964539,"Another viable approach is to consider the SR as a function of the portfolio and directly optimize it
under some realistic constraints. Hung et al. [18] propose the following IPSRM-D model to optimize
the SR:"
SHARPE RATIO OPTIMIZATION,0.07659574468085106,"max
w‚àà‚àÜ"
SHARPE RATIO OPTIMIZATION,0.07801418439716312,"
S (w) := w‚ä§¬µ + Œ∫1w‚ä§Uw"
SHARPE RATIO OPTIMIZATION,0.07943262411347518,"w‚ä§Dw
+ Œ∫2w‚ä§(1N ‚àíw)

,
(2.8)"
SHARPE RATIO OPTIMIZATION,0.08085106382978724,"where U ‚ààRN√óN and D ‚ààRN√óN are upside and downside risk matrices, respectively, w‚ä§(1N ‚àí
w) is a diversiÔ¨Åcation term. Œ∫1 and Œ∫2 are hyperparameters that control the strength of upside risk and
diversiÔ¨Åcation, respectively. Interested readers can further refer to [35] for some practical estimators
for ¬µ, U and D."
SHARPE RATIO OPTIMIZATION,0.08226950354609928,"However, S (w) in model (2.8) is different from the original SR S0(w) (1.1) in several signiÔ¨Åcant
parts. First, S (w) uses second-order moments w‚ä§Uw and w‚ä§Dw as risk metrics, but S0(w)
uses the Ô¨Årst-order moment
‚àö"
SHARPE RATIO OPTIMIZATION,0.08368794326241134,"w‚ä§Œ£w instead. In general, a Ô¨Årst-order moment is more appropriate
because the expected return w‚ä§¬µ should remain in the same order of magnitude as
‚àö"
SHARPE RATIO OPTIMIZATION,0.0851063829787234,"w‚ä§Œ£w.
Second, the numerator of S0(w) does not contain any risk term, while the numerator of S (w)
contains w‚ä§Uw. This may change the meaning of SR as an equilibrium point in the efÔ¨Åcient frontier
based on the CAPM theory [32]. These facts may affect the performance of SR optimization."
SHARPE RATIO OPTIMIZATION,0.08652482269503546,"Another problem is the lack of effective solving algorithms that could really maximize the SR under
constraints. A conventional way is to adopt gradient methods, since S (w) is a differentiable function
when w Ã∏= 0N. Hung et al. [18], Yu and Xu [35] propose to adopt the augmented Lagrangian method
to optimize (2.8). Though they do not specify which form of Lagrangian models is used, we give the
following one without loss of generality:"
SHARPE RATIO OPTIMIZATION,0.08794326241134752,"L (w, Œª) := S (w) + œ±"
SHARPE RATIO OPTIMIZATION,0.08936170212765958,"2(w‚ä§1N ‚àí1)2 + Œª‚ä§w,
(2.9)"
SHARPE RATIO OPTIMIZATION,0.09078014184397164,where œ±
SHARPE RATIO OPTIMIZATION,0.09219858156028368,"2(w‚ä§1N ‚àí1)2 with hyperparameter œ± ‚©Ω0 is a regularization term for the self-Ô¨Ånancing
constraint, and Œª ‚ààRN
+ is the dual variable with respect to (w.r.t.) w for the long-only constraint
w ‚©æ0N. RN
+ denotes the set of all N-dimensional nonnegative vectors. The update scheme is
w(k+1) = w(k) + Œ∑1‚àáwL (w(k), Œª(k)),
Œª(k+1) = Œª(k) ‚àíŒ∑2‚àáŒªL (w(k+1), Œª(k)),
(2.10)"
SHARPE RATIO OPTIMIZATION,0.09361702127659574,"where Œ∑1, Œ∑2 ‚©æ0 are update step sizes. Note that S is a nonconvex function w.r.t. w, and the
augmented Lagrangian method is a surrogate method that approximates model (2.8). Hence (2.10)
does not necessarily lead to the maximum SR. Worse still, due to the augmented term œ±"
SHARPE RATIO OPTIMIZATION,0.0950354609929078,"2(w‚ä§1N ‚àí1)2,
(2.10) may not even decrease the objective function S . Moreover, the Lagrangian L (w(k), Œª(k))
is increased by the w(k) updates but decreased by the Œª(k) updates, hence (2.10) cannot guarantee
convergence to a point (w‚àó, Œª‚àó) without a thorough investigation of the update scheme. To summarize,
the augmented Lagrangian method and most existing gradient methods cannot guarantee global or
local optimality for model (2.8)."
PGA FOR M-SPARSE SHARPE RATIO MAXIMIZATION,0.09645390070921986,"3
PGA for m-Sparse Sharpe Ratio Maximization"
PGA FOR M-SPARSE SHARPE RATIO MAXIMIZATION,0.09787234042553192,"In this section, we formulate the maximization problem of SR as a nonconvex fractional optimization
under constraints. Instead of directly solving the proposed model, we develop an efÔ¨Åcient proximal
gradient algorithm to solve a simpler surrogate model (subtraction form) that is equivalent to the
original constrained fractional optimization model."
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.09929078014184398,"3.1
m-Sparse Sharpe Ratio Maximization Model"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.10070921985815603,"In order to retain the risk premium meaning of SR in Ô¨Ånance, we directly maximize the original SR
in (1.1) instead of a variant like (2.8). In the perspective of statistical estimation, suppose we have a
sample asset return (in excess of the risk-free rate) matrix R ‚ààRT √óN with T trading times and N
assets. Then the original SR (1.1) can be represented by"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.10212765957446808,S(w) :=
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.10354609929078014,"1
T 1‚ä§
T Rw
q"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.1049645390070922,"1
T ‚àí1‚à•Rw ‚àí( 1"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.10638297872340426,"T 1‚ä§
T Rw)1T ‚à•2
2 + œµ‚à•w‚à•2
2
,
(3.1)"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.10780141843971631,"where œµ‚à•w‚à•2
2 is a regularization term for a positive deÔ¨Ånite Qœµ deÔ¨Åned in (3.2). The parameter œµ
can be an arbitrarily-small positive parameter, whose effect on the risk term can be negligible. To
simplify the notation, we deÔ¨Åne"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.10921985815602837,p := 1
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.11063829787234042,"T R‚ä§1T , Q :=
1
‚àö T ‚àí1"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.11205673758865248,"
R ‚àí1"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.11347517730496454,"T 1T √óT R

and Qœµ := Q‚ä§Q + œµI.
(3.2)"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.1148936170212766,"Then the maximization of SR under the m-sparse, long-only and self-Ô¨Ånancing constraints is given
by"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.11631205673758865,"max
w‚àà‚àÜ
‚à•w‚à•0‚©Ωm
S(w) :=
p‚ä§w
p"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.11773049645390071,"w‚ä§Qœµw
,
(3.3)"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.11914893617021277,"where the simplex ‚àÜis deÔ¨Åned in (2.5). Note that minimizing ‚àíS(w) under the constraint ‚à•w‚à•0 ‚©Ωm
is essentially quite different from minimizing the ‚Ñì0-regularization version ‚àíS(w) + œÑ‚à•w‚à•0 with
some positive œÑ. In general, the latter is easier because it incorporates the ‚Ñì0 norm into the objective
function and enlarges the feasible set by dropping the constraint ‚à•w‚à•0 ‚©Ωm. We simply call (3.3)
the m-Sparse Sharpe Ratio Maximization (mSSRM) model. In fact, to solve the mSSRM model, it
sufÔ¨Åces to solve the following simpler constrained minimization model"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.12056737588652482,"min
v‚©æ0N
‚à•v‚à•0‚©Ωm 1"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.12198581560283688,"2v‚ä§Qœµv ‚àíp‚ä§v

.
(3.4)"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.12340425531914893,"To see this, we establish the relation between the solutions of these two models in the following
theorem, whose proof is provided in Appendix A.1. We deÔ¨Åne the constraint sets in model (3.3) and
(3.4) by"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.12482269503546099,"‚Ñ¶1 := {w ‚àà‚àÜ| ‚à•w‚à•0 ‚©Ωm} and ‚Ñ¶:= {v ‚ààRN| v ‚©æ0N and ‚à•v‚à•0 ‚©Ωm},
(3.5)"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.12624113475177304,respectively. It is obvious that ‚Ñ¶1 ‚´ã‚Ñ¶.
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.1276595744680851,"Theorem 1 Suppose that there exists some Àúw ‚àà‚Ñ¶1 such that p‚ä§Àúw > 0. If ÀÜv is an optimal solution
of model (3.3), then
p‚ä§ÀÜv
ÀÜv‚ä§Qœµ ÀÜv ÀÜv is an optimal solution of model (3.4). Conversely, if ÀÜv is an optimal
solution of model (3.4), then
ÀÜv
ÀÜv‚ä§1N is an optimal solution of model (3.3)."
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.12907801418439716,DeÔ¨Åning the indicator function Œπ‚Ñ¶by
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.13049645390070921,"Œπ‚Ñ¶(v) :=
0,
if v ‚àà‚Ñ¶;
+‚àû,
otherwise,
(3.6)"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.13191489361702127,we can rewrite model (3.4) as the following two-term unconstrained minimization model:
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.13333333333333333,"min
v‚ààRN {f(v) + Œπ‚Ñ¶(v)} , where f(v) := 1"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.1347517730496454,"2v‚ä§Qœµv ‚àíp‚ä§v.
(3.7)"
M-SPARSE SHARPE RATIO MAXIMIZATION MODEL,0.13617021276595745,We then turn to solving model (3.7) instead of the mSSRM model in (3.3).
PROXIMAL GRADIENT ALGORITHM,0.1375886524822695,"3.2
Proximal Gradient Algorithm"
PROXIMAL GRADIENT ALGORITHM,0.13900709219858157,"To develop a proximal gradient algorithm for solving model (3.7), we recall the notion of proximity
operator, and then establish the relation between the solution of model (3.7) and the proximity
characterization (3.8) in Theorem 2. For a proper function œà : Rn ‚ÜíR, its proximity operator
at x ‚ààRn is deÔ¨Åned by proxœà(x) := argminu‚ààRn
 1"
PROXIMAL GRADIENT ALGORITHM,0.14042553191489363,"2‚à•u ‚àíx‚à•2
2 + œà(u)
	
. We remark that for
function œà that is nonconvex, proxœà(x) may not be unique. Throughout this paper, the formula
h = proxœà(x) represents h ‚ààproxœà(x). For v‚àó‚ààRN and Œ¥ > 0, we denote by B(v‚àó; Œ¥) the
neighborhood of v‚àówith radius Œ¥. If there exists some Œ¥ > 0 such that f(v‚àó) ‚©Ωf(v) holds for all
v ‚àà‚Ñ¶‚à©B(v‚àó; Œ¥), then we say that v‚àóis a locally optimal solution of model (3.7)."
PROXIMAL GRADIENT ALGORITHM,0.14184397163120568,"Theorem 2 Let function Œπ‚Ñ¶and f be deÔ¨Åned by (3.6) and (3.7), respectively. If v‚àóis a globally
optimal solution of model (3.7), then for any Œ± ‚àà

0,
1
‚à•Qœµ‚à•2 i
,"
PROXIMAL GRADIENT ALGORITHM,0.14326241134751774,"v‚àó= proxŒπ‚Ñ¶(v‚àó‚àíŒ±‚àáf(v‚àó)) .
(3.8)"
PROXIMAL GRADIENT ALGORITHM,0.14468085106382977,"Conversely, we have the following two statements:"
PROXIMAL GRADIENT ALGORITHM,0.14609929078014183,(i) If Œ± ‚©æ1
PROXIMAL GRADIENT ALGORITHM,0.1475177304964539,"œµ and (3.8) holds, then v‚àóis a globally optimal solution of model (3.7).
(ii) For any Œ± > 0, if (3.8) holds, then v‚àóis a locally optimal solution of model (3.7)."
PROXIMAL GRADIENT ALGORITHM,0.14893617021276595,"The proof of Theorem 2 is provided in Appendix A.2. Based on this theorem, the Proximal Gradient
Algorithm (PGA) for solving model (3.7) can be given by the following iterative scheme:"
PROXIMAL GRADIENT ALGORITHM,0.150354609929078,"v(k+1) = proxŒπ‚Ñ¶

v(k) ‚àíŒ±‚àáf(v(k))

, where k ‚ààN, Œ± > 0, v(0) ‚àà‚Ñ¶.
(3.9)"
PROXIMAL GRADIENT ALGORITHM,0.15177304964539007,"We then compute the closed form of proxŒπ‚Ñ¶. For a vector v ‚ààRN, we denote by mv and Jv
pos
the number of positive components and the index set of positive components of v. If mv ‚©æm,
then we denote by Jv
m-pos an index set of the m-largest positive components of v. SpeciÔ¨Åcally,
by letting {vji}i‚ààNN be an rearrangement of {vj}j‚ààNN such that vj1 ‚©ævj2 ‚©æ¬∑ ¬∑ ¬∑ ‚©ævjN , then
Jv
m-pos := {j1, j2, . . . , jm}. Throughout this paper, for a given vector v ‚ààRN, we shall always
compute proxŒπ‚Ñ¶(v) according to the following proposition. Its proof is given in Appendix A.3."
PROXIMAL GRADIENT ALGORITHM,0.15319148936170213,"Proposition 3 Let Œπ‚Ñ¶be deÔ¨Åned by (3.6), v ‚ààRN, and deÔ¨Åne the index set Jv by"
PROXIMAL GRADIENT ALGORITHM,0.15460992907801419,"Jv =
Jv
m-pos,
if mv > m;
Jv
pos,
if mv ‚©Ωm."
PROXIMAL GRADIENT ALGORITHM,0.15602836879432624,"Then the vector h given by hj =
vj,
if j ‚ààJv;
0,
if j ‚ààNN\Jv satisÔ¨Åes that h ‚ààproxŒπ‚Ñ¶(v)."
CONVERGENCE ANALYSIS OF PGA,0.1574468085106383,"3.3
Convergence Analysis of PGA"
CONVERGENCE ANALYSIS OF PGA,0.15886524822695036,"In this subsection, we delve into the convergence analysis of PGA. We aim to demonstrate that PGA
possesses the capability to converge to a globally optimal solution of model (3.4). The limit point
obtained by PGA can also yield a globally optimal solution of the original model (3.3), under certain
conditions. We also demonstrate the convergence rates of PGA."
CONVERGENCE ANALYSIS OF PGA,0.16028368794326242,"Firstly, we introduce a proposition that illustrates the convergence and monotonic decreasing behavior
of the objective function values for the iterative sequence, as well as the vanishing gap between
consecutive iterates. The proof of this proposition is provided in Appendix A.4."
CONVERGENCE ANALYSIS OF PGA,0.16170212765957448,"Proposition 4 Let function Œπ‚Ñ¶and f be deÔ¨Åned by (3.6) and (3.7), respectively, and let F := f + Œπ‚Ñ¶.
If Œ± ‚àà

0,
1
‚à•Qœµ‚à•2"
CONVERGENCE ANALYSIS OF PGA,0.16312056737588654,"
, then for arbitrary initial vector v(0) ‚ààRN, the sequence {v(k)}k‚ààN generated
by PGA satisÔ¨Åes the following properties:"
CONVERGENCE ANALYSIS OF PGA,0.16453900709219857,"(i) v(k) ‚àà‚Ñ¶, for all k ‚ààN;
(ii) F(v(k+1))+a‚à•v(k+1)‚àív(k)‚à•2
2 ‚©ΩF(v(k)) for all k ‚ààN, where a := 1 2
  1"
CONVERGENCE ANALYSIS OF PGA,0.16595744680851063,"Œ± ‚àí‚à•Qœµ‚à•2

> 0;
(iii) limk‚Üí‚àûF(v(k)) exists;"
CONVERGENCE ANALYSIS OF PGA,0.1673758865248227,(iv) limk‚Üí‚àû‚à•v(k+1) ‚àív(k)‚à•2 = 0.
CONVERGENCE ANALYSIS OF PGA,0.16879432624113475,"Though we have established the convergence of {F(v(k))}k‚ààN and the vanishing gap between
consecutive iterates, further efforts are necessary to rigorously conÔ¨Årm the convergence of the iterative
sequence {v(k)}k‚ààN. We demonstrate the convergence of {v(k)}k‚ààN to a local minimizer of function
F and the corresponding convergence rates in the following theorem. In order to maintain consistency
with the original SR maximization model (as outlined in Theorem 1), we further deÔ¨Åne sequence
{w(k)}k‚ààN based on {v(k)}k‚ààN and conduct an analysis of the convergence rate of {w(k)}k‚ààN. To
prove the theorem, we need to introduce the notions of subdifferential, semi-algebraic function and
Kurdyka-≈Åojasiewicz property, along with several technical lemmas. Detailed proofs and relevant
content can be found in Appendix A.5."
CONVERGENCE ANALYSIS OF PGA,0.1702127659574468,"Theorem 5 Suppose that there exists some Àúw ‚àà‚Ñ¶such that p‚ä§Àúw > 0. For arbitrary initial vector
v(0) ‚ààRN, let {v(k)}k‚ààN be generated by PGA, and let {w(k)}k‚ààN be deÔ¨Åned by"
CONVERGENCE ANALYSIS OF PGA,0.17163120567375886,w(k) :=
CONVERGENCE ANALYSIS OF PGA,0.17304964539007092,"(
v(k)"
CONVERGENCE ANALYSIS OF PGA,0.17446808510638298,"(v(k))‚ä§1N ,
if (v(k))‚ä§1N Ã∏= 0;
0N,
otherwise."
CONVERGENCE ANALYSIS OF PGA,0.17588652482269504,"If Œ± ‚àà

0,
1
‚à•Qœµ‚à•2"
CONVERGENCE ANALYSIS OF PGA,0.1773049645390071,"
, then the following statements hold:"
CONVERGENCE ANALYSIS OF PGA,0.17872340425531916,"(i) {v(k)}k‚ààN converge to a locally optimal solution v‚àóof model (3.4) with convergence rates
‚à•v(k) ‚àív‚àó‚à•2 = O(1/
‚àö"
CONVERGENCE ANALYSIS OF PGA,0.18014184397163122,"k) and |f(v(k)) ‚àíf(v‚àó)| = O(1/k).
(ii) The limit point v‚àóof {v(k)}k‚ààN satisÔ¨Åes that v‚àó‚©æ0N and v‚àóÃ∏= 0N.
(iii) {w(k)}k‚ààN converge to w‚àó:=
v‚àó"
CONVERGENCE ANALYSIS OF PGA,0.18156028368794327,"(v‚àó)‚ä§1N with convergence rates ‚à•w(k) ‚àíw‚àó‚à•2 = O(1/
‚àö k)"
CONVERGENCE ANALYSIS OF PGA,0.1829787234042553,"and |S(w(k)) ‚àíS(w‚àó)| = O(1/
‚àö"
CONVERGENCE ANALYSIS OF PGA,0.18439716312056736,"k), where S is deÔ¨Åned in (3.3)."
CONVERGENCE ANALYSIS OF PGA,0.18581560283687942,"In the remainder of this section, we always let v‚àó‚àà‚Ñ¶be the locally optimal solution of model (3.7)
that sequence {v(k)}k‚ààN converges to. We recall that mv‚àóand Jv‚àó
pos denote the number of positive
components and the index set of positive components of v‚àó, respectively. Suppose that there exists
some Àúw ‚àà‚Ñ¶such that p‚ä§Àúw > 0. Then item (ii) in Theorem 5 together with v‚àó‚àà‚Ñ¶yields that
1 ‚©Ωmv‚àó‚©Ωm. In fact, v‚àóis also the globally optimal solution of the convex model"
CONVERGENCE ANALYSIS OF PGA,0.18723404255319148,"min
v‚ààÀÜ‚Ñ¶ 1"
CONVERGENCE ANALYSIS OF PGA,0.18865248226950354,"2v‚ä§Qœµv‚àíp‚ä§v

, where ÀÜ‚Ñ¶:={v ‚ààRN| v‚©æ0N and vj=0 for all j ‚ààNN\Jv‚àó
pos}.
(3.10)"
CONVERGENCE ANALYSIS OF PGA,0.1900709219858156,"Certainly, ÀÜ‚Ñ¶Ã∏= ‚àÖdue to the condition mv‚àó‚©æ1. According to the deÔ¨Ånition of ÀÜ‚Ñ¶, it is straightforward
to observe that v‚àó‚ààÀÜ‚Ñ¶. Furthermore, ÀÜ‚Ñ¶is a closed convex set and ÀÜ‚Ñ¶‚äÇ‚Ñ¶. To analyze the relation
between v‚àóand the original m-sparse Sharpe ratio maximization model (3.3), we deÔ¨Åne"
CONVERGENCE ANALYSIS OF PGA,0.19148936170212766,"ÀÜ‚Ñ¶1 := {v ‚àà‚àÜ| vj = 0 for all j ‚ààNN\Jv‚àó
pos},
(3.11)"
CONVERGENCE ANALYSIS OF PGA,0.19290780141843972,"where ‚àÜis given by (2.5). It is easy to see that ÀÜ‚Ñ¶1 ‚äÇ‚Ñ¶1, where ‚Ñ¶1 deÔ¨Åned in (3.5) is the constraint
set of model (3.3). We then have the following theorem, whose proof is provided in Appendix A.6."
CONVERGENCE ANALYSIS OF PGA,0.19432624113475178,"Theorem 6 Suppose that there exists some Àúw ‚ààÀÜ‚Ñ¶such that p‚ä§Àúw > 0, where ÀÜ‚Ñ¶is deÔ¨Åned in (3.10),
and let w‚àó:=
v‚àó"
CONVERGENCE ANALYSIS OF PGA,0.19574468085106383,(v‚àó)‚ä§1N . Then the following statements hold:
CONVERGENCE ANALYSIS OF PGA,0.1971631205673759,"(i) v‚àóis the unique globally optimal solution of model (3.10).
(ii) w‚àóis a globally optimal solution of model max
w‚ààÀÜ‚Ñ¶1
S(w)."
CONVERGENCE ANALYSIS OF PGA,0.19858156028368795,"(iii) If mv‚àó= m, then w‚àóis a locally optimal solution of model (3.3)."
CONVERGENCE ANALYSIS OF PGA,0.2,"Item (iii) in Theorem 6 demonstrates that the limit point of the sequence obtained by PGA can yield
a locally optimal solution of model (3.3). In fact, according to item (i) in Theorem 2, we have the
following theorem that provides sufÔ¨Åcient conditions for obtaining a globally optimal solution of
model (3.3), whose proof is provided in Appendix A.7."
CONVERGENCE ANALYSIS OF PGA,0.20141843971631207,"Theorem 7 Suppose that there exists some Àúw ‚àà‚Ñ¶1 such that p‚ä§Àúw > 0, and let w‚àó:=
v‚àó"
CONVERGENCE ANALYSIS OF PGA,0.2028368794326241,"(v‚àó)‚ä§1N . If
one of the following two conditions holds:"
CONVERGENCE ANALYSIS OF PGA,0.20425531914893616,(i) mv‚àó< m;
CONVERGENCE ANALYSIS OF PGA,0.20567375886524822,"(ii) mv‚àó= m and ‚àáif(v‚àó) > ‚àíœµ ¬∑ min{v‚àó
i |i ‚ààsupp(v‚àó)} for all i ‚ààNN\supp(v‚àó),"
CONVERGENCE ANALYSIS OF PGA,0.20709219858156028,then w‚àóis a globally optimal solution of model (3.3).
CONVERGENCE ANALYSIS OF PGA,0.20851063829787234,"Combining Theorem 7 and item (iii) in Theorem 6, we see that the proposed method can obtain
a globally optimal solution of model (3.3) when mv‚àó< m. Even if this condition does not hold,
we can obtain at least a locally optimal solution. To test validation of PGA‚Äôs global optimality, we
conduct a set of simulation experiments, whose details are presented in Appendix A.8. The codes
for the simulation experiments are accessible via the link: https://github.com/linyizun2024/
mSSRM/tree/main/Codes_for_Simulation."
CONVERGENCE ANALYSIS OF PGA,0.2099290780141844,"We call the existence of Àúw ‚ààÀÜ‚Ñ¶such that p‚ä§Àúw > 0 in Theorem 6 the Existence of Positive Expected
Return (EPER) condition. Although the EPER condition is required to guarantee the convergence
of w‚àóto a locally optimal solution of the original model (3.3), the proposed method is still of high
practical signiÔ¨Åcance in the case that the EPER condition does not hold. From the proofs of item
(i) in Theorem 5 and item (i) in Theorem 6, we see that even if the EPER condition does not hold,
the sequence generated by PGA still converges to a locally optimal solution v‚àóof model (3.4),
which is also the globally optimal solution of model (3.10). In these two models, the objective
function 1"
CONVERGENCE ANALYSIS OF PGA,0.21134751773049645,"2v‚ä§Qœµv ‚àíp‚ä§v (subtraction form) w.r.t. v represents risk minus expected return, whose
minimization gives smaller risk and less loss in revenue, even if the expected return is not positive.
For the case that the expected return is not positive, compared with the failure of Sharpe ratio of
fractional form, the globally or locally optimal solution of the subtraction form seems to have more
realistic signiÔ¨Åcance. We recall from item (ii) in Theorem 5 that v‚àómay be equal to 0N if the EPER
condition does not hold. In this case, we shall set w‚àó= 0N and keep all the wealth in the risk-free
asset to avoid loss in revenue. To close this section, we summarize the whole m-sparse Sharpe ratio
maximization method, which we abbreviate to mSSRM-PGA, in Appendix A.9."
EXPERIMENTAL RESULTS,0.2127659574468085,"4
Experimental results"
EXPERIMENTAL RESULTS,0.21418439716312057,"Extensive experiments with real-world Ô¨Ånancial data sets are conducted to evaluate the performance
of the proposed mSSRM-PGA. Moreover, we also consider one baseline method: 1/N [14], as well as
9 state-of-the-art methods: IPSRM-D [18], PLCT [29], SSMP [10], MAXER [2], SSPO [24], SPOLC
[22], S1, S2 and S3 [28], as competitors in the experiments. We use 6 real-world monthly benchmark
data sets: FF25, FF25EU, FF32, FF49, FF100 and FF100MEINV to compare different methods.
These data sets are collected from the baseline and commonly-used Kenneth R. French‚Äôs Real-world
Data Library2. Details regarding these competitors and data sets are given in Appendix A.10. As
for mSSRM-PGA, we examine three levels of sparsity m = 10, m = 15, m = 20 and set œµ = 10‚àí3.
The setting of other parameters are presented in Appendix A.9. The codes of mSSRM-PGA are
accessible via the link: https://github.com/linyizun2024/mSSRM/tree/main/Codes_for_
Experiments_in_Paper."
RESULTS FOR SHARPE RATIOS,0.21560283687943263,"4.1
Results for Sharpe ratios"
RESULTS FOR SHARPE RATIOS,0.2170212765957447,"We adopt the moving-window trading framework in [23] to imitate real-world portfolio management.
For each method, we use the asset returns {r(t)}T
t=1 or the price relatives {x(t) := r(t) + 1N}T
t=1
in the time window t = [1 : T] to update the portfolio ÀÜw(T +1) for the next trading time. On the
(T + 1)-th time, we compute the portfolio return by ÀÜr(T +1), ÀÜ
w = x‚ä§
(T +1) ÀÜw(T +1) ‚àí1 and then turn
to the next round where the time window moves to t = [2 : (T + 1)] and a new portfolio ÀÜw(T +2) is
computed. This procedure is repeated till the last trading time T , which yields a return sequence
{ÀÜr(t), ÀÜ
w}T
t=1. This sequence can be used to compute the test SR:"
RESULTS FOR SHARPE RATIOS,0.21843971631205675,"c
SR =
(PT
t=T +1 ÀÜr(t), ÀÜ
w)/(T ‚àíT)
q"
RESULTS FOR SHARPE RATIOS,0.2198581560283688,"(PT
s=T +1(ÀÜr(s), ÀÜ
w ‚àí(PT
t=T +1 ÀÜr(t), ÀÜ
w)/(T ‚àíT))2)/(T ‚àíT ‚àí1)
."
RESULTS FOR SHARPE RATIOS,0.22127659574468084,"The 1/N strategy does not involve the time window size T. For all other methods, we examine two
conventional settings for the time window size in the Ô¨Ånance industry [2, 17]: T = 60 and T = 120."
RESULTS FOR SHARPE RATIOS,0.2226950354609929,2http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html
RESULTS FOR SHARPE RATIOS,0.22411347517730495,"Table 1 shows the (monthly) SRs of the 11 compared methods. Because MAXER requires T >
(N +2), it is unavailable on FF100 and FF100MEINV when T = 60. It is worth noting that the trivial
strategy 1/N outperforms most competitors in most situations. The reason is that 1/N diversiÔ¨Åes the
risk over all the assets, which is also an effective risk control approach [14]. However, mSSRM-PGA
outperforms all the competitors including 1/N on all the 6 data sets when T = 60 and on 5 data sets
when T = 120. For example, its SR is more than 70% higher than that of 1/N on FF25EU whether
T = 60 or T = 120. Hence mSSRM-PGA achieves competitive SRs with sparse portfolios, which
saves much managerial cost."
RESULTS FOR SHARPE RATIOS,0.225531914893617,Table 1: Sharpe ratios of different portfolio optimization methods on 6 benchmark data sets.
RESULTS FOR SHARPE RATIOS,0.22695035460992907,"Strategy
FF25 FF25EU FF32
FF49
FF100 FF100MEINV FF25 FF25EU FF32
FF49
FF100 FF100MEINV
T = 60
T = 120
1/N
0.2276 0.1574 0.2234 0.2057 0.2087
0.2151
0.2276 0.1574 0.2234 0.2057 0.2087
0.2151
SPOLC
0.1452 0.0315 0.1734 0.0752 0.0562
0.1009
0.1545 0.0350 0.1830 0.1291 0.0988
0.1218
SSPO
0.1544 0.0411 0.1181 0.0588 0.0425
0.0872
0.1789 0.0719 0.1557 0.0601 0.0529
0.1109
S1
0.1497 0.0369 0.1169 0.0559 0.0327
0.0879
0.1789 0.0736 0.1525 0.0648 0.0467
0.0999
S2
0.1382 0.0633 0.1225 0.0573 0.0456
0.1034
0.1578 0.0725 0.1438 0.0605 0.0602
0.1203
S3
0.1428 0.0607 0.1238 0.0570 0.0469
0.1100
0.1609 0.0709 0.1463 0.0617 0.0603
0.1215
SSMP
0.1934 0.1596 0.1535 0.1658 0.0883
0.1448
0.1920 0.0849 0.1512 0.1581 0.0573
0.1495
MAXER
0.1825 0.2229 0.1625 0.1581
N/A
N/A
0.1921 0.2379 0.1465 0.1433 0.1351
0.1479
IPSRM-D
0.2239 0.1994 0.1952 0.1436 0.1766
0.1662
0.2439 0.2358 0.2240 0.1410 0.2012
0.1712
PLCT
0.2475 0.2708 0.2600 0.2119 0.2270
0.2220
0.2468 0.2796 0.2577 0.2025 0.2369
0.2279
mSSRM-PGA(m=10) 0.2481 0.2712 0.2612 0.2151 0.2290
0.2217
0.2472 0.2796 0.2592 0.2041 0.2391
0.2271
mSSRM-PGA(m=15) 0.2481 0.2708 0.2615 0.2135 0.2289
0.2232
0.2474 0.2796 0.2592 0.2040 0.2381
0.2293
mSSRM-PGA(m=20) 0.2481 0.2708 0.2615 0.2134 0.2285
0.2234
0.2474 0.2796 0.2592 0.2041 0.2384
0.2292"
RESULTS FOR CUMULATIVE WEALTHS,0.22836879432624113,"4.2
Results for Cumulative Wealths"
RESULTS FOR CUMULATIVE WEALTHS,0.2297872340425532,"Ordinary investors are also concerned about how much they gain when using an investing strategy.
Without loss of generality, we can set the initial wealth for an investing strategy as S(0) = 1, then the
Ô¨Ånal cumulative wealth can be conveniently computed by S(T ) = QT
t=1(ÀÜr(t), ÀÜ
w + 1). The results of
Ô¨Ånal cumulative wealths are shown in Table 2. The two competitors 1/N and PLCT perform well in
general. Nevertheless, mSSRM-PGA achieves the best Ô¨Ånal cumulative wealths on 4 out of the 6
data sets. Besides, it outperforms each competitor on at least 5 out of the 6 data sets. For example,
mSSRM-PGA is about 20% higher than the second best competitor PLCT on FF49 when T = 60
and m = 10. On the data sets where mSSRM-PGA is not the best method, it is still the second best
method. These results indicate that mSSRM-PGA is an effective strategy for pursuing return gain in
a practical perspective."
RESULTS FOR CUMULATIVE WEALTHS,0.23120567375886525,Table 2: Cumulative wealths of different portfolio optimization methods on 6 benchmark data sets.
RESULTS FOR CUMULATIVE WEALTHS,0.2326241134751773,"Strategy
FF25 FF25EU FF32
FF49
FF100 FF100MEINV FF25 FF25EU FF32
FF49
FF100 FF100MEINV
T = 60
T = 120
1/N
355.98
13.05
424.42 235.48 364.87
428.70
355.98
13.05
424.42 235.48 364.87
428.70
SPOLC
57.53
0.96
169.58
5.44
2.39
14.05
70.46
1.03
259.74 100.49 16.03
36.20
SSPO
129.35
1.22
30.20
1.33
0.89
8.98
286.51
2.67
130.21
1.61
1.74
25.62
S1
100.76
1.08
29.47
1.09
0.54
9.25
265.82
2.78
121.47
2.23
1.27
15.57
S2
66.24
2.17
39.27
1.39
1.15
20.45
130.31
2.73
93.13
1.89
2.67
43.66
S3
70.88
2.01
36.88
1.28
1.20
23.73
129.90
2.61
89.51
1.92
2.62
38.70
SSMP
248.67
13.47
158.98 186.79 10.09
154.27
237.45
3.25
149.65 143.18
2.26
222.35
MAXER
173.39
47.56
200.03 142.31
N/A
N/A
216.94
55.71
117.42 98.85
79.82
188.54
IPSRM-D
398.55
37.25
243.83 69.57 240.12
146.40
567.76
77.79
507.47 50.04 457.86
188.34
PLCT
581.41 126.04 918.62 238.27 471.44
354.70
608.65 148.19 854.83 157.50 552.41
399.48
mSSRM-PGA (m=10) 615.34
126.02 991.89 285.02 527.09
375.75
640.89
147.17 928.19 188.38 635.65
421.97
mSSRM-PGA (m=15) 614.71
125.19 996.32 262.54 522.28
383.44
643.44
147.17 927.21 172.95 597.67
435.01
mSSRM-PGA (m=20) 614.70
125.19 996.23 262.06 515.50
384.65
643.44
147.17 927.16 173.27 603.05
433.15"
RESULTS FOR TRANSACTION COSTS,0.23404255319148937,"4.3
Results for Transaction Costs"
RESULTS FOR TRANSACTION COSTS,0.23546099290780143,"Cumulative wealth with transaction cost can also be tested to see how the transaction cost inÔ¨Çuences
the performance of different methods. We adopt the proportional transaction cost model [8, 26, 21]"
RESULTS FOR TRANSACTION COSTS,0.23687943262411348,"SŒΩ=S(0) T
Y t=1 """
RESULTS FOR TRANSACTION COSTS,0.23829787234042554,"( ÀÜw‚ä§
(t)x(t)) ¬∑  1 ‚àíŒΩ 2 N
X"
RESULTS FOR TRANSACTION COSTS,0.2397163120567376,"i=1
| ÀÜw(t),i ‚àíÀúw(t‚àí1),i| !#"
RESULTS FOR TRANSACTION COSTS,0.24113475177304963,",
Àúw(t‚àí1),i= ÀÜw(t‚àí1),ix(t‚àí1),i"
RESULTS FOR TRANSACTION COSTS,0.2425531914893617,"ÀÜw‚ä§
(t‚àí1)x(t‚àí1)
,"
RESULTS FOR TRANSACTION COSTS,0.24397163120567375,"where Àúw(t‚àí1),i is the evolved portfolio weight of the i-th asset at the end of the (t ‚àí1)-th period,
and ŒΩ is the bidirectional transaction cost rate. When the cost rate of buying is the same as that"
RESULTS FOR TRANSACTION COSTS,0.2453900709219858,"of selling, updating the evolved portfolio Àúw(t‚àí1) as the next portfolio ÀÜw(t) yields a proportional
transaction cost of ŒΩ"
PN,0.24680851063829787,"2
PN
i=1 | ÀÜw(t),i ‚àíÀúw(t‚àí1),i|. Figure 2 in Appendix A.10 shows the Ô¨Ånal cumulative
wealths of different methods as ŒΩ varies from 0 to 0.5% with T = 60. mSSRM-PGA outperforms all
other competitors on FF25, FF25EU and FF32 for all ŒΩ ‚àà[0, 0.5%], and on FF100 for ŒΩ ‚©Ω0.45%.
mSSRM-PGA is the second best method on FF100MEINV, following 1/N. This is because 1/N
naturally keeps a small trading volume. Note that a manager for a mutual fund with sufÔ¨Åcient trades
and capital is able to negotiate for a small enough ŒΩ. Thus mSSRM-PGA is applicable to scenarios
with a certain level of transaction cost."
SPARSITY FOR MSSRM-PGA,0.24822695035460993,"4.4
Sparsity for mSSRM-PGA"
SPARSITY FOR MSSRM-PGA,0.24964539007092199,"In this subsection, we examine the sparsity of the portfolios { ÀÜw(t)} generated by mSSRM-PGA. The
sparsity can be measured by the cardinality of the support set of ÀÜw(t): |supp( ÀÜw(t))|. For each data set
and each setting of m, the mean and the standard deviation (STD) of {|supp( ÀÜw(t))|} are computed
to provide a general description, shown in Table 3. It indicates that mSSRM-PGA further increases
sparsity compared with the preseted sparsity level m. Moreover, mSSRM-PGA keeps stable sparsity
w.r.t. the change of m. For example, the average sparsity for mSSRM-PGA is about 4.9 when T = 60
(or 4.4 when T = 120) on FF25EU, for all the settings m = 10, 15, 20. As the total number of
assets N increases, mSSRM-PGA gets more advantageous in sparsity. For example, the average
sparsity for mSSRM-PGA is about 8 ‚àº11 on FF100 and FF100MEINV, compared with N = 100.
It indicates that mSSRM-PGA selects only 8% ‚àº11% of the assets in the whole asset pool, while
the widely-used 1/N strategy has to maintain the whole asset pool. Therefore, mSSRM-PGA can
save much managerial cost by reducing the proportion of selected assets, while keeping a competitive
performance in SR optimization."
SPARSITY FOR MSSRM-PGA,0.251063829787234,"Table 3: Sparsity of the portfolios generated by mSSRM-PGA: |supp( ÀÜw(t))|.
m
FF25 FF25EU FF32
FF49
FF100 FF100MEINV FF25 FF25EU FF32
FF49
FF100 FF100MEINV
T = 60
T = 120"
SPARSITY FOR MSSRM-PGA,0.2524822695035461,"10 Mean 6.3511 4.8342 6.4159 8.1214 8.0097
7.9175
7.1359 4.4560 7.0825 8.4790 8.3706
8.8722
STD 2.4164 2.1763 2.3654 1.9918 2.2473
2.3915
2.2221 1.4645 2.4464 1.9080 2.3343
2.0117"
SPARSITY FOR MSSRM-PGA,0.25390070921985813,"15 Mean 6.4746 4.9352 7.4286 9.0000 8.9709
9.0825
7.1637 4.4430 7.4919 9.6003 9.8754
10.5906
STD 3.0451 2.3573 3.0590 3.0713 3.3964
3.5906
2.2995 1.4462 2.2567 3.1731 3.6169
3.4845"
SPARSITY FOR MSSRM-PGA,0.2553191489361702,"20 Mean 6.4763 4.9352 7.4692 9.0421 9.1974
9.2994
7.1637 4.4430 7.4919 9.6828 10.0437
10.7621
STD 3.0462 2.3573 3.1734 3.1620 3.8949
4.0125
2.2995 1.4462 2.2567 3.3349 3.9160
3.7460"
CONCLUDING REMARKS,0.25673758865248225,"5
Concluding Remarks"
CONCLUDING REMARKS,0.2581560283687943,"The Sharpe ratio (SR) is a very important measurement for the performance of returns attributable
to risk in Ô¨Ånance. On the other hand, modern portfolio management usually restricts the number of
selected assets to a relatively small size, in order to save managerial and Ô¨Ånancial costs. The m-sparse
(‚Ñì0) constraint is an exact constraint for a sparse portfolio, but it is nonconvex and complex. Thus
few existing methods can optimize the SR with the m-sparse constraint. In this study, we convert
the m-sparse fractional optimization problem into an equivalent m-sparse quadratic programming
problem. Then we develop an efÔ¨Åcient, easy-to-implement and mathematically sound proximal
gradient algorithm to solve this nonconvex problem. We theoretically prove that this algorithm yields
a portfolio that achieves the globally optimal m-sparse Sharpe ratio under certain conditions."
CONCLUDING REMARKS,0.25957446808510637,"We conduct extensive experiments on 6 real-world monthly benchmark data sets built on the Kenneth
R. French‚Äôs widely-used public data library. The numerical results demonstrate that the proposed
mSSRM-PGA improves the SR, compared with 9 state-of-the-art portfolio optimization methods
including SPOLC, SSPO, S1, S2, S3, SSMP, MAXER, IPSRM-D, PLCT and one baseline method 1/N.
For another evaluating metric cumulative wealth, mSSRM-PGA outperforms each competitor on at
least 5 out of the 6 data sets. Besides, mSSRM-PGA can withstand a considerable level of transaction
cost rate. Sparsity experiments indicate that mSSRM-PGA successfully generates portfolios with
stable sparsity, and its advantage increases as the size of the whole asset pool increases. In summary,
the proposed mSSRM-PGA is a promising approach in managing portfolios or other Ô¨Ånancial issues,
which is worth further investigations. A limitation of this research lies in its inability to directly apply
to fractional optimization models featuring nondifferentiable numerator and denominator. Future
work will strive to broaden the theoretical and methodological foundations, ultimately enabling its
application to a broader spectrum of fractional optimization models in machine learning."
CONCLUDING REMARKS,0.26099290780141843,Acknowledgements
CONCLUDING REMARKS,0.2624113475177305,"The authors thank the anonymous reviewers for their constructive comments and valuable suggestions
in improving this paper. This work was supported in part by National Natural Science Founda-
tion of China under Grants 12401120 and 62176103, in part by Guangdong Basic and Applied
Basic Research Foundation under Grants 2021A1515110541 and 2023B1515120064, in part by the
Science and Technology Planning Project of Guangdong under Grant 2023A0505030013, and in
part by the Science and Technology Planning Project of Guangzhou under Grants 2024A04J3940,
2024A04J9896, 202206030007, Nansha District: 2023ZD001 and Development District: 2023GH01."
REFERENCES,0.26382978723404255,References
REFERENCES,0.2652482269503546,"[1] Paul H. Algoet and Thomas M. Cover. Asymptotic optimality and asymptotic equipartition
properties of log-optimum investment. The Annals of Probability, 16(2):876‚Äì898, 1988."
REFERENCES,0.26666666666666666,"[2] Mengmeng Ao, Li Yingying, and Xinghua Zheng. Approaching mean-variance efÔ¨Åciency for
large portfolios. The Review of Financial Studies, 32(7):2890‚Äì2919, 2019."
REFERENCES,0.2680851063829787,"[3] H√©dy Attouch, J√©r√¥me Bolte, Patrick Redont, and Antoine Soubeyran. Proximal alternating
minimization and projection methods for nonconvex problems: An approach based on the
Kurdyka-≈Åojasiewicz inequality. Mathematics of Operations Research, 35(2):438‚Äì457, 2010."
REFERENCES,0.2695035460992908,"[4] Hedy Attouch, J√©r√¥me Bolte, and Benar Fux Svaiter. Convergence of descent methods for
semi-algebraic and tame problems: proximal algorithms, forward‚Äìbackward splitting, and
regularized Gauss‚ÄìSeidel methods. Mathematical Programming, 137(1):91‚Äì129, 2013."
REFERENCES,0.27092198581560284,"[5] Gah-Yi Ban, Noureddine El Karoui, and Andrew E. B. Lim. Machine learning and portfolio
optimization. Management Science, 64(3):1136‚Äì1154, 2018."
REFERENCES,0.2723404255319149,"[6] Heinz H. Bauschke and Patrick L. Combettes. Convex Analysis and Monotone Operator Theory
in Hilbert Space. Springer, New York, 2nd edition, 2017."
REFERENCES,0.27375886524822696,"[7] Dimitri P. Bertsekas. Nonlinear Programming. Athena ScientiÔ¨Åc, Belmont, MA, 2nd edition,
1999."
REFERENCES,0.275177304964539,"[8] Avrim Blum and Adam Kalai. Universal portfolios with and without transaction costs. Machine
Learning, 35(3):193‚Äì205, 1999."
REFERENCES,0.2765957446808511,"[9] J√©r√¥me Bolte, Shoham Sabach, and Marc Teboulle. Proximal alternating linearized minimization
for nonconvex and nonsmooth problems. Mathematical Programming, 146(1):459‚Äì494, 2014."
REFERENCES,0.27801418439716313,"[10] Joshua Brodie, Ingrid Daubechies, Christine De Mol, Domenico Giannone, and Ignace Loris.
Sparse and stable Markowitz portfolios. Proceedings of the National Academy of Sciences of
the United States of America, 106(30):12267‚Äì12272, 2009."
REFERENCES,0.2794326241134752,"[11] Raul O. Chao, Stylianos Kavadias, and Cheryl Gaimon. Revenue driven resource allocation:
Funding authority, incentives, and new product development portfolio management. Manage-
ment Science, 55(9):1556‚Äì1569, 2009."
REFERENCES,0.28085106382978725,"[12] Richard W. Cottle. Monotone solutions of the parametric linear complementarity problem.
Mathematical Programming, 3(1):210‚Äì224, 1972."
REFERENCES,0.2822695035460993,"[13] Thomas M. Cover. Universal portfolios. Mathematical Finance, 1(1):1‚Äì29, 1991."
REFERENCES,0.28368794326241137,"[14] Victor DeMiguel, Lorenzo Garlappi, and Raman Uppal. Optimal versus naive diversiÔ¨Åcation:
How inefÔ¨Åcient is the 1/N portfolio strategy? The Review of Financial Studies, 22(5):1915‚Äì1953,
2009."
REFERENCES,0.2851063829787234,"[15] Stephen G. Dimmock, Neng Wang, and Jinqiang Yang. The endowment model and modern
portfolio theory. Management Science, 70(3):1554‚Äì1579, 2024."
REFERENCES,0.2865248226950355,"[16] John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. EfÔ¨Åcient projections
onto the ‚Ñì1-ball for learning in high dimensions. In Proceedings of the International Conference
on Machine Learning (ICML), pages 272‚Äì279, 2008."
REFERENCES,0.28794326241134754,"[17] Shingo Goto and Yan Xu. Improving mean variance optimization through sparse hedging
restrictions. The Journal of Financial and Quantitative Analysis, 50(6):1415‚Äì1441, 2015."
REFERENCES,0.28936170212765955,"[18] Kei Keung Hung, Chi Chiu Cheung, and Lei Xu. New Sharpe-ratio-related methods for portfolio
selection. In Proceedings of the IEEE/IAFE/INFORMS 2000 Conference on Computational
Intelligence for Financial Engineering (CIFEr), pages 34‚Äì37, 2000."
REFERENCES,0.2907801418439716,"[19] John L. Kelly. A new interpretation of information rate. The Bell System Technical Journal,
35(4):917‚Äì926, 1956."
REFERENCES,0.29219858156028367,"[20] Min Jeong Kim, Yongjae Lee, Jang Ho Kim, and Woo Chang Kim. Sparse tangent portfolio
selection via semi-deÔ¨Ånite relaxation. Operations Research Letters, 44(4):540‚Äì543, 2016."
REFERENCES,0.2936170212765957,"[21] Zhao-Rong Lai, Dao-Qing Dai, Chuan-Xian Ren, and Ke-Kun Huang. Radial basis func-
tions with adaptive input and composite trend representation for portfolio selection. IEEE
Transactions on Neural Networks and Learning Systems, 29(12):6214‚Äì6226, 2018."
REFERENCES,0.2950354609929078,"[22] Zhao-Rong Lai, Liming Tan, Xiaotian Wu, and Liangda Fang. Loss control with rank-one co-
variance estimate for short-term portfolio optimization. Journal of Machine Learning Research,
21(97):1‚Äì37, 2020."
REFERENCES,0.29645390070921984,"[23] Zhao-Rong Lai and Haisheng Yang. A survey on gaps between mean-variance approach
and exponential growth rate approach for portfolio optimization. ACM Computing Surveys,
55(2):1‚Äì36, 2023. Article No. 25."
REFERENCES,0.2978723404255319,"[24] Zhao-Rong Lai, Pei-Yi Yang, Liangda Fang, and Xiaotian Wu. Short-term sparse portfolio
optimization based on alternating direction method of multipliers. Journal of Machine Learning
Research, 19(63):1‚Äì28, 2018."
REFERENCES,0.29929078014184396,"[25] Olivier Ledoit and Michael Wolf. Nonlinear shrinkage of the covariance matrix for portfolio
selection: Markowitz meets Goldilocks. The Review of Financial Studies, 30(12):4349‚Äì4388,
2017."
REFERENCES,0.300709219858156,"[26] Bin Li, Steven C.H. Hoi, Doyen Sahoo, and Zhi-Yong Liu. Moving average reversion strategy
for on-line portfolio selection. ArtiÔ¨Åcial Intelligence, 222:104‚Äì123, 2015."
REFERENCES,0.3021276595744681,"[27] Hong Liu and Mark Loewenstein. Market crashes, correlated illiquidity, and portfolio choice.
Management Science, 59(3):715‚Äì732, 2013."
REFERENCES,0.30354609929078014,"[28] Ziyan Luo, Xiaotong Yu, Naihua Xiu, and Xingyuan Wang. Closed-form solutions for short-term
sparse portfolio optimization. Optimization, 71(7):1937‚Äì1953, 2022."
REFERENCES,0.3049645390070922,"[29] Jong-Shi Pang. A parametric linear complementarity technique for optimal portfolio selection
with a risk-free asset. Operations Research, 28(4):927‚Äì941, 1980."
REFERENCES,0.30638297872340425,"[30] Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends R‚Éùin Optimization,
1(3):127‚Äì239, 2014."
REFERENCES,0.3078014184397163,"[31] R. Tyrrell Rockafellar and Roger J-B. Wets. Variational Analysis, volume 317. Springer Science
& Business Media, 2009."
REFERENCES,0.30921985815602837,"[32] William F. Sharpe. Capital asset prices: A theory of market equilibrium under conditions of
risk. Journal of Finance, 19(3):425‚Äì442, 1964."
REFERENCES,0.31063829787234043,"[33] William F. Sharpe. Mutual fund performance. Journal of Business, 39(1):119‚Äì138, 1966."
REFERENCES,0.3120567375886525,"[34] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society, 58(1):267‚Äì288, 1996."
REFERENCES,0.31347517730496455,"[35] Xiaohui Yu and Lei Xu. Adaptive improved portfolio Sharpe ratio maximization with diversi-
Ô¨Åcation. In Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural
Networks (IJCNN), pages 472‚Äì476, 2000."
REFERENCES,0.3148936170212766,"A
Appendix"
REFERENCES,0.31631205673758866,"A.1
Proof of Theorem 1"
REFERENCES,0.3177304964539007,"To prove Theorem 1, we need the following lemma."
REFERENCES,0.3191489361702128,"Lemma 8 Suppose that there exists some Àúw ‚àà‚Ñ¶1 such that p‚ä§Àúw > 0. If ÀÜv is an optimal solution of
model (3.4), then ÀÜv Ã∏= 0N and p‚ä§ÀÜv = ÀÜv‚ä§QœµÀÜv > 0."
REFERENCES,0.32056737588652484,"Proof. Since ÀÜv is an optimal solution of model (3.4), we know that ÀÜv ‚àà‚Ñ¶. Let w :=
p‚ä§Àú
w
Àú
w‚ä§Qœµ Àú
w Àúw. The
facts Àúw ‚àà‚Ñ¶1 and p‚ä§Àúw > 0 imply that w ‚àà‚Ñ¶. Then it follows that"
REFERENCES,0.3219858156028369,"1
2 ÀÜv‚ä§QœµÀÜv ‚àíp‚ä§ÀÜv ‚©Ω1"
REFERENCES,0.32340425531914896,2w‚ä§Qœµw ‚àíp‚ä§w = ‚àí1
REFERENCES,0.324822695035461,"2
(p‚ä§Àúw)2"
REFERENCES,0.3262411347517731,Àúw‚ä§Qœµ Àúw < 0.
REFERENCES,0.3276595744680851,Hence p‚ä§ÀÜv > 1
REFERENCES,0.32907801418439714,"2 ÀÜv‚ä§QœµÀÜv ‚©æ0 and ÀÜv Ã∏= 0N. Now by letting v :=
p‚ä§ÀÜv
ÀÜv‚ä§Qœµ ÀÜv ÀÜv, then v ‚àà‚Ñ¶and"
REFERENCES,0.3304964539007092,"1
2 ÀÜv‚ä§QœµÀÜv ‚àíp‚ä§ÀÜv ‚©Ω1"
REFERENCES,0.33191489361702126,2v‚ä§Qœµv ‚àíp‚ä§v = ‚àí1
REFERENCES,0.3333333333333333,"2
(p‚ä§ÀÜv)2"
REFERENCES,0.3347517730496454,"ÀÜv‚ä§QœµÀÜv .
(A.1)"
REFERENCES,0.33617021276595743,"Multiplying both sides of (A.1) by 2ÀÜv‚ä§QœµÀÜv yields
 
p‚ä§ÀÜv ‚àíÀÜv‚ä§QœµÀÜv
2 ‚©Ω0,"
REFERENCES,0.3375886524822695,which implies that p‚ä§ÀÜv = ÀÜv‚ä§QœµÀÜv > 0.
REFERENCES,0.33900709219858155,Proof of Theorem 1. Let ÀÜv be an optimal solution of model (3.3). Then ÀÜv ‚àà‚Ñ¶1 and
REFERENCES,0.3404255319148936,"p‚ä§ÀÜv
p"
REFERENCES,0.34184397163120567,"ÀÜv‚ä§QœµÀÜv
‚©æ
p‚ä§Àúw
p"
REFERENCES,0.3432624113475177,"Àúw‚ä§Qœµ Àúw
> 0."
REFERENCES,0.3446808510638298,"DeÔ¨Åning Àúv :=
p‚ä§ÀÜv
ÀÜv‚ä§Qœµ ÀÜv ÀÜv, we see that Àúv ‚àà‚Ñ¶and"
REFERENCES,0.34609929078014184,"1
2 Àúv‚ä§QœµÀúv ‚àíp‚ä§Àúv = 1"
REFERENCES,0.3475177304964539,"2
(p‚ä§ÀÜv)2"
REFERENCES,0.34893617021276596,(ÀÜv‚ä§QœµÀÜv)2 ÀÜv‚ä§QœµÀÜv ‚àí(p‚ä§ÀÜv)2
REFERENCES,0.350354609929078,ÀÜv‚ä§QœµÀÜv = ‚àí1
REFERENCES,0.3517730496453901,"2
(p‚ä§ÀÜv)2"
REFERENCES,0.35319148936170214,"ÀÜv‚ä§QœµÀÜv < 0.
(A.2)"
REFERENCES,0.3546099290780142,For any u ‚àà‚Ñ¶such that 1
REFERENCES,0.35602836879432626,"2u‚ä§Qœµu ‚àíp‚ä§u < 0, we have p‚ä§u > 0, u Ã∏= 0N and Àúu :=
u
u‚ä§1N ‚àà‚Ñ¶1.
Then the fact ÀÜv is an optimal solution of model (3.3) implies that"
REFERENCES,0.3574468085106383,(p‚ä§ÀÜv)2
REFERENCES,0.3588652482269504,ÀÜv‚ä§QœµÀÜv ‚©æ(p‚ä§Àúu)2
REFERENCES,0.36028368794326243,Àúu‚ä§Qœµ Àúu = (p‚ä§u)2
REFERENCES,0.3617021276595745,"u‚ä§Qœµu.
(A.3)"
REFERENCES,0.36312056737588655,"Note that
1
2u‚ä§Qœµu ‚àíp‚ä§u + 1"
REFERENCES,0.3645390070921986,"2
(p‚ä§u)2"
REFERENCES,0.3659574468085106,"u‚ä§Qœµu =
1
2u‚ä§Qœµu(u‚ä§Qœµu ‚àíp‚ä§u)2 ‚©æ0,"
REFERENCES,0.36737588652482267,which combined with (A.3) and (A.2) yields
REFERENCES,0.36879432624113473,"1
2u‚ä§Qœµu ‚àíp‚ä§u ‚©æ‚àí1"
REFERENCES,0.3702127659574468,"2
(p‚ä§u)2"
REFERENCES,0.37163120567375885,u‚ä§Qœµu ‚©æ‚àí1
REFERENCES,0.3730496453900709,"2
(p‚ä§ÀÜv)2"
REFERENCES,0.37446808510638296,ÀÜv‚ä§QœµÀÜv = 1
REFERENCES,0.375886524822695,2 Àúv‚ä§QœµÀúv ‚àíp‚ä§Àúv.
REFERENCES,0.3773049645390071,"Therefore, Àúv is an optimal solution of model (3.4)."
REFERENCES,0.37872340425531914,"Conversely, let ÀÜv be an optimal solution of model (3.4). It follows from Lemma 8 that ÀÜv Ã∏= 0N and
p‚ä§ÀÜv = ÀÜv‚ä§QœµÀÜv > 0. Thus ÀÜv =
p‚ä§ÀÜv
ÀÜv‚ä§Qœµ ÀÜv ÀÜv. For any v ‚àà‚Ñ¶such that p‚ä§v > 0, we let u :=
p‚ä§v
v‚ä§Qœµvv.
Then u ‚àà‚Ñ¶and ‚àí1"
REFERENCES,0.3801418439716312,"2
(p‚ä§ÀÜv)2"
REFERENCES,0.38156028368794326,ÀÜv‚ä§QœµÀÜv = 1
REFERENCES,0.3829787234042553,2 ÀÜv‚ä§QœµÀÜv ‚àíp‚ä§ÀÜv ‚©Ω1
REFERENCES,0.3843971631205674,2u‚ä§Qœµu ‚àíp‚ä§u = ‚àí1
REFERENCES,0.38581560283687943,"2
(p‚ä§v)2"
REFERENCES,0.3872340425531915,"v‚ä§Qœµv .
(A.4)"
REFERENCES,0.38865248226950355,"Now we let ¬Øv :=
ÀÜv
ÀÜv‚ä§1N . Then ¬Øv ‚àà‚Ñ¶1. Inequality (A.4) yields that"
REFERENCES,0.3900709219858156,"p‚ä§¬Øv
p"
REFERENCES,0.39148936170212767,"¬Øv‚ä§Qœµ¬Øv
=
p‚ä§ÀÜv
p"
REFERENCES,0.39290780141843973,"ÀÜv‚ä§QœµÀÜv
‚©æ
p‚ä§v
p"
REFERENCES,0.3943262411347518,"v‚ä§Qœµv
."
REFERENCES,0.39574468085106385,"Note that ‚Ñ¶1 ‚äÇ‚Ñ¶. Therefore, ¬Øv is an optimal solution of model (3.3)."
REFERENCES,0.3971631205673759,"A.2
Proof of Theorem 2"
REFERENCES,0.39858156028368796,"To prove Theorem 2, we Ô¨Årst investigate the properties of function f in Proposition 9, and then recall
two well-known results as Lemmas 10 and 11. Let œà be a function from Rn to [‚àí‚àû, +‚àû]. Then
œà is proper if ‚àí‚àû/‚ààœà(Rn) and {x ‚ààRn| œà(x) < +‚àû} Ã∏= ‚àÖ. Let œà : Rn ‚ÜíR be a proper
function. We say that œà is convex if for any x, y ‚ààRn and any Œª ‚àà(0, 1), œà(Œªx + (1 ‚àíŒª)y) ‚©Ω
Œªœà(x) + (1 ‚àíŒª)œà(y). If there exists Œ≤ > 0 such that œà ‚àíŒ≤"
REFERENCES,0.4,"2 ‚à•¬∑ ‚à•2
2 is convex, then œà is said to be
Œ≤-strongly convex."
REFERENCES,0.4014184397163121,Proposition 9 Let f : RN ‚ÜíR be deÔ¨Åned in (3.7). Then the following hold:
REFERENCES,0.40283687943262414,(i) f is œµ-strongly convex on RN;
REFERENCES,0.40425531914893614,(ii) ‚àáf is ‚à•Qœµ‚à•2-Lipschitz continuous on RN.
REFERENCES,0.4056737588652482,Proof. Let Àúf(v) := f(v) ‚àíœµ
REFERENCES,0.40709219858156026,"2‚à•v‚à•2
2 = 1"
REFERENCES,0.4085106382978723,"2v‚ä§Q‚ä§Qv ‚àíp‚ä§v, v ‚ààRN. Since the Hessian matrix
Q‚ä§Q of Àúf is positive semideÔ¨Ånite, we know that Àúf is convex on RN (see Proposition B.4 of [7]).
Thus item (i) holds. The gradient of f is given by ‚àáf(v) = Qœµv ‚àíp. For all x, y ‚ààRN,
‚à•‚àáf(x) ‚àí‚àáf(y)‚à•2 ‚©Ω‚à•Qœµ‚à•2‚à•x ‚àíy‚à•2, which implies item (ii)."
REFERENCES,0.4099290780141844,"Lemma 10 (Proposition A.24 of [7]) Let function œà : Rn ‚ÜíR be differentiable with an L-
Lipschitz continuous gradient, where L > 0. Then"
REFERENCES,0.41134751773049644,"œà(y) ‚àíœà(x) ‚©Ω‚ü®‚àáœà(x), y ‚àíx‚ü©+ L"
REFERENCES,0.4127659574468085,"2 ‚à•y ‚àíx‚à•2
2"
REFERENCES,0.41418439716312055,"holds for all x, y ‚ààRn."
REFERENCES,0.4156028368794326,"Lemma 11 (Exercise 17.5 of [6]) Let œà : Rn ‚ÜíR be differentiable and Œ≤ > 0. Then œà is Œ≤-
strongly convex if and only if"
REFERENCES,0.41702127659574467,"œà(y) ‚àíœà(x) ‚©æ‚ü®‚àáœà(x), y ‚àíx‚ü©+ Œ≤"
REFERENCES,0.41843971631205673,"2 ‚à•y ‚àíx‚à•2
2"
REFERENCES,0.4198581560283688,"holds for all x, y ‚ààRn."
REFERENCES,0.42127659574468085,"Proof of Theorem 2. We Ô¨Årst show that (3.8) holds when v‚àóis a globally optimal solution of model
(3.7). By the deÔ¨Ånition of proximity operator, (3.8) is equivalent to"
REFERENCES,0.4226950354609929,"v‚àó= argmin
u‚ààRN Œπ‚Ñ¶(u) + 1"
REFERENCES,0.42411347517730497,"2 ‚à•u ‚àív‚àó+ Œ±‚àáf(v‚àó)‚à•2
2 ,"
REFERENCES,0.425531914893617,"that is,"
REFERENCES,0.4269503546099291,Œπ‚Ñ¶(u) + 1
REFERENCES,0.42836879432624114,"2 ‚à•u ‚àív‚àó+ Œ±‚àáf(v‚àó)‚à•2
2 ‚©æŒπ‚Ñ¶(v‚àó) + 1"
REFERENCES,0.4297872340425532,"2 ‚à•Œ±‚àáf(v‚àó)‚à•2
2 , for all u ‚ààRN."
REFERENCES,0.43120567375886526,"According to the deÔ¨Ånition of Œπ‚Ñ¶in (3.6) and the fact v‚àó‚àà‚Ñ¶, the above inequality can be simply
rewritten as
‚ü®‚àáf(v‚àó), u ‚àív‚àó‚ü©+ 1"
REFERENCES,0.4326241134751773,"2Œ±‚à•u ‚àív‚àó‚à•2
2 ‚©æ0, for all u ‚àà‚Ñ¶.
(A.5)"
REFERENCES,0.4340425531914894,"To prove (3.8), it sufÔ¨Åces to show that (A.5) holds. From Proposition 9, we know that f is œµ-strongly
convex and ‚àáf is ‚à•Qœµ‚à•-Lipschitz continuous on RN. Then Lemma 10 yields that"
REFERENCES,0.43546099290780144,"f(u) ‚àíf(v‚àó) ‚©Ω‚ü®‚àáf(v‚àó), u ‚àív‚àó‚ü©+ ‚à•Qœµ‚à•2"
REFERENCES,0.4368794326241135,"2
‚à•u ‚àív‚àó‚à•2
2, for all u ‚àà‚Ñ¶.
(A.6)"
REFERENCES,0.43829787234042555,"Since v‚àóis a globally optimal solution of model (3.7), f(u) ‚àíf(v‚àó) ‚©æ0 for all u ‚àà‚Ñ¶, which"
REFERENCES,0.4397163120567376,"together with (A.6) and the fact Œ± ‚àà

0,
1
‚à•Qœµ‚à•2"
REFERENCES,0.44113475177304967,"i
implies (A.5). This proves that (3.8) holds."
REFERENCES,0.4425531914893617,"Conversely, if Œ± ‚©æ1"
REFERENCES,0.44397163120567373,"œµ and (3.8) holds, then we have (A.5). Recall that f is œµ-strongly convex. It
follows from Lemma 11 that"
REFERENCES,0.4453900709219858,"f(u) ‚àíf(v‚àó) ‚©æ‚ü®‚àáf(v‚àó), u ‚àív‚àó‚ü©+ œµ"
REFERENCES,0.44680851063829785,"2‚à•u ‚àív‚àó‚à•2
2,"
REFERENCES,0.4482269503546099,which together with the fact Œ± ‚©æ1
REFERENCES,0.44964539007092197,"œµ and (A.5) implies that f(u) ‚àíf(v‚àó) ‚©æ0 for all u ‚àà‚Ñ¶. Thus
the assertion in item (i) holds."
REFERENCES,0.451063829787234,"We then prove item (ii). The fact (3.8) holds implies (A.5). For Œ¥ > 0, we deÔ¨Åne"
REFERENCES,0.4524822695035461,"Àú‚Ñ¶Œ¥ := {u ‚ààB(v‚àó; Œ¥)| ‚ü®‚àáf(v‚àó), u ‚àív‚àó‚ü©= 0}."
REFERENCES,0.45390070921985815,"Note that when u tends to v‚àó, the quadratic term
1
2Œ±‚à•u ‚àív‚àó‚à•2
2 is of higher order inÔ¨Ånitesimal than
the linear term |‚ü®‚àáf(v‚àó), u ‚àív‚àó‚ü©|. There must be some Œ¥ > 0 such that"
REFERENCES,0.4553191489361702,"|‚ü®‚àáf(v‚àó), u ‚àív‚àó‚ü©| > 1"
REFERENCES,0.45673758865248226,"2Œ±‚à•u ‚àív‚àó‚à•2
2, for all u ‚ààB(v‚àó; Œ¥)\Àú‚Ñ¶Œ¥.
(A.7)"
REFERENCES,0.4581560283687943,We then show that
REFERENCES,0.4595744680851064,"‚ü®‚àáf(v‚àó), u ‚àív‚àó‚ü©‚©æ0, for all u ‚àà

B(v‚àó; Œ¥)\Àú‚Ñ¶Œ¥

‚à©‚Ñ¶.
(A.8)"
REFERENCES,0.46099290780141844,"Otherwise, there exists some Àúu ‚àà

B(v‚àó; Œ¥)\Àú‚Ñ¶Œ¥

‚à©‚Ñ¶such that ‚ü®‚àáf(v‚àó), Àúu ‚àív‚àó‚ü©< 0. It follows
from (A.7) that"
REFERENCES,0.4624113475177305,"‚ü®‚àáf(v‚àó), Àúu ‚àív‚àó‚ü©+ 1"
REFERENCES,0.46382978723404256,"2Œ±‚à•Àúu ‚àív‚àó‚à•2
2 < 0,"
REFERENCES,0.4652482269503546,which contradicts (A.5). Hence (A.8) holds. This together with the deÔ¨Ånition of Àú‚Ñ¶Œ¥ yields that
REFERENCES,0.4666666666666667,"‚ü®‚àáf(v‚àó), u ‚àív‚àó‚ü©‚©æ0, for all u ‚ààB(v‚àó; Œ¥) ‚à©‚Ñ¶.
(A.9)"
REFERENCES,0.46808510638297873,"Recall that f is convex and differentiable on RN. According to (A.9) and the Ô¨Årst order condition for
convexity (Proposition B.3 of [7]),"
REFERENCES,0.4695035460992908,"f(u) ‚àíf(v‚àó) ‚©æ‚ü®‚àáf(v‚àó), u ‚àív‚àó‚ü©‚©æ0, for all u ‚ààB(v‚àó; Œ¥) ‚à©‚Ñ¶,"
REFERENCES,0.47092198581560285,which implies that v‚àóis a locally optimal solution of model (3.7).
REFERENCES,0.4723404255319149,"A.3
Proof of Proposition 3"
REFERENCES,0.47375886524822697,"Proof. By the deÔ¨Ånitions of Œπ‚Ñ¶and its proximity operator, we have"
REFERENCES,0.475177304964539,"proxŒπ‚Ñ¶(v) = argmin
u‚àà‚Ñ¶
‚à•u ‚àív‚à•2."
REFERENCES,0.4765957446808511,"To prove that h ‚ààproxŒπ‚Ñ¶(v), it is equivalent to show that"
REFERENCES,0.47801418439716314,"‚à•h ‚àív‚à•2
2 ‚©Ω‚à•u ‚àív‚à•2
2, for all u ‚àà‚Ñ¶.
(A.10)"
REFERENCES,0.4794326241134752,"For any u ‚àà‚Ñ¶, there exists an index set Ju ‚ààNN with m elements such that uj = 0 for all
j ‚ààNN\Ju. Let Jv
neg be the index set of negative components in v and J‚Ä≤
u := (NN\Ju)‚à™Jv
neg. Since
u ‚©æ0N, ‚à•u ‚àív‚à•2
2 ‚©æP"
REFERENCES,0.4808510638297872,"j‚ààJ‚Ä≤
u v2
j . Let J‚Ä≤
h = NN\Jv. Then Jv
neg ‚äÇJ‚Ä≤
h and ‚à•h ‚àív‚à•2
2 = P"
REFERENCES,0.48226950354609927,"j‚ààJ‚Ä≤
p v2
j .
If mv > m, then Jv = Jv
m-pos. We are easy to see from the deÔ¨Ånition of Jv
m-pos that X"
REFERENCES,0.4836879432624113,"j‚ààJ‚Ä≤
u
v2
j ‚àí
X"
REFERENCES,0.4851063829787234,"j‚ààJ‚Ä≤
h
v2
j =
X"
REFERENCES,0.48652482269503544,"j‚ààNN\(Ju‚à™Jv
neg)
v2
j ‚àí
X"
REFERENCES,0.4879432624113475,"j‚ààNN\(Jv
m-pos‚à™Jv
neg)
v2
j ‚©æ0."
REFERENCES,0.48936170212765956,"If mv ‚©Ωm, then Jv = Jv
pos and P
j‚ààJ‚Ä≤u v2
j ‚àíP
j‚ààJ‚Ä≤
h v2
j = P
j‚àà(NN\Ju)‚à™Jv
neg v2
j ‚àíP
j‚ààJv
neg v2
j ‚©æ0.
Now we conclude from the above two cases that"
REFERENCES,0.4907801418439716,"‚à•u ‚àív‚à•2
2 ‚àí‚à•h ‚àív‚à•2
2 ‚©æ
X"
REFERENCES,0.4921985815602837,"j‚ààJ‚Ä≤u
v2
j ‚àí
X"
REFERENCES,0.49361702127659574,"j‚ààJ‚Ä≤
h
v2
j ‚©æ0,"
REFERENCES,0.4950354609929078,"that is, (A.10) holds. This completes the proof."
REFERENCES,0.49645390070921985,"A.4
Proof of Proposition 4"
REFERENCES,0.4978723404255319,"Proof. Item (i) follows from (3.9) and the deÔ¨Ånition of proxŒπ‚Ñ¶directly. Then we have that Œπ‚Ñ¶(v(k)) =
0 for all k ‚ààN. To prove item (ii), it sufÔ¨Åces to show that"
REFERENCES,0.49929078014184397,"f(v(k+1)) + a‚à•v(k+1) ‚àív(k)‚à•2
2 ‚©Ωf(v(k)), for all k ‚ààN.
(A.11)"
REFERENCES,0.500709219858156,Note that a = 1
REFERENCES,0.502127659574468,"Œ± ‚àí‚à•Qœµ‚à•2 > 0, since Œ± ‚àà

0,
1
‚à•Qœµ‚à•2"
REFERENCES,0.5035460992907801,"
. Let"
REFERENCES,0.5049645390070922,œï(u) := 1 2
REFERENCES,0.5063829787234042,"u ‚àív(k) + Œ±‚àáf(v(k))

2"
REFERENCES,0.5078014184397163,"2 + Œπ‚Ñ¶(u), u ‚ààRN.
(A.12)"
REFERENCES,0.5092198581560283,"Then (3.9) implies that œï(v(k+1)) ‚â§œï(v(k)), that is,"
REFERENCES,0.5106382978723404,"‚ü®‚àáf(v(k)), v(k+1) ‚àív(k)‚ü©‚©Ω‚àí1"
REFERENCES,0.5120567375886524,"2Œ±‚à•v(k+1) ‚àív(k)‚à•2
2, for all k ‚ààN,
(A.13)"
REFERENCES,0.5134751773049645,It follows from Lemma 10 that
REFERENCES,0.5148936170212766,"f(v(k+1)) ‚àíf(v(k)) ‚©Ω‚ü®‚àáf(v(k)), v(k+1) ‚àív(k)‚ü©+ ‚à•Qœµ‚à•2"
REFERENCES,0.5163120567375886,"2
‚à•v(k+1) ‚àív(k)‚à•2
2.
(A.14)"
REFERENCES,0.5177304964539007,"Combining (A.13) and (A.14) yields (A.11). Thus item (ii) holds. Now that F is monotonically
decreasing, according to the monotone convergence theorem, to prove item (iii), it sufÔ¨Åces to
show that function F is bounded below on ‚Ñ¶. Solving ‚àáf(v‚àó) = 0 gives v‚àó= Q‚àí1
œµ p. Since
f is convex and differentiable on RN, f attains the minimum value at Q‚àí1
œµ p on RN. Hence
f(v) ‚©æf
 
Q‚àí1
œµ p

= ‚àí1"
REFERENCES,0.5191489361702127,"2p‚ä§Q‚àí1
œµ p for all v ‚ààRN, which implies that F(v) ‚©æ‚àí1"
REFERENCES,0.5205673758865248,"2p‚ä§Q‚àí1
œµ p for all
v ‚àà‚Ñ¶. Therefore, item (iii) holds. Now taking the limit on both sides of the inequality in item (ii)
yields item (iv) immediately. This completes the proof."
REFERENCES,0.5219858156028369,"A.5
Proof of Theorem 5"
REFERENCES,0.5234042553191489,"In order to prove Theorem 5, it is necessary to review several deÔ¨Ånitions and establish several
preliminary results. First, We recall the notions of subdifferentials and critical point. The lower limit
of function œà at x and the domain of œà are deÔ¨Åned by"
REFERENCES,0.524822695035461,"lim inf
y‚Üíx œà(y) := lim
Œ¥‚Üí0+"
REFERENCES,0.526241134751773,"
inf
y‚ààB(x;Œ¥) œà(y)

(A.15)"
REFERENCES,0.5276595744680851,"and
dom œà := {x ‚ààRn| œà(x) < +‚àû},
respectively. We say that œà is lower semicontinuous at x ‚ààRn if œà(x) ‚©Ωlim inf
u‚Üíx œà(u). If œà is lower
semicontinuous at every x ‚ààRn, then œà is lower semicontinuous on Rn [31]."
REFERENCES,0.5290780141843971,"DeÔ¨Ånition 1 (Subdifferentials and critical point) Let œà : Rn ‚ÜíR be a proper lower semicontinu-
ous function."
REFERENCES,0.5304964539007092,"(i) For each x ‚ààdom œà, the Fr√©chet subdifferential of œà at x, written by ÀÜ‚àÇœà(x), is the set of
all vectors u ‚ààRn which satisfy"
REFERENCES,0.5319148936170213,"lim inf
y‚Üíx
yÃ∏=x"
REFERENCES,0.5333333333333333,"œà(y) ‚àíœà(x) ‚àí‚ü®u, y ‚àíx‚ü©"
REFERENCES,0.5347517730496454,"‚à•y ‚àíx‚à•2
‚©æ0."
REFERENCES,0.5361702127659574,"When x /‚ààdom œà, we set ÀÜ‚àÇœà(x) = ‚àÖ."
REFERENCES,0.5375886524822695,"(ii) The limiting-subdifferential, or simply the subdifferential of œà at x ‚ààdom œà, written by
‚àÇœà(x), is deÔ¨Åned through the following closure process"
REFERENCES,0.5390070921985816,"‚àÇœà(x) := {u ‚ààRn| ‚àÉxk ‚Üíx, œà(xk) ‚Üíœà(x) and uk ‚ààÀÜ‚àÇœà(xk) ‚Üíu as k ‚Üí+‚àû}."
REFERENCES,0.5404255319148936,"We call an element in ‚àÇœà(x) subgradient of œà at x. We say that x is a critical point of œà if
0n ‚àà‚àÇœà(x)."
REFERENCES,0.5418439716312057,"We also recall the following known results about subdifferential from Theorem 8.6, Exercise 8.8 (c)
and Theorem 10.1 of [31], respectively."
REFERENCES,0.5432624113475177,"Fact 12 For x ‚ààdom œà, ÀÜ‚àÇœà(x) ‚äÇ‚àÇœà(x)."
REFERENCES,0.5446808510638298,"Fact 13 Let œà1 : Rn ‚ÜíR and œà2 : Rn ‚ÜíR be two proper lower semicontinuous functions and
x ‚ààRn. If œà1 is differentiable on a neighborhood of x and œà2 is Ô¨Ånite at x, then"
REFERENCES,0.5460992907801419,‚àÇ(œà1 + œà2)(x) = ‚àáœà1(x) + ‚àÇœà2(x).
REFERENCES,0.5475177304964539,"Fact 14 (Fermat‚Äôs rule) If x ‚ààRn is a local minimizer of œà, then 0n ‚àà‚àÇœà(x)."
REFERENCES,0.548936170212766,"We shall use Theorem 2.9 of [4], which is recalled as Proposition 15, to prove the convergence of the
PGA. For this purpose, we recall the notions of Kurdyka-≈Åojasiewicz (KL) property and KL function."
REFERENCES,0.550354609929078,"DeÔ¨Ånition 2 (KL property) Let œà : Rn ‚ÜíR be a proper semicontinuous function. We say that œà
satisÔ¨Åes the KL property at ÀÜx ‚ààdom ‚àÇœà if there exist Œ∑ ‚àà(0, +‚àû], a neighborhood U of ÀÜx and a
continuous concave function œï : [0, Œ∑) ‚Üí[0, +‚àû] such that"
REFERENCES,0.5517730496453901,(i) œï(0) = 0;
REFERENCES,0.5531914893617021,"(ii) œï is continuously differentiable on (0, Œ∑) with œï‚Ä≤ > 0;"
REFERENCES,0.5546099290780142,"(iii) œï‚Ä≤(œà(x) ‚àíœà(ÀÜx))¬∑dist(0, ‚àÇœà(x)) ‚©æ1 for any x ‚ààU ‚à©{x ‚ààRn : œà(ÀÜx) < œà(x) <
œà(ÀÜx) + Œ∑}."
REFERENCES,0.5560283687943263,"DeÔ¨Ånition 3 (KL function) We call a proper lower semicontinuous function œà : Rn ‚ÜíR KL
function if œà satisÔ¨Åes the KL property at all points in dom ‚àÇœà."
REFERENCES,0.5574468085106383,"Proposition 15 Let œà : Rn ‚ÜíR be a proper lower semicontinuous function. Consider a sequence
{x(k)}k‚ààN ‚äÇRn satisfying the following conditions:"
REFERENCES,0.5588652482269504,(i) There exists a > 0 such that
REFERENCES,0.5602836879432624,"œà(x(k+1)) + a‚à•x(k+1) ‚àíx(k)‚à•2
2 ‚©Ωœà(x(k)), for all k ‚ààN."
REFERENCES,0.5617021276595745,(ii) There exist b > 0 and y(k+1) ‚àà‚àÇœà(x(k+1)) such that
REFERENCES,0.5631205673758866,"‚à•y(k+1)‚à•2 ‚©Ωb‚à•x(k+1) ‚àíx(k)‚à•2, for all k ‚ààN."
REFERENCES,0.5645390070921986,(iii) There exist a subsequence {x(kj)}j‚ààN+ and x‚àó‚ààRn such that
REFERENCES,0.5659574468085107,"lim
j‚Üí‚àûx(kj) = x‚àóand
lim
j‚Üí‚àûœà(x(kj)) = œà(x‚àó)."
REFERENCES,0.5673758865248227,"If œà satisÔ¨Åes the KL property at x‚àó, then"
REFERENCES,0.5687943262411348,"lim
k‚Üí‚àûx(k) = x‚àóand 0n ‚àà‚àÇœà(x‚àó)."
REFERENCES,0.5702127659574469,"We then focus on verifying that the sequence {v(k)}k‚ààN generated by PGA satisÔ¨Åes all the conditions
in Proposition 15. The satisfaction of item (i) has been shown in Proposition 4. We next consider the
satisfaction of item (ii) in Proposition 15."
REFERENCES,0.5716312056737589,"Proposition 16 Let {v(k)}k‚ààN be generated by PGA. Then there exist q(k+1) ‚àà‚àÇF(v(k+1)) and
b > 0 such that
‚à•q(k+1)‚à•2 ‚©Ωb‚à•v(k+1) ‚àív(k)‚à•2, for k ‚ààN.
(A.16)"
REFERENCES,0.573049645390071,Proof. Let
REFERENCES,0.574468085106383,q(k+1) := 1
REFERENCES,0.5758865248226951,"Œ±(v(k) ‚àív(k+1)) + ‚àáf(v(k+1)) ‚àí‚àáf(v(k)), k ‚ààN,"
REFERENCES,0.577304964539007,"and function œï be deÔ¨Åned by (A.12). We Ô¨Årst prove that q(k+1) ‚àà‚àÇF(v(k+1)). It follows from (3.9)
and Fact 14 that 0N ‚àà‚àÇœï(v(k+1)), which together with Fact 13 yields that"
REFERENCES,0.5787234042553191,"v(k) ‚àív(k+1) ‚àíŒ±‚àáf(v(k)) ‚àà‚àÇŒπ‚Ñ¶(v(k+1)), for all k ‚ààN."
REFERENCES,0.5801418439716312,Note that Œπ‚Ñ¶= Œ±Œπ‚Ñ¶. The above inclusion relation can be rewritten as
REFERENCES,0.5815602836879432,"1
Œ±(v(k) ‚àív(k+1)) ‚àí‚àáf(v(k)) ‚àà‚àÇŒπ‚Ñ¶(v(k+1)), for all k ‚ààN.
(A.17)"
REFERENCES,0.5829787234042553,"Now combining (A.17) and the fact ‚àÇF(v(k+1)) = ‚àáf(v(k+1)) + ‚àÇŒπ‚Ñ¶(v(k+1)) yields that q(k+1) ‚àà
‚àÇF(v(k+1)), k ‚ààN."
REFERENCES,0.5843971631205673,"We next prove that (A.16) holds. Since ‚àáf is Qœµ-Lipschitz continuous,"
REFERENCES,0.5858156028368794,‚à•q(k+1)‚à•2 ‚©Ω1
REFERENCES,0.5872340425531914,"Œ±‚à•v(k+1) ‚àív(k)‚à•2 + ‚à•‚àáf(v(k+1)) ‚àí‚àáf(v(k))‚à•2 ‚©Ωb‚à•v(k+1) ‚àív(k)‚à•2,"
REFERENCES,0.5886524822695035,"where b :=
  1"
REFERENCES,0.5900709219858156,"Œ± + ‚à•Qœµ‚à•2

. This completes the proof."
REFERENCES,0.5914893617021276,"We then consider the satisfaction of item (iii) in Proposition 15. To this end, we need the following
two lemmas."
REFERENCES,0.5929078014184397,"Lemma 17 Let {v(k)}k‚ààN be generated by PGA. If Œ± ‚àà

0,
1
‚à•Qœµ‚à•2"
REFERENCES,0.5943262411347517,"
, then {v(k)}k‚ààN is bounded."
REFERENCES,0.5957446808510638,"Proof. We let Œ≥ := ‚à•I‚àíŒ±Qœµ‚à•2, and denote by Œªmax(Qœµ), Œªmin(Qœµ) the maximum and the minimum
eigenvalues of Qœµ, respectively. Since Qœµ is symmetric positive deÔ¨Ånite and Œ± <
1
‚à•Qœµ‚à•2 =
1
Œªmax(Qœµ),
we have Œ≥ = 1‚àíŒ±¬∑Œªmin(Qœµ) ‚àà(0, 1). From Proposition 3, we are easy to see that ‚à•proxŒπ‚Ñ¶(v)‚à•2 ‚â§
‚à•v‚à•2 for all v ‚ààRN, which together with (3.9) yields that"
REFERENCES,0.5971631205673759,"‚à•v(k+1)‚à•2 ‚©Ω‚à•v(k) ‚àíŒ±‚àáf(v(k))‚à•2 = ‚à•(I ‚àíŒ±Qœµ)v(k) + Œ±p‚à•2 ‚©ΩŒ≥‚à•v(k)‚à•2 + Œ±‚à•p‚à•2,"
REFERENCES,0.5985815602836879,for all k ‚ààN. The above inequality implies that
REFERENCES,0.6,"‚à•v(k+1)‚à•2 ‚©ΩŒ≥k+1‚à•v(0)‚à•2 + Œ±‚à•p‚à•2 k
X"
REFERENCES,0.601418439716312,"j=0
Œ≥j = Œ≥k+1‚à•v(0)‚à•2 + Œ±‚à•p‚à•2 ¬∑ 1 ‚àíŒ≥k+1"
REFERENCES,0.6028368794326241,"1 ‚àíŒ≥
,"
REFERENCES,0.6042553191489362,"for all k ‚ààN. Therefore, {v(k)}k‚ààN is bounded, since Œ≥ ‚àà(0, 1)."
REFERENCES,0.6056737588652482,"Lemma 18 Let {v(k)}k‚ààN be generated by PGA. If v‚àóis an accumulation point of {v(k)}k‚ààN, then
v‚àó‚àà‚Ñ¶."
REFERENCES,0.6070921985815603,"Proof. Since v‚àóis an accumulation point of {v(k)}k‚ààN, there exists a subsequence {v(kj)}j‚ààN+ of
{v(k)}k‚ààN such that lim
j‚Üí‚àûv(kj) = v‚àó. Note that the set ‚Ñ¶is closed and v(kj) ‚àà‚Ñ¶for all j ‚ààN+."
REFERENCES,0.6085106382978723,"Hence v‚àó‚àà‚Ñ¶, which completes the proof."
REFERENCES,0.6099290780141844,"Proposition 19 Let {v(k)}k‚ààN be generated by PGA and F := f + Œπ‚Ñ¶. If Œ± ‚àà

0,
1
‚à•Qœµ‚à•2"
REFERENCES,0.6113475177304964,"
, then"
REFERENCES,0.6127659574468085,there exist a subsequence {v(kj)}j‚ààN+ of {v(k)}k‚ààN and v‚àó‚àà‚Ñ¶such that
REFERENCES,0.6141843971631206,"lim
j‚Üí‚àûv(kj) = v‚àóand
lim
j‚Üí‚àûF(v(kj)) = F(v‚àó)."
REFERENCES,0.6156028368794326,"Proof. It follows from Lemma 17 that {v(k)}k‚ààN is bounded. So there exists a subsequence
{v(kj)}j‚ààN+ of {v(k)}k‚ààN converges to some v‚àó‚ààRN. It follows from Lemma 18 that v‚àó‚àà‚Ñ¶.
By the continuity of f on RN, we have limj‚Üí‚àûf(v(kj)) = f(v‚àó). We also know that Œπ‚Ñ¶(v‚àó) =
Œπ‚Ñ¶(v(k)) = 0 for all k ‚ààN. Therefore, limj‚Üí‚àûF(v(kj)) = F(v‚àó), which completes the proof."
REFERENCES,0.6170212765957447,"To employ Proposition 15 for the convergence of PGA. We still need to show that F satisÔ¨Åes the KL
property at v‚àó. To this end, we recall the notions of semi-algebraic sets and functions, and recall a
known result in [4, 9] that establishes the relation between semi-algebraic property and KL property
as Lemma 20."
REFERENCES,0.6184397163120567,"DeÔ¨Ånition 4 (Semi-algebraic sets and functions) A subset S ‚äÇRn is called real semi-algebraic if
it can be represented by S = s[ j=1 t\"
REFERENCES,0.6198581560283688,"i=1
{x ‚ààRn| pij(x) = 0, qij(x) < 0} ,
(A.18)"
REFERENCES,0.6212765957446809,"where pij and qij are real polynomial functions for i ‚ààNt, j ‚ààNs, for some s, t ‚ààN+. A function
œà : Rn ‚ÜíR is called semi-algebraic if its graph {(x, œà(x)) : x ‚ààdom œà} is a semi-algebraic
subset of Rn+1."
REFERENCES,0.6226950354609929,"Lemma 20 Let œà : Rn ‚ÜíR be a proper lower semicontinuous function. If œà is semi-algebraic, then
it satisÔ¨Åes the KL property at any point of dom ‚àÇœà := {u ‚ààRn| ‚àÇœà(u) Ã∏= ‚àÖ}."
REFERENCES,0.624113475177305,"Proposition 21 Let F := f +Œπ‚Ñ¶, where function Œπ‚Ñ¶and f are deÔ¨Åned by (3.6) and (3.7), respectively.
Then dom ‚àÇF = ‚Ñ¶and F is semi-algebraic."
REFERENCES,0.625531914893617,"Proof. We Ô¨Årst prove that dom ‚àÇF = ‚Ñ¶. Note that dom F = ‚Ñ¶is closed, which together with
the deÔ¨Ånition of limiting-subdifferential implies that dom ‚àÇF ‚äÇ‚Ñ¶. For any v ‚àà‚Ñ¶, it is easy to
verify that 0N ‚ààÀÜ‚àÇŒπ‚Ñ¶(v). Then we see from Fact 12 that 0N ‚àà‚àÇŒπ‚Ñ¶(v). Now by Fact 13, we have
‚àáf(v) ‚àà‚àÇF(v), which means that ‚àÇF(v) Ã∏= ‚àÖ, that is, v ‚ààdom ‚àÇF. Hence ‚Ñ¶‚äÇdom ‚àÇF. This
proves dom ‚àÇF = ‚Ñ¶."
REFERENCES,0.6269503546099291,"We then prove that F is semi-algebraic. From the deÔ¨Ånition of semi-algebraic function, we are easy
to see that the sum of semi-algebraic functions is still semi-algebraic. It is obvious that function f is
semi-algebraic. To prove that F is semi-algebraic, it sufÔ¨Åces to show that Œπ‚Ñ¶is semi-algebraic. The
graph of Œπ‚Ñ¶is given by"
REFERENCES,0.6283687943262412,"gra Œπ‚Ñ¶=

x ‚ààRN+1 x1:N ‚©æ0N, ‚à•x1:N‚à•0 ‚©Ωm and xN+1 = 0
	
,
(A.19)"
REFERENCES,0.6297872340425532,"where x1:N := (x1, x2, . . . , xN)‚ä§. Note that there are K :=
 
N
N‚àím

combinations to choose an
index set with (N ‚àím) elements out of the set {1, 2, . . . , N}. We denote these index sets with size
(N ‚àím) by J1, J2, . . . , JK, and let ÀúJi := Ji ‚à™{N + 1}, i ‚ààNK. Then the graph of Œπ‚Ñ¶in (A.19)
can be represented by"
REFERENCES,0.6312056737588653,"gra Œπ‚Ñ¶= K
[ j=1 Ô£Æ Ô£∞ Ô£´ Ô£≠\"
REFERENCES,0.6326241134751773,"i‚ààÀú
Jj"
REFERENCES,0.6340425531914894,"
x ‚ààRN+1 xi = 0
	
Ô£∂ Ô£∏\
Ô£´ Ô£≠\"
REFERENCES,0.6354609929078014,"i/‚ààÀú
Jj"
REFERENCES,0.6368794326241135,"
x ‚ààRN+1 ‚àíxi ‚©Ω0
	
Ô£∂ Ô£∏ Ô£π Ô£ª,"
REFERENCES,0.6382978723404256,which implies that Œπ‚Ñ¶is a semi-algebraic function. This completes the proof.
REFERENCES,0.6397163120567376,"We show in the following proposition that the objective function F := f + Œπ‚Ñ¶satisÔ¨Åes the KL
property at any accumulation point of sequence {v(k)}k‚ààN."
REFERENCES,0.6411347517730497,"Proposition 22 Let {v(k)}k‚ààN be generated by PGA. If v‚àóis an accumulation point of {v(k)}k‚ààN,
then F satisÔ¨Åes the KL property at v‚àó."
REFERENCES,0.6425531914893617,"Proof. Since v‚àóis an accumulation point of {v(k)}k‚ààN, it follows from Lemma 18 that v‚àó‚àà‚Ñ¶. By
Proposition 21, we know that F is semi-algebraic and v‚àó‚ààdom ‚àÇF. Thus the desired result follows
from Lemma 20 immediately."
REFERENCES,0.6439716312056738,"We then show the continuity of the proximity operator proxŒπ‚Ñ¶in the following proposition, which is
also required to prove the convergence of PGA."
REFERENCES,0.6453900709219859,"Proposition 23 Let {x(k)}k‚ààN ‚äÇRN be a sequence converges to some x‚àó‚àà‚Ñ¶, and let
h(k) = proxŒπ‚Ñ¶(x(k)) for k ‚ààN and h‚àó= proxŒπ‚Ñ¶(x‚àó) be given according to Proposition 3.
Then lim
k‚Üí‚àûh(k) = h‚àó."
REFERENCES,0.6468085106382979,"Proof. Since x‚àó‚àà‚Ñ¶, mx‚àó‚©Ωm. We Ô¨Årst consider the case mx‚àó= 0, that is, x‚àó
j ‚©Ω0 for all j ‚ààNN."
REFERENCES,0.64822695035461,"Then h‚àó= 0N. For all Œµ > 0, we let Œ¥1 :=
Œµ
‚àö"
REFERENCES,0.649645390070922,"N . Then there exists K1 ‚ààN such that x(k)
j
‚©ΩŒ¥1 for"
REFERENCES,0.6510638297872341,"all j ‚ààNN and k ‚©æK1. By the deÔ¨Ånition of h(k), we know that 0 ‚©Ωh(k)
j
‚©ΩŒ¥1 for all j ‚ààNN and
k ‚©æK1. Hence
‚à•h(k) ‚àíh‚àó‚à•2 = ‚à•h(k)‚à•2 ‚©Ω
‚àö"
REFERENCES,0.6524822695035462,"NŒ¥1 = Œµ, for all k ‚©æK1,"
REFERENCES,0.6539007092198581,"which implies lim
k‚Üí‚àûh(k) = h‚àó."
REFERENCES,0.6553191489361702,"We then consider the case 0 < mx‚àó‚©Ωm. In this case, the set {j ‚ààNN| x‚àó
j > 0} is nonempty. For"
REFERENCES,0.6567375886524822,"all Œµ > 0, we let Œ¥2 := min
n
1
3x‚àó
min-pos,
Œµ
‚àö N"
REFERENCES,0.6581560283687943,"o
, where"
REFERENCES,0.6595744680851063,"x‚àó
min-pos := min
j‚ààNN{x‚àó
j| x‚àó
j > 0}."
REFERENCES,0.6609929078014184,"There exists K2 ‚ààN such that for all k ‚©æK2, ‚à•x(k) ‚àíx‚àó‚à•2 ‚©ΩŒ¥2, which indicates that"
REFERENCES,0.6624113475177305,"x(k)
j
‚©æx‚àó
j ‚àíŒ¥2 ‚©æ2"
REFERENCES,0.6638297872340425,"3x‚àó
min-pos > 0, for j ‚ààJx‚àó
pos
(A.20)"
REFERENCES,0.6652482269503546,"and
x(k)
j
‚©Ωx‚àó
j + Œ¥2 ‚©ΩŒ¥2 ‚©Ω1"
REFERENCES,0.6666666666666666,"3x‚àó
min-pos, for j ‚ààNN\Jx‚àó
pos.
(A.21)"
REFERENCES,0.6680851063829787,"By the fact mx‚àó‚©Ωm and the deÔ¨Ånitions of h(k) and h‚àó, we can conclude from (A.20) and (A.21)
that for all k ‚©æK2,
h(k)
j
= x(k)
j , h‚àó
j = x‚àó
j, for j ‚ààJx‚àó
pos
and
0 ‚©Ωh(k)
j
‚©ΩŒ¥2, h‚àó
j = 0, for j ‚ààNN\Jx‚àó
pos."
REFERENCES,0.6695035460992907,"Thus ‚à•h(k) ‚àíh‚àó‚à•2 ‚©Ω
‚àö"
REFERENCES,0.6709219858156028,"NŒ¥2 ‚©ΩŒµ, which yields lim
k‚Üí‚àûh(k) = h‚àó. This completes the proof."
REFERENCES,0.6723404255319149,We are now in a position to utilize Proposition 15 and Theorem 2 to prove Theorem 5.
REFERENCES,0.6737588652482269,"Proof of Theorem 5. We Ô¨Årst prove item (i). According to Propositions 4, 16, 19 and 22, the
convergence of {v(k)}k‚ààN to a critical point v‚àó‚àà‚Ñ¶of F := f + Œπ‚Ñ¶follows from Proposition 15
immediately. By item (ii) in Theorem 2, to prove that v‚àóis a locally optimal solution of model (3.7)
(or model (3.4)), it sufÔ¨Åces to show that (3.8) holds. Since lim
k‚Üí‚àûv(k) = v‚àó, the Lipschitz continuity"
REFERENCES,0.675177304964539,"of ‚àáf yields that
lim
k‚Üí‚àû"
REFERENCES,0.676595744680851,"
v(k) ‚àíŒ±‚àáf(v(k))

= v‚àó‚àíŒ±‚àáf(v‚àó)."
REFERENCES,0.6780141843971631,"Now by letting k ‚Üí‚àûon both side of (3.9) and employing Proposition 23, we obtain (3.8), which
proves the convergence of {v(k)}k‚ààN to a locally optimal solution v‚àóof model (3.4)."
REFERENCES,0.6794326241134752,"We then prove the convergence rates of {v(k)}k‚ààN. Let Œ¶k(v) := ‚à•v ‚àív(k) + Œ±‚àáf(v(k))‚à•2
2,
v ‚ààRN. It is obvious that Œ¶k is 2-strongly convex, since the Hessian matrix of function Œ¶k ‚àí‚à•¬∑ ‚à•2
2
is positive semideÔ¨Ånite. To prove the convergence rate, we Ô¨Årst show that there exists K ‚ààN such
that
‚ü®‚àáf(v‚àó), v(k) ‚àív‚àó‚ü©‚©æ0 and ‚ü®‚àáŒ¶k(v(k+1)), v‚àó‚àív(k+1)‚ü©‚©æ0,
(A.22)
for all k ‚©æK. From the proof of Theorem 2 in Appendix A.2, we see that (A.9) holds. It has been
shown that limk‚Üí‚àûv(k) = v‚àóand v(k) ‚àà‚Ñ¶for all k ‚ààN (see item (i) in Proposition 4). Then there
exists K1 ‚ààN such that v(k) ‚ààB(v‚àó; Œ¥) ‚à©‚Ñ¶for all k ‚©æK1, which together with (A.9) implies that
the Ô¨Årst inequality in (A.22) holds for all k ‚©æK1. According to the deÔ¨Ånition of Œ¶k and (3.9), we
see that"
REFERENCES,0.6808510638297872,"v(k+1) = argmin
v‚ààRN
1
2‚à•v ‚àív(k) + Œ±‚àáf(v(k))‚à•2
2 + Œπ‚Ñ¶(v) = argmin
v‚àà‚Ñ¶
Œ¶k(v)."
REFERENCES,0.6822695035460993,"By a procedure similar to the Ô¨Årst paragraph of the proof of Theorem 2, we can establish that"
REFERENCES,0.6836879432624113,"v(k+1) = proxŒπ‚Ñ¶

v(k+1) ‚àíŒ±‚àáŒ¶k(v(k+1))

."
REFERENCES,0.6851063829787234,"Note that Œ¶k is also strongly convex. Using a proof analogous to that of the Ô¨Årst inequality in (A.22),
we can establish the existence of K2 ‚ààN such that the second inequality in (A.22) holds for all
k ‚©æK2. By setting K = max{K1, K2}, we conclude that (A.22) holds for all k ‚©æK."
REFERENCES,0.6865248226950355,It follows from Lemma 11 that
REFERENCES,0.6879432624113475,"Œ¶k(v‚àó) ‚©æŒ¶k(v(k+1)) + ‚ü®‚àáŒ¶k(v(k+1)), v‚àó‚àív(k+1)‚ü©+ ‚à•v‚àó‚àív(k+1)‚à•2
2,
(A.23)"
REFERENCES,0.6893617021276596,which together with the second inequality in (A.22) implies that
REFERENCES,0.6907801418439716,"Œ¶k(v‚àó) ‚©æŒ¶k(v(k+1)) + ‚à•v‚àó‚àív(k+1)‚à•2
2,"
REFERENCES,0.6921985815602837,"that is,"
REFERENCES,0.6936170212765957,"‚à•v‚àó‚àív(k) + Œ±‚àáf(v(k))‚à•2
2 ‚©æ‚à•v(k+1) ‚àív(k) + Œ±‚àáf(v(k))‚à•2
2 + ‚à•v‚àó‚àív(k+1)‚à•2
2,
(A.24)"
REFERENCES,0.6950354609929078,"for all k ‚©æK. For simplicity of notation, we deÔ¨Åne z(k) := v(k+1) ‚àív(k), k ‚ààN. Expanding
(A.24) and dividing the resulting inequality by 2Œ± yields"
REFERENCES,0.6964539007092199,"‚ü®‚àáf(v(k)), z(k)‚ü©+ 1"
REFERENCES,0.6978723404255319,"2Œ±‚à•z(k)‚à•2
2 + 1"
REFERENCES,0.699290780141844,"2Œ±‚à•v‚àó‚àív(k+1)‚à•2
2 ‚©Ω1"
REFERENCES,0.700709219858156,"2Œ±‚à•v‚àó‚àív(k)‚à•2
2 + ‚ü®‚àáf(v(k)), v‚àó‚àív(k)‚ü©,
(A.25)
for all k ‚©æK. Recall that (A.14) in the proof of Proposition 4 holds. Since Œ± ‚àà

0,
1
‚à•Qœµ‚à•2"
REFERENCES,0.7021276595744681,"
, (A.14)
implies that"
REFERENCES,0.7035460992907802,"f(v(k+1)) ‚àíf(v(k)) ‚©Ω‚ü®‚àáf(v(k)), z(k)‚ü©+ 1"
REFERENCES,0.7049645390070922,"2Œ±‚à•z(k)‚à•2
2, for all k ‚ààN.
(A.26)"
REFERENCES,0.7063829787234043,"Combining (A.26) and (A.25), we obtain that"
REFERENCES,0.7078014184397163,f(v(k+1)) ‚àíf(v(k)) + 1
REFERENCES,0.7092198581560284,"2Œ±‚à•v‚àó‚àív(k+1)‚à•2
2 ‚©Ω1"
REFERENCES,0.7106382978723405,"2Œ±‚à•v‚àó‚àív(k)‚à•2
2 + ‚ü®‚àáf(v(k)), v‚àó‚àív(k)‚ü©, (A.27)"
REFERENCES,0.7120567375886525,for all k ‚©æK. It follows from the Ô¨Årst order condition for convexity (Proposition B.3 of [7]) that
REFERENCES,0.7134751773049646,"f(v‚àó) ‚©æf(v(k)) + ‚ü®‚àáf(v(k)), v‚àó‚àív(k)‚ü©,"
REFERENCES,0.7148936170212766,which together with (A.27) yields
REFERENCES,0.7163120567375887,f(v(k+1)) + 1
REFERENCES,0.7177304964539007,"2Œ±‚à•v‚àó‚àív(k+1)‚à•2
2 ‚©Ωf(v‚àó) + 1"
REFERENCES,0.7191489361702128,"2Œ±‚à•v‚àó‚àív(k)‚à•2
2,"
REFERENCES,0.7205673758865249,"that is,"
REFERENCES,0.7219858156028369,f(v(k+1)) ‚àíf(v‚àó) ‚©Ω1 2Œ±
REFERENCES,0.723404255319149,"
‚à•v(k) ‚àív‚àó‚à•2
2 ‚àí‚à•v(k+1) ‚àív‚àó‚à•2
2

, for all k ‚©æK."
REFERENCES,0.724822695035461,"We see from (A.11) that {f(v(k))}k‚ààN is monotonically decreasing. Then for all j ‚ààN+,"
REFERENCES,0.7262411347517731,"j

f(v(K+j)) ‚àíf(v‚àó)

‚©Ω"
REFERENCES,0.7276595744680852,"K+j‚àí1
X i=K"
REFERENCES,0.7290780141843972,"
f(v(i+1)) ‚àíf(v‚àó)
 ‚©Ω1 2Œ±"
REFERENCES,0.7304964539007093,"K+j‚àí1
X i=K"
REFERENCES,0.7319148936170212,"
‚à•v(i) ‚àív‚àó‚à•2
2 ‚àí‚à•v(i+1) ‚àív‚àó‚à•2
2
 = 1 2Œ±"
REFERENCES,0.7333333333333333,"
‚à•v(K) ‚àív‚àó‚à•2
2 ‚àí‚à•v(K+j) ‚àív‚àó‚à•2
2

."
REFERENCES,0.7347517730496453,"Hence
f(v(K+j)) ‚àíf(v‚àó) ‚©Ω
1
2Œ±j ‚à•v(K) ‚àív‚àó‚à•2
2.
(A.28)"
REFERENCES,0.7361702127659574,"Note that ‚à•v(K) ‚àív‚àó‚à•2
2 is a constant. Let k = K + j and C :=
1
Œ±‚à•v(K) ‚àív‚àó‚à•2
2. Then (A.28)
implies that"
REFERENCES,0.7375886524822695,f(v(k)) ‚àíf(v‚àó) ‚©ΩC ¬∑ 1
REFERENCES,0.7390070921985815,2j ‚©ΩC ¬∑ 1
REFERENCES,0.7404255319148936,"k , for j ‚©æK. Thus"
REFERENCES,0.7418439716312056,"|f(v(k)) ‚àíf(v‚àó)| = O
1 k"
REFERENCES,0.7432624113475177,"
.
(A.29)"
REFERENCES,0.7446808510638298,"Combining Lemma 11 and the Ô¨Årst inequality in (A.22), we obtain that"
REFERENCES,0.7460992907801418,f(v(k)) ‚àíf(v‚àó) ‚©æœµ
REFERENCES,0.7475177304964539,"2‚à•v(k) ‚àív‚àó‚à•2
2, for all k ‚©æK."
REFERENCES,0.7489361702127659,"This together with (A.29) yields that ‚à•v(k) ‚àív‚àó‚à•2 = O

1
‚àö k 
."
REFERENCES,0.750354609929078,"We next prove item (ii). The fact v‚àó‚©æ0N follows from (3.8) and Proposition 3 directly. Assume,
to reach a contradiction, that v‚àó= 0N. Item (i) in this theorem shows that v‚àóis a locally optimal
solution of model (3.7). Then there exists Œ¥ > 0 such that"
REFERENCES,0.75177304964539,"f(v) ‚©æf(v‚àó) = f(0N) = 0, for all v ‚àà‚Ñ¶‚à©B(v‚àó; Œ¥).
(A.30)"
REFERENCES,0.7531914893617021,"We recall from the assumption of this theorem that there exists Àúw ‚àà‚Ñ¶such that p‚ä§Àúw > 0. Let
ÀúwŒ± := Œ± Àúw, where Œ± > 0. Then ÀúwŒ± ‚àà‚Ñ¶and p‚ä§ÀúwŒ± > 0. Note that when Œ± tends to 0, the quadratic
term 1"
REFERENCES,0.7546099290780142,"2 Àúw‚ä§
Œ± Qœµ ÀúwŒ± is of higher order inÔ¨Ånitesimal than the linear term p‚ä§ÀúwŒ±. There exists some
sufÔ¨Åcient small Œ± > 0 such that ÀúwŒ± ‚ààB(v‚àó; Œ¥) and"
REFERENCES,0.7560283687943262,f( ÀúwŒ±) = 1
REFERENCES,0.7574468085106383,"2 Àúw‚ä§
Œ± Qœµ ÀúwŒ± ‚àíp‚ä§ÀúwŒ± < 0,"
REFERENCES,0.7588652482269503,"which contradicts (A.30). Therefore, v‚àóÃ∏= 0N."
REFERENCES,0.7602836879432624,"Lastly, we prove item (iii). Since v‚àó‚©æ0N and v‚àóÃ∏= 0N. There exit Œµ0 > 0 and K‚Ä≤ ‚ààN such that
(v‚àó)‚ä§1N > Œµ0 and (v(k))‚ä§1N > Œµ0 for all k ‚©æK‚Ä≤, and hence w(k) =
v(k)"
REFERENCES,0.7617021276595745,(v(k))‚ä§1N for all k ‚©æK‚Ä≤.
REFERENCES,0.7631205673758865,Note that v‚àóand {v(k)}k‚ààN are both bounded. There exist C1 > 0 and C2 > 0 such that
REFERENCES,0.7645390070921986,"‚à•v(k)‚à•2
(v(k))‚ä§1N
 ¬∑ |(v‚àó)‚ä§1N| ‚©ΩC1 and C1
‚àö"
REFERENCES,0.7659574468085106,"N +

1
(v‚àó)‚ä§1N ‚©ΩC2."
REFERENCES,0.7673758865248227,"Then for all k ‚©æK‚Ä≤,
w(k) ‚àí
v(k)"
REFERENCES,0.7687943262411348,(v‚àó)‚ä§1N
REFERENCES,0.7702127659574468,"2
‚©Ω

1
(v(k))‚ä§1N
‚àí
1
(v‚àó)‚ä§1N"
REFERENCES,0.7716312056737589,¬∑ ‚à•v(k)‚à•2
REFERENCES,0.7730496453900709,"=
‚à•v(k)‚à•2
(v(k))‚ä§1N
 ¬∑ |(v‚àó)‚ä§1N| ¬∑
(v(k) ‚àív‚àó)‚ä§1N ‚©ΩC1
‚àö"
REFERENCES,0.774468085106383,"N‚à•v(k) ‚àív‚àó‚à•2,"
REFERENCES,0.775886524822695,and hence
REFERENCES,0.7773049645390071,"‚à•w(k) ‚àíw‚àó‚à•2 =
w(k) ‚àí
v(k)"
REFERENCES,0.7787234042553192,"(v‚àó)‚ä§1N
+
v(k)"
REFERENCES,0.7801418439716312,"(v‚àó)‚ä§1N
‚àíw‚àó

2 ‚©ΩC1
‚àö"
REFERENCES,0.7815602836879433,"N‚à•v(k) ‚àív‚àó‚à•2 +

1
(v‚àó)‚ä§1N"
REFERENCES,0.7829787234042553,‚à•v(k) ‚àív‚àó‚à•2
REFERENCES,0.7843971631205674,‚©ΩC2‚à•v(k) ‚àív‚àó‚à•2.
REFERENCES,0.7858156028368795,"This implies that ‚à•w(k) ‚àíw‚àó‚à•2 = O

1
‚àö k"
REFERENCES,0.7872340425531915,"
. Similarly, we can prove that there exists C3 > 0 such
that
|S(w(k)) ‚àíS(w‚àó)| ‚©ΩC3‚à•w(k) ‚àíw‚àó‚à•2, for all k ‚©æK‚Ä≤,"
REFERENCES,0.7886524822695036,"which implies that |S(w(k)) ‚àíS(w‚àó)| = O

1
‚àö k"
REFERENCES,0.7900709219858156,"
. This completes the proof."
REFERENCES,0.7914893617021277,"A.6
Proof of Theorem 6"
REFERENCES,0.7929078014184398,"To prove Theorem 6, we recall Proposition 11.4 of [6] as the following lemma."
REFERENCES,0.7943262411347518,"Lemma 24 Let œà : Rn ‚ÜíR be be proper and convex. Then every local minimizer of œà is a global
minimizer."
REFERENCES,0.7957446808510639,Proof of Theorem 6. We Ô¨Årst prove item (i). Let ŒπÀÜ‚Ñ¶be deÔ¨Åned by
REFERENCES,0.7971631205673759,"ŒπÀÜ‚Ñ¶(v) :=
0,
if v ‚ààÀÜ‚Ñ¶;
+‚àû,
otherwise."
REFERENCES,0.798581560283688,"Then model (3.10) is equilvalent to min
v‚ààRN ÀÜF(v), where ÀÜF := f + ŒπÀÜ‚Ñ¶. Of course, ŒπÀÜ‚Ñ¶is proper. The"
REFERENCES,0.8,"convexity of ÀÜ‚Ñ¶implies that ŒπÀÜ‚Ñ¶is convex (see Example 8.3 of [6]). Recall that f is strictly convex,
and hence ÀÜF is proper and strictly convex. Since v‚àóis a locally optimal solution of model (3.7) and
ÀÜ‚Ñ¶‚äÇ‚Ñ¶, there exists Œ¥ > 0 such that"
REFERENCES,0.8014184397163121,"f(u) ‚©æf(v‚àó), for all u ‚ààB(v‚àó; Œ¥) ‚à©ÀÜ‚Ñ¶.
(A.31)"
REFERENCES,0.8028368794326242,"The fact v‚àó‚ààÀÜ‚Ñ¶gives ŒπÀÜ‚Ñ¶(v‚àó) = 0. Then (A.31) implies that ÀÜF(u) ‚©æÀÜF(v‚àó) for all u ‚ààB(v‚àó; Œ¥),
that is, v‚àóis a local minimizer of ÀÜF. Now it follows from Lemma 24 that v‚àóis also a global minimizer
of ÀÜF. The strict convexity of ÀÜF implies the uniqueness of its global minimizer."
REFERENCES,0.8042553191489362,"We next prove item (ii). From item (i) in this theorem and item (ii) in Theorem 5, we see that
v‚àó‚ààÀÜ‚Ñ¶is a globally optimal solution of model (3.10) and v‚àóÃ∏= 0N. Then we are able to prove that
p‚ä§v‚àó= (v‚àó)‚ä§Qœµv‚àó> 0. We omit this proof here since it is very similar to the proof of Lemma 8.
Now we have v‚àó=
p‚ä§v‚àó"
REFERENCES,0.8056737588652483,"(v‚àó)‚ä§Qœµv‚àóv‚àó. For any v ‚ààÀÜ‚Ñ¶such that p‚ä§v > 0, we let u :=
p‚ä§v
v‚ä§Qœµvv. Then"
REFERENCES,0.8070921985815603,u ‚ààÀÜ‚Ñ¶and p‚ä§u > 0. Hence ‚àí1
REFERENCES,0.8085106382978723,"2
(p‚ä§v‚àó)2"
REFERENCES,0.8099290780141843,(v‚àó)‚ä§Qœµv‚àó= 1
REFERENCES,0.8113475177304964,2(v‚àó)‚ä§Qœµv‚àó‚àíp‚ä§v‚àó‚©Ω1
REFERENCES,0.8127659574468085,2u‚ä§Qœµu ‚àíp‚ä§u = ‚àí1
REFERENCES,0.8141843971631205,"2
(p‚ä§v)2"
REFERENCES,0.8156028368794326,"v‚ä§Qœµv ,"
REFERENCES,0.8170212765957446,"which implies that
p‚ä§v‚àó
p"
REFERENCES,0.8184397163120567,"(v‚àó)‚ä§Qœµv‚àó‚©æ
p‚ä§v
p"
REFERENCES,0.8198581560283688,"v‚ä§Qœµv
, for all v ‚ààÀÜ‚Ñ¶."
REFERENCES,0.8212765957446808,"Since v‚àó‚ààÀÜ‚Ñ¶and v‚àóÃ∏= 0N, by the deÔ¨Ånition of ÀÜ‚Ñ¶1, we see that w‚àó‚ààÀÜ‚Ñ¶1. Note that ÀÜ‚Ñ¶1 ‚äÇÀÜ‚Ñ¶. For
all w ‚ààÀÜ‚Ñ¶1,
p‚ä§w
p"
REFERENCES,0.8226950354609929,"w‚ä§Qœµw
‚©Ω
p‚ä§v‚àó
p"
REFERENCES,0.8241134751773049,"(v‚àó)‚ä§Qœµv‚àó=
p‚ä§w‚àó
p"
REFERENCES,0.825531914893617,"(w‚àó)‚ä§Qœµw‚àó,"
REFERENCES,0.826950354609929,"which implies that w‚àóis a globally optimal solution of model max
w‚ààÀÜ‚Ñ¶1
S(w)."
REFERENCES,0.8283687943262411,"Lastly, we prove item (iii). Note that w‚àó
j > 0 for all j ‚ààJv‚àó
pos. Let w‚àó
min-pos :=
min
j‚ààJv‚àó
pos
{w‚àó
j },"
REFERENCES,0.8297872340425532,Œ¥ := 1
REFERENCES,0.8312056737588652,"3w‚àó
min-pos, and let w be any vector in B(w‚àó; Œ¥) ‚à©‚Ñ¶1. Then wj ‚©æ2Œ¥ > 0 for j ‚ààJv‚àó
pos, and
|wj| ‚©ΩŒ¥ for j ‚ààNN\Jv‚àó
pos. Since w ‚àà‚Ñ¶1 and mv‚àó= m, we conclude that wj = 0 for j ‚ààNN\Jv‚àó
pos,
which implies that w ‚ààÀÜ‚Ñ¶1. It has been shown that w‚àóis an optimal solution of model max
w‚ààÀÜ‚Ñ¶1
S(w)."
REFERENCES,0.8326241134751773,"Therefore, S(w) ‚©ΩS(w‚àó) for all w ‚ààB(w‚àó; Œ¥) ‚à©‚Ñ¶1, that is, w‚àóis a locally optimal solution of
model (3.3). This completes the proof."
REFERENCES,0.8340425531914893,"A.7
Proof of Theorem 7"
REFERENCES,0.8354609929078014,"Proof. According to Theorem 1 and item (i) in Theorem 2, to prove the desired result, it sufÔ¨Åces to
show that"
REFERENCES,0.8368794326241135,v‚àó= proxŒπ‚Ñ¶
REFERENCES,0.8382978723404255,"
v‚àó‚àí1"
REFERENCES,0.8397163120567376,"œµ ‚àáf(v‚àó)

.
(A.32)"
REFERENCES,0.8411347517730496,"From the proof of Theorem 5, we know that (3.8) holds. According to the computation of proxŒπ‚Ñ¶
in Proposition 3, to guarantee the validity of (3.8), we have ‚àáif(v‚àó) = 0 for all i ‚ààsupp(v‚àó).
Otherwise, there exists some i0 ‚ààsupp(v‚àó) such that v‚àó
i0 Ã∏= v‚àó
i0 ‚àíŒ±‚àái0f(v‚àó), which together with
Proposition 3 implies that v‚àóÃ∏= proxŒπ‚Ñ¶(v‚àó‚àíŒ±‚àáf(v‚àó)), a contradiction to (3.8)."
REFERENCES,0.8425531914893617,"Suppose that mv‚àó< m. Then we have ‚àáif(v‚àó) ‚©æ0 for all i ‚ààNN\supp(v‚àó). Otherwise, there
exists some i1 ‚ààNN\supp(v‚àó) such that v‚àó
i1 ‚àíŒ±‚àái1f(v‚àó) > 0. Note that mv‚àó< m. The operation"
REFERENCES,0.8439716312056738,"of proxŒπ‚Ñ¶will preserve the positive value v‚àó
i1 ‚àíŒ±‚àái1f(v‚àó) instead of truncating it as 0, which violates
(3.8). In this case, we now have v‚àó
i ‚àí1"
REFERENCES,0.8453900709219858,"œµ ‚àáif(v‚àó) = v‚àó
i for i ‚ààsupp(v‚àó) and v‚àó
i ‚àí1"
REFERENCES,0.8468085106382979,"œµ ‚àáif(v‚àó) ‚©Ω0
for i ‚ààNN\supp(v‚àó), which imply that (A.32) holds."
REFERENCES,0.8482269503546099,"Suppose that item (ii) holds. Let Œ¥ := min{v‚àó
i |i ‚ààsupp(v‚àó)} > 0. For i ‚ààNN\supp(v‚àó),
since 1"
REFERENCES,0.849645390070922,"œµ ‚àáif(v‚àó) > ‚àíŒ¥ and vi = 0, we have v‚àó
i ‚àí1"
REFERENCES,0.851063829787234,"œµ ‚àáif(v‚àó) < Œ¥. Note that mv‚àó= m. The
operation of proxŒπ‚Ñ¶makes v‚àó
i ‚àí1"
REFERENCES,0.8524822695035461,"œµ ‚àáif(v‚àó) = v‚àó
i for i ‚ààsupp(v‚àó) and v‚àó
i ‚àí1"
REFERENCES,0.8539007092198582,"œµ ‚àáif(v‚àó) = 0 for
i ‚ààNN\supp(v‚àó), that is, (A.32) holds. This completes the proof."
REFERENCES,0.8553191489361702,"A.8
Validation of PGA‚Äôs Global Optimality Through Simulation Experiments"
REFERENCES,0.8567375886524823,"To test the validation of PGA‚Äôs global optimality, we conduct a set of simulation experiments by
considering model (3.7), where Qœµ := Q‚ä§Q + œµI. The iterative scheme of PGA for solving this
model is given by (3.9) with Œ± =
0.99
‚à•Q‚à•2 ."
REFERENCES,0.8581560283687943,"In the simulation experiments, we set Œ£ ‚ààR10√ó10 by Œ£ij := 0.5|i‚àíj|, and randomly generate a
matrix Q ‚ààR50√ó10 from the multivariate normal distribution, with mean vector 010 and covariance
matrix Œ£. We set p as a random vector with components that are randomly generated numbers in the
range [‚àí10, 10], and casually set œµ = 0.001 and the sparsity m = 3."
REFERENCES,0.8595744680851064,"The direct exhaustive approach enumerates all possible support set conÔ¨Ågurations, totaling C3
10 = 120
cases. In each case, we solve a 3-dimension quadratic programming problem. By comparing the
optimal solutions corresponding to these 120 cases, we obtain the exact globally optimal solution of
model (3.7). After that, we can evaluate the optimality of PGA‚Äôs convergence."
REFERENCES,0.8609929078014185,"For each experiment, we performed 500 iterations of PGA. To ensure the robustness of our Ô¨Åndings,
we used three different initializations: 0N, 1N/N and 1N. We repeated the experiments 104 times
for each initialization, with different Q and p in each run. We found that in over 7,200 of the
104 trials, for any of the three initializations, both the normalized error of the iterative sequence
‚à•v(k) ‚àív‚àó‚à•2/‚à•v‚àó‚à•2 and the normalized error of the function value |f(v(k)) ‚àíf(v‚àó)|/|f(v‚àó)|
were smaller than 10‚àí10. Here v‚àódenotes the globally optimal solution, and v(k) represents the
iterative sequence at the k-th iteration of PGA. We show the plots of ‚à•v(k) ‚àív‚àó‚à•2/‚à•v‚àó‚à•2 and
|f(v(k)) ‚àíf(v‚àó)|/|f(v‚àó)| in the following Figure 1, and show in Table 4 these two normalized
errors obtained at 500 iterations of PGA in ten simulation experiments."
REFERENCES,0.8624113475177305,"0
100
200
300
400
500
Iteration number 0 0.2 0.4 0.6 0.8"
REFERENCES,0.8638297872340426,"0
100
200
300
400
500
Iteration number 0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.8652482269503546,"Figure 1: Simulation results of PGA for model (3.7). Left: normalized error of the iterative sequence
versus number of iterations. Right: normalized error of function value versus number of iterations."
REFERENCES,0.8666666666666667,Table 4: The normalized errors obtained at 500 iterations of PGA in 10 simulation experiments.
REFERENCES,0.8680851063829788,"k
1
2
3
4
5
6
7
8
9
10"
REFERENCES,0.8695035460992908,‚à•v(k)‚àív‚àó‚à•2
REFERENCES,0.8709219858156029,"‚à•v‚àó‚à•2
2.45E-16 1.62E-16 3.03E-16 6.23E-01 1.14E-16 7.68E-16 7.41E-16 9.55E-17 1.50E-16 2.52E-16"
REFERENCES,0.8723404255319149,‚à•f(v(k))‚àíf(v‚àó)‚à•2
REFERENCES,0.873758865248227,"‚à•f(v‚àó)‚à•2
0.00
1.48E-16 1.68E-16 1.17E-01
0.00
1.40E-16 4.73E-16 1.36E-16
0.00
0.00"
REFERENCES,0.875177304964539,"From the simulation experiments, we conclude that the proposed PGA has a high probability (over
72%) of directly converging to a globally optimal solution of model (3.7). This Ô¨Ånding is consistent
with the sufÔ¨Åcient conditions for global optimality in Theorem 7."
REFERENCES,0.8765957446808511,"A.9
Solving Algorithm: mSSRM-PGA"
REFERENCES,0.8780141843971632,"Algorithm A1 mSSRM-PGA
Input: Given the sample asset return matrix R ‚ààRT √óN and the positive parameter œµ.
Preparation: Let p =
1
T R‚ä§1T , Q =
1
‚àöT ‚àí1
 
R ‚àí1"
REFERENCES,0.8794326241134752,"T 1T √óT R

and Qœµ = Q‚ä§Q + œµI. Compute
the largest eigenvalue Œª1 of Qœµ, and set Œ± = 0.999"
REFERENCES,0.8808510638297873,"Œª1 .
Initialization: Set v(0) = p, tol = 10‚àí5, MaxIter = 104 and k = 0.
repeat
1. v(k+1) = proxŒπ‚Ñ¶
 
v(k) ‚àíŒ±
 
Qœµv(k) ‚àíp
"
REFERENCES,0.8822695035460993,2. k = k + 1
REFERENCES,0.8836879432624114,until ‚à•v(k)‚àív(k‚àí1)‚à•2
REFERENCES,0.8851063829787233,"‚à•v(k‚àí1)‚à•2
‚©Ωtol or k > MaxIter."
REFERENCES,0.8865248226950354,"if v(k) Ã∏= 0N
3. w‚àó=
v(k)"
REFERENCES,0.8879432624113475,"(v(k))‚ä§1N
else
4. w‚àó= 0N
Output: The portfolio w‚àó."
REFERENCES,0.8893617021276595,"A.10
Additional Experimental Results"
REFERENCES,0.8907801418439716,"The 1/N strategy rebalances to the equally-weighted portfolio on each trading time. S1, S2 and
S3 are different versions of SSPO-‚Ñì0 in (2.5), among which S1 is deterministic but S2 and S3 are
randomized. The hyperparameters of these competitors are set according to the original papers."
REFERENCES,0.8921985815602836,"FF25 contains 25 portfolios developed by BE/ME (book equity to market equity) and investment in
the US market. FF25EU contains 25 portfolios developed by ME and prior return in the European
market. FF32 contains 32 portfolios developed by BE/ME and investment in the US market. FF49
contains 49 industry portfolios in the US market. FF100 contains 100 portfolios developed by ME
and BE/ME, while FF100MEINV contains 100 portfolios developed by ME and investment, both in
the US market. The information of these data sets are given in Table 5."
REFERENCES,0.8936170212765957,Table 5: Information of 6 real-world monthly benchmark data sets.
REFERENCES,0.8950354609929078,"Data Set
Region
Time
Months
Assets
FF25
US
Jul/1971 ‚àºMay/2023
623
25
FF25EU
EU
Nov/1990 ‚àºMay/2023
391
25
FF32
US
Jul/1971 ‚àºMay/2023
623
32
FF49
US
Jul/1971 ‚àºMay/2023
623
49
FF100
US
Jul/1971 ‚àºMay/2023
623
100
FF100MEINV
US
Jul/1971 ‚àºMay/2023
623
100"
REFERENCES,0.8964539007092198,"There is a relaxation approach based on the semi-deÔ¨Ånite programming (SDP Relaxation, [20])
that intends to address nearly the same mSSRM model (3.3) of this paper, except for relaxing the
cardinality constraint and the long-only constraint. Therefore, this method fails to control cardinality
exactly and a simplex projection [16] should be implemented to ensure feasibility. Its experimental
results are also provided in Table 6, which are not so good as those of mSSRM-PGA."
REFERENCES,0.8978723404255319,"Table 6: Final cumulative wealths (CW) and Sharpe Ratios (SR) of SDP Relaxation and mSSRM-PGA
on 6 data sets (T = 60)."
REFERENCES,0.8992907801418439,"Data Set
FF25
FF25EU
FRENCH32
FF49
FF100
FF100MEINV
Strategy
CW
SR
CW
SR
CW
SR
CW
SR
CW
SR
CW
SR
SDP Relaxation
323.76 0.2340
14.25
0.1674 290.24 0.2224 280.46 0.2151
0.51
0.0218 194.09 0.1528
mSSRM-PGA (m=10) 615.34 0.2481 126.02 0.2712 991.89 0.2612 285.02 0.2151 527.09 0.2290 375.75 0.2217
mSSRM-PGA (m=15) 614.71 0.2481 125.19 0.2708 996.32 0.2615 262.54 0.2135 522.28 0.2289 383.44 0.2232
mSSRM-PGA (m=20) 614.70 0.2481 125.19 0.2708 996.23 0.2615 262.06 0.2134 515.50 0.2285 384.65 0.2234"
REFERENCES,0.900709219858156,"As for practical issues, Table 7 shows the running times for different methods with T = 60, where
mSSRM-PGA achieves competitive computational efÔ¨Åciency. Figure 2 shows the Ô¨Ånal cumulative
wealths of different methods as the transaction cost rate ŒΩ varies from 0 to 0.5% with T = 60, which
indicates that mSSRM-PGA can withstand considerable levels of transaction cost rates."
REFERENCES,0.902127659574468,"Table 7: The average running times (in seconds) of different portfolio optimization models for one
period on 6 data sets."
REFERENCES,0.9035460992907801,"Data Set
SPOLC
SSPO
S1
S2
S3
SSMP MAXER IPSRM-D PLCT SDP Relaxation mSSRM-PGA
FF25
0.0263
0.0234 5.72E-05 5.63E-05 8.46E-05 0.0122
0.0525
0.0009
0.0020
1.0115
0.0052
FF25EU
0.0222
0.0239 1.70E-05 2.89E-05 2.59E-05 0.0316
0.0588
0.0009
0.0015
0.8178
0.0059
FRENCH32
0.0239
0.0250 2.93E-05 2.81E-05 3.08E-05 0.0075
0.0392
0.0012
0.0021
1.2862
0.0075
FF49
0.0252
0.0458 2.94E-05 3.51E-05 2.82E-05 0.0083
0.0270
0.0029
0.0034
12.3780
0.0114
FF100
0.0306
0.0854 5.38E-05 4.48E-05 4.27E-05 0.0451
0.0458
0.0132
0.0052
24.5852
0.0713
FF100MEINV
0.0296
0.0864 5.08E-05 4.81E-05 4.53E-05 0.0145
0.0152
0.0144
0.0059
23.9911
0.0696"
REFERENCES,0.9049645390070922,"0
0.1
0.2
0.3
0.4
0.5
Transaction Cost Rate (%) 0 100 200 300 400 500 600 700"
REFERENCES,0.9063829787234042,Cumulative Wealth
REFERENCES,0.9078014184397163,"1/N
SPOLC
SSPO
S3
S2
S1
SSMP"
REFERENCES,0.9092198581560283,"MAXER
IPSRM-D
PLCP
SDP-relaxation
mSSRM-PGA(m=10)
mSSRM-PGA(m=15)
mSSRM-PGA(m=20)"
REFERENCES,0.9106382978723404,(a) FF25
REFERENCES,0.9120567375886525,"0
0.1
0.2
0.3
0.4
0.5
Transaction Cost Rate (%) 0 20 40 60 80 100 120"
REFERENCES,0.9134751773049645,Cumulative Wealth
REFERENCES,0.9148936170212766,"1/N
SPOLC
SSPO
S3
S2
S1
SSMP"
REFERENCES,0.9163120567375886,"MAXER
IPSRM-D
PLCP
SDP-relaxation
mSSRM-PGA(m=10)
mSSRM-PGA(m=15)
mSSRM-PGA(m=20)"
REFERENCES,0.9177304964539007,(b) FF25EU
REFERENCES,0.9191489361702128,"0
0.1
0.2
0.3
0.4
0.5
Transaction Cost Rate (%) 0 100 200 300 400 500 600 700 800 900 1000"
REFERENCES,0.9205673758865248,Cumulative Wealth
REFERENCES,0.9219858156028369,"1/N
SPOLC
SSPO
S3
S2
S1
SSMP"
REFERENCES,0.9234042553191489,"MAXER
IPSRM-D
PLCP
SDP-relaxation
mSSRM-PGA(m=10)
mSSRM-PGA(m=15)
mSSRM-PGA(m=20)"
REFERENCES,0.924822695035461,(c) FF32
REFERENCES,0.926241134751773,"0
0.1
0.2
0.3
0.4
0.5
Transaction Cost Rate (%) 0 50 100 150 200 250 300 350 400"
REFERENCES,0.9276595744680851,Cumulative Wealth
REFERENCES,0.9290780141843972,"1/N
SPOLC
SSPO
S3
S2
S1
SSMP"
REFERENCES,0.9304964539007092,"MAXER
IPSRM-D
PLCP
SDP-relaxation
mSSRM-PGA(m=10)
mSSRM-PGA(m=15)
mSSRM-PGA(m=20)"
REFERENCES,0.9319148936170213,(d) FF49
REFERENCES,0.9333333333333333,"0
0.1
0.2
0.3
0.4
0.5
Transaction Cost Rate (%) 0 50 100 150 200 250 300 350 400 450 500 550"
REFERENCES,0.9347517730496454,Cumulative Wealth
REFERENCES,0.9361702127659575,"1/N
SPOLC
SSPO
S3
S2
S1
SSMP"
REFERENCES,0.9375886524822695,"MAXER
IPSRM-D
PLCP
SDP-relaxation
mSSRM-PGA(m=10)
mSSRM-PGA(m=15)
mSSRM-PGA(m=20)"
REFERENCES,0.9390070921985816,(e) FF100
REFERENCES,0.9404255319148936,"0
0.1
0.2
0.3
0.4
0.5
Transaction Cost Rate (%) 0 100 200 300 400 500 600 700"
REFERENCES,0.9418439716312057,Cumulative Wealth
REFERENCES,0.9432624113475178,"1/N
SPOLC
SSPO
S3
S2
S1
SSMP"
REFERENCES,0.9446808510638298,"MAXER
IPSRM-D
PLCP
SDP-relaxation
mSSRM-PGA(m=10)
mSSRM-PGA(m=15)
mSSRM-PGA(m=20)"
REFERENCES,0.9460992907801419,"(f) FF100MEINV
Figure 2: Final cumulative wealths of portfolio optimization methods w.r.t. transaction cost rate ŒΩ on
6 benchmark data sets."
REFERENCES,0.9475177304964539,NeurIPS Paper Checklist
CLAIMS,0.948936170212766,1. Claims
CLAIMS,0.950354609929078,"Question: Do the main claims made in the abstract and introduction accurately reÔ¨Çect the
paper‚Äôs contributions and scope?"
CLAIMS,0.9517730496453901,Answer: [Yes]
LIMITATIONS,0.9531914893617022,2. Limitations
LIMITATIONS,0.9546099290780142,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.9560283687943263,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.9574468085106383,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9588652482269504,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.9602836879432625,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9617021276595744,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9631205673758865,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9645390070921985,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9659574468085106,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9673758865248226,"Question: Does the paper provide open access to the data and code, with sufÔ¨Åcient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.9687943262411347,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9702127659574468,"JustiÔ¨Åcation: The whole project with all the codes for reproductions would be made public
if this paper were to be accepted."
OPEN ACCESS TO DATA AND CODE,0.9716312056737588,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9730496453900709,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.9744680851063829,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.975886524822695,7. Experiment Statistical SigniÔ¨Åcance
OPEN ACCESS TO DATA AND CODE,0.9773049645390071,"Question: Does the paper report error bars suitably and correctly deÔ¨Åned or other appropriate
information about the statistical signiÔ¨Åcance of the experiments?"
OPEN ACCESS TO DATA AND CODE,0.9787234042553191,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9801418439716312,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9815602836879432,"Question: For each experiment, does the paper provide sufÔ¨Åcient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9829787234042553,Answer: [Yes]
CODE OF ETHICS,0.9843971631205674,9. Code Of Ethics
CODE OF ETHICS,0.9858156028368794,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9872340425531915,Answer: [Yes]
BROADER IMPACTS,0.9886524822695035,10. Broader Impacts
BROADER IMPACTS,0.9900709219858156,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9914893617021276,Answer: [Yes]
SAFEGUARDS,0.9929078014184397,11. Safeguards
SAFEGUARDS,0.9943262411347518,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
JustiÔ¨Åcation: This work has no social risk of misuse.
12. Licenses for existing assets"
SAFEGUARDS,0.9957446808510638,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
13. New Assets"
SAFEGUARDS,0.9971631205673759,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
14. Crowdsourcing and Research with Human Subjects"
SAFEGUARDS,0.9985815602836879,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]"
