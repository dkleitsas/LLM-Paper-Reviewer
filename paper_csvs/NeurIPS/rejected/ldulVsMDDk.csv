Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0016501650165016502,"Modern advancements in large-scale machine learning would be impossible without
1"
ABSTRACT,0.0033003300330033004,"the paradigm of data-parallel distributed computing. Since distributed computing
2"
ABSTRACT,0.0049504950495049506,"with large-scale models imparts excessive pressure on communication channels, a
3"
ABSTRACT,0.006600660066006601,"lot of recent research was directed towards co-designing communication compres-
4"
ABSTRACT,0.00825082508250825,"sion strategies and training algorithms with the goal of reducing communication
5"
ABSTRACT,0.009900990099009901,"costs. While pure data parallelism allows better data scaling, it suffers from poor
6"
ABSTRACT,0.01155115511551155,"model scaling properties. Indeed, compute nodes are severely limited by memory
7"
ABSTRACT,0.013201320132013201,"constraints, preventing further increases in model size. For this reason, the latest
8"
ABSTRACT,0.01485148514851485,"achievements in training giant neural network models rely on some form of model
9"
ABSTRACT,0.0165016501650165,"parallelism as well. In this work, we take a closer theoretical look at Independent
10"
ABSTRACT,0.018151815181518153,"Subnetwork Training (IST), which is a recently proposed and highly effective
11"
ABSTRACT,0.019801980198019802,"technique for solving the aforementioned problems. We identify fundamental
12"
ABSTRACT,0.02145214521452145,"differences between IST and alternative approaches, such as distributed methods
13"
ABSTRACT,0.0231023102310231,"with compressed communication, and provide a precise analysis of its optimization
14"
ABSTRACT,0.024752475247524754,"performance on a quadratic model.
15"
INTRODUCTION,0.026402640264026403,"1
Introduction
16"
INTRODUCTION,0.028052805280528052,"A huge part of today’s machine learning success drives from the possibility to build more and more
17"
INTRODUCTION,0.0297029702970297,"complex models and train them on increasingly larger datasets. This fast progress has become
18"
INTRODUCTION,0.03135313531353135,"feasible due to advancements in distributed optimization, which is necessary for proper scaling
19"
INTRODUCTION,0.033003300330033,"when the training data sizes grow [50]. In a typical scenario data parallelism is used for efficiency
20"
INTRODUCTION,0.034653465346534656,"which consists of sharding the dataset across computing devices. This allowed very efficient scaling
21"
INTRODUCTION,0.036303630363036306,"and accelerating of training moderately sized models by using additional hardware [19]. Though,
22"
INTRODUCTION,0.037953795379537955,"such data parallel approach can suffer from communication bottleneck, which sparked a lot of
23"
INTRODUCTION,0.039603960396039604,"research on distributed optimization with compressed communication of the parameters between
24"
INTRODUCTION,0.041254125412541254,"nodes [3, 27, 38].
25"
THE NEED FOR MODEL PARALLEL,0.0429042904290429,"1.1
The need for model parallel
26"
THE NEED FOR MODEL PARALLEL,0.04455445544554455,"Despite the efficiency gains of data parallelism, it has some fundamental limitations when it comes to
27"
THE NEED FOR MODEL PARALLEL,0.0462046204620462,"scaling up the model size. As the model dimension grows, the amount of memory required to store
28"
THE NEED FOR MODEL PARALLEL,0.04785478547854786,"and update the parameters also increases, which becomes problematic due to resource constraints
29"
THE NEED FOR MODEL PARALLEL,0.04950495049504951,"on individual devices. This has led to the development of model parallelism [11, 37], which splits
30"
THE NEED FOR MODEL PARALLEL,0.05115511551155116,"a large model across multiple nodes, with each node responsible for computations of model parts
31"
THE NEED FOR MODEL PARALLEL,0.052805280528052806,"[15, 47]. However, naive model parallelism also poses challenges because each node can only update
32"
THE NEED FOR MODEL PARALLEL,0.054455445544554455,"its portion of the model based on the data it has access to. This creates a need for a very careful
33"
THE NEED FOR MODEL PARALLEL,0.056105610561056105,"management of communication between devices. Thus, a combination of both data and model
34"
THE NEED FOR MODEL PARALLEL,0.057755775577557754,"parallelism is often necessary to achieve efficient and scalable training of huge models.
35"
THE NEED FOR MODEL PARALLEL,0.0594059405940594,Algorithm 1 Distributed Submodel (Stochastic) Gradient Descent
THE NEED FOR MODEL PARALLEL,0.06105610561056106,"1: Parameters: learning rate γ > 0; sketches C1, . . . , Cn; initial model x0 ∈Rd"
THE NEED FOR MODEL PARALLEL,0.0627062706270627,"2: for k = 0, 1, 2 . . . do
3:
Select submodels wk
i = Ck
i xk for i ∈[n] and broadcast to all computing nodes
4:
for i = 1, . . . , n in parallel do
5:
Compute local (stochastic) gradient w.r.t. submodel: Ck
i ∇fi(wk
i )
6:
Take (maybe multiple) gradient descent step z+
i = wk
i −γCk
i ∇fi(wk
i )
7:
Send z+
i to the server
8:
end for
9:
Aggregate/merge received submodels: xk+1 = 1"
THE NEED FOR MODEL PARALLEL,0.06435643564356436,"n
Pn
i=1 z+
i
10: end for"
THE NEED FOR MODEL PARALLEL,0.066006600660066,"IST.
Independent Subnetwork Training (IST) is a technique which suggests dividing the neural
36"
THE NEED FOR MODEL PARALLEL,0.06765676567656766,"network into smaller independent sub-parts, training them in a distributed parallel fashion and then
37"
THE NEED FOR MODEL PARALLEL,0.06930693069306931,"aggregating the results to update the weights of the whole model. According to IST, every subnetwork
38"
THE NEED FOR MODEL PARALLEL,0.07095709570957096,"is operational on its own, has fewer parameters than the full model, and this not only reduces the load
39"
THE NEED FOR MODEL PARALLEL,0.07260726072607261,"on computing nodes but also results in faster synchronization. A generalized analog of the described
40"
THE NEED FOR MODEL PARALLEL,0.07425742574257425,"method is formalized as an iterative procedure in Algorithm 1. This paradigm was pioneered by
41"
THE NEED FOR MODEL PARALLEL,0.07590759075907591,"[45] for networks with fully-connected layers and was later extended to ResNets [14] and Graph
42"
THE NEED FOR MODEL PARALLEL,0.07755775577557755,"architectures [43]. Previous experimental studies have shown that IST is a very promising approach
43"
THE NEED FOR MODEL PARALLEL,0.07920792079207921,"for various applications as it allows to effectively combine data with model parallelism and train
44"
THE NEED FOR MODEL PARALLEL,0.08085808580858085,"larger models with limited compute. In addition, [28] performed theoretical analysis of IST for
45"
THE NEED FOR MODEL PARALLEL,0.08250825082508251,"overparameterized single hidden layer neural networks with ReLU activations. The idea of IST was
46"
THE NEED FOR MODEL PARALLEL,0.08415841584158416,"also recently extended to the federated setting via an asynchronous distributed dropout [13] technique.
47"
THE NEED FOR MODEL PARALLEL,0.0858085808580858,"Federated Learning.
Another important setting when the data is distributed (due to privacy reasons)
48"
THE NEED FOR MODEL PARALLEL,0.08745874587458746,"is Federated Learning [22, 27, 31]. In this scenario computing devices are often heterogeneous and
49"
THE NEED FOR MODEL PARALLEL,0.0891089108910891,"more resource-constrained [5] (e.g. mobile phones) in comparison to data-center setting. Such
50"
THE NEED FOR MODEL PARALLEL,0.09075907590759076,"challenges prompted extensive research efforts into selecting smaller and more efficient submodels
51"
THE NEED FOR MODEL PARALLEL,0.0924092409240924,"for local on-device training [2, 6, 8, 12, 20, 21, 29, 35, 42, 44]. Many of these works propose
52"
THE NEED FOR MODEL PARALLEL,0.09405940594059406,"approaches to adapt submodels, often tailored to specific neural network architectures, based on
53"
THE NEED FOR MODEL PARALLEL,0.09570957095709572,"the capabilities of individual clients for various machine learning tasks. However, there is a lack of
54"
THE NEED FOR MODEL PARALLEL,0.09735973597359736,"comprehension regarding the theoretical properties of these methods.
55"
SUMMARY OF CONTRIBUTIONS,0.09900990099009901,"1.2
Summary of contributions
56"
SUMMARY OF CONTRIBUTIONS,0.10066006600660066,"When reviewing the literature, we have found that a rigorous understanding of IST convergence
57"
SUMMARY OF CONTRIBUTIONS,0.10231023102310231,"virtually does not exist, which motivates our work. The main contributions of this paper include
58"
SUMMARY OF CONTRIBUTIONS,0.10396039603960396,"• A novel approach to analyzing distributed methods that combine data and model parallelism
59"
SUMMARY OF CONTRIBUTIONS,0.10561056105610561,"by operating with sparse submodels for a quadratic model.
60"
SUMMARY OF CONTRIBUTIONS,0.10726072607260725,"• The first analysis of independent subnetwork training in homogeneous and heterogeneous
61"
SUMMARY OF CONTRIBUTIONS,0.10891089108910891,"scenarios without restrictive assumptions on gradient estimators.
62"
SUMMARY OF CONTRIBUTIONS,0.11056105610561057,"• Identification of settings when IST can optimize very efficiently or converge not to the
63"
SUMMARY OF CONTRIBUTIONS,0.11221122112211221,"optimal solution but only to an irreducible neighborhood which is also tightly characterized.
64"
SUMMARY OF CONTRIBUTIONS,0.11386138613861387,"• Experimental validation of the proposed theory through carefully designed illustrative
65"
SUMMARY OF CONTRIBUTIONS,0.11551155115511551,"experiments. Due to space limitations, the results (and proofs) are provided in the Appendix.
66"
FORMALISM AND SETUP,0.11716171617161716,"2
Formalism and Setup
67"
FORMALISM AND SETUP,0.1188118811881188,"We consider the standard optimization formulation of distributed/federated learning problem [41],
68"
FORMALISM AND SETUP,0.12046204620462046,"min
x∈Rd """
FORMALISM AND SETUP,0.12211221122112212,"f(x) := 1 n n
X"
FORMALISM AND SETUP,0.12376237623762376,"i=1
fi(x) # ,
(1)"
FORMALISM AND SETUP,0.1254125412541254,"where n is the number of clients/workers, each fi : Rd →Rd represents the loss of the model
69"
FORMALISM AND SETUP,0.12706270627062707,"parameterized by vector x ∈Rd on the data of client i.
70"
FORMALISM AND SETUP,0.12871287128712872,"A typical Stochastic Gradient Descent (SGD) type method for solving this problem has the form
71"
FORMALISM AND SETUP,0.13036303630363036,"xk+1 = xk −γgk,
gk = 1 n nP"
FORMALISM AND SETUP,0.132013201320132,"i=1
gk
i ,
(2)"
FORMALISM AND SETUP,0.13366336633663367,"where γ > 0 is a stepsize and gk
i is a suitably constructed estimator of ∇fi(xk). In the distributed
72"
FORMALISM AND SETUP,0.1353135313531353,"setting, computation of gradient estimators gk
i is typically performed by clients, sent to the server,
73"
FORMALISM AND SETUP,0.13696369636963696,which subsequently performs aggregation via averaging gk = 1
FORMALISM AND SETUP,0.13861386138613863,"n
Pn
i=1 gk
i . The result is then used to
74"
FORMALISM AND SETUP,0.14026402640264027,"update the model xk+1 via a gradient-type method (2), and at the next iteration the model is broadcast
75"
FORMALISM AND SETUP,0.1419141914191419,"back to the clients. The process is repeated iteratively until a model of suitable qualities is found.
76"
FORMALISM AND SETUP,0.14356435643564355,"One of the main techniques used to accelerate distributed training is lossy communication compres-
77"
FORMALISM AND SETUP,0.14521452145214522,"sion [3, 27, 38]. It suggests applying a (possibly randomized) lossy compression mapping C to a
78"
FORMALISM AND SETUP,0.14686468646864687,"vector/matrix/tensor x before it is transmitted. This saves bits sent per every communication round
79"
FORMALISM AND SETUP,0.1485148514851485,"at the cost of transmitting a less accurate estimate C(x) of x. The error caused by this routine also
80"
FORMALISM AND SETUP,0.15016501650165018,"causes convergence issues, and to the best of our knowledge, convergence of IST-based techniques is
81"
FORMALISM AND SETUP,0.15181518151815182,"for this reason not yet understood.
82"
FORMALISM AND SETUP,0.15346534653465346,"Definition 1 (Unbiased compressor). A randomized mapping C : Rd →Rd is an unbiased compres-
83"
FORMALISM AND SETUP,0.1551155115511551,"sion operator (C ∈U(ω) for brevity) if for some ω ≥0 and ∀x ∈Rd
84"
FORMALISM AND SETUP,0.15676567656765678,"E [C(x)] = x,
E"
FORMALISM AND SETUP,0.15841584158415842,"
∥C(x) −x∥2
≤ω∥x∥2.
(3)"
FORMALISM AND SETUP,0.16006600660066006,"A notable example of a mapping from this class is the random sparsification (Rand-q for q ∈
85"
FORMALISM AND SETUP,0.1617161716171617,"{1, . . . , d}) operator defined by
86"
FORMALISM AND SETUP,0.16336633663366337,"CRand-q(x) := Cqx = d q
P"
FORMALISM AND SETUP,0.16501650165016502,"i∈S
eie⊤
i x,
(4)"
FORMALISM AND SETUP,0.16666666666666666,"where e1, . . . , ed ∈Rd are standard unit basis vectors in Rd, and S is a random subset of [d] :=
87"
FORMALISM AND SETUP,0.16831683168316833,"{1, . . . , d} sampled from the uniform distribution on the all subsets of [d] with cardinality q. Rand-q
88"
FORMALISM AND SETUP,0.16996699669966997,"belongs to U (d/q −1), which means that the more elements are “dropped” (lower q), the higher is
89"
FORMALISM AND SETUP,0.1716171617161716,"the variance ω of the compressor.
90"
FORMALISM AND SETUP,0.17326732673267325,"In this work, we are mainly interested in a somewhat more general class of operators than mere
91"
FORMALISM AND SETUP,0.17491749174917492,"sparsifiers. In particular, we are interested in compressing via the application of random matrices, i.e.,
92"
FORMALISM AND SETUP,0.17656765676567657,"via sketching. A sketch Ck
i ∈Rd×d can be used to represent submodel computations in the following
93"
FORMALISM AND SETUP,0.1782178217821782,"way:
94"
FORMALISM AND SETUP,0.17986798679867988,"gk
i := Ck
i ∇fi(Ck
i xk),
(5)
where we require Ck
i to be a symmetric positive semidefinite matrix. Such gradient estimate
95"
FORMALISM AND SETUP,0.18151815181518152,"corresponds to computing the local gradient with respect to a sparse submodel model Ck
i xk, and
96"
FORMALISM AND SETUP,0.18316831683168316,"additionally sketching the resulting gradient with the same matrix Ck
i to guarantee that the resulting
97"
FORMALISM AND SETUP,0.1848184818481848,"update lies in the lower-dimensional subspace.
98"
FORMALISM AND SETUP,0.18646864686468648,"Using this notion, Algorithm 1 (with one local gradient step) can be represented in the following form
99"
FORMALISM AND SETUP,0.18811881188118812,xk+1 = 1 n nP i=1
FORMALISM AND SETUP,0.18976897689768976,"
Ck
i xk −γCk
i ∇fi(Ck
i xk)

,
(6)"
FORMALISM AND SETUP,0.19141914191419143,which is equivalent to the SGD-type update (2) when perfect reconstruction property holds
FORMALISM AND SETUP,0.19306930693069307,Ck := 1 n nP
FORMALISM AND SETUP,0.19471947194719472,"i=1
Ck
i = I,"
FORMALISM AND SETUP,0.19636963696369636,"where I is the identity matrix (with probability one). This property holds for a specific class of
100"
FORMALISM AND SETUP,0.19801980198019803,"compressors that are particularly useful for capturing the concept of an independent subnetwork
101"
FORMALISM AND SETUP,0.19966996699669967,"partition.
102"
FORMALISM AND SETUP,0.20132013201320131,"Definition 2 (Permutation sketch). Assume that model size is greater than number of clients d ≥n
103"
FORMALISM AND SETUP,0.20297029702970298,"and d = qn, where q ≥1 is an integer1. Let π = (π1, . . . , πd) be a random permutation of [d]. Then
104"
FORMALISM AND SETUP,0.20462046204620463,"for all x ∈Rd and each i ∈[n] we define Perm-q operator
105"
FORMALISM AND SETUP,0.20627062706270627,"Ci := n ·
qi
P"
FORMALISM AND SETUP,0.2079207920792079,"j=q(i−1)+1
eπje⊤
πj.
(7)"
FORMALISM AND SETUP,0.20957095709570958,"1While this condition may look restrictive it naturally holds for distributed learning in a data-center setting.
For other scenarios [40] generalized it for n ≥d and block permutation case."
FORMALISM AND SETUP,0.21122112211221122,"Perm-q is unbiased and can be conveniently used for representing (non-overlapping) structured
106"
FORMALISM AND SETUP,0.21287128712871287,"decomposition of the model such that every client i is responsible for computations over a submodel
107"
FORMALISM AND SETUP,0.2145214521452145,"Cixk.
108"
FORMALISM AND SETUP,0.21617161716171618,"Our convergence analysis relies on assumption previously used for coordinate descent type methods.
109"
FORMALISM AND SETUP,0.21782178217821782,"Assumption 1 (Matrix smoothness). A differentiable function f : Rd →R is L-smooth, if there
110"
FORMALISM AND SETUP,0.21947194719471946,"exists a positive semi-definite matrix L ∈Rd×d such that
111"
FORMALISM AND SETUP,0.22112211221122113,"f(x + h) ≤f(x) + ⟨∇f(x), h⟩+ 1"
FORMALISM AND SETUP,0.22277227722772278,"2 ⟨Lh, h⟩,
∀x, h ∈Rd.
(8)"
FORMALISM AND SETUP,0.22442244224422442,"Standard L-smoothness condition is obtained as a special case of (8) for L = L · I.
112"
ISSUES WITH EXISTING APPROACHES,0.22607260726072606,"2.1
Issues with existing approaches
113"
ISSUES WITH EXISTING APPROACHES,0.22772277227722773,"Consider the simplest gradient type method with compressed model in the single node setting
114"
ISSUES WITH EXISTING APPROACHES,0.22937293729372937,"xk+1 = xk −γ∇f(C(xk)).
(9)"
ISSUES WITH EXISTING APPROACHES,0.23102310231023102,"Algorithms belonging to this family require a different analysis in comparison to SGD [16, 18],
115"
ISSUES WITH EXISTING APPROACHES,0.23267326732673269,"Distributed Compressed Gradient Descent [3, 26] and Randomized Coordinate Descent [34, 36] type
116"
ISSUES WITH EXISTING APPROACHES,0.23432343234323433,"methods because the gradient estimator is no longer unbiased
117"
ISSUES WITH EXISTING APPROACHES,0.23597359735973597,"E [∇f(C(x))] ̸= ∇f(x) = E [C(∇f(x))] .
(10)"
ISSUES WITH EXISTING APPROACHES,0.2376237623762376,"That is why such kind of algorithms are harder to analyze. So, prior results for unbiased SGD [25]
118"
ISSUES WITH EXISTING APPROACHES,0.23927392739273928,"can not be directly reused. Furthermore, the nature of the bias in this type of gradient estimator does
119"
ISSUES WITH EXISTING APPROACHES,0.24092409240924093,"not exhibit additive (zero-mean) noise, thereby preventing the application of previous analyses for
120"
ISSUES WITH EXISTING APPROACHES,0.24257425742574257,"biased SGD [1].
121"
ISSUES WITH EXISTING APPROACHES,0.24422442244224424,"An assumption like bounded stochastic gradient norm extensively used in previous works [30, 48]
122"
ISSUES WITH EXISTING APPROACHES,0.24587458745874588,"hinders an accurate understanding of such methods. This assumption hides the fundamental difficulty
123"
ISSUES WITH EXISTING APPROACHES,0.24752475247524752,"of analyzing biased gradient estimator:
124 E"
ISSUES WITH EXISTING APPROACHES,0.24917491749174916,"h
∥∇f(C(x))∥2i
≤G
(11)"
ISSUES WITH EXISTING APPROACHES,0.2508250825082508,"and may not hold even for quadratic functions f(x) = x⊤Ax. In addition, in the distributed
125"
ISSUES WITH EXISTING APPROACHES,0.2524752475247525,"setting such condition can result in vacuous bounds [23] as it does not allow to accurately capture
126"
ISSUES WITH EXISTING APPROACHES,0.25412541254125415,"heterogeneity.
127"
RESULTS IN THE INTERPOLATION CASE,0.25577557755775576,"3
Results in the Interpolation Case
128"
RESULTS IN THE INTERPOLATION CASE,0.25742574257425743,"To conduct a thorough theoretical analysis of methods that combine data with model parallelism,
129"
RESULTS IN THE INTERPOLATION CASE,0.2590759075907591,"we simplify the algorithm and problem setting to isolate the unique effects of this approach. The
130"
RESULTS IN THE INTERPOLATION CASE,0.2607260726072607,"following considerations are made:
131"
RESULTS IN THE INTERPOLATION CASE,0.2623762376237624,"(1) We assume that every node i computes the true gradient at the submodel Ci∇fi(Cixk).
132"
RESULTS IN THE INTERPOLATION CASE,0.264026402640264,"(2) A notable difference from the original IST algorithm 1 is that workers perform single
133"
RESULTS IN THE INTERPOLATION CASE,0.26567656765676567,"gradient descent step (or just gradient computation).
134"
RESULTS IN THE INTERPOLATION CASE,0.26732673267326734,"(3) Finally, we consider a special case of quadratic model (12) as a loss function (1).
135"
RESULTS IN THE INTERPOLATION CASE,0.26897689768976896,"Condition (1) is mainly for the sake of simplicity and clarity of exposition and can be potentially
136"
RESULTS IN THE INTERPOLATION CASE,0.2706270627062706,"generalized to stochastic gradient computations. (2) is imposed because local steps did not bring
137"
RESULTS IN THE INTERPOLATION CASE,0.2722772277227723,"any theoretical efficiency improvements for heterogeneous settings until very recently [32]. And
138"
RESULTS IN THE INTERPOLATION CASE,0.2739273927392739,"even then, only with the introduction of additional control variables, which goes against resource-
139"
RESULTS IN THE INTERPOLATION CASE,0.2755775577557756,"constrained device setting. The reason behind (3) is that despite the seeming simplicity quadratic
140"
RESULTS IN THE INTERPOLATION CASE,0.27722772277227725,"problem has been used extensively to study properties of neural networks [46, 49]. Moreover, it is a
141"
RESULTS IN THE INTERPOLATION CASE,0.27887788778877887,"non-trivial model which allows to understand complex optimization algorithms [4, 10, 17]. It serves
142"
RESULTS IN THE INTERPOLATION CASE,0.28052805280528054,"as a suitable problem for observing complex phenomena and providing theoretical insights, which
143"
RESULTS IN THE INTERPOLATION CASE,0.28217821782178215,"can also be observed in practical scenarios.
144"
RESULTS IN THE INTERPOLATION CASE,0.2838283828382838,"Having said that we consider a special case of problem (1)
145"
RESULTS IN THE INTERPOLATION CASE,0.2854785478547855,f(x) = 1 n nP
RESULTS IN THE INTERPOLATION CASE,0.2871287128712871,"i=1
fi(x),
fi(x) ≡1"
RESULTS IN THE INTERPOLATION CASE,0.2887788778877888,"2x⊤Lix −b⊤
i x.
(12)"
RESULTS IN THE INTERPOLATION CASE,0.29042904290429045,"In this case, f(x) is L-smooth, and ∇f(x) = L x −b, where L = 1"
RESULTS IN THE INTERPOLATION CASE,0.29207920792079206,"n
Pn
i=1 Li and b := 1"
RESULTS IN THE INTERPOLATION CASE,0.29372937293729373,"n
Pn
i=1 bi.
146"
RESULTS IN THE INTERPOLATION CASE,0.2953795379537954,"3.1
No linear term: problems and solutions
147"
RESULTS IN THE INTERPOLATION CASE,0.297029702970297,"First, let us examine the case of bi ≡0, which we call interpolation for quadratics, and perform the
148"
RESULTS IN THE INTERPOLATION CASE,0.2986798679867987,"analysis for general sketches Ck
i . In this case the gradient estimator (2) takes the form
149"
RESULTS IN THE INTERPOLATION CASE,0.30033003300330036,gk = 1 n nP
RESULTS IN THE INTERPOLATION CASE,0.30198019801980197,"i=1
Ck
i ∇fi(Ck
i xk) = 1 n nP"
RESULTS IN THE INTERPOLATION CASE,0.30363036303630364,"i=1
Ck
i LiCk
i xk = B
k xk
(13)"
RESULTS IN THE INTERPOLATION CASE,0.30528052805280526,"where B
k := 1"
RESULTS IN THE INTERPOLATION CASE,0.3069306930693069,"n
Pn
i=1 Ck
i LiCk
i . We prove the following result for a method with such an estimator.
150"
RESULTS IN THE INTERPOLATION CASE,0.3085808580858086,"Theorem 1. Consider the method (2) with estimator (13) for a quadratic problem (12) with L ≻0
151"
RESULTS IN THE INTERPOLATION CASE,0.3102310231023102,and bi ≡0. Then if W := 1
E,0.3118811881188119,2E h
E,0.31353135313531355,"L B
k + B
k L
i
⪰0 and there exists constant θ > 0:
152 E h"
E,0.31518151815181517,"B
k L B
ki
⪯θ W,
(14)"
E,0.31683168316831684,and the step size is chosen as 0 < γ ≤1
E,0.3184818481848185,"θ, the iterates satisfy
153"
K,0.3201320132013201,"1
K K−1
P k=0 E"
K,0.3217821782178218,"h∇f(xk)
2"
K,0.3234323432343234,"L
−1 W L
−1
i
≤
2(f(x0)−E[f(xK)])"
K,0.3250825082508251,"γK
,
(15)"
K,0.32673267326732675,"and
154 E"
K,0.32838283828382836,"h
∥xk −x⋆∥2 L"
K,0.33003300330033003,"i
≤

1 −γλmin
 L
−1"
W L,0.3316831683168317,"2 W L
−1"
W L,0.3333333333333333,"2 k
∥x0 −x⋆∥2"
W L,0.334983498349835,"L.
(16)"
W L,0.33663366336633666,"This theorem establishes an O(1/K) convergence rate with constant step size up to a stationary point
155"
W L,0.33828382838283827,"and linear convergence for the expected distance to the optimum. Note that we employ weighted
156"
W L,0.33993399339933994,"norms in our analysis, as the considered class of loss functions satisfies the matrix L-smoothness
157"
W L,0.3415841584158416,"Assumption 1. The use of standard Euclidean distance may result in loose bounds that do not recover
158"
W L,0.3432343234323432,"correct rates for special cases like Gradient Descent.
159"
W L,0.3448844884488449,"It is important to highlight that inequality (14) may not hold (for any θ > 0) in the general case
160"
W L,0.3465346534653465,"as the matrix W is not guaranteed to be positive (semi-)definite in the case of general sampling.
161"
W L,0.3481848184818482,"The intuition behind it is that arbitrary sketches Ck
i can result in gradient estimator gk, which is
162"
W L,0.34983498349834985,"misaligned with the true gradient ∇f(xk). Specifically, the inner product

∇f(xk), gk
can be
163"
W L,0.35148514851485146,"negative, and there is no expected descent after one step.
164"
W L,0.35313531353135313,"Next, we give examples of samplings for which the inequality (14) can be satisfied.
165"
W L,0.3547854785478548,"1. Identity. Consider Ci ≡I. Then B
k = L, B
k L B
k = L
3, W = L
2 ≻0 and hence (14) is
166"
W L,0.3564356435643564,"satisfied for θ = λmax(L). So, (15) says that if we choose γ = 1"
W L,0.3580858085808581,"θ, then
167"
K,0.35973597359735976,"1
K K−1
P k=0"
K,0.3613861386138614,"∇f(xk)
2
I ≤
2λmax(L)(f(x0)−f(xK)) K
,"
K,0.36303630363036304,"which exactly matches the rate of Gradient Descent in the non-convex setting. As for iterates
168"
K,0.36468646864686466,"convergence, the rate in (16) is λmax(L)/λmin(L) corresponding to precise Gradient Descent result for
169"
K,0.36633663366336633,"strongly convex functions.
170"
K,0.367986798679868,"2. Permutation. Assume n = d2 and the use of Perm-1 (special case of Definition 2) sketch
Ck
i = neπk
i e⊤
πk
i , where πk = (πk
1, . . . , πk
n) is a random permutation of [n]. Then E h"
K,0.3696369636963696,"B
ki
= 1 n nP"
K,0.3712871287128713,"i=1
n2E"
K,0.37293729372937295,"
Ck
i LiCk
i

= 1 n nP"
K,0.37458745874587457,"i=1
nDiag(Li) = Pn
i=1 Di = n D,"
K,0.37623762376237624,"2This is done mainly for simplifying the presentation. Results can be generalized to the case of n ̸= d in the
similar way as done in [40] which can be found in the Appendix."
K,0.3778877887788779,where D := 1
K,0.3795379537953795,"n
Pn
i=1 Di, Di := Diag(Li). Then inequality (14) leads to
171"
K,0.3811881188118812,"n D L D ⪯θ 2
 "
K,0.38283828382838286,"L D + D L

,
(17)"
K,0.3844884488448845,"which may not always hold as L D + D L is not guaranteed to be positive definite even in case of
172"
K,0.38613861386138615,"L ≻0. However, such kind of condition can be enforced via a slight modification of permutation
173"
K,0.38778877887788776,"sketches { ˜Ci}n
i=1, which is done in Section 3.1.2. The limitation of such an approach is that
174"
K,0.38943894389438943,"compressors ˜Ci become no longer unbiased.
175"
K,0.3910891089108911,"Remark 1. Matrix W in case of permutation sketches may not be positive-definite. Consider the
176"
K,0.3927392739273927,"following homogeneous (Li ≡L) two-dimensional problem example
177"
K,0.3943894389438944,"L =

a
c
c
b"
K,0.39603960396039606,"
.
(18)"
K,0.3976897689768977,"Then
178 W = 1 2
"
K,0.39933993399339934,"L D + D L

=

a2
c(a + b)/2
c(a + b)/2
b2"
K,0.400990099009901,"
,
(19)"
K,0.40264026402640263,which for c > 2ab
K,0.4042904290429043,"a+b has det(W) < 0, and thus W ⊁0 according to Sylvester’s criterion.
179"
K,0.40594059405940597,"Next, we focus on the particular case of Permutation sketches, which are the most suitable for
180"
K,0.4075907590759076,"model partitioning according to Independent Subnetwork Training (IST). At the rest of the section,
181"
K,0.40924092409240925,"we discuss how the condition (14) can be enforced via a specially designed preconditioning of the
182"
K,0.41089108910891087,"problem (12) or modification of sketch mechanism (7).
183"
HOMOGENEOUS PROBLEM PRECONDITIONING,0.41254125412541254,"3.1.1
Homogeneous problem preconditioning
184"
HOMOGENEOUS PROBLEM PRECONDITIONING,0.4141914191419142,To start consider a homogeneous setting fi(x) = 1
HOMOGENEOUS PROBLEM PRECONDITIONING,0.4158415841584158,"2x⊤Lx, so Li ≡L. Now define D = Diag(L) –
185"
HOMOGENEOUS PROBLEM PRECONDITIONING,0.4174917491749175,"diagonal matrix with elements equal to diagonal of L. Then problem can be converted to
186"
HOMOGENEOUS PROBLEM PRECONDITIONING,0.41914191419141916,fi(D−1
HOMOGENEOUS PROBLEM PRECONDITIONING,0.4207920792079208,2 x) = 1
HOMOGENEOUS PROBLEM PRECONDITIONING,0.42244224422442245,"2

D−1"
X,0.4240924092409241,"2 x
⊤
L

D−1"
X,0.42574257425742573,"2 x

= 1"
X,0.4273927392739274,"2x⊤
D−1"
X,0.429042904290429,"2 LD−1 2
"
X,0.4306930693069307,"|
{z
}
˜L"
X,0.43234323432343236,"x,
(20)"
X,0.43399339933993397,which is equivalent to the original problem after a change of variables ˜x := D−1
X,0.43564356435643564,"2 x. Note that
187"
X,0.4372937293729373,"D = Diag(L) is positive definite as L ≻0, and therefore ˜L ≻0. Moreover, the preconditioned
188"
X,0.4389438943894389,"matrix ˜L has all ones on the diagonal: Diag(˜L) = I. If we now combine it with Perm-1 sketches
189 E h"
X,0.4405940594059406,"B
ki
= E"
X,0.44224422442244227,"h
1
n
Pn
i=1 Ci ˜L Ci
i
= nDiag(˜L) = nI."
X,0.4438943894389439,"Therefore, inequality (14) takes the form ˜
W = n ˜L ⪰1"
X,0.44554455445544555,"θn2 ˜L, which holds for θ ≥n, and left hand
190"
X,0.4471947194719472,"side of (15) can be transformed the following way
191"
X,0.44884488448844884,"∇f(xk)
2
˜L
−1 ˜
W ˜L
−1 ≥nλmin

˜L
−1 ∇f(xk)
2"
X,0.4504950495049505,"I = nλmax(˜L)
∇f(xk)
2"
X,0.4521452145214521,"I
(21)"
X,0.4537953795379538,"for an accurate comparison to standard methods. The resulting convergence guarantee
192"
K,0.45544554455445546,"1
K K−1
P k=0 E"
K,0.4570957095709571,"h∇f(xk)
2
I"
K,0.45874587458745875,"i
≤
2λmax(˜L)(f(x0)−E[f(xK)])"
K,0.4603960396039604,"K
,
(22)"
K,0.46204620462046203,"which matches classical Gradient Descent.
193"
HETEROGENEOUS SKETCH PRECONDITIONING,0.4636963696369637,"3.1.2
Heterogeneous sketch preconditioning
194"
HETEROGENEOUS SKETCH PRECONDITIONING,0.46534653465346537,In contrast to homogeneous case the heterogeneous problem fi(x) = 1
HETEROGENEOUS SKETCH PRECONDITIONING,0.466996699669967,"2x⊤Lix can not be so easily
195"
HETEROGENEOUS SKETCH PRECONDITIONING,0.46864686468646866,preconditioned by a simple change of variables ˜x := D−1
HETEROGENEOUS SKETCH PRECONDITIONING,0.47029702970297027,"2 x, as every client i has its own matrix
196"
HETEROGENEOUS SKETCH PRECONDITIONING,0.47194719471947194,"Li. However, this problem can be fixed via the following modification of Perm-1, which scales the
197"
HETEROGENEOUS SKETCH PRECONDITIONING,0.4735973597359736,"output according to the diagonal elements of local smoothness matrix Li:
198"
HETEROGENEOUS SKETCH PRECONDITIONING,0.4752475247524752,"˜Ci := √n
h
L
−1"
"I
I",0.4768976897689769,"2
i
i"
"I
I",0.47854785478547857,"πi,πi
eπie⊤
πi.
(23)"
"I
I",0.4801980198019802,In this case E
"I
I",0.48184818481848185,"h
˜CiLi ˜Ci
i
= I, E h"
"I
I",0.4834983498349835,"B
ki
= I, and W = L. Then inequality (14) is satisfied for θ ≥1.
199"
"I
I",0.48514851485148514,"If one plugs these results into (15), such convergence guarantee can be obtained
200"
K,0.4867986798679868,"1
K K−1
P k=0 E"
K,0.4884488448844885,"h∇f(xk)
2
I"
K,0.4900990099009901,"i
≤
2λmax(L)(f(x0)−E[f(xK)])"
K,0.49174917491749176,"K
,
(24)"
K,0.4933993399339934,"which matches the Gradient Descent result as well. Thus we can conclude that heterogeneity does not
201"
K,0.49504950495049505,"bring such a fundamental challenge in this scenario. In addition, a method with Perm-1 is significantly
202"
K,0.4966996699669967,"better in terms of computational and communication complexity as it requires calculating the local
203"
K,0.49834983498349833,"gradients with respect to much smaller submodels and transmits only sparse updates.
204"
K,0.5,"This construction also shows that for γ = 1/θ = 1
205"
K,0.5016501650165016,"γλmin
 L
−1"
W L,0.5033003300330033,"2 W L
−1"
W L,0.504950495049505,"2 
= λmin
 L
−1"
L L,0.5066006600660066,"2 L L
−1"
L L,0.5082508250825083,"2 
= 1,
(25)"
L L,0.5099009900990099,"which after plugging into the bound for the iterates (16) shows that the method basically converges in
206"
L L,0.5115511551155115,"1 iteration. This observation that sketch preconditioning can be extremely efficient, although it uses
207"
L L,0.5132013201320133,"only the diagonal elements of matrices Li.
208"
L L,0.5148514851485149,"Now when we understand that the method can perform very well in the special case of ˜bi ≡0 we can
209"
L L,0.5165016501650165,"move on to a more complicated situation.
210"
IRREDUCIBLE BIAS IN THE GENERAL CASE,0.5181518151815182,"4
Irreducible Bias in the General Case
211"
IRREDUCIBLE BIAS IN THE GENERAL CASE,0.5198019801980198,"Now we look at the most general heterogeneous case with different matrices and linear terms
212"
IRREDUCIBLE BIAS IN THE GENERAL CASE,0.5214521452145214,fi(x) ≡1
IRREDUCIBLE BIAS IN THE GENERAL CASE,0.523102310231023,"2x⊤Lix −x⊤bi . In this instance gradient estimator (2) takes the form
213"
IRREDUCIBLE BIAS IN THE GENERAL CASE,0.5247524752475248,gk = 1 n nP
IRREDUCIBLE BIAS IN THE GENERAL CASE,0.5264026402640264,"i=1
Ck
i ∇fi(Ck
i xk) = 1 n nP"
IRREDUCIBLE BIAS IN THE GENERAL CASE,0.528052805280528,"i=1
Ck
i
 
LiCk
i xk −bi

= B
k xk −Cb,
(26)"
IRREDUCIBLE BIAS IN THE GENERAL CASE,0.5297029702970297,where Cb = 1
IRREDUCIBLE BIAS IN THE GENERAL CASE,0.5313531353135313,"n
Pn
i=1 Ck
i bi. Herewith let us use a heterogeneous permutation sketch preconditioner
214"
IRREDUCIBLE BIAS IN THE GENERAL CASE,0.533003300330033,(23) like in Section 3.1.2 Then E h
IRREDUCIBLE BIAS IN THE GENERAL CASE,0.5346534653465347,"B
ki
= I and E "
IRREDUCIBLE BIAS IN THE GENERAL CASE,0.5363036303630363,"Cb

=
1
√n g
D b, where g
D b := 1"
IRREDUCIBLE BIAS IN THE GENERAL CASE,0.5379537953795379,"n
Pn
i=1 D
−1"
I,0.5396039603960396,"2
i
bi.
215"
I,0.5412541254125413,Furthermore expected gradient estimator (26) results in E
I,0.5429042904290429,"
gk
= xk−1
√n g
D b and can be transformed
216"
I,0.5445544554455446,"the following way
217 E"
I,0.5462046204620462,"
gk
= L
−1 L xk ± L
−1 b −1
√n g
D b = L
−1 ∇f(xk) + L
−1 b −1
√n
g
D b
|
{z
}
h"
I,0.5478547854785478,",
(27)"
I,0.5495049504950495,"which reflects the decomposition of the estimator into optimally preconditioned true gradient and a
218"
I,0.5511551155115512,"bias, depending on the linear terms bi.
219"
BIAS OF THE METHOD,0.5528052805280528,"4.1
Bias of the method
220"
BIAS OF THE METHOD,0.5544554455445545,"Estimator (27) can be directly plugged (with proper conditioning) into general SGD update (2)
221 E"
BIAS OF THE METHOD,0.5561056105610561,"
xk+1
= xk −γE"
BIAS OF THE METHOD,0.5577557755775577,"
gk
= (1 −γ)xk +
γ
√n g
D b = (1 −γ)k+1 x0 +
γ
√n g
D b
kP"
BIAS OF THE METHOD,0.5594059405940595,"j=0
(1 −γ)j. (28)"
BIAS OF THE METHOD,0.5610561056105611,"The resulting recursion (28) is exact, and its asymptotic limit can be analyzed. Thus for constant
222"
BIAS OF THE METHOD,0.5627062706270627,"γ < 1 by using the formula for the sum of the first k terms of a geometric series, one gets
223 E"
BIAS OF THE METHOD,0.5643564356435643,"
xk
= (1 −γ)k x0 + 1−(1−γ)k"
BIAS OF THE METHOD,0.566006600660066,"√n
g
D b −→
k→∞
1
√n g
D b,"
BIAS OF THE METHOD,0.5676567656765676,"which shows that in the limit, the first initialization term (with x0) vanishes while the second converges
to
1
√n g
D b. This reasoning shows that the method does not converge to the exact solution"
BIAS OF THE METHOD,0.5693069306930693,"xk →x∞̸= x⋆∈arg min
x∈Rd  1"
BIAS OF THE METHOD,0.570957095709571,"2x⊤L x −x⊤b
	
,"
BIAS OF THE METHOD,0.5726072607260726,"which for the positive-definite L can be defined as x⋆= L
−1 b, while x∞=
1
n√n
Pn
i=1 D
−1"
I,0.5742574257425742,"2
i
bi. So,
224"
I,0.5759075907590759,"in general, there is an unavoidable bias. However, in the limit case: n = d →∞, the bias diminishes.
225"
GENERIC CONVERGENCE ANALYSIS,0.5775577557755776,"4.2
Generic convergence analysis
226"
GENERIC CONVERGENCE ANALYSIS,0.5792079207920792,"While the analysis in Section 4.1 is precise, it does not allow us to compare the convergence of IST
227"
GENERIC CONVERGENCE ANALYSIS,0.5808580858085809,"to standard optimization methods. Due to this, we also analyze the non-asymptotic behavior of the
228"
GENERIC CONVERGENCE ANALYSIS,0.5825082508250825,"method to understand the convergence speed. Our result is formalized in the following theorem.
229"
GENERIC CONVERGENCE ANALYSIS,0.5841584158415841,"Theorem 2. Consider the method (2) with estimator (26) for a quadratic problem (12) with the
230"
GENERIC CONVERGENCE ANALYSIS,0.5858085808580858,"positive definite matrix L ≻0. Assume that for every Di := Diag(Li) matrices D
−1"
I,0.5874587458745875,"2
i
exist, scaled
231"
I,0.5891089108910891,permutation sketches (23) are used and heterogeneity is bounded as E
I,0.5907590759075908,"hgk −E"
I,0.5924092409240924,"
gk2 L"
I,0.594059405940594,"i
≤σ2.
232"
I,0.5957095709570958,"Then for step size is chosen as
233"
I,0.5973597359735974,"0 < γ ≤γc,β := 1/2−β"
I,0.599009900990099,"β+1/2,
(29)"
I,0.6006600660066007,"where γc,β ∈(0, 1] for β ∈(0, 1/2), the iterates satisfy
234"
K,0.6023102310231023,"1
K K−1
P k=0 E"
K,0.6039603960396039,"h∇f(xk)
2"
K,0.6056105610561056,"L
−1
i
≤
2(f(x0)−E[f(xK)])"
K,0.6072607260726073,"γK
+
 
2β−1 (1 −γ) + γ

∥h∥2"
K,0.6089108910891089,"L + γσ2,
(30)"
K,0.6105610561056105,where L = 1
K,0.6122112211221122,"n
Pn
i=1 Li, h = L
−1 b −1
√n
1
n
Pn
i=1 D
−1"
I,0.6138613861386139,"2
i
bi and b = 1"
I,0.6155115511551155,"n
Pn
i=1 bi.
235"
I,0.6171617161716172,"Note that the derived convergence upper bound has a neighborhood proportional to the bias of
236"
I,0.6188118811881188,"the gradient estimator h and level of heterogeneity σ2. Some of these terms with factor γ can be
237"
I,0.6204620462046204,"eliminated via decreasing learning rate schedule (e.g., ∼1/
√"
I,0.6221122112211221,"k). However, such a strategy does not
238"
I,0.6237623762376238,"diminish the term with a multiplier 2β−1 (1 −γ), making the neighborhood irreducible. Moreover,
239"
I,0.6254125412541254,"this term can be eliminated for γ = 1, which also minimizes the first term that decreases as 1/K.
240"
I,0.6270627062706271,"Though, such step size choice maximizes the terms with factor γ. Furthermore, there exists an
241"
I,0.6287128712871287,"inherent trade-off between convergence speed and the size of the neighborhood.
242"
I,0.6303630363036303,"In addition, convergence to the stationary point is measured in the weighted by L
−1 squared norm of
243"
I,0.6320132013201321,"the gradient. At the same time, the neighborhood term depends on the weighted by L norm of h. This
244"
I,0.6336633663366337,"fine-grained decoupling is achieved by carefully applying Fenchel-Young inequality and provides a
245"
I,0.6353135313531353,"tighter characterization of the convergence compared to using standard Euclidean distances.
246"
I,0.636963696369637,"Homogeneous case.
In this scenario, every worker has access to the all data fi(x) ≡1"
I,0.6386138613861386,"2x⊤Lx−x⊤b.
247"
I,0.6402640264026402,"Then diagonal preconditioning of the problem can be used as in the previous Section 3.1.1. This
248"
I,0.641914191419142,results in a gradient ∇f(x) = ˜L x−˜b for ˜L = D−1
I,0.6435643564356436,2 LD−1
I,0.6452145214521452,2 and ˜b = D−1
I,0.6468646864686468,"2 b. If it is further combined
249"
I,0.6485148514851485,"with a scaled by 1/√n Permutation sketch Ci := √neπie⊤
πi, the resulting gradient estimator is
250"
I,0.6501650165016502,"gk = xk −
1
√n ˜b = ˜L
−1 ∇f(xk) + ˜h,
(31)"
I,0.6518151815181518,"for ˜h = ˜L
−1 ˜b −1
√n ˜b. In this case heterogeneity term σ2 from upper bound (30) disappears
251 as E"
I,0.6534653465346535,"hgk −E"
I,0.6551155115511551,"
gk2 L"
I,0.6567656765676567,"i
= 0, thus the neighborhood size can significantly decrease. However,
252"
I,0.6584158415841584,"the bias term depending on ˜h still remains as the method does not converge to the exact solution
253"
I,0.6600660066006601,"xk →x∞̸= x⋆= ˜L
−1 ˜b for positive-definite ˜L. Nevertheless the method’s fixed point x∞= ˜b /√n
254"
I,0.6617161716171617,"and solution x⋆can coincide when ˜L
−1 ˜b =
1
√n ˜b, which means that ˜b is the right eigenvector of
255"
I,0.6633663366336634,"matrix ˜L
−1 with eigenvalue
1
√n.
256"
I,0.665016501650165,"Let us contrast obtained result (30) with non-convex rate of SGD [25] with constant step size γ for
257"
I,0.6666666666666666,"L-smooth and lower-bounded f
258"
I,0.6683168316831684,"min
k∈{0,...,K−1}"
I,0.66996699669967,"∇f(xk)
2 ≤
6(f(x0)−inf f)"
I,0.6716171617161716,"γK
+ γLC,
(32)"
I,0.6732673267326733,"where constant C depends, for example, on the variance of stochastic gradient estimates. Observe
259"
I,0.6749174917491749,"that the first term in the compared upper bounds (32) and (30) is almost identical and decreases with
260"
I,0.6765676567656765,"speed 1/K. But unlike (30) the neighborhood for SGD can be completely eliminated by reducing the
261"
I,0.6782178217821783,"step size γ. This highlights a fundamental difference of our results to unbiased methods.
262"
I,0.6798679867986799,"The intuition behind this issue is that for SGD-type methods like Compressed Gradient Descent
263"
I,0.6815181518151815,"xk+1 = xk −C(∇f(xk))
(33)"
I,0.6831683168316832,"the gradient estimate is unbiased and enjoys the property that variance
264 E"
I,0.6848184818481848,"
∥C(∇f(xk)) −∇f(xk)∥2
≤ω∥∇f(xk)∥2
(34)"
I,0.6864686468646864,"goes down to zero as the method progresses because ∇f(xk) →∇f(x⋆) = 0 in the unconstrained
265"
I,0.6881188118811881,"case. In addition, any stationary point x⋆ceases to be a fixed point of the iterative procedure as
266"
I,0.6897689768976898,"x⋆̸= x⋆−∇f(C(x⋆)),
(35)"
I,0.6914191419141914,"in the general case, unlike for Compressed Gradient Descent with both biased and unbiased compres-
267"
I,0.693069306930693,"sors C. So, even if the method (computing gradient at sparse model) is initialized from the solution
268"
I,0.6947194719471947,"after one gradient step, it may get away from there.
269"
COMPARISON TO PREVIOUS WORKS,0.6963696369636964,"4.3
Comparison to previous works
270"
COMPARISON TO PREVIOUS WORKS,0.698019801980198,"Independent Subnetwork Training [45].
There are several improvements over the previous works
271"
COMPARISON TO PREVIOUS WORKS,0.6996699669966997,"that tried to theoretically analyze the convergence of Distributed IST.
272"
COMPARISON TO PREVIOUS WORKS,0.7013201320132013,"The first difference is that our results allow for an almost arbitrary level of model sparsification,
273"
COMPARISON TO PREVIOUS WORKS,0.7029702970297029,"i.e., work for any ω ≥0 as permutation sketches can be viewed as a special case of compression
274"
COMPARISON TO PREVIOUS WORKS,0.7046204620462047,"operators (1). This improves significantly over the work of [45], which demands3 ω ≲µ2/L2. Such a
275"
COMPARISON TO PREVIOUS WORKS,0.7062706270627063,"requirement is very restrictive as the condition number L/µ of the loss function f is typically very
276"
COMPARISON TO PREVIOUS WORKS,0.7079207920792079,"large for any non-trivial optimization problem. Thus, the sparsifier’s (4) variance ω = d/q −1 has to
277"
COMPARISON TO PREVIOUS WORKS,0.7095709570957096,"be very close to 0 and q ≈d. So, the previous theory allows almost no compression (sparsification)
278"
COMPARISON TO PREVIOUS WORKS,0.7112211221122112,"because it is based on the analysis of Gradient Descent with Compressed Iterates [24].
279"
COMPARISON TO PREVIOUS WORKS,0.7128712871287128,"The second distinction is that the original IST work [45] considered a single node setting and thus
280"
COMPARISON TO PREVIOUS WORKS,0.7145214521452146,"their convergence bounds did not capture the effect of heterogeneity, which we believe is of crucial
281"
COMPARISON TO PREVIOUS WORKS,0.7161716171617162,"importance for distributed setting [9, 39]. Besides, they consider Lipschitz continuity of the loss
282"
COMPARISON TO PREVIOUS WORKS,0.7178217821782178,"function f, which is not satisfied for a simple quadratic model. A more detailed comparison including
283"
COMPARISON TO PREVIOUS WORKS,0.7194719471947195,"additional assumptions on the gradient estimator made in [45] is presented in the Appendix.
284"
COMPARISON TO PREVIOUS WORKS,0.7211221122112211,"FL with Model Pruning.
In a recent work [48] made an attempt to analyze a variant of the FedAvg
285"
COMPARISON TO PREVIOUS WORKS,0.7227722772277227,"algorithm with sparse local initialization and compressed gradient training (pruned local models).
286"
COMPARISON TO PREVIOUS WORKS,0.7244224422442245,"They considered a case of L-smooth loss and sparsification operator satisfying a similar condition to
287"
COMPARISON TO PREVIOUS WORKS,0.7260726072607261,"(1). However, they also assumed that the squared norm of stochastic gradient is uniformly bounded
288"
COMPARISON TO PREVIOUS WORKS,0.7277227722772277,"(11), which is “pathological” [23] especially in the case of local methods as it does not allow to
289"
COMPARISON TO PREVIOUS WORKS,0.7293729372937293,"capture the very important effect of heterogeneity and can result in vacuous bounds.
290"
COMPARISON TO PREVIOUS WORKS,0.731023102310231,"In the Appendix we show some limitations of other relevant previous approaches to training with
291"
COMPARISON TO PREVIOUS WORKS,0.7326732673267327,"compressed models: too restrictive assumptions on the algorithm [33] or not applicability in our
292"
COMPARISON TO PREVIOUS WORKS,0.7343234323432343,"problem setting [7].
293"
CONCLUSIONS AND FUTURE WORK,0.735973597359736,"5
Conclusions and Future Work
294"
CONCLUSIONS AND FUTURE WORK,0.7376237623762376,"In this study, we introduced a novel approach to understanding training with combined model and
295"
CONCLUSIONS AND FUTURE WORK,0.7392739273927392,"data parallelism for a quadratic model. This framework allowed to shed light on distributed submodel
296"
CONCLUSIONS AND FUTURE WORK,0.740924092409241,"optimization which revealed the advantages and limitations Independent Subnetwork Training (IST).
297"
CONCLUSIONS AND FUTURE WORK,0.7425742574257426,"Moreover, we accurately characterized the behavior of the considered method in both homogeneous
298"
CONCLUSIONS AND FUTURE WORK,0.7442244224422442,"and heterogeneous scenarios without imposing restrictive assumptions on gradient estimators.
299"
CONCLUSIONS AND FUTURE WORK,0.7458745874587459,"In future research, it would be valuable to explore extensions of our findings to settings that are closer
300"
CONCLUSIONS AND FUTURE WORK,0.7475247524752475,"to practical scenarios, such as cross-device federated learning. This could involve investigating partial
301"
CONCLUSIONS AND FUTURE WORK,0.7491749174917491,"participation support, leveraging local training benefits, and ensuring robustness against stragglers.
302"
CONCLUSIONS AND FUTURE WORK,0.7508250825082509,"Additionally, it would be interesting to generalize our results to non-quadratic scenarios without
303"
CONCLUSIONS AND FUTURE WORK,0.7524752475247525,"relying on pathological assumptions.
304"
CONCLUSIONS AND FUTURE WORK,0.7541254125412541,"3µ refers to constant from Polyak-Łojasiewicz (or strong convexity) condition. In case of a quadratic problem
with positive-definite matrix A: µ = λmin(A)"
REFERENCES,0.7557755775577558,"References
305"
REFERENCES,0.7574257425742574,"[1] Ahmad Ajalloeian and Sebastian U Stich. On the convergence of SGD with biased gradients.
306"
REFERENCES,0.759075907590759,"arXiv preprint arXiv:2008.00051, 2020.
307"
REFERENCES,0.7607260726072608,"[2] Samiul Alam, Luyang Liu, Ming Yan, and Mi Zhang. FedRolex: Model-heterogeneous federated
308"
REFERENCES,0.7623762376237624,"learning with rolling sub-model extraction. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,
309"
REFERENCES,0.764026402640264,"and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.
310"
REFERENCES,0.7656765676567657,"[3] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
QSGD:
311"
REFERENCES,0.7673267326732673,"Communication-efficient SGD via gradient quantization and encoding. In Advances in Neural
312"
REFERENCES,0.768976897689769,"Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
313"
REFERENCES,0.7706270627062707,"[4] Yossi Arjevani, Ohad Shamir, and Nathan Srebro. A tight convergence analysis for stochastic
314"
REFERENCES,0.7722772277227723,"gradient descent with delayed updates. In Algorithmic Learning Theory, pages 111–132. PMLR,
315"
REFERENCES,0.7739273927392739,"2020.
316"
REFERENCES,0.7755775577557755,"[5] Sebastian Caldas, Jakub Koneˇcny, H Brendan McMahan, and Ameet Talwalkar. Expanding
317"
REFERENCES,0.7772277227722773,"the reach of federated learning by reducing client resource requirements. arXiv preprint
318"
REFERENCES,0.7788778877887789,"arXiv:1812.07210, 2018.
319"
REFERENCES,0.7805280528052805,"[6] Zachary Charles, Kallista Bonawitz, Stanislav Chiknavaryan, Brendan McMahan, et al. Fed-
320"
REFERENCES,0.7821782178217822,"erated select: A primitive for communication-and memory-efficient federated learning. arXiv
321"
REFERENCES,0.7838283828382838,"preprint arXiv:2208.09432, 2022.
322"
REFERENCES,0.7854785478547854,"[7] El Mahdi Chayti and Sai Praneeth Karimireddy. Optimization with access to auxiliary informa-
323"
REFERENCES,0.7871287128712872,"tion. arXiv preprint arXiv:2206.00395, 2022.
324"
REFERENCES,0.7887788778877888,"[8] Yuanyuan Chen, Zichen Chen, Pengcheng Wu, and Han Yu. Fedobd: Opportunistic block
325"
REFERENCES,0.7904290429042904,"dropout for efficiently training large-scale neural networks through federated learning. arXiv
326"
REFERENCES,0.7920792079207921,"preprint arXiv:2208.05174, 2022.
327"
REFERENCES,0.7937293729372937,"[9] Sélim Chraibi, Ahmed Khaled, Dmitry Kovalev, Peter Richtárik, Adil Salim, and Martin Takáˇc.
328"
REFERENCES,0.7953795379537953,"Distributed fixed point methods with compressed iterates. arXiv preprint arXiv:2102.07245,
329"
REFERENCES,0.7970297029702971,"2019.
330"
REFERENCES,0.7986798679867987,"[10] Leonardo Cunha, Gauthier Gidel, Fabian Pedregosa, Damien Scieur, and Courtney Paque-
331"
REFERENCES,0.8003300330033003,"tte. Only tails matter: Average-case universality and robustness in the convex regime. In
332"
REFERENCES,0.801980198019802,"International Conference on Machine Learning, pages 4474–4491. PMLR, 2022.
333"
REFERENCES,0.8036303630363036,"[11] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio
334"
REFERENCES,0.8052805280528053,"Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks.
335"
REFERENCES,0.806930693069307,"Advances in neural information processing systems, 25, 2012.
336"
REFERENCES,0.8085808580858086,"[12] Enmao Diao, Jie Ding, and Vahid Tarokh. Heterofl: Computation and communication efficient
337"
REFERENCES,0.8102310231023102,"federated learning for heterogeneous clients. arXiv preprint arXiv:2010.01264, 2020.
338"
REFERENCES,0.8118811881188119,"[13] Chen Dun, Mirian Hipolito, Chris Jermaine, Dimitrios Dimitriadis, and Anastasios Kyrillidis.
339"
REFERENCES,0.8135313531353136,"Efficient and light-weight federated learning via asynchronous distributed dropout. arXiv
340"
REFERENCES,0.8151815181518152,"preprint arXiv:2210.16105, 2022.
341"
REFERENCES,0.8168316831683168,"[14] Chen Dun, Cameron R Wolfe, Christopher M Jermaine, and Anastasios Kyrillidis. ResIST:
342"
REFERENCES,0.8184818481848185,"Layer-wise decomposition of resnets for distributed training. In Uncertainty in Artificial
343"
REFERENCES,0.8201320132013201,"Intelligence, pages 610–620. PMLR, 2022.
344"
REFERENCES,0.8217821782178217,"[15] Philipp Farber and Krste Asanovic. Parallel neural network training on multi-spert. In Proceed-
345"
REFERENCES,0.8234323432343235,"ings of 3rd International Conference on Algorithms and Architectures for Parallel Processing,
346"
REFERENCES,0.8250825082508251,"pages 659–666. IEEE, 1997.
347"
REFERENCES,0.8267326732673267,"[16] Eduard Gorbunov, Filip Hanzely, and Peter Richtárik. A unified theory of SGD: Variance
348"
REFERENCES,0.8283828382838284,"reduction, sampling, quantization and coordinate descent. In International Conference on
349"
REFERENCES,0.83003300330033,"Artificial Intelligence and Statistics, pages 680–690. PMLR, 2020.
350"
REFERENCES,0.8316831683168316,"[17] Baptiste Goujaud, Damien Scieur, Aymeric Dieuleveut, Adrien B Taylor, and Fabian Pedregosa.
351"
REFERENCES,0.8333333333333334,"Super-acceleration with cyclical step-sizes. In International Conference on Artificial Intelligence
352"
REFERENCES,0.834983498349835,"and Statistics, pages 3028–3065. PMLR, 2022.
353"
REFERENCES,0.8366336633663366,"[18] Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter
354"
REFERENCES,0.8382838283828383,"Richtárik. SGD: General analysis and improved rates. Proceedings of the 36th International
355"
REFERENCES,0.8399339933993399,"Conference on Machine Learning, Long Beach, California, 2019.
356"
REFERENCES,0.8415841584158416,"[19] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
357"
REFERENCES,0.8432343234323433,"Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training
358"
REFERENCES,0.8448844884488449,"imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2018.
359"
REFERENCES,0.8465346534653465,"[20] Samuel Horvath, Stefanos Laskaridis, Mario Almeida, Ilias Leontiadis, Stylianos Venieris, and
360"
REFERENCES,0.8481848184818482,"Nicholas Lane. FjORD: Fair and accurate federated learning under heterogeneous targets with
361"
REFERENCES,0.8498349834983498,"ordered dropout. Advances in Neural Information Processing Systems, 34:12876–12889, 2021.
362"
REFERENCES,0.8514851485148515,"[21] Yuang Jiang, Shiqiang Wang, Victor Valls, Bong Jun Ko, Wei-Han Lee, Kin K Leung, and
363"
REFERENCES,0.8531353135313532,"Leandros Tassiulas. Model pruning enables efficient federated learning on edge devices. IEEE
364"
REFERENCES,0.8547854785478548,"Transactions on Neural Networks and Learning Systems, 2022.
365"
REFERENCES,0.8564356435643564,"[22] Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Ar-
366"
REFERENCES,0.858085808580858,"jun Nitin Bhagoji, Kallista A. Bonawitz, Zachary Charles, Graham Cormode, Rachel Cum-
367"
REFERENCES,0.8597359735973598,"mings, Rafael G. L. D’Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner,
368"
REFERENCES,0.8613861386138614,"Zachary Garrett, Adrià Gascón, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaïd Har-
369"
REFERENCES,0.863036303630363,"chaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara
370"
REFERENCES,0.8646864686468647,"Javidi, Gauri Joshi, Mikhail Khodak, Jakub Koneˇcný, Aleksandra Korolova, Farinaz Koushanfar,
371"
REFERENCES,0.8663366336633663,"Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock,
372"
REFERENCES,0.8679867986798679,"Ayfer Özgür, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova, Dawn
373"
REFERENCES,0.8696369636963697,"Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramèr,
374"
REFERENCES,0.8712871287128713,"Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu,
375"
REFERENCES,0.8729372937293729,"and Sen Zhao. Advances and open problems in federated learning. Found. Trends Mach. Learn.,
376"
REFERENCES,0.8745874587458746,"14(1-2):1–210, 2021.
377"
REFERENCES,0.8762376237623762,"[23] Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. Tighter theory for local SGD on
378"
REFERENCES,0.8778877887788779,"identical and heterogeneous data. In International Conference on Artificial Intelligence and
379"
REFERENCES,0.8795379537953796,"Statistics, pages 4519–4529. PMLR, 2020.
380"
REFERENCES,0.8811881188118812,"[24] Ahmed Khaled and Peter Richtárik. Gradient descent with compressed iterates. arXiv preprint
381"
REFERENCES,0.8828382838283828,"arXiv:1909.04716, 2019.
382"
REFERENCES,0.8844884488448845,"[25] Ahmed Khaled and Peter Richtárik. Better theory for SGD in the nonconvex world. Transactions
383"
REFERENCES,0.8861386138613861,"on Machine Learning Research, 2023. Survey Certification.
384"
REFERENCES,0.8877887788778878,"[26] Sarit Khirirat, Hamid Reza Feyzmahdavian, and Mikael Johansson. Distributed learning with
385"
REFERENCES,0.8894389438943895,"compressed gradients. arXiv preprint arXiv:1806.06573, 2018.
386"
REFERENCES,0.8910891089108911,"[27] Jakub Koneˇcný, H. Brendan McMahan, Felix X. Yu, Peter Richtárik, Ananda Theertha Suresh,
387"
REFERENCES,0.8927392739273927,"and Dave Bacon. Federated learning: Strategies for improving communication efficiency. NIPS
388"
REFERENCES,0.8943894389438944,"Private Multi-Party Machine Learning Workshop, 2016.
389"
REFERENCES,0.8960396039603961,"[28] Fangshuo Liao and Anastasios Kyrillidis. On the convergence of shallow neural network training
390"
REFERENCES,0.8976897689768977,"with randomly masked neurons. Transactions on Machine Learning Research, 2022.
391"
REFERENCES,0.8993399339933993,"[29] Rongmei Lin, Yonghui Xiao, Tien-Ju Yang, Ding Zhao, Li Xiong, Giovanni Motta, and
392"
REFERENCES,0.900990099009901,"Françoise Beaufays. Federated pruning: Improving neural network efficiency with federated
393"
REFERENCES,0.9026402640264026,"learning. arXiv preprint arXiv:2209.06359, 2022.
394"
REFERENCES,0.9042904290429042,"[30] Tao Lin, Sebastian U Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model
395"
REFERENCES,0.905940594059406,"pruning with feedback. In International Conference on Learning Representations, 2019.
396"
REFERENCES,0.9075907590759076,"[31] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
397"
REFERENCES,0.9092409240924092,"Communication-Efficient Learning of Deep Networks from Decentralized Data. In Proceedings
398"
REFERENCES,0.9108910891089109,"of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of
399"
REFERENCES,0.9125412541254125,"Proceedings of Machine Learning Research, pages 1273–1282, 20–22 Apr 2017.
400"
REFERENCES,0.9141914191419142,"[32] Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter Richtárik. ProxSkip:
401"
REFERENCES,0.9158415841584159,"Yes! local gradient steps provably lead to communication acceleration! finally! In International
402"
REFERENCES,0.9174917491749175,"Conference on Machine Learning, pages 15750–15769. PMLR, 2022.
403"
REFERENCES,0.9191419141914191,"[33] Amirkeivan Mohtashami, Martin Jaggi, and Sebastian Stich. Masked training of neural networks
404"
REFERENCES,0.9207920792079208,"with partial gradients. In International Conference on Artificial Intelligence and Statistics,
405"
REFERENCES,0.9224422442244224,"pages 5876–5890. PMLR, 2022.
406"
REFERENCES,0.9240924092409241,"[34] Yu Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems.
407"
REFERENCES,0.9257425742574258,"SIAM Journal on Optimization, 22(2):341–362, 2012.
408"
REFERENCES,0.9273927392739274,"[35] Xinchi Qiu, Javier Fernandez-Marques, Pedro PB Gusmao, Yan Gao, Titouan Parcollet, and
409"
REFERENCES,0.929042904290429,"Nicholas Donald Lane. ZeroFL: Efficient on-device training for federated learning with local
410"
REFERENCES,0.9306930693069307,"sparsity. In International Conference on Learning Representations, 2022.
411"
REFERENCES,0.9323432343234324,"[36] Peter Richtárik and Martin Takáˇc. Iteration complexity of randomized block-coordinate descent
412"
REFERENCES,0.933993399339934,"methods for minimizing a composite function. Mathematical Programming, 144(1-2):1–38,
413"
REFERENCES,0.9356435643564357,"2014.
414"
REFERENCES,0.9372937293729373,"[37] Peter Richtárik and Martin Takáˇc. Distributed coordinate descent method for learning with big
415"
REFERENCES,0.9389438943894389,"data. Journal of Machine Learning Research, 17(75):1–25, 2016.
416"
REFERENCES,0.9405940594059405,"[38] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent
417"
REFERENCES,0.9422442244224423,"and its application to data-parallel distributed training of speech dnns. In Fifteenth Annual
418"
REFERENCES,0.9438943894389439,"Conference of the International Speech Communication Association, 2014.
419"
REFERENCES,0.9455445544554455,"[39] Egor Shulgin and Peter Richtárik. Shifted compression framework: Generalizations and
420"
REFERENCES,0.9471947194719472,"improvements. In The 38th Conference on Uncertainty in Artificial Intelligence, 2022.
421"
REFERENCES,0.9488448844884488,"[40] Rafał Szlendak, Alexander Tyurin, and Peter Richtárik. Permutation compressors for prov-
422"
REFERENCES,0.9504950495049505,"ably faster distributed nonconvex optimization. In International Conference on Learning
423"
REFERENCES,0.9521452145214522,"Representations, 2022.
424"
REFERENCES,0.9537953795379538,"[41] Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-
425"
REFERENCES,0.9554455445544554,"Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field
426"
REFERENCES,0.9570957095709571,"guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021.
427"
REFERENCES,0.9587458745874587,"[42] Dingzhu Wen, Ki-Jun Jeon, and Kaibin Huang. Federated dropout—a simple approach for
428"
REFERENCES,0.9603960396039604,"enabling federated learning on resource constrained devices. IEEE Wireless Communications
429"
REFERENCES,0.9620462046204621,"Letters, 11(5):923–927, 2022.
430"
REFERENCES,0.9636963696369637,"[43] Cameron R Wolfe, Jingkang Yang, Arindam Chowdhury, Chen Dun, Artun Bayer, Santiago
431"
REFERENCES,0.9653465346534653,"Segarra, and Anastasios Kyrillidis. Gist: Distributed training for large-scale graph convolutional
432"
REFERENCES,0.966996699669967,"networks. arXiv preprint arXiv:2102.10424, 2021.
433"
REFERENCES,0.9686468646864687,"[44] Tien-Ju Yang, Dhruv Guliani, Françoise Beaufays, and Giovanni Motta. Partial variable training
434"
REFERENCES,0.9702970297029703,"for efficient on-device federated learning. In ICASSP 2022-2022 IEEE International Conference
435"
REFERENCES,0.971947194719472,"on Acoustics, Speech and Signal Processing (ICASSP), pages 4348–4352. IEEE, 2022.
436"
REFERENCES,0.9735973597359736,"[45] Binhang Yuan, Cameron R Wolfe, Chen Dun, Yuxin Tang, Anastasios Kyrillidis, and Chris
437"
REFERENCES,0.9752475247524752,"Jermaine. Distributed learning of fully connected neural networks using independent subnet
438"
REFERENCES,0.976897689768977,"training. Proceedings of the VLDB Endowment, 15(8):1581–1590, 2022.
439"
REFERENCES,0.9785478547854786,"[46] Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl, Chris
440"
REFERENCES,0.9801980198019802,"Shallue, and Roger B Grosse. Which algorithmic choices matter at which batch sizes? insights
441"
REFERENCES,0.9818481848184818,"from a noisy quadratic model. Advances in neural information processing systems, 32, 2019.
442"
REFERENCES,0.9834983498349835,"[47] Xiru Zhang, Michael Mckenna, Jill Mesirov, and David Waltz. An efficient implementation
443"
REFERENCES,0.9851485148514851,"of the back-propagation algorithm on the connection machine cm-2. Advances in neural
444"
REFERENCES,0.9867986798679867,"information processing systems, 2, 1989.
445"
REFERENCES,0.9884488448844885,"[48] Hanhan Zhou, Tian Lan, Guru Venkataramani, and Wenbo Ding. On the convergence of
446"
REFERENCES,0.9900990099009901,"heterogeneous federated learning with arbitrary adaptive online model pruning. arXiv preprint
447"
REFERENCES,0.9917491749174917,"arXiv:2201.11803, 2022.
448"
REFERENCES,0.9933993399339934,"[49] Libin Zhu, Chaoyue Liu, Adityanarayanan Radhakrishnan, and Mikhail Belkin. Quadratic
449"
REFERENCES,0.995049504950495,"models for understanding neural network dynamics. arXiv preprint arXiv:2205.11787, 2022.
450"
REFERENCES,0.9966996699669967,"[50] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex Smola. Parallelized stochastic gradient
451"
REFERENCES,0.9983498349834984,"descent. Advances in neural information processing systems, 23, 2010.
452"
