Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001001001001001001,"There has been significant interest in the development of personalized and adaptive
1"
ABSTRACT,0.002002002002002002,"educational tools that cater to a student’s individual learning progress. A crucial
2"
ABSTRACT,0.003003003003003003,"aspect in developing such tools is in exploring how mastery can be achieved across
3"
ABSTRACT,0.004004004004004004,"a diverse yet related range of content in an efficient manner. While Reinforcement
4"
ABSTRACT,0.005005005005005005,"Learning and Multi-armed Bandits have shown promise in educational settings,
5"
ABSTRACT,0.006006006006006006,"existing works often assume the independence of learning content, neglecting
6"
ABSTRACT,0.007007007007007007,"the prevalent interdependencies between such content. In response, we introduce
7"
ABSTRACT,0.008008008008008008,"Education Network Restless Multi-armed Bandits (EdNetRMABs), utilizing a
8"
ABSTRACT,0.009009009009009009,"network to represent the relationships between interdependent arms. Subsequently,
9"
ABSTRACT,0.01001001001001001,"we propose EduQate, a method employing interdependency-aware Q-learning to
10"
ABSTRACT,0.011011011011011011,"make informed decisions on arm selection at each time step. We establish the
11"
ABSTRACT,0.012012012012012012,"optimality guarantee of EduQate and demonstrate its efficacy compared to baseline
12"
ABSTRACT,0.013013013013013013,"policies, using students modeled from both synthetic and real-world data.
13"
INTRODUCTION,0.014014014014014014,"1
Introduction
14"
INTRODUCTION,0.015015015015015015,"The COVID-19 pandemic has accelerated the adoption of educational technologies, especially
15"
INTRODUCTION,0.016016016016016016,"on eLearning platforms. Despite abundant data and advancements in modeling student learning,
16"
INTRODUCTION,0.01701701701701702,"effectively capturing the learning process with interdependent content remains a significant challenge
17"
INTRODUCTION,0.018018018018018018,"[9]. The conventional rules-based approach to creating personalized learning curricula is impractical
18"
INTRODUCTION,0.01901901901901902,"due to its labor-intensive nature and need for expert knowledge. Machine learning-based systems offer
19"
INTRODUCTION,0.02002002002002002,"a scalable alternative, automatically generating personalized content to optimize learning [22, 24].
20"
INTRODUCTION,0.021021021021021023,"One possible approach to model the learning process is the Restless Multi-Armed Bandits (RMAB,
21"
INTRODUCTION,0.022022022022022022,"[26]), where a teacher agent selects a subset of arms (concepts) to teach each round. However,
22"
INTRODUCTION,0.023023023023023025,"RMAB’s assumption that arms are independent is unrealistic in educational settings. For example,
23"
INTRODUCTION,0.024024024024024024,"solving a math question on the area of a triangle requires knowledge of algebra, arithmetic, and
24"
INTRODUCTION,0.025025025025025027,"geometry. Practicing this question should enhance proficiency in all three areas. Models that ignore
25"
INTRODUCTION,0.026026026026026026,"such interdependencies may inaccurately predict knowledge levels by assuming each exercise impacts
26"
INTRODUCTION,0.02702702702702703,"only a single area.
27"
INTRODUCTION,0.028028028028028028,"In response to this challenge, we introduce an interdependency-aware RMAB model to the education
28"
INTRODUCTION,0.02902902902902903,"setting. We posit that by acknowledging and modeling the learning dynamics of interdependent
29"
INTRODUCTION,0.03003003003003003,"content, both teachers and algorithms can strategically leverage overlapping utility to foster mastery
30"
INTRODUCTION,0.031031031031031032,"over a broader range of topics within a curriculum. We advocate for RMABs as a fitting model for
31"
INTRODUCTION,0.03203203203203203,"this context, as the inherent dynamics of such a model align closely with the learning process.
32"
INTRODUCTION,0.03303303303303303,"In this study, our objective is to derive a teacher policy that effectively recommends educational
33"
INTRODUCTION,0.03403403403403404,"content to students, accounting for interdependencies among the content to enhance overall utility
34"
INTRODUCTION,0.035035035035035036,"(that characterizes understanding and retention of content). Our contributions are as follows:
35"
INTRODUCTION,0.036036036036036036,"1. We introduce Restless Multi-armed Bandits for Education (EdNetRMABs), enabling the
36"
INTRODUCTION,0.037037037037037035,"modeling of learning processes with interdependent educational content.
37"
INTRODUCTION,0.03803803803803804,"2. We propose EduQate, a Whittle index-based heuristic algorithm that uses Q-learning to
38"
INTRODUCTION,0.03903903903903904,"compute an inter-dependency-aware teacher policy. Unlike previous methods, EduQate does
39"
INTRODUCTION,0.04004004004004004,"not require knowledge of the transition matrix to compute an optimal policy.
40"
INTRODUCTION,0.04104104104104104,"3. We provide a theoretical analysis of EduQate, demonstrating guarantees of optimality.
41"
INTRODUCTION,0.042042042042042045,"4. We present empirical results on simulated students and real-world datasets, showing the
42"
INTRODUCTION,0.043043043043043044,"effectiveness of EduQate over other teacher policies.
43"
RELATED WORK AND PRELIMINARIES,0.044044044044044044,"2
Related Work and Preliminaries
44"
RESTLESS MULTI-ARMED BANDITS,0.04504504504504504,"2.1
Restless Multi-Armed Bandits
45"
RESTLESS MULTI-ARMED BANDITS,0.04604604604604605,"The selection of the right time and manner for limited interventions is a problem of great practical im-
46"
RESTLESS MULTI-ARMED BANDITS,0.04704704704704705,"portance across various domains, including health intervention [17, 5], anti-poaching operations [20],
47"
RESTLESS MULTI-ARMED BANDITS,0.04804804804804805,"education [13, 6, 2], etc. These problems share a common characteristic of having multiple arms
48"
RESTLESS MULTI-ARMED BANDITS,0.04904904904904905,"in a Multi-armed Bandit (MAB) problem, representing entities such as patients, regions of a forest,
49"
RESTLESS MULTI-ARMED BANDITS,0.05005005005005005,"or students’ mastery of concepts. These arms evolve in an uncertain manner, and interventions are
50"
RESTLESS MULTI-ARMED BANDITS,0.05105105105105105,"required to guide them from ""bad"" states to ""good"" states. The inherent challenge lies in the limited
51"
RESTLESS MULTI-ARMED BANDITS,0.05205205205205205,"number of interventions, dictated by the limited resources (e.g., public health workers, the number of
52"
RESTLESS MULTI-ARMED BANDITS,0.05305305305305305,"student interactions). RMAB, a generalization of MAB, offers an ideal model for representing the
53"
RESTLESS MULTI-ARMED BANDITS,0.05405405405405406,"aforementioned problems of interest. RMAB allows non-active bandits to also undergo the Markovian
54"
RESTLESS MULTI-ARMED BANDITS,0.055055055055055056,"state transition, effectively capturing uncertainty in arm state transitions (reflecting uncertain state
55"
RESTLESS MULTI-ARMED BANDITS,0.056056056056056056,"evolution), actions (representing interventions), and budget constraints (illustrating limited resources).
56"
RESTLESS MULTI-ARMED BANDITS,0.057057057057057055,"RMABs and the associated Markov Decision Processes (MDP) for each arm offer a valuable model for
57"
RESTLESS MULTI-ARMED BANDITS,0.05805805805805806,"representing the learning process. Firstly, leveraging the MDPs associated with each arm provides the
58"
RESTLESS MULTI-ARMED BANDITS,0.05905905905905906,"flexibility to adopt nuanced modeling of learning content, accommodating different learning curves
59"
RESTLESS MULTI-ARMED BANDITS,0.06006006006006006,"for various content based on students’ strengths and weaknesses. Secondly, the transition probabilities
60"
RESTLESS MULTI-ARMED BANDITS,0.06106106106106106,"serve as a useful mechanism to model forgetting (through state decay due to passivity or negligence)
61"
RESTLESS MULTI-ARMED BANDITS,0.062062062062062065,"and learning (through state transitions to the positive state from repeated practice). Considering
62"
RESTLESS MULTI-ARMED BANDITS,0.06306306306306306,"these aspects, RMABs prove to be a beneficial framework for personalizing and generating adaptive
63"
RESTLESS MULTI-ARMED BANDITS,0.06406406406406406,"curricula across a diverse range of students.
64"
RESTLESS MULTI-ARMED BANDITS,0.06506506506506507,"In general, computing the optimal policy for a given set of restless arms in RMABs is recognized as a
65"
RESTLESS MULTI-ARMED BANDITS,0.06606606606606606,"PSPACE-hard problem [18]. The Whittle index [26] provides an approach with a tractable solution
66"
RESTLESS MULTI-ARMED BANDITS,0.06706706706706707,"that is provably optimal, especially when each arm is indexable. However, proving indexability can
67"
RESTLESS MULTI-ARMED BANDITS,0.06806806806806807,"be challenging and often requires specification of the problem’s structure, such as the optimality of
68"
RESTLESS MULTI-ARMED BANDITS,0.06906906906906907,"threshold policies [17, 16]. Moreover, much of the research on Whittle Index policies has focused
69"
RESTLESS MULTI-ARMED BANDITS,0.07007007007007007,"on two-action settings or requires prior knowledge of the transition matrix of the RMABs. Meeting
70"
RESTLESS MULTI-ARMED BANDITS,0.07107107107107107,"these conditions proves challenging in the educational context, where diverse students interact with
71"
RESTLESS MULTI-ARMED BANDITS,0.07207207207207207,"educational systems, each possessing different prior knowledge and distinct learning curves for
72"
RESTLESS MULTI-ARMED BANDITS,0.07307307307307308,"various topics.
73"
RESTLESS MULTI-ARMED BANDITS,0.07407407407407407,"WIQL [5], on the other hand, employs a Q-learning-based method to estimate the Whittle Index and
74"
RESTLESS MULTI-ARMED BANDITS,0.07507507507507508,"has demonstrated provable optimality without requiring prior knowledge of the transition matrix. We
75"
RESTLESS MULTI-ARMED BANDITS,0.07607607607607608,"utilize WIQL as a baseline method in our subsequent experiments.
76"
RESTLESS MULTI-ARMED BANDITS,0.07707707707707707,"In a recent investigation by [12], RMABs were explored within a network framework, requiring the
77"
RESTLESS MULTI-ARMED BANDITS,0.07807807807807808,"agent to manage a budget while allocating a high-cost, high-benefit resource to one arm to “unlock""
78"
RESTLESS MULTI-ARMED BANDITS,0.07907907907907907,"potential lower-cost, intermediate-benefit resources for the arm’s neighbors. The network effects
79"
RESTLESS MULTI-ARMED BANDITS,0.08008008008008008,"emphasized in their work are triggered by an intentional, active action, enabling the agent to choose
80"
RESTLESS MULTI-ARMED BANDITS,0.08108108108108109,"to propagate positive externalities to a selected arm’s neighbors within budget constraints. In contrast,
81"
RESTLESS MULTI-ARMED BANDITS,0.08208208208208208,"our study delves into scenarios where network effects are indirect results of an active action, and the
82"
RESTLESS MULTI-ARMED BANDITS,0.08308308308308308,"agent lacks direct control over such effects. Thus, the challenge lies in accurately modeling these
83"
RESTLESS MULTI-ARMED BANDITS,0.08408408408408409,"network effects and leveraging them when beneficial.
84"
REINFORCEMENT LEARNING IN EDUCATION,0.08508508508508508,"2.2
Reinforcement Learning in Education
85"
REINFORCEMENT LEARNING IN EDUCATION,0.08608608608608609,"In the realm of education, numerous researchers have explored optimizing the sequencing of in-
86"
REINFORCEMENT LEARNING IN EDUCATION,0.08708708708708708,"structional activities and content, assuming that optimal sequencing can significantly impact student
87"
REINFORCEMENT LEARNING IN EDUCATION,0.08808808808808809,"learning. RL is a natural approach for making sequential decisions under uncertainty [1]. While RL
88"
REINFORCEMENT LEARNING IN EDUCATION,0.0890890890890891,"has seen success in various educational applications, effectively sequencing interdependent content in
89"
REINFORCEMENT LEARNING IN EDUCATION,0.09009009009009009,"a personalized and adaptive manner has yielded mixed or insignificant results compared to baseline
90"
REINFORCEMENT LEARNING IN EDUCATION,0.09109109109109109,"teacher policies [11, 21, 8]. In general, these RL works focus on data-driven methods using student
91"
REINFORCEMENT LEARNING IN EDUCATION,0.0920920920920921,"activity logs to estimate students’ knowledge states and progress, assuming that the interdependencies
92"
REINFORCEMENT LEARNING IN EDUCATION,0.09309309309309309,"between learning content are encapsulated in students’ learning histories [9, 3, 19]. In contrast, our
93"
REINFORCEMENT LEARNING IN EDUCATION,0.0940940940940941,"work focuses on modelling these interdependencies directly.
94"
REINFORCEMENT LEARNING IN EDUCATION,0.09509509509509509,"Of particular relevance are factored MDPs applied to skill acquisition introduced by [11]. While fac-
95"
REINFORCEMENT LEARNING IN EDUCATION,0.0960960960960961,"tored MDPs account for interdependencies amongst skills, decentralized policy learning is infeasible
96"
REINFORCEMENT LEARNING IN EDUCATION,0.0970970970970971,"as policies must consider the joint state space. Our work leverages the advantage of decentralized
97"
REINFORCEMENT LEARNING IN EDUCATION,0.0980980980980981,"policy learning provided by RMABs and introduces a novel decentralized learning approach that
98"
REINFORCEMENT LEARNING IN EDUCATION,0.0990990990990991,"exploits interdependencies between arms.
99"
REINFORCEMENT LEARNING IN EDUCATION,0.1001001001001001,"Complementary to RL methods in education is the utilization of knowledge graphs to uncover
100"
REINFORCEMENT LEARNING IN EDUCATION,0.1011011011011011,"relationships between learning content [9]. Existing research primarily focuses on establishing these
101"
REINFORCEMENT LEARNING IN EDUCATION,0.1021021021021021,"relationships through data-driven methods (e.g. [7, 23]) often leveraging student-activity logs. In this
102"
REINFORCEMENT LEARNING IN EDUCATION,0.1031031031031031,"work, we complement such research by presenting an approach where bandit methods can effectively
103"
REINFORCEMENT LEARNING IN EDUCATION,0.1041041041041041,"operate with knowledge graphs derived by such methods.
104"
MODEL,0.10510510510510511,"3
Model
105"
MODEL,0.1061061061061061,"In this section, we introduce the Restless Multi-Armed Bandits for Education (EdNetRMABs). It
106"
MODEL,0.10710710710710711,"is important to note that while we specifically apply EdNetRMABs to the education setting, the
107"
MODEL,0.10810810810810811,"framework can be seamlessly translated to other scenarios where modeling the effects of active
108"
MODEL,0.1091091091091091,"actions within a network is critical. For ease of access, a table of notations is provided in Table 2.
109"
MODEL,0.11011011011011011,"In education, a teacher recommends learning content, or items, to maximize student education, often
110"
MODEL,0.1111111111111111,"with content from online platforms. Items are grouped by topics, such as “Geometry,"" where exposure
111"
MODEL,0.11211211211211211,"to one piece of content can enhance knowledge across others in the same group. This cumulative
112"
MODEL,0.11311311311311312,"learning effect which we refer to as “network effects"", implies that exposure to an item is likely
113"
MODEL,0.11411411411411411,"to positively impact the student’s success on items within the same group. A successful teacher
114"
MODEL,0.11511511511511512,"accurately estimates a student’s knowledge state over repeated interactions, leveraging these network
115"
MODEL,0.11611611611611612,"effects to promote both breadth and depth of understanding through recommendations.
116"
EDNETRMABS,0.11711711711711711,"3.1
EdNetRMABs
117"
EDNETRMABS,0.11811811811811812,"The RMAB model tasks an agent with selecting k arms from N arms, constrained by a limit on the
118"
EDNETRMABS,0.11911911911911911,"number of arms that can be pulled at each time step. The objective is to find a policy that maximizes
119"
EDNETRMABS,0.12012012012012012,"the total expected discounted reward, assuming that the state of each arm evolves independently
120"
EDNETRMABS,0.12112112112112113,"according to an underlying MDP.
121"
EDNETRMABS,0.12212212212212212,"The EdNetRMABs model extends RMABs by allowing for active actions to propagate to other arms
122"
EDNETRMABS,0.12312312312312312,"dependent on the current arm when it is being pulled, thus relaxing the assumption of independent
123"
EDNETRMABS,0.12412412412412413,"arms. This is operationalized by organising the arms in a network, and pulling of an arm results in
124"
EDNETRMABS,0.12512512512512514,"changes for its neighbors, or members in the same group.
125"
EDNETRMABS,0.12612612612612611,"When applied to education setting, the EdNetRMABs is formalized as follows:
126"
EDNETRMABS,0.12712712712712712,"Arms
Each arm, denoted as i ∈1, ..., N, signifies an item. In the context of this networked
127"
EDNETRMABS,0.12812812812812813,"environment, each arm belongs to a group ϕ ∈{1, ..., L} representing the overarching topic that
128"
EDNETRMABS,0.12912912912912913,"encompasses related items. It’s important to note that arm membership is not mutually exclusive,
129"
EDNETRMABS,0.13013013013013014,"allowing arms to be part of multiple groups. This flexibility enables a more nuanced modeling of
130"
EDNETRMABS,0.13113113113113112,"interdependencies among educational content. For instance, a question involving the calculation of
131"
EDNETRMABS,0.13213213213213212,"the area of a triangle may span both arithmetic and geometry groups.
132"
EDNETRMABS,0.13313313313313313,"State space
In this framework, each arm possesses a binary latent state, denoted as si ∈{0, 1},
133"
EDNETRMABS,0.13413413413413414,"where “0"" represents an “unlearned"" state, and “1"" indicates a “learned"" state. Considering all arms
134"
EDNETRMABS,0.13513513513513514,"collectively, these states serve as a representation of the student’s overall knowledge state. In the
135"
EDNETRMABS,0.13613613613613615,"current work, it is assumed that the states of all arms are fully observable, providing a comprehensive
136"
EDNETRMABS,0.13713713713713713,"model of the student’s understanding of the various educational concepts.
137"
EDNETRMABS,0.13813813813813813,"Action space
To capture the network effects associated with arm pulls, we depart from the conven-
138"
EDNETRMABS,0.13913913913913914,"tional RMAB framework with a binary action space A = {0, 1} by introducing a pseudo-action. In
139"
EDNETRMABS,0.14014014014014015,"this modified setup, the action space is extended to A = {0, 1, 2}, where actions 0 and 2 represent
140"
EDNETRMABS,0.14114114114114115,"“no-pull"" and “pull"", as commonly used in bandit literature. Notably, in EdNetRMABs, a third action
141"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.14214214214214213,"1 is introduced to simulate the network effects resulting from pulling another arm within the same
142"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.14314314314314314,"group. It is important to clarify that agents do not directly engage with action 1 but we employ it
143"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.14414414414414414,"solely for modeling network effects, hence the term “pseudo-action"".
144"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.14514514514514515,"Transition function
For a given arm i, let P a,i
s,s′ represent the probability of the arm transitioning
145"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.14614614614614616,"from state s to s′ under action a. It’s noteworthy that, in typical real-world educational settings, the
146"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.14714714714714713,"actual transition functions governing the states of the arms are often unknown and, even for the same
147"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.14814814814814814,"concept, may vary among students due to differences in prior knowledge. To address this challenge,
148"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.14914914914914915,"we adopt model-free approaches in this study, devising methods to compute the teacher policy without
149"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.15015015015015015,"relying on explicit knowledge of these transition functions. In the following experiments, we maintain
150"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.15115115115115116,"the assumption of non-zero transition probabilities, and enforce constraints that are aligned with the
151"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.15215215215215216,"current domain [17]: (i) The arms are more likely to stay in the positive state than change to the
152"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.15315315315315314,"negative state: P 0
0,1 < P 0
1,1, P 1
0,1 < P 1
1,1 and P 2
0,1 < P 2
1,1; (ii) The arm tends to improve the latent
153"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.15415415415415415,"state if more efforts is spent on that arm, i.e., it is active or semi-active: P 0
0,1 < P 1
0,1 < P 2
0,1 and
154"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.15515515515515516,"P 0
1,1 < P 1
1,1 < P 2
1,1.
155"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.15615615615615616,"With the formalization of the EdNetRMABs model provided, we now apply it to an educational
156"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.15715715715715717,"context. In this scenario, the agent assumes the role of a teacher and takes actions during each time
157"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.15815815815815815,"step t ∈{1, ..., T}. Specifically, at each time step, the teacher recommends an item for the student to
158"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.15915915915915915,"study. We represent the vector of actions taken by the teacher at time step t as at ∈{0, 1, 2}N. Here,
159"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.16016016016016016,"arm i is considered to be active at time t if at(i) = 2 and passive when at(i) = 0. When arm i is
160"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.16116116116116116,"pulled, the set of arms that share the same group membership as arm i, denoted as ϕ−
i under goes
161"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.16216216216216217,"the pseudo-action, represented as at(j) = 1 for all j ∈ϕ−. In our framework, the teacher agent
162"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.16316316316316315,"acts on exactly one arm per time step to simulate the real-world constraint that the teacher can only
163"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.16416416416416416,recommend one concept to students ( P
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.16516516516516516,"i Iat(i)=2 = 1, ∀t ). Subsequent to taking action, the teacher
164"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.16616616616616617,"receives st ∈{0, 1}N, a vector reflecting the state of all arms, and reward rt = PN
i=1 st(i). The
165"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.16716716716716717,"vector st represents the overall knowledge state of the student. The teacher agent’s goal, therefore, is
166"
IS INTRODUCED TO SIMULATE THE NETWORK EFFECTS RESULTING FROM PULLING ANOTHER ARM WITHIN THE SAME,0.16816816816816818,"to maximize the long term rewards, either discounted or averaged.
167"
EDUQATE,0.16916916916916916,"4
EduQate
168"
EDUQATE,0.17017017017017017,"Q-learning [25] is a popular reinforcement learning method that enables an agent to learn optimal
169"
EDUQATE,0.17117117117117117,"actions in an environment by iteratively updating its estimate of state-action value, Q(s, a), based on
170"
EDUQATE,0.17217217217217218,"the rewards it receives. At each time step t, the agent takes an action a using its current estimate of Q
171"
EDUQATE,0.17317317317317318,"values and current state s, thus received a reward of r(s) and new state s′. We provide an abridged
172"
EDUQATE,0.17417417417417416,"introduction to Q-learning in the Appendix F.
173"
EDUQATE,0.17517517517517517,"Expanding upon Q-learning, we introduce EduQate, a tailored Q-learning approach designed for
174"
EDUQATE,0.17617617617617617,"learning Whittle-index policies in EdNetRMABs. In the interaction with the environment, the agent
175"
EDUQATE,0.17717717717717718,"chooses a single item, represented by arm i, to recommend to the student. In this context, the agent
176"
EDUQATE,0.1781781781781782,"possesses knowledge of the group membership ϕi of the selected arm and observes the rewards
177"
EDUQATE,0.17917917917917917,"generated by activating arm i and semi-activating arms in ϕ−
i . EduQate utilizes this interaction to
178"
EDUQATE,0.18018018018018017,"learn the Q-values for all arms and actions.
179"
EDUQATE,0.18118118118118118,"To adapt Q-learning to EdNetRMABs, we propose leveraging the learned Q-values to select the arm
180"
EDUQATE,0.18218218218218218,"with the highest estimate of the Whittle index, defined as:
181"
EDUQATE,0.1831831831831832,Algorithm 1 Q-Learning for EdNetRMABs (EduQate)
EDUQATE,0.1841841841841842,"Input: Number of arms N
Initialize Qi(s, a) ←0 and λi(s) ←0 for each state s ∈S and each action a ∈{0, 1, 2}, for each
arm i ∈1, ..., N.
Initialize replay buffer D with capacity C.
for t in 1, ..., T do"
EDUQATE,0.18518518518518517,"ϵ ←
N
N+t
With probability ϵ, select one arm uniformly at random. Otherwise, select arm with highest
Whittle Index, i = arg maxi λi.
for arm n in 1, ..., N do"
EDUQATE,0.18618618618618618,if n ̸= i then
EDUQATE,0.1871871871871872,"Set arm n to passive, at
n = 0
else"
EDUQATE,0.1881881881881882,"Set arm n to active, at
n = 2
for j ∈ϕ−
i
do
Set arms in same group as i to semi-active, at
j = 1
end for
end if
end for
Execute actions at and observe reward rt and next state st+1 for all arms
Store experience (st, at, rt, st+1)in replay buffer D.
Sample minibatch B of Experience from replay buffer D.
for Experience in minibatch B do"
EDUQATE,0.1891891891891892,"Update Qn(s, a) using Q-learning update in Equation 11.
Compute λn using Equation 1
end for
end for"
EDUQATE,0.19019019019019018,"λi = Q(si, ai = 2) −Q(si, ai = 0) +
X"
EDUQATE,0.19119119119119118,"j∈ϕ−
i"
EDUQATE,0.1921921921921922,"(Q(sj, aj = 1) −Q(sj, aj = 0))
(1)"
EDUQATE,0.1931931931931932,"Here, λi is the Whittle Index estimate for arm i. In essence, the Whittle Index of arm i is computed as
182"
EDUQATE,0.1941941941941942,"the linear combination of the value associated with taking action on arm i over passivity and the value
183"
EDUQATE,0.19519519519519518,"of associated with semi-actively engaging with members from same group, compared to passivity.
184"
EDUQATE,0.1961961961961962,"To improve the convergence of Q-learning, we incorporate Experience Replay [15]. This involves
185"
EDUQATE,0.1971971971971972,"saving the teacher algorithm’s previous experiences in a replay buffer and drawing mini-batches
186"
EDUQATE,0.1981981981981982,"of samples from this buffer during updates to enhance convergence. In Section 4.1, we prove that
187"
EDUQATE,0.1991991991991992,"EduQate will converge to the optimal policy. However, in practice, we may not have enough episodes
188"
EDUQATE,0.2002002002002002,"to fully train EduQate. Therefore, we propose Experience Replay to mitigate the cold-start problem
189"
EDUQATE,0.2012012012012012,"common in RL applications, a common problem where initial student interactions with sub-optimal
190"
EDUQATE,0.2022022022022022,"teachers can lead to poor learning experiences [3].
191"
EDUQATE,0.2032032032032032,"The pseudo-code is provided in Algorithm 1. Similar to WIQL [5], we employ a ϵ-decay policy that
192"
EDUQATE,0.2042042042042042,"facilitates exploration and learning in the early steps, and proceeds to exploit the learned Q-values in
193"
EDUQATE,0.20520520520520522,"later stages.
194"
ANALYSIS OF EDUQATE,0.2062062062062062,"4.1
Analysis of EduQate
195"
ANALYSIS OF EDUQATE,0.2072072072072072,"In this section, we analyze EduQate closely, and show that EduQate does not alter the optimality
196"
ANALYSIS OF EDUQATE,0.2082082082082082,"guarantees of Q-learning under the constraint that k = 1 (Theorem 1). Our method relies on the
197"
ANALYSIS OF EDUQATE,0.2092092092092092,"assumption that teachers are limited to assign 1 item to the student at each time step. Theorem 2
198"
ANALYSIS OF EDUQATE,0.21021021021021022,"analyzes EduQate under the conditions that k > 1. Since our setting involves the semi-active actions,
199"
ANALYSIS OF EDUQATE,0.21121121121121122,"we should compute Equation 1. To reiterate, ϕi here refers to the group that arm i belongs to, and
200"
ANALYSIS OF EDUQATE,0.2122122122122122,"ϕ−
i is the same group but does not include arm i. If arm i is selected, then all the remaining arms in
201"
ANALYSIS OF EDUQATE,0.2132132132132132,"group ϕ−
i should be semi-active.
202"
ANALYSIS OF EDUQATE,0.21421421421421422,"Theorem 1 Choosing the top arm with the largest λ value in Equation 1 is equivalent to maximizing
203"
ANALYSIS OF EDUQATE,0.21521521521521522,"the cumulative long-term reward.
204"
ANALYSIS OF EDUQATE,0.21621621621621623,"Proof.
According to the approach, we select the arm according to the λ value. Assume arm i has
205"
ANALYSIS OF EDUQATE,0.2172172172172172,"the highest λ value, then for any arm j where j ̸= i, we have
206"
ANALYSIS OF EDUQATE,0.2182182182182182,"λi ≥λj
(2)
According to the definition of λ in Equation 1, we move the negative part to the other side, and the
207"
ANALYSIS OF EDUQATE,0.21921921921921922,"left side becomes:
208"
ANALYSIS OF EDUQATE,0.22022022022022023,"Q(si, ai = 1) +
X"
ANALYSIS OF EDUQATE,0.22122122122122123,"i∈ϕ−
i"
ANALYSIS OF EDUQATE,0.2222222222222222,"(Q(si, ai = 1)) + Q(sj, aj = 0) +
X"
ANALYSIS OF EDUQATE,0.22322322322322322,"j∈ϕ−
j"
ANALYSIS OF EDUQATE,0.22422422422422422,"(Q(sj, aj = 0))"
ANALYSIS OF EDUQATE,0.22522522522522523,"and the right side is similar. There are three cases:
209"
ANALYSIS OF EDUQATE,0.22622622622622623,"• arm i and arm j are not connected, and group ϕi and ϕj has no overlap, i.e., ϕi ∩ϕj = ∅. We add
210
P"
ANALYSIS OF EDUQATE,0.22722722722722724,"z /∈ϕi∧z /∈ϕj
Q(sz, az = 0) on both sides. This denotes the addition of Q(sz, az = 0) for all arm z
211"
ANALYSIS OF EDUQATE,0.22822822822822822,"that are not included in the set of ϕi or ϕj. We have the left side:
212"
ANALYSIS OF EDUQATE,0.22922922922922923,"Q(si, ai = 1) +
X"
ANALYSIS OF EDUQATE,0.23023023023023023,"i∈ϕ−
i"
ANALYSIS OF EDUQATE,0.23123123123123124,"(Q(si, ai = 1)) + Q(sj, aj = 0) +
X"
ANALYSIS OF EDUQATE,0.23223223223223224,"j∈ϕ−
j"
ANALYSIS OF EDUQATE,0.23323323323323322,"(Q(sj, aj = 0)) +
X"
ANALYSIS OF EDUQATE,0.23423423423423423,"z /∈ϕi∧z /∈ϕj
Q(sz, az = 0)"
ANALYSIS OF EDUQATE,0.23523523523523523,"=Q(si, ai = 1) +
X"
ANALYSIS OF EDUQATE,0.23623623623623624,"i∈ϕ−
i"
ANALYSIS OF EDUQATE,0.23723723723723725,"(Q(si, ai = 1)) +
X"
ANALYSIS OF EDUQATE,0.23823823823823823,"j /∈ϕi
(Q(sj, aj = 0))"
ANALYSIS OF EDUQATE,0.23923923923923923,"=Q(s, a = Ii)"
ANALYSIS OF EDUQATE,0.24024024024024024,"(3)
Similarly, we do the same for the right side and thus, the equation 2 becomes
213"
ANALYSIS OF EDUQATE,0.24124124124124124,"Q(s, a = Ii) ≥Q(s, a = Ij)"
ANALYSIS OF EDUQATE,0.24224224224224225,"• arm i and arm j are not connected, but group ϕi and ϕj has overlap, i.e., ϕi ∩ϕj ̸= ∅. In this case,
214"
ANALYSIS OF EDUQATE,0.24324324324324326,"we add
P"
ANALYSIS OF EDUQATE,0.24424424424424424,"z /∈ϕi∧z /∈ϕj
Q(sz, az = 0) −
P"
ANALYSIS OF EDUQATE,0.24524524524524524,"z∈ϕi∩ϕj
Q(sz, az = 0) on both sides.
215"
ANALYSIS OF EDUQATE,0.24624624624624625,"• arm i and arm j are connected, and group ϕi and ϕj has overlap, i.e., ϕi∩ϕj ̸= ∅, and {i, j} ⊂ϕi∩
216"
ANALYSIS OF EDUQATE,0.24724724724724725,"ϕj. This case is similar to the previous one, we add
P"
ANALYSIS OF EDUQATE,0.24824824824824826,"z /∈ϕi∧z /∈ϕj
Q(sz, az = 0) −
P"
ANALYSIS OF EDUQATE,0.24924924924924924,"z∈ϕi∩ϕj
Q(sz, az =
217"
ANALYSIS OF EDUQATE,0.2502502502502503,"0) on both sides.
218"
ANALYSIS OF EDUQATE,0.25125125125125125,"The detailed proof is provided in Appendix B.
□
219"
ANALYSIS OF EDUQATE,0.25225225225225223,"Thus when k = 1, selecting the top arm according to the λ value is equivalent to maximizing the
220"
ANALYSIS OF EDUQATE,0.25325325325325326,"cumulative long-term reward, and is guaranteed to be optimal.
221"
ANALYSIS OF EDUQATE,0.25425425425425424,"Theorem 2 When k > 1, selecting the k arms is a NP-hard problem. The non-asymptotic tight
222"
ANALYSIS OF EDUQATE,0.2552552552552553,"upper bound and non-asymptotic tight lower bound for getting the optimal solution are o(C(n, k))
223"
ANALYSIS OF EDUQATE,0.25625625625625625,"and ω(N), respectively.
224"
ANALYSIS OF EDUQATE,0.25725725725725723,"Proof Sketch.
This problem can be considered as a variant of the knapsack problem. If we disregard
225"
ANALYSIS OF EDUQATE,0.25825825825825827,"the influence of the shared neighbor nodes for two selected arms, then selecting arm i will not
226"
ANALYSIS OF EDUQATE,0.25925925925925924,"influence the future selection of arm j. In such instances, the problem of selecting the k arms is
227"
ANALYSIS OF EDUQATE,0.2602602602602603,"simplified to the traditional 0/1 knapsack problem, a classic NP-hard problem. Therefore, when
228"
ANALYSIS OF EDUQATE,0.26126126126126126,"considering the effect of shared neighbor nodes for two selected arms, this problem is at least as
229"
ANALYSIS OF EDUQATE,0.26226226226226224,"challenging as the 0/1 knapsack problem.
□
230"
ANALYSIS OF EDUQATE,0.26326326326326327,"When k > 1, it is difficult to compute the optimal solution, we provide a heuristic greedy algorithm
231"
ANALYSIS OF EDUQATE,0.26426426426426425,with the complexity of O( (2N−k)∗k
ANALYSIS OF EDUQATE,0.2652652652652653,"2
) in Section C in the appendix.
232"
EXPERIMENT,0.26626626626626626,"5
Experiment
233"
EXPERIMENT,0.2672672672672673,"In this section, we demonstrate the effectiveness of EduQate against benchmark algorithms on
234"
EXPERIMENT,0.2682682682682683,"synthetic students and students derived from a real-world dataset, the Junyi Dataset and the OLI
235"
EXPERIMENT,0.26926926926926925,"Statics dataset. All experiments are run on CPU only. In our experiments, we compare EduQate with
236"
EXPERIMENT,0.2702702702702703,"the following policies:
237"
EXPERIMENT,0.27127127127127126,"Figure 1: Average rewards for the respective algorithms on 3 datasets, averaged across 30 runs.
Shaded regions represent standard error."
EXPERIMENT,0.2722722722722723,"• Threshold Whittle (TW): This algorithm, proposed by [17], utilizes an efficient closed-form
238"
EXPERIMENT,0.2732732732732733,"approach to compute the Whittle index, considering only the pull action as active. It operates under
239"
EXPERIMENT,0.27427427427427425,"the assumption that transition probabilities are known and stands as the state-of-the-art in RMABs.
240"
EXPERIMENT,0.2752752752752753,"• WIQL: This algorithm employs a Q-learning-based Whittle Index approach [5]. It learns Q-values
241"
EXPERIMENT,0.27627627627627627,"using the pull action as the only active strategy and calculates the Whittle Index based on the
242"
EXPERIMENT,0.2772772772772773,"acquired Q-values.
243"
EXPERIMENT,0.2782782782782783,"• Myopic: This strategy disregards the impact of the current action on future rewards, concentrating
244"
EXPERIMENT,0.27927927927927926,"solely on predicted immediate rewards. It selects the arm that maximizes the expected reward at
245"
EXPERIMENT,0.2802802802802803,"the immediate time step.
246"
EXPERIMENT,0.28128128128128127,"• Random: This strategy randomly selects arms with uniform probability, irrespective of the under-
247"
EXPERIMENT,0.2822822822822823,"lying state.
248"
EXPERIMENT,0.2832832832832833,"Inspired by work in healthcare settings [12, 14], we compare the policies by the Intervention Benefit
249"
EXPERIMENT,0.28428428428428426,"(IB), as shown in the following equation:
250"
EXPERIMENT,0.2852852852852853,"IBRandom,EQ(π) = Eπ(R(.)) −ERandom(R(.))"
EXPERIMENT,0.2862862862862863,"EEQ(R(.)) −ERandom(R(.))
(4)"
EXPERIMENT,0.2872872872872873,"where EQ represents EduQate, and Random represents a policy where the arms are selected at random.
251"
EXPERIMENT,0.2882882882882883,"Prior work in educational settings has demonstrated that random policies can yield robust learning
252"
EXPERIMENT,0.28928928928928926,"outcomes through spaced repetition [9, 10]. Therefore, to establish efficacy, successful algorithms
253"
EXPERIMENT,0.2902902902902903,"must demonstrate superiority over random policies. Our chosen metric, IB, effectively compares
254"
EXPERIMENT,0.2912912912912913,"the extent to which a challenger algorithm π outperforms a random policy in comparison to our
255"
EXPERIMENT,0.2922922922922923,"algorithm.
256"
EXPERIMENT SETUP,0.2932932932932933,"5.1
Experiment setup
257"
EXPERIMENT SETUP,0.29429429429429427,"In all experiments, we commence by initializing all arms in state 0 and permit the teacher algorithms
258"
EXPERIMENT SETUP,0.2952952952952953,"to engage with the student for a total of 50 actions, pulling exactly 1 arm (i.e. k = 1) at each time step.
259"
EXPERIMENT SETUP,0.2962962962962963,"Following the completion of these actions, the episode concludes, and the student state is reset. This
260"
EXPERIMENT SETUP,0.2972972972972973,"process is iterated across 800 episodes, for a total of 30 seeds. The datasets used in our experiment
261"
EXPERIMENT SETUP,0.2982982982982983,"are described below:
262"
EXPERIMENT SETUP,0.2992992992992993,"Synthetic dataset. Given the domain-motivated constraints on the transition functions highlighted
263"
EXPERIMENT SETUP,0.3003003003003003,"in Section 3.1, we create a simulator based on N = 50, S ∈{0, 1}, Ntopics = 20. We randomly
264"
EXPERIMENT SETUP,0.3013013013013013,"assign arms to topic groups, and allow arms to be assigned to be more than one topic. Under this
265"
EXPERIMENT SETUP,0.3023023023023023,"method, number of arms under each group may not be equal. For each trial, a new transition matrix
266"
EXPERIMENT SETUP,0.3033033033033033,"is generated to simulate distinct student scenarios.
267"
EXPERIMENT SETUP,0.30430430430430433,"Junyi dataset. The Junyi dataset [7] is an extensive dataset collected from the Junyi Academy 1,
268"
EXPERIMENT SETUP,0.3053053053053053,"an eLearning platform established in 2012 on the basis of the open-source code released by Khan
269"
EXPERIMENT SETUP,0.3063063063063063,1http://www.Junyiacademy.org/
EXPERIMENT SETUP,0.3073073073073073,"Table 1: Comparison of policies on synthetic, Junyi, and OLI datasets. E[R] represents the average
reward obtained in the final episode of training. Statistic after ± represents standard error across 30
trials."
EXPERIMENT SETUP,0.3083083083083083,"Policy
Synthetic
Junyi
OLI
E[IB](%)±
E[R]±
E[IB](%)±
E[R]±
E[IB](%)±
E[R]±
Random
-
26.84 ± 0.46
-
15.82 ± 0.34
-
18.46 ± 0.35
WIQL
−49.03 ± 15.07
24.60 ± 0.43
−26.77 ± 7.39
14.01 ± 0.97
−60.20 ± 19.38
14.33 ± 0.42
Myopic
−3.44 ± 5.81
27.07 ± 0.52
10.74 ± 3.13
16.86 ± 0.356
39.92 ± 12.00
20.51 ± 0.48
TW
37.21 ± 17.02
28.50 ± 0.47
31.284 ± 2.65
15.819 ± 0.34
0.20 ± 9.27
18.07 ± 0.21
EduQate
100.0
34.33 ± 0.49
100.0
24.53 ± 0.31
100.0
25.47 ± 0.47"
EXPERIMENT SETUP,0.30930930930930933,"Academy. In this dataset, there are nearly 26 million student-exercise interactions across 250 000
270"
EXPERIMENT SETUP,0.3103103103103103,"students in its mathematics curriculum. For this experiment, we selected the top 100 exercises with
271"
EXPERIMENT SETUP,0.3113113113113113,"the most student interactions to create our student models. Using our method to generate groups, the
272"
EXPERIMENT SETUP,0.3123123123123123,"resultant EdNetRMAB has N = 100 and Ntopics = 21.
273"
EXPERIMENT SETUP,0.3133133133133133,"OLI Statics dataset. The OLI Statics dataset [4] comprises student interactions with an online
274"
EXPERIMENT SETUP,0.31431431431431434,"Engineering Statics course2. In this dataset, each item is assigned one or more Knowledge Compo-
275"
EXPERIMENT SETUP,0.3153153153153153,"nents (KCs) based on the related topics. After filtering for the top 100 items with the most student
276"
EXPERIMENT SETUP,0.3163163163163163,"interactions, the resultant EdNetRMAB includes N = 100 items and Ntopics = 76 distinct topics.
277"
CREATING STUDENT MODELS,0.3173173173173173,"5.2
Creating student models
278"
CREATING STUDENT MODELS,0.3183183183183183,"In this section, we outline the procedure for generating student models aimed at simulating the
279"
CREATING STUDENT MODELS,0.31931931931931934,"learning process. To clarify, a student model in this context is defined as a set of transition matrices
280"
CREATING STUDENT MODELS,0.3203203203203203,"for all items. These matrices are employed with EdNetRMABs to simulate the learning dynamics.
281"
CREATING STUDENT MODELS,0.3213213213213213,"We employ various strategies to model transitions within the RMAB framework. Active transitions
282"
CREATING STUDENT MODELS,0.32232232232232233,"are determined by assessing the average success rate on a question before and after a learning
283"
CREATING STUDENT MODELS,0.3233233233233233,"intervention. Passive transitions are influenced by difficulty ratings, with more challenging questions
284"
CREATING STUDENT MODELS,0.32432432432432434,"more prone to rapid forgetting. Semi-active transitions, on the other hand, are computed as proportion
285"
CREATING STUDENT MODELS,0.3253253253253253,"of active transition, guided by similarity scores. Here, we provide an outline and the full details can
286"
CREATING STUDENT MODELS,0.3263263263263263,"be found in Appendix D.
287"
CREATING STUDENT MODELS,0.32732732732732733,"Active Transitions. We use data on students’ correct response rate after interacting with an item to
288"
CREATING STUDENT MODELS,0.3283283283283283,"create the transition matrix for action 2, based on the change in correctness rates before and after a
289"
CREATING STUDENT MODELS,0.32932932932932935,"learning intervention.
290"
CREATING STUDENT MODELS,0.3303303303303303,"Passive Transitions. To construct passive transitions for items, we use relative difficulty scores to
291"
CREATING STUDENT MODELS,0.33133133133133136,"determine transitions based on difficulty levels. We assume that higher difficulty correlates with a
292"
CREATING STUDENT MODELS,0.33233233233233234,"greater likelihood of forgetting, resulting in higher failure rates. Specifically, higher difficulty values
293"
CREATING STUDENT MODELS,0.3333333333333333,"correspond to higher P 0
1,0 values, indicating a greater likelihood of forgetting. The transition matrix
294"
CREATING STUDENT MODELS,0.33433433433433435,"for the passive action a = 0 is then randomly generated, with values influenced by difficulty levels.
295"
CREATING STUDENT MODELS,0.3353353353353353,"Semi-active Transitions. To derive semi-active transitions, we use similarity scores between exercises
296"
CREATING STUDENT MODELS,0.33633633633633636,"from the Junyi dataset. We first normalize these scores to the range [0, 1]. Then, for any chosen arm,
297"
CREATING STUDENT MODELS,0.33733733733733734,"we compute its transition matrix under the semi-active action a = 1 as a proportion of its active
298"
CREATING STUDENT MODELS,0.3383383383383383,"action transitions, P 1
0,1 = σ(P 2
0,1), where σ signifies the similarity proportion.
299"
CREATING STUDENT MODELS,0.33933933933933935,"The arm’s transition matrix for the semi-active action varies due to different similarity scores between
300"
CREATING STUDENT MODELS,0.34034034034034033,"pairs in the same group. To address this, we use the average similarity score to determine the
301"
CREATING STUDENT MODELS,0.34134134134134136,"proportion. Since the OLI dataset does not contain similarity ratings, we assume a constant similarity
302"
CREATING STUDENT MODELS,0.34234234234234234,"rating of σ = 0.8 for all pairs.
303"
RESULTS,0.3433433433433433,"6
Results
304"
RESULTS,0.34434434434434436,"The experimental results for the synthetic, Junyi, and OLI datasets are shown in Table 1. We report
305"
RESULTS,0.34534534534534533,"the average intervention benefit IB and final episode rewards from thirty independent runs for five
306"
RESULTS,0.34634634634634637,"algorithms: EduQate, TW, WIQL, Myopic, and Random. EduQate consistently outperforms the other
307"
RESULTS,0.34734734734734735,"policies across all datasets, demonstrating higher intervention benefits and average rewards.
308"
RESULTS,0.3483483483483483,2https://oli.cmu.edu/courses/engineering-statics-open-free/
RESULTS,0.34934934934934936,"Synthetic Network N = 100,
Ntopics = 20"
RESULTS,0.35035035035035034,"Junyi network, abridged to Ntopics = 7
for brevity.
OLI network, N = 100, Ntopics = 76."
RESULTS,0.35135135135135137,"Figure 2: This visualization compares network complexities from our experiments. The synthetic
dataset (left) shows simpler, isolated groups, while the real-world datasets (Junyi, middle; OLI,right)
displays more intricate and interconnected relationships amongst items."
RESULTS,0.35235235235235235,"In terms of IB, we note that all challenger policies do not exceed 50%, indicating two key points.
309"
RESULTS,0.3533533533533533,"First, as noted in prior works [9], our results confirm that random policies in educational settings are
310"
RESULTS,0.35435435435435436,"robust and difficult to surpass, even when algorithms are equipped with knowledge of the learning
311"
RESULTS,0.35535535535535534,"dynamics. Second, our interdependency-aware EduQate performs well over random policies and
312"
RESULTS,0.3563563563563564,"other algorithms, highlighting the importance of considering network effects and interdependencies
313"
RESULTS,0.35735735735735735,"in EdNetRMABs.
314"
RESULTS,0.35835835835835833,"Notably, WIQL, which relies solely on Q-learning for active and passive actions, performs worse
315"
RESULTS,0.35935935935935936,"than a random policy, likely due to misattributing positive network effects to passive actions. Despite
316"
RESULTS,0.36036036036036034,"having access to the transition matrix, TW does not perform as well as the interdependency-aware
317"
RESULTS,0.3613613613613614,"EduQate. While it has demonstrated effectiveness in traditional RMABs, TW weaknesses become
318"
RESULTS,0.36236236236236236,"evident in the current setting, where pulling an arm has wider implications to other arms. Overall,
319"
RESULTS,0.3633633633633634,"EduQate has demonstrated robust and effective performance in maximizing rewards across different
320"
RESULTS,0.36436436436436437,"datasets. Figure 1 shows the average rewards obtained in the final episode for each algorithm.
321"
RESULTS,0.36536536536536535,"Figure 2 provides visualizations of the networks generated from synthetic students and mined from
322"
RESULTS,0.3663663663663664,"real-world datasets. The synthetic dataset produces networks with distinct isolated groups, contrasting
323"
RESULTS,0.36736736736736736,"with the more intricate and interconnected networks from the Junyi and OLI datasets, reflecting
324"
RESULTS,0.3683683683683684,"real-world complexities. Despite these differing topologies and levels of interdependency, EduQate
325"
RESULTS,0.36936936936936937,"performs well under all network setups. In Appendix E.1, we explore the effects of different network
326"
RESULTS,0.37037037037037035,"topologies by varying the number of topics while limiting the membership of each item. We find that
327"
RESULTS,0.3713713713713714,"as network interdependencies are reduced, the network effects diminish, and such EdNetRMABs
328"
RESULTS,0.37237237237237236,"can be approximated to traditional RMABs with independent arms. Under these conditions, our
329"
RESULTS,0.3733733733733734,"algorithm does not perform as well as other baseline policies.
330"
RESULTS,0.3743743743743744,"Finally, an ablation study detailed in Appendix E.2 examines the effectiveness of the replay buffer in
331"
RESULTS,0.37537537537537535,"EduQate. The study shows that the replay buffer helps overcome the cold-start problem, where initial
332"
RESULTS,0.3763763763763764,"learning episodes provide sub-optimal experiences for students [3].
333"
CONCLUSION AND LIMITATIONS,0.37737737737737737,"7
Conclusion and Limitations
334"
CONCLUSION AND LIMITATIONS,0.3783783783783784,"In this paper, we introduced EdNetRMABs to the education setting, a variant of MAB designed to
335"
CONCLUSION AND LIMITATIONS,0.3793793793793794,"model interdependencies in educational content. We also proposed EduQate, a novel Whittle-based
336"
CONCLUSION AND LIMITATIONS,0.38038038038038036,"learning algorithm tailored for EdNetRMABs. Unlike other Whittle-based algorithms, EduQate com-
337"
CONCLUSION AND LIMITATIONS,0.3813813813813814,"putes an optimal policy without requiring knowledge of the transition matrix, while still accounting
338"
CONCLUSION AND LIMITATIONS,0.38238238238238237,"for the network effects of pulling an arm. We demonstrated the guaranteed optimality of a policy
339"
CONCLUSION AND LIMITATIONS,0.3833833833833834,"trained under EduQate and showcased its effectiveness on synthetic and real-world datasets, each
340"
CONCLUSION AND LIMITATIONS,0.3843843843843844,"with its own characteristic.
341"
CONCLUSION AND LIMITATIONS,0.38538538538538536,"Our work assumes that student knowledge states are fully observable and available at all times, which
342"
CONCLUSION AND LIMITATIONS,0.3863863863863864,"is a limitation. Despite this, we believe our work is significant and can inspire further research to
343"
CONCLUSION AND LIMITATIONS,0.38738738738738737,"improve efficiencies in education. For future work, we aim to extend EduQate to handle partially
344"
CONCLUSION AND LIMITATIONS,0.3883883883883884,"observable states and address the cold-start problem in education systems by minimizing the initial
345"
CONCLUSION AND LIMITATIONS,0.3893893893893894,"exploratory phase.
346"
REFERENCES,0.39039039039039036,"References
347"
REFERENCES,0.3913913913913914,"[1] Richard C Atkinson. Ingredients for a theory of instruction. American Psychologist, 27(10):
348"
REFERENCES,0.3923923923923924,"921, 1972.
349"
REFERENCES,0.3933933933933934,"[2] Aqil Zainal Azhar, Avi Segal, and Kobi Gal. Optimizing representations and policies for
350"
REFERENCES,0.3943943943943944,"question sequencing using reinforcement learning. International Educational Data Mining
351"
REFERENCES,0.3953953953953954,"Society, 2022.
352"
REFERENCES,0.3963963963963964,"[3] Jonathan Bassen, Bharathan Balaji, Michael Schaarschmidt, Candace Thille, Jay Painter, Dawn
353"
REFERENCES,0.3973973973973974,"Zimmaro, Alex Games, Ethan Fast, and John C Mitchell. Reinforcement learning for the
354"
REFERENCES,0.3983983983983984,"adaptive scheduling of educational activities. In Proceedings of the 2020 CHI conference on
355"
REFERENCES,0.3993993993993994,"human factors in computing systems, pages 1–12, 2020.
356"
REFERENCES,0.4004004004004004,"[4] Norman Bier.
Oli engineering statics - fall 2011 (114 students), 2011.
URL https://
357"
REFERENCES,0.4014014014014014,"pslcdatashop.web.cmu.edu/DatasetInfo?datasetId=590.
358"
REFERENCES,0.4024024024024024,"[5] Arpita Biswas, Gaurav Aggarwal, Pradeep Varakantham, and Milind Tambe. Learn to intervene:
359"
REFERENCES,0.4034034034034034,"An adaptive learning policy for restless bandits in application to preventive healthcare. arXiv
360"
REFERENCES,0.4044044044044044,"preprint arXiv:2105.07965, 2021.
361"
REFERENCES,0.40540540540540543,"[6] Colton Botta, Avi Segal, and Kobi Gal. Sequencing educational content using diversity aware
362"
REFERENCES,0.4064064064064064,"bandits. 2023.
363"
REFERENCES,0.4074074074074074,"[7] Haw-Shiuan Chang, Hwai-Jung Hsu, and Kuan-Ta Chen. Modeling exercise relationships in
364"
REFERENCES,0.4084084084084084,"e-learning: A unified approach. In EDM, pages 532–535, 2015.
365"
REFERENCES,0.4094094094094094,"[8] Shayan Doroudi, Vincent Aleven, and Emma Brunskill. Robust evaluation matrix: Towards a
366"
REFERENCES,0.41041041041041043,"more principled offline exploration of instructional policies. In Proceedings of the fourth (2017)
367"
REFERENCES,0.4114114114114114,"ACM conference on learning@ scale, pages 3–12, 2017.
368"
REFERENCES,0.4124124124124124,"[9] Shayan Doroudi, Vincent Aleven, and Emma Brunskill. Where’s the reward? a review of rein-
369"
REFERENCES,0.4134134134134134,"forcement learning for instructional sequencing. International Journal of Artificial Intelligence
370"
REFERENCES,0.4144144144144144,"in Education, 29:568–620, 2019.
371"
REFERENCES,0.41541541541541543,"[10] Hermann Ebbinghaus. Über das gedächtnis: untersuchungen zur experimentellen psychologie.
372"
REFERENCES,0.4164164164164164,"Duncker & Humblot, 1885.
373"
REFERENCES,0.4174174174174174,"[11] Derek Green, Thomas Walsh, Paul Cohen, and Yu-Han Chang. Learning a skill-teaching
374"
REFERENCES,0.4184184184184184,"curriculum with dynamic bayes nets. In Proceedings of the AAAI Conference on Artificial
375"
REFERENCES,0.4194194194194194,"Intelligence, volume 25, pages 1648–1654, 2011.
376"
REFERENCES,0.42042042042042044,"[12] Christine Herlihy and John P. Dickerson. Networked restless bandits with positive externalities,
377"
REFERENCES,0.4214214214214214,"2022.
378"
REFERENCES,0.42242242242242245,"[13] Andrew S Lan and Richard G Baraniuk. A contextual bandits framework for personalized
379"
REFERENCES,0.42342342342342343,"learning action selection. In EDM, pages 424–429, 2016.
380"
REFERENCES,0.4244244244244244,"[14] Dexun Li and Pradeep Varakantham. Avoiding starvation of arms in restless multi-armed bandits.
381"
REFERENCES,0.42542542542542544,"In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent
382"
REFERENCES,0.4264264264264264,"Systems, pages 1303–1311, 2023.
383"
REFERENCES,0.42742742742742745,"[15] Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and
384"
REFERENCES,0.42842842842842843,"teaching. Machine learning, 8:293–321, 1992.
385"
REFERENCES,0.4294294294294294,"[16] Keqin Liu and Qing Zhao. Indexability of restless bandit problems and optimality of whittle
386"
REFERENCES,0.43043043043043044,"index for dynamic multichannel access. IEEE Transactions on Information Theory, 56(11):
387"
REFERENCES,0.4314314314314314,"5547–5567, 2010.
388"
REFERENCES,0.43243243243243246,"[17] Aditya Mate, Jackson A Killian, Haifeng Xu, Andrew Perrault, and Milind Tambe. Collapsing
389"
REFERENCES,0.43343343343343343,"bandits and their application to public health interventions. arXiv preprint arXiv:2007.04432,
390"
REFERENCES,0.4344344344344344,"2020.
391"
REFERENCES,0.43543543543543545,"[18] Christos H Papadimitriou and John N Tsitsiklis. The complexity of optimal queueing network
392"
REFERENCES,0.4364364364364364,"control. In Proceedings of IEEE 9th Annual Conference on Structure in Complexity Theory,
393"
REFERENCES,0.43743743743743746,"pages 318–322. IEEE, 1994.
394"
REFERENCES,0.43843843843843844,"[19] Chris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami, Leonidas J
395"
REFERENCES,0.4394394394394394,"Guibas, and Jascha Sohl-Dickstein. Deep knowledge tracing. Advances in neural information
396"
REFERENCES,0.44044044044044045,"processing systems, 28, 2015.
397"
REFERENCES,0.44144144144144143,"[20] Yundi Qian, Chao Zhang, Bhaskar Krishnamachari, and Milind Tambe. Restless poachers:
398"
REFERENCES,0.44244244244244246,"Handling exploration-exploitation tradeoffs in security domains. In Proceedings of the 2016
399"
REFERENCES,0.44344344344344344,"International Conference on Autonomous Agents & Multiagent Systems, pages 123–131, 2016.
400"
REFERENCES,0.4444444444444444,"[21] Avi Segal, Yossi Ben David, Joseph Jay Williams, Kobi Gal, and Yaar Shalom. Combining
401"
REFERENCES,0.44544544544544545,"difficulty ranking with multi-armed bandits to sequence educational content. In Artificial
402"
REFERENCES,0.44644644644644643,"Intelligence in Education: 19th International Conference, AIED 2018, London, UK, June 27–30,
403"
REFERENCES,0.44744744744744747,"2018, Proceedings, Part II 19, pages 317–321. Springer, 2018.
404"
REFERENCES,0.44844844844844844,"[22] Shitian Shen, Markel Sanz Ausin, Behrooz Mostafavi, and Min Chi. Improving learning &
405"
REFERENCES,0.4494494494494494,"reducing time: A constrained action-based reinforcement learning approach. In Proceedings of
406"
REFERENCES,0.45045045045045046,"the 26th conference on user modeling, adaptation and personalization, pages 43–51, 2018.
407"
REFERENCES,0.45145145145145144,"[23] Anni Siren and Vassilios Tzerpos. Automatic learning path creation using oer: a systematic
408"
REFERENCES,0.45245245245245247,"literature mapping. IEEE Transactions on Learning Technologies, 2022.
409"
REFERENCES,0.45345345345345345,"[24] Utkarsh Upadhyay, Abir De, and Manuel Gomez Rodriguez. Deep reinforcement learning of
410"
REFERENCES,0.4544544544544545,"marked temporal point processes. Advances in Neural Information Processing Systems, 31,
411"
REFERENCES,0.45545545545545546,"2018.
412"
REFERENCES,0.45645645645645644,"[25] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3):279–292, 1992.
413"
REFERENCES,0.4574574574574575,"[26] Peter Whittle. Restless bandits: Activity allocation in a changing world. Journal of applied
414"
REFERENCES,0.45845845845845845,"probability, pages 287–298, 1988.
415"
REFERENCES,0.4594594594594595,"Appendix/Supplementary Materials
416"
REFERENCES,0.46046046046046046,"A
Table of Notations
417"
REFERENCES,0.46146146146146144,Table 2: Notations
REFERENCES,0.4624624624624625,"Notation
Description"
REFERENCES,0.46346346346346345,"N, Ntopics
N: number of arms in EdNetRMABs; Ntopics: number of topic groups"
REFERENCES,0.4644644644644645,"st
i
st
i: state of arm i at time step t. 1: learned, 0: unlearned."
REFERENCES,0.46546546546546547,"at
i
at
i: action of arm i at time step t. 0: passive action, 1: semi-active action, 2: active action."
REFERENCES,0.46646646646646645,"s, a
s, a: joint state vector and joint action vector of EdNetRMABs."
REFERENCES,0.4674674674674675,"ϕi, ϕ−
i
ϕi: the set of arms that includes the arm i and its connected neighbors, ϕ−
i : ϕi that exclude arm i."
REFERENCES,0.46846846846846846,"P i,a
s,s′
P i,a
s,s′ is the probability of transition from state s to s′ when arm i is taking action a."
REFERENCES,0.4694694694694695,"Qi(si, ai)
Qi(si, ai) is the state-action value function for the arm i when taking action ai with state si."
REFERENCES,0.47047047047047047,"Vi(si)
The value function for arm i at the state si."
REFERENCES,0.47147147147147145,"B
Proof for the theorem
418"
REFERENCES,0.4724724724724725,"We rewrite the theorem here for ease of explanation.
419"
REFERENCES,0.47347347347347346,"Theorem 3 Choose top arms according to the λ value in Equation 1 is equivalent to maximize the
420"
REFERENCES,0.4744744744744745,"cumulative long-term reward.
421"
REFERENCES,0.4754754754754755,"Proof.
According to the approach, we select the arm according to the λ value. Assume arm i has
422"
REFERENCES,0.47647647647647645,"the highest λ value, then for any arm j, where i ̸= j, we have
423"
REFERENCES,0.4774774774774775,λi ≥λj
REFERENCES,0.47847847847847846,"Q(si, ai = 1) −Q(si, ai = 0) +
X"
REFERENCES,0.4794794794794795,"i∈ϕ−
i"
REFERENCES,0.4804804804804805,"(Q(si, ai = 1) −Q(si, ai = 0)) ≥Q(sj, aj = 1) −Q(sj, aj = 0) +
X"
REFERENCES,0.48148148148148145,"j∈ϕ−
j"
REFERENCES,0.4824824824824825,"(Q(sj, aj = 1) −Q(sj, aj = 0))"
REFERENCES,0.48348348348348347,"Q(si, ai = 1) +
X"
REFERENCES,0.4844844844844845,"i∈ϕ−
i"
REFERENCES,0.4854854854854855,"(Q(si, ai = 1)) + Q(sj, aj = 0) +
X"
REFERENCES,0.4864864864864865,"j∈ϕ−
j"
REFERENCES,0.4874874874874875,"(Q(sj, aj = 0)) ≥Q(sj, aj = 1) +
X"
REFERENCES,0.48848848848848847,"j∈ϕ−
j"
REFERENCES,0.4894894894894895,"(Q(sj, aj = 1)) + Q(si, ai = 0) +
X"
REFERENCES,0.4904904904904905,"i∈ϕ−
i"
REFERENCES,0.4914914914914915,"(Q(si, ai = 0))"
REFERENCES,0.4924924924924925,"(5)
There are two cases:
424"
REFERENCES,0.4934934934934935,"• arm i and arm j are not connected, and group ϕi and ϕj has no overlap, i.e., ϕi ∩ϕj = ∅. We
425 add
P"
REFERENCES,0.4944944944944945,"z /∈ϕi∧z /∈ϕj
Q(sz, az = 0) on both sides, we can have the left side:
426"
REFERENCES,0.4954954954954955,"Q(si, ai = 1) +
X"
REFERENCES,0.4964964964964965,"i∈ϕ−
i"
REFERENCES,0.4974974974974975,"(Q(si, ai = 1)) + Q(sj, aj = 0) +
X"
REFERENCES,0.4984984984984985,"j∈ϕ−
j"
REFERENCES,0.4994994994994995,"(Q(sj, aj = 0)) +
X"
REFERENCES,0.5005005005005005,"z /∈ϕi∧z /∈ϕj
Q(sz, az = 0)"
REFERENCES,0.5015015015015015,"=Q(si, ai = 1) +
X"
REFERENCES,0.5025025025025025,"i∈ϕ−
i"
REFERENCES,0.5035035035035035,"(Q(si, ai = 1)) +
X"
REFERENCES,0.5045045045045045,"j /∈ϕ−
i"
REFERENCES,0.5055055055055055,"(Q(sj, aj = 0))"
REFERENCES,0.5065065065065065,"=Q(s, a = Ii)"
REFERENCES,0.5075075075075075,"(6)
Similarly, the right side becomes
427"
REFERENCES,0.5085085085085085,"Q(sj, aj = 1) +
X"
REFERENCES,0.5095095095095095,"j∈ϕ−
j"
REFERENCES,0.5105105105105106,"(Q(sj, aj = 1)) +
X"
REFERENCES,0.5115115115115115,"i/∈ϕj
(Q(si, ai = 0)) = Q(s, a = Ij)
(7)"
REFERENCES,0.5125125125125125,"Thus, the equation 2 becomes
428"
REFERENCES,0.5135135135135135,"Q(s, a = Ii) ≥Q(s, a = Ij)
(8)"
REFERENCES,0.5145145145145145,"• arm i and arm j are not connected, but group ϕi and ϕj has overlap, i.e., ϕi ∩ϕj ̸= ∅. In this
429"
REFERENCES,0.5155155155155156,"case, we add
P"
REFERENCES,0.5165165165165165,"z /∈ϕi∧z /∈ϕj
Q(sz, az = 0) −P
z∈ϕi∩ϕj Q(sz, az = 0) on both sides, we can have the
430"
REFERENCES,0.5175175175175175,"left side:
431"
REFERENCES,0.5185185185185185,"Q(si, ai = 1) +
X"
REFERENCES,0.5195195195195195,"i∈ϕ−
i"
REFERENCES,0.5205205205205206,"(Q(si, ai = 1)) + Q(sj, aj = 0) +
X"
REFERENCES,0.5215215215215215,"j∈ϕ−
j"
REFERENCES,0.5225225225225225,"(Q(sj, aj = 0)) +
X"
REFERENCES,0.5235235235235235,"z /∈ϕi∧z /∈ϕj
Q(sz, az = 0) −
X"
REFERENCES,0.5245245245245245,"z∈ϕi∩ϕj
Q(sz, az = 0)"
REFERENCES,0.5255255255255256,"=Q(si, ai = 1) +
X"
REFERENCES,0.5265265265265265,"i∈ϕ−
i"
REFERENCES,0.5275275275275275,"(Q(si, ai = 1)) +
X"
REFERENCES,0.5285285285285285,"j∈ϕj
(Q(sj, aj = 0)) +
X"
REFERENCES,0.5295295295295295,"z /∈ϕi∧z /∈ϕj
Q(sz, az = 0) −
X"
REFERENCES,0.5305305305305306,"z∈ϕi∩ϕj
Q(sz, az = 0)"
REFERENCES,0.5315315315315315,"=Q(si, ai = 1) +
X"
REFERENCES,0.5325325325325325,"i∈ϕ−
i"
REFERENCES,0.5335335335335335,"(Q(si, ai = 1)) +
X"
REFERENCES,0.5345345345345346,"j /∈ϕ−
i"
REFERENCES,0.5355355355355356,"(Q(sj, aj = 0))"
REFERENCES,0.5365365365365365,"=Q(s, a = Ii)"
REFERENCES,0.5375375375375375,"(9)
Similarly, the right side becomes
432"
REFERENCES,0.5385385385385385,"Q(sj, aj = 1) +
X"
REFERENCES,0.5395395395395396,"j∈ϕ−
j"
REFERENCES,0.5405405405405406,"(Q(sj, aj = 1)) +
X"
REFERENCES,0.5415415415415415,"i/∈ϕj
(Q(si, ai = 0)) = Q(s, a = Ij)
(10)"
REFERENCES,0.5425425425425425,"• arm i and arm j are connected, and group ϕi and ϕj has overlap, i.e., ϕi ∩ϕj ̸= ∅, and
433"
REFERENCES,0.5435435435435435,"{i, j} ⊂ϕi ∩ϕj. This case is similar to the previous one, we add
P"
REFERENCES,0.5445445445445446,"z /∈ϕi∧z /∈ϕj
Q(sz, az = 0) −
434 P"
REFERENCES,0.5455455455455456,"z∈ϕi∩ϕj Q(sz, az = 0) on both sides, we can have the left side: Q(s, a = Ii) and the right side
435"
REFERENCES,0.5465465465465466,"Q(s, a = Ij).
436 □
437"
REFERENCES,0.5475475475475475,"We show that, using Theorem 1, selecting the top arms according to the λ value is guaranteed to
438"
REFERENCES,0.5485485485485485,"maximize the cumulative long-term reward, thus proving it to be optimal.
439"
REFERENCES,0.5495495495495496,"However when it comes to the case where k > 1, selecting the top k arms according to the λ value
440"
REFERENCES,0.5505505505505506,"is not guaranteed to be optimal. Let the Φ denote the set of arms that are selected, i.e., ai = 2 if
441"
REFERENCES,0.5515515515515516,"i ∈Φ. Because once the arm i is added to the selected arm set Φ, the benefit of selecting arm j will
442"
REFERENCES,0.5525525525525525,"also be influenced if the arm j has the shared connected neighbor arms with arm i, i.e., ϕi ∩ϕj ̸= ∅.
443"
REFERENCES,0.5535535535535535,"To this end, finding the optimal solution is difficult, as we need to list all the possible solution sets.
444"
REFERENCES,0.5545545545545546,"The non-asymptotic tight upper bound and non-asymptotic tight lower bound for getting the optimal
445"
REFERENCES,0.5555555555555556,"solution are o(C(n, k)) and ω(N), respectively.
446"
REFERENCES,0.5565565565565566,"We provide the proof for Theorem 2: Proof.
When considering the influence of the shared neighbor
447"
REFERENCES,0.5575575575575575,"nodes for two selected arms, then selecting arm i will influence the future benefit of selecting arm
448"
REFERENCES,0.5585585585585585,"j if arm i and arm j have the overlapped neighbor nodes, i.e., ϕi ∩ϕj ̸= ∅. This is because the
449"
REFERENCES,0.5595595595595596,"calculation of λj, as some arms z ∈ϕi ∩ϕj already receive the semi-active action a = 1 due to the
450"
REFERENCES,0.5605605605605606,"selection of arm i, the subsequent selection of arm j would not double introduce the benefit from
451"
REFERENCES,0.5615615615615616,"those arms z who already included in ϕi. However, if the top k arms ranked according to their λ
452"
REFERENCES,0.5625625625625625,"value do not have any overlaps in their connected neighbor nodes, i.e, ϕi ∩ϕj = ∅for ∀i, j, where
453"
REFERENCES,0.5635635635635635,"arm i and arm j are top k arms according to λ value. We can directly add those top k arms to the
454"
REFERENCES,0.5645645645645646,"action set Φ, and the solution is guaranteed to be optimal. Then we have the non-asymptotic tight
455"
REFERENCES,0.5655655655655656,"lower bound for getting the optimal solution which is ω(N). Otherwise, if the top k arms ranked
456"
REFERENCES,0.5665665665665666,"according to their λ value have any overlaps in their connected neighbor nodes, to get the optimal
457"
REFERENCES,0.5675675675675675,"solutions, we need to list all possible combinations of the k arms, which have the C(n, k) cases, and
458"
REFERENCES,0.5685685685685685,"computing the corresponding sum of the λ value. In this case, we can derive that the non-asymptotic
459"
REFERENCES,0.5695695695695696,"tight upper bound for getting the optimal solution is o(C(n, k)).
□
460"
REFERENCES,0.5705705705705706,"C
Greedy algorithm when k > 1
461"
REFERENCES,0.5715715715715716,"When k > 1, it is difficult to compute the optimal solution as we might list all possible solutions, and
462"
REFERENCES,0.5725725725725725,"the complexity is O(C(n, k)), Thus we provide a heuristic greedy algorithm to find the near-optimal
463"
REFERENCES,0.5735735735735735,"solutions. The process to decide the selected arm set Φ is as follows:
464"
REFERENCES,0.5745745745745746,"1. We first compute the independent λ value for each arm i, where i ∈{1, . . . , N}, where
465"
REFERENCES,0.5755755755755756,"λi = Q(si, ai = 1) −Q(si, ai = 0) + P"
REFERENCES,0.5765765765765766,"j∈ϕ−
i (Q(si, ai = 2) −Q(si, ai = 0));
466"
REFERENCES,0.5775775775775776,"2. We add the arm with the top λ value to the set Φ;
467"
REFERENCES,0.5785785785785785,"3. We recompute the λ value for the each arm, note that we will remove Q(sj, aj) in the λ
468"
REFERENCES,0.5795795795795796,"equation if j ∈Φ or j ∈ϕj for ∀i ∈Φ;
469"
REFERENCES,0.5805805805805806,"4. we add the arm with the top λ value to the set Φ, and repeat the step 3 and 4 until we add k
470"
REFERENCES,0.5815815815815816,"arms to set Φ.
471"
REFERENCES,0.5825825825825826,"The intuition of such a heuristic greedy algorithm is to add the arm that maximizes the marginal gain
472"
REFERENCES,0.5835835835835835,to the action. And the complexity for the greedy algorithm is O( (2N−k)∗k
REFERENCES,0.5845845845845846,"2
).
473"
REFERENCES,0.5855855855855856,"D
Generating Student Models from Junyi and OLI Dataset
474"
REFERENCES,0.5865865865865866,"In this section, we describe the features in Junyi and OLI dataset which we use in developing the
475"
REFERENCES,0.5875875875875876,"transition matrices.
476"
REFERENCES,0.5885885885885885,"The datasets contain the following features which we use in various aspects to generate the student
477"
REFERENCES,0.5895895895895896,"models and the network:
478"
REFERENCES,0.5905905905905906,"• Topic & Knowledge Component Classification: Items are classified into topics (Junyi) or
479"
REFERENCES,0.5915915915915916,"KCs (OLI). This classification is employed to group items and establish the initial network.
480"
REFERENCES,0.5925925925925926,"• Similarity: The Junyi dataset offers expert ratings for exercise similarity, enabling a nuanced
481"
REFERENCES,0.5935935935935935,"approach to form richer group memberships. High similarity scores group exercises together,
482"
REFERENCES,0.5945945945945946,"irrespective of topic tags.
483"
REFERENCES,0.5955955955955956,"• Difficulty: The Junyi dataset provides expert ratings to determine the relative difficulty of
484"
REFERENCES,0.5965965965965966,"exercise pairs. In the OLI dataset, we use the overall correct response rate as a measure of
485"
REFERENCES,0.5975975975975976,"difficulty.
486"
REFERENCES,0.5985985985985987,"• Rate of Correctness: By analyzing student-exercise interactions, we calculate the frequency
487"
REFERENCES,0.5995995995995996,"of correct answers for each question, offering insights into the improvement of knowledge
488"
REFERENCES,0.6006006006006006,"over time.
489"
REFERENCES,0.6016016016016016,"D.1
Active Transitions
490"
REFERENCES,0.6026026026026026,"Junyi Dataset
The Junyi dataset contains earned_proficiency feature which indicates if the
491"
REFERENCES,0.6036036036036037,"student has achieved mastery of the topic based on Khan Academy’s algorithm3. Thus, we take the
492"
REFERENCES,0.6046046046046046,"number of attempts before earned_proficiency=1 as P 2
0,1, and the errors made during mastery as
493"
REFERENCES,0.6056056056056056,"P 2
1,0.
494"
REFERENCES,0.6066066066066066,"OLI Dataset
We possess records of students’ accuracy on quiz questions after studying specific
495"
REFERENCES,0.6076076076076076,"topics. To derive the transition matrix for the student with the corresponding action 2, we utilize the
496"
REFERENCES,0.6086086086086087,"change in correctness rate before and after a learning intervention.
497"
REFERENCES,0.6096096096096096,"Given that proportion of correct attempts at time t as at, then at+1 = P 2
0,1(1 −at) + P 2
1,1(at). We
498"
REFERENCES,0.6106106106106106,"use a linear regressor to estimate the respective P 2, constraining it to produce positive values and
499"
REFERENCES,0.6116116116116116,"clipping the values to 0.99 when required.
500"
REFERENCES,0.6126126126126126,"D.2
Passive Transitions
501"
REFERENCES,0.6136136136136137,"To construct passive transitions for exercises, we utilize relative difficulty scores to determine
502"
REFERENCES,0.6146146146146146,"transitions based on difficulty levels. We operate under the assumption that the difficulty of an
503"
REFERENCES,0.6156156156156156,"exercise is linked to its likelihood of being forgotten, thereby resulting in a higher failure rate. More
504"
REFERENCES,0.6166166166166166,"precisely, higher difficulty values of an exercise correspond to higher P 0
1,0 values, indicating a greater
505"
REFERENCES,0.6176176176176176,"likelihood of forgetting. The transition matrix for the passive action a = 0 is then randomly generated,
506"
REFERENCES,0.6186186186186187,"with the values influenced by the difficulty levels.
507"
REFERENCES,0.6196196196196196,"D.3
Semi-active Transitions
508"
REFERENCES,0.6206206206206206,"To derive semi-active transitions, the Junyi dataset contains similarity scores between two distinct
509"
REFERENCES,0.6216216216216216,"exercises, quantifying their similarity on a 9-point Likert scale. Once the transition matrices are
510"
REFERENCES,0.6226226226226226,"computed under the active action a = 2 for all arms, we proceed to calculate the transition matrix
511"
REFERENCES,0.6236236236236237,"3http://david-hu.com/2011/11/02/how-khan-academy-is-using-machine-learning-to-assess-student-
mastery.html"
REFERENCES,0.6246246246246246,"for the semi-active action a = 1. This involves normalizing the similarity scores to the range [0, 1],
512"
REFERENCES,0.6256256256256256,"denoted as σ. For any chosen arm/topic, we can then compute its neighbor’s transition matrix under
513"
REFERENCES,0.6266266266266266,"the semi-active action a = 1 with P 1
0,1 = σ(P 2
0,1), where σ signifies the similarity proportion. It is
514"
REFERENCES,0.6276276276276276,"worth noting that an arm’s transition matrix for the semi-active action varies due to different neighbors
515"
REFERENCES,0.6286286286286287,"being selected — different neighbors correspond to different similarity scores.
516"
REFERENCES,0.6296296296296297,"To address this, we can store the transition matrix of semi-active actions for different neighbor
517"
REFERENCES,0.6306306306306306,"selection scenarios, preserving the flexibility of our algorithm. In this work, for simplicity, we opt
518"
REFERENCES,0.6316316316316316,"not to distinguish the impact of different neighbors being selected. Instead, we calculate the average
519"
REFERENCES,0.6326326326326326,"similarity for all arms in a group average them, and use the resultant average as σ.
520"
REFERENCES,0.6336336336336337,"For the OLI Statics dataset, we use a constant value of σ = 0.8 since there are no similarity scores
521"
REFERENCES,0.6346346346346347,"available.
522"
REFERENCES,0.6356356356356356,"E
Additional Experiment Results and Discussion
523"
REFERENCES,0.6366366366366366,"E.1
Comparing Different Network Setups
524"
REFERENCES,0.6376376376376376,"Figure 3: Average rewards for the respective algorithms, on the last episode of training. Note that as
Ntopics increase, the network effects are reduced, and most algorithms are not better than a random
policy."
REFERENCES,0.6386386386386387,"Table 3: Comparison of policies on synthetic dataset, with different network setups. Note that that as
Ntopics increase, the reliability of any algorithms decreases, as seen by the standard deviations of
their average IB. EduQate- here refers to the EduQate algorithm without replay buffer."
REFERENCES,0.6396396396396397,"Ntopics
POLICY
E[IB] (%) (±) 20"
REFERENCES,0.6406406406406406,"WIQL
-57.9 ± 13.1
MYOPIC
0.24 ± 8.2
TW
32.6 ± 7.0
EDUQATE-
100.0 30"
REFERENCES,0.6416416416416416,"WIQL
-292 ± 1162
MYOPIC
180 ± 600
TW
122 ± 277
EDUQATE-
100 40"
REFERENCES,0.6426426426426426,"WIQL
307 ± 1069
MYOPIC
212 ± 526
TW
4.34 ± 1124
EDUQATE-
100"
REFERENCES,0.6436436436436437,"We present the results for different network setups in Table 3. We note that as the number of topics
525"
REFERENCES,0.6446446446446447,"approach the number of arms (i.e. Ntopics = {30, 40}, all algorithms perform in a highly unstable
526"
REFERENCES,0.6456456456456456,"manner, as reflected in the standard deviations presented. We emphasizes here that the performance
527"
REFERENCES,0.6466466466466466,"of EduQate is dependent on the quality of the network it is working on, and tends to thrive in more
528"
REFERENCES,0.6476476476476476,"complex, yet realistic scenarios, such as the Junyi dataset presented in Figure 2. We present an
529"
REFERENCES,0.6486486486486487,"example of a graph generated when Ntopics = 40 in Figure 4, where we notice that many arms do
530"
REFERENCES,0.6496496496496497,"not belong to a group. Under this network, the EdNetRMAB can be approximated to a traditional
531"
REFERENCES,0.6506506506506506,"RMAB, where the arms are independent of each other."
REFERENCES,0.6516516516516516,"Figure 4: Synthetic network when Ntopics = 40. Note that some arms are without group members,
and do not receive benefits from networks. Node colors represent topic groups. 532"
REFERENCES,0.6526526526526526,"Figure 5: Average rewards across 800 episodes of training, across 30 seeds. EduQate- (orange) refers
to the EduQate algorithm without replay buffer."
REFERENCES,0.6536536536536537,"E.2
Ablation of Replay Buffer
533"
REFERENCES,0.6546546546546547,"Table 4: Comparison of EduQate with and without (EduQate-) Experience Replay Buffer policies
across different datasets. Results reported are of the final episode of training."
REFERENCES,0.6556556556556556,"POLICY
E[IB] (%) ±"
REFERENCES,0.6566566566566566,"SYNTHETIC
JUNYI
OLI"
REFERENCES,0.6576576576576577,"EDUQATE-
104.74 ± 32.56
76.90 ± 4.72
107.30 ± 11.77
EDUQATE
100.0
100.0
100.0"
REFERENCES,0.6586586586586587,"POLICY
E[R] ±"
REFERENCES,0.6596596596596597,"SYNTHETIC
JUNYI
OLI"
REFERENCES,0.6606606606606606,"EDUQATE-
32.032 ± 0.469
22.133 ± 0.544
25.16 ± 0.432
EDUQATE
34.331 ± 0.489
24.527 ± 0.314
25.468 ± 0.469"
REFERENCES,0.6616616616616616,"We investigate the importance of the Experience Replay buffer in EduQate, as shown in Figure 5 and
534"
REFERENCES,0.6626626626626627,"Table 4. For the Simulated and Junyi datasets, EduQate without Experience Replay (EduQate-) does
535"
REFERENCES,0.6636636636636637,"not achieve the performance levels of the full EduQate algorithm within 800 episodes, highlighting the
536"
REFERENCES,0.6646646646646647,"importance of methods that aid Q-learning convergence. In real-world applications, slow convergence
537"
REFERENCES,0.6656656656656657,"can result in students experiencing a curriculum similar to a random policy, leading to sub-optimal
538"
REFERENCES,0.6666666666666666,"learning experiences during the early stages. This issue is known as the cold-start problem [3].
539"
REFERENCES,0.6676676676676677,"Future work in EdNetRMABs should explore methods to overcome cold-start problems and improve
540"
REFERENCES,0.6686686686686687,"convergence in Q-learning-based methods.
541"
REFERENCES,0.6696696696696697,"F
Q-Learning
542"
REFERENCES,0.6706706706706707,"Q-learning [25] is a popular reinforcement learning method that enables an agent to learn optimal
543"
REFERENCES,0.6716716716716716,"actions in an environment by iteratively updating its estimate of state-action value, Q(s, a), based on
544"
REFERENCES,0.6726726726726727,"the rewards it receives. The objective, therefore, to learn Q∗(s, a) for each state-action pair of an
545"
REFERENCES,0.6736736736736737,"MDP, given by:
546"
REFERENCES,0.6746746746746747,"Q∗(s, a) = r(s) +
X"
REFERENCES,0.6756756756756757,"s′∈S
P(s, a, s′) · V ∗(s′)"
REFERENCES,0.6766766766766766,"where V ∗(s′) is the optimal expected value of a state, is given by:
547"
REFERENCES,0.6776776776776777,"V ∗(s) = maxa∈A(Q(s, a))"
REFERENCES,0.6786786786786787,"Q-learning estimates Q∗through repeated interactions with the environment. At each time step t,
548"
REFERENCES,0.6796796796796797,"the agent takes an action a using its current estimate of Q values and current state s, thus received a
549"
REFERENCES,0.6806806806806807,"reward of r(s) and new state s′. Q-learning then updates the current estimate using the following:
550"
REFERENCES,0.6816816816816816,"Qnew(s, a) ←(1 −α) · Qold(s, a)
+ α · (r(s)"
REFERENCES,0.6826826826826827,"+ γ · maxa∈AQold(s′, a))
(11)"
REFERENCES,0.6836836836836837,"where α ∈[0, 1] is the learning rate that controls updates, and γ is the discount on future rewards
551"
REFERENCES,0.6846846846846847,"associated with the MDP.
552"
REFERENCES,0.6856856856856857,"G
Experiment Details and Hyperparameters
553"
REFERENCES,0.6866866866866866,"Category
Parameter
Value"
REFERENCES,0.6876876876876877,"Replay buffer
buffer_size
10000
batch_size
64"
REFERENCES,0.6886886886886887,"WIQL/EduQate
γ
0.95
α
0.1
Table 5: Hyperparameters for Replay Buffer and Q-learning"
REFERENCES,0.6896896896896897,"H
NeurIPS Paper Checklist
554"
CLAIMS,0.6906906906906907,"1. Claims
555"
CLAIMS,0.6916916916916916,"Question: Do the main claims made in the abstract and introduction accurately reflect the
556"
CLAIMS,0.6926926926926927,"paper’s contributions and scope?
557"
CLAIMS,0.6936936936936937,"Answer: [Yes]
558"
CLAIMS,0.6946946946946947,"Justification: We summarize our contributions and provide the scope of the paper in the
559"
ABSTRACT,0.6956956956956957,"abstract and introduction.
560"
ABSTRACT,0.6966966966966966,"Guidelines:
561"
ABSTRACT,0.6976976976976977,"• The answer NA means that the abstract and introduction do not include the claims
562"
ABSTRACT,0.6986986986986987,"made in the paper.
563"
ABSTRACT,0.6996996996996997,"• The abstract and/or introduction should clearly state the claims made, including the
564"
ABSTRACT,0.7007007007007007,"contributions made in the paper and important assumptions and limitations. A No or
565"
ABSTRACT,0.7017017017017017,"NA answer to this question will not be perceived well by the reviewers.
566"
ABSTRACT,0.7027027027027027,"• The claims made should match theoretical and experimental results, and reflect how
567"
ABSTRACT,0.7037037037037037,"much the results can be expected to generalize to other settings.
568"
ABSTRACT,0.7047047047047047,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
569"
ABSTRACT,0.7057057057057057,"are not attained by the paper.
570"
LIMITATIONS,0.7067067067067067,"2. Limitations
571"
LIMITATIONS,0.7077077077077077,"Question: Does the paper discuss the limitations of the work performed by the authors?
572"
LIMITATIONS,0.7087087087087087,"Answer: [Yes]
573"
LIMITATIONS,0.7097097097097097,"Justification: Limitations were discussed in the final section.
574"
LIMITATIONS,0.7107107107107107,"Guidelines:
575"
LIMITATIONS,0.7117117117117117,"• The answer NA means that the paper has no limitation while the answer No means that
576"
LIMITATIONS,0.7127127127127127,"the paper has limitations, but those are not discussed in the paper.
577"
LIMITATIONS,0.7137137137137137,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
578"
LIMITATIONS,0.7147147147147147,"• The paper should point out any strong assumptions and how robust the results are to
579"
LIMITATIONS,0.7157157157157157,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
580"
LIMITATIONS,0.7167167167167167,"model well-specification, asymptotic approximations only holding locally). The authors
581"
LIMITATIONS,0.7177177177177178,"should reflect on how these assumptions might be violated in practice and what the
582"
LIMITATIONS,0.7187187187187187,"implications would be.
583"
LIMITATIONS,0.7197197197197197,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
584"
LIMITATIONS,0.7207207207207207,"only tested on a few datasets or with a few runs. In general, empirical results often
585"
LIMITATIONS,0.7217217217217218,"depend on implicit assumptions, which should be articulated.
586"
LIMITATIONS,0.7227227227227228,"• The authors should reflect on the factors that influence the performance of the approach.
587"
LIMITATIONS,0.7237237237237237,"For example, a facial recognition algorithm may perform poorly when image resolution
588"
LIMITATIONS,0.7247247247247247,"is low or images are taken in low lighting. Or a speech-to-text system might not be
589"
LIMITATIONS,0.7257257257257257,"used reliably to provide closed captions for online lectures because it fails to handle
590"
LIMITATIONS,0.7267267267267268,"technical jargon.
591"
LIMITATIONS,0.7277277277277278,"• The authors should discuss the computational efficiency of the proposed algorithms
592"
LIMITATIONS,0.7287287287287287,"and how they scale with dataset size.
593"
LIMITATIONS,0.7297297297297297,"• If applicable, the authors should discuss possible limitations of their approach to
594"
LIMITATIONS,0.7307307307307307,"address problems of privacy and fairness.
595"
LIMITATIONS,0.7317317317317318,"• While the authors might fear that complete honesty about limitations might be used by
596"
LIMITATIONS,0.7327327327327328,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
597"
LIMITATIONS,0.7337337337337337,"limitations that aren’t acknowledged in the paper. The authors should use their best
598"
LIMITATIONS,0.7347347347347347,"judgment and recognize that individual actions in favor of transparency play an impor-
599"
LIMITATIONS,0.7357357357357357,"tant role in developing norms that preserve the integrity of the community. Reviewers
600"
LIMITATIONS,0.7367367367367368,"will be specifically instructed to not penalize honesty concerning limitations.
601"
THEORY ASSUMPTIONS AND PROOFS,0.7377377377377378,"3. Theory Assumptions and Proofs
602"
THEORY ASSUMPTIONS AND PROOFS,0.7387387387387387,"Question: For each theoretical result, does the paper provide the full set of assumptions and
603"
THEORY ASSUMPTIONS AND PROOFS,0.7397397397397397,"a complete (and correct) proof?
604"
THEORY ASSUMPTIONS AND PROOFS,0.7407407407407407,"Answer: [Yes]
605"
THEORY ASSUMPTIONS AND PROOFS,0.7417417417417418,"Justification: Proofs are provided in Appendix 4.1.
606"
THEORY ASSUMPTIONS AND PROOFS,0.7427427427427428,"Guidelines:
607"
THEORY ASSUMPTIONS AND PROOFS,0.7437437437437437,"• The answer NA means that the paper does not include theoretical results.
608"
THEORY ASSUMPTIONS AND PROOFS,0.7447447447447447,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
609"
THEORY ASSUMPTIONS AND PROOFS,0.7457457457457457,"referenced.
610"
THEORY ASSUMPTIONS AND PROOFS,0.7467467467467468,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
611"
THEORY ASSUMPTIONS AND PROOFS,0.7477477477477478,"• The proofs can either appear in the main paper or the supplemental material, but if
612"
THEORY ASSUMPTIONS AND PROOFS,0.7487487487487487,"they appear in the supplemental material, the authors are encouraged to provide a short
613"
THEORY ASSUMPTIONS AND PROOFS,0.7497497497497497,"proof sketch to provide intuition.
614"
THEORY ASSUMPTIONS AND PROOFS,0.7507507507507507,"• Inversely, any informal proof provided in the core of the paper should be complemented
615"
THEORY ASSUMPTIONS AND PROOFS,0.7517517517517518,"by formal proofs provided inappendix or supplemental material.
616"
THEORY ASSUMPTIONS AND PROOFS,0.7527527527527528,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
617"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7537537537537538,"4. Experimental Result Reproducibility
618"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7547547547547547,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
619"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7557557557557557,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
620"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7567567567567568,"of the paper (regardless of whether the code and data are provided or not)?
621"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7577577577577578,"Answer: [Yes]
622"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7587587587587588,"Justification: Experriment details are provided in both the main body and the appendix.
623"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7597597597597597,"Guidelines:
624"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7607607607607607,"• The answer NA means that the paper does not include experiments.
625"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7617617617617618,"• If the paper includes experiments, a No answer to this question will not be perceived
626"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7627627627627628,"well by the reviewers: Making the paper reproducible is important, regardless of
627"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7637637637637638,"whether the code and data are provided or not.
628"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7647647647647647,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
629"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7657657657657657,"to make their results reproducible or verifiable.
630"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7667667667667668,"• Depending on the contribution, reproducibility can be accomplished in various ways.
631"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7677677677677678,"For example, if the contribution is a novel architecture, describing the architecture fully
632"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7687687687687688,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
633"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7697697697697697,"be necessary to either make it possible for others to replicate the model with the same
634"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7707707707707707,"dataset, or provide access to the model. In general. releasing code and data is often
635"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7717717717717718,"one good way to accomplish this, but reproducibility can also be provided via detailed
636"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7727727727727728,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
637"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7737737737737738,"of a large language model), releasing of a model checkpoint, or other means that are
638"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7747747747747747,"appropriate to the research performed.
639"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7757757757757757,"• While NeurIPS does not require releasing code, the conference does require all submis-
640"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7767767767767768,"sions to provide some reasonable avenue for reproducibility, which may depend on the
641"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7777777777777778,"nature of the contribution. For example
642"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7787787787787788,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
643"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7797797797797797,"to reproduce that algorithm.
644"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7807807807807807,"(b) If the contribution is primarily a new model architecture, the paper should describe
645"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7817817817817818,"the architecture clearly and fully.
646"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7827827827827828,"(c) If the contribution is a new model (e.g., a large language model), then there should
647"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7837837837837838,"either be a way to access this model for reproducing the results or a way to reproduce
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7847847847847848,"the model (e.g., with an open-source dataset or instructions for how to construct
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7857857857857858,"the dataset).
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7867867867867868,"(d) We recognize that reproducibility may be tricky in some cases, in which case
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7877877877877878,"authors are welcome to describe the particular way they provide for reproducibility.
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7887887887887888,"In the case of closed-source models, it may be that access to the model is limited in
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7897897897897898,"some way (e.g., to registered users), but it should be possible for other researchers
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7907907907907908,"to have some path to reproducing or verifying the results.
655"
OPEN ACCESS TO DATA AND CODE,0.7917917917917918,"5. Open access to data and code
656"
OPEN ACCESS TO DATA AND CODE,0.7927927927927928,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
657"
OPEN ACCESS TO DATA AND CODE,0.7937937937937938,"tions to faithfully reproduce the main experimental results, as described in supplemental
658"
OPEN ACCESS TO DATA AND CODE,0.7947947947947948,"material?
659"
OPEN ACCESS TO DATA AND CODE,0.7957957957957958,"Answer: [Yes]
660"
OPEN ACCESS TO DATA AND CODE,0.7967967967967968,"Justification: Code and the transition matrices are provided as supplementary materials.
661"
OPEN ACCESS TO DATA AND CODE,0.7977977977977978,"Guidelines:
662"
OPEN ACCESS TO DATA AND CODE,0.7987987987987988,"• The answer NA means that paper does not include experiments requiring code.
663"
OPEN ACCESS TO DATA AND CODE,0.7997997997997998,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
664"
OPEN ACCESS TO DATA AND CODE,0.8008008008008008,"public/guides/CodeSubmissionPolicy) for more details.
665"
OPEN ACCESS TO DATA AND CODE,0.8018018018018018,"• While we encourage the release of code and data, we understand that this might not be
666"
OPEN ACCESS TO DATA AND CODE,0.8028028028028028,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
667"
OPEN ACCESS TO DATA AND CODE,0.8038038038038038,"including code, unless this is central to the contribution (e.g., for a new open-source
668"
OPEN ACCESS TO DATA AND CODE,0.8048048048048048,"benchmark).
669"
OPEN ACCESS TO DATA AND CODE,0.8058058058058059,"• The instructions should contain the exact command and environment needed to run to
670"
OPEN ACCESS TO DATA AND CODE,0.8068068068068068,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
671"
OPEN ACCESS TO DATA AND CODE,0.8078078078078078,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
672"
OPEN ACCESS TO DATA AND CODE,0.8088088088088088,"• The authors should provide instructions on data access and preparation, including how
673"
OPEN ACCESS TO DATA AND CODE,0.8098098098098098,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
674"
OPEN ACCESS TO DATA AND CODE,0.8108108108108109,"• The authors should provide scripts to reproduce all experimental results for the new
675"
OPEN ACCESS TO DATA AND CODE,0.8118118118118118,"proposed method and baselines. If only a subset of experiments are reproducible, they
676"
OPEN ACCESS TO DATA AND CODE,0.8128128128128128,"should state which ones are omitted from the script and why.
677"
OPEN ACCESS TO DATA AND CODE,0.8138138138138138,"• At submission time, to preserve anonymity, the authors should release anonymized
678"
OPEN ACCESS TO DATA AND CODE,0.8148148148148148,"versions (if applicable).
679"
OPEN ACCESS TO DATA AND CODE,0.8158158158158159,"• Providing as much information as possible in supplemental material (appended to the
680"
OPEN ACCESS TO DATA AND CODE,0.8168168168168168,"paper) is recommended, but including URLs to data and code is permitted.
681"
OPEN ACCESS TO DATA AND CODE,0.8178178178178178,"6. Experimental Setting/Details
682"
OPEN ACCESS TO DATA AND CODE,0.8188188188188188,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
683"
OPEN ACCESS TO DATA AND CODE,0.8198198198198198,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
684"
OPEN ACCESS TO DATA AND CODE,0.8208208208208209,"results?
685"
OPEN ACCESS TO DATA AND CODE,0.8218218218218218,"Answer: [Yes]
686"
OPEN ACCESS TO DATA AND CODE,0.8228228228228228,"Justification: Relevant details are provided in the main body, as well as the appendix.
687"
OPEN ACCESS TO DATA AND CODE,0.8238238238238238,"Guidelines:
688"
OPEN ACCESS TO DATA AND CODE,0.8248248248248248,"• The answer NA means that the paper does not include experiments.
689"
OPEN ACCESS TO DATA AND CODE,0.8258258258258259,"• The experimental setting should be presented in the core of the paper to a level of detail
690"
OPEN ACCESS TO DATA AND CODE,0.8268268268268268,"that is necessary to appreciate the results and make sense of them.
691"
OPEN ACCESS TO DATA AND CODE,0.8278278278278278,"• The full details can be provided either with the code, in appendix, or as supplemental
692"
OPEN ACCESS TO DATA AND CODE,0.8288288288288288,"material.
693"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8298298298298298,"7. Experiment Statistical Significance
694"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8308308308308309,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
695"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8318318318318318,"information about the statistical significance of the experiments?
696"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8328328328328328,"Answer: [Yes]
697"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8338338338338338,"Justification: In our experiments, we report and display the standard error across all seeds.
698"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8348348348348348,"Guidelines:
699"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8358358358358359,"• The answer NA means that the paper does not include experiments.
700"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8368368368368369,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
701"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8378378378378378,"dence intervals, or statistical significance tests, at least for the experiments that support
702"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8388388388388388,"the main claims of the paper.
703"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8398398398398398,"• The factors of variability that the error bars are capturing should be clearly stated (for
704"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8408408408408409,"example, train/test split, initialization, random drawing of some parameter, or overall
705"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8418418418418419,"run with given experimental conditions).
706"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8428428428428428,"• The method for calculating the error bars should be explained (closed form formula,
707"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8438438438438438,"call to a library function, bootstrap, etc.)
708"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8448448448448449,"• The assumptions made should be given (e.g., Normally distributed errors).
709"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8458458458458459,"• It should be clear whether the error bar is the standard deviation or the standard error
710"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8468468468468469,"of the mean.
711"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8478478478478478,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
712"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8488488488488488,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
713"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8498498498498499,"of Normality of errors is not verified.
714"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8508508508508509,"• For asymmetric distributions, the authors should be careful not to show in tables or
715"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8518518518518519,"figures symmetric error bars that would yield results that are out of range (e.g. negative
716"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8528528528528528,"error rates).
717"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8538538538538538,"• If error bars are reported in tables or plots, The authors should explain in the text how
718"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8548548548548549,"they were calculated and reference the corresponding figures or tables in the text.
719"
EXPERIMENTS COMPUTE RESOURCES,0.8558558558558559,"8. Experiments Compute Resources
720"
EXPERIMENTS COMPUTE RESOURCES,0.8568568568568569,"Question: For each experiment, does the paper provide sufficient information on the com-
721"
EXPERIMENTS COMPUTE RESOURCES,0.8578578578578578,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
722"
EXPERIMENTS COMPUTE RESOURCES,0.8588588588588588,"the experiments?
723"
EXPERIMENTS COMPUTE RESOURCES,0.8598598598598599,"Answer: [Yes]
724"
EXPERIMENTS COMPUTE RESOURCES,0.8608608608608609,"Justification: The current paper only requires CPU-level of compute and is mentioned in the
725"
EXPERIMENTS COMPUTE RESOURCES,0.8618618618618619,"Experiment section.
726"
EXPERIMENTS COMPUTE RESOURCES,0.8628628628628628,"Guidelines:
727"
EXPERIMENTS COMPUTE RESOURCES,0.8638638638638638,"• The answer NA means that the paper does not include experiments.
728"
EXPERIMENTS COMPUTE RESOURCES,0.8648648648648649,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
729"
EXPERIMENTS COMPUTE RESOURCES,0.8658658658658659,"or cloud provider, including relevant memory and storage.
730"
EXPERIMENTS COMPUTE RESOURCES,0.8668668668668669,"• The paper should provide the amount of compute required for each of the individual
731"
EXPERIMENTS COMPUTE RESOURCES,0.8678678678678678,"experimental runs as well as estimate the total compute.
732"
EXPERIMENTS COMPUTE RESOURCES,0.8688688688688688,"• The paper should disclose whether the full research project required more compute
733"
EXPERIMENTS COMPUTE RESOURCES,0.8698698698698699,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
734"
EXPERIMENTS COMPUTE RESOURCES,0.8708708708708709,"didn’t make it into the paper).
735"
CODE OF ETHICS,0.8718718718718719,"9. Code Of Ethics
736"
CODE OF ETHICS,0.8728728728728729,"Question: Does the research conducted in the paper conform, in every respect, with the
737"
CODE OF ETHICS,0.8738738738738738,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
738"
CODE OF ETHICS,0.8748748748748749,"Answer: [Yes]
739"
CODE OF ETHICS,0.8758758758758759,"Justification: All datasets used were anonymized by the respective authors.
740"
CODE OF ETHICS,0.8768768768768769,"Guidelines:
741"
CODE OF ETHICS,0.8778778778778779,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
742"
CODE OF ETHICS,0.8788788788788788,"• If the authors answer No, they should explain the special circumstances that require a
743"
CODE OF ETHICS,0.8798798798798799,"deviation from the Code of Ethics.
744"
CODE OF ETHICS,0.8808808808808809,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
745"
CODE OF ETHICS,0.8818818818818819,"eration due to laws or regulations in their jurisdiction).
746"
BROADER IMPACTS,0.8828828828828829,"10. Broader Impacts
747"
BROADER IMPACTS,0.8838838838838838,"Question: Does the paper discuss both potential positive societal impacts and negative
748"
BROADER IMPACTS,0.8848848848848849,"societal impacts of the work performed?
749"
BROADER IMPACTS,0.8858858858858859,"Answer: [Yes]
750"
BROADER IMPACTS,0.8868868868868869,"Justification: The current work has positive implications for applied machine learning in
751"
BROADER IMPACTS,0.8878878878878879,"education settings, and is discussed in the Introduction section. As far as we can see, we
752"
BROADER IMPACTS,0.8888888888888888,"don’t think there are negative impacts for education.
753"
BROADER IMPACTS,0.8898898898898899,"Guidelines:
754"
BROADER IMPACTS,0.8908908908908909,"• The answer NA means that there is no societal impact of the work performed.
755"
BROADER IMPACTS,0.8918918918918919,"• If the authors answer NA or No, they should explain why their work has no societal
756"
BROADER IMPACTS,0.8928928928928929,"impact or why the paper does not address societal impact.
757"
BROADER IMPACTS,0.8938938938938938,"• Examples of negative societal impacts include potential malicious or unintended uses
758"
BROADER IMPACTS,0.8948948948948949,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
759"
BROADER IMPACTS,0.8958958958958959,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
760"
BROADER IMPACTS,0.8968968968968969,"groups), privacy considerations, and security considerations.
761"
BROADER IMPACTS,0.8978978978978979,"• The conference expects that many papers will be foundational research and not tied
762"
BROADER IMPACTS,0.8988988988988988,"to particular applications, let alone deployments. However, if there is a direct path to
763"
BROADER IMPACTS,0.8998998998998999,"any negative applications, the authors should point it out. For example, it is legitimate
764"
BROADER IMPACTS,0.9009009009009009,"to point out that an improvement in the quality of generative models could be used to
765"
BROADER IMPACTS,0.9019019019019019,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
766"
BROADER IMPACTS,0.9029029029029029,"that a generic algorithm for optimizing neural networks could enable people to train
767"
BROADER IMPACTS,0.9039039039039038,"models that generate Deepfakes faster.
768"
BROADER IMPACTS,0.9049049049049049,"• The authors should consider possible harms that could arise when the technology is
769"
BROADER IMPACTS,0.9059059059059059,"being used as intended and functioning correctly, harms that could arise when the
770"
BROADER IMPACTS,0.9069069069069069,"technology is being used as intended but gives incorrect results, and harms following
771"
BROADER IMPACTS,0.9079079079079079,"from (intentional or unintentional) misuse of the technology.
772"
BROADER IMPACTS,0.908908908908909,"• If there are negative societal impacts, the authors could also discuss possible mitigation
773"
BROADER IMPACTS,0.9099099099099099,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
774"
BROADER IMPACTS,0.9109109109109109,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
775"
BROADER IMPACTS,0.9119119119119119,"feedback over time, improving the efficiency and accessibility of ML).
776"
SAFEGUARDS,0.9129129129129129,"11. Safeguards
777"
SAFEGUARDS,0.913913913913914,"Question: Does the paper describe safeguards that have been put in place for responsible
778"
SAFEGUARDS,0.914914914914915,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
779"
SAFEGUARDS,0.9159159159159159,"image generators, or scraped datasets)?
780"
SAFEGUARDS,0.9169169169169169,"Answer: [NA]
781"
SAFEGUARDS,0.9179179179179179,"Justification: The current paper does not release any new assets.
782"
SAFEGUARDS,0.918918918918919,"Guidelines:
783"
SAFEGUARDS,0.91991991991992,"• The answer NA means that the paper poses no such risks.
784"
SAFEGUARDS,0.9209209209209209,"• Released models that have a high risk for misuse or dual-use should be released with
785"
SAFEGUARDS,0.9219219219219219,"necessary safeguards to allow for controlled use of the model, for example by requiring
786"
SAFEGUARDS,0.9229229229229229,"that users adhere to usage guidelines or restrictions to access the model or implementing
787"
SAFEGUARDS,0.923923923923924,"safety filters.
788"
SAFEGUARDS,0.924924924924925,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
789"
SAFEGUARDS,0.9259259259259259,"should describe how they avoided releasing unsafe images.
790"
SAFEGUARDS,0.9269269269269269,"• We recognize that providing effective safeguards is challenging, and many papers do
791"
SAFEGUARDS,0.9279279279279279,"not require this, but we encourage authors to take this into account and make a best
792"
SAFEGUARDS,0.928928928928929,"faith effort.
793"
LICENSES FOR EXISTING ASSETS,0.92992992992993,"12. Licenses for existing assets
794"
LICENSES FOR EXISTING ASSETS,0.9309309309309309,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
795"
LICENSES FOR EXISTING ASSETS,0.9319319319319319,"the paper, properly credited and are the license and terms of use explicitly mentioned and
796"
LICENSES FOR EXISTING ASSETS,0.9329329329329329,"properly respected?
797"
LICENSES FOR EXISTING ASSETS,0.933933933933934,"Answer: [Yes]
798"
LICENSES FOR EXISTING ASSETS,0.934934934934935,"Justification: Code [17] and datasets [7, 4] were appropriately cited.
799"
LICENSES FOR EXISTING ASSETS,0.9359359359359359,"Guidelines:
800"
LICENSES FOR EXISTING ASSETS,0.9369369369369369,"• The answer NA means that the paper does not use existing assets.
801"
LICENSES FOR EXISTING ASSETS,0.9379379379379379,"• The authors should cite the original paper that produced the code package or dataset.
802"
LICENSES FOR EXISTING ASSETS,0.938938938938939,"• The authors should state which version of the asset is used and, if possible, include a
803"
LICENSES FOR EXISTING ASSETS,0.93993993993994,"URL.
804"
LICENSES FOR EXISTING ASSETS,0.9409409409409409,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
805"
LICENSES FOR EXISTING ASSETS,0.9419419419419419,"• For scraped data from a particular source (e.g., website), the copyright and terms of
806"
LICENSES FOR EXISTING ASSETS,0.9429429429429429,"service of that source should be provided.
807"
LICENSES FOR EXISTING ASSETS,0.943943943943944,"• If assets are released, the license, copyright information, and terms of use in the
808"
LICENSES FOR EXISTING ASSETS,0.944944944944945,"package should be provided. For popular datasets, paperswithcode.com/datasets
809"
LICENSES FOR EXISTING ASSETS,0.9459459459459459,"has curated licenses for some datasets. Their licensing guide can help determine the
810"
LICENSES FOR EXISTING ASSETS,0.9469469469469469,"license of a dataset.
811"
LICENSES FOR EXISTING ASSETS,0.9479479479479479,"• For existing datasets that are re-packaged, both the original license and the license of
812"
LICENSES FOR EXISTING ASSETS,0.948948948948949,"the derived asset (if it has changed) should be provided.
813"
LICENSES FOR EXISTING ASSETS,0.94994994994995,"• If this information is not available online, the authors are encouraged to reach out to
814"
LICENSES FOR EXISTING ASSETS,0.950950950950951,"the asset’s creators.
815"
NEW ASSETS,0.9519519519519519,"13. New Assets
816"
NEW ASSETS,0.9529529529529529,"Question: Are new assets introduced in the paper well documented and is the documentation
817"
NEW ASSETS,0.953953953953954,"provided alongside the assets?
818"
NEW ASSETS,0.954954954954955,"Answer: [NA]
819"
NEW ASSETS,0.955955955955956,"Justification: [NA]
820"
NEW ASSETS,0.9569569569569569,"Guidelines:
821"
NEW ASSETS,0.9579579579579579,"• The answer NA means that the paper does not release new assets.
822"
NEW ASSETS,0.958958958958959,"• Researchers should communicate the details of the dataset/code/model as part of their
823"
NEW ASSETS,0.95995995995996,"submissions via structured templates. This includes details about training, license,
824"
NEW ASSETS,0.960960960960961,"limitations, etc.
825"
NEW ASSETS,0.9619619619619619,"• The paper should discuss whether and how consent was obtained from people whose
826"
NEW ASSETS,0.9629629629629629,"asset is used.
827"
NEW ASSETS,0.963963963963964,"• At submission time, remember to anonymize your assets (if applicable). You can either
828"
NEW ASSETS,0.964964964964965,"create an anonymized URL or include an anonymized zip file.
829"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.965965965965966,"14. Crowdsourcing and Research with Human Subjects
830"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9669669669669669,"Question: For crowdsourcing experiments and research with human subjects, does the paper
831"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9679679679679679,"include the full text of instructions given to participants and screenshots, if applicable, as
832"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.968968968968969,"well as details about compensation (if any)?
833"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.96996996996997,"Answer: [NA]
834"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.970970970970971,"Justification: [NA]
835"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9719719719719719,"Guidelines:
836"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.972972972972973,"• The answer NA means that the paper does not involve crowdsourcing nor research with
837"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.973973973973974,"human subjects.
838"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974974974974975,"• Including this information in the supplemental material is fine, but if the main contribu-
839"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.975975975975976,"tion of the paper involves human subjects, then as much detail as possible should be
840"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9769769769769769,"included in the main paper.
841"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.977977977977978,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
842"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.978978978978979,"or other labor should be paid at least the minimum wage in the country of the data
843"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.97997997997998,"collector.
844"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.980980980980981,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
845"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9819819819819819,"Subjects
846"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.982982982982983,"Question: Does the paper describe potential risks incurred by study participants, whether
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.983983983983984,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
848"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.984984984984985,"approvals (or an equivalent approval/review based on the requirements of your country or
849"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985985985985986,"institution) were obtained?
850"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986986986986987,"Answer: [NA]
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987987987987988,"Justification: [NA]
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988988988988989,"Guidelines:
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.98998998998999,"• The answer NA means that the paper does not involve crowdsourcing nor research with
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.990990990990991,"human subjects.
855"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991991991991992,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
856"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.992992992992993,"may be required for any human subjects research. If you obtained IRB approval, you
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.993993993993994,"should clearly state this in the paper.
858"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.994994994994995,"• We recognize that the procedures for this may vary significantly between institutions
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.995995995995996,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996996996996997,"guidelines for their institution.
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.997997997997998,"• For initial submissions, do not include any information that would break anonymity (if
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998998998998999,"applicable), such as the institution conducting the review.
863"
