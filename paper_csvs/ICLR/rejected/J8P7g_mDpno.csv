Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0033003300330033004,"While larger neural models are pushing the boundaries on what deep learning can
achieve, often more weights are needed to train models rather than to run inference
for tasks. This paper explores how can we remove weights during training without
impacting the ability to traverse parameter spaces (or the set of all weights a model
can take). We ﬁrst discover weights pruned for inference do not learn meaningful
information over time but their short-term behavior is necessary for training. We
then provide recommendations for removing such weights in order to train sparse
neural models. With these recommendations, we attain competitive scores across
dozens of deep learning workloads. We also ﬁnd sparse models are tolerant of
structures targeting existing hardware, opening avenues for training and inference
acceleration. Our work encourages research to explore beyond massive neural
models being used today."
INTRODUCTION,0.006600660066006601,"1
INTRODUCTION"
INTRODUCTION,0.009900990099009901,"In the area of deep learning, increasing the size of neural models has led to dramatic advances (Hes-
tness et al., 2017; Kaplan et al., 2020; Henighan et al., 2020), motivating training with hundreds of
billions of parameters (Brown et al., 2020; Fedus et al., 2021). However, larger models incur higher
memory costs and runtimes, which limits training and inference tasks that can run on existing hard-
ware (Thompson et al., 2020). Nevertheless, larger models have been shown to train faster (Li et al.,
2020) and better (Kaplan et al., 2020), which drives their adoption."
INTRODUCTION,0.013201320132013201,"An area of research that seeks to compensate the increasing costs of model size is sparsity (Hoeﬂer
et al., 2021), which moves weights to zero so they can be discarded from storage or computations.
Sparsity emerges as a promising option to reduce costs of inference (Luo et al., 2017; Allen-Zhu
et al., 2019), as neural models are capable of performing tasks on sizes smaller than they were
trained for (Narang et al., 2017; Renda et al., 2020). However, since overparameterization remains
critical for training, sparsity observes limited success there (Lee et al., 2019; Frankle & Carbin,
2019; Wang et al., 2020; Tanaka et al., 2020; Frankle et al., 2021; Bellec et al., 2018; Mocanu et al.,
2018; Dettmers & Zettlemoyer, 2019; Evci et al., 2020a; Jayakumar et al., 2020)."
INTRODUCTION,0.0165016501650165,"The ability to run inference but not training with fewer weights points to a problem with search (or
how well can models navigate parameter spaces during training) rather than model capacity (LeCun
et al., 1990). Literature shows adding more weights to training creates extra degrees of freedom that
form new paths for optimization, rendering neural model training more effective (Evci et al., 2020b).
While it is widely believed that having more weights facilitates escape from critical points during
training, the question remains whether these weights are always needed (e.g., learn meaningful
representations) or could be eventually discarded if they only help with the optimization process."
INTRODUCTION,0.019801980198019802,"In this paper we explore how to remove weights from training without restricting its ability to ef-
fectively traverse parameter spaces towards good solutions. Surprisingly, we discover that weights
pruned at inference time are transient and do not learn representations in the long-term. As such,
these weights do not need to be stored throughout all of training, in contrast to weights that learn
meaningful information for inference and thus need to be stored. The pruned weights however do
learn short-term representations that provide the model extra degrees to escape regions of bad sad-
dle points (Kawaguchi, 2016) and high error plateaus (Dauphin et al., 2014) that can slow down
learning, after which they can be discarded."
INTRODUCTION,0.0231023102310231,"With this understanding we propose new recommendations for training sparse models that consol-
idate recent advances in sparsity literature: (1) rewire weights to expand the parameter space, (2)"
INTRODUCTION,0.026402640264026403,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0297029702970297,"update gradients of all weights to encourage alternate paths of optimization, and (3) discard inter-
mediate representations to reduce noise from gradient accumulations. Following these recommen-
dations, we show that sparse neural models achieve competitive results on dozens of deep learning
workloads, even when satisfying constraints needed to accelerate training and inference using Sparse
Tensor Cores (Mishra et al., 2021) in NVIDIA GPUs."
INTRODUCTION,0.033003300330033,"The paper is organized as follows. In Section 2 we introduce a metric for learning and investigate the
duration which weights retain meaningful information throughout training. We then devise recom-
mendations for removing weights that do not learn long-term representations when training sparse
models in Section 3. Section 4 summarizes our methodology for sparse training. Section 5 shows
sparse models constructed in this fashion perform competitively across a plethora of deep learning
workloads and can target current hardware accelerators (Mishra et al., 2021) for training and in-
ference. Section 6 relates to prior research. In Section 7 we conclude with directions for future
work."
METRIC FOR LEARNING,0.036303630363036306,"2
METRIC FOR LEARNING"
METRIC FOR LEARNING,0.039603960396039604,"The crux of this paper is to understand whether weights that are being added for training are learn-
ing meaningful representations or solely being used to facilitate the optimization process. For this
purpose, we use correlations to measure the duration that weights in a neural model retain (or learn)
information that is relevant for inference."
METRIC FOR LEARNING,0.0429042904290429,"Correlations determine how similar are two sets of data. When applied to weights learned over time,
correlations measure how weights in future training time steps are similar to past ones. We postulate
correlations represent the degree with which weights in a neural model learn. Weights that learn
meaningful representations for inference are repeatedly reinforced throughout training, and their
values form distinct temporal patterns that are correlated to past values. In contrast, weights that do
not learn (and can be removed during inference) exhibit random behavior and have no dependence
over time."
METRIC FOR LEARNING,0.0462046204620462,"We measure correlations between a pair of time series x and y using the Pearson coefﬁcient (Rodgers
& Nicewander, 1988): ρx,y ≡E[(x −µx)(y −µy)]/σxσy, where µ is the mean, σ the standard
deviation, and E[·] the expected value. Coefﬁcients of value +1 denote two series have identical
trends, 0 indicates the series are random, and −1 represents series with opposite behavior. We treat
each individual weight as a time series w ∈{w1, w2, . . . , wt} and compute Pearson coefﬁcients be-
tween windows x ∈{w1, w2, . . . , wt−τ} and y ∈{wτ, wτ+1, . . . , wt}, representing the similarity
between the weights and their values at some future time τ. Correlations naturally decay with τ
since temporal patterns are less likely to persist over long durations. An important metric we adopt
is the time ∆after which weights no longer correlate to their future values, namely when ρ = 0."
METRIC FOR LEARNING,0.04950495049504951,"Figure 1: Time (∆) it takes for weights to decorrelate (normalized by the total number of training
steps) as a function of weight magnitudes obtained after training. Points correspond to the me-
dian ∆over weight bins sampled from a single neural layer. Shaded regions distinguish between
weights that are needed (white) or can be removed (yellow) at inference time. From left to right:
Transformer-XL (Language Modeling), GNMT (Machine Translation), ResNet50 (Image Classiﬁ-
cation)."
METRIC FOR LEARNING,0.052805280528052806,Under review as a conference paper at ICLR 2022
METRIC FOR LEARNING,0.056105610561056105,"Figure 1 plots the time (∆) after which weights become completely uncorrelated with their past
values. We observe that the correlation length grows with weight magnitudes obtained after training.
While weights needed for inference exhibit correlations that persist across a signiﬁcant fraction of
training, weights that are not needed have short-term correlations and behave randomly over short
periods. As a result, long-term correlations of large weights are characteristic of learning, signifying
repeated reinforcement along the weight direction. On the other hand, the near-random motion of
small weights suggests they don’t learn useful representations and can be removed during inference."
METRIC FOR LEARNING,0.0594059405940594,"Because weights removable during inference are absent of correlations over long periods of training,
we conjecture they do not need to be stored all the time. However, we still need pruned weights
during training due to their short-term correlations. More speciﬁcally, their short-term interactions
are critical for training to take different paths for optimization, facilitating escape from bad saddle
points (Kawaguchi, 2016) or high error plateaus (Dauphin et al., 2014) that can slow down learning.
Combining this understanding, in later sections we discover their learned values can be periodically
destroyed throughout training (once correlations fall to zero) with no impact on accuracy, which
paves the road for sparse training."
RECOMMENDATIONS FOR SPARSE TRAINING,0.0627062706270627,"3
RECOMMENDATIONS FOR SPARSE TRAINING"
RECOMMENDATIONS FOR SPARSE TRAINING,0.066006600660066,"This section outlines recommendations for training sparse neural models. While many of these
ideas have been circling in sparsity literature, they remain misunderstood which hinders adoption.
We use the understanding from the previous section to provide a deeper intuition in these cases. To
summarize, we uncover the following steps to train sparse neural models:"
RECOMMENDATIONS FOR SPARSE TRAINING,0.06930693069306931,(1) Frequently rewire weights that participate in training.
RECOMMENDATIONS FOR SPARSE TRAINING,0.07260726072607261,(2) Perform gradient updates for weights that do not participate in training.
RECOMMENDATIONS FOR SPARSE TRAINING,0.07590759075907591,"(3) Induce exploitation (e.g., stop rewiring after some amount of time, reset non-participating
weights to zero, or regularize them)."
REWIRING OF NEURAL WEIGHTS,0.07920792079207921,"3.1
REWIRING OF NEURAL WEIGHTS"
REWIRING OF NEURAL WEIGHTS,0.08250825082508251,"Sparse models are commonly trained by rewiring (or sampling) a subset of weights from a neural
model according to some criteria (magnitude, sign, gradients, etc), allowing them to learn which
weights are needed for inference. Some works rewire weights every training iteration (Bellec et al.,
2018; Wortsman et al., 2019; Raihan & Aamodt, 2020; Jayakumar et al., 2020; Zhou et al., 2021),
while others rewire every hundreds of training steps (Evci et al., 2020a) or after an entire pass
through the data (Mocanu et al., 2018; Dettmers & Zettlemoyer, 2019). Since there are no clear
reasons behind these choices, an important question becomes how often must weights be rewired so
neural models can learn effectively."
REWIRING OF NEURAL WEIGHTS,0.0858085808580858,"We study the effects rewiring rates have on accuracy when training sparse models of various sizes d,
where d denotes the fraction of total weights being used in the sparse model. Figure 2 (left) plots the
task error (or accuracy difference between dense and sparse models) as a function of the number of
training steps r taken between rewirings. We ﬁnd rewiring frequency does not matter when sparsity
is low or moderate (d ∼0.5), but errors increase with r as models get sparser (d ≪0.5). Therefore,
weights should be rewired frequently (within a few training steps) to avoid losing accuracy."
REWIRING OF NEURAL WEIGHTS,0.0891089108910891,"Correlations can help us explain why training improves when we rewire sparse neural models. While
weights that do not learn long-term representations can be discarded during training, they have short-
term correlations that create extra degrees of freedom for escaping critical points (Evci et al., 2020b).
But this escape only occurs when weights are rewired. For example, neural models always operate
on reduced parameter spaces if weights being used remain unchanged throughout training (r →∞),
and expand exactly once after going through the entire data when they are rewired every epoch. By
swapping between weights that participate (are used in forward and backward propagations) and do
not participate (are ﬁxed to zero during training), training can take different paths for optimization
using the short-term correlations, which helps neural models explore the parameter space."
REWIRING OF NEURAL WEIGHTS,0.0924092409240924,Under review as a conference paper at ICLR 2022
REWIRING OF NEURAL WEIGHTS,0.09570957095709572,"100
101
102
103"
REWIRING OF NEURAL WEIGHTS,0.09900990099009901,Rewiring Steps r 0.0 0.5 1.0 1.5 2.0 2.5 3.0
REWIRING OF NEURAL WEIGHTS,0.10231023102310231,Task Error
REWIRING OF NEURAL WEIGHTS,0.10561056105610561,"d = 0.5
d = 0.4
d = 0.3"
REWIRING OF NEURAL WEIGHTS,0.10891089108910891,"10−2
10−1
100"
REWIRING OF NEURAL WEIGHTS,0.11221122112211221,Gradient Scaling s 0 1 2 3 4
REWIRING OF NEURAL WEIGHTS,0.11551155115511551,"d = 0.25
d = 0.2
d = 0.1"
REWIRING OF NEURAL WEIGHTS,0.1188118811881188,"100
101
102
103"
REWIRING OF NEURAL WEIGHTS,0.12211221122112212,Resetting Steps z 0 1 2 3 4
REWIRING OF NEURAL WEIGHTS,0.1254125412541254,"Figure 2: Investigations into various aspects of training sparse models using ResNet50. Left: Task
error (or accuracy difference between dense and sparse models) as a function of rewiring steps r.
Middle: Task error as a function of scaling factor s applied to gradients of weights that do not
participate in training. Right: Task error where non-participating weights are reset to zero every z
training steps. We consider sparse models of different d denoting the ratio of weights being used.
Lines represent polynomial ﬁts of sample points. Appendix A covers data for a broader span of
neural models and tasks."
UPDATES FOR WEIGHTS NOT PARTICIPATING IN TRAINING,0.12871287128712872,"3.2
UPDATES FOR WEIGHTS NOT PARTICIPATING IN TRAINING"
UPDATES FOR WEIGHTS NOT PARTICIPATING IN TRAINING,0.132013201320132,"Previously, we found neural weights not needed for inference form short-term interactions that make
training more effective. Since sparse models lack weights that have similar roles, we can approxi-
mate this behavior using gradient updates for non-participating weights. Because non-participating
weights do not contribute to the loss, their gradients determine whether another optimization path is
better suited to traverse the region being explored in the loss landscape. Repeating gradient updates
registers the importance of these paths over some period of training and can trigger rewiring when
weights exceed a threshold, which allows training to explore different regions of the parameter space
while operating on (or forward and backward propagating) a smaller set of weights."
UPDATES FOR WEIGHTS NOT PARTICIPATING IN TRAINING,0.1353135313531353,"We evaluate the importance of gradients updates on non-participating weights by reducing their
contribution with a scale factor s. Figure 2 (middle) shows the task error as a function of the scale
factor s using various model sizes d. We observe error increases as s →0 (no gradients contribute
when s = 0), which can be attributed to premature convergence when training lacks expressive
power to explore different paths. Dampening the gradient updates affect sparser models (d ≪0.5),
which are more sensitive to critical points, more than larger models (d ∼0.5), which may still
have sufﬁcient weights to train effectively. As a result, non-participating weights should be updated
regularly to retain accuracy."
UPDATES FOR WEIGHTS NOT PARTICIPATING IN TRAINING,0.13861386138613863,"Gradient updates for weights that do not participate in training could mean they also learn represen-
tations, such that more capacity rather than search improves accuracy. We verify this by resetting
non-participating weights to zero after every z training steps, thus removing any representations
they might have learned. Figure 2 (right) shows the task error as a function of z. Errors are the
highest when values are reset every iteration (z = 1), which is equivalent when training without any
rewiring. Conversely, error decreases as weights are reset less frequently and saturates after sufﬁ-
cient training steps (z ∼1k). Interestingly, this also represents the time it takes for added weights to
decorrelate, as shown in previous sections, supporting our conjecture that correlations are indicative
of learning, and weights can be discarded once they cease to exist. The ability to remove information
from non-participating weights reinforces the idea that they make training more effective rather than
augment model capacity."
UPDATES FOR WEIGHTS NOT PARTICIPATING IN TRAINING,0.1419141914191419,"While most of literature trains sparse models using fewer weights and gradients (Bellec et al., 2018;
Mocanu et al., 2018; Dettmers & Zettlemoyer, 2019), more recent success has been found by updates
gradients for weights that do not participate in training (Wortsman et al., 2019; Liu et al., 2020;
Zhou et al., 2021; Hubara et al., 2021) as described above. However, so far there has been little
understanding of why this helps improve sparse training, which we attribute to the use of short-term
correlations to escape critical points."
UPDATES FOR WEIGHTS NOT PARTICIPATING IN TRAINING,0.14521452145214522,Under review as a conference paper at ICLR 2022
BALANCING BETWEEN EXPLORATION AND EXPLOITATION,0.1485148514851485,"3.3
BALANCING BETWEEN EXPLORATION AND EXPLOITATION"
BALANCING BETWEEN EXPLORATION AND EXPLOITATION,0.15181518151815182,"Deep learning like other optimization problems treads a delicate balance between exploration and
exploitation. In early stages of training neural models explore search spaces using high learn-
ing rates, whereas late stages exploit speciﬁc regions using small learning rates. Training sparse
neural models can also affect this balance, as fewer weights reduces the degrees of freedom and
thus hinders exploration during training. On the other hand, gradient updates on non-participating
weights introduces noise as any nonzero value will not reﬂect what is being used for training,
which limits exploitation (training bounces around basins of attraction due to gradient noise)."
BALANCING BETWEEN EXPLORATION AND EXPLOITATION,0.1551155115511551,"Figure 3: Task error after training Transformer-
XL Base using different exploration versus ex-
ploitation strategies as a function of model size
d. Appendix A covers data for a broader span of
neural models and tasks."
BALANCING BETWEEN EXPLORATION AND EXPLOITATION,0.15841584158415842,"Figure 3 shows task error degrades without
proper exploration, by training smaller mod-
els with neural layers of reduced widths (NO
EXPLORE), or exploitation, by training sparse
models delineated in previous sections (NO EX-
PLOIT). Therefore, we seek ways to induce ex-
ploitation when augmenting search spaces for
more exploration. One course of action is to
remove noise introduced by non-participating
weights during late stages, so training can
take steepest descents towards the minima.
To achieve this we stop rewiring weights af-
ter sufﬁcient training (FIX), such that non-
participating weights can no longer contribute
to the loss. Figure 3 shows this decreases error
rates tremendously."
BALANCING BETWEEN EXPLORATION AND EXPLOITATION,0.1617161716171617,"Another option is to draw analogies with added
weights, which can be safely discarded af-
ter sufﬁcient training since they do not learn
representations over time.
We reset non-
participating weights to zero roughly every 1k training steps (or the time it takes for added weights to
decorrelate) in order to remove gradient noise that may trigger unnecessary rewiring, allowing train-
ing to exploit nearby regions. Figure 3 shows error decreases substantially when non-participating
weights are either reset to zero (RESET) or regularized with a decay factor (REGULARIZE) as sug-
gested in (Zhou et al., 2021). Sparsity literature introduces alternative courses for inducing exploita-
tion (Mocanu et al., 2018; Dettmers & Zettlemoyer, 2019; Evci et al., 2020a)."
METHODOLOGY,0.16501650165016502,"4
METHODOLOGY"
METHODOLOGY,0.16831683168316833,"Using recommendations from the previous section, we summarize a methodology for sparse training
in Algorithm 1. Namely, we rewire weights w of sparse neural models based on their magnitudes
in order to preserve long-term correlations that were discovered in Section 2. At each point in time,
the top fraction d of weights are chosen to participate in training. Participating weights p are used
to compute the loss and activation gradients, while the optimizer performs updates on all the weight
gradients. We then induce exploitation on non-participating weights n using any of the approaches
discussed earlier."
METHODOLOGY,0.1716171617161716,"We note some limitations of the methodology described above. Sparse training observes no memory
savings because all weights need to be stored for their gradient updates. Sparsity is also only applied
on weights, which means we can only accelerate the forward pass and part of the backward pass
that computes activation gradients. For further acceleration, we can consider applying sparsity to
activations or their gradients to accelerate weight gradient computations (Raihan & Aamodt, 2020)."
METHODOLOGY,0.17491749174917492,"The methods delineated above make a tradeoff between exploration and exploitation using variables
v, z, and β. Stopping rewiring after half of training (v = 0.5) often provides the best results. z
should be large enough to retain short-term correlations (on the order of 1k training steps). β =
0.0002 chosen in (Zhou et al., 2021) roughly matches our choice for z. All methods perform equally"
METHODOLOGY,0.1782178217821782,Under review as a conference paper at ICLR 2022
METHODOLOGY,0.18151815181518152,"well with their optimal variables, so these variables can either be automated or kept as default. For
most experiments we adopt RESET and use FIX in a few select cases."
METHODOLOGY,0.1848184818481848,"Algorithm 1 TRAIN(D, γ, d, v, z, β)"
METHODOLOGY,0.18811881188118812,"1: Initialize neural weights w at random
2: for each training iteration t do
3:
Sample a mini batch of data D
4:
p ←wi if wi ≥τ, where τ is chosen so that |p| = d
▷Get participating weights
5:
n ←wi if wi < τ
▷Get non-participating weights
6:
ℓ←L(p, D)
▷Forward pass"
METHODOLOGY,0.19141914191419143,"7:
∂ℓ
∂w ←∂L(p,D)"
METHODOLOGY,0.19471947194719472,"∂w
▷Backward pass
8:
w ←w + γ ∂ℓ"
METHODOLOGY,0.19801980198019803,"∂w
▷Optimizer step
9:
if t ≥v then n ←0
▷FIX
10:
if t mod z = 0 then n ←0
▷RESET
11:
n ←n −βn
▷REGULARIZE"
EMPIRICAL DATA,0.20132013201320131,"5
EMPIRICAL DATA"
EMPIRICAL DATA,0.20462046204620463,"We are now in the position to train sparse neural models using our recommendations. This section
presents empirical evidence that short-term correlations can help sparse models achieve better ac-
curacy for inference tasks. We ﬁrst demonstrate that our strategy performs competitively against
state-of-the-art sparsity research. Then, we explore sparse models are tolerant to sparsity structures
that target hardware acceleration using Sparse Tensor Cores (Mishra et al., 2021)."
COMPARISONS TO DENSE MODELS,0.2079207920792079,"5.1
COMPARISONS TO DENSE MODELS"
COMPARISONS TO DENSE MODELS,0.21122112211221122,"Experiments are conducted across a wide range of deep learning tasks and neural architectures
trained on large data sets, as detailed in Appendix B. We draw comparisons between dense models
(DENSE) and sparse models as described in this work (SPARSE). We also compare to smaller dense
models (SMALL) consisting of neural layers with reduced widths that can run on existing dense
hardware. We reduce the widths of neural layers by a factor of 1/d along one of the dimensions to
match the number of weights used in the sparse models. Appendix C demonstrates the beneﬁt of
sparse training compared to smaller dense models."
COMPARISONS TO DENSE MODELS,0.2145214521452145,"Table 1 lists accuracies for dense models and their differences for sparse models (adding the two
numbers produces accuracies for sparse models) across various tasks as a function of model size d
(or the ratio of weights being used for the sparse models). We ﬁnd using our recommendations most
sparse models can be halved in size (d = 0.5) without sacriﬁcing any task accuracy, whereas training
with a quarter of the weights (d = 0.25) often reduces accuracy by less than 1%. Some exceptions
include efﬁcient convolutional models and sparser models (d = 0.1), which may be constrained by
capacity."
COMPARISONS TO SPARSITY RESEARCH,0.21782178217821782,"5.2
COMPARISONS TO SPARSITY RESEARCH"
COMPARISONS TO SPARSITY RESEARCH,0.22112211221122113,We also draw comparisons to sparsity literature as detailed below.
COMPARISONS TO SPARSITY RESEARCH,0.22442244224422442,"LOTTERY. We construct lottery tickets (LOTTERY) by training models to completion (e.g., k steps)
and removing weights based on their trained values (Frankle & Carbin, 2019; Frankle et al., 2019).
Sparse models are initialized with weights obtained after some amount of training (t = ϵ) and then
trained for k−t steps. We choose ϵ ∈[k/10, k/100] across various workloads (Frankle et al., 2019)."
COMPARISONS TO SPARSITY RESEARCH,0.22772277227722773,"SET AND RIGL. Participating weights are rewired over time based on magnitude and adding new
ones either randomly (SET) (Mocanu et al., 2018) or based on their gradients (RIGL) (Evci et al.,
2020a). Weights are initialized to zero when they become participating. The fraction of weights to
rewire decays linearly throughout training. We sweep across workloads for the best initial value for
the rewiring fraction and frequency."
COMPARISONS TO SPARSITY RESEARCH,0.23102310231023102,Under review as a conference paper at ICLR 2022
COMPARISONS TO SPARSITY RESEARCH,0.23432343234323433,Table 1: Accuracies for dense and their differences for sparse (positive means better) for different d.
COMPARISONS TO SPARSITY RESEARCH,0.2376237623762376,"Model
DENSE
d = 0.5
d = 0.25
d = 0.1
Model
DENSE
d = 0.5
d = 0.25
d = 0.1"
COMPARISONS TO SPARSITY RESEARCH,0.24092409240924093,"ResNet18
70.30
+0.01
−0.92
−2.75
SqueezeNet V1
60.77
−0.88
−5.20
−
ResNet34
73.87
−0.20
−0.65
−2.27
MobileNet V2
71.53
−0.87
−0.87
−
ResNet50
76.71
−0.05
−0.59
−2.17
Stacked UNet-64
69.53
−1.41
−3.79
−8.19
ResNet101
77.50
−0.25
−0.61
−1.52
SSD-ResNet18
19.15
−0.81
−2.49
−5.55
ResNeXt50
77.68
+0.02
−0.49
−2.01
SSD-ResNet50
24.93
−0.52
−2.06
−5.31
ResNeXt101
79.27
+0.20
+0.19
−
Faster R-CNN
37.71
−0.25
−1.61
−5.51
WideResNet50
78.13
+0.13
−0.34
−1.06
Mask R-CNN
38.26
−0.34
−1.42
−4.80
WideResNet101
78.63
−0.12
+0.04
−1.00
Mask R-CNN
35.03
−0.88
−2.65
−7.44
InceptionV3
77.10
−0.08
−0.88
−3.25
Mask R-CNN 3×
40.78
−0.12
−1.12
−3.51
Xception
79.28
+0.04
−0.28
−1.26
Mask R-CNN 3×
37.05
−0.03
−0.81
−2.98
DenseNet121
75.46
−0.46
−1.75
−4.35
RetinaNet
36.48
−0.42
−2.42
−6.00
DenseNet161
78.77
+0.01
−0.86
−2.35
RPN
57.61
−0.16
−0.90
−2.56
DenseNet169
76.97
+0.04
−0.96
−3.23
DETR
39.90
+0.10
−0.60
−0.80
VGG11-BN
70.70
−0.33
−0.79
−2.24
Pix2PixHD
68.83
+2.27
−3.02
−3.52
VGG16-BN
74.00
−0.25
−0.46
−1.75
Few-Shot Vid2Vid
25.78
+0.52
−0.63
−4.52
VGG19-BN
74.88
+0.09
−0.48
−1.52
FAZE
2.94
−0.04
+0.02
−0.04
DRN-C-26
75.22
−0.30
−0.74
−2.35
Vaswani Base
26.87
−0.68
−1.92
−3.62
DRN-C-42
76.78
−0.10
−0.62
−1.98
Vaswani Large
28.43
−0.09
−0.92
−2.12
DRN-A-50
78.30
−0.23
−0.74
−2.28
Levenshtein
6.16
−0.11
−0.23
−0.45
DeiT Tiny
72.70
−2.81
−8.09
−16.49
GNMT
24.81
−0.12
+0.26
−0.15
DeiT Small
80.08
−1.53
−3.75
−8.30
XL Base
22.88
−0.49
−2.04
−5.41
DeiT Base
81.95
−0.75
−
−
XL Large
17.90
−0.16
−1.01
−2.65
ShufﬂeNetV2
68.44
−0.43
−1.44
−
BERT Base
87.66
−0.04
−
−
MNASNet V1
71.80
−1.09
−3.36
−
BERT Large
90.92
−0.02
−
−"
COMPARISONS TO SPARSITY RESEARCH,0.24422442244224424,"Figure 4 plots the task error across the sparsity methods. We ﬁnd that our strategy vastly outperforms
competing approaches on all tasks and model sizes by a wide margin that increases with sparsity.
Interestingly, the methods compared typically omit at least one of the recommendations listed in this
work. For example, LOTTERY performs worse even when starting from a dense model because it
does not rewire. Conversely, SET and RIGL underperform because they do not accumulate gradient
updates. However, other works (Zhou et al., 2021; Hubara et al., 2021) that adopt concepts similar
to those explored in this paper should perform similarly to our approach. As a result, leveraging
short-term correlations seems to be a critical component for recent success in sparse training."
APPLICATION ON HARDWARE ACCELERATORS,0.24752475247524752,"5.3
APPLICATION ON HARDWARE ACCELERATORS"
APPLICATION ON HARDWARE ACCELERATORS,0.2508250825082508,"We have shown earlier that we can train sparse neural models while maintaining accuracy. However,
such models cannot be accelerated on modern hardware with current matrix-math pipelines (Park
et al., 2017; Gale et al., 2020) without imposing particular structures (or positions of weights used
during training and inference). On the other hand, neural structures targeting hardware acceleration
(e.g., removing blocks (Gray et al., 2017), channels or ﬁlters (Wen et al., 2016; Li et al., 2017),"
APPLICATION ON HARDWARE ACCELERATORS,0.25412541254125415,"Figure 4: Accuracy difference between dense and sparse models comparing various methods as
a function of model size d. Left to right: ResNet50 (Classiﬁcation), Transformer (Translation),
Transformer-XL (Language Modeling). Appendix F covers a broader span of data."
APPLICATION ON HARDWARE ACCELERATORS,0.25742574257425743,Under review as a conference paper at ICLR 2022
APPLICATION ON HARDWARE ACCELERATORS,0.2607260726072607,Table 2: Accuracies for dense and their differences for 2:4 sparse (positive means better).
APPLICATION ON HARDWARE ACCELERATORS,0.264026402640264,"Model
DENSE
2:4 1D
2:4 2D
Model
DENSE
2:4 1D
2:4 2D"
APPLICATION ON HARDWARE ACCELERATORS,0.26732673267326734,"ResNet18
72.17
+0.00
−0.28
VGG19
74.88
+0.04
−0.21
ResNet34
75.14
+0.06
−0.27
Xception
79.28
+0.04
−0.11
ResNet50
77.67
+0.05
+0.09
DETR
39.90
−0.30
−0.40
ResNet101
78.94
−0.09
−0.31
Pix2PixHD
68.83
+1.34
−0.68
InceptionV3
78.11
−0.11
−0.18
Few-Shot Vid2Vid
26.06
+0.49
−0.04
ResNext50
78.36
−0.21
−0.19
FAZE
2.49
+0.08
−
ResNext101
79.27
+0.28
+0.36
GNMT
24.81
+0.15
+0.09
WideResNet50
78.13
−0.07
−0.08
Transformer Large
28.43
−0.10
−0.39
WideResNet101
78.63
+0.08
−0.07
BERT Large
90.92
+0.03
−0.55
DRN C 26
77.66
−0.05
−0.11"
APPLICATION ON HARDWARE ACCELERATORS,0.2706270627062706,"layers (Michel et al., 2019)) often degrade accuracy. This apparent tradeoff between accuracy and
performance (or speed) has hindered their adoption."
APPLICATION ON HARDWARE ACCELERATORS,0.2739273927392739,"Therefore, we explore whether our recommendations can make hardware-aware structures more
amenable for deep learning.
As a case study, we consider Sparse Tensor Cores introduced in
NVIDIA Ampere GPU architecture (Mishra et al., 2021), which have twice the math throughput
of regular matrix operations. The hardware expects a 2:4 sparsity structure that takes at least two
values to be zero for each group of four values. Appendix E illustrates examples of 2:4 sparsity for
inference (1D) and for training (2D)."
APPLICATION ON HARDWARE ACCELERATORS,0.27722772277227725,"Table 2 lists accuracy differences between sparse and dense models (positive values mean sparse
performs better), where we extend learning rate schedules for some workloads compared to what
is typically used in literature (see Appendix D for details). From the table, we ﬁnd that structured
sparse neural models can generally retain accuracy for tasks. While neural models adopting coarser
structures such as block sparsity (Narang et al., 2017; Gray et al., 2017) fail to beneﬁt from our
recommendations and perform no better than smaller models (see Appendix E), using 2:4 for train-
ing (2D) and inference (1D) roughly matches accuracy of the dense models. Therefore, we can
conclude 2:4 sparsity is particularly effective at leveraging short-term correlations due to its ﬁner
granularity. This suggests possible avenues towards accelerating training (Hubara et al., 2021) as
well as obtaining efﬁcient models for inference without having to repeat the training process (Mishra
et al., 2021; Zhou et al., 2021)."
RELATED WORK,0.28052805280528054,"6
RELATED WORK"
RELATED WORK,0.2838283828382838,"Applying sparsity to reduce the size of neural models has been a topic of interest for the past three
decades (LeCun et al., 1990; Hassibi & Stork, 1993; Reed, 1993; Castellano et al., 1997; Han et al.,
2015; Jayakumar et al., 2020). Typically, sparse models are constructed by training much larger
models (that are easier to train) and removing some of their weights either after training (Han et al.,
2015; Mishra et al., 2021) or gradually alongside training (Narang et al., 2017; Zhu & Gupta, 2018;
Gale et al., 2019). However, the above are only useful to reduce costs for inference."
RELATED WORK,0.2871287128712871,"For training acceleration, early works (Lee et al., 2019; Wang et al., 2020; Tanaka et al., 2020)
sought to remove weights before training with limited success (Gale et al., 2019; Frankle et al.,
2021). Surprisingly, it was shown sparse models can be trained when initialized the same way as
the trained dense model (Frankle & Carbin, 2019). After this breakthrough, many works (Dettmers
& Zettlemoyer, 2019; Evci et al., 2020a; Jayakumar et al., 2020) tried to dynamically learn sparse
models by rewiring their weights throughout training, though some works existed earlier (Bellec
et al., 2018; Mocanu et al., 2018). While this greatly improved accuracy, it was not enough to match
that of dense models. More recent advances (Wortsman et al., 2019; Liu et al., 2020; Savarese et al.,
2020; Zhou et al., 2021; Hubara et al., 2021) introduced gradient updates for all weights in a neural
model, including ones that do not participate in training, and observed better success."
RELATED WORK,0.29042904290429045,"Our recommendations are similar to recent works (Zhou et al., 2021; Hubara et al., 2021) that were
conducted in tandem with our research, but differ in some crucial aspects. (Zhou et al., 2021) adopts"
RELATED WORK,0.29372937293729373,Under review as a conference paper at ICLR 2022
RELATED WORK,0.297029702970297,"2:4 1D sparsity patterns which cannot accelerate training. We also show alternatives to regularizing
the non-participating weights which achieve the same effect. (Hubara et al., 2021) uses 4:8 sparsity
patterns which are not useful for Sparse Tensor Cores, but they also adopt 2D patterns as we do."
RELATED WORK,0.30033003300330036,"While sparsity has been extensively explored for neural model training, our paper presents new
perspectives on why certain methods should or not work. Our explanations about how additional
weights do not learn meaningful information throughout training complement existing observations
about the beneﬁts of overparameterization (Evci et al., 2020b). We use this understanding to con-
solidate many ideas that have been circling around in sparsity literature but remain not clearly un-
derstood. For example, how frequently and why must weights be rewired."
CONCLUSION,0.30363036303630364,"7
CONCLUSION"
CONCLUSION,0.3069306930693069,"In this paper we provide a better understanding of what is needed for sparse training. We ﬁrst
discover weights that can be pruned during inference do not learn meaningful representations over
time, but exhibit short-term correlations that are necessary to train effectively. Based on this under-
standing, we uncover recommendations to remove such weights when training sparse models and
conduct extensive empirical studies to determine the effects of different hyperparameters. We then
demonstrate that sparse training can work across dozens of deep learning workloads that have not
been investigated before in sparsity literature. Lastly, we are the ﬁrst to employ sparsity patterns that
can accelerate training of sparse models on existing hardware."
CONCLUSION,0.3102310231023102,"We believe these results open many questions. On the practical side, it may be interesting to con-
sider how could this strategy be adopted to accelerate real-world training and inference workloads
today, reducing the environmental impact from training very large models. On the theoretical side,
we would like to understand better ways to approximate the short-term behavior in cases where
accuracy still suffers as well as how to address potential biases induced by sparse models which
may affect applications. We hope that our results will spur further research on these unconventional
architectures, which challenge the default choice held by massive neural models today."
REFERENCES,0.31353135313531355,REFERENCES
REFERENCES,0.31683168316831684,"Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In ICML, 2019."
REFERENCES,0.3201320132013201,"Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring: Training
very sparse deep networks. In ICLR, 2018."
REFERENCES,0.3234323432343234,"Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020."
REFERENCES,0.32673267326732675,"Giovanna Castellano, Anna Maria Fanelli, and Marcello Pelillo. An iterative pruning algorithm for
feedforward neural networks. IEEE Transactions Neural Networks, 1997."
REFERENCES,0.33003300330033003,"Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdi-
nov. Transformer-XL: Attentive language models beyond a ﬁxed-length context. arXiv preprint
arXiv:1901.02860, 2019."
REFERENCES,0.3333333333333333,"Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex op-
timization. In NeurIPS. 2014."
REFERENCES,0.33663366336633666,"Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing
performance. arXiv preprint arXiv:1907.04840, 2019."
REFERENCES,0.33993399339933994,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.3432343234323432,Under review as a conference paper at ICLR 2022
REFERENCES,0.3465346534653465,"Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery:
Making all tickets winners. In ICML. 2020a."
REFERENCES,0.34983498349834985,"Utku Evci, Fabian Pedregosa, Aidan Gomez, and Erich Elsen. The difﬁculty of training sparse
neural networks. arXiv preprint arXiv:1906.10732, 2020b."
REFERENCES,0.35313531353135313,"William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961, 2021."
REFERENCES,0.3564356435643564,"Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In ICLR, 2019."
REFERENCES,0.35973597359735976,"Jonathan Frankle, Gintare K. Dziugaite, Daniel M. Roy, and Michael Carbin. The lottery ticket
hypothesis at scale. arXiv preprint arXiv:1903.01611, 2019."
REFERENCES,0.36303630363036304,"Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Pruning neural
networks at initialization: Why are we missing the mark? In ICLR, 2021."
REFERENCES,0.36633663366336633,"Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv
preprint arXiv:1902.09574, 2019."
REFERENCES,0.3696369636963696,"Trevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen. Sparse gpu kernels for deep learning.
In SC20: International Conference for High Performance Computing, Networking, Storage and
Analysis, 2020."
REFERENCES,0.37293729372937295,"S. Gray, A. Radford, and D. P. Kingma. GPU kernels for block-sparse weights. https://cdn.
openai.com/blocksparse/blocksparsepaper.pdf, 2017."
REFERENCES,0.37623762376237624,"Jiatao Gu, Changhan Wang, and Junbo Zhao. Levenshtein transformer. In NeurIPS. 2019."
REFERENCES,0.3795379537953795,"Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efﬁcient neural network. In NeurIPS. 2015."
REFERENCES,0.38283828382838286,"Babak Hassibi and David G. Stork. Second order derivatives for network pruning: Optimal brain
surgeon. In NeurIPS. 1993."
REFERENCES,0.38613861386138615,"K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask R-CNN. In ICCV, 2017."
REFERENCES,0.38943894389438943,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016."
REFERENCES,0.3927392739273927,"Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo
Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford,
Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam Mc-
Candlish. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701,
2020."
REFERENCES,0.39603960396039606,"Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad,
Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically. arXiv preprint arXiv:1712.00409, 2017."
REFERENCES,0.39933993399339934,"Torsten Hoeﬂer, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste.
Sparsity in
deep learning: Pruning and growth for efﬁcient inference and training in neural networks. arXiv
preprint arXiv:2102.00554, 2021."
REFERENCES,0.40264026402640263,"Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, SefﬁNaor, and Daniel Soudry. Accelerated
sparse neural training: A provable and efﬁcient method to ﬁnd n:m transposable masks. arXiv
preprint arXiv:2102.08124, 2021."
REFERENCES,0.40594059405940597,"Siddhant M. Jayakumar, Razvan Pascanu, Jack W. Rae, Simon Osindero, and Erich Elsen. Top-
KAST: Top-k always sparse training. In NeurIPS. 2020."
REFERENCES,0.40924092409240925,"Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361, 2020."
REFERENCES,0.41254125412541254,Under review as a conference paper at ICLR 2022
REFERENCES,0.4158415841584158,Kenji Kawaguchi. Deep learning without poor local minima. In NeurIPS. 2016.
REFERENCES,0.41914191419141916,"Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In NeurIPS. 1990."
REFERENCES,0.42244224422442245,"Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. SNIP: Single-shot network pruning based
on connection sensitivity. In ICLR, 2019."
REFERENCES,0.42574257425742573,"Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for
efﬁcient convnets. In ICLR. 2017."
REFERENCES,0.429042904290429,"Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joey Gonzalez.
Train big, then compress: Rethinking model size for efﬁcient training and inference of transform-
ers. In ICML, 2020."
REFERENCES,0.43234323432343236,"T. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar. Focal loss for dense object detection. In ICCV,
2017."
REFERENCES,0.43564356435643564,"Junjie Liu, Zhe Xu, Runbin Shi, Ray C. C. Cheung, and Hayden K.H. So. Dynamic sparse training:
Find efﬁcient sparse network from scratch with trainable masked layers. In ICLR, 2020."
REFERENCES,0.4389438943894389,"Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and
Alexander C. Berg. SSD: Single shot multibox detector. arXiv preprint arXiv:1512.02325, 2015."
REFERENCES,0.44224422442244227,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019."
REFERENCES,0.44554455445544555,"Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A ﬁlter level pruning method for deep neural
network compression. In ICCV, 2017."
REFERENCES,0.44884488448844884,"Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In NeurIPS,
volume 32, 2019."
REFERENCES,0.4521452145214521,"Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh,
Chong Yu, and Paulius Micikevicius. Accelerating sparse deep neural networks. arXiv preprint
arXiv:2104.08378, 2021."
REFERENCES,0.45544554455445546,"Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H. Nguyen, Madeleine Gibescu,
and Antonio Liotta. Scalable training of artiﬁcial neural networks with adaptive sparse connec-
tivity inspired by network science. Nature Communications, 2018."
REFERENCES,0.45874587458745875,"Sharan Narang, Eric Undersander, and Gregory Diamos. Block-sparse recurrent neural networks.
arXiv preprint arXiv:1711.02782, 2017."
REFERENCES,0.46204620462046203,"NVIDIA.
Deep
learning
examples.
https://github.com/NVIDIA/
DeepLearningExamples, 2020a."
REFERENCES,0.46534653465346537,"NVIDIA. Imaginaire. https://github.com/NVlabs/imaginaire, 2020b."
REFERENCES,0.46864686468646866,"NVIDIA. Megatron-LM. https://github.com/NVIDIA/Megatron-LM, 2020c."
REFERENCES,0.47194719471947194,"Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In NAACL-HLT, 2019."
REFERENCES,0.4752475247524752,"Jongsoo Park, Sheng Li, Wei Wen, Ping Tak Peter Tang, Hai Li, Yiran Chen, and Pradeep Dubey.
Faster cnns with direct sparse convolutions and guided pruning. In ICLR, 2017."
REFERENCES,0.47854785478547857,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance
deep learning library. In NeurIPS. 2019."
REFERENCES,0.48184818481848185,Under review as a conference paper at ICLR 2022
REFERENCES,0.48514851485148514,"A Radford, K Narasimhan, T Salimans, and I Sutskever. Improving language understanding by gen-
erative pre-training.
https://s3-us-west-2.amazonaws.com/openai-assets/
research-covers/language-unsupervised/language_understanding_
paper.pdf2, 2018."
REFERENCES,0.4884488448844885,Md Aamir Raihan and Tor M. Aamodt. Sparse weight activation training. In NeurIPS. 2020.
REFERENCES,0.49174917491749176,"R. Reed. Pruning algorithms-a survey. IEEE Transactions on Neural Networks, 1993."
REFERENCES,0.49504950495049505,"Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object
detection with region proposal networks. In NeurIPS. 2015."
REFERENCES,0.49834983498349833,"Alex Renda, Jonathan Frankle, and Michael Carbin. Comparing rewinding and ﬁne-tuning in neural
network pruning. In ICLR, 2020."
REFERENCES,0.5016501650165016,"Joseph Lee Rodgers and W. Alan Nicewander. Thirteen ways to look at the correlation coefﬁcient.
The American Statistician, 1988."
REFERENCES,0.504950495049505,"Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bileNetV2: Inverted residuals and linear bottlenecks. In CVPR, 2018."
REFERENCES,0.5082508250825083,"Pedro Savarese, Hugo Silva, and Michael Maire. Winning the lottery with continuous sparsiﬁcation.
In NeurIPS, 2020."
REFERENCES,0.5115511551155115,"Sohil Shah, Pallabi Ghosh, Larry S. Davis, and Tom Goldstein. Stacked U-Nets: A no-frills approach
to natural image segmentation. arXiv preprint arXiv:1804.10343, 2018."
REFERENCES,0.5148514851485149,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In ICLR, 2015."
REFERENCES,0.5181518151815182,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
CVPR, 2015. URL http://arxiv.org/abs/1409.4842."
REFERENCES,0.5214521452145214,"Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, and Surya Ganguli. Pruning neural networks
without any data by iteratively conserving synaptic ﬂow. In NeurIPS, 2020."
REFERENCES,0.5247524752475248,"Neil C. Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso. The computational
limits of deep learning. arXiv preprint arXiv:2007.05558, 2020."
REFERENCES,0.528052805280528,"Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Herv´e J´egou. Training data-efﬁcient image transformers distillation through attention. arXiv
preprint arXiv:2012.12877, 2020."
REFERENCES,0.5313531353135313,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS. 2017."
REFERENCES,0.5346534653465347,"Chaoqi Wang, Guodong Zhang, and Roger Grosse.
Picking winning tickets before training by
preserving gradient ﬂow. In ICLR, 2020."
REFERENCES,0.5379537953795379,"Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catan-
zaro. Video-to-video synthesis. In NeurIPS, 2018a."
REFERENCES,0.5412541254125413,"Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-
resolution image synthesis and semantic manipulation with conditional gans. In CVPR, 2018b."
REFERENCES,0.5445544554455446,"Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, and Bryan Catanzaro. Few-
shot video-to-video synthesis. In NeurIPS, 2019."
REFERENCES,0.5478547854785478,"Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in
deep neural networks. In NeurIPS. 2016."
REFERENCES,0.5511551155115512,"Mitchell Wortsman, Ali Farhadi, and Mohammad Rastegari.
Discovering neural wirings.
In
NeurIPS. 2019."
REFERENCES,0.5544554455445545,Under review as a conference paper at ICLR 2022
REFERENCES,0.5577557755775577,"Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin John-
son, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa,
Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa,
Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s neural
machine translation system: Bridging the gap between human and machine translation. arXiv
preprint arXiv:1609.08144, 2016."
REFERENCES,0.5610561056105611,"Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2.
https://github.com/facebookresearch/detectron2, 2019."
REFERENCES,0.5643564356435643,"Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated residual networks. In CVPR, 2017."
REFERENCES,0.5676567656765676,"A. Zhou, Y. Ma, J. Zhu, J. Liu, Z. Zhang, K. Yuan, W. Sun, , and H. Li. Learning n:m ﬁne-grained
structured sparse neural networks from scratch. In ICLR, 2021."
REFERENCES,0.570957095709571,"M. Zhu and S. Gupta. To prune, or not to prune: Exploring the efﬁcacy of pruning for model
compression. In ICLR. 2018."
REFERENCES,0.5742574257425742,Under review as a conference paper at ICLR 2022
REFERENCES,0.5775577557755776,"A
RECOMMENDATIONS FOR TRAINING"
REFERENCES,0.5808580858085809,"We expand our investigations on how to remove weights that do not learn meaningful information
when training sparse models, covering more neural architectures and deep learning tasks."
REFERENCES,0.5841584158415841,"Figure 5 plots the task error as a function of rewiring steps r for sparse models of different d.
We observe that error increases with less frequent rewiring (r →∞) for other vision tasks, since
rewiring is related to how often traversal through the parameter space expands."
REFERENCES,0.5874587458745875,"100
101
102
103"
REFERENCES,0.5907590759075908,Rewiring Steps r 0 2 4 6
REFERENCES,0.594059405940594,Task Error
REFERENCES,0.5973597359735974,Image Classification
REFERENCES,0.6006600660066007,"100
101
102
103"
REFERENCES,0.6039603960396039,Rewiring Steps r 0 2 4
SEMANTIC SEGMENTATION,0.6072607260726073,6 Semantic Segmentation
SEMANTIC SEGMENTATION,0.6105610561056105,"100
101
102
103"
SEMANTIC SEGMENTATION,0.6138613861386139,Rewiring Steps r 0 20 40
IMAGE GENERATION,0.6171617161716172,"60
Image Generation"
IMAGE GENERATION,0.6204620462046204,"d = 0.5
d = 0.4
d = 0.3
d = 0.25
d = 0.2
d = 0.1"
IMAGE GENERATION,0.6237623762376238,"Figure 5: Same as Figure 2 (left). From left to right: InceptionV3 (Image Classiﬁcation), Mask
RCNN (Semantic Segmentation), Pix2PixHD (Image Generation)."
IMAGE GENERATION,0.6270627062706271,"Figure 6 shows the task error as a function of the scale factor s applied to gradient updates for non-
participating weights. We observe the error increases with decreasing contributions of the gradients
(s →0), which suggests updates to non-participating weights are also important during training for
other tasks."
IMAGE GENERATION,0.6303630363036303,"10−2
10−1
100"
IMAGE GENERATION,0.6336633663366337,Gradient Scaling s 0 2 4 6
IMAGE GENERATION,0.636963696369637,Task Error
IMAGE GENERATION,0.6402640264026402,Semantic Segmentation
IMAGE GENERATION,0.6435643564356436,"d = 0.5
d = 0.4
d = 0.3"
IMAGE GENERATION,0.6468646864686468,"10−2
10−1
100"
IMAGE GENERATION,0.6501650165016502,Gradient Scaling s 0 1 2
IMAGE GENERATION,0.6534653465346535,Machine Translation
IMAGE GENERATION,0.6567656765676567,"d = 0.2
d = 0.1"
IMAGE GENERATION,0.6600660066006601,"10−2
10−1
100"
IMAGE GENERATION,0.6633663366336634,Gradient Scaling s 0 2 4 6
LANGUAGE MODELING,0.6666666666666666,"8
Language Modeling"
LANGUAGE MODELING,0.66996699669967,"Figure 6: Same as Figure 2 (middle). From left to right: Mask R-CNN (Semantic Segmentation),
Transformer (Machine Translation), Transformer-XL (Language Modeling)."
LANGUAGE MODELING,0.6732673267326733,"Figure 7 demonstrates the task error as a function of the number of training steps z at which non-
participating weights are reset to zero. Similar to results in Section 3.2, error rates saturate after
sufﬁcient training (z ∼1k), which reinforces the idea that non-participating weights augment pa-
rameter spaces rather than model capacity."
LANGUAGE MODELING,0.6765676567656765,Under review as a conference paper at ICLR 2022
LANGUAGE MODELING,0.6798679867986799,"100
101
102
103"
LANGUAGE MODELING,0.6831683168316832,Resetting Steps z 0 2 4 6 8
LANGUAGE MODELING,0.6864686468646864,Task Error
LANGUAGE MODELING,0.6897689768976898,Semantic Segmentation
LANGUAGE MODELING,0.693069306930693,"d = 0.5
d = 0.4
d = 0.3"
LANGUAGE MODELING,0.6963696369636964,"100
101
102
103"
LANGUAGE MODELING,0.6996699669966997,Resetting Steps z 0 3 6 9
LANGUAGE MODELING,0.7029702970297029,"12
Language Modeling"
LANGUAGE MODELING,0.7062706270627063,"d = 0.2
d = 0.1"
LANGUAGE MODELING,0.7095709570957096,"100
101
102
103"
LANGUAGE MODELING,0.7128712871287128,Resetting Steps z 0 1 2 3
MACHINE TRANSLATION,0.7161716171617162,4 Machine Translation
MACHINE TRANSLATION,0.7194719471947195,"Figure 7: Same as Figure 2 (right). From left to right: Mask RCNN (Semantic Segmentation),
Transformer-XL (Language Modeling), GNMT (Machine Translation)."
MACHINE TRANSLATION,0.7227722772277227,"Figure 8 compares various exploration and exploitation strategies for training sparse models, as
described in Section 3.3. While task accuracy degrades with lack of proper exploration or exploita-
tion, inducing exploitation by removing gradient noise from non-participating weights (FIX, RESET,
REGULARIZE) substantially decreases the error rates."
MACHINE TRANSLATION,0.7260726072607261,"Figure 8: Same as Figure 3. From left to right: Transformer (Translation), Transformer-XL Large
(Language Modeling)."
MACHINE TRANSLATION,0.7293729372937293,"The above results across more deep learning workloads further validate recommendations put forth
in Section 3 for training sparse models."
MACHINE TRANSLATION,0.7326732673267327,"B
EXPERIMENTAL SETUP"
MACHINE TRANSLATION,0.735973597359736,"We design experiments on PyTorch (Paszke et al., 2019) using custom autograd functions for con-
volutions and linear layers. Our functions emulate sparsity using a binary tensor (or mask) that
we multiply elementwise with the weights of each layer during forward and backward propagation.
We determine masks based on weight magnitudes, as described earlier, after the optimizer step and
before the next training iteration."
MACHINE TRANSLATION,0.7392739273927392,"B.1
IMAGE CLASSIFICATION"
MACHINE TRANSLATION,0.7425742574257426,"We train popular convolutional models like ResNets (He et al., 2016), VGG (Simonyan & Zis-
serman, 2015), Stacked U-Nets (Shah et al., 2018), Dilated Residual Networks (Yu et al., 2017),
Inception (Szegedy et al., 2015), MobileNet (Sandler et al., 2018), as well as vision transformers
like DeiT (Touvron et al., 2020). Training involves standard pipelines for image classiﬁcation on
ImageNet-2012 as described in literature and found in public code repositories. For most workloads,
we adopt learning rates with linear warmups for the ﬁrst 5 epochs, drop the learning rate by a fac-
tor of ten at epochs 30-60-80, and stop training after 90 epochs. A few select neural models (e.g.,"
MACHINE TRANSLATION,0.7458745874587459,Under review as a conference paper at ICLR 2022
MACHINE TRANSLATION,0.7491749174917491,"mobilenets and vision transformers), however, are trained for more epochs using linear or cosine
schedules."
MACHINE TRANSLATION,0.7524752475247525,"We measure model quality using top-1 classiﬁcation accuracy. We apply sparsity to convolutions
and linear layers with some exceptions: convolutions whose input channels are not divisible by 16
(e.g., ﬁrst convolution layer, group and depthwise separable convolutions)."
MACHINE TRANSLATION,0.7557755775577558,"B.2
IMAGE SEGMENTATION AND DETECTION"
MACHINE TRANSLATION,0.759075907590759,"Image segmentation and detection tasks include both regression and classiﬁcation components. Pop-
ular detectors and segmentors are typically trained in two phases: ﬁrst a backbone is trained for
image classiﬁcation, followed by the addition of model components that are trained for detection
or segmentation. Backbones are trained on ImageNet-2012, while downstream tasks are trained on
COCO. We adapt training scripts and code from Detectron2 (Wu et al., 2019)."
MACHINE TRANSLATION,0.7623762376237624,"We train neural models such as regions with convolutional neural networks (R-CNN) variants (Ren
et al., 2015; He et al., 2017), vanilla one-shot detectors (Liu et al., 2015), and with focal loss (Lin
et al., 2017). Convolution and linear layers encountered in pretrained backbones are sparse, like for
classiﬁcation tasks. Detection and segmentation heads are also targeted."
MACHINE TRANSLATION,0.7656765676567657,"B.3
GENERATIVE MODELING"
MACHINE TRANSLATION,0.768976897689769,"Generative Adversarial Networks (GANs) contain two subnetworks: a generative model and a dis-
criminative model which combine regression and discrimination tasks during training. For image
and video tasks, the generator model regresses pixel colors. We explore conditional GANs for super
image-to-image translation and video-to-video synthesis using Imaginaire (NVIDIA, 2020b). We
measure quality of generated outputs using the Frechet Inception Distance (FID). We experiment
with generative neural models like Pix2PixHD (Wang et al., 2018b), Vid2Vid (Wang et al., 2018a),
and FewShot-Vid2Vid (Wang et al., 2019), targeting convolution and linear layers."
MACHINE TRANSLATION,0.7722772277227723,"B.4
MACHINE TRANSLATION"
MACHINE TRANSLATION,0.7755775577557755,"We explore transformer and recurrent neural models for language translation.
All models are
encoder-decoder style architectures trained for English to German (En-De) translation on WMT.
We adapt model and training code from Fairseq (Ott et al., 2019) and NVIDIA Deep Learning Ex-
amples (NVIDIA, 2020a). We measure model quality using BLEU scores."
MACHINE TRANSLATION,0.7788778877887789,"We experiment with GNMT (Wu et al., 2016) and transformer-based architectures (Vaswani et al.,
2017; Gu et al., 2019). All linear layers are sparse, except for embeddings and vocabulary projec-
tions."
MACHINE TRANSLATION,0.7821782178217822,"B.5
LANGUAGE MODELING"
MACHINE TRANSLATION,0.7854785478547854,"We consider recent advances in word-level language modeling using transformer decoder (left-to-
right) or encoder (bi-directional) architectures (Radford et al., 2018; Devlin et al., 2018). We pretrain
language models in an unsupervised fashion on WikiText-103 or Wikipedia corpus, and evaluate on
downstream tasks that are zero shot or require additional ﬁnetuning. We train them using Mega-
tron (NVIDIA, 2020c) and NVIDIA Deep Learning Examples (NVIDIA, 2020a). Model quality is
measured in terms of perplexity or F1 score."
MACHINE TRANSLATION,0.7887788778877888,"We train language models such as Transformer-XL Dai et al. (2019) and BERT Devlin et al. (2018).
We make all linear layers sparse, except for embeddings, vocabulary projections, and classiﬁcation
heads for downstream tasks."
MACHINE TRANSLATION,0.7920792079207921,"C
COMPARISONS TO DENSE MODELS"
MACHINE TRANSLATION,0.7953795379537953,"It is also interesting to understand where does accuracy of our sparse models (SPARSE) fall be-
tween accuracies of dense (DENSE) and smaller neural models (SMALL).
We deﬁne a metric
f = (DENSE −SEARCH)/(DENSE −SMALL), where a value of one means accuracy matches
that of dense, and zero implies accuracy is no better than a smaller model."
MACHINE TRANSLATION,0.7986798679867987,Under review as a conference paper at ICLR 2022
MACHINE TRANSLATION,0.801980198019802,"Figure 9 illustrates f as a function of model size d for various tasks. For d ∼0.5, sparse models are
able to approximate dense models across all neural architectures and tasks. While f decreases for
sparser models (d ≪0.5), they are still much more efﬁcient than smaller models. Even at d = 0.1,
most models can retain a large amount of accuracy."
MACHINE TRANSLATION,0.8052805280528053,"0.50.4 0.3
0.2
0.1
0.05
Model Size d 0.6 0.7 0.8 0.9 1.0"
MACHINE TRANSLATION,0.8085808580858086,Fraction f
MACHINE TRANSLATION,0.8118811881188119,Classification Tasks
MACHINE TRANSLATION,0.8151815181518152,"ResNet50
DenseNet161
InceptionV3
Vgg19"
MACHINE TRANSLATION,0.8184818481848185,"0.50.4 0.3
0.2
0.1
0.05
Model Size d 0.4 0.6 0.8 1.0"
MACHINE TRANSLATION,0.8217821782178217,Vision Tasks
MACHINE TRANSLATION,0.8250825082508251,"MRCNN
RetinaNet
RPN
Pix2PixHD"
MACHINE TRANSLATION,0.8283828382838284,"0.50.4 0.3
0.2
0.1
0.05
Model Size d 0.6 0.8 1.0"
MACHINE TRANSLATION,0.8316831683168316,NLP Tasks
MACHINE TRANSLATION,0.834983498349835,"Gnmt
Transformer
Transformer-XL"
MACHINE TRANSLATION,0.8382838283828383,"Figure 9: Fraction of accuracy that sparse models achieve between dense and smaller models as
a function of d across various tasks and neural architectures. We measure the fraction as f =
(DENSE −SEARCH)/(DENSE −SMALL)."
MACHINE TRANSLATION,0.8415841584158416,"D
EFFECTS OF LONGER TRAINING"
MACHINE TRANSLATION,0.8448844884488449,"Since conventional models can achieve higher accuracy when trained on larger data sets or with
longer training schedules (Liu et al., 2019), another interesting direction is to explore the effects of
sparsity on models that are trained to the limits of their capacity. We train neural models longer by
extending their learning rate schedules after warmup , e.g. a schedule 1/t becomes 2/t."
MACHINE TRANSLATION,0.8481848184818482,"Figure 10 (left) plots accuracy differences as a function of training time t, which denotes the ratio
of training steps to the original training schedule. The fact accuracy improves with more training
suggests neural models are often not trained to capacity using conventional schedules. We observe
sparse models achieve worse accuracy than dense models that are undertrained (t ∼1), but can
match accuracy when trained to capacity (t ≫1)."
MACHINE TRANSLATION,0.8514851485148515,"Figure 10 (middle) illustrates task errors between dense and sparse models that have been trained
for the same number of steps. We ﬁnd error scales inversely with t, since more training gives neural
models better chances to explore parameter spaces. Particularly, errors can reach zero with sufﬁcient
training (Evci et al., 2020a) for models that are not constrained by capacity (d ≥0.25). Figure 10
(right) shows the time it takes for sparse models to recover accuracy of dense models is relatively
short when t ∼1, but signiﬁcantly longer as accuracy saturates (t ≫1)."
MACHINE TRANSLATION,0.8547854785478548,"Investigations on the effects of longer training are expanded to more neural architectures and deep
learning tasks. Figure 11 illustrates accuracy deltas as a function of training time t. For vision
tasks, we ﬁnd sparse models of moderate sizes (d ≥0.25) can match accuracy of dense models
after sufﬁcient training (t ≥3). On the other hand, for language modeling, sparse models cannot
match accuracy for any time t because dense models are already near capacity for the task at hand.
Obviously, when using popular training schedules (t ∼1), sparse models can be trained a bit longer
to recover the accuracy lost."
MACHINE TRANSLATION,0.858085808580858,"E
APPLICATION ON HARDWARE ACCELERATORS"
MACHINE TRANSLATION,0.8613861386138614,"This appendix explores various sparsity structures considered in the paper that are amenable for
acceleration using modern matrix-math hardware."
MACHINE TRANSLATION,0.8646864686468647,Under review as a conference paper at ICLR 2022
MACHINE TRANSLATION,0.8679867986798679,"1
2
3 4
8
16
Training Time t −3 −2 −1 0 1 2"
MACHINE TRANSLATION,0.8712871287128713,Accuracy Difference
MACHINE TRANSLATION,0.8745874587458746,"Regular
d=0.5
d=0.25
d=0.1"
MACHINE TRANSLATION,0.8778877887788779,"1
2
3
4
8
Training Time t 0 1 2 3"
MACHINE TRANSLATION,0.8811881188118812,Task Error
MACHINE TRANSLATION,0.8844884488448845,Iso-Time
MACHINE TRANSLATION,0.8877887788778878,"1
2
3
4
Training Time t 0 5 10 15 Time"
MACHINE TRANSLATION,0.8910891089108911,Iso-Perf
MACHINE TRANSLATION,0.8943894389438944,"Figure 10: Effects of sparsity with longer training of ResNet18. Left: Accuracy differences as a
function of training time t (or ratio of steps to the original training schedule). Middle: Task error
between dense and sparse models trained for the same duration. Right: Training time it takes for
sparse models to match accuracy of dense models."
MACHINE TRANSLATION,0.8976897689768977,"1
2
3
4
8
Training Time t −2 −1 0 1"
MACHINE TRANSLATION,0.900990099009901,Accuracy Difference
MACHINE TRANSLATION,0.9042904290429042,Classification
MACHINE TRANSLATION,0.9075907590759076,"Regular
D=0.5
D=0.25
D=0.1"
MACHINE TRANSLATION,0.9108910891089109,"1
2
3
4
8
Training Time t −3 −2 −1 0 1"
MACHINE TRANSLATION,0.9141914191419142,Classification
MACHINE TRANSLATION,0.9174917491749175,"1
2
3
4
Training Time t −12 −9 −6 −3 0"
LANGUAGE MODELING,0.9207920792079208,"3
Language Modeling"
LANGUAGE MODELING,0.9240924092409241,"Figure 11: Same as Figure 10 (left). Left to right: ResNet50 (Classiﬁcation), InceptionV3 (Classiﬁ-
cation), Transformer-XL (Language Modeling)."
LANGUAGE MODELING,0.9273927392739274,"E.1
BLOCK SPARSITY"
LANGUAGE MODELING,0.9306930693069307,"We ﬁrst consider block sparsity (Narang et al., 2017; Gray et al., 2017), which removes blocks of
contiguous elements (or weights) in a neural layer as shown in Figure 12. Block sparsity addresses
common issues that are present for unstructured formats: indices for active blocks reduce storage
overhead by a factor of the block size, blocks are stored contiguously in memory which reduces
irregular memory accesses, and their computations can exploit faster matrix-math hardware, such as
Tensor Cores in NVIDIA GPUs."
LANGUAGE MODELING,0.933993399339934,"We construct block sparse structures by (1) partitioning a neural layer into a set of blocks, (2)
aggregating elements in each block into a metric, and (3) removing blocks according to some criteria
based on their metrics. While we remove blocks based on largest magnitude maxi∈(1,b2) wi. Other"
LANGUAGE MODELING,0.9372937293729373,choices such as the p-norm (Pb2
LANGUAGE MODELING,0.9405940594059405,i |wi|p)1/p achieve similar results.
LANGUAGE MODELING,0.9438943894389439,"Because structures restrict the combination of weights that can be formed in a neural model, an
important question is then for what block sizes b (if any) can sparse models retain accuracy. Table 3
lists accuracy differences between sparse and dense models for d = 0.5 using various block sizes.
We ﬁnd block sparse models fail to maintain accuracy, performing no better than smaller models.
Notably, accuracy deteriorates for all tasks and block sizes, including smaller blocks (b ≤4) that are
less amenable for hardware acceleration. In other words, block structures are too coarse for retaining
short-term correlations. For example, different weights in a block may have different roles during
training: a block that participates in training may contain weights that are not important, wheres
a non-participating block may have weights that were crucial to keep. Both cases prevent sparse"
LANGUAGE MODELING,0.9471947194719472,Under review as a conference paper at ICLR 2022
LANGUAGE MODELING,0.9504950495049505,"BLOCK SPARSE
2:4 SPARSE (1D)
2:4 SPARSE (2D)"
LANGUAGE MODELING,0.9537953795379538,"Figure 12: Block sparse, 2:4 1D, and 2:4 2D structures for a 4×4 neural layer. Blank cells represent
weights that do not participate in training (are assumed zero) and colored cells denote weights that
participate (have nonzero values). Arrows indicate direction along which structure is imposed."
LANGUAGE MODELING,0.9570957095709571,"Table 3: Accuracies for dense models and their differences for block sparse models using different
block sizes b."
LANGUAGE MODELING,0.9603960396039604,"Model
DENSE
SMALL
b = 1
b = 2
b = 4
b = 8
b = 16
b = 32"
LANGUAGE MODELING,0.9636963696369637,"Transformer-XL
22.88
−2.48
−0.82
−2.03
−2.35
−2.24
−2.36
−2.41
Transformer
28.43
−0.77
−0.21
−0.75
−1.10
−1.47
−1.98
−1.94
GNMT
24.81
−2.75
−0.15
−0.15
+0.02
−0.08
−0.15
+0.30
ResNet50
76.71
−1.63
−0.46
−0.99
−2.19
−2.79
−
−
Mask RCNN
35.03
−0.88
−0.28
−1.17
−2.43
−3.74
−4.41
−5.00"
LANGUAGE MODELING,0.966996699669967,"models from retaining weights over time that are relevant for traversing the parameter space, and
thus impacting accuracy."
LANGUAGE MODELING,0.9702970297029703,"E.2
2:4 SPARSITY"
LANGUAGE MODELING,0.9735973597359736,"We next consider Sparse Tensor Cores (Mishra et al., 2021) introduced in NVIDIA Ampere GPU
architecture which exploit 2:4 sparsity and have twice the math throughput of regular matrix units.
Figure 12 shows that 2:4 sparsity mandates each group of four values must have at least two values
that are zero. Typically, 2:4 is applied on weights w in the forward pass, y = wx. However, we
can also apply 2:4 on weight transposes wT for the backward pass, ∂L/∂x = ∂L/∂y × wT . We
denote these two options as 2:4 1D that accelerates forward pass for inference Mishra et al. (2021),
and 2:4 2D that accelerates both forward and backward passes for training."
LANGUAGE MODELING,0.976897689768977,"The 2:4 sparsity structure must always be imposed along the inner dimension of dot products. For
linear layers, we apply 2:4 on a n × k weight tensor along k or n (for forward or backward pass,
respectively). For convolutions, we apply 2:4 on a k × c × r × s weight tensor along input channels
c or k × r × s (for forward or backward pass, respectively), where k denotes output channels, r and
s are kernel dimensions."
LANGUAGE MODELING,0.9801980198019802,"We can satisfy 2:4 1D constraints by removing weights with lowest magnitudes. Since 2:4 2D
constraints have no trivial solution, we seek to minimize the cumulative magnitude of the removed
weights. In other words, for each 4 × 4 block in the tensor, we construct all possible combinations
of 2:4 2D patterns, compute their 1-norm, and choose the structure that has the largest norm."
LANGUAGE MODELING,0.9834983498349835,"F
COMPARISONS TO SPARSITY RESEARCH"
LANGUAGE MODELING,0.9867986798679867,"We expand our comparisons to literature covering more neural architectures and deep learning tasks.
Figure 13 illustrates the task error across various methods, neural architectures, and tasks. We ﬁnd
our strategy outperforms competing approaches in most cases with a few exceptions: we do not"
LANGUAGE MODELING,0.9900990099009901,Under review as a conference paper at ICLR 2022
LANGUAGE MODELING,0.9933993399339934,"Figure 13: Same as Figure 4. Clockwise: InceptionV3 (Classiﬁcation), DenseNet161 (Classiﬁca-
tion), VGG19 (Classiﬁcation), GNMT (Translation), Pix2PixHD (Generation), Mask RCNN (Seg-
mentation)."
LANGUAGE MODELING,0.9966996699669967,"outperform lottery tickets in segmentation tasks because detectors and segmentors are trained with
small learning rates which limit exploration. For generation tasks, our models are slightly worse for
d ∼0.5 (though noisy scores make it difﬁcult to draw conclusions), but the asymptotic behavior of
error rates as d →0 clearly indicates that our approach is superior."
