Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001422475106685633,"We
propose
Compressed
Vertical
Federated
Learning
(C-VFL)
for
communication-efﬁcient training on vertically partitioned data.
In C-VFL,
a server and multiple parties collaboratively train a model on their respective
features utilizing several local iterations and sharing compressed intermediate
results periodically. Our work provides the ﬁrst theoretical analysis of the effect
message compression has on distributed training over vertically partitioned data.
We prove convergence of non-convex objectives to a ﬁxed point at a rate of
O( 1
√"
ABSTRACT,0.002844950213371266,"T ) when the compression error is bounded over the course of training. We
provide speciﬁc requirements for convergence with common compression tech-
niques, such as quantization and top-k sparsiﬁcation. Finally, we experimentally
show compression can reduce communication by over 90% without a signiﬁcant
decrease in accuracy over VFL without compression."
INTRODUCTION,0.004267425320056899,"1
INTRODUCTION"
INTRODUCTION,0.005689900426742532,"Federated Learning (McMahan et al., 2017) is a distributed machine learning approach that has
become of much interest in both theory (Li et al., 2020; Wang et al., 2019; Liu et al., 2020) and
practice (Bonawitz et al., 2019; Rieke et al., 2020; Lim et al., 2020) in recent years. Naive distributed
learning algorithms may require frequent exchanges of large amounts of data, which can lead to
slow training performance (Lin et al., 2020). Further, participants may be globally distributed, with
high latency network connections. To mitigate these factors, Federated Learning algorithms aim
to be communication-efﬁcient by design. Methods such as local updates (Moritz et al., 2016; Liu
et al., 2019), where parties train local parameters for multiple iterations without communication, and
message compression (Stich et al., 2018; Wen et al., 2017; Karimireddy et al., 2019) reduce message
frequency and size, respectively, with little impact on training performance."
INTRODUCTION,0.007112375533428165,"Federated Learning methods often target the case where the data among parties is distributed hori-
zontally: each party’s data shares the same features but parties hold data corresponding to different
sample IDs. This is known as Horizontal Federated Learning (HFL) (Yang et al., 2019). However,
there are several application areas where data is partitioned in a vertical manner: the parties store
data on the same sample IDs but different feature spaces."
INTRODUCTION,0.008534850640113799,"An example of a vertically partitioned setting includes a hospital, bank, and insurance company
seeking to train a model to predict something of mutual interest, such as customer credit score.
Each of these institutions may have data on the same individuals but store medical history, ﬁnan-
cial transactions, and vehicle accident reports, respectively. These features must remain local to the
institutions due to privacy concerns, rules and regulations (e.g., GDPR, HIPAA), and/or communi-
cation network limitations. In such a scenario, Vertical Federated Learning (VFL) methods must be
employed. Although VFL is less well-studied than HFL, there has been a growing interest in VFL
algorithms recently (Hu et al., 2019; Gu et al., 2021; Cha et al., 2021), and VFL algorithms have
important applications including risk prediction, smart manufacturing, and discovery of pharmaceu-
ticals (Kairouz et al., 2021)."
INTRODUCTION,0.00995732574679943,"Typically in VFL, each party trains a local embedding function that maps raw data features to a
meaningful vector representation, or embedding, for prediction tasks. For example, a neural network
can be an embedding function for mapping the text of an online article to a vector space for classi-
ﬁcation (Koehrsen, 2018). Referring to Figure 1a, suppose Party 1 is a hospital with medical data"
INTRODUCTION,0.011379800853485065,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.012802275960170697,"(a) Example of a global model.
(b) Local view of a global model."
INTRODUCTION,0.01422475106685633,"Figure 1: Example global model with neural networks and its local view. a) To obtain a ˆy prediction
for a data sample x, each party m feeds the local features of x, xm, into a neural network. The output
of this neural network is the embedding hm(θm; xm). All embeddings are then fed into the server
model neural network with parameters θ0. b) When running C-VFL, Party 1 (in green) only has a
compressed snapshot of the other parties embeddings and the server model. To calculate ˆy, Party 1
uses its own embedding calculated at iteration t, and the embeddings and server model calculated at
time t0, the latest communication iteration, and compressed with Cm."
INTRODUCTION,0.015647226173541962,"features x1. The hospital computes its embedding h1(θ1; x1) for the features by feeding x1 through
a neural network. The other parties (the bank and insurance company), compute embeddings for
their features, then all parties share the embeddings in a private manner (e.g., homomorphic encryp-
tion, secure multi-party computation, or secure aggregation). The embeddings are then combined in
a server model θ0 to determine the ﬁnal loss of the global model. A server model (or fusion network)
captures the complicated interactions of embeddings and is often a complex, non-linear model (Gu
et al., 2019; Nie et al., 2021; Han et al., 2021). Embeddings can be very large, in practice, sometimes
requiring terabytes of communication over the course of training."
INTRODUCTION,0.017069701280227598,"Motivated by this, we propose Compressed Vertical Federated Learning (C-VFL), a general frame-
work for communication-efﬁcient Federated Learning over vertically partitioned data. In our algo-
rithm, parties communicate compressed embeddings periodically, and the parties and the server each
run block-coordinate descent for multiple local iterations, in parallel, using stochastic gradients to
update their local parameters."
INTRODUCTION,0.01849217638691323,"C-VFL is the ﬁrst theoretically veriﬁed VFL algorithm that applies embedding compression. Unlike
in HFL algorithms, C-VFL compresses embeddings rather than gradients. Previous work has proven
convergence for HFL algorithms with gradient compression (Stich et al., 2018; Wen et al., 2017;
Karimireddy et al., 2019). However, no previous work analyzes the convergence requirements for
VFL algorithms that use embedding compression. Embeddings are parameters in the partial deriva-
tives calculated at each party. The effect of compression error on the resulting partial derivatives may
be complex; therefore, the analysis in previous work on gradient compression in HFL does not apply
to compression in VFL. In our work, we prove that, under a diminishing compression error, C-VFL
converges at a rate of O( 1
√"
INTRODUCTION,0.01991465149359886,"T ), which is comparable to previous VFL algorithms that do not employ
compression. We also analyze common compressors, such as quantization and sparsiﬁcation, in
C-VFL and provide bounds on their compression parameters to ensure convergence."
INTRODUCTION,0.021337126600284494,"C-VFL also generalizes previous work by supporting an arbitrary server model. Previous work in
VFL has either only analyzed an arbitrary server model without local updates (Chen et al., 2020),
or analyzed local updates with a linear server model (Liu et al., 2019; Zhang et al., 2020; Das &
Patterson, 2021). C-VFL is designed with an arbitrary server model, allowing support for more
complex prediction tasks than those supported by previous VFL algorithms."
INTRODUCTION,0.02275960170697013,We summarize our main contributions in this work.
INTRODUCTION,0.02418207681365576,"1. We introduce C-VFL with an arbitrary compression scheme. Our algorithm generalizes previous
work in VFL by including both an arbitrary server model and multiple local iterations."
INTRODUCTION,0.025604551920341393,"2. We prove convergence of C-VFL to a ﬁxed point on non-convex objectives at a rate of O( 1
√"
INTRODUCTION,0.02702702702702703,"T ) for
a ﬁxed step size when the compression error is bounded over the course of training. We also prove
that the algorithm convergence error goes to zero for a diminishing step size if the compression error
diminishes as well. Our work provides novel analysis for the effect of compressing embeddings on"
INTRODUCTION,0.02844950213371266,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.029871977240398292,"convergence in a VFL algorithm. Our analysis also applies to Split Learning when uploads to the
server are compressed."
WE PROVIDE CONVERGENCE BOUNDS ON PARAMETERS IN COMMON COMPRESSORS THAT CAN BE USED IN C-,0.031294452347083924,"3. We provide convergence bounds on parameters in common compressors that can be used in C-
VFL. In particular, we examine scalar quantization (Bennett, 1948), lattice vector quantization (Za-
mir & Feder, 1996), and top-k sparsiﬁcation (Lin et al., 2018)."
WE EVALUATE OUR ALGORITHM BY TRAINING LSTMS ON THE MIMIC-III DATASET AND CNNS ON THE MOD-,0.032716927453769556,"4. We evaluate our algorithm by training LSTMs on the MIMIC-III dataset and CNNs on the Mod-
elNet10 dataset. We empirically show how C-VFL can reduce the number of bits sent by over 90%
compared to VFL with no compression without a signiﬁcant loss in accuracy of the ﬁnal model."
WE EVALUATE OUR ALGORITHM BY TRAINING LSTMS ON THE MIMIC-III DATASET AND CNNS ON THE MOD-,0.034139402560455195,"Related Work.
Richt´arik & Tak´ac (2016); Hardy et al. (2017) were the ﬁrst works to propose
Federated Learning algorithms for vertically partitioned data. Chen et al. (2020); Romanini et al.
(2021) propose the inclusion of an arbitrary server model in a VFL algorithm. However, these works
do not consider multiple local iterations, and thus communicate at every iteration. Liu et al. (2019),
Feng & Yu (2020), and Das & Patterson (2021) all propose different VFL algorithms with local
iterations for vertically partitioned data but do not consider an arbitrary server model. In contrast to
previous works, our work addresses a vertical scenario, an arbitrary server model, local iterations,
and message compression."
WE EVALUATE OUR ALGORITHM BY TRAINING LSTMS ON THE MIMIC-III DATASET AND CNNS ON THE MOD-,0.03556187766714083,"Message compression is a common topic in HFL scenarios, where participants exchange gradi-
ents determined by their local datasets. Methods of gradient compression in HFL include scalar
quantization (Bernstein et al., 2018), vector quantization (Shlezinger et al., 2021), and top-k spar-
siﬁcation (Shi et al., 2019). In C-VFL, compressed embeddings are shared, rather than compressed
gradients. Analysis in previous work on gradient compression in HFL does not apply to compres-
sion in VFL, as the effect of embedding compression error on each party’s partial derivatives may
be complex. No prior work has analyzed the impact of compression on convergence in VFL."
WE EVALUATE OUR ALGORITHM BY TRAINING LSTMS ON THE MIMIC-III DATASET AND CNNS ON THE MOD-,0.03698435277382646,"Outline.
In Section 2, we provide the problem formulation and our assumptions.
Section 3
presents the details of C-VFL. In Section 4, we present our main theoretical results. Our experi-
mental results are given in Section 5. Finally, we conclude in Section 6."
PROBLEM FORMULATION,0.03840682788051209,"2
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.03982930298719772,"We present our problem formulation and notation to be used in the rest of the paper. We let ∥a∥be
the 2-norm of a vector a, and let ∥A∥F be the Frobenius norm of a matrix A."
PROBLEM FORMULATION,0.041251778093883355,"We consider a set of M parties {1, . . . , M} and a server. The dataset X ∈RN×D is vertically
partitioned a priori across the M parties, where N is the number of data samples and D is the
number of features. The i-th row of X corresponds to a data sample xi. For each sample xi, a party
m holds a disjoint subset of the features, denoted xi
m, so that xi = [xi
1, . . . , xi
M]. For each xi, there
is a corresponding label yi. Let y ∈RN×1 be the vector of all sample labels. We let Xm ∈RN×Dm
be the local dataset of a party m, where the i-th row correspond to data features xi
m. We assume
that the server and all parties have a copy of the labels y. For scenarios where the labels are private
and only present at a single party, the label holder can provide enough information for the parties to
compute gradients for some classes of model architectures (Liu et al., 2019)."
PROBLEM FORMULATION,0.04267425320056899,"Each party m holds a set of model parameters θm as well as a local embedding function hm(·). The
server holds a set of parameters θ0 called the server model and a loss function l(·) that combines the
embeddings hm(θm; xi
m) from all parties. Our objective is as follows:"
PROBLEM FORMULATION,0.044096728307254626,"minimize
Θ
F(Θ; X; y) := 1 N N
X"
PROBLEM FORMULATION,0.04551920341394026,"i=1
l(θ0, h1(θ1; xi
1), . . . , hM(θM; xi
M); yi)
(1)"
PROBLEM FORMULATION,0.04694167852062589,"where Θ = [θT
0 , . . . , θT
M]T is the global model. An example of a global model Θ is in Figure 1a."
PROBLEM FORMULATION,0.04836415362731152,"For simplicity, we let m
=
0 refer to the server, and deﬁne h0(θ0; xi)
:=
θ0 for
all xi, where h0(·) is equivalent to the identity function.
Let hm(θm; xi
m)
∈
RPm for
m
=
0, . . . , M, where Pm is the size of the m-th embedding.
Let ∇mF(Θ; X; y)
:=
1
N
PN
i=1 ∇θml(θ0, h1(θ1; xi
1), . . . , hM(θM; xi
M); yi) be the partial derivatives for parameters θm."
PROBLEM FORMULATION,0.049786628733997154,Under review as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.051209103840682786,"Let XB and yB be the set of samples and labels corresponding to a randomly sampled mini-batch
B of size B. We let the stochastic partial derivatives for parameters θm be ∇mFB(Θ; X; y) :=
1
B
P"
PROBLEM FORMULATION,0.05263157894736842,"xi,yi∈XB,yB ∇θml(θ0, h1(θ1; xi
1), . . . , hM(θM; xi
M); y). We may drop X and y from F(·) and"
PROBLEM FORMULATION,0.05405405405405406,"FB(·). With a minor abuse of notation, we let hm(θm; XB
m) := {hm(θm; xB1
m ), . . . , hm(θm; xBB
m )}
be the set of all party m embeddings associated with mini-batch B, where Bi is the i-th sample in
the mini-batch B. We let ∇mFB(Θ) and ∇mFB(θ0, h1(θ1; XB
1), . . . , hM(θM; XB
M)) be equivalent,
and use them interchangeably."
PROBLEM FORMULATION,0.05547652916073969,"Assumption 1. Smoothness:
There exists positive constants L < ∞and Lm < ∞, for
m = 0, . . . , M, such that for all Θ1, Θ2, the objective function satisﬁes ∥∇F(Θ1) −∇F(Θ2)∥≤
L ∥Θ1 −Θ2∥and ∥∇mFB(Θ1) −∇mFB(Θ2)∥≤Lm ∥Θ1 −Θ2∥."
PROBLEM FORMULATION,0.05689900426742532,"Assumption 2. Unbiased gradients: For m = 0, . . . , M, for a randomly selected mini-batch B, the
stochastic partial derivatives are unbiased, i.e., EB∇mFB(Θ) = ∇mF(Θ)."
PROBLEM FORMULATION,0.05832147937411095,"Assumption 3. Bounded variance: For m = 0, . . . , M, there exists constants σm < ∞such that
the variance of the stochastic partial derivatives are bounded as: EB∥∇mF(Θ) −∇mFB(Θ)∥2 ≤
σ2
m
B for a randomly selected mini-batch B of size B."
PROBLEM FORMULATION,0.059743954480796585,"Assumption 1 bounds how fast the gradient and stochastic partial derivatives can change. Assump-
tions 2 and 3 require that the stochastic partial derivatives are unbiased estimators of the true partial
derivatives with bounded variance. Assumptions 1–3 are common assumptions in convergence anal-
ysis of gradient-based algorithms (Tsitsiklis et al., 1986; Nguyen et al., 2018; Bottou et al., 2018).
We note Assumptions 2–3 are similar to the IID assumptions in HFL convergence analysis. How-
ever, in VFL settings, all parties store identical sample IDs but different subsets of features. Hence,
there is no equivalent notion of a non-IID distribution in VFL."
PROBLEM FORMULATION,0.06116642958748222,"Assumption 4. Bounded Hessian:
There exists positive constants Hm for m = 0, . . . , M
such that for all Θ, the second partial derivatives of FB with respect to hm(θm; XB
m) satisfy:
∥∇2
hm(θm;XB
m)FB(Θ)∥F ≤Hm for any mini-batch B."
PROBLEM FORMULATION,0.06258890469416785,"Assumption 5. Bounded Embedding Gradients:
There exists positive constants Gm for
m = 0, . . . , M such that for all θm, the stochastic embedding gradients are bounded by:
∥∇θmhm(θm; XB
m)∥F ≤Gm for any mini-batch B."
PROBLEM FORMULATION,0.06401137980085349,"Since we are assuming a Lipschitz-continuous loss function (Assumption 1), we know the Hessian
of F is bounded. Assumption 4 strengthens this assumption slightly to also bound the Hessian
over any mini-batch. Assumption 5 bounds the magnitude of the partial derivatives with respect to
embeddings. This embedding gradient bound is necessary to ensure convergence in the presence of
embedding compression error (see appendix for details)."
ALGORITHM,0.06543385490753911,"3
ALGORITHM"
ALGORITHM,0.06685633001422475,"We now present C-VFL, a communication-efﬁcient method for training a global model with verti-
cally partitioned data. In each global round, a mini-batch B is chosen randomly from all samples and
parties share necessary information for local training on this mini-batch. Each party, in parallel, runs
block-coordinate stochastic gradient descent on its local model parameters θm for Q local iterations.
C-VFL runs for a total of R global rounds, and thus runs for T = RQ total local iterations."
ALGORITHM,0.06827880512091039,"For party m to compute the stochastic gradient with respect to its features, it must receive embed-
dings from all parties. We reduce communication cost by only sharing embeddings every global
round. Further, each party compresses their embeddings before sharing. We deﬁne a set of general
compressors for compressing party embeddings and the server model: Cm(·) : RPm →RPm for
m = 0, . . . , M. To calculate the gradient for data sample xi, party m receives Cj(hj(θj; xi
j)) from
all parties j ̸= m. With this information, a party m can compute ∇mFB and update its parameters
θm for multiple local iterations. Note that each party uses a stale view of the global model to com-
pute its gradient during these local iterations, as it is reusing the embeddings it receives at the start of
the round. In Section 4, we show that C-VFL converges even though parties use stale information.
An example view a party has of the global model during training is in Figure 1b. Here, t is the
current iteration and t0 is the start of the most recent global round, when embeddings were shared."
ALGORITHM,0.06970128022759602,Under review as a conference paper at ICLR 2022
ALGORITHM,0.07112375533428165,Algorithm 1 Compressed Vertical Federated Learning
ALGORITHM,0.07254623044096728,"1: Initialize: θ0
m for all parties m and server model θ0
0
2: for t ←0, . . . , T −1 do
3:
if t mod Q = 0 then
4:
Randomly sample Bt ∈{X, y}
5:
for m ←1, . . . , M in parallel do"
ALGORITHM,0.07396870554765292,"6:
Send Cm(hm(θt
m; XBt
m)) to server
7:
end for
8:
Server sends {C0(θ0), C1(h1(θt
1; XBt
1 )), . . . , CM(hM(θt
M; XBt
M))} to all parties
9:
end if
10:
for m ←0, . . . , M in parallel do"
ALGORITHM,0.07539118065433854,"11:
ˆΦt
m ←{C0(θt0
0 ), C1(h1(θt0
1 ; XBt0
1
)), . . . , hm(θt
m; XBt0
m ), . . . , CM(hM(θt0
M; XBt0
M ))}"
ALGORITHM,0.07681365576102418,"12:
θt+1
m
←θt
m −ηt0∇mFB(ˆΦt
m; yBt0)
13:
end for
14: end for"
ALGORITHM,0.07823613086770982,"Algorithm 1 details the procedure of C-VFL. In each global round, when t mod Q = 0, a mini-
batch B is randomly sampled from X and the parties exchange the associated embeddings, com-
pressed using Cm(·), via the server (lines 3-9). Each party m completes Q local iterations, using the
compressed embeddings it received in iteration t0 and its own m-th uncompressed embedding set
hm(θt
m, XBt0
m ). We denote the set of embeddings that party m uses as:"
ALGORITHM,0.07965860597439545,"ˆΦt
m := {C0(θt0
0 ), C1(h1(θt0
1 ; XBt0
1
)), . . . , hm(θt
m; XBt0
m ), . . . , CM(hM(θt0
M; XBt0
M ))}.
(2)"
ALGORITHM,0.08108108108108109,"For each local iteration, each party m updates θm by computing the stochastic partial derivatives
∇mFB(ˆΦt
m; yBt0) and applying a gradient step with step size ηt0 (lines 13-16)."
ALGORITHM,0.08250355618776671,"A key difference here from previous VFL algorithms is that C-VFL shares the server model with
all parties in order to support multiple local gradient updates with a non-linear server model. Also
note that the same mini-batch is used for all Q local iterations, thus communication is only required
every Q iterations. Therefore, without any compression, the total communication cost is O(R · M ·
(B ·Pm +|θ0|)) for R global rounds. Our compression technique replaces Pm and |θ0| with smaller
values based on the compression factor. For cases where embeddings, the batch size, and the server
model are large, this reduction can greatly decrease the communication cost."
ALGORITHM,0.08392603129445235,"Privacy.
We now discuss privacy-preserving mechanisms for C-VFL. In HFL settings, model up-
date or gradient information is shared in messages. It has been shown that gradients can leak infor-
mation about the raw data (Phong et al., 2018; Geiping et al., 2020). However in C-VFL, parties only
share embeddings and can only calculate the partial derivatives associated with the server model and
their local models. Commonly proposed HFL gradient attacks cannot be performed on C-VFL. Em-
beddings may be vulnerable to model inversion attacks (Mahendran & Vedaldi, 2015), which are
methods by which an attacker can recover raw input to a model using the embedding output and
black-box access to the model. One can protect against such an attack using homomorphic encryp-
tion (Cheng et al., 2019; Hardy et al., 2017) or secure multi-party computation (Gu et al., 2021).
An efﬁcient implementation of encryption for embeddings in VFL has been provided in the FATE
open-source project (FederatedAI, 2021). Alternatively, if the input to the server model is the sum
of party embeddings, then secure aggregation methods (Bonawitz et al., 2016) can be applied."
ALGORITHM,0.08534850640113797,"Note that C-VFL assumes all parties have access to the labels. For low-risk scenarios, such as
predicting credit score, labels may not need to be private among the parties. In cases where labels
are private, one can augment C-VFL to apply the method in Liu et al. (2019) for gradient calculation
without the need for sharing labels. Our analysis in Section 4 would still hold in this case, and the
additional communication is reduced by the use of message compression."
ANALYSIS,0.08677098150782361,"4
ANALYSIS"
ANALYSIS,0.08819345661450925,"In this section, we discuss our analytical approach and present our theoretical results. We ﬁrst deﬁne
the compression error associated with Cm(·):"
ANALYSIS,0.08961593172119488,Under review as a conference paper at ICLR 2022
ANALYSIS,0.09103840682788052,"Deﬁnition 1. Compression Error: Let vectors ϵxi
m for m = 0, . . . , M, be the compression errors of
Cm(·) on a data sample xi: ϵxi
m := Cm(hm(θm; xi)) −hm(θm; xi). Let ϵt0
m be the Pm × B matrix
with ϵxi
m for all data samples xi in mini-batch Bt0 as the columns. We denote the expected squared
message compression error from party m at round t0 as Et0
m := E ∥ϵt0
m∥2
F."
ANALYSIS,0.09246088193456614,"Let ˆG
t = [(∇0FB(ˆΦt
0; yBt0))T , . . . , (∇MFB(ˆΦt
M; yBt0))T ]T . The model Θ evolves as:"
ANALYSIS,0.09388335704125178,"Θt+1 = Θt −ηt0 ˆG
t.
(3)"
ANALYSIS,0.0953058321479374,"We note the reuse of the mini-batch of Bt0 for Q iterations in this recursion. This indicates that the
stochastic gradients are not unbiased during local iterations t0+1 ≤t ≤t0+Q−1. However, using
conditional expectation, we can apply Assumption 2 to the gradient calculated at iteration t0 when
there is no compression error. We deﬁne Φt
m to be the set of embeddings that would be received by
party m if no compression error were applied:"
ANALYSIS,0.09672830725462304,"Φt
m = {θt0
0 , h1(θt0
1 ; XBt0
1
), . . . , hm(θt
m; XBt0
m ), . . . , hM(θt0
M; XBt0
M )}.
(4)"
ANALYSIS,0.09815078236130868,"Then, if we take expectation over Bt0 conditioned on previous global models Θt up to t0:"
ANALYSIS,0.09957325746799431,"EBt0[∇mFB(Φt0
m) | {Θτ}t0
τ=0] = ∇mF(Φt0
m).
(5)"
ANALYSIS,0.10099573257467995,"With the help of (5), we can prove convergence by bounding the difference between the gradient at
the start of each global round and those calculated during local iterations (see the proof of Lemma 2
in the appendix for details)."
ANALYSIS,0.10241820768136557,"To account for compression error, using the chain rule and Taylor series expansion, we obtain:
Lemma 1. Under Assumptions 4-5, the norm of the difference between the objective function value
with compressed and uncompressed embeddings is bounded as:"
ANALYSIS,0.10384068278805121,"E∥∇mFB(ˆΦt
m) −∇mFB(Φt
m)∥2 ≤H2
mG2
m
PM
j=0,j̸=mEt0
j .
(6)"
ANALYSIS,0.10526315789473684,"The proof of Lemma 1 is given in the appendix. Using Lemma 1, we can bound the effect of
compression error on convergence."
ANALYSIS,0.10668563300142248,"We present our main theoretical results. All proofs are provided in the appendix.
Theorem 1. Convergence with ﬁxed step size: Under Assumptions 1-5, if ηt0 = η for all iterations
and satisﬁes ηt0 ≤
1
16Q max{L,maxm Lm}, then the average squared gradient over R global rounds
of Algorithm 1 is bounded by:"
R,0.10810810810810811,"1
R R−1
X"
R,0.10953058321479374,"t0=0
E
h∇F(Θt0)
2i
≤2

F(Θ0) −E

F(ΘT )
"
R,0.11095305832147938,"ηT
+ 4ηQL M
X m=0"
R,0.112375533428165,"σ2
m
B"
R,0.11379800853485064,"+ 68Q2 R M
X"
R,0.11522048364153627,"m=0
H2
mG2
m R−1
X t0=0 M
X"
R,0.1166429587482219,"j=0,j̸=m
Et0
j .
(7)"
R,0.11806543385490754,"The ﬁrst term in (7) is based on the difference between the initial model and ﬁnal model of the
algorithm. The second term is the error associated with the variance of the stochastic gradients and
the Lipschitz constants L and Lm’s. The third term relates to the average compression error over
all iterations. The larger the error introduced by a compressor, the larger the convergence error is.
We note that setting Et0
j
= 0 for all parties and iterations provides an error bound on VFL without
compression and is an improvement over the bound in Liu et al. (2019) in terms of Q, M, and B.
The second and third terms include a coefﬁcient relating to local iterations. As the number of local
iterations Q increases, the convergence error increases. However, increasing Q also has the effect of
reducing the number of communication rounds. Thus, it may be beneﬁcial to have Q > 1 in practice.
We explore this more in experiments in Section 5. The second and third terms scale with M, the
number of parties. However, VFL scenarios typically have a small number of parties (Kairouz et al.,
2021), and thus M plays a small role in convergence error. We note that when M = 1 and Q = 1,
Theorem 1 applies to Split Learning (Gupta & Raskar, 2018) when only uploads to the server are
compressed."
R,0.11948790896159317,Under review as a conference paper at ICLR 2022
R,0.12091038406827881,"Table 1: Choice of common compressor parameters to achieve a convergence rate of O(1/
√"
R,0.12233285917496443,"T). Pm
is the size of the m-th embedding. In scalar quantization, we let there be 2q quantization levels, and
let hmax and hmin be respectively the maximum and minimum components in hm(θt
m; xi
m) for all
iterations t, parties m, and xi
m. We let V be the size of the lattice cell in vector quantization. We let
k be the number of parameters sent in an embedding after top-k sparsiﬁcation, and (∥h∥2)max be
the maximum value of ∥hm(θt
m; xi
m)∥2 for all iterations t, parties m, and xi
m."
R,0.12375533428165007,"Uniform Scalar Quantizer
Lattice Quantization
Top-k Sparsiﬁcation"
R,0.1251778093883357,"Parameter choice
q = Ω

log2
"
R,0.12660028449502134,BPm(hmax −hmin)2√
R,0.12802275960170698,"T

V = O

1
BPm
√ T"
R,0.12944523470839261,"
k = Ω

Pm −
Pm
B(∥h∥2)max
√ T "
R,0.13086770981507823,"Compression error
Et0
m ≤BPm
(hmax−hmin)2"
R,0.13229018492176386,"12
2−2q = O(
1
√"
R,0.1337126600284495,"T )
Et0
m ≤V BPm"
R,0.13513513513513514,"24
= O(
1
√"
R,0.13655761024182078,"T )
Et0
m ≤B(1 −
k
Pm )(∥h∥2)max = O(
1
√ T )"
R,0.1379800853485064,"Remark 1. Let E =
1
R
PR−1
t0=0
PM
m=0 Et0
m. If ηt0 =
1
√"
R,0.13940256045519203,"T for all global rounds t0, for Q and B"
R,0.14082503556187767,"independent of T, then 1"
R,0.1422475106685633,"R
PR−1
t0=0E

∥∇F(Θt0)∥2
= O( 1
√"
R,0.14366998577524892,"T +E). This indicates that if E = O( 1
√"
R,0.14509246088193456,"T )
then we can achieve a convergence rate of O( 1
√"
R,0.1465149359886202,"T ). Informally, this means that C-VFL can afford
compression error and not worsen asymptotic convergence when this condition is satisﬁed. We
discuss how this affects commonly used compressors in practice later in the section."
R,0.14793741109530584,"We consider a diminishing step size in the following theorem.
Theorem 2. Convergence with diminishing step size: Under Assumptions 1-5, if 0 < ηt0 < 1
satisﬁes ηt0 ≤
1
16Q max{L,maxm Lm}, then the minimum squared gradient over R global rounds of
Algorithm 1 is bounded by:"
R,0.14935988620199148,"min
t0=0,...,R−1 E
h∇F(Θt0)
2i
= O"
R,0.1507823613086771,"1
PR−1
t0=0 ηt0 +"
R,0.15220483641536273,"PR−1
t0=0(ηt0)2
PT −1
t=0 ηt0
+"
R,0.15362731152204837,"PR−1
t0=0
PM
m=0 ηt0Et0
m
PR−1
t0=0 ηt0 ! ."
R,0.155049786628734,"If ηt0 and Et0
m satisfy P∞
t0=0 ηt0 = ∞, P∞
t0=0(ηt0)2 < ∞, and P∞
t0=0
PM
m=0 ηt0Et0
m < ∞, then"
R,0.15647226173541964,"mint0=0,...,R−1 E
h
∥∇F(Θt0)∥2i
→0 as R →∞."
R,0.15789473684210525,"According to Theorem 2, the product of the step size and the compression error must be summable
over all iterations. In the next subsection, we discuss how to choose common compressor parameters
to ensure this property is satisiﬁed. We also see in Section 5 that good results can be achieved
empirically without diminishing the step size or compression error."
R,0.1593172119487909,"Common Compressors.
In this section, we show how to choose common compressor parameters
to achieve a convergence rate of O( 1
√"
R,0.16073968705547653,"T ) in the context of Theorem 1, and guarantee convergence in
the context of Theorem 2. We analyze three common compressors: a uniform scalar quantizer (Ben-
nett, 1948), a 2-dimensional hexagonal lattice quantizer (Zamir & Feder, 1996), and top-k sparsiﬁ-
cation (Lin et al., 2018). For uniform scalar quantizer, we let there be 2q quantization levels. For the
lattice vector quantizer, we let V be the volume of each lattice cell. For top-k sparsiﬁcation, we let
k be the number of embedding components sent in a message. In Table 1, we present the choice of
compressor parameters in order to achieve a convergence rate of O( 1
√"
R,0.16216216216216217,"T ) in the context of Theorem 1.
We show how we calculate these bounds in the appendix and provide some implementation details
for their use. We can also use Table 1 to choose compressor parameters to ensure convergence in
the context of Theorem 2. Let ηt0 = O( 1"
R,0.16358463726884778,"t0 ), where t0 is the current round. Then setting T = t0
in Table 1 provides a choice of compression parameters at each iteration to ensure the compression
error diminishes at a rate of O(
1
√t0 ), guaranteeing convergence. Diminishing compression error can
be achieved by increasing the number of quantization levels, decreasing the volume of each lattice
cell, or increasing the number of components sent in messages."
EXPERIMENTS,0.16500711237553342,"5
EXPERIMENTS"
EXPERIMENTS,0.16642958748221906,"We present experiments to examine the performance of C-VFL in practice. The goal of our exper-
iments is to examine the effects different compression techniques have on training, and investigate
the accuracy/communication trade-off empirically."
EXPERIMENTS,0.1678520625889047,"Unless otherwise speciﬁed, our experimental setup consists of four parties and a server. Most real-
world VFL settings include collaboration between a few institutions (Kairouz et al., 2021), so we"
EXPERIMENTS,0.16927453769559034,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.17069701280227595,"(a) MIMIC by epochs
(b) MIMIC by cost
(c) ModelNet by epochs
(d) ModelNet by cost"
EXPERIMENTS,0.1721194879089616,"Figure 2: C-VFL when compressing to 2 bits per component. We show test F1-Score on MIMIC-III
dataset and test accuracy on ModelNet10 dataset, plotted by epochs and communication cost (MB)."
EXPERIMENTS,0.17354196301564723,"expect the number of parties to be small. We train our system with two datasets: the MIMIC-
III dataset (Johnson et al., 2016) and the ModelNet10 dataset (Wu et al., 2015). MIMIC-III is an
anonymized hospital patient time series dataset, while ModelNet10 are CAD photos of objects,
each with 12 different views. For MIMIC-III, the task is binary classiﬁcation to predict in-hospital
mortality. Each party trains on 19 of the 76 features with an LSTM and the server model consists
of two fully-connected layers. For ModelNet10, the task is classiﬁcation of images into 10 object
classes. Each party trains on three views with three convolutional layers and the server model
consists of a fully-connected layer. We use a ﬁxed step size of 0.01 for the MIMIC-III dataset and
0.001 for the ModelNet10 dataset. For MIMIC-III, we use a batch size of 1000, and for ModelNet10,
we use a batch size of 16. We train on the MIMIC-III dataset for 1000 epochs and the ModelNet10
dataset for 50 epochs, where an epoch consists of all iterations to fully iterate over the dataset. More
details on the datasets and training procedure can be found in the appendix."
EXPERIMENTS,0.17496443812233287,"We consider the three compressors discussed in Section 4: a uniform scalar quantizer, a 2-
dimensional hexagonal lattice quantizer, and top-k sparsiﬁcation. For both quantizers, the embed-
ding values need to be bounded. In the case of MIMIC-III’s LSTM, the embedding values are the
output of a tanh activation function and have a bounded range of [−1, 1]. For ModelNet10, the
embeddings are the output of a ReLU activation function, and may be unbounded. We scale em-
bedding values for ModelNet10 to the range [0, 1]. We apply subtractive dithering to both the scalar
quantizer (Wannamaker, 1997) and vector quantizer (Shlezinger et al., 2021)."
EXPERIMENTS,0.1763869132290185,"In our experiments, each embedding component is a 32-bit ﬂoat. Let b be the bits per component
we compress to. For the scalar quantizer, this means there are 2b quantization levels. For the 2-D
vector quantizer, this means there are 22b vectors in the codebook. The volume V of the vector
quantizer is a function of the number of codebook vectors. For top-k sparsiﬁcation, k = Pm b"
AS,0.17780938833570412,"32 as
we are using 32-bit components. We train using C-VFL and consider cases where b = 2, 3, and
4. We compare with a case where b = 32. This corresponds to a standard VFL algorithm without
embedding compression, acting as a baseline for accuracy."
AS,0.17923186344238975,"In Figure 2, we plot the test F1-Score and test accuracy for MIMIC-III and ModelNet10, respec-
tively, when training with b = 2. We use F1-Score for MIMIC-III as the in-hospital mortality
prediction task is highly skewed; most people in the dataset did not die in the hospital. The solid
line in each plot represents the average loss over ﬁve runs, while the shaded regions represent the
standard deviation. In Figures 2a and 2c, we plot by the number of training epochs. We can see in all
cases, although convergence can be a bit slower, training with compressed embeddings still reaches
similar accuracy to no compression. In Figures 2b and 2d, we plot by the communication cost in
MB. The cost of communication includes both the upload of (compressed) embeddings to the server
and download of embeddings and server model to all parties. We can see that by compressing em-
beddings, we can reach higher accuracy with signiﬁcantly less communication cost. In both datasets,
the compressors reach similar accuracy to each other, though top-k sparsiﬁcation performs slightly
worse than the others on MIMIC-III, while scalar quantization performs worse on ModelNet10."
AS,0.1806543385490754,"In Table 2, we show the maximum test accuracy reached during training and the communication cost
to reach a target accuracy for both datasets. We show results for all three compressors with b = 2,
3, and 4 bits per component, as well as the baseline of b = 32. For the MIMIC-III dataset, we show
the maximum test F1-score reached and the total communication cost of reaching an F1-Score of
0.4. The maximum F1-score for each case is within a standard deviation of each other. However,
the cost to reach target score is much smaller as the value of b decreases for all compressors. We can
see that when b = 2, we can achieve over 90% communication cost reduction over no compression
to reach a target F1-score."
AS,0.18207681365576103,"For the ModelNet10 dataset, Table 2 shows the maximum test accuracy reached and the total com-
munication cost of reaching an accuracy of 75%. We can see similar results as for the MIMIC-III"
AS,0.18349928876244664,Under review as a conference paper at ICLR 2022
AS,0.18492176386913228,"Table 2: Maximum F1-Score and test accuracy reached during training, and communication cost to
reach a target accuracy. For MIMIC-III, the target test F1-Score is 0.4, For ModelNet10, the target
test accuracy is 75%. In these experiments, Q = 10 and M = 4."
AS,0.18634423897581792,"Compressor
MIMIC-III dataset
ModelNet10 dataset"
AS,0.18776671408250356,"Max F1-Score
Cost (MB)
Max Accuracy
Cost (MB)
Reached
Target = 0.4
Reached
Target = 75%"
AS,0.1891891891891892,"None b = 32
0.448 ± 0.010
3830.0 ± 558.2
83.81% ± 0.54%
9715.9 ± 2819.3"
AS,0.1906116642958748,"Scalar b = 2
0.441 ± 0.018
233.1 ± 28.7
79.63% ± 1.74%
374.7 ± 48.3
Vector b = 2
0.451 ± 0.021
236.1 ± 17.9
83.92% ± 0.66%
620.2 ± 194.2
Top-k b = 2
0.431 ± 0.016
309.8 ± 93.6
81.98% ± 0.43%
594.3 ± 259.7"
AS,0.19203413940256045,"Scalar b = 3
0.446 ± 0.011
343.1 ± 18.8
79.47% ± 1.58%
581.4 ± 106.1
Vector b = 3
0.455 ± 0.020
330.5 ± 10.6
83.85% ± 0.65%
930.2 ± 264.3
Top-k b = 3
0.435 ± 0.030
470.7 ± 116.8
82.64% ± 0.41%
833.3 ± 355.2"
AS,0.1934566145092461,"Scalar b = 4
0.451 ± 0.020
456.0 ± 87.8
79.41% ± 2.23%
749.4 ± 96.7
Vector b = 4
0.446 ± 0.017
446.5 ± 21.3
83.83% ± 0.62%
1240.3 ± 352.4
Top-k b = 4
0.453 ± 0.014
519.1 ± 150.4
83.17% ± 0.74%
1137.0 ± 450.5"
AS,0.19487908961593173,"Table 3: MIMIC-III time in seconds to reach
a target F1-Score for different local iterations
Q and communication latency tc with vector
quantization and b = 3."
AS,0.19630156472261737,"tc
Time to Reach Target F1-Score 0.45
Q = 1
Q = 10
Q = 25"
AS,0.19772403982930298,"1
694.53 ± 150.75
470.86 ± 235.35
445.21 ± 51.44
10
1262.78 ± 274.10
512.82 ± 256.32
461.17 ± 53.29
50
3788.32 ± 822.30
699.30 ± 349.53
532.12 ± 61.49
200
13259.14 ± 2878.04
1398.60 ± 699.05
798.19 ± 92.23"
AS,0.19914651493598862,"(a) Parties M = 4
(b) Parties M = 12"
AS,0.20056899004267426,"Figure 3: Communication cost of training on
ModelNet10 with vector quantization."
AS,0.2019914651493599,"dataset with the exception of scalar quantization. Scalar quantization achieves the target accuracy
with much lower communication cost, but the maximum test accuracy is signiﬁcantly lower than the
other cases. This can be due to scalar quantization always quantizing embedding components to the
same values when nearing convergence. Vector quantization beneﬁts from considering components
jointly, and thus can have better reconstruction quality than scalar quantization (Woods, 2006)."
AS,0.2034139402560455,"In Table 3, we consider the communication/computation tradeoff of local iterations. We show how
the number of local iterations affects the time to reach a target F1-score in the MIMIC-III dataset.
We train C-VFL with vector quantization b = 3 and set the local iterations Q to 1, 10, and 25. We
simulate a scenario where computation time for training a mini-batch of data at each party takes
10 ms, and communication of embeddings takes a total of 1, 10, 50, and 200 ms roundtrip. These
different communication latencies correspond to the distance between the parties and the server:
within the same cluster, on the same local network, within the same region, and across the globe.
According to Theorem 1, increasing the number of local iterations Q increases convergence error.
However, the target test accuracy is reached within less time when Q increases. The improvement
over Q = 1 local iterations increases as the communication latency increases. In systems where
communication latency is high, it may be beneﬁcial to increase the number of local iterations. The
choice of Q will depend on the accuracy requirements of the given prediction task and the time
constraints on the prediction problem."
AS,0.20483641536273114,"Finally, in Figure 3, we plot the test accuracy of ModelNet10 against the communication cost when
using vector quantization with b = 2, 3, 4, and 32. We include plots for 4 and 12 parties. In the 12
party setup, each party stores one view for each CAD model. We note that changing the number of
parties changes the global model structure Θ as well. We can see in both cases that smaller values of
b reach higher test accuracies at lower communication cost. The total communication cost is larger
with 12 parties, but the impact of increasing compression is similar for both M = 4 and M = 12."
CONCLUSION,0.20625889046941678,"6
CONCLUSION"
CONCLUSION,0.20768136557610242,"We proposed C-VFL, a distributed communication-efﬁcient algorithm for training a model over
vertically partitioned data. We proved convergence of the algorithm to a ﬁxed point at a rate of
O( 1
√"
CONCLUSION,0.20910384068278806,"T ), and we showed experimentally that communication could be reduced by over 90% without a
signiﬁcant decrease in accuracy. For future work, we seek to relax our bounded gradient assumption
and explore the effect of adaptive compressors."
CONCLUSION,0.21052631578947367,Under review as a conference paper at ICLR 2022
REFERENCES,0.2119487908961593,REFERENCES
REFERENCES,0.21337126600284495,"W. R. Bennett. Spectra of quantized signals. Bell System Technical Journal, 1948."
REFERENCES,0.2147937411095306,"Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar.
SIGNSGD: compressed optimisation for non-convex problems. In Proceedings of the 35th In-
ternational Conference on Machine Learning, 2018."
REFERENCES,0.21621621621621623,"Kallista A. Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan,
Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for fed-
erated learning on user-held data. arXiv, 2016."
REFERENCES,0.21763869132290184,"Kallista A. Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman,
Vladimir Ivanov, Chlo´e Kiddon, Jakub Koneˇcn´y, Stefano Mazzocchi, Brendan McMahan, Ti-
mon Van Overveldt, David Petrou, Daniel Ramage, and Jason Roselander. Towards federated
learning at scale: System design. In Proceedings of Machine Learning and Systems, 2019."
REFERENCES,0.21906116642958748,"L´eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 2018."
REFERENCES,0.22048364153627312,"Dongchul Cha, MinDong Sung, and Yu-Rang Park. Implementing vertical federated learning using
autoencoders: Practical application, generalizability, and utility study. JMIR Medical Informatics,
2021."
REFERENCES,0.22190611664295876,"Tianyi Chen, Xiao Jin, Yuejiao Sun, and Wotao Yin. VAFL: a method of vertical asynchronous
federated learning. arXiv, 2020."
REFERENCES,0.2233285917496444,"Kewei Cheng, Tao Fan, Yilun Jin, Yang Liu, Tianjian Chen, and Qiang Yang. Secureboost: A
lossless federated learning framework. arXiv, 2019."
REFERENCES,0.22475106685633,"Anirban Das and Stacy Patterson. Multi-tier federated learning for vertically partitioned data. In
IEEE International Conference on Acoustics, Speech and Signal Processing, 2021."
REFERENCES,0.22617354196301565,"FederatedAI. Fate. https://github.com/FederatedAI/FATE, 2021."
REFERENCES,0.22759601706970128,"Siwei Feng and Han Yu. Multi-participant multi-class vertical federated learning. arXiv, 2020."
REFERENCES,0.22901849217638692,"Jonas Geiping, Hartmut Bauermeister, Hannah Dr¨oge, and Michael Moeller. Inverting gradients
- how easy is it to break privacy in federated learning?
In Advances in Neural Information
Processing Systems, 2020."
REFERENCES,0.23044096728307253,"Bin Gu, An Xu, Zhouyuan Huo, Cheng Deng, and Heng Huang. Privacy-preserving asynchronous
vertical federated learning algorithms for multiparty collaborative learning. IEEE Transactions
on Neural Networks and Learning Systems, 2021."
REFERENCES,0.23186344238975817,"Yue Gu, Xinyu Lyu, Weijia Sun, Weitian Li, Shuhong Chen, Xinyu Li, and Ivan Marsic. Mutual
correlation attentive factors in dyadic fusion networks for speech emotion recognition. In Pro-
ceedings of the 27th ACM International Conference on Multimedia, 2019."
REFERENCES,0.2332859174964438,"Otkrist Gupta and Ramesh Raskar. Distributed learning of deep neural network over multiple agents.
Journal of Network and Computer Applications, 2018."
REFERENCES,0.23470839260312945,"Wei Han, Hui Chen, and Soujanya Poria. Improving multimodal fusion with hierarchical mutual in-
formation maximization for multimodal sentiment analysis. Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing, 2021."
REFERENCES,0.2361308677098151,"Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini, Guillaume
Smith, and Brian Thorne.
Private federated learning on vertically partitioned data via entity
resolution and additively homomorphic encryption. arXiv, 2017."
REFERENCES,0.2375533428165007,"Yaochen Hu, Di Niu, Jianming Yang, and Shengping Zhou.
FDML: A collaborative machine
learning framework for distributed features. In 25th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, 2019."
REFERENCES,0.23897581792318634,Under review as a conference paper at ICLR 2022
REFERENCES,0.24039829302987198,"Alistair E.W. Johnson, Tom J. Pollard, Lu Shen, Li-wei H. Lehman, Mengling Feng, Mohammad
Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G. Mark. Mimic-iii,
a freely accessible critical care database. Nature, 2016."
REFERENCES,0.24182076813655762,"Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista A. Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael
G. L. D’Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Gar-
rett, Adri`a Gasc´on, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Za¨ıd Harchaoui, Chaoyang
He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi,
Mikhail Khodak, Jakub Koneˇcn´y, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo,
Tancr`ede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer ¨Ozg¨ur, Rasmus
Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova, Dawn Song, Weikang Song,
Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tram`er, Praneeth Vepakomma,
Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances
and open problems in federated learning. Foundations and Trends in Machine Learning, 2021."
REFERENCES,0.24324324324324326,"Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian U. Stich, and Martin Jaggi. Error feedback
ﬁxes signsgd and other gradient compression schemes. In Proceedings of the 36th International
Conference on Machine Learning, 2019."
REFERENCES,0.24466571834992887,"Will Koehrsen. Book recommendation system. https://github.com/WillKoehrsen/wikipedia-data-
science/blob/master/notebooks/Book2018."
REFERENCES,0.2460881934566145,"Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. In Proceedings of Machine Learning and
Systems, 2020."
REFERENCES,0.24751066856330015,"Wei Yang Bryan Lim, Nguyen Cong Luong, Dinh Thai Hoang, Yutao Jiao, Ying-Chang Liang,
Qiang Yang, Dusit Niyato, and Chunyan Miao. Federated learning in mobile edge networks: A
comprehensive survey. IEEE Communication Surveys and Tutorials, 2020."
REFERENCES,0.24893314366998578,"Tao Lin, Sebastian U. Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches,
use local SGD. In 8th International Conference on Learning Representations, 2020."
REFERENCES,0.2503556187766714,"Yujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally. Deep gradient compression: Reduc-
ing the communication bandwidth for distributed training. In 6th International Conference on
Learning Representations, 2018."
REFERENCES,0.25177809388335703,"Lumin Liu, Jun Zhang, Shenghui Song, and Khaled B. Letaief.
Client-edge-cloud hierarchical
federated learning. In IEEE International Conference on Communications, 2020."
REFERENCES,0.2532005689900427,"Yang Liu, Yan Kang, Xinwei Zhang, Liping Li, Yong Cheng, Tianjian Chen, Mingyi Hong, and
Qiang Yang. A communication efﬁcient vertical federated learning framework. NeurIPS Work-
shop on Federated Learning for Data Privacy and Conﬁdentiality, 2019."
REFERENCES,0.2546230440967283,"Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting
them. In IEEE Conference on Computer Vision and Pattern Recognition, 2015."
REFERENCES,0.25604551920341395,"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag¨uera y Arcas.
Communication-efﬁcient learning of deep networks from decentralized data. In Proceedings of
the 20th International Conference on Artiﬁcial Intelligence, 2017."
REFERENCES,0.2574679943100996,"Philipp Moritz, Robert Nishihara, Ion Stoica, and Michael I. Jordan. Sparknet: Training deep net-
works in spark. 2016."
REFERENCES,0.25889046941678523,"Lam M. Nguyen, Phuong Ha Nguyen, Marten van Dijk, Peter Richt´arik, Katya Scheinberg, and
Martin Tak´ac. SGD and hogwild! convergence without the bounded gradients assumption. In
Proceedings of the 35th International Conference on Machine Learning, 2018."
REFERENCES,0.2603129445234708,"Weizhi Nie, Qi Liang, Yixin Wang, Xing Wei, and Yuting Su. MMFN: multimodal information
fusion networks for 3d model classiﬁcation and retrieval. ACM Transactions on Multimedia Com-
puting, Communications, and Applications, 2021."
REFERENCES,0.26173541963015645,Under review as a conference paper at ICLR 2022
REFERENCES,0.2631578947368421,"Le Trieu Phong, Yoshinori Aono, Takuya Hayashi, Lihua Wang, and Shiho Moriai.
Privacy-
preserving deep learning via additively homomorphic encryption. IEEE Transactions on Infor-
mation Forensics and Security, 2018."
REFERENCES,0.26458036984352773,"Peter Richt´arik and Martin Tak´ac. Parallel coordinate descent methods for big data optimization.
Mathematical Programming, 2016."
REFERENCES,0.26600284495021337,"Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletar`ı, Holger R. Roth, Shadi Albarqouni, Spyri-
don Bakas, Mathieu N. Galtier, Bennett A. Landman, Klaus Maier-Hein, S´ebastien Ourselin,
Micah Sheller, Ronald M. Summers, Andrew Trask, Daguang Xu, Maximilian Baust, and
M. Jorge Cardoso. Digital Medicine, 2020."
REFERENCES,0.267425320056899,"Daniele Romanini, Adam James Hall, Pavlos Papadopoulos, Tom Titcombe, Abbas Ismail, Tudor
Cebere, Robert Sandmann, Robin Roehm, and Michael A. Hoeh. Pyvertical: A vertical feder-
ated learning framework for multi-headed splitnn. ICLR Workshop on Distributed and Private
Machine Learning, 2021."
REFERENCES,0.26884779516358465,"Shaohuai Shi, Kaiyong Zhao, Qiang Wang, Zhenheng Tang, and Xiaowen Chu. A convergence
analysis of distributed SGD with communication-efﬁcient gradient sparsiﬁcation. In Proceedings
of the 28th International Joint Conference on Artiﬁcial Intelligence, 2019."
REFERENCES,0.2702702702702703,"Nir Shlezinger, Mingzhe Chen, Yonina C. Eldar, H. Vincent Poor, and Shuguang Cui. Uveqfed:
Universal vector quantization for federated learning. IEEE Transactions on Signal Processing,
2021."
REFERENCES,0.2716927453769559,"Sebastian U. Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsiﬁed SGD with memory. In
Advances in Neural Information Processing Systems, 2018."
REFERENCES,0.27311522048364156,"John Tsitsiklis, Dimitri Bertsekas, and Michael Athans. Distributed asynchronous deterministic and
stochastic gradient optimization algorithms. IEEE transactions on automatic control, 1986."
REFERENCES,0.27453769559032715,"Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K. Leung, Christian Makaya, Ting He, and
Kevin Chan. Adaptive federated learning in resource constrained edge computing systems. IEEE
Journal on Selected Areas in Communications, 2019."
REFERENCES,0.2759601706970128,"Robert Alexander Wannamaker. The Theory of Dithered Quantization. PhD thesis, 1997."
REFERENCES,0.2773826458036984,"Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad:
Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural
Information Processing Systems, 2017."
REFERENCES,0.27880512091038406,"John W Woods. Multidimensional signal, image, and video processing and coding. Elsevier, 2006."
REFERENCES,0.2802275960170697,"Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong
Xiao. 3d shapenets: A deep representation for volumetric shapes. In Conference on Computer
Vision and Pattern Recognition, 2015."
REFERENCES,0.28165007112375534,"Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept
and applications. ACM Transactions on Intelligent Systems and Technology, 2019."
REFERENCES,0.283072546230441,"Ram Zamir and Meir Feder.
On lattice quantization noise.
IEEE Transactions on Information
Theory, 1996."
REFERENCES,0.2844950213371266,"Xinwei Zhang, Wotao Yin, Mingyi Hong, and Tianyi Chen. Hybrid federated learning: Algorithms
and implementation. arXiv, 2020."
REFERENCES,0.28591749644381226,Under review as a conference paper at ICLR 2022
REFERENCES,0.28733997155049784,"A
APPENDIX"
REFERENCES,0.2887624466571835,"A.1
PROOFS OF THEOREMS 1 AND 2"
REFERENCES,0.2901849217638691,"In this section, we provide the proofs for Theorems 1 and 2."
REFERENCES,0.29160739687055476,"A.1.1
ADDITIONAL NOTATION"
REFERENCES,0.2930298719772404,"Before starting the proofs, we deﬁne some additional notation to be used throughout. At each
iteration t, each party m trains with the embeddings ˆΦt
m. This is equivalent to the party training
directly with the models θt
m and θt0
j for all j ̸= m, where t0 is the last communication iteration
when party m received the embeddings. We deﬁne:"
REFERENCES,0.29445234708392604,"γt
m,j =
θt
j
m = j
θt0
j
otherwise
(A.1)"
REFERENCES,0.2958748221906117,"to represent party m’s view of party j’s model at iteration t.
We deﬁne the column vector
Γt
m = [(γt
m,0)T ; . . . ; (γt
m,M)T ]T to be party m’s view of the global model at iteration t."
REFERENCES,0.2972972972972973,"We introduce some notation to help with bounding the error introduced by compression. We deﬁne
ˆFB(Γt
m) to be the stochastic loss with compression error for a randomly selected mini-batch B
calculated by party m at iteration t:"
REFERENCES,0.29871977240398295,"ˆFB(Γt
m) := FB

θt0
0 + ϵt0
0 , h1(θt0
1 ; XBt0
1
) + ϵt0
1 , . . . , hm(θt
m; XBt0
m ), . . . , hM(θt0
M; XBt0
M ) + ϵt0
M 
. (A.2)"
REFERENCES,0.30014224751066854,Recall the recursion over the global model Θ:
REFERENCES,0.3015647226173542,"Θt+1 = Θt −ηt0 ˆG
t.
(A.3)"
REFERENCES,0.3029871977240398,"We can equivalently deﬁne ˆG
t as follows:"
REFERENCES,0.30440967283072545,"ˆG
t =
h
(∇0 ˆFB(Γt
0))T , . . . , (∇M ˆFB(Γt
M))T iT
.
(A.4)"
REFERENCES,0.3058321479374111,"Note that the compression error in ˆF(·) is applied to the embeddings, and not the model parameters.
Thus, F(·) and ˆF(·) are different functions. In several parts of the proof, we need to bound the
compression error in ∇m ˆFB(Γt
m)."
REFERENCES,0.30725462304409673,"For our analysis, we redeﬁne the set of embeddings for a mini-batch B of size B from party m as a
matrix:"
REFERENCES,0.30867709815078237,"hm(θm; XB
m) :=
h
hm(θm; xB1
m ), . . . , hm(θm; xBB
m )
i
.
(A.5)"
REFERENCES,0.310099573257468,"hm(θm; XB
m) is a matrix with dimensions Pm × B where each column is the embedding from party
m for a single sample in the mini-batch."
REFERENCES,0.31152204836415365,"Let P = PM
m=0 Pm be the sum of the sizes of all embeddings. We redeﬁne the set of embeddings
used by a party m to calculate its gradient without compression error as a matrix:"
REFERENCES,0.3129445234708393,"ˆΦt
m =
h
(θt0
0 )T , (h1(θt0
1 ; XBt0
1
))T , . . . , (hm(θt
m; XBt0
m ))T , . . . , (hM(θt0
M; XBt0
M ))T iT
.
(A.6)"
REFERENCES,0.31436699857752487,"ˆΦt
m is a matrix with dimensions P × B where each column is the concatenation of embeddings for
all parties for a single sample in the mini-batch."
REFERENCES,0.3157894736842105,Recall the set of compression error vectors for a mini-batch B of size B from party m is the matrix:
REFERENCES,0.31721194879089615,"ϵt0
m :=
h
ϵB1
m , . . . , ϵBB
m
i
.
(A.7)"
REFERENCES,0.3186344238975818,"ϵt0
m is a matrix of dimensions Pm × B where each column is the compression error from party m
for a single sample in the mini-batch."
REFERENCES,0.3200568990042674,Under review as a conference paper at ICLR 2022
REFERENCES,0.32147937411095306,"We deﬁne the compression error on each embedding used in party m’s gradient calculation at itera-
tion t:"
REFERENCES,0.3229018492176387,"Et0
m =
h
(ϵt0
0 )T , . . . , (ϵt0
m−1)T , 0T , (ϵt0
m−1)T , . . . , (ϵt0
M)T iT
.
(A.8)"
REFERENCES,0.32432432432432434,"Et0
m is a matrix with dimensions P × B where each column is the concatenation of compression
error on embeddings for all parties for a single sample in the mini-batch."
REFERENCES,0.32574679943101,"With some abuse of notation, we deﬁne:"
REFERENCES,0.32716927453769556,"∇mFB(Φt
m + Et0
m) := ∇m ˆFB(Γt
m).
(A.9)"
REFERENCES,0.3285917496443812,"Note that we can apply the chain rule to ∇m ˆFB(Γt
m):"
REFERENCES,0.33001422475106684,"∇m ˆFB(Γt
m) = ∇θmhm(θt
m)∇hm(θm)FB(Φt
m + Et0
m).
(A.10)"
REFERENCES,0.3314366998577525,"With this expansion, we can now apply Taylor series expansion to ∇hm(θm)FB(Φt
m + Et0
m) around
the point Φt
m:"
REFERENCES,0.3328591749644381,"∇hm(θm)FB(Φt
m + Et0
m) = ∇hm(θm)FB(Φt
m) + ∇2
hm(θm)FB(Φt
m)tEt0
m + . . .
(A.11)"
REFERENCES,0.33428165007112376,"We let the inﬁnite sum of all terms in this Taylor series from the second partial derivatives and up be
denoted as Rm
0 :"
REFERENCES,0.3357041251778094,"Rm
0 (Φt
m + Et0
m) := ∇2
hm(θm)FB(Φt
m)T Et0
m + . . .
(A.12)"
REFERENCES,0.33712660028449504,"Note that all compression error is in Rm
0 (Φt
m + Et0
m).
Presented in Section A.1.2, the proof
of Lemma 1’ shows how we can bound Rm
0 (Φt
m + Et0
m), bounding the compression error in
∇m ˆFB(Γt
m)."
REFERENCES,0.3385490753911807,"Let Et0 = EBt0[ · | {Θτ}t0
τ=0]. Note that by Assumption 2, Et0 
Gt0
= ∇F(Θt0) as when there is
no compression error in the gradients G, they are equal to the full-batch gradient in expectation when
conditioned on the model parameters up to the iteration t0. However, this is not true for iterations
t0 + 1 ≤t ≤t0 + Q −1, as we reuse the mini-batch Bt0 in these local iterations. We upper bound
the error introduced by stochastic gradients calculated during local iterations in Lemma 2."
REFERENCES,0.3399715504978663,"A.1.2
SUPPORTING LEMMAS"
REFERENCES,0.3413940256045519,"Next, we provide supporting lemmas and their proofs."
REFERENCES,0.34281650071123754,"We restate Lemma 1 here:
Lemma 1. Under Assumptions 4-5, the norm of the difference between the objective function value
with and without error is bounded by:"
REFERENCES,0.3442389758179232,"E
∇mFB(ˆΦt
m) −∇mFB(Φt
m)

2
≤H2
mG2
m M
X"
REFERENCES,0.3456614509246088,"j=0,j̸=m"
REFERENCES,0.34708392603129445,"Et0
j .
(A.13)"
REFERENCES,0.3485064011379801,"To prove Lemma 1, we ﬁrst prove the following lemma:
Lemma 1’. Under Assumptions 4-5, the squared norm of the partial derivatives for party m’s
embedding multiplied by the Taylor series terms Rm
0 (Φt
m + Et0
m) is bounded by:
∇θmhm(θt
m)Rm
0 (Φt
m + Et0
m)
2 ≤H2
mG2
m
Et0
m

F .
(A.14)"
REFERENCES,0.34992887624466573,Proof.
REFERENCES,0.35135135135135137,"∇θmhm(θt
m)Rm
0 (Φt
m + Et0
m)
2 ≤
∇θmhm(θt
m)
2
F
Rm
0 (Φt
m + Et0
m)
2
F
(A.15)"
REFERENCES,0.352773826458037,"≤H2
m
∇θmhm(θt
m)
2
F
Et0
m
2
F
(A.16)"
REFERENCES,0.3541963015647226,"where (A.16) follows from Assumption 4 and the following property of the Taylor series approxi-
mation error:
Rm
0 (Φt
m + Et0
m)

F ≤Hm
Et0
m

F .
(A.17)"
REFERENCES,0.35561877667140823,Under review as a conference paper at ICLR 2022
REFERENCES,0.35704125177809387,"Applying Assumption 5, we have:"
REFERENCES,0.3584637268847795,"∇θmhm(θt
m)Rm
0 (Φt
m + Et0
m)
2 ≤H2
mG2
m
Et0
m
2
F .
(A.18)"
REFERENCES,0.35988620199146515,We now prove Lemma 1.
REFERENCES,0.3613086770981508,Proof. Recall that:
REFERENCES,0.3627311522048364,"∇m ˆFB(Γt
m) = ∇mFB(Φt
m + Et0
m)
(A.19)"
REFERENCES,0.36415362731152207,"= ∇θmhm(θt
m)∇hm(θm)FB(Φt
m + Et0
m).
(A.20)"
REFERENCES,0.3655761024182077,Next we apply Taylor series expansion as in (A.11):
REFERENCES,0.3669985775248933,"∇m ˆFB(Γt
m) = ∇θmhm(θt
m)
 
∇hm(θm)FB(Φt
m) + Rm
0 (Φt
m + Et0
m)

(A.21)"
REFERENCES,0.3684210526315789,"= ∇mFB(Γt
m) + ∇θmhm(θt
m)Rm
0 (Φt
m + Et0
m)
(A.22)"
REFERENCES,0.36984352773826457,"Rearranging and applying expectation and the squared 2-norm, we can bound further:"
REFERENCES,0.3712660028449502,"E
∇m ˆFB(Γt
m) −∇mFB(Γt
m)

2
= E
∇θmhm(θt
m)Rm
0 (Φt
m + Et0
m)
2
(A.23)"
REFERENCES,0.37268847795163584,"≤H2
mG2
mE
Et0
m
2
F
(A.24)"
REFERENCES,0.3741109530583215,"= H2
mG2
m
X"
REFERENCES,0.3755334281650071,"j̸=m
E
ϵt0
j 2"
REFERENCES,0.37695590327169276,"F
(A.25)"
REFERENCES,0.3783783783783784,"= H2
mG2
m
X"
REFERENCES,0.37980085348506404,"j̸=m
Et0
j
(A.26)"
REFERENCES,0.3812233285917496,"where (A.24) follows from Lemma 1’, (A.25) follows from the deﬁnition of Et0
m, and (A.26) follows
from Deﬁnition 1."
REFERENCES,0.38264580369843526,"Lemma 2. If ηt0 ≤
1
4Q maxm Lm , then under Assumptions 1-5 we can bound the conditional ex-"
REFERENCES,0.3840682788051209,"pected squared norm difference of gradients Gt0 and ˆG
t for iterations t0 to t0 + Q −1 as follows:"
REFERENCES,0.38549075391180654,"t0+Q−1
X"
REFERENCES,0.3869132290184922,"t=t0
Et0
ˆG
t −Gt0

2
≤16Q3(ηt0)2
M
X"
REFERENCES,0.3883357041251778,"m=0
L2
m
∇mF(Θt0)
2"
REFERENCES,0.38975817923186346,"+ 16Q3(ηt0)2
M
X"
REFERENCES,0.3911806543385491,"m=0
L2
m
σ2
m
B"
REFERENCES,0.39260312944523473,"+ 64Q3
M
X"
REFERENCES,0.3940256045519203,"m=0
H2
mG2
m
Et0
m
2
F .
(A.27)"
REFERENCES,0.39544807965860596,Under review as a conference paper at ICLR 2022
REFERENCES,0.3968705547652916,Proof.
REFERENCES,0.39829302987197723,"Et0
ˆG
t −Gt0

2
= M
X"
REFERENCES,0.39971550497866287,"m=0
Et0
∇m ˆFB(Γt
m) −∇mFB(Γt0
m)

2
(A.28) = M
X"
REFERENCES,0.4011379800853485,"m=0
Et0
∇m ˆFB(Γt
m) −ˆFB(Γt−1
m ) + ∇m ˆFB(Γt−1
m ) −∇mFB(Γt0
m)

2
(A.29)"
REFERENCES,0.40256045519203415,"≤(1 + n) M
X"
REFERENCES,0.4039829302987198,"m=0
Et0
∇m ˆFB(Γt
m) −∇m ˆFB(Γt−1
m )

2"
REFERENCES,0.40540540540540543,"+

1 + 1 n 
M
X"
REFERENCES,0.406827880512091,"m=0
Et0
∇m ˆFB(Γt−1
m ) −∇mFB(Γt0
m)

2
(A.30)"
REFERENCES,0.40825035561877665,"≤2 (1 + n) M
X"
REFERENCES,0.4096728307254623,"m=0
Et0 h∇mFB(Γt
m) −∇mFB(Γt−1
m )
2i"
REFERENCES,0.41109530583214793,"+ 2 (1 + n) M
X"
REFERENCES,0.41251778093883357,"m=0
Et0 h∇θmhm(θt
m)Rm
0 (Φt
m + Et0
m) −∇θmhm(θt−1
m )Rm
0 (Φt−1
m
+ Et−1
m )
2i"
REFERENCES,0.4139402560455192,"+

1 + 1 n 
M
X"
REFERENCES,0.41536273115220484,"m=0
Et0
∇m ˆFB(Γt−1
m ) −∇mFB(Γt0
m)

2
(A.31)"
REFERENCES,0.4167852062588905,"≤2 (1 + n) M
X"
REFERENCES,0.4182076813655761,"m=0
Et0 h∇mFB(Γt
m) −∇mFB(Γt−1
m )
2i"
REFERENCES,0.41963015647226176,"+ 8 (1 + n) M
X"
REFERENCES,0.42105263157894735,"m=0
H2
mG2
m
Et0
m
2"
REFERENCES,0.422475106685633,"+

1 + 1 n 
M
X"
REFERENCES,0.4238975817923186,"m=0
Et0
∇m ˆFB(Γt−1
m ) −∇mFB(Γt0
m)

2
(A.32)"
REFERENCES,0.42532005689900426,where (A.30) follows from the fact that (X + Y )2 ≤(1 + n)X2 + (1 + 1
REFERENCES,0.4267425320056899,"n)Y 2 for some positive n
and (A.32) follows from Lemma 1’."
REFERENCES,0.42816500711237554,Applying Assumption 1 to the ﬁrst term in (A.30) we have:
REFERENCES,0.4295874822190612,"Et0
ˆG
t −Gt0

2
≤2 (1 + n) M
X"
REFERENCES,0.4310099573257468,"m=0
L2
mEt0 hΓt
m −Γt−1
m
2i"
REFERENCES,0.43243243243243246,"+ 2

1 + 1 n 
M
X"
REFERENCES,0.43385490753911804,"m=0
Et0
∇m ˆFB(Γt−1
m ) −∇mFB(Γt0
m)

2"
REFERENCES,0.4352773826458037,"+ 8 (1 + n) M
X"
REFERENCES,0.4366998577524893,"m=0
H2
mG2
m
Et0
m
2
(A.33)"
REFERENCES,0.43812233285917496,"= 2(ηt0)2 (1 + n) M
X"
REFERENCES,0.4395448079658606,"m=0
L2
mEt0
∇m ˆFB(Γt−1
m )

2"
REFERENCES,0.44096728307254623,"+ 2

1 + 1 n 
M
X"
REFERENCES,0.4423897581792319,"m=0
Et0
∇m ˆFB(Γt−1
m ) −∇mFB(Γt0
m)

2"
REFERENCES,0.4438122332859175,"+ 8 (1 + n) M
X"
REFERENCES,0.44523470839260315,"m=0
H2
mG2
m
Et0
m
2
(A.34)"
REFERENCES,0.4466571834992888,"where (A.34) follows from the update rule Γt
m = Γt−1
m
−ηt0∇m ˆFB(Γt−1
m )."
REFERENCES,0.4480796586059744,Under review as a conference paper at ICLR 2022
REFERENCES,0.44950213371266,Bounding further:
REFERENCES,0.45092460881934565,"Et0
ˆG
t −Gt0

2"
REFERENCES,0.4523470839260313,"≤2(ηt0)2 (1 + n) M
X"
REFERENCES,0.45376955903271693,"m=0
L2
mEt0
∇m ˆFB(Γt−1
m ) −∇mFB(Γt0
m) + ∇mFB(Γt0
m)

2"
REFERENCES,0.45519203413940257,"+

1 + 1 n 
M
X"
REFERENCES,0.4566145092460882,"m=0
Et0
∇m ˆFB(Γt−1
m ) −∇mFB(Γt0
m)

2"
REFERENCES,0.45803698435277385,"+ 8 (1 + n) M
X"
REFERENCES,0.4594594594594595,"m=0
H2
mG2
m
Et0
m
2
(A.35)"
REFERENCES,0.46088193456614507,"≤4(ηt0)2 (1 + n) M
X"
REFERENCES,0.4623044096728307,"m=0
L2
mEt0
∇m ˆFB(Γt−1
m ) −∇mFB(Γt0
m)

2"
REFERENCES,0.46372688477951635,"+ 4(ηt0)2 (1 + n) M
X"
REFERENCES,0.465149359886202,"m=0
L2
mEt0 h∇mFB(Γt0
m)
2i"
REFERENCES,0.4665718349928876,"+

1 + 1 n 
M
X"
REFERENCES,0.46799431009957326,"m=0
Et0
∇m ˆFB(Γt−1
m ) −∇mFB(Γt0
m)

2"
REFERENCES,0.4694167852062589,"+ 8 (1 + n) M
X"
REFERENCES,0.47083926031294454,"m=0
H2
mG2
m
Et0
m
2
(A.36) = M
X m=0"
REFERENCES,0.4722617354196302,"
4(ηt0)2 (1 + n) L2
m +

1 + 1 n"
REFERENCES,0.47368421052631576,"
Et0
∇m ˆFB(Γt−1
m ) −∇mFB(Γt0
m)

2"
REFERENCES,0.4751066856330014,"+ 4(ηt0)2 (1 + n) M
X"
REFERENCES,0.47652916073968704,"m=0
L2
mEt0 h∇mFB(Γt0
m)
2i"
REFERENCES,0.4779516358463727,"+ 8 (1 + n) M
X"
REFERENCES,0.4793741109530583,"m=0
H2
mG2
m
Et0
m
2 .
(A.37)"
REFERENCES,0.48079658605974396,Let n = Q. We simplify (A.37) further:
REFERENCES,0.4822190611664296,"Et0
ˆG
t −Gt0

2 ≤ M
X m=0"
REFERENCES,0.48364153627311524,"
4(ηt0)2 (1 + Q) L2
m +

1 + 1 Q"
REFERENCES,0.4850640113798009,"
Et0
∇m ˆFB(Γt−1
m ) −∇mFB(Γt0
m)

2"
REFERENCES,0.4864864864864865,"+ 4(ηt0)2 (1 + Q) M
X"
REFERENCES,0.4879089615931721,"m=0
L2
mEt0 h∇mFB(Γt0
m)
2i"
REFERENCES,0.48933143669985774,"+ 8 (1 + Q) M
X"
REFERENCES,0.4907539118065434,"m=0
H2
mG2
m
Et0
m
2
F .
(A.38)"
REFERENCES,0.492176386913229,Under review as a conference paper at ICLR 2022
REFERENCES,0.49359886201991465,"Let ηt0 ≤
1
4Q maxm Lm . We bound (A.38) as follows:"
REFERENCES,0.4950213371266003,"Et0
ˆG
t −Gt0

2
≤
(1 + Q)"
REFERENCES,0.49644381223328593,"4Q2
+

1 + 1 Q"
REFERENCES,0.49786628733997157,"
M
X"
REFERENCES,0.4992887624466572,"m=0
Et0
∇m ˆFB(Γt−1
m ) −∇mFB(Γt0
m)

2"
REFERENCES,0.5007112375533428,"+ 4(ηt0)2 (1 + Q) M
X"
REFERENCES,0.5021337126600285,"m=0
L2
mEt0 h∇mFB(Γt0
m)
2i"
REFERENCES,0.5035561877667141,"+ 8(1 + Q) M
X"
REFERENCES,0.5049786628733998,"m=0
H2
mG2
m
Et0
m
2
F
(A.39) ≤
 1"
REFERENCES,0.5064011379800853,"2Q +

1 + 1 Q"
REFERENCES,0.5078236130867709,"
M
X"
REFERENCES,0.5092460881934566,"m=0
Et0
∇m ˆFB(Γt−1
m ) −∇mFB(Γt0
m)

2"
REFERENCES,0.5106685633001422,"+ 4(ηt0)2 (1 + Q) M
X"
REFERENCES,0.5120910384068279,"m=0
L2
mEt0 h∇mFB(Γt0
m)
2i"
REFERENCES,0.5135135135135135,"+ 8(1 + Q) M
X"
REFERENCES,0.5149359886201992,"m=0
H2
mG2
m
Et0
m
2
F
(A.40)"
REFERENCES,0.5163584637268848,"≤

1 + 2 Q 
M
X"
REFERENCES,0.5177809388335705,"m=0
Et0
∇m ˆFB(Γt−1
m ) −∇mFB(Γt0
m)

2"
REFERENCES,0.519203413940256,"+ 4(ηt0)2 (1 + Q) M
X"
REFERENCES,0.5206258890469416,"m=0
L2
mEt0 h∇mFB(Γt0
m)
2i"
REFERENCES,0.5220483641536273,"+ 8(1 + Q) M
X"
REFERENCES,0.5234708392603129,"m=0
H2
mG2
m
Et0
m
2
F .
(A.41)"
REFERENCES,0.5248933143669986,"We deﬁne the following notation for simplicity: At := M
X"
REFERENCES,0.5263157894736842,"m=0
Et0
∇m ˆFB(Γt
m) −∇mFB(Γt0
m)

2
(A.42)"
REFERENCES,0.5277382645803699,"B0 := 4(ηt0)2 (1 + Q) M
X"
REFERENCES,0.5291607396870555,"m=0
L2
mEt0 h∇mFB(Γt0
m)
2i
(A.43)"
REFERENCES,0.5305832147937412,"B1 := 8(1 + Q) M
X"
REFERENCES,0.5320056899004267,"m=0
H2
mG2
m
Et0
m
2
F
(A.44)"
REFERENCES,0.5334281650071123,"C :=

1 + 2 Q"
REFERENCES,0.534850640113798,"
.
(A.45)"
REFERENCES,0.5362731152204836,Note that we have shown that At ≤CAt−1 + B0 + B1. Therefore:
REFERENCES,0.5376955903271693,"At0+1 ≤CAt0 + (B0 + B1)
(A.46)"
REFERENCES,0.5391180654338549,"At0+2 ≤C2At0 + C(B0 + B1) + (B0 + B1)
(A.47)"
REFERENCES,0.5405405405405406,"At0+3 ≤C3At0 + C2(B0 + B1) + C(B0 + B1) + (B0 + B1)
(A.48)
...
(A.49)"
REFERENCES,0.5419630156472262,At ≤Ct−t0−1At0 + (B0 + B1)
REFERENCES,0.5433854907539118,"t−t0−2
X"
REFERENCES,0.5448079658605974,"k=0
Ck
(A.50)"
REFERENCES,0.5462304409672831,= Ct−t0−1At0 + (B0 + B1)Ct−t0−1 −1
REFERENCES,0.5476529160739687,"C −1
.
(A.51)"
REFERENCES,0.5490753911806543,Under review as a conference paper at ICLR 2022
REFERENCES,0.55049786628734,"We bound the ﬁrst term in (A.51) by applying Lemma 1: At0 = M
X"
REFERENCES,0.5519203413940256,"m=0
Et0
∇m ˆFB(Γt0
m) −∇mFB(Γt0
m)

2
(A.52) ≤ M
X"
REFERENCES,0.5533428165007113,"m=0
H2
mG2
m
Et0
m
2
F .
(A.53)"
REFERENCES,0.5547652916073968,"Summing over the set of local iterations t0, . . . , t+
0 , where t+
0 := t0 + Q −1:"
REFERENCES,0.5561877667140825,"t+
0
X"
REFERENCES,0.5576102418207681,"t=t0
Ct−t0−1At0 = At0"
REFERENCES,0.5590327169274538,"t+
0
X"
REFERENCES,0.5604551920341394,"t=t0
Ct−t0−1
(A.54)"
REFERENCES,0.561877667140825,= At0 CQ −1
REFERENCES,0.5633001422475107,"C −1
(A.55) = At0"
REFERENCES,0.5647226173541963,"
1 + 2"
REFERENCES,0.566145092460882,"Q
Q
−1

1 + 2"
REFERENCES,0.5675675675675675,"Q

−1
(A.56)"
REFERENCES,0.5689900426742532,= QAt0
REFERENCES,0.5704125177809388,"
1 + 2"
REFERENCES,0.5718349928876245,"Q
Q
−1"
REFERENCES,0.5732574679943101,"2
(A.57)"
REFERENCES,0.5746799431009957,≤QAt0 e2 −1
REFERENCES,0.5761024182076814,"2
(A.58)"
REFERENCES,0.577524893314367,"≤4QAt0
(A.59) ≤4Q M
X"
REFERENCES,0.5789473684210527,"m=0
H2
mG2
m
Et0
m
2
F .
(A.60)"
REFERENCES,0.5803698435277382,"It is left to bound the second term in (A.51) over the set of local iterations t0, . . . , t0 + Q −1."
REFERENCES,0.5817923186344239,"t+
0
X"
REFERENCES,0.5832147937411095,"t=t0
(B0 + B1)Ct−t0−1 −1"
REFERENCES,0.5846372688477952,"C −1
≤"
REFERENCES,0.5860597439544808,"t+
0
X"
REFERENCES,0.5874822190611664,"t=t0
(B0 + B1)Ct−t0−1 −1"
REFERENCES,0.5889046941678521,"C −1
(A.61)"
REFERENCES,0.5903271692745377,= (B0 + B1) C −1  
REFERENCES,0.5917496443812233,"t+
0
X"
REFERENCES,0.5931721194879089,"t=t0
Ct−t0−1 −Q "
REFERENCES,0.5945945945945946,"
(A.62)"
REFERENCES,0.5960170697012802,= (B0 + B1) C −1
REFERENCES,0.5974395448079659,CQ −1
REFERENCES,0.5988620199146515,"C −1 −Q

(A.63)"
REFERENCES,0.6002844950213371,"=
(B0 + B1)

1 + 2"
REFERENCES,0.6017069701280228,"Q

−1  
"
REFERENCES,0.6031294452347084,"
1 + 2"
REFERENCES,0.604551920341394,"Q
Q
−1

1 + 2"
REFERENCES,0.6059743954480796,"Q

−1
−Q "
REFERENCES,0.6073968705547653,"

(A.64)"
REFERENCES,0.6088193456614509,"= Q(B0 + B1) 2  

"
REFERENCES,0.6102418207681366,"Q

1 + 2"
REFERENCES,0.6116642958748222,"Q
Q
−1
 2
−Q "
REFERENCES,0.6130867709815079,"


(A.65)"
REFERENCES,0.6145092460881935,"= Q2(B0 + B1) 2  
"
REFERENCES,0.615931721194879,"
1 + 2"
REFERENCES,0.6173541963015647,"Q
Q
−1 2
−1 "
REFERENCES,0.6187766714082503,"

(A.66)"
REFERENCES,0.620199146514936,≤Q2(B0 + B1) 2
REFERENCES,0.6216216216216216,e2 −1
REFERENCES,0.6230440967283073,"2
−1

(A.67)"
REFERENCES,0.6244665718349929,"≤2Q2(B0 + B1)
(A.68)
(A.69)"
REFERENCES,0.6258890469416786,Under review as a conference paper at ICLR 2022
REFERENCES,0.6273115220483642,Plugging the values for B0 and B1:
REFERENCES,0.6287339971550497,"t+
0
X"
REFERENCES,0.6301564722617354,"t=t0
(B0 + B1)Ct−t0−1 −1"
REFERENCES,0.631578947368421,"C −1
≤8Q2(ηt0)2 (1 + Q) M
X"
REFERENCES,0.6330014224751067,"m=0
L2
mEt0 h∇mFB(Γt0
m)
2i"
REFERENCES,0.6344238975817923,"+ 16Q2(1 + Q) M
X"
REFERENCES,0.635846372688478,"m=0
H2
mG2
m
Et0
m
2
F
(A.70)"
REFERENCES,0.6372688477951636,Applying Assumption 3 and adding in the ﬁrst term in (A.51):
REFERENCES,0.6386913229018493,"t+
0
X"
REFERENCES,0.6401137980085349,"t=t0
At ≤8Q2(ηt0)2 (1 + Q) M
X"
REFERENCES,0.6415362731152204,"m=0
L2
m
∇mF(Θt0)
2"
REFERENCES,0.6429587482219061,"+ 8Q2(ηt0)2 (1 + Q) M
X"
REFERENCES,0.6443812233285917,"m=0
L2
m
σ2
m
B"
REFERENCES,0.6458036984352774,"+ 4(4Q2(1 + Q) + Q) M
X"
REFERENCES,0.647226173541963,"m=0
H2
mG2
m
Et0
m
2
F
(A.71)"
REFERENCES,0.6486486486486487,"≤16Q3(ηt0)2
M
X"
REFERENCES,0.6500711237553343,"m=0
L2
m
∇mF(Θt0)
2"
REFERENCES,0.65149359886202,"+ 16Q3(ηt0)2
M
X"
REFERENCES,0.6529160739687055,"m=0
L2
m
σ2
m
B"
REFERENCES,0.6543385490753911,"+ 64Q3
M
X"
REFERENCES,0.6557610241820768,"m=0
H2
mG2
m
Et0
m
2
F .
(A.72)"
REFERENCES,0.6571834992887624,"A.1.3
PROOF OF THEOREMS 1 AND 2"
REFERENCES,0.6586059743954481,"Let t+
0 := t0 + Q −1. By Assumption 1:"
REFERENCES,0.6600284495021337,"F(Θt+
0 ) −F(Θt0) ≤
D
∇F(Θt0), Θt+
0 −Θt0E
+ L 2"
REFERENCES,0.6614509246088194,"Θt+
0 −Θt0

2
(A.73) = − *"
REFERENCES,0.662873399715505,"∇F(Θt0),"
REFERENCES,0.6642958748221907,"t+
0
X"
REFERENCES,0.6657183499288762,"t=t0
ηt0 ˆG
t
+ + L 2 "
REFERENCES,0.6671408250355618,"t+
0
X"
REFERENCES,0.6685633001422475,"t=t0
ηt0 ˆG
t 2"
REFERENCES,0.6699857752489331,(A.74) ≤−
REFERENCES,0.6714082503556188,"t+
0
X"
REFERENCES,0.6728307254623044,"t=t0
ηt0 D
∇F(Θt0), ˆG
tE
+ LQ 2"
REFERENCES,0.6742532005689901,"t+
0
X"
REFERENCES,0.6756756756756757,"t=t0
(ηt0)2 ˆG
t
2
(A.75)"
REFERENCES,0.6770981507823614,"where (A.75) follows from fact that (PN
n=1 xn)2 ≤N PN
n=1 x2
n."
REFERENCES,0.6785206258890469,Under review as a conference paper at ICLR 2022
REFERENCES,0.6799431009957326,We bound further:
REFERENCES,0.6813655761024182,"F(Θt+
0 ) −F(Θt0) ≤−"
REFERENCES,0.6827880512091038,"t+
0
X"
REFERENCES,0.6842105263157895,"t=t0
ηt0 D
∇F(Θt0), ˆG
t −Gt0E
−"
REFERENCES,0.6856330014224751,"t+
0
X"
REFERENCES,0.6870554765291608,"t=t0
ηt0 
∇F(Θt0), Gt0 + LQ 2"
REFERENCES,0.6884779516358464,"t+
0
X"
REFERENCES,0.689900426742532,"t=t0
(ηt0)2 ˆG
t −Gt0 + Gt0

2
(A.76) ≤−"
REFERENCES,0.6913229018492176,"t+
0
X"
REFERENCES,0.6927453769559033,"t=t0
ηt0 D
∇F(Θt0), ˆG
t −Gt0E
−"
REFERENCES,0.6941678520625889,"t+
0
X"
REFERENCES,0.6955903271692745,"t=t0
ηt0 
∇F(Θt0), Gt0 + LQ"
REFERENCES,0.6970128022759602,"t+
0
X"
REFERENCES,0.6984352773826458,"t=t0
(ηt0)2 ˆG
t −Gt0

2
+ LQ"
REFERENCES,0.6998577524893315,"t+
0
X"
REFERENCES,0.701280227596017,"t=t0
(ηt0)2 Gt02
(A.77) ="
REFERENCES,0.7027027027027027,"t+
0
X"
REFERENCES,0.7041251778093883,"t=t0
ηt0 D
−∇F(Θt0), Gt0 −ˆG
tE
−"
REFERENCES,0.705547652916074,"t+
0
X"
REFERENCES,0.7069701280227596,"t=t0
ηt0 
∇F(Θt0), Gt0 + LQ"
REFERENCES,0.7083926031294452,"t+
0
X"
REFERENCES,0.7098150782361309,"t=t0
(ηt0)2 ˆG
t −Gt0

2
+ LQ"
REFERENCES,0.7112375533428165,"t+
0
X"
REFERENCES,0.7126600284495022,"t=t0
(ηt0)2 Gt02 .
(A.78) ≤1 2"
REFERENCES,0.7140825035561877,"t+
0
X"
REFERENCES,0.7155049786628734,"t=t0
ηt0 ∇F(Θt0)
2 + 1 2"
REFERENCES,0.716927453769559,"t+
0
X"
REFERENCES,0.7183499288762447,"t=t0
ηt0
ˆG
t −Gt0

2
−"
REFERENCES,0.7197724039829303,"t+
0
X"
REFERENCES,0.7211948790896159,"t=t0
ηt0 
∇F(Θt0), Gt0 + LQ"
REFERENCES,0.7226173541963016,"t+
0
X"
REFERENCES,0.7240398293029872,"t=t0
(ηt0)2 ˆG
t −Gt0

2
+ LQ"
REFERENCES,0.7254623044096729,"t+
0
X"
REFERENCES,0.7268847795163584,"t=t0
(ηt0)2 Gt02
(A.79)"
REFERENCES,0.7283072546230441,where (A.79) follows from the fact that A · B = 1
REFERENCES,0.7297297297297297,2A2 + 1
REFERENCES,0.7311522048364154,2B2 −1
REFERENCES,0.732574679943101,2(A −B)2.
REFERENCES,0.7339971550497866,We apply the expectation Et0 to both sides of (A.79):
REFERENCES,0.7354196301564723,"Et0 h
F(Θt+
0 )
i
−F(Θt0) ≤−1 2"
REFERENCES,0.7368421052631579,"t+
0
X"
REFERENCES,0.7382645803698435,"t=t0
ηt0 ∇F(Θt0)
2 + 1 2"
REFERENCES,0.7396870554765291,"t+
0
X"
REFERENCES,0.7411095305832148,"t=t0
ηt0(1 + LQηt0)Et0
ˆG
t −Gt0

2 + LQ"
REFERENCES,0.7425320056899004,"t+
0
X"
REFERENCES,0.7439544807965861,"t=t0
(ηt0)2Et0 hGt02i
(A.80) ≤−1 2"
REFERENCES,0.7453769559032717,"t+
0
X"
REFERENCES,0.7467994310099573,"t=t0
ηt0(1 −LQηt0)
∇F(Θt0)
2 + 1 2"
REFERENCES,0.748221906116643,"t+
0
X"
REFERENCES,0.7496443812233285,"t=t0
ηt0(1 + LQηt0)Et0
ˆG
t −Gt0

2
+ LQ M
X m=0"
REFERENCES,0.7510668563300142,"σ2
m
B"
REFERENCES,0.7524893314366998,"t+
0
X"
REFERENCES,0.7539118065433855,"t=t0
(ηt0)2"
REFERENCES,0.7553342816500711,(A.81) = −Q
REFERENCES,0.7567567567567568,"2 ηt0(1 −LQηt0)
∇F(Θt0)
2 + 1 2"
REFERENCES,0.7581792318634424,"t+
0
X"
REFERENCES,0.7596017069701281,"t=t0
ηt0(1 + LQηt0)Et0
ˆG
t −Gt0

2
+ LQ2(ηt0)2
M
X m=0"
REFERENCES,0.7610241820768137,"σ2
m
B"
REFERENCES,0.7624466571834992,(A.82)
REFERENCES,0.7638691322901849,Under review as a conference paper at ICLR 2022
REFERENCES,0.7652916073968705,"where (A.80) follows from applying Assumption 2 and noting that Et0 
Gt0
= ∇F(Θt0), and
(A.82) follows from Assumption 3."
REFERENCES,0.7667140825035562,Applying Lemma 2 to (A.82):
REFERENCES,0.7681365576102418,"Et0 h
F(Θt+
0 )
i
−F(Θt0) ≤−Q"
REFERENCES,0.7695590327169275,"2 ηt0(1 −LQηt0)
∇F(Θt0)
2"
REFERENCES,0.7709815078236131,"+ 8Q3(ηt0)3(1 + LQηt0) M
X"
REFERENCES,0.7724039829302988,"m=0
L2
m
∇mF(Θt0
m)
2"
REFERENCES,0.7738264580369844,"+ 8Q3(ηt0)3(1 + LQηt0) M
X"
REFERENCES,0.7752489331436699,"m=0
L2
m
σ2
m
B"
REFERENCES,0.7766714082503556,"+ 32Q3ηt0(1 + LQηt0) M
X"
REFERENCES,0.7780938833570412,"m=0
H2
mG2
m
Et0
m
2
F"
REFERENCES,0.7795163584637269,"+ LQ2(ηt0)2
M
X m=0"
REFERENCES,0.7809388335704125,"σ2
m
B
(A.83) ≤−Q 2 M
X"
REFERENCES,0.7823613086770982,"m=0
ηt0(1 −LQηt0 −16Q2L2
m(ηt0)2 −16Q3L2
mL(ηt0)3))
∇mF(Θt0)
2"
REFERENCES,0.7837837837837838,"+ (LQ2(ηt0)2 + 8Q3L2
m(ηt0)3 + 8Q4LL2
m(ηt0)4) M
X m=0"
REFERENCES,0.7852062588904695,"σ2
m
B"
REFERENCES,0.786628733997155,"+ 32Q3ηt0(1 + LQηt0) M
X"
REFERENCES,0.7880512091038406,"m=0
H2
mG2
m
Et0
m
2
F .
(A.84)"
REFERENCES,0.7894736842105263,"Let ηt0 ≤
1
16Q max{L,maxm Lm}. Then we bound (A.84) further:"
REFERENCES,0.7908961593172119,"Et0 h
F(Θt+
0 )
i
−F(Θt0) ≤−Q 2 M
X"
REFERENCES,0.7923186344238976,"m=0
ηt0(1 −1 16 −1"
REFERENCES,0.7937411095305832,"16 −
1
162 ))
∇mF(Θt0)
2"
REFERENCES,0.7951635846372689,"+ (LQ2(ηt0)2 + 8Q3L2
m(ηt0)3 + 8Q4LL2
m(ηt0)4) M
X m=0"
REFERENCES,0.7965860597439545,"σ2
m
B"
REFERENCES,0.7980085348506402,"+ 16Q3ηt0(1 + LQηt0) M
X"
REFERENCES,0.7994310099573257,"m=0
H2
mG2
m
Et0
m
2
F
(A.85) ≤−Q"
REFERENCES,0.8008534850640113,"2 ηt0 ∇F(Θt0)
2"
REFERENCES,0.802275960170697,"+ (LQ2(ηt0)2 + 8Q3L2
m(ηt0)3 + 8Q4LL2
m(ηt0)4) M
X m=0"
REFERENCES,0.8036984352773826,"σ2
m
B"
REFERENCES,0.8051209103840683,"+ 32Q3ηt0(1 + LQηt0) M
X"
REFERENCES,0.8065433854907539,"m=0
H2
mG2
m
Et0
m
2
F
(A.86)"
REFERENCES,0.8079658605974396,Under review as a conference paper at ICLR 2022
REFERENCES,0.8093883357041252,After some rearranging of terms:
REFERENCES,0.8108108108108109,"ηt0 ∇F(Θt0)
2 ≤
2
h
F(Θt0) −Et0
h
F(Θt+
0 )
ii Q"
REFERENCES,0.8122332859174964,"+ 2(LQ(ηt0)2 + 8Q2L2
m(ηt0)3 + 8Q3LL2
m(ηt0)4) M
X m=0"
REFERENCES,0.813655761024182,"σ2
m
B"
REFERENCES,0.8150782361308677,"+ 64Q2ηt0(1 + LQηt0) M
X"
REFERENCES,0.8165007112375533,"m=0
H2
mG2
m
Et0
m
2
F
(A.87)"
REFERENCES,0.817923186344239,"Summing over all communication rounds t0 = 0, . . . , R −1 and taking total expectation: R−1
X"
REFERENCES,0.8193456614509246,"t0=0
ηt0E
h∇F(Θt0)
2i
≤2

F(Θ0) −E

F(ΘT )
 Q + 2 R−1
X"
REFERENCES,0.8207681365576103,"t0=0
(LQ(ηt0)2 + 8Q2L2
m(ηt0)3 + 8Q3LL2
m(ηt0)4) M
X m=0"
REFERENCES,0.8221906116642959,"σ2
m
B"
REFERENCES,0.8236130867709816,"+ 64Q2ηt0(1 + LQηt0) M
X"
REFERENCES,0.8250355618776671,"m=0
H2
mG2
m
Et0
m
2
F
(A.88)"
REFERENCES,0.8264580369843528,"≤2

F(Θ0) −E

F(ΘT )
 QR + 2 R−1
X"
REFERENCES,0.8278805120910384,"t0=0
(QL(ηt0)2 + 8Q2L2
m(ηt0)3 + 8Q3LL2
m(ηt0)4) M
X m=0"
REFERENCES,0.829302987197724,"σ2
m
B"
REFERENCES,0.8307254623044097,"+ 64Q2
R−1
X"
REFERENCES,0.8321479374110953,"t0=0
ηt0(1 + LQηt0) M
X"
REFERENCES,0.833570412517781,"m=0
H2
mG2
mE
hEt0
m
2
F"
REFERENCES,0.8349928876244666,"i
.
(A.89)"
REFERENCES,0.8364153627311522,"Note that: M
X"
REFERENCES,0.8378378378378378,"m=0
H2
mG2
mE
hEt0
m
2
F i
= M
X"
REFERENCES,0.8392603129445235,"m=0
H2
mG2
m
X"
REFERENCES,0.8406827880512091,"j̸=m
E
hϵt0
j 2 F"
REFERENCES,0.8421052631578947,"i
(A.90) = M
X"
REFERENCES,0.8435277382645804,"m=0
H2
mG2
m
X"
REFERENCES,0.844950213371266,"j̸=m
Et0
j
(A.91)"
REFERENCES,0.8463726884779517,where (A.91) follows from Deﬁnition 1.
REFERENCES,0.8477951635846372,"Plugging this into (A.89) R−1
X"
REFERENCES,0.8492176386913229,"t0=0
ηt0E
h∇F(Θt0)
2i
≤2

F(Θ0) −E

F(ΘT )
 QR + 2 R−1
X"
REFERENCES,0.8506401137980085,"t0=0
(QL(ηt0)2 + 8Q2L2
m(ηt0)3 + 8Q3LL2
m(ηt0)4) M
X m=0"
REFERENCES,0.8520625889046942,"σ2
m
B"
REFERENCES,0.8534850640113798,"+ 64Q2
R−1
X"
REFERENCES,0.8549075391180654,"t0=0
ηt0(1 + LQηt0) M
X"
REFERENCES,0.8563300142247511,"m=0
H2
mG2
m
X"
REFERENCES,0.8577524893314367,"j̸=m
Et0
j .
(A.92)"
REFERENCES,0.8591749644381224,Under review as a conference paper at ICLR 2022
REFERENCES,0.8605974395448079,"Suppose that ηt0 = η for all communication rounds t0. Then, averaging over R communication
rounds, we have:"
R,0.8620199146514936,"1
R R−1
X"
R,0.8634423897581792,"t0=0
E
h∇F(Θt0)
2i
≤2

F(Θ0) −E

F(ΘT )
"
R,0.8648648648648649,"QRη
+ 2 M
X"
R,0.8662873399715505,"m=0
(QLη + 8Q2L2
mη2 + 8Q3LL2
mη3)σ2
m
B"
R,0.8677098150782361,"+ 64Q2 R R−1
X"
R,0.8691322901849218,"t0=0
(1 + LQη) M
X"
R,0.8705547652916074,"m=0
H2
mG2
m
X"
R,0.871977240398293,"j̸=m
Et0
j .
(A.93)"
R,0.8733997155049786,"≤2

F(Θ0) −E

F(ΘT )
"
R,0.8748221906116643,"QRη
+ 4 M
X"
R,0.8762446657183499,"m=0
QLη σ2
m
B"
R,0.8776671408250356,"+ 68Q2 R R−1
X t0=0 M
X"
R,0.8790896159317212,"m=0
H2
mG2
m
X"
R,0.8805120910384068,"j̸=m
Et0
j .
(A.94)"
R,0.8819345661450925,"where (A.94) follows from our assumption that ηt0 ≤
1
16Q max{L,maxm Lm}. This completes the
proof of Theorem 1."
R,0.883357041251778,"We continue our analysis to prove Theorem 2. Starting from (A.92), we bound the left-hand side
with the minimum over all iterations:"
R,0.8847795163584637,"min
t0=0,...,R−1 E
h∇F(Θt0)
2i
≤2

F(Θ0) −Et0 
F(ΘT )
"
R,0.8862019914651493,"Q PR−1
t0=0 ηt0 + 2  QL"
R,0.887624466571835,"PR−1
t0=0(ηt0)2
PR−1
t0=0 ηt0
+ 8Q2L2
m"
R,0.8890469416785206,"PR−1
t0=0(ηt0)3
PR−1
t0=0 ηt0
+ 8Q3LL2
m"
R,0.8904694167852063,"PR−1
t0=0(ηt0)4
PR−1
t0=0 ηt0 !
M
X m=0"
R,0.8918918918918919,"σ2
m
B"
R,0.8933143669985776,"+ 64Q2
M
X"
R,0.8947368421052632,"m=0
H2
mG2
m"
R,0.8961593172119487,"PR−1
t0=0 ηt0 P"
R,0.8975817923186344,"j̸=m Et0
j
PR−1
t0=0 ηt0
+ 64LQ3
M
X"
R,0.89900426742532,"m=0
H2
mG2
m"
R,0.9004267425320057,"PR−1
t0=0(ηt0)2 P"
R,0.9018492176386913,"j̸=m Et0
j
PR−1
t0=0 ηt0
(A.95)"
R,0.903271692745377,"As R →∞, if PR−1
t0=0 ηt0 = ∞, PR−1
t0=0(ηt0)2 < ∞, and PR−1
t0=0 ηt0 P"
R,0.9046941678520626,"j̸=m Et0
j
< ∞, then"
R,0.9061166429587483,"mint0=0,...,R−1 E
h
∥∇F(Θt0)∥2i
→0. This completes the proof of Theorem 2."
R,0.9075391180654339,"A.2
COMMON COMPRESSORS"
R,0.9089615931721194,"In this section, we calculate the compression error and parameter bounds for uniform scalar quanti-
zation, lattice vector quantization and top-k sparsiﬁcation, as well as discuss implementation details
of these compressors in C-VFL."
R,0.9103840682788051,"We ﬁrst consider a uniform scalar quantizer (Bennett, 1948) with a set of 2q quantization levels,
where q is the number of bits to represent compressed values.
We deﬁne the range of values
that quantize to the same quantization level as the quantization bin. In C-VFL, a scalar quan-
tizer quantizes each individual component of embeddings. The error in each embedding of a batch
B in scalar quantization is ≤Pm ∆2"
R,0.9118065433854907,"12 = Pm
(hmax−hmin)2"
R,0.9132290184921764,"12
2−2q where ∆the size of a quantiza-
tion bin, Pm is the size of the m-th embedding, hmax and hmin are respectively the maximum
and minimum value hm(θt
m; xi
m) can be for all iterations t, parties m, and xi
m. We note that if
hmax or hmin are unbounded, then the error is unbounded as well. By Theorem 1, we know that
1
R
PR−1
t0=0
PM
m=0 Et0
m = O( 1
√"
R,0.914651493598862,"T ) to obtain a convergence rate of O( 1
√"
R,0.9160739687055477,"T ). If we use the same q for
all parties and iterations, we can solve for q to ﬁnd that the value q must be lower bounded by
q = Ω(log2(Pm(hmax −hmin)2√"
R,0.9174964438122333,"T)) to reach a convergence rate of O( 1
√"
R,0.918918918918919,"T ). For a diminishing
compression error, required by Theorem 2, we let T = t0 in this bound, indicating that q, the number
of quantization bins, must increase as training continues."
R,0.9203413940256046,"A vector quantizer creates a set of d-dimensional vectors called a codebook (Zamir & Feder, 1996).
A vector is quantized by dividing the components into sub-vectors of size d, then quantizing each
sub-vector to the nearest codebook vector in Euclidean distance. A cell in vector quantization is"
R,0.9217638691322901,Under review as a conference paper at ICLR 2022
R,0.9231863442389758,"deﬁned as all points in d-space that quantizes to a single codeword. The volume of these cells are de-
termined by how closely packed codewords are. We consider the commonly applied 2-dimensional
hexagonal lattice quantizer (Shlezinger et al., 2021). In C-VFL, each embedding is divided into
sub-vectors of size two, scaled to the unit square, then quantized to the nearest vector by Euclidean
distance in the codebook. The error in this vector quantizer is ≤V Pm"
WHERE V IS THE VOLUME OF A,0.9246088193456614,"24
where V is the volume of a
lattice cell. The more bits available for quantization, the smaller the volume of the cells, the smaller
the compression error. We can calculate an upper bound on V based on Theorem 1: V = O(
1
Pm
√"
WHERE V IS THE VOLUME OF A,0.9260312944523471,"T ).
If a diminishing compression error is required, we can set T = t0 in this bound, indicating that V
must decrease at a rate of O(
1
Pm
√t0 ). As the number of iterations increases, the smaller V must be,
and thus the more bits that must be communicated."
WHERE V IS THE VOLUME OF A,0.9274537695590327,"In top-k sparsiﬁcation (Lin et al., 2018), when used in distributed SGD algorithms, the k largest
magnitude components of the gradient are sent while the rest are set to zero. In the case of embed-
dings in C-VFL, a large element may be as important as an input to the server model as a small
one. We can instead select the k embedding elements to send with the largest magnitude partial
derivatives in ∇θmhm(θt
m). Since a party m cannot calculate ∇θmhm(θt
m) until all parties send
their embeddings, party m can use the embedding gradient calculated in the previous iteration,
∇θmhm(θt−1
m ). This is an intuitive method, as we assume our gradients are Lipschitz continuous,
and thus do not change too rapidly. The error of sparsiﬁcation is ≤(1 −
k
Pm )(∥h∥2)max where
(∥h∥2)max is the maximum value of ∥hm(θt
m; xi
m)∥2 for all iterations t, parties m, and xi
m. Note
that if (∥h∥2)max is unbounded, then the error is unbounded. We can calculate a lower bound on
k: k = Ω(Pm −
Pm
(∥h∥2)max
√"
WHERE V IS THE VOLUME OF A,0.9288762446657184,"T ). Note that the larger (∥h∥2)max, the larger k must be. More com-
ponents must be sent if embedding magnitude is large in order to achieve a convergence rate of
O( 1
√"
WHERE V IS THE VOLUME OF A,0.930298719772404,"T ). When considering a diminishing compression error, we set T = t0, showing that k must
increase over the course of training."
WHERE V IS THE VOLUME OF A,0.9317211948790897,"A.3
EXPERIMENTAL DETAILS"
WHERE V IS THE VOLUME OF A,0.9331436699857752,"For our experiments, we used an internal cluster of 40 compute nodes running CentOS 7 each with
2× 20-core 2.5 GHz Intel Xeon Gold 6248 CPUs, 8× NVIDIA Tesla V100 GPUs with 32 GB
HBM, and 768 GB of RAM."
WHERE V IS THE VOLUME OF A,0.9345661450924608,"A.3.1
MIMIC-III"
WHERE V IS THE VOLUME OF A,0.9359886201991465,"The MIMIC-III dataset can be found at: mimic.physionet.org. The dataset consists of time-series
data from ∼60,000 intensive care unit admissions. The data includes many features about each
patient, such as demographic, vital signs, medications, and more. All the data is anonymized. In
order to gain access to the dataset, one must take the short online course provided on their website."
WHERE V IS THE VOLUME OF A,0.9374110953058321,"Our code for training with the MIMIC-III dataset can be found in in the folder titled “mimic3”.
This is an extension of the MIMIC-III benchmarks repo found at: github.com/YerevaNN/mimic3-
benchmarks. The original code preprocesses the MIMIC-III dataset and provides starter code for
training LSTMs using centralized SGD. Our code has updated their existing code to TensorFlow 2.
The new ﬁle of interest in our code base is “mimic3models/in hospital mortality/quant.py” which
runs C-VFL. Both our code base and the original are under the MIT License. More details on
installation, dependencies, and running our experiments can be found in “README.md”. Each
experiment took approximately six hours to run on a node in our cluster."
WHERE V IS THE VOLUME OF A,0.9388335704125178,"The benchmarking preprocessing code splits the data up into different prediction cases. Our exper-
iments train models to predict for in-hospital mortality. For in-hospital mortality, there are 14,681
training samples, and 3,236 test samples. In our experiments, we use a step size of 0.01, as is
standard for training an LSTM on the MIMIC-III dataset."
WHERE V IS THE VOLUME OF A,0.9402560455192034,"A.3.2
MODELNET10"
WHERE V IS THE VOLUME OF A,0.9416785206258891,"Details on the ModelNet10 dataset can be found at:
modelnet.cs.princeton.edu/.
The
speciﬁc
link
we
downloaded
the
dataset
from
is
the
following
Google
Drive
link:
https://drive.google.com/ﬁle/d/0B4v2jR3WsindMUE3N2xiLVpyLW8/view. The dataset consists of
3D CAD models of different common objects in the world. For each CAD model, there are 12 views"
WHERE V IS THE VOLUME OF A,0.9431009957325747,Under review as a conference paper at ICLR 2022
WHERE V IS THE VOLUME OF A,0.9445234708392604,"from different angles saved as PNG ﬁles. We only trained our models on the following 10 classes:
bathtub, bed, chair, desk, dresser, monitor, night stand, sofa, table, toilet. We used a subset of the
data with 1,008 training samples and 918 test samples. In our experiments, we use a step size of
0.001, as is standard for training a CNN on the ModelNet10 dataset."
WHERE V IS THE VOLUME OF A,0.9459459459459459,"Our code for learning on the ModelNet10 dataset is in the folder “MVCNN Pytorch” and is an
extension of the MVCNN-PyTorch repo: github.com/RBirkeland/MVCNN-PyTorch. The ﬁle of
interest in our code base is “quant.py” which runs C-VFL. Both our code base and the original are
under the MIT License. Details on how to run our experiments can be found in the “README.md”.
Each experiment took approximately six hours to run on a node in our cluster."
WHERE V IS THE VOLUME OF A,0.9473684210526315,"A.4
ADDITIONAL EXPERIMENTS"
WHERE V IS THE VOLUME OF A,0.9487908961593172,"In this section we provide some additional experiments to test the scalability of C-VFL for a larger
number of parties and larger datasets."
WHERE V IS THE VOLUME OF A,0.9502133712660028,"First, we run C-VFL using the same parameters as described in Section 5, now with 48 parties and a
server with the ModelNet10 dataset. Each 3D CAD model in the ModelNet10 dataset has 12 views,
so we assign every four parties the same view, and have each store a different quadrant of the image."
WHERE V IS THE VOLUME OF A,0.9516358463726885,"(a) Plotted by epochs
(b) Plotted by cost
(c) Vector quantization"
WHERE V IS THE VOLUME OF A,0.9530583214793741,"Figure A.1: Test accuracy on ModelNet10 dataset with the number of parties M = 48. In the ﬁrst
two plots, the compressors have b = 2, where b is the number of bits used to represent embedding
components. In the third plot, b = 32 indicates there is no compression. The results show little
variation between compressors and no compression, leading to a large beneﬁt in communication
cost versus test accuracy."
WHERE V IS THE VOLUME OF A,0.9544807965860598,"In Figure A.1, we plot the test accuracy for the ModelNet10 dataset. The test accuracy is overall
lower than when running with 4 and 12 parties. This is expected, as each party has less information
individually, making the prediction task more difﬁcult. Figure A.1a shows the test accuracy plotted
by epochs. There is very little variation between compressors and no compression here. This leads
to a very large beneﬁt for compression when plotting by communication cost, seen in Figure A.1b.
In Figure A.1c, we plot the test accuracy of C-VFL using vector quantization for different values of
b, the number of bits to represent compressed values. Similar to previous results, lower b tends to
improve test accuracy reached with the same amount of communication cost. We can also see that
the total cost of communication has increased compared to the case of 4 and 12 parties in Figure 3.
This is expected, as there are more embeddings being exchanged in each global round."
WHERE V IS THE VOLUME OF A,0.9559032716927454,"We also run C-VFL on CIFAR-10, a large dataset of 60,000 images with 10 classes of objects
and animals. To simulate a VFL scenario with the CIFAR-10 dataset, we split the images into 4
quadrants and run C-VFL with 4 parties and a server. Each party trains ResNet-18 locally, and the
server model is a single fully-connected layer."
WHERE V IS THE VOLUME OF A,0.957325746799431,Under review as a conference paper at ICLR 2022
WHERE V IS THE VOLUME OF A,0.9587482219061166,"(a) Plotted by epochs
(b) Plotted by cost
(c) Vector quantization"
WHERE V IS THE VOLUME OF A,0.9601706970128022,"Figure A.2: Test accuracy on CIFAR-10 dataset with the number of parties M = 4. In the ﬁrst
two plots, the compressors have b = 2, where b is the number of bits used to represent embedding
components. In the third plot, b = 32 indicates there is no compression. The results show vector
quantization performs the best our of the compressors, and all compressors show improvement over
no compression in terms of communication cost to reach target test accuracies."
WHERE V IS THE VOLUME OF A,0.9615931721194879,"In Figure A.2, we plot the test accuracy for the CIFAR-10 dataset. The test accuracy is fairly
low compared to typical baseline accuracies, which is expected, as learning object classiﬁcation
from only a quadrant of a 32 × 32 pixel image is difﬁcult. Figure A.2a shows the test accuracy
plotted by epochs. We can see that vector quantization performs almost as well as no compression
in the CIFAR-10 dataset. When plotting by communication cost, seen in Figure A.2b, we can
see that vector quantization performs the best, though scalar quantization and top-k sparsiﬁcation
show communication savings as well. In Figure A.2c, we plot the test accuracy of C-VFL using
vector quantization for different values of b, the number of bits to represent compressed values.
Similar to previous results, lower b tends to improve test accuracy reached with the same amount of
communication cost."
WHERE V IS THE VOLUME OF A,0.9630156472261735,"A.5
ADDITIONAL PLOTS"
WHERE V IS THE VOLUME OF A,0.9644381223328592,"In this section, we include additional plots using the results from the experiments introduced in
Section 5 of the main paper. The setup for the experiments is described in the main paper. These
plots provide some additional insight into the effect of each compressor on convergence in both
datasets. As with the plots in the main paper, the solid lines in each plot are the average of ﬁve runs
and the shaded regions represent the standard deviation."
WHERE V IS THE VOLUME OF A,0.9658605974395448,"(a) 2 bits per parameter
(b) 3 bits per parameter
(c) 4 bits per parameter"
WHERE V IS THE VOLUME OF A,0.9672830725462305,"Figure A.3: Training loss on MIMIC-III dataset. One can see that, with the exception of top-k
sparsiﬁcation, allowing more bits for compression moves the training loss closer to the baseline of
no compression. Top-k appears to be more unstable in the MIMIC-III dataset."
WHERE V IS THE VOLUME OF A,0.968705547652916,Under review as a conference paper at ICLR 2022
WHERE V IS THE VOLUME OF A,0.9701280227596017,"(a) 2 bits per parameter
(b) 3 bits per parameter
(c) 4 bits per parameter"
WHERE V IS THE VOLUME OF A,0.9715504978662873,"Figure A.4: Test F1-Score on MIMIC-III dataset. Scalar and vector quantization achieve similar
test F1-score even when only using 2 bits in quantization. On the other hand, top-k sparsiﬁcation
performs worse than the other compressors in the MIMIC-III dataset."
WHERE V IS THE VOLUME OF A,0.972972972972973,"Figures A.3 and A.4 plot the training loss and test F1-Score for training on the MIMIC-III dataset
for different levels of compression. We can see that scalar and vector quantization perform similarly
to no compression and improve as the number of bits available increase. We can also see that top-k
sparsiﬁcation has high variability on the MIMIC-III dataset and generally performs worse than the
other compressors."
WHERE V IS THE VOLUME OF A,0.9743954480796586,"(a) 2 bits per parameter
(b) 3 bits per parameter
(c) 4 bits per parameter"
WHERE V IS THE VOLUME OF A,0.9758179231863442,"Figure A.5: Test F1-Score on MIMIC-III dataset plotted by communication cost. We can see that
all compressors reach higher F1-scores with lower communication cost than no compression. We
can see that the standard deviation for each compressor decreases as the number of bits available
increases. Top-k sparsiﬁcation generally performs worse than the other compressors on the MIMIC-
III-dataset."
WHERE V IS THE VOLUME OF A,0.9772403982930299,"(a) Scalar quantization
(b) Vector quantization
(c) Top-k sparsiﬁcation"
WHERE V IS THE VOLUME OF A,0.9786628733997155,"Figure A.6: Test F1-Score on MIMIC-III dataset plotted by communication cost. We can see that
all compressors reach higher F1-scores with lower communication cost than no compression. We
can see that the standard deviation for each compressor decreases as the number of bits available
increases. Top-k sparsiﬁcation generally performs worse than the other compressors on the MIMIC-
III-dataset."
WHERE V IS THE VOLUME OF A,0.9800853485064012,"Figures A.5 and A.6 plot the test F1-Score for training on the MIMIC-III dataset plotted against the
communication cost. The plots in Figure A.5 include all compression techniques for a given level of"
WHERE V IS THE VOLUME OF A,0.9815078236130867,Under review as a conference paper at ICLR 2022
WHERE V IS THE VOLUME OF A,0.9829302987197724,"compression, while the plots in Figure A.6 include all levels of compression for a given compression
technique. We can see that all compressors reach higher F1-scores with lower communication cost
than no compression. It is interesting to note that increasing the number of bits per parameter reduces
the variability in all compressors."
WHERE V IS THE VOLUME OF A,0.984352773826458,"(a) 2 bits per parameter
(b) 3 bits per parameter
(c) 4 bits per parameter"
WHERE V IS THE VOLUME OF A,0.9857752489331437,"Figure A.7: Training loss on ModelNet10 dataset. Vector quantization and top-k sparsiﬁcation per-
form similarly to no compression, even when only 2 bits are available. Scalar quantization converges
to a higher loss and has high variability on the ModelNet10 dataset."
WHERE V IS THE VOLUME OF A,0.9871977240398293,"(a) 2 bits per parameter
(b) 3 bits per parameter
(c) 4 bits per parameter"
WHERE V IS THE VOLUME OF A,0.9886201991465149,"Figure A.8: Test accuracy on ModelNet10 dataset. Vector quantization and top-k sparsiﬁcation per-
form similarly to no compression, even when only 2 bits are available. Scalar quantization converges
to a lower test accuracy, and has high variability on the ModelNet10 dataset."
WHERE V IS THE VOLUME OF A,0.9900426742532006,"Figures A.7 and A.8 plot the training loss and test accuracy for training on the ModelNet10 dataset.
Vector quantization and top-k sparsiﬁcation perform similarly to no compression in both training
loss and test accuracy, even when only 2 bits are available. We can see that scalar quantization has
high variability on the ModelNet10 dataset."
WHERE V IS THE VOLUME OF A,0.9914651493598862,"(a) 2 bits per parameter
(b) 3 bits per parameter
(c) 4 bits per parameter"
WHERE V IS THE VOLUME OF A,0.9928876244665719,"Figure A.9: Test accuracy on ModelNet10 dataset plotted by communication cost. We can see that
all compressors reach higher accuracies with lower communication cost than no compression. Scalar
quantization generally performs worse than the other compressors on the ModelNet10 dataset."
WHERE V IS THE VOLUME OF A,0.9943100995732574,Under review as a conference paper at ICLR 2022
WHERE V IS THE VOLUME OF A,0.9957325746799431,"(a) Scalar quantization
(b) Vector quantization
(c) Top-k sparsiﬁcation"
WHERE V IS THE VOLUME OF A,0.9971550497866287,"Figure A.10: Test accuracy on ModelNet10 dataset plotted by communication cost. We can see that
all compressors reach higher accuracies with lower communication cost than no compression. We
can see that when less bits are used in each compressor, higher test accuracies are reached at lower
communication costs. Scalar quantization generally performs worse than the other compressors on
the ModelNet10 dataset."
WHERE V IS THE VOLUME OF A,0.9985775248933144,"Figures A.9 and A.10 plot the test accuracy for training on the ModelNet10 dataset against the com-
munication cost. The plots in Figure A.9 include all compression techniques for a given level of
compression, while the plots in Figure A.10 include all levels of compression for a given compres-
sion technique. We can see that all compressors reach higher accuracies with lower communication
cost than no compression. Scalar quantization generally performs worse than the other compressors
on the ModelNet10 dataset. From Figure A.10, we also see that when fewer bits are used in each
compressor, higher test accuracies are reached at lower communication costs."
