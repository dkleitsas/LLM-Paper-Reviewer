Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009624639076034649,"A central task in computational drug discovery is to construct models from known
1"
ABSTRACT,0.0019249278152069298,"active molecules to ﬁnd further promising molecules for subsequent screening.
2"
ABSTRACT,0.0028873917228103944,"However, typically only very few active molecules are known. Therefore, few-shot
3"
ABSTRACT,0.0038498556304138597,"learning methods have the potential to improve the effectiveness of this critical
4"
ABSTRACT,0.004812319538017324,"phase of the drug discovery process. We introduce a new method for few-shot
5"
ABSTRACT,0.005774783445620789,"drug discovery. Its main idea is to enrich a molecule representation by knowledge
6"
ABSTRACT,0.006737247353224254,"about known context or reference molecules. Our novel concept for molecule
7"
ABSTRACT,0.007699711260827719,"representation enrichment is to associate molecules from both the support set and
8"
ABSTRACT,0.008662175168431183,"the query set with a large set of reference (context) molecules through a modern
9"
ABSTRACT,0.009624639076034648,"Hopﬁeld network. Intuitively, this enrichment step is analogous to a human expert
10"
ABSTRACT,0.010587102983638113,"who would associate a given molecule with familiar molecules whose properties
11"
ABSTRACT,0.011549566891241578,"are known. The enrichment step reinforces and ampliﬁes the covariance structure
12"
ABSTRACT,0.012512030798845043,"of the data, while simultaneously removing spurious correlations arising from the
13"
ABSTRACT,0.013474494706448507,"decoration of molecules. Our approach is compared with other few-shot methods
14"
ABSTRACT,0.014436958614051972,"for drug discovery on the FS-Mol benchmark dataset. On FS-Mol, our approach
15"
ABSTRACT,0.015399422521655439,"outperforms all compared methods and therefore sets a new state-of-the art for
16"
ABSTRACT,0.016361886429258902,"few-shot learning in drug discovery. An ablation study shows that the enrichment
17"
ABSTRACT,0.017324350336862367,"step of our method is the key to improve the predictive quality. In a domain shift
18"
ABSTRACT,0.01828681424446583,"experiment, we further demonstrate the robustness of our method.
19"
INTRODUCTION,0.019249278152069296,"1
Introduction
20"
INTRODUCTION,0.02021174205967276,"To improve human health, combat diseases, and tackle pandemics there is a steady need of discovering
21"
INTRODUCTION,0.021174205967276226,"new drugs in a fast and efﬁcient way. However, the drug discovery process is time-consuming and
22"
INTRODUCTION,0.02213666987487969,"cost-intensive (Arrowsmith, 2011). Deep learning methods have recently been shown to reduce
23"
INTRODUCTION,0.023099133782483156,"time and costs of this process (Chen et al., 2018; Walters and Barzilay, 2021). They diminish the
24"
INTRODUCTION,0.02406159769008662,"required number of both wet-lab measurements and molecules that must be synthesized (Merk et al.,
25"
INTRODUCTION,0.025024061597690085,"2018; Schneider et al., 2020). However, as of now, deep learning approaches use only the molecular
26"
INTRODUCTION,0.02598652550529355,"information about the ligands after being trained on a large training set. At inference time, they yield
27"
INTRODUCTION,0.026948989412897015,"highly accurate property and activity prediction (Mayr et al., 2018; Yang et al., 2019), generative
28"
INTRODUCTION,0.02791145332050048,"(Segler et al., 2018a; Gómez-Bombarelli et al., 2018), or synthesis models (Segler et al., 2018b; Seidl
29"
INTRODUCTION,0.028873917228103944,"et al., 2022).
30"
INTRODUCTION,0.029836381135707413,"Deep learning methods in drug discovery usually require large amounts of biological measure-
31"
INTRODUCTION,0.030798845043310877,"ments. To train deep learning-based activity and property prediction models with high predictive per-
32"
INTRODUCTION,0.03176130895091434,"formance, hundreds or thousands of data points per task are required. For example, well-performing
33"
INTRODUCTION,0.032723772858517804,"predictive models for activity prediction tasks of ChEMBL have been trained with an average of 3,621
34"
INTRODUCTION,0.03368623676612127,"activity points per task, i.e., drug target, by (Mayr et al., 2018). The ExCAPE-DB dataset provides on
35"
INTRODUCTION,0.03464870067372473,"average 42,501 measurements per task (Sun et al., 2017; Sturm et al., 2020). (Wu et al., 2018) pub-
36"
INTRODUCTION,0.0356111645813282,"lished a large scale benchmark for molecular machine learning, including prediction models for the
37"
INTRODUCTION,0.03657362848893166,"SIDER dataset (Kuhn et al., 2016) with an average of 5,187 data points, Tox21 (Huang et al., 2016b;
38"
INTRODUCTION,0.03753609239653513,"Mayr et al., 2016) with on average 9,031, and ClinTox (Wu et al., 2018) with 1,491 measurements
39"
INTRODUCTION,0.03849855630413859,"per task. However, for typical drug design projects, the amount of available measurements is very
40"
INTRODUCTION,0.03946102021174206,"limited (Stanley et al., 2021; Waring et al., 2015; Hochreiter et al., 2018), since in-vitro experiments
41"
INTRODUCTION,0.04042348411934552,"are expensive and time-consuming. Therefore, methods that need only few measurements to build
42"
INTRODUCTION,0.04138594802694899,"precise prediction models are desirable. This problem — i.e., the challenge of learning from few
43"
INTRODUCTION,0.04234841193455245,"data points — is the focus of machine learning areas like meta-learning (Schmidhuber, 1987; Bengio
44"
INTRODUCTION,0.04331087584215592,"et al., 1991; Hochreiter et al., 2001) and few-shot learning (Miller et al., 2000; Bendre et al., 2020;
45"
INTRODUCTION,0.04427333974975938,"Wang et al., 2020).
46"
INTRODUCTION,0.04523580365736285,"Few-shot learning tackles the low-data problem that is ubiquitous in drug discovery. Few-shot
47"
INTRODUCTION,0.04619826756496631,"learning methods have been predominantly developed and tested on image datasets (Bendre et al.,
48"
INTRODUCTION,0.04716073147256978,"2020; Wang et al., 2020), and have recently been adapted to drug discovery problems (Chen et al.,
49"
INTRODUCTION,0.04812319538017324,"2022; Wang et al., 2021; Stanley et al., 2021; Altae-Tran et al., 2017). They are usually categorized
50"
INTRODUCTION,0.04908565928777671,"into three groups according to their main approach (Bendre et al., 2020; Wang et al., 2020; Adler et al.,
51"
INTRODUCTION,0.05004812319538017,"2020). a) Data-augmentation-based approaches augment the available samples and generate new, more
52"
INTRODUCTION,0.05101058710298364,"diverse data points (Chen et al., 2020; Zhao et al., 2019; Antoniou and Storkey, 2019). b) Embedding-
53"
INTRODUCTION,0.0519730510105871,"based and nearest neighbour approaches learn embedding space representations. Predictive models
54"
INTRODUCTION,0.05293551491819057,"can then be constructed from only few net data points by comparing these embeddings. For example
55"
INTRODUCTION,0.05389797882579403,"in Matching Networks (Vinyals et al., 2016) an attention mechanism that relies on embeddings is the
56"
INTRODUCTION,0.0548604427333975,"basis for the predictions. Prototypical Networks (Snell et al., 2017) create prototype representations
57"
INTRODUCTION,0.05582290664100096,"for each class using the above mentioned representations in the embedding space. c) Optimization-
58"
INTRODUCTION,0.05678537054860443,"based or ﬁne-tuning methods utilize a meta-optimizer that focuses on efﬁciently navigating the
59"
INTRODUCTION,0.05774783445620789,"parameter space. For example, with MAML the meta-optimizer learns initial weights that can be
60"
INTRODUCTION,0.05871029836381136,"adapted to a novel task by few optimization steps (Finn et al., 2017).
61"
INTRODUCTION,0.059672762271414825,"Most of these approaches have already been applied to few-shot drug discovery (see Sec. 4). Surpris-
62"
INTRODUCTION,0.06063522617901829,"ingly, almost all these few-shot learning methods in drug discovery are worse than a naive baseline,
63"
INTRODUCTION,0.061597690086621755,"which does not even use the support set (see Section 5). We hypothesize that the under-performance
64"
INTRODUCTION,0.06256015399422522,"of these methods stems from disregarding the context — both in terms of similar molecules and
65"
INTRODUCTION,0.06352261790182868,"similar activities. Therefore, we propose a method that informs the representations of the query and
66"
INTRODUCTION,0.06448508180943215,"support set with a large number of context molecules covering the chemical space.
67"
INTRODUCTION,0.06544754571703561,"Enriching molecule representations with context using associative memories. In data-scarce
68"
INTRODUCTION,0.06641000962463908,"situations, humans extract co-occurrences and covariances by associating current perceptions with
69"
INTRODUCTION,0.06737247353224254,"memories (Bonner and Epstein, 2021; Potter, 2012). When we show a small set of active molecules to
70"
INTRODUCTION,0.068334937439846,"a human expert in drug discovery, the expert associates them with known molecules to suggest further
71"
INTRODUCTION,0.06929740134744947,"active molecules (Gomez, 2018; He et al., 2021). In an analogous manner, our novel concept for
72"
INTRODUCTION,0.07025986525505294,"few-shot learning uses associative memories to extract co-occurrences and the covariance structure
73"
INTRODUCTION,0.0712223291626564,"of the original data and to amplify them in the representations (Fürst et al., 2021). We use Modern
74"
INTRODUCTION,0.07218479307025986,"Hopﬁeld Networks (MHNs) as an associative memory, since they can store a large set of context
75"
INTRODUCTION,0.07314725697786333,"molecule representations (Ramsauer et al., 2021, Theorem 3). The representations that are retrieved
76"
INTRODUCTION,0.0741097208854668,"from the MHNs replace the original representations of the query and support set molecules. Those
77"
INTRODUCTION,0.07507218479307026,"retrieved representations have ampliﬁed co-occurrences and covariance structures, while peculiarities
78"
INTRODUCTION,0.07603464870067372,"and spurious co-occurrences of the query and support set molecules are averaged out.
79"
INTRODUCTION,0.07699711260827719,"In this work, our contributions are the following:
80"
INTRODUCTION,0.07795957651588066,"• We propose a new architecture MHNfs for few-shot learning in drug discovery.
81"
INTRODUCTION,0.07892204042348412,"• We achieve a new state-of-the-art on the benchmarking dataset FS-Mol.
82"
INTRODUCTION,0.07988450433108758,"• We introduce a novel concept to enrich the molecule representations with context by associ-
83"
INTRODUCTION,0.08084696823869104,"ating them with a large set of context molecules.
84"
INTRODUCTION,0.08180943214629452,"• We add a naive baseline to the FS-Mol benchmark that yields better results than almost all
85"
INTRODUCTION,0.08277189605389798,"other published few-shot learning methods.
86"
INTRODUCTION,0.08373435996150144,"• We provide results of an ablation study and a domain shift experiment to further demonstrate
87"
INTRODUCTION,0.0846968238691049,"the effectiveness of our new method.
88"
PROBLEM SETTING,0.08565928777670838,"2
Problem setting
89"
PROBLEM SETTING,0.08662175168431184,"Drug discovery projects revolve around models g(m) that can predict a molecular property or activity
90"
PROBLEM SETTING,0.0875842155919153,"ˆy, given a representation m of an input molecule from a chemical space M. We consider machine
91"
PROBLEM SETTING,0.08854667949951876,"learning models ˆy = gw(m) with parameters w that have been selected using a training set. Typically,
92"
PROBLEM SETTING,0.08950914340712224,"deep learning based property prediction uses a molecule encoder f ME : M →Rd. The molecule
93"
PROBLEM SETTING,0.0904716073147257,"encoder can process different symbolic or low-level representations of molecules, such as molecular
94"
PROBLEM SETTING,0.09143407122232916,"descriptors (Bender et al., 2004; Unterthiner et al., 2014; Mayr et al., 2016), SMILES (Weininger,
95"
PROBLEM SETTING,0.09239653512993262,"1988; Mayr et al., 2018; Winter et al., 2019; Segler et al., 2018a), or molecular graphs (Merkwirth
96"
PROBLEM SETTING,0.0933589990375361,"and Lengauer, 2005; Kearnes et al., 2016; Yang et al., 2019; Jiang et al., 2021) and can be pre-trained
97"
PROBLEM SETTING,0.09432146294513956,"on related property prediction tasks.
98"
PROBLEM SETTING,0.09528392685274302,"For few-shot learning, the goal is to select a high-quality predictive model based on a small set of
99"
PROBLEM SETTING,0.09624639076034648,"molecules {x1, . . . , xN} with associated measurements y = {y1, . . . , yN}. The measurements are
100"
PROBLEM SETTING,0.09720885466794996,"usually assumed to be binary yn ∈{−1, 1}, corresponding to the molecule being inactive or active.
101"
PROBLEM SETTING,0.09817131857555342,"The set {(xn, yn)}N
n=1 is called the support set that contains samples from a prediction task and N is
102"
PROBLEM SETTING,0.09913378248315688,"the support set size. The goal is to construct a model that correctly predicts y for an x that is not in
103"
PROBLEM SETTING,0.10009624639076034,"the support set — in other words, a model that generalizes well.
104"
PROBLEM SETTING,0.10105871029836382,"Standard supervised machine learning approaches typically just show limited predictive power at
105"
PROBLEM SETTING,0.10202117420596728,"this task (Stanley et al., 2021) since they tend to overﬁt on the support set due to a small number of
106"
PROBLEM SETTING,0.10298363811357074,"training samples. These approaches learn the parameters w of the model gw from the support set in
107"
PROBLEM SETTING,0.1039461020211742,"a supervised manner. However, they heavily overﬁt to the support set when N is small. Therefore,
108"
PROBLEM SETTING,0.10490856592877768,"few-shot learning methods are necessary to construct models from the support set that generalize
109"
PROBLEM SETTING,0.10587102983638114,"well to new data.
110"
PROBLEM SETTING,0.1068334937439846,"3
MHNfs: Hopﬁeld-based molecular context enrichment for few-shot drug
111"
PROBLEM SETTING,0.10779595765158806,"discovery
112"
PROBLEM SETTING,0.10875842155919153,"We aim at increasing the generalization capabilities of few-shot learning methods in drug discovery
113"
PROBLEM SETTING,0.109720885466795,"by enriching the molecule representations with molecular context. In comparison to the support set,
114"
PROBLEM SETTING,0.11068334937439846,"which encodes information about the task, the context set – i.e. a large set of molecules – includes
115"
PROBLEM SETTING,0.11164581328200192,"information about a large chemical space. The query and the support set molecules perform a retrieval
116"
PROBLEM SETTING,0.1126082771896054,"from the context set and thereby enrich their representations. We detail this in the following.
117"
MODEL ARCHITECTURE,0.11357074109720885,"3.1
Model architecture
118"
MODEL ARCHITECTURE,0.11453320500481232,"We propose an architecture which consists of three consecutive modules. The ﬁrst module, a) the
119"
MODEL ARCHITECTURE,0.11549566891241578,"context module f CM, enriches molecule representations by retrieving from a large set of molecules.
120"
MODEL ARCHITECTURE,0.11645813282001925,"The second module, b) the cross-attention module f CAM (Hou et al., 2019; Chen et al., 2021), enables
121"
MODEL ARCHITECTURE,0.11742059672762271,"the effective exchange of information between the query molecule and the support set molecules.
122"
MODEL ARCHITECTURE,0.11838306063522618,"Finally the prediction for the query molecule is computed by using the usual c) similarity module
123"
MODEL ARCHITECTURE,0.11934552454282965,"f SM (Koch et al., 2015; Altae-Tran et al., 2017):
124"
MODEL ARCHITECTURE,0.12030798845043311,"context module:
m′ = f CM(m, C)"
MODEL ARCHITECTURE,0.12127045235803657,"X′ = f CM(X, C),
(1)"
MODEL ARCHITECTURE,0.12223291626564003,"cross-attention module:
[m′′, X′′] = f CAM([m′, X′]),
(2)"
MODEL ARCHITECTURE,0.12319538017324351,"similarity module:
ˆy = f SM(m′′, X′′, y),
(3)"
MODEL ARCHITECTURE,0.12415784408084697,"where m ∈Rd is a molecule embedding from a trainable or ﬁxed molecule encoder, and m′ and m′′
125"
MODEL ARCHITECTURE,0.12512030798845045,"are enriched versions of it. Similarly, X ∈Rd×N contains the stacked embeddings of the support set
126"
MODEL ARCHITECTURE,0.1260827718960539,"molecules and X′ and X′′ are their enriched versions. C ∈Rd×M is a large set of stacked molecule
127"
MODEL ARCHITECTURE,0.12704523580365737,"embeddings, y are the support set labels, and ˆy is the prediction for the query molecule. Square
128"
MODEL ARCHITECTURE,0.12800769971126083,"brackets indicate concatenation, for example [m′, X′] is a matrix with N + 1 columns. The modules
129"
MODEL ARCHITECTURE,0.1289701636188643,"f CM, f CAM, and f SM are detailed in the paragraphs below. An overview of our architecture is given
130"
MODEL ARCHITECTURE,0.12993262752646775,"in Figure 1. The architecture also includes skip connections bypassing f CM(., .) and f CAM(.) and
131"
MODEL ARCHITECTURE,0.13089509143407121,"layer normalization (Ba et al., 2016), which are not shown in Figure1.
132"
MODEL ARCHITECTURE,0.13185755534167468,"Figure 1: Schematic overview of our architecture. Left: All molecules are fed through a shared
molecule encoder to obtain embeddings. Then, the context module (CM) enriches the representations
by associating them with context molecules. The cross-attention module (CAM) enriches representa-
tions by mutually associating the query and support set molecules. Finally, the similarity module
computes the prediction for the query molecule. Right: Detailed depiction of the operations in the
CM and the CAM."
MODEL ARCHITECTURE,0.13282001924927817,"A shared molecule encoder f ME creates embeddings for the query molecule m = f ME(m), the
133"
MODEL ARCHITECTURE,0.13378248315688163,"support set molecules xn = f ME(xn), and the context molecules cm = f ME(cm). There are many
134"
MODEL ARCHITECTURE,0.1347449470644851,"possible choices for ﬁxed or adaptive molecule encoders (see Section 2), of which we use descriptor-
135"
MODEL ARCHITECTURE,0.13570741097208855,"based fully-connected networks because of their computational efﬁciency and good accuracy (Dahl
136"
MODEL ARCHITECTURE,0.136669874879692,"et al., 2014; Mayr et al., 2016, 2018). For notational clarity we denote the course of the representations
137"
MODEL ARCHITECTURE,0.13763233878729547,"through the architecture:
138"
MODEL ARCHITECTURE,0.13859480269489893,"m
symbolic or
low-level repr."
MODEL ARCHITECTURE,0.1395572666025024,"f ME
−→
m
molecule
embedding"
MODEL ARCHITECTURE,0.14051973051010588,"f CM
−→m′"
MODEL ARCHITECTURE,0.14148219441770934,"context
repr."
MODEL ARCHITECTURE,0.1424446583253128,"f CAM
−→
m′′"
MODEL ARCHITECTURE,0.14340712223291627,"similarity
repr.
,
(4)"
MODEL ARCHITECTURE,0.14436958614051973,"xn
symbolic or
low-level repr."
MODEL ARCHITECTURE,0.1453320500481232,"f ME
−→
xn
molecule
embedding"
MODEL ARCHITECTURE,0.14629451395572665,"f CM
−→x′
n
context
repr."
MODEL ARCHITECTURE,0.1472569778633301,"f CAM
−→
x′′
n
similarity
repr. .
(5)"
MODEL ARCHITECTURE,0.1482194417709336,"3.2
Context module (CM)
139"
MODEL ARCHITECTURE,0.14918190567853706,"The context module associates the query and support set molecules with a large set of context
140"
MODEL ARCHITECTURE,0.15014436958614052,"molecules, and represents them as weighted average of context molecule embeddings. The context
141"
MODEL ARCHITECTURE,0.15110683349374399,"module is realised by a continuous Modern Hopﬁeld Network (MHN) (Ramsauer et al., 2021). An
142"
MODEL ARCHITECTURE,0.15206929740134745,"MHN is a content-addressable associative memory which can be built into deep learning architectures.
143"
MODEL ARCHITECTURE,0.1530317613089509,"There exists an analogy between the energy update of MHNs and the attention mechanism of
144"
MODEL ARCHITECTURE,0.15399422521655437,"Transformers (Vaswani et al., 2017; Ramsauer et al., 2021). MHNs are capable of storing and
145"
MODEL ARCHITECTURE,0.15495668912415783,"retrieving patterns from a memory M ∈Re×M given a state pattern ξ ∈Re that represents the query.
146"
MODEL ARCHITECTURE,0.15591915303176132,"The retrieved pattern ξnew ∈Re is obtained by
147"
MODEL ARCHITECTURE,0.15688161693936478,"ξnew = M p = M softmax
 
βM T ξ

,
(6)
where p is called the vector of associations and β is a scaling factor or inverse temperature. Modern
148"
MODEL ARCHITECTURE,0.15784408084696824,"Hopﬁeld Networks have been successfully applied to chemistry and computational immunology
149"
MODEL ARCHITECTURE,0.1588065447545717,"(Seidl et al., 2022; Widrich et al., 2020).
150"
MODEL ARCHITECTURE,0.15976900866217517,"We use this mechanism in the form of a Hopﬁeld layer, which ﬁrst maps raw patterns to an associative
151"
MODEL ARCHITECTURE,0.16073147256977863,"space using linear transformations, and uses multiple simultaneous queries Ξ ∈Rd×N:
152"
MODEL ARCHITECTURE,0.1616939364773821,"Hopﬁeld(Ξ, C) := (WEC) softmax
 
β (WCC)T (WΞΞ)

,
(7)"
MODEL ARCHITECTURE,0.16265640038498555,"where WE ∈Rd×d and WC, WΞ ∈Re×d are trainable parameters of the Hopﬁeld layer, softmax
153"
MODEL ARCHITECTURE,0.16361886429258904,"is applied column-wise, and β is a hyperparameter. Note that in principle the Ξ and C could have a
154"
MODEL ARCHITECTURE,0.1645813282001925,"different second dimension as long as the linear transformations map to the same dimension e. Note
155"
MODEL ARCHITECTURE,0.16554379210779596,"that all embeddings that enter this module are ﬁrst layer normalized (Ba et al., 2016). Several of
156"
MODEL ARCHITECTURE,0.16650625601539942,"these Hopﬁeld layers can run in parallel and we refer to them as ""heads"" in analogy to Transformers
157"
MODEL ARCHITECTURE,0.16746871992300288,"(Vaswani et al., 2017).
158"
MODEL ARCHITECTURE,0.16843118383060635,"The context module of our new architecture uses a Hopﬁeld layer, where the query patterns are the
159"
MODEL ARCHITECTURE,0.1693936477382098,"embeddings of the query molecule m and the support set molecules X. The memory is composed of
160"
MODEL ARCHITECTURE,0.17035611164581327,"embeddings of a large set of M molecules from a chemical space, for example reference molecules,
161"
MODEL ARCHITECTURE,0.17131857555341676,"here called context molecules C. Then the original embeddings m and X are replaced by the
162"
MODEL ARCHITECTURE,0.17228103946102022,"retrieved embeddings, which are weighted averages of context molecule embeddings:
163"
MODEL ARCHITECTURE,0.17324350336862368,"m′ = Hopﬁeld(m, C)
and
X′ = Hopﬁeld(X, C).
(8)"
MODEL ARCHITECTURE,0.17420596727622714,"This retrieval step reinforces the covariance structure of the retrieved representations (see Ap-
164"
MODEL ARCHITECTURE,0.1751684311838306,"pendix A.7). Note that the embeddings of the query and the support set molecules have not yet
165"
MODEL ARCHITECTURE,0.17613089509143406,"inﬂuenced each other. These updated representations m′, X′ are passed to the cross-attention module.
166"
MODEL ARCHITECTURE,0.17709335899903753,"3.3
Cross-attention module (CAM)
167"
MODEL ARCHITECTURE,0.17805582290664101,"For embedding-based few-shot learning methods in the ﬁeld of drug discovery, Altae-Tran et al. (2017)
168"
MODEL ARCHITECTURE,0.17901828681424448,"showed that the representations of the molecules can be enriched, if the architecture allows information
169"
MODEL ARCHITECTURE,0.17998075072184794,"exchange between query and support set molecules. Altae-Tran et al. (2017) uses an attention-
170"
MODEL ARCHITECTURE,0.1809432146294514,"enhanced LSTM variant which updates the query and the support set molecule representations in an
171"
MODEL ARCHITECTURE,0.18190567853705486,"iterative fashion, being aware of each other. We further develop this idea and combine it with the idea
172"
MODEL ARCHITECTURE,0.18286814244465832,"of using a transformer encoder layer (Vaswani et al., 2017) as a cross-attention module (Hou et al.,
173"
MODEL ARCHITECTURE,0.18383060635226178,"2019; Chen et al., 2021).
174"
MODEL ARCHITECTURE,0.18479307025986524,"The cross-attention module updates the query molecule representation m′ and the support set
175"
MODEL ARCHITECTURE,0.18575553416746873,"molecule representations X′ by mutually exchanging information, using the usual Transformer
176"
MODEL ARCHITECTURE,0.1867179980750722,"mechanism:
177"
MODEL ARCHITECTURE,0.18768046198267566,"[m′′, X′′] = Hopﬁeld([m′, X′], [m′, X′]),
(9)"
MODEL ARCHITECTURE,0.18864292589027912,"where [m′, X′] ∈Rd×(N+1) is the concatenation of the representations of the query molecule m′
178"
MODEL ARCHITECTURE,0.18960538979788258,"with the support set molecules X′ and we exploited that the Transformer is a special case of the
179"
MODEL ARCHITECTURE,0.19056785370548604,"Hopﬁeld layer. Again, normalization is applied (Ba et al., 2016) and multiple Hopﬁeld layers, i.e.,
180"
MODEL ARCHITECTURE,0.1915303176130895,"heads, can run in parallel, be stacked, and equipped with skip-connections. The representations m′′
181"
MODEL ARCHITECTURE,0.19249278152069296,"and X′′ are passed to the similarity module.
182"
MODEL ARCHITECTURE,0.19345524542829645,"3.4
Similarity module (SM)
183"
MODEL ARCHITECTURE,0.1944177093358999,"In this module, pairwise similarity values k(m′′, x′′
n) are computed between the representation of
184"
MODEL ARCHITECTURE,0.19538017324350337,"a query molecule m′′ and each molecule x′′
n in the support set as done recently (Koch et al., 2015;
185"
MODEL ARCHITECTURE,0.19634263715110684,"Altae-Tran et al., 2017). Based on these similarity values, the activity for the query molecule is
186"
MODEL ARCHITECTURE,0.1973051010587103,"predicted, building a weighted mean over the support set labels:
187"
MODEL ARCHITECTURE,0.19826756496631376,ˆy = σ 
MODEL ARCHITECTURE,0.19923002887391722,"τ −1 1 N N
X"
MODEL ARCHITECTURE,0.20019249278152068,"n=1
y′
n k(m′′, x′′
n) !"
MODEL ARCHITECTURE,0.20115495668912417,",
(10)"
MODEL ARCHITECTURE,0.20211742059672763,"where our architecture employs dot product similarity of normalized representations k(m′′, x′′
n) =
188"
MODEL ARCHITECTURE,0.2030798845043311,"m′′T x′′
n. σ(.) is the sigmoid function and τ is a hyperparameter. Note that we use a balancing
189"
MODEL ARCHITECTURE,0.20404234841193455,"strategy for the labels y′
n =
N/(2√NA)
if yn = 1
−N/(2√NI)
else
, where NA is the number of actives and NI
190"
MODEL ARCHITECTURE,0.20500481231953802,"is the number of inactives of the support set.
191"
MODEL ARCHITECTURE,0.20596727622714148,"3.5
Architecture, hyperparameter selection, and training details
192"
MODEL ARCHITECTURE,0.20692974013474494,"Hyperparameters. The main hyperparameters of our architecture are the number of heads, the
193"
MODEL ARCHITECTURE,0.2078922040423484,"embedding dimension, the dimension of the association space of the CAM and CM, the learning
194"
MODEL ARCHITECTURE,0.2088546679499519,"rate schedule, the scaling parameter β, and the molecule encoder. The following hyperparameters
195"
MODEL ARCHITECTURE,0.20981713185755535,"were selected by manual hyperparameter selection on the validation tasks. The molecule encoder
196"
MODEL ARCHITECTURE,0.2107795957651588,"consists of a single layer with output size d = 1024 and SELU activation (Klambauer et al., 2017).
197"
MODEL ARCHITECTURE,0.21174205967276227,"The CM consists of one Hopﬁeld layer with 8 heads. The dimension e of the association space is set
198"
MODEL ARCHITECTURE,0.21270452358036573,"to 512 and β = 1/√e. Since we use skip connections between all modules the output dimension of
199"
MODEL ARCHITECTURE,0.2136669874879692,"the CM and CAM matches the input dimension. The CAM comprises one layer with 8 heads and an
200"
MODEL ARCHITECTURE,0.21462945139557266,"association-space dimension of 1088. For the input to the CAM, an activity encoding was added to
201"
MODEL ARCHITECTURE,0.21559191530317612,"the support set molecule representations to provide label information. The SM uses τ = 22.6. For the
202"
MODEL ARCHITECTURE,0.2165543792107796,"context set, we randomly sample 5% from a large set of molecules – i.e., the molecules in the FS-Mol
203"
MODEL ARCHITECTURE,0.21751684311838307,"training split – for each batch. For inference, we used a ﬁxed set of 5% of training set molecules as
204"
MODEL ARCHITECTURE,0.21847930702598653,"the context set for each seed. We hypothesize that these choices about the context could be further
205"
MODEL ARCHITECTURE,0.21944177093359,"improved (Section 6). We provide considered and selected hyperparameters in Appendix A.1.6.
206"
MODEL ARCHITECTURE,0.22040423484119345,"Loss function, regularization and optimization. We use the Adam optimizer (Kingma and Ba,
207"
MODEL ARCHITECTURE,0.22136669874879691,"2014) to minimize the cross-entropy loss between the predicted and known activity labels. We use
208"
MODEL ARCHITECTURE,0.22232916265640038,"a learning rate scheduler which includes a warm up phase, followed by a section with a constant
209"
MODEL ARCHITECTURE,0.22329162656400384,"learning rate, which is 0.0001, and a third phase in which the learning rate steadily decreases. As a
210"
MODEL ARCHITECTURE,0.22425409047160733,"regularization strategy, for the CM and the CAM a dropout rate of 0.5 is used. The molecule encoder
211"
MODEL ARCHITECTURE,0.2252165543792108,"has a dropout with rate 0.1 for the input and 0.5 elsewhere (see also Appendix A.1.6).
212"
MODEL ARCHITECTURE,0.22617901828681425,"Compute time and resources. Training a single MHNfs model on the benchmarking dataset FS-
213"
MODEL ARCHITECTURE,0.2271414821944177,"Mol takes roughly 90 hours of wall-clock time on an A100 GPU. In total, roughly 15,000 GPU hours
214"
MODEL ARCHITECTURE,0.22810394610202117,"were consumed for this work.
215"
RELATED WORK,0.22906641000962463,"4
Related work
216"
RELATED WORK,0.2300288739172281,"Several approaches to few-shot learning in drug discovery have been suggested (Altae-Tran et al.,
217"
RELATED WORK,0.23099133782483156,"2017; Nguyen et al., 2020; Guo et al., 2021; Wang et al., 2021). (Nguyen et al., 2020) evaluated
218"
RELATED WORK,0.23195380173243504,"the applicability of MAML and its variants to graph neural networks (GNNs) and (Guo et al., 2021)
219"
RELATED WORK,0.2329162656400385,"also combine GNNs and meta-learning. (Altae-Tran et al., 2017) suggested an approach called
220"
RELATED WORK,0.23387872954764197,"Iterative Reﬁnement Long Short-Term Memory, in which query and support set embeddings can
221"
RELATED WORK,0.23484119345524543,"share information and update their embeddings. Property-aware relation networks (PAR) (Wang
222"
RELATED WORK,0.2358036573628489,"et al., 2021) use an attention mechanism to enrich representations from cluster centers and then learn
223"
RELATED WORK,0.23676612127045235,"a relation graph between molecules. (Chen et al., 2022) propose to adaptively learn kernels and apply
224"
RELATED WORK,0.2377285851780558,"their method to few-shot drug discovery with predictive performance for larger support set sizes.
225"
RELATED WORK,0.2386910490856593,"Recently, (Stanley et al., 2021) generated a benchmark dataset for few-shot learning methods in drug
226"
RELATED WORK,0.23965351299326276,"discovery and provided some baseline results.
227"
RELATED WORK,0.24061597690086622,"Many successful deep neural network architectures use external memories, such as the neural Turing
228"
RELATED WORK,0.24157844080846969,"machine (Graves et al., 2014), memory networks (Weston et al., 2014), end-to-end memory networks
229"
RELATED WORK,0.24254090471607315,"(Sukhbaatar et al., 2015). Recently, the connection between continuous modern Hopﬁeld networks
230"
RELATED WORK,0.2435033686236766,"(Ramsauer et al., 2021), which are content-addressable associative memories, and Transformer
231"
RELATED WORK,0.24446583253128007,"architectures (Vaswani et al., 2017) has been established. We refer to (Le, 2021) for an extensive
232"
RELATED WORK,0.24542829643888353,"overview of memory-based architectures. Architectures with external memories have also been used
233"
RELATED WORK,0.24639076034648702,"for meta-learning (Vinyals et al., 2016; Santoro et al., 2016) and few-shot learning (Munkhdalai and
234"
RELATED WORK,0.24735322425409048,"Yu, 2017; Ramalho and Garnelo, 2018; Ma et al., 2021).
235"
EXPERIMENTS,0.24831568816169394,"5
Experiments
236"
BENCHMARKING ON FS-MOL,0.2492781520692974,"5.1
Benchmarking on FS-Mol
237"
BENCHMARKING ON FS-MOL,0.2502406159769009,"Experimental setup. Recently, the dataset FS-Mol (Stanley et al., 2021) was proposed to benchmark
238"
BENCHMARKING ON FS-MOL,0.25120307988450435,"few-shot learning methods in drug discovery. It was extracted from ChEMBL27 and comprises in
239"
BENCHMARKING ON FS-MOL,0.2521655437921078,"total 489,133 measurements, 233,786 compounds and 5,120 tasks. Per task, the mean number of
240"
BENCHMARKING ON FS-MOL,0.2531280076997113,"data points is 94. The dataset is well balanced as the mean ratio of active and inactive molecules is
241"
BENCHMARKING ON FS-MOL,0.25409047160731474,"close to 1. The FS-Mol benchmark dataset deﬁnes 4,938 training, 40 validation and 157 test tasks,
242"
BENCHMARKING ON FS-MOL,0.2550529355149182,"guaranteeing disjoint task sets. (Stanley et al., 2021) precomputed extended connectivity ﬁngerprints
243"
BENCHMARKING ON FS-MOL,0.25601539942252166,"(ECFP) (Rogers and Hahn, 2010) and key molecular physical descriptors, which were deﬁned by
244"
BENCHMARKING ON FS-MOL,0.2569778633301251,"RDKit (Landrum et al., 2006). While methods would be allowed to use other representations of
245"
BENCHMARKING ON FS-MOL,0.2579403272377286,"the input molecules, such as the molecular graph, we used a concatenation of these ECFPs and
246"
BENCHMARKING ON FS-MOL,0.25890279114533205,"Table 1: Results on FS-MOL [∆AUC-PR]. The best method is marked bold. Error bars represent
standard errors across tasks according to Stanley et al. (2021). The metrics are also averaged across ﬁve
training re-runs and ten draws of support sets. In brackets the number of tasks per category is reported."
BENCHMARKING ON FS-MOL,0.2598652550529355,"Method
All [157]
Kin. [125]
Hydrol. [20]
Oxid.[7]"
BENCHMARKING ON FS-MOL,0.26082771896053897,"GNN-STa (Stanley et al., 2021)
.029 ± .004
.027 ± .004
.040 ± .018
.020 ± .016
MATa (Maziarka et al., 2020)
.052 ± .005
.043 ± .005
.095 ± .019
.062 ± .024
Random Foresta (Breiman, 2001)
.092 ± .007
.081 ± .009
.158 ± .028
.080 ± .029
GNN-MTa (Stanley et al., 2021)
.093 ± .006
.093 ± .006
.108 ± .025
.053 ± .018
Similarity Search
.118 ± .008
.109 ± .008
.166 ± .029
.097 ± .033
GNN-MAMLa (Guo et al., 2021)
.159 ± .009
.177 ± .009
.105 ± .024
.054 ± .028
PAR(Wang et al., 2021)
.164 ± .008
.182 ± .009
.109 ± .020
.039 ± .008
Frequent hitters
.182 ± .010
.207 ± .009
.098 ± .009
.041 ± .005
ProtoNeta (Snell et al., 2017)
.207 ± .008
.215 ± .009
.209 ± .030
.095 ± .029
Siamese Networks (Koch et al., 2015)
.223 ± .010
.241 ± .010
.178 ± .026
.082 ± .025
IterRefLSTM (Altae-Tran et al., 2017)
.234 ± .010
.251 ± .010
.199 ± .026
.098 ± .027
ADKF-IFTb(Chen et al., 2022)
.234 ± .009
.248 ± .020
.217 ± .017
.106 ± .008
MHNfs (ours)
.241 ± .009
.259 ± .010
.199 ± .027
.096 ± .019"
BENCHMARKING ON FS-MOL,0.26179018286814243,"a metrics from Stanley et al. (2021).
b results from Chen et al. (2022)."
BENCHMARKING ON FS-MOL,0.2627526467757459,"RDKit-based descriptors. For the main benchmark, the support set size was ﬁxed to 16, using a
247"
BENCHMARKING ON FS-MOL,0.26371511068334935,"stratiﬁed random split. We use all these settings of FS-Mol and therefore ensure a fair method
248"
BENCHMARKING ON FS-MOL,0.2646775745909528,"comparison.
249"
BENCHMARKING ON FS-MOL,0.26564003849855633,"Methods compared. Baselines for few-shot learning and our proposed method MHNfs were com-
250"
BENCHMARKING ON FS-MOL,0.2666025024061598,"pared against each other. The Frequent Hitters model is a naive baseline that ignores the provided
251"
BENCHMARKING ON FS-MOL,0.26756496631376325,"support set and therefore has to learn to predict the average activity of a molecule. This method can
252"
BENCHMARKING ON FS-MOL,0.2685274302213667,"potentially discriminate so-called frequent-hitter molecules (Stork et al., 2019) against molecules
253"
BENCHMARKING ON FS-MOL,0.2694898941289702,"that are inactive across many tasks. We also added Similarity Search (Cereto-Massagué et al., 2015)
254"
BENCHMARKING ON FS-MOL,0.27045235803657364,"as a baseline. Similarity search is a standard chemoinformatics technique, used in situations with
255"
BENCHMARKING ON FS-MOL,0.2714148219441771,"single or few known actives. In the simplest case, the search ﬁnds similar molecules by computing
256"
BENCHMARKING ON FS-MOL,0.27237728585178056,"a ﬁngerprint or descriptor-representation of the molecules and using a similarity measure k(., .) —
257"
BENCHMARKING ON FS-MOL,0.273339749759384,"such as Tanimoto Similarity (Tanimoto, 1960). Thus, Similarity Search, as used in chemoinfor-
258"
BENCHMARKING ON FS-MOL,0.2743022136669875,"matics, can be formally written as ˆy = 1/N PN
n=1 yn k(m, xn); where x1, . . . , xn come from a
259"
BENCHMARKING ON FS-MOL,0.27526467757459094,"ﬁxed molecule encoder, such as chemical ﬁngerprint or descriptor calculation. A natural exten-
260"
BENCHMARKING ON FS-MOL,0.2762271414821944,"sion of Similarity Search with ﬁxed chemical descriptors is Neural Similarity Search or Siamese
261"
BENCHMARKING ON FS-MOL,0.27718960538979787,"networks (Koch et al., 2015), which extend the classic similarity search by learning a molecule
262"
BENCHMARKING ON FS-MOL,0.2781520692974013,"encoder: ˆy = σ

τ −1 1"
BENCHMARKING ON FS-MOL,0.2791145332050048,"N
PN
n=1 y′
n f ME
w (m)T f ME
w (xn)

. Furthermore, we re-implemented the
263"
BENCHMARKING ON FS-MOL,0.28007699711260825,"IterRefLSTM (Altae-Tran et al., 2017) in Pytorch. The IterRefLSTM model consists of three
264"
BENCHMARKING ON FS-MOL,0.28103946102021177,"modules. First, a molecule encoder maps the query and support set molecules to its representations
265"
BENCHMARKING ON FS-MOL,0.28200192492781523,"m and X. Second, an attention-enhanced LSTM variant, the actual IterRefLSTM, iteratively
266"
BENCHMARKING ON FS-MOL,0.2829643888354187,"updates the query and support set molecules, enabling information sharing between the molecules:
267"
BENCHMARKING ON FS-MOL,0.28392685274302215,"[m′, X′] = IterRefLSTML([m, X]), where the hyperparameter L controls the number of iteration
268"
BENCHMARKING ON FS-MOL,0.2848893166506256,"steps of the IterRefLSTM. Third, a similarity module computes attention weights based on the rep-
269"
BENCHMARKING ON FS-MOL,0.2858517805582291,"resentations: a = softmax (k (m′, X′)). These representations are then used for the ﬁnal prediction:
270"
BENCHMARKING ON FS-MOL,0.28681424446583254,"ˆy = PN
i=1 aiyi. For further details, see Appendix A.1.5. The Random Forest baseline uses the
271"
BENCHMARKING ON FS-MOL,0.287776708373436,"chemical descriptors and is trained in standard supervised manner on the support set molecules for
272"
BENCHMARKING ON FS-MOL,0.28873917228103946,"each task. The method GNN-ST is a graph neural network (Stanley et al., 2021; Gilmer et al., 2017)
273"
BENCHMARKING ON FS-MOL,0.2897016361886429,"that is trained from scratch for each task. The GNN-MT uses a two step strategy: First, the model is
274"
BENCHMARKING ON FS-MOL,0.2906641000962464,"pretrained on a large dataset on related tasks; second, an output layer is constructed to the few-shot
275"
BENCHMARKING ON FS-MOL,0.29162656400384984,"task via linear probing (Stanley et al., 2021; Alain and Bengio, 2016). The Molecule Attention
276"
BENCHMARKING ON FS-MOL,0.2925890279114533,"Transformer (MAT) is pre-trained in a self-supervised fashion and ﬁne-tuning is performed for the
277"
BENCHMARKING ON FS-MOL,0.29355149181905676,"few-shot task (Maziarka et al., 2020). GNN-MAML is based on MAML (Finn et al., 2017), and uses
278"
BENCHMARKING ON FS-MOL,0.2945139557266602,"a model-agnostic meta-learning strategy to ﬁnd a general core model from which one can easily adapt
279"
BENCHMARKING ON FS-MOL,0.29547641963426374,"to single tasks. ProtoNet (Snell et al., 2017) includes a molecule encoder, which maps query and
280"
BENCHMARKING ON FS-MOL,0.2964388835418672,"support set molecules to representations in an embedding space. In this embedding space, prototypical
281"
BENCHMARKING ON FS-MOL,0.29740134744947067,"MHNfs
 (CM+CAM+SM)"
BENCHMARKING ON FS-MOL,0.2983638113570741,"MHNfs -CM
MHNfs -CM
 
(CAM,IterRefLSTM) 0.225 0.230 0.235 0.240 0.245 0.250"
BENCHMARKING ON FS-MOL,0.2993262752646776,dAUPRC
BENCHMARKING ON FS-MOL,0.30028873917228105,"MHNfs
 (CM+CAM+SM)"
BENCHMARKING ON FS-MOL,0.3012512030798845,"MHNfs -CM
MHNfs -CM
 
(CAM,IterRefLSTM) 0.720 0.725 0.730 0.735 0.740 0.745 AUC"
BENCHMARKING ON FS-MOL,0.30221366698748797,"Figure 2: Results of the ablation study. The boxes show the median, mean and the variability of the
average predictive performance of the methods across training re-runs and draws of support sets.
The performance signiﬁcantly drops when the context module is removed (light red bars), and when
additionally the cross-attention module is replaced with the IterRefLSTM module (light blue bars).
This indicates that our two newly introduced modules, CM and CAM, play a crucial role in MHNfs."
BENCHMARKING ON FS-MOL,0.30317613089509143,"representations of each class are built by taking the mean across all related support set molecules for
282"
BENCHMARKING ON FS-MOL,0.3041385948026949,"each class (details in Appendix A.1.4). For all methods the most important hyperparameters were
283"
BENCHMARKING ON FS-MOL,0.30510105871029836,"adjusted on the validation tasks of FS-Mol. The PAR model (Wang et al., 2021) includes a GNN
284"
BENCHMARKING ON FS-MOL,0.3060635226179018,"which creates initial molecule embeddings. These molecule embeddings are then enriched by an
285"
BENCHMARKING ON FS-MOL,0.3070259865255053,"attention mechanism. Finally, another GNN learns relations between support and query set molecules.
286"
BENCHMARKING ON FS-MOL,0.30798845043310874,"The PAR model has shown good results for datasets which just include very few tasks such as Tox21
287"
BENCHMARKING ON FS-MOL,0.3089509143407122,"(Wang et al., 2021). Chen et al. (2022) suggest a framework for learning deep kernels by interpolating
288"
BENCHMARKING ON FS-MOL,0.30991337824831566,"between meta-learning and conventional deep kernels, which results in the ADKF-IFT model. The
289"
BENCHMARKING ON FS-MOL,0.3108758421559192,"model has exhibited especially high performance for large support set sizes.
290"
BENCHMARKING ON FS-MOL,0.31183830606352264,"Training and evaluation. For the model implementations, we used PyTorch (Paszke et al., 2019,
291"
BENCHMARKING ON FS-MOL,0.3128007699711261,"BSD license). We used PyTorch Lightning (Falcon et al., 2019, Apache 2.0 license) as a framework
292"
BENCHMARKING ON FS-MOL,0.31376323387872956,"for training and test logic, hydra for conﬁg ﬁle handling (Yadan, 2019, Apache 2.0 license) and
293"
BENCHMARKING ON FS-MOL,0.314725697786333,"Weights & Biases (Biewald, 2020, MIT license) as an experiment tracking tool. We performed ﬁve
294"
BENCHMARKING ON FS-MOL,0.3156881616939365,"training reruns with different seeds for all methods, except Classic Similarity Search as there is
295"
BENCHMARKING ON FS-MOL,0.31665062560153995,"no variability across seeds. Each model was evaluated ten times by drawing support sets with ten
296"
BENCHMARKING ON FS-MOL,0.3176130895091434,"different seeds.
297"
BENCHMARKING ON FS-MOL,0.31857555341674687,"Results. The results in terms of area under precision-recall curve (AUC-PR) are presented in Table 1,
298"
BENCHMARKING ON FS-MOL,0.31953801732435033,"where the difference to a random classiﬁer is reported (∆AUC-PR). The standard error is reported
299"
BENCHMARKING ON FS-MOL,0.3205004812319538,"across tasks. Surprisingly, the naive baseline Frequent Hitters, that neglects the support set, has out-
300"
BENCHMARKING ON FS-MOL,0.32146294513955725,"performed most of the few-shot learning methods, except for the embedding-based methods Siamese
301"
BENCHMARKING ON FS-MOL,0.3224254090471607,"Networks, ProtoNet, IterRefLSTM, and MHNfs. IterRefLSTM, which has not been included
302"
BENCHMARKING ON FS-MOL,0.3233878729547642,"in the FS-Mol benchmark study, reaches the second best performance. MHNfs has outperformed
303"
BENCHMARKING ON FS-MOL,0.32435033686236764,"all other methods with respect to ∆AUC-PR across all tasks, including the IterRefLSTM model
304"
BENCHMARKING ON FS-MOL,0.3253128007699711,"(p-value 1.72e-7, paired Wilcoxon test), the ADKF-IFT model (p-value <1.0e-8, Wilcoxon test), and
305"
BENCHMARKING ON FS-MOL,0.3262752646775746,"the PAR model (p-value <1.0e-8, paired Wilcoxon test).
306"
ABLATION STUDY,0.3272377285851781,"5.2
Ablation study
307"
ABLATION STUDY,0.32820019249278154,"MHNfs has two new main components compared to the previous state-of-the-art method Iter-
308"
ABLATION STUDY,0.329162656400385,"RefLSTM: i) the context module, and ii) the cross-attention module which replaces the LSTM-like
309"
ABLATION STUDY,0.33012512030798846,"module. To assess the effects of these components, we performed an ablation study. Therefore,
310"
ABLATION STUDY,0.3310875842155919,"we compared MHNfs to a method that does not have the context module (""MHNfs -CM"") and to
311"
ABLATION STUDY,0.3320500481231954,"a method that does not have the context module and uses an LSTM-like module instead of the
312"
ABLATION STUDY,0.33301251203079885,"CAM (""MHNfs -CM ⇌(CAM,IterRefLSTM)""). For the ablation study, we used all 5 training reruns
313"
ABLATION STUDY,0.3339749759384023,"and evaluated each model 10 times on the test set with different support sets. The results of this
314"
ABLATION STUDY,0.33493743984600577,"ablation steps are presented in Figure 2. Both removing the CM and exchanging the CAM with
315"
ABLATION STUDY,0.33589990375360923,"the IterRefLSTM module were detrimental for the performance of the method (p-value 0.002 and
316"
ABLATION STUDY,0.3368623676612127,"1.72e−7, respectively; paired Wilcoxon test). The difference was even more pronounced under
317"
ABLATION STUDY,0.33782483156881615,"domain shift (see Appendix A.3.3). Appendix A.3.2 contains a second ablation study that examines
318"
ABLATION STUDY,0.3387872954764196,"the overall effects of the context, the cross-attention, the similarity module, and the molecule encoder
319"
ABLATION STUDY,0.3397497593840231,"of MHNfs.
320"
DOMAIN SHIFT EXPERIMENT,0.34071222329162654,"5.3
Domain shift experiment
321"
DOMAIN SHIFT EXPERIMENT,0.34167468719923005,"We performed an experiment in which we evaluate models, that were pretrained on FS-Mol, on the
322"
DOMAIN SHIFT EXPERIMENT,0.3426371511068335,"Tox21 (Mayr et al., 2016) dataset. There is a strong domain shift from the drug-like molecules of
323"
DOMAIN SHIFT EXPERIMENT,0.343599615014437,"FS-Mol to the environmental chemicals, pesticides, and food additives of Tox21, such this dataset
324"
DOMAIN SHIFT EXPERIMENT,0.34456207892204044,"poses a challenging setting for few-shot learning methods. The experiment is described in detail
325"
DOMAIN SHIFT EXPERIMENT,0.3455245428296439,"in Appendix A.2. Our MHNfs approach has reached an AUC of .679 ± .018 and has signiﬁcantly
326"
DOMAIN SHIFT EXPERIMENT,0.34648700673724736,"outperformed the IterRefLSTM-based model (p∆AUC−PR-value 3.4e−5, paired Wilcoxon test) and
327"
DOMAIN SHIFT EXPERIMENT,0.3474494706448508,"the Classic Similarity Search (p∆AUC−PR-value 2.4e-9 paired Wilcoxon test) and therefore showed
328"
DOMAIN SHIFT EXPERIMENT,0.3484119345524543,"robust performance on the toxicity domain, see Table A6.
329"
CONCLUSION AND DISCUSSION,0.34937439846005774,"6
Conclusion and discussion
330"
CONCLUSION AND DISCUSSION,0.3503368623676612,"We have introduced a new architecture for few-shot learning in drug discovery that is based on
331"
CONCLUSION AND DISCUSSION,0.35129932627526467,"the novel concept to enrich molecule representations with context. In a benchmarking experiment,
332"
CONCLUSION AND DISCUSSION,0.35226179018286813,"the architecture was assessed for its ability to learn accurate predictive models from small sets of
333"
CONCLUSION AND DISCUSSION,0.3532242540904716,"labelled molecules and in this setting it outperformed all other methods. In a domain shift study, the
334"
CONCLUSION AND DISCUSSION,0.35418671799807505,"robustness and transferability of the learned models has been assessed and again MHNfs exhibited
335"
CONCLUSION AND DISCUSSION,0.3551491819056785,"the best performance. The resulting predictive models often reach an AUC larger than .70, which
336"
CONCLUSION AND DISCUSSION,0.35611164581328203,"means that enrichment of active molecules is expected (Simm et al., 2018) when the models are used
337"
CONCLUSION AND DISCUSSION,0.3570741097208855,"for virtual screening. It has not escaped our notice that the speciﬁc context module we have proposed
338"
CONCLUSION AND DISCUSSION,0.35803657362848895,"could immediately be used for few-shot learning tasks in computer vision, but might be hampered
339"
CONCLUSION AND DISCUSSION,0.3589990375360924,"by computational constraints. Limitations. While the implementation of our method is currently
340"
CONCLUSION AND DISCUSSION,0.3599615014436959,"limited to small, organic drug-like molecules as inputs, our conceptual approach can also be used
341"
CONCLUSION AND DISCUSSION,0.36092396535129934,"for macro-molecules such as RNA, DNA or proteins. The output domain of our method comprises
342"
CONCLUSION AND DISCUSSION,0.3618864292589028,"biological effects, such that the prediction must be understood in that domain. Our method demands
343"
CONCLUSION AND DISCUSSION,0.36284889316650626,"higher computational costs and memory footprint as other embedding-based methods because of
344"
CONCLUSION AND DISCUSSION,0.3638113570741097,"the calculations necessary for the context module. While we hypothesize that our approach could
345"
CONCLUSION AND DISCUSSION,0.3647738209817132,"also be successful for similar data in the materials science domain, this has not been assessed. Our
346"
CONCLUSION AND DISCUSSION,0.36573628488931664,"study is also constrained by a limited amount of hyperparameter search for all methods. Deep
347"
CONCLUSION AND DISCUSSION,0.3666987487969201,"learning methods usually have a large number of hyperparameters, such as hidden dimensions,
348"
CONCLUSION AND DISCUSSION,0.36766121270452357,"number of layers, learning rates, of which we were only able to explore the most important ones. The
349"
CONCLUSION AND DISCUSSION,0.368623676612127,"composition and choice of the context set is also under-explored and might be improved by selecting
350"
CONCLUSION AND DISCUSSION,0.3695861405197305,"reference molecules with an appropriate strategy. Broader impact. Impact on machine learning and
351"
CONCLUSION AND DISCUSSION,0.37054860442733395,"related scientiﬁc ﬁelds. We envision that with (a) the increasing availability of drug discovery and
352"
CONCLUSION AND DISCUSSION,0.37151106833493747,"material science datasets, (b) further improved biotechnologies, and (c) accounting for characteristics
353"
CONCLUSION AND DISCUSSION,0.37247353224254093,"of individuals, the drug and materials discovery process will be made more efﬁcient. For machine
354"
CONCLUSION AND DISCUSSION,0.3734359961501444,"learning and artiﬁcial intelligence, the novel way in which representations are enriched with context
355"
CONCLUSION AND DISCUSSION,0.37439846005774785,"might strengthen the general research stream to include more context into deep learning systems. Our
356"
CONCLUSION AND DISCUSSION,0.3753609239653513,"approach also shows that such a system is more robust against domain shifts, which could be a step
357"
CONCLUSION AND DISCUSSION,0.3763233878729548,"towards Broad AI (Chollet, 2019; Hochreiter, 2022). Impact on society. If the approach proves useful,
358"
CONCLUSION AND DISCUSSION,0.37728585178055823,"it could lead to a faster and more cost-efﬁcient drug discovery process. Especially the COVID-19
359"
CONCLUSION AND DISCUSSION,0.3782483156881617,"pandemic has shown that it is crucial for humanity to speed up the drug discovery process to few years
360"
CONCLUSION AND DISCUSSION,0.37921077959576516,"or even months. We hope that this work contributes to this effort and eventually leads to safer drugs
361"
CONCLUSION AND DISCUSSION,0.3801732435033686,"developed faster. Consequences of failures of the method. As common with methods in machine
362"
CONCLUSION AND DISCUSSION,0.3811357074109721,"learning, potential danger lies in the possibility that users rely too much on our new approach and use
363"
CONCLUSION AND DISCUSSION,0.38209817131857554,"it without reﬂecting on the outcomes. Failures of the proposed method would lead to unsuccessful
364"
CONCLUSION AND DISCUSSION,0.383060635226179,"wet lab validation and negative wet lab tests. Since the proposed algorithm does not directly suggest
365"
CONCLUSION AND DISCUSSION,0.38402309913378246,"treatment or therapy, human beings are not directly at risk of being treated with a harmful therapy.
366"
CONCLUSION AND DISCUSSION,0.3849855630413859,"Wet lab and in-vitro testing would indicate wrong decisions by the system. Leveraging of biases in
367"
CONCLUSION AND DISCUSSION,0.3859480269489894,"the data and potential discrimination. As for almost all machine learning methods, confounding
368"
CONCLUSION AND DISCUSSION,0.3869104908565929,"factors, lab or batch effects, could be used for classiﬁcation. This might lead to biases in predictions
369"
CONCLUSION AND DISCUSSION,0.38787295476419636,"or uneven predictive performance across different drug targets or bioassays.
370"
REFERENCES,0.3888354186717998,"References
371"
REFERENCES,0.3897978825794033,"Adler, T., Brandstetter, J., Widrich, M., Mayr, A., Kreil, D., Kopp, M., Klambauer, G., and
372"
REFERENCES,0.39076034648700675,"Hochreiter, S. (2020). Cross-domain few-shot learning by representation fusion. arXiv preprint
373"
REFERENCES,0.3917228103946102,"arXiv:2010.06498.
374"
REFERENCES,0.39268527430221367,"Alain, G. and Bengio, Y. (2016). Understanding intermediate layers using linear classiﬁer probes.
375"
REFERENCES,0.39364773820981713,"arXiv preprint arXiv:1610.01644.
376"
REFERENCES,0.3946102021174206,"Alperstein, Z., Cherkasov, A., and Rolfe, J. T. (2019). All smiles variational autoencoder. arXiv
377"
REFERENCES,0.39557266602502406,"preprint arXiv:1905.13343.
378"
REFERENCES,0.3965351299326275,"Altae-Tran, H., Ramsundar, B., Pappu, A. S., and Pande, V. (2017). Low data drug discovery with
379"
REFERENCES,0.397497593840231,"one-shot learning. ACS central science, 3(4):283–293.
380"
REFERENCES,0.39846005774783444,"Antoniou, A. and Storkey, A. (2019). Assume, augment and learn: Unsupervised few-shot meta-
381"
REFERENCES,0.3994225216554379,"learning via random labels and data augmentation. arXiv preprint arXiv:1902.09884.
382"
REFERENCES,0.40038498556304136,"Arrowsmith, J. (2011). Phase ii failures: 2008-2010. Nature reviews drug discovery, 10(5).
383"
REFERENCES,0.4013474494706448,"Axelrod, S. and Gomez-Bombarelli, R. (2022). Geom, energy-annotated molecular conformations
384"
REFERENCES,0.40230991337824834,"for property prediction and molecular generation. Scientiﬁc Data, 9(1):1–14.
385"
REFERENCES,0.4032723772858518,"Ba, J. L., Kiros, J. R., and Hinton, G. E. (2016).
Layer normalization.
arXiv preprint
386"
REFERENCES,0.40423484119345526,"arXiv:1607.06450.
387"
REFERENCES,0.4051973051010587,"Bender, A., Mussa, H. Y., Glen, R. C., and Reiling, S. (2004). Similarity searching of chemical
388"
REFERENCES,0.4061597690086622,"databases using atom environment descriptors (molprint 2d): evaluation of performance. Journal
389"
REFERENCES,0.40712223291626565,"of chemical information and computer sciences, 44(5):1708–1718.
390"
REFERENCES,0.4080846968238691,"Bendre, N., Marín, H. T., and Najaﬁrad, P. (2020). Learning from few samples: A survey. arXiv
391"
REFERENCES,0.40904716073147257,"preprint arXiv:2007.15484.
392"
REFERENCES,0.41000962463907603,"Bengio, Y., Bengio, S., and Cloutier, J. (1991). Learning a synaptic learning rule. In Seattle
393"
REFERENCES,0.4109720885466795,"international joint conference on neural networks.
394"
REFERENCES,0.41193455245428295,"Biewald, L. (2020).
Experiment tracking with weights and biases.
Software available from
395"
REFERENCES,0.4128970163618864,"wandb.com.
396"
REFERENCES,0.4138594802694899,"Bonner, M. F. and Epstein, R. A. (2021). Object representations in the human brain reﬂect the
397"
REFERENCES,0.41482194417709334,"co-occurrence statistics of vision and language. Nature Communications, 12(4081).
398"
REFERENCES,0.4157844080846968,"Breiman, L. (2001). Random forests. Machine learning, 45(1):5–32.
399"
REFERENCES,0.4167468719923003,"Cereto-Massagué, A., Ojeda, M. J., Valls, C., Mulero, M., Garcia-Vallvé, S., and Pujadas, G. (2015).
400"
REFERENCES,0.4177093358999038,"Molecular ﬁngerprint similarity search in virtual screening. Methods, 71:58–63.
401"
REFERENCES,0.41867179980750724,"Chen, H., Engkvist, O., Wang, Y., Olivecrona, M., and Blaschke, T. (2018). The rise of deep learning
402"
REFERENCES,0.4196342637151107,"in drug discovery. Drug discovery today, 23(6):1241–1250.
403"
REFERENCES,0.42059672762271416,"Chen, H., Li, H., Li, Y., and Chen, C. (2021). Sparse spatial transformers for few-shot learning. arXiv
404"
REFERENCES,0.4215591915303176,"preprint arXiv:2109.12932.
405"
REFERENCES,0.4225216554379211,"Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020). A simple framework for contrastive
406"
REFERENCES,0.42348411934552455,"learning of visual representations. In International conference on machine learning, pages 1597–
407"
REFERENCES,0.424446583253128,"1607. PMLR.
408"
REFERENCES,0.42540904716073147,"Chen, W., Tripp, A., and Hernández-Lobato, J. M. (2022). Meta-learning feature representations for
409"
REFERENCES,0.42637151106833493,"adaptive gaussian processes via implicit differentiation. arXiv preprint arXiv:2205.02708.
410"
REFERENCES,0.4273339749759384,"Chollet, F. (2019). On the measure of intelligence. arXiv preprint arXiv:1911.01547.
411"
REFERENCES,0.42829643888354185,"Dahl, G. E., Jaitly, N., and Salakhutdinov, R. (2014). Multi-task neural networks for qsar predictions.
412"
REFERENCES,0.4292589027911453,"arXiv preprint arXiv:1406.1231.
413"
REFERENCES,0.4302213666987488,"Duvenaud, D., Maclaurin, D., Aguilera-Iparraguirre, J., Gómez-Bombarelli, R., Hirzel, T., Aspuru-
414"
REFERENCES,0.43118383060635224,"Guzik, A., and Adams, R. P. (2015). Convolutional networks on graphs for learning molecular
415"
REFERENCES,0.43214629451395575,"ﬁngerprints. arXiv preprint arXiv:1509.09292.
416"
REFERENCES,0.4331087584215592,"Eckert, H. and Bajorath, J. (2007). Molecular similarity analysis in virtual screening: foundations,
417"
REFERENCES,0.4340712223291627,"limitations and novel approaches. Drug discovery today, 12(5-6):225–233.
418"
REFERENCES,0.43503368623676614,"Falcon,
W.
et
al.
(2019).
Pytorch
lightning.
GitHub.
Note:
https://github.
419"
REFERENCES,0.4359961501443696,"com/PyTorchLightning/pytorch-lightning, 3:6.
420"
REFERENCES,0.43695861405197306,"Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep
421"
REFERENCES,0.4379210779595765,"networks. In International conference on machine learning, pages 1126–1135. PMLR.
422"
REFERENCES,0.43888354186718,"Fürst, A., Rumetshofer, E., Tran, V., Ramsauer, H., Tang, F., Lehner, J., Kreil, D., Kopp, M.,
423"
REFERENCES,0.43984600577478344,"Klambauer, G., Bitto-Nemling, A., et al. (2021). Cloob: Modern hopﬁeld networks with infoloob
424"
REFERENCES,0.4408084696823869,"outperform clip. arXiv preprint arXiv:2110.11316.
425"
REFERENCES,0.44177093358999037,"Geppert, H., Horváth, T., Gärtner, T., Wrobel, S., and Bajorath, J. (2008). Support-vector-machine-
426"
REFERENCES,0.44273339749759383,"based ranking signiﬁcantly improves the effectiveness of similarity searching using 2d ﬁngerprints
427"
REFERENCES,0.4436958614051973,"and multiple reference compounds. Journal of chemical information and modeling, 48(4):742–746.
428"
REFERENCES,0.44465832531280075,"Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. (2017). Neural message
429"
REFERENCES,0.4456207892204042,"passing for quantum chemistry. In International conference on machine learning, pages 1263–1272.
430"
REFERENCES,0.4465832531280077,"PMLR.
431"
REFERENCES,0.4475457170356112,"Gomez, L. (2018). Decision making in medicinal chemistry: The power of our intuition. ACS
432"
REFERENCES,0.44850818094321465,"Medicinal Chemistry Letters, 9(10):956–958.
433"
REFERENCES,0.4494706448508181,"Gómez-Bombarelli, R., Wei, J. N., Duvenaud, D., Hernández-Lobato, J. M., Sánchez-Lengeling,
434"
REFERENCES,0.4504331087584216,"B., Sheberla, D., Aguilera-Iparraguirre, J., Hirzel, T. D., Adams, R. P., and Aspuru-Guzik, A.
435"
REFERENCES,0.45139557266602504,"(2018). Automatic chemical design using a data-driven continuous representation of molecules.
436"
REFERENCES,0.4523580365736285,"ACS central science, 4(2):268–276.
437"
REFERENCES,0.45332050048123196,"Graves, A., Wayne, G., and Danihelka, I. (2014).
Neural turing machines.
arXiv preprint
438"
REFERENCES,0.4542829643888354,"arXiv:1410.5401.
439"
REFERENCES,0.4552454282964389,"Guo, Z., Zhang, C., Yu, W., Herr, J., Wiest, O., Jiang, M., and Chawla, N. V. (2021). Few-shot graph
440"
REFERENCES,0.45620789220404234,"learning for molecular property prediction. In Proceedings of the web conference 2021, pages
441"
REFERENCES,0.4571703561116458,"2559–2567.
442"
REFERENCES,0.45813282001924927,"He, J., You, H., Sandström, E., Nittinger, E., Bjerrum, E. J., Tyrchan, C., Czechtizky, W., and
443"
REFERENCES,0.4590952839268527,"Engkvist, O. (2021). Molecular optimization by capturing chemist’s intuition using deep neural
444"
REFERENCES,0.4600577478344562,"networks. Journal of cheminformatics, 13(1):1–17.
445"
REFERENCES,0.46102021174205965,"Hertz, T., Hillel, A. B., and Weinshall, D. (2006). Learning a kernel function for classiﬁcation with
446"
REFERENCES,0.4619826756496631,"small training samples. In Proceedings of the 23rd international conference on machine learning,
447"
REFERENCES,0.4629451395572666,"pages 401–408.
448"
REFERENCES,0.4639076034648701,"Hochreiter, S. (2022). Toward a broad ai. Communications of the ACM, 65(4):56–57.
449"
REFERENCES,0.46487006737247355,"Hochreiter, S., Klambauer, G., and Rarey, M. (2018). Machine learning in drug discovery. Journal of
450"
REFERENCES,0.465832531280077,"Chemical Information and Modeling, 58(9):1723–1724.
451"
REFERENCES,0.4667949951876805,"Hochreiter, S., Younger, A. S., and Conwell, P. R. (2001). Learning to learn using gradient descent.
452"
REFERENCES,0.46775745909528393,"In International conference on artiﬁcial neural networks, pages 87–94. Springer.
453"
REFERENCES,0.4687199230028874,"Hou, R., Chang, H., Ma, B., Shan, S., and Chen, X. (2019). Cross attention network for few-shot
454"
REFERENCES,0.46968238691049086,"classiﬁcation. Advances in neural information processing systems 32.
455"
REFERENCES,0.4706448508180943,"Huang, R., Xia, M., Nguyen, D.-T., Zhao, T., Sakamuru, S., Zhao, J., Shahane, S. A., Rossoshek,
456"
REFERENCES,0.4716073147256978,"A., and Simeonov, A. (2016a). Tox21challenge to build predictive models of nuclear receptor and
457"
REFERENCES,0.47256977863330124,"stress response pathways as mediated by exposure to environmental chemicals and drugs. Frontiers
458"
REFERENCES,0.4735322425409047,"in Environmental Science, 3:85.
459"
REFERENCES,0.47449470644850816,"Huang, R., Xia, M., Sakamuru, S., Zhao, J., Shahane, S. A., Attene-Ramos, M., Zhao, T., Austin,
460"
REFERENCES,0.4754571703561116,"C. P., and Simeonov, A. (2016b). Modelling the tox21 10 k chemical proﬁles for in vivo toxicity
461"
REFERENCES,0.4764196342637151,"prediction and mechanism characterization. Nature communications, 7(1):1–10.
462"
REFERENCES,0.4773820981713186,"Jiang, D., Wu, Z., Hsieh, C.-Y., Chen, G., Liao, B., Wang, Z., Shen, C., Cao, D., Wu, J., and Hou, T.
463"
REFERENCES,0.47834456207892206,"(2021). Could graph neural networks learn better molecular representation for drug discovery?
464"
REFERENCES,0.4793070259865255,"a comparison study of descriptor-based and graph-based models. Journal of cheminformatics,
465"
REFERENCES,0.480269489894129,"13(1):1–23.
466"
REFERENCES,0.48123195380173245,"Kearnes, S., McCloskey, K., Berndl, M., Pande, V., and Riley, P. (2016). Molecular graph convolu-
467"
REFERENCES,0.4821944177093359,"tions: moving beyond ﬁngerprints. Journal of computer-aided molecular design, 30(8):595–608.
468"
REFERENCES,0.48315688161693937,"Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint
469"
REFERENCES,0.48411934552454283,"arXiv:1412.6980.
470"
REFERENCES,0.4850818094321463,"Klambauer, G., Unterthiner, T., Mayr, A., and Hochreiter, S. (2017). Self-normalizing neural networks.
471"
REFERENCES,0.48604427333974976,"In Advances in neural information processing systems 30, pages 972–981.
472"
REFERENCES,0.4870067372473532,"Koch, G., Zemel, R., Salakhutdinov, R., et al. (2015). Siamese neural networks for one-shot image
473"
REFERENCES,0.4879692011549567,"recognition. In ICML deep learning workshop, volume 2. Lille.
474"
REFERENCES,0.48893166506256014,"Kuhn, M., Letunic, I., Jensen, L. J., and Bork, P. (2016). The sider database of drugs and side effects.
475"
REFERENCES,0.4898941289701636,"Nucleic acids research, 44(D1):D1075–D1079.
476"
REFERENCES,0.49085659287776706,"Landrum, G. et al. (2006). Rdkit: Open-source cheminformatics.
477"
REFERENCES,0.4918190567853705,"Le, H. (2021). Memory and attention in deep learning. arXiv preprint arXiv:2107.01390.
478"
REFERENCES,0.49278152069297404,"Li, J., Cai, D., and He, X. (2017). Learning graph-level representation for drug discovery. arXiv
479"
REFERENCES,0.4937439846005775,"preprint arXiv:1709.03741.
480"
REFERENCES,0.49470644850818096,"Li, P., Li, Y., Hsieh, C.-Y., Zhang, S., Liu, X., Liu, H., Song, S., and Yao, X. (2021). Trimnet: learning
481"
REFERENCES,0.4956689124157844,"molecular representation from triplet messages for biomedicine. Brieﬁngs in Bioinformatics,
482"
REFERENCES,0.4966313763233879,"22(4):bbaa266.
483"
REFERENCES,0.49759384023099135,"Ma, Y., Liu, W., Bai, S., Zhang, Q., Liu, A., Chen, W., and Liu, X. (2021). Few-shot visual
484"
REFERENCES,0.4985563041385948,"learning with contextual memory and ﬁne-grained calibration. In Proceedings of the Twenty-Ninth
485"
REFERENCES,0.49951876804619827,"International Conference on International Joint Conferences on Artiﬁcial Intelligence, pages
486"
REFERENCES,0.5004812319538018,"811–817.
487"
REFERENCES,0.5014436958614052,"Mayr, A., Klambauer, G., Unterthiner, T., and Hochreiter, S. (2016). Deeptox: toxicity prediction
488"
REFERENCES,0.5024061597690087,"using deep learning. Frontiers in environmental science, 3:80.
489"
REFERENCES,0.5033686236766122,"Mayr, A., Klambauer, G., Unterthiner, T., Steijaert, M., Wegner, J. K., Ceulemans, H., Clevert, D.-A.,
490"
REFERENCES,0.5043310875842156,"and Hochreiter, S. (2018). Large-scale comparison of machine learning methods for drug target
491"
REFERENCES,0.5052935514918191,"prediction on chembl. Chemical science, 9(24):5441–5451.
492"
REFERENCES,0.5062560153994226,"Maziarka, Ł., Danel, T., Mucha, S., Rataj, K., Tabor, J., and Jastrz˛ebski, S. (2020). Molecule attention
493"
REFERENCES,0.507218479307026,"transformer. arXiv preprint arXiv:2002.08264.
494"
REFERENCES,0.5081809432146295,"Merk, D., Friedrich, L., Grisoni, F., and Schneider, G. (2018). De novo design of bioactive small
495"
REFERENCES,0.5091434071222329,"molecules by artiﬁcial intelligence. Molecular informatics, 37(1-2):1700153.
496"
REFERENCES,0.5101058710298364,"Merkwirth, C. and Lengauer, T. (2005). Automatic generation of complementary descriptors with
497"
REFERENCES,0.5110683349374399,"molecular graph networks. Journal of chemical information and modeling, 45(5):1159–1168.
498"
REFERENCES,0.5120307988450433,"Miller, E. G., Matsakis, N. E., and Viola, P. A. (2000). Learning from one example through shared
499"
REFERENCES,0.5129932627526468,"densities on transforms. In Proceedings ieee conference on computer vision and pattern recognition.
500"
REFERENCES,0.5139557266602502,"cvpr 2000 (cat. no. PR00662), volume 1, pages 464–471.
501"
REFERENCES,0.5149181905678537,"Munkhdalai, T. and Yu, H. (2017). Meta networks. In International Conference on Machine Learning,
502"
REFERENCES,0.5158806544754572,"pages 2554–2563. PMLR.
503"
REFERENCES,0.5168431183830606,"Nguyen, C. Q., Kreatsoulas, C., and Branson, K. M. (2020). Meta-learning gnn initializations for
504"
REFERENCES,0.5178055822906641,"low-resource molecular property prediction. arXiv preprint arXiv:2003.05996.
505"
REFERENCES,0.5187680461982676,"Oord, A. v. d., Li, Y., and Vinyals, O. (2018). Representation learning with contrastive predictive
506"
REFERENCES,0.519730510105871,"coding. arXiv preprint arXiv:1807.03748.
507"
REFERENCES,0.5206929740134745,"Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga,
508"
REFERENCES,0.5216554379210779,"L., and Lerer, A. (2019). Automatic differentiation in pytorch. In Conference on neural information
509"
REFERENCES,0.5226179018286814,"processing systems.
510"
REFERENCES,0.5235803657362849,"Potter, M. (2012). Conceptual short term memory in perception and thought. Frontiers in Psychology,
511"
REFERENCES,0.5245428296438883,"3:113.
512"
REFERENCES,0.5255052935514918,"Ramalho, T. and Garnelo, M. (2018). Adaptive posterior learning: few-shot learning with a surprise-
513"
REFERENCES,0.5264677574590952,"based memory module. In International Conference on Learning Representations.
514"
REFERENCES,0.5274302213666987,"Ramsauer, H., Schäﬂ, B., Lehner, J., Seidl, P., Widrich, M., Gruber, L., Holzleitner, M., Adler, T.,
515"
REFERENCES,0.5283926852743022,"Kreil, D., Kopp, M. K., Klambauer, G., Brandstetter, J., and Hochreiter, S. (2021). Hopﬁeld
516"
REFERENCES,0.5293551491819056,"networks is all you need. In International conference on learning representations.
517"
REFERENCES,0.5303176130895092,"Riniker, S. and Landrum, G. A. (2013). Open-source platform to benchmark ﬁngerprints for ligand-
518"
REFERENCES,0.5312800769971127,"based virtual screening. Journal of cheminformatics, 5(1):1–17.
519"
REFERENCES,0.5322425409047161,"Rogers, D. and Hahn, M. (2010). Extended-connectivity ﬁngerprints. Journal of chemical information
520"
REFERENCES,0.5332050048123196,"and modeling, 50(5):742–754.
521"
REFERENCES,0.534167468719923,"Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. (2016). Meta-learning with
522"
REFERENCES,0.5351299326275265,"memory-augmented neural networks. In International conference on machine learning, pages
523"
REFERENCES,0.53609239653513,"1842–1850. PMLR.
524"
REFERENCES,0.5370548604427334,"Schmidhuber, J. (1987). Evolutionary principles in self-referential learning.
525"
REFERENCES,0.5380173243503369,"Schneider, P., Walters, W. P., Plowright, A. T., Sieroka, N., Listgarten, J., Goodnow, R. A., Fisher,
526"
REFERENCES,0.5389797882579404,"J., Jansen, J. M., Duca, J. S., Rush, T. S., et al. (2020). Rethinking drug design in the artiﬁcial
527"
REFERENCES,0.5399422521655438,"intelligence era. Nature reviews drug discovery, 19(5):353–364.
528"
REFERENCES,0.5409047160731473,"Segler, M. H., Kogej, T., Tyrchan, C., and Waller, M. P. (2018a). Generating focused molecule
529"
REFERENCES,0.5418671799807507,"libraries for drug discovery with recurrent neural networks. ACS central science, 4(1):120–131.
530"
REFERENCES,0.5428296438883542,"Segler, M. H., Preuss, M., and Waller, M. P. (2018b). Planning chemical syntheses with deep neural
531"
REFERENCES,0.5437921077959577,"networks and symbolic ai. Nature, 555(7698):604–610.
532"
REFERENCES,0.5447545717035611,"Seidl, P., Renz, P., Dyubankova, N., Neves, P., Verhoeven, J., Wegner, J. K., Segler, M., Hochreiter,
533"
REFERENCES,0.5457170356111646,"S., and Klambauer, G. (2022). Improving few-and zero-shot reaction template prediction using
534"
REFERENCES,0.546679499518768,"modern hopﬁeld networks. Journal of chemical information and modeling, 62(9):2111–2120.
535"
REFERENCES,0.5476419634263715,"Sheridan, R. P. and Kearsley, S. K. (2002). Why do we need so many chemical similarity search
536"
REFERENCES,0.548604427333975,"methods? Drug discovery today, 7(17):903–911.
537"
REFERENCES,0.5495668912415784,"Simm, J., Klambauer, G., Arany, A., Steijaert, M., Wegner, J. K., Gustin, E., Chupakhin, V., Chong,
538"
REFERENCES,0.5505293551491819,"Y. T., Vialard, J., Buijnsters, P., et al. (2018). Repurposed high-throughput image assays enables
539"
REFERENCES,0.5514918190567853,"biological activity prediction for drug discovery. Cell Chemical Biology, page 108399.
540"
REFERENCES,0.5524542829643888,"Snell, J., Swersky, K., and Zemel, R. S. (2017). Prototypical networks for few-shot learning. arXiv
541"
REFERENCES,0.5534167468719923,"preprint arXiv:1703.05175.
542"
REFERENCES,0.5543792107795957,"Stanley, M., Bronskill, J. F., Maziarz, K., Misztela, H., Lanini, J., Segler, M., Schneider, N., and
543"
REFERENCES,0.5553416746871992,"Brockschmidt, M. (2021). Fs-mol: A few-shot learning dataset of molecules. In Conference on
544"
REFERENCES,0.5563041385948027,"neural information processing systems workshop.
545"
REFERENCES,0.5572666025024061,"Stork, C., Chen, Y., Sicho, M., and Kirchmair, J. (2019). Hit dexter 2.0: machine-learning models for
546"
REFERENCES,0.5582290664100096,"the prediction of frequent hitters. Journal of chemical information and modeling, 59(3):1030–1043.
547"
REFERENCES,0.559191530317613,"Sturm, N., Mayr, A., Le Van, T., Chupakhin, V., Ceulemans, H., Wegner, J., Golib-Dzib, J.-F.,
548"
REFERENCES,0.5601539942252165,"Jeliazkova, N., Vandriessche, Y., Böhm, S., et al. (2020). Industry-scale application and evaluation
549"
REFERENCES,0.5611164581328201,"of deep learning for drug target prediction. Journal of Cheminformatics, 12(1):1–13.
550"
REFERENCES,0.5620789220404235,"Sukhbaatar, S., Weston, J., Fergus, R., et al. (2015). End-to-end memory networks. Advances in
551"
REFERENCES,0.563041385948027,"neural information processing systems, 28.
552"
REFERENCES,0.5640038498556305,"Sun, J., Jeliazkova, N., Chupakhin, V., Golib-Dzib, J.-F., Engkvist, O., Carlsson, L., Wegner, J.,
553"
REFERENCES,0.5649663137632339,"Ceulemans, H., Georgiev, I., Jeliazkov, V., et al. (2017). Excape-db: an integrated large scale
554"
REFERENCES,0.5659287776708374,"dataset facilitating big data analysis in chemogenomics. Journal of cheminformatics, 9(1):1–9.
555"
REFERENCES,0.5668912415784408,"Tanimoto, T. (1960). Ibm type 704 medical diagnosis program. IRE transactions on medical
556"
REFERENCES,0.5678537054860443,"electronics, (4):280–283.
557"
REFERENCES,0.5688161693936478,"Torres, L., Monteiro, N., Oliveira, J., Arrais, J., and Ribeiro, B. (2020). Exploring a siamese neural
558"
REFERENCES,0.5697786333012512,"network architecture for one-shot drug discovery. In 2020 ieee 20th international conference on
559"
REFERENCES,0.5707410972088547,"bioinformatics and bioengineering (bibe), pages 168–175.
560"
REFERENCES,0.5717035611164581,"Unterthiner, T., Mayr, A., Klambauer, G., Steijaert, M., Wegner, J. K., Ceulemans, H., and Hochreiter,
561"
REFERENCES,0.5726660250240616,"S. (2014). Deep learning as an opportunity in virtual screening. In Advances in neural information
562"
REFERENCES,0.5736284889316651,"processing systems workshop.
563"
REFERENCES,0.5745909528392685,"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and
564"
REFERENCES,0.575553416746872,"Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing
565"
REFERENCES,0.5765158806544755,"systems, pages 5998–6008.
566"
REFERENCES,0.5774783445620789,"Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. (2016). Matching networks for one shot
567"
REFERENCES,0.5784408084696824,"learning. Advances in neural information processing systems, 29:3630–3638.
568"
REFERENCES,0.5794032723772858,"Walters, W. P. and Barzilay, R. (2021). Critical assessment of ai in drug discovery. Expert opinion on
569"
REFERENCES,0.5803657362848893,"drug discovery, pages 1–11.
570"
REFERENCES,0.5813282001924928,"Wang, X., Huan, J., Smalter, A., and Lushington, G. H. (2010). Application of kernel functions for
571"
REFERENCES,0.5822906641000962,"accurate similarity search in large chemical databases. In BMC bioinformatics, volume 11, pages
572"
REFERENCES,0.5832531280076997,"1–14. BioMed Central.
573"
REFERENCES,0.5842155919153031,"Wang, Y., Abuduweili, A., Yao, Q., and Dou, D. (2021). Property-aware relation networks for
574"
REFERENCES,0.5851780558229066,"few-shot molecular property prediction. Advances in Neural Information Processing Systems,
575"
REFERENCES,0.5861405197305101,"34:17441–17454.
576"
REFERENCES,0.5871029836381135,"Wang, Y., Yao, Q., Kwok, J. T., and Ni, L. M. (2020). Generalizing from a few examples: A survey
577"
REFERENCES,0.588065447545717,"on few-shot learning. ACM computing surveys (csur), 53(3):1–34.
578"
REFERENCES,0.5890279114533205,"Waring, M. J., Arrowsmith, J., Leach, A. R., Leeson, P. D., Mandrell, S., Owen, R. M., Pairaudeau, G.,
579"
REFERENCES,0.5899903753609239,"Pennie, W. D., Pickett, S. D., Wang, J., et al. (2015). An analysis of the attrition of drug candidates
580"
REFERENCES,0.5909528392685275,"from four major pharmaceutical companies. Nature reviews drug discovery, 14(7):475–486.
581"
REFERENCES,0.591915303176131,"Weininger, D. (1988). Smiles, a chemical language and information system. 1. introduction to
582"
REFERENCES,0.5928777670837344,"methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):31–
583"
REFERENCES,0.5938402309913379,"36.
584"
REFERENCES,0.5948026948989413,"Weston, J., Chopra, S., and Bordes, A. (2014). Memory networks. arXiv preprint arXiv:1410.3916.
585"
REFERENCES,0.5957651588065448,"Widrich, M., Schäﬂ, B., Pavlovi´c, M., Ramsauer, H., Gruber, L., Holzleitner, M., Brandstetter, J.,
586"
REFERENCES,0.5967276227141483,"Sandve, G. K., Greiff, V., Hochreiter, S., et al. (2020). Modern hopﬁeld networks and attention for
587"
REFERENCES,0.5976900866217517,"immune repertoire classiﬁcation. In Advances in neural information processing systems 33.
588"
REFERENCES,0.5986525505293552,"Willett, P. (2014). The calculation of molecular structural similarity: principles and practice. Molecu-
589"
REFERENCES,0.5996150144369586,"lar informatics, 33(6-7):403–413.
590"
REFERENCES,0.6005774783445621,"Winter, R., Montanari, F., Noé, F., and Clevert, D.-A. (2019). Learning continuous and data-driven
591"
REFERENCES,0.6015399422521656,"molecular descriptors by translating equivalent chemical representations. Chemical science,
592"
REFERENCES,0.602502406159769,"10(6):1692–1701.
593"
REFERENCES,0.6034648700673725,"Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Geniesse, C., Pappu, A. S., Leswing, K., and
594"
REFERENCES,0.6044273339749759,"Pande, V. (2018). Moleculenet: a benchmark for molecular machine learning. Chemical science,
595"
REFERENCES,0.6053897978825794,"9(2):513–530.
596"
REFERENCES,0.6063522617901829,"Yadan, O. (2019). Hydra - a framework for elegantly conﬁguring complex applications. Github.
597"
REFERENCES,0.6073147256977863,"Visited 2022-04-25.
598"
REFERENCES,0.6082771896053898,"Yang, K., Swanson, K., Jin, W., Coley, C., Eiden, P., Gao, H., Guzman-Perez, A., Hopper, T., Kelley,
599"
REFERENCES,0.6092396535129933,"B., Mathea, M., et al. (2019). Analyzing learned molecular representations for property prediction.
600"
REFERENCES,0.6102021174205967,"Journal of chemical information and modeling, 59(8):3370–3388.
601"
REFERENCES,0.6111645813282002,"Ye, M. and Guo, Y. (2018). Deep triplet ranking networks for one-shot recognition. arXiv preprint
602"
REFERENCES,0.6121270452358036,"arXiv:1804.07275.
603"
REFERENCES,0.6130895091434071,"Zaslavskiy, M., Jégou, S., Tramel, E. W., and Wainrib, G. (2019). Toxicblend: Virtual screening of
604"
REFERENCES,0.6140519730510106,"toxic compounds with ensemble predictors. Computational Toxicology, 10:81–88.
605"
REFERENCES,0.615014436958614,"Zhao, A., Balakrishnan, G., Durand, F., Guttag, J. V., and Dalca, A. V. (2019). Data augmentation
606"
REFERENCES,0.6159769008662175,"using learned transformations for one-shot medical image segmentation. In Proceedings of the
607"
REFERENCES,0.6169393647738209,"ieee conference on computer vision and pattern recognition, pages 8543–8553.
608"
REFERENCES,0.6179018286814244,"Checklist
609"
REFERENCES,0.6188642925890279,"1. For all authors...
610"
REFERENCES,0.6198267564966313,"(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
611"
REFERENCES,0.6207892204042348,"contributions and scope? [Yes]
612"
REFERENCES,0.6217516843118384,"(b) Did you describe the limitations of your work? [Yes] See Section 6.
613"
REFERENCES,0.6227141482194418,"(c) Did you discuss any potential negative societal impacts of your work? [Yes] See
614"
REFERENCES,0.6236766121270453,"Section 6.
615"
REFERENCES,0.6246390760346487,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
616"
REFERENCES,0.6256015399422522,"them? [Yes]
617"
REFERENCES,0.6265640038498557,"2. If you are including theoretical results...
618"
REFERENCES,0.6275264677574591,"(a) Did you state the full set of assumptions of all theoretical results? [N/A]
619"
REFERENCES,0.6284889316650626,"(b) Did you include complete proofs of all theoretical results? [N/A]
620"
REFERENCES,0.629451395572666,"3. If you ran experiments...
621"
REFERENCES,0.6304138594802695,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
622"
REFERENCES,0.631376323387873,"mental results (either in the supplemental material or as a URL)? [Yes]
623"
REFERENCES,0.6323387872954764,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
624"
REFERENCES,0.6333012512030799,"were chosen)? [Yes] We refer e.g. to Section 5 in which we provide this information.
625"
REFERENCES,0.6342637151106834,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
626"
REFERENCES,0.6352261790182868,"ments multiple times)? [Yes] We report error bars for all performance metrics. For all
627"
REFERENCES,0.6361886429258903,"experiments the variability across re-runs, different support sets and prediction tasks
628"
REFERENCES,0.6371511068334937,"was assessed.
629"
REFERENCES,0.6381135707410972,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
630"
REFERENCES,0.6390760346487007,"of GPUs, internal cluster, or cloud provider)? [Yes] See Section 3.5.
631"
REFERENCES,0.6400384985563041,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
632"
REFERENCES,0.6410009624639076,"(a) If your work uses existing assets, did you cite the creators? [Yes]
633"
REFERENCES,0.641963426371511,"(b) Did you mention the license of the assets? [Yes]
634"
REFERENCES,0.6429258902791145,"(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
635"
REFERENCES,0.643888354186718,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
636"
REFERENCES,0.6448508180943214,"using/curating? [N/A]
637"
REFERENCES,0.6458132820019249,"(e) Did you discuss whether the data you are using/curating contains personally identiﬁable
638"
REFERENCES,0.6467757459095284,"information or offensive content? [Yes] See Section 6.
639"
REFERENCES,0.6477382098171318,"5. If you used crowdsourcing or conducted research with human subjects...
640"
REFERENCES,0.6487006737247353,"(a) Did you include the full text of instructions given to participants and screenshots, if
641"
REFERENCES,0.6496631376323387,"applicable? [N/A]
642"
REFERENCES,0.6506256015399422,"(b) Did you describe any potential participant risks, with links to Institutional Review
643"
REFERENCES,0.6515880654475458,"Board (IRB) approvals, if applicable? [N/A]
644"
REFERENCES,0.6525505293551492,"(c) Did you include the estimated hourly wage paid to participants and the total amount
645"
REFERENCES,0.6535129932627527,"spent on participant compensation? [N/A]
646"
REFERENCES,0.6544754571703562,"Contents of the appendix
647"
REFERENCES,0.6554379210779596,"A Appendix
17
648"
REFERENCES,0.6564003849855631,"A.1
Details on methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
649"
REFERENCES,0.6573628488931665,"A.1.1
Frequent hitters: details and hyperparameters . . . . . . . . . . . . . . . .
17
650"
REFERENCES,0.65832531280077,"A.1.2
Classic similarity search: details and hyperparameters
. . . . . . . . . . .
18
651"
REFERENCES,0.6592877767083735,"A.1.3
Neural Similarity Search or Siamese networks: details and hyperparameters
19
652"
REFERENCES,0.6602502406159769,"A.1.4
ProtoNet: details and hyperparameters . . . . . . . . . . . . . . . . . . . .
19
653"
REFERENCES,0.6612127045235804,"A.1.5
IterRefLSTM: details and hyperparameters . . . . . . . . . . . . . . . . .
20
654"
REFERENCES,0.6621751684311838,"A.1.6
MHNfs: details and hyperparameters
. . . . . . . . . . . . . . . . . . . . .
21
655"
REFERENCES,0.6631376323387873,"A.1.7
PAR: details and hyperparameters . . . . . . . . . . . . . . . . . . . . . .
22
656"
REFERENCES,0.6641000962463908,"A.2
Domain shift experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
657"
REFERENCES,0.6650625601539942,"A.3
Details on the ablation study . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
658"
REFERENCES,0.6660250240615977,"A.3.1
Ablation study A: comparison against IterRefLSTM
. . . . . . . . . . . .
25
659"
REFERENCES,0.6669874879692012,"A.3.2
Ablation study B: all design elements . . . . . . . . . . . . . . . . . . . .
25
660"
REFERENCES,0.6679499518768046,"A.3.3
Ablation study C: Under domain shift on Tox21 . . . . . . . . . . . . . . .
26
661"
REFERENCES,0.6689124157844081,"A.4
Generalization to different support set sizes
. . . . . . . . . . . . . . . . . . . . .
26
662"
REFERENCES,0.6698748796920115,"A.5
Generalization to different context sets . . . . . . . . . . . . . . . . . . . . . . . .
26
663"
REFERENCES,0.670837343599615,"A.6
Details and insights on the context module . . . . . . . . . . . . . . . . . . . . . .
27
664"
REFERENCES,0.6717998075072185,"A.7
Reinforcing the covariance structure in the data using modern Hopﬁeld networks
.
27
665"
REFERENCES,0.6727622714148219,"A
Appendix
666"
REFERENCES,0.6737247353224254,"A.1
Details on methods
667"
REFERENCES,0.6746871992300288,"Few-shot learning methods in drug discovery can be described as models with adaptive parameters w
668"
REFERENCES,0.6756496631376323,"that use a support set Z = {(x1, y1), . . . , (xN, yN)} 1 as additional input to predict a label ˆy for a
669"
REFERENCES,0.6766121270452358,"molecule m
670"
REFERENCES,0.6775745909528392,"ˆy = gw(m, Z).
(A1)"
REFERENCES,0.6785370548604427,"Optimization-based methods, such as MAML (Finn et al., 2017), use the support set to update the
671"
REFERENCES,0.6794995187680462,"parameters w
672"
REFERENCES,0.6804619826756496,"ˆy = ga(w;Z)(m),
(A2)"
REFERENCES,0.6814244465832531,"where a(.) is a function that adapts w of g based on Z for example via gradient-descent.
673"
REFERENCES,0.6823869104908566,"Embedding-based methods use a different approach and learn representations of the support set
674"
REFERENCES,0.6833493743984601,"molecules {x1, . . . , xN}, sometimes written as stacked embeddings X ∈Rd×N, and the query
675"
REFERENCES,0.6843118383060636,"molecule m, and some function that associates these two types of information with each other. We
676"
REFERENCES,0.685274302213667,"describe the embedding-based methods Similarity Search in Section A.1.2, Neural Similarity Search
677"
REFERENCES,0.6862367661212705,"in Section A.1.3, ProtoNet in Section A.1.4, IterRefLSTM in Section A.1.5, PAR in Section A.1.7,
678"
REFERENCES,0.687199230028874,"and MHNfs in the main paper and details in Section A.1.6. The ""frequent hitters"" baseline is described
679"
REFERENCES,0.6881616939364774,"in Section A.1.1.
680"
REFERENCES,0.6891241578440809,"A.1.1
Frequent hitters: details and hyperparameters
681"
REFERENCES,0.6900866217516843,"The ""frequent hitters"" model gFH is a baseline that we implemented and included in the method
682"
REFERENCES,0.6910490856592878,"comparison. This method uses the usual training scheme of sampling a query molecule m with a
683"
REFERENCES,0.6920115495668913,"label y, having access to a support set Z. In contrast to the usual models of the type gw(m, Z), the
684"
REFERENCES,0.6929740134744947,"frequent hitters model gFH neglects the support set:
685"
REFERENCES,0.6939364773820982,"ˆy = gFH
w (m).
(A3)"
REFERENCES,0.6948989412897016,"Thus, during training for the same molecule m, the model might have to predict both y = 1 and
686"
REFERENCES,0.6958614051973051,"y = −1, since the molecule can be active in one task and inactive in another task. Therefore, the
687"
REFERENCES,0.6968238691049086,"1We use Z to denote the support set of already embedded molecules to keep the notation uncluttered.
More correctly, the methods have access to the raw support set Z = {(x1, y1), . . . , (xN, yN)}, where xn is a
symbolic, such as the molecular graph, or low-level representation of the molecule."
REFERENCES,0.697786333012512,"Table A1: Hyperparameter space considered for the Frequent
hitters model. The hyperparameters of the best conﬁguration are
marked bold."
REFERENCES,0.6987487969201155,"Hyperparameter
Explored values"
REFERENCES,0.699711260827719,"Number of hidden layers
1, 2, 4
Number of units per hidden layer
1024, 2048, 4096
Output dimension
512, 1024
Activation function
ReLU
Learning rate
0.0001, 0.001
Optimizer
Adam,AdamW
Weight decay
0, 0.01
Batch size
32, 128, 512, 2048, 4096
Input Dropout
0, 0.1
Dropout
0.1, 0.2, 0.3, 0.4, 0.5
Layer-normalization
False, True
• Afﬁne
False, True
Similarity function
dot product"
REFERENCES,0.7006737247353224,"model tends to predict average activity of a molecule to minimize the cross-entropy loss. We chose
688"
REFERENCES,0.7016361886429259,"an additive combination of the Morgan ﬁngerprints, RDKit ﬁngerprints, and MACCS keys for the
689"
REFERENCES,0.7025986525505293,"input representation to the MLP.
690"
REFERENCES,0.7035611164581328,"Hyperparameter search.
We performed manual hyperparameter search on the validation set and
691"
REFERENCES,0.7045235803657363,"report the explored hyperparameter space (Table A1). We use early-stopping based on validation
692"
REFERENCES,0.7054860442733397,"average-precision, a patience of 3 epochs and train for a maximum of 20 epochs with a linear warm-up
693"
REFERENCES,0.7064485081809432,"learning-rate schedule for the ﬁrst 3 epochs.
694"
REFERENCES,0.7074109720885466,"A.1.2
Classic similarity search: details and hyperparameters
695"
REFERENCES,0.7083734359961501,"Similarity Search (Cereto-Massagué et al., 2015) is a classic chemoinformatics technique used in
696"
REFERENCES,0.7093358999037536,"situations in which a single or few actives are known. In the simplest case, molecules that are similar to
697"
REFERENCES,0.710298363811357,"a given active molecule are searched by computing a ﬁngerprint or descriptor-representation f desc(m)
698"
REFERENCES,0.7112608277189605,"of the molecules and using a similarity measure k(., .), such as Tanimoto Similarity(Tanimoto, 1960).
699"
REFERENCES,0.7122232916265641,"Thus, the Similarity Search as used in chemoinformatics can be formally written as:
700"
REFERENCES,0.7131857555341675,"ˆy = 1/N N
X"
REFERENCES,0.714148219441771,"n=1
yn k(f desc(m), f desc(xn)),
(A4)"
REFERENCES,0.7151106833493744,"where the function f desc maps the molecule to its chemical descriptors or ﬁngerprints and takes
701"
REFERENCES,0.7160731472569779,"the role of both the molecule encoder and the support set encoder. The association function f assoc
702"
REFERENCES,0.7170356111645814,"consists of a) the similarity measure k(., .) and then b) mean pooling across molecules weighted by
703"
REFERENCES,0.7179980750721848,"their similarity and activity.
704"
REFERENCES,0.7189605389797883,"Notably, there are many variants of Similarity Search (Cereto-Massagué et al., 2015; Wang et al.,
705"
REFERENCES,0.7199230028873917,"2010; Eckert and Bajorath, 2007; Geppert et al., 2008; Willett, 2014; Sheridan and Kearsley, 2002;
706"
REFERENCES,0.7208854667949952,"Riniker and Landrum, 2013) of which some correspond to recent few-shot learning methods with a
707"
REFERENCES,0.7218479307025987,"ﬁxed molecule encoder. For example, (Geppert et al., 2008) suggest to use centroid molecules, i.e.,
708"
REFERENCES,0.7228103946102021,"prototypes or averages of active molecules. This is equivalent to the idea of Prototypical Networks
709"
REFERENCES,0.7237728585178056,"(Snell et al., 2017). Riniker and Landrum (2013) are aware of different fusion strategies for sets of
710"
REFERENCES,0.7247353224254091,"active or inactive molecules, which corresponds to different pooling strategies of the support set.
711"
REFERENCES,0.7256977863330125,"Overall, the variants of the classic Similarity Search are highly similar to embedding-based few-shot
712"
REFERENCES,0.726660250240616,"learning methods except that they have a ﬁxed instead of a learned molecule encoder.
713"
REFERENCES,0.7276227141482194,"Hyperparameter search.
For the Similarity Search, there were two decisions to make which was
714"
REFERENCES,0.7285851780558229,"ﬁrstly the similarity metric and secondly the question whether we should use a balancing strategy like
715"
REFERENCES,0.7295476419634264,"shown in Section 3.4. We decided for the dot-product as a similarity metric and using the balancing
716"
REFERENCES,0.7305101058710298,"strategy. These decisions were made by evaluating the models on the validation set.
717"
REFERENCES,0.7314725697786333,Figure A1: Schematic overview of the implemented Neural Similarity Search variant
REFERENCES,0.7324350336862367,"A.1.3
Neural Similarity Search or Siamese networks: details and hyperparameters
718"
REFERENCES,0.7333974975938402,"A lot of related work already was done (Koch et al., 2015; Hertz et al., 2006; Ye and Guo, 2018;
719"
REFERENCES,0.7343599615014437,"Torres et al., 2020). We adapted these ideas, such that a fully-connected deep neural network followed
720"
REFERENCES,0.7353224254090471,"by a Layer Normalization (Ba et al., 2016) operation, f ME
w
with adaptive parameters w, is used in a
721"
REFERENCES,0.7362848893166506,"Siamese fashion to compute the embeddings for the input molecule and the support set molecules.
722"
REFERENCES,0.737247353224254,"Within the association function block, pairwise similarity values for the input molecule and each
723"
REFERENCES,0.7382098171318575,"support set molecule are computed, associating both embeddings via the dot product. Based on these
724"
REFERENCES,0.739172281039461,"similarity values, the activity for the input molecule is predicted, building the weighted mean over
725"
REFERENCES,0.7401347449470644,"the support set molecule labels:
726"
REFERENCES,0.7410972088546679,ˆy = σ 
REFERENCES,0.7420596727622714,"τ −1 1 N N
X"
REFERENCES,0.7430221366698749,"n=1
y′
n f ME(m)T f ME(xn) !"
REFERENCES,0.7439846005774784,",
(A5)"
REFERENCES,0.7449470644850819,"where σ(.) is the sigmoid function and τ is a hyperparameter in the range of
√"
REFERENCES,0.7459095283926853,"d. Note that this
727"
REFERENCES,0.7468719923002888,"method uses a balancing strategy for the labels y′
n =
N/(2√NA)
if yn = 1
−N/(2√NI)
else
, where NA is the
728"
REFERENCES,0.7478344562078922,"number of actives and NI is the number of inactives of the support set. Figure A1 provides an
729"
REFERENCES,0.7487969201154957,"schematic overview of the Neural Similarity Search variant.
730"
REFERENCES,0.7497593840230992,"We trained the networks using the Adam optimizer (Kingma and Ba, 2014) to minimize binary
731"
REFERENCES,0.7507218479307026,"cross-entropy loss.
732"
REFERENCES,0.7516843118383061,"Hyperparameter search.
We performed manual hyperparameter search on the validation set. We
733"
REFERENCES,0.7526467757459095,"report the explored hyperparameter space (Table A2). Bold values indicate the selected hyperparame-
734"
REFERENCES,0.753609239653513,"ters for the ﬁnal model.
735"
REFERENCES,0.7545717035611165,"A.1.4
ProtoNet: details and hyperparameters
736"
REFERENCES,0.7555341674687199,"Prototypical Networks (ProtoNet) (Snell et al., 2017), learn a prototype r for each class. Concretely,
737"
REFERENCES,0.7564966313763234,"the support set Z is class-wise separated into Z+ := {(x, y) ∈Z | y = 1} and Z−:= {(x, y) ∈Z |
738"
REFERENCES,0.7574590952839269,"y = −1}. For the subsets Z+ and Z−prototypical representations r+ and r−can be computed by
739"
REFERENCES,0.7584215591915303,"r+ =
1
|Z+| ·
X"
REFERENCES,0.7593840230991338,"(x,y)∈Z+
f ME(x)
(A6)"
REFERENCES,0.7603464870067372,"and
740"
REFERENCES,0.7613089509143407,"r−=
1
|Z−| ·
X"
REFERENCES,0.7622714148219442,"(x,y)∈Z−
f ME(x).
(A7)"
REFERENCES,0.7632338787295476,"The prototypical representations r+, r−∈Rd and the query molecule embedding m ∈Rd are then
741"
REFERENCES,0.7641963426371511,"used to make the ﬁnal prediction:
742"
REFERENCES,0.7651588065447545,"ˆy =
exp(−d(m, r+))
exp(−d(m, r+)) + exp(−d(m, r−)),
(A8)"
REFERENCES,0.766121270452358,"Table A2: Hyperparameter space considered for the Neural Search model selection. The
hyperparameters of the best conﬁguration are marked bold."
REFERENCES,0.7670837343599615,"Hyperparameter
Explored values"
REFERENCES,0.7680461982675649,"Number of hidden layers
1, 2, 4
Number of units per hidden layer
1024, 4096
Output dimension
512, 1024
Activation function
ReLU, SELU
Learning rate
0.0001, 0.001, 0.01
Optimizer
Adam
Weight decay
0, 1 · 10−4
Batch size
4096
Input Dropout
0.1
Dropout
0.5
Layer-normalization
False, True
• Afﬁne
False
Similarity function
cosine similarity, dot product, MinMax similarity"
REFERENCES,0.7690086621751684,"where d is a distance metric.
743"
REFERENCES,0.7699711260827719,"Hyperparameter search.
Hyperparameter search has been done in Stanley et al. (2021), to which
744"
REFERENCES,0.7709335899903753,"we refer here. ECFP ﬁngerprints and descriptors created by a GNN, which operates on the molecular
745"
REFERENCES,0.7718960538979788,"graph, are fed into a fully connected neural network, which maps the input into an embedding space
746"
REFERENCES,0.7728585178055823,"with the dimension of 512. (Stanley et al., 2021) use the Mahalanobis distance to measure the
747"
REFERENCES,0.7738209817131858,"similarity between a query molecule and the prototypical representations in the embedding space.
748"
REFERENCES,0.7747834456207893,"The learning rate is 0.001 and the batch size is 256. The implementation can be found here https:
749"
REFERENCES,0.7757459095283927,"//github.com/microsoft/FS-Mol/blob/main/fs_mol/protonet_train.py and important
750"
REFERENCES,0.7767083734359962,"hyperparameters are chosen here https://github.com/microsoft/FS-Mol/blob/main/fs_
751"
REFERENCES,0.7776708373435997,"mol/utils/protonet_utils.py.
752"
REFERENCES,0.7786333012512031,"Connection to Siamese networks and contrastive learning with InfoNCE.
If instead of the neg-
753"
REFERENCES,0.7795957651588066,"ative distance −d(., .) the dot product similarity measure with appropriate scaling is used, ProtoNet
754"
REFERENCES,0.78055822906641,"for two classes becomes equivalent to Siamese Networks. Note that in our study, another differ-
755"
REFERENCES,0.7815206929740135,"ence is that ProtoNet uses a GNN as encoder, whereas Siamese Networks use a descriptor-based
756"
REFERENCES,0.782483156881617,"fully-connected network as encoder. In case of dot product as similarity measure, the objective also
757"
REFERENCES,0.7834456207892204,"becomes equivalent to contrastive learning with the InfoNCE objective (Oord et al., 2018).
758"
REFERENCES,0.7844080846968239,"A.1.5
IterRefLSTM: details and hyperparameters
759"
REFERENCES,0.7853705486044273,"(Altae-Tran et al., 2017) modiﬁed the idea of Matching Networks (Vinyals et al., 2016) by replacing
760"
REFERENCES,0.7863330125120308,"the LSTM with their Iterative Reﬁnement Long Short-Term Memory (IterRefLSTM). The use of the
761"
REFERENCES,0.7872954764196343,"IterRefLSTM empowers the architecutre to update not only the embeddings for the input molecule
762"
REFERENCES,0.7882579403272377,"but also adjust the representations of the support set molecules.
763"
REFERENCES,0.7892204042348412,"For IterRefLSTM, m = f ME
θ1 (m) and xn = f ME
θ2 (xn) are two potentially different molecule en-
764"
REFERENCES,0.7901828681424446,"coders for input molecule m and the support set molecules x1, . . . , xN. The next step in IterRefLSTM
765"
REFERENCES,0.7911453320500481,"is:
766"
REFERENCES,0.7921077959576516,"[m′, X′] = IterRefLSTML([m, X])."
REFERENCES,0.793070259865255,"Here, m′ and X′ contain the updated representations for the query molecule and the support
767"
REFERENCES,0.7940327237728585,"set molecules. The IterRefLSTM denotes the function which updates these representations. The
768"
REFERENCES,0.794995187680462,"main property of the IterRefLSTM module is that it is permutation-equivariant, thus a permu-
769"
REFERENCES,0.7959576515880654,"tation π(.) of the input elements results in the permutation of output elements: π([m′, X′]) =
770"
REFERENCES,0.7969201154956689,"IterRefLSTML(π([m, X])). The full architecture in invariant to permutations of the support set
771"
REFERENCES,0.7978825794032723,"elements. For details, we refer to (Altae-Tran et al., 2017). The hyperparameter L ∈N controls the
772"
REFERENCES,0.7988450433108758,"number of iteration steps of the IterRefLSTM.
773"
REFERENCES,0.7998075072184793,"Table A3: Hyperparameter space considered for the IterRef model selection. The hyperparam-
eters of the best conﬁguration are marked bold."
REFERENCES,0.8007699711260827,"Hyperparameter
Explored values"
REFERENCES,0.8017324350336862,"Molecule encoder
• Number of hidden layers
0, 1, 2, 4
• Number of units per hidden layer
1024, 4096
• Output dimension
512, 1024
• Activation function
ReLU, SELU
• Input dropout
0.1
• Dropout
0.5
IterRef embedding layer
• L
1, 3
Similarity module:
• Metric
cosine similarity, dot product, MinMax similarity
• Similarity space dimension
512, 1024
Layer-normalization
False, True
• Afﬁne
False, True
Training
• Learning rate
0.0001, 0.001, 0.01
• Optimizer
Adam, AdamW
• Weight decay
0, 0.0001
• Batch size
2048, 4096"
REFERENCES,0.8026948989412896,"As similarity module, the IterRefLSTM uses the following:
774"
REFERENCES,0.8036573628488932,"a = softmax (k (m′, X′)) ˆy = N
X"
REFERENCES,0.8046198267564967,"n=1
an yn,"
REFERENCES,0.8055822906641001,"where ˆy is the prediction for the query molecule. For the computation of the attention values a, the
775"
REFERENCES,0.8065447545717036,"softmax function is used. k is a similarity metric, such as the cosine similarity.
776"
REFERENCES,0.8075072184793071,"Hyperparameter search.
All hyperparameters were selected based on manual tuning on the
777"
REFERENCES,0.8084696823869105,"validation set. We report the explored hyperparameter space in Table A3. Bold values indicate the
778"
REFERENCES,0.809432146294514,"selected hyperparameters for the ﬁnal model.
779"
REFERENCES,0.8103946102021174,"A.1.6
MHNfs: details and hyperparameters
780"
REFERENCES,0.8113570741097209,"The MHNfs consists of a molecule encoder, the context module, the cross-attention-module, and the
781"
REFERENCES,0.8123195380173244,"similarity module. The molecule encoder is a fully-connected Neural Network, consisting of one
782"
REFERENCES,0.8132820019249278,"layer with 1024 units. For the context module, a Hopﬁeld layer with 8 heads is used and also the cross-
783"
REFERENCES,0.8142444658325313,"attention module include 8 heads. We chose a concatenation of ECFPs and RDKit-based descriptors
784"
REFERENCES,0.8152069297401348,"as the inputs for the MHNfs model. Notably, the RDKit-based descriptors were pre-processed in a
785"
REFERENCES,0.8161693936477382,"way that instead of raw values quantils, which were computed by comparing a raw value with the
786"
REFERENCES,0.8171318575553417,"distributation of all FS-Mol training molecules, were used. All descriptors were normalized based on
787"
REFERENCES,0.8180943214629451,"the FS-Mol training data.
788"
REFERENCES,0.8190567853705486,"Hyperparameter search.
All hyperparameters were selected based on manual tuning on the
789"
REFERENCES,0.8200192492781521,"validation set. We report the explored hyperparameter space in Table A4. Bold values indicate the
790"
REFERENCES,0.8209817131857555,"selected hyperparameters for the ﬁnal model. Early stopping points for the different re-runs are
791"
REFERENCES,0.821944177093359,"chosen based on the ∆AUC-PR metric on the validation set. For the ﬁve re-runs the early-stopping
792"
REFERENCES,0.8229066410009624,"points, that were automatically chosen by their validation metrics, were the checkpoints at epoch 94,
793"
REFERENCES,0.8238691049085659,"192, 253, 253 and 309.
794"
REFERENCES,0.8248315688161694,"Model training.
Figure A2 shows the learning curve of an exemplary training run of a MHNfs
795"
REFERENCES,0.8257940327237728,"model on FS-Mol. The left plot shows the loss on the training set and the right plot shows the
796"
REFERENCES,0.8267564966313763,"Table A4: Hyperparameter space considered for the MHNfs model selection. The hyperparameters of the
best conﬁguration are marked bold."
REFERENCES,0.8277189605389798,"Hyperparameter
Explored values"
REFERENCES,0.8286814244465832,"Molecule encoder
• Number of hidden layers
0, 1, 2, 4
• Number of units per hidden layer
1024, 4096
• Output dimension
512, 1024
• Activation function
ReLU, SELU
• Input dropout
0.1
• Dropout
0.5
Context module (hopﬁeld layer)
• Heads
8, 16
• Association space dimension
512 [512;2048]
• τ
22.6 [15;40]
• Dropout
0.1, 0.5
Cross-attention module (transformer mechanism)
• Heads
1, 8, 10, 16, 32, 64
• Number units in the hidden feedforward layer
567 [512; 4096]
• Association space dimension
1088 [512;2048]
• Dropout
0.1, 0.5, 0.6, 0.7
• Number of layers:
1, 2, 3
Similarity module:
• Metric
cosine similarity, dot product, MinMax similarity
• Similarity space dimension
512, 1024
Layer-normalization
False, True
• Afﬁne
False, True
Training
• Learning rate
0.0001, 0.001, 0.01
• Optimizer
Adam, AdamW
• Weight decay
0, 0.0001
• Batch size
4096
• Warm-up phase (epochs)
5
• Constant learning rate phase (epochs)
25, 35
• Decay rate
0.994
• Max. number of epochs
350"
REFERENCES,0.8296438883541867,"validation loss. The dashed line indicates the checkpoint of the model which was saved and then used
797"
REFERENCES,0.8306063522617901,"for inference on the test set, whereas the stopping point was evaluated maximizing the ∆AUC-PR
798"
REFERENCES,0.8315688161693936,"metric on the validation set.
799"
REFERENCES,0.8325312800769971,"Performance improvements in comparison to a naive baseline.
Figure A3 shows a task-wise
800"
REFERENCES,0.8334937439846006,"performance comparison between MHNfs and the frequent hitter model. Each point indicates a task
801"
REFERENCES,0.8344562078922041,"in the test set and is colored according to their super-class membership. In 132 cases the MHNfs
802"
REFERENCES,0.8354186717998076,"outperforms the frequent hitter model. In 25 cases the frequent hitter model yields better performance.
803"
REFERENCES,0.836381135707411,"A.1.7
PAR: details and hyperparameters
804"
REFERENCES,0.8373435996150145,"The PAR model (Wang et al., 2021) includes a pre-trained GNN encoder, which creates initial
805"
REFERENCES,0.8383060635226179,"embeddings for the query and support set molecules. These embeddings are fed into an attention
806"
REFERENCES,0.8392685274302214,"mechanism module which also uses activity information of the support set molecules to create
807"
REFERENCES,0.8402309913378249,"enriched representations. Another GNN learns relations between query and support set molecules.
808"
REFERENCES,0.8411934552454283,"Hyperparameter search.
For details we refer to (Wang et al., 2021) and https://github.com/
809"
REFERENCES,0.8421559191530318,"tata1661/PAR-NeurIPS21/blob/main/parser.py. All hyperparameters were selected based
810"
REFERENCES,0.8431183830606352,"on manual tuning on the validation set. The hyperparameter choice for Tox21 (Wang et al., 2021)
811"
REFERENCES,0.8440808469682387,"was used as a starting point. We report the explored hyperparameter space in Table A5. Bold values
812"
REFERENCES,0.8450433108758422,"0
50
100
150
200
250
300
350
Epoch 0.10 0.12 0.14 0.16 0.18 0.20 0.22"
REFERENCES,0.8460057747834456,Loss on training set
REFERENCES,0.8469682386910491,"0
50
100
150
200
250
300
350
Epoch 0.7 0.8 0.9 1.0 1.1 1.2"
REFERENCES,0.8479307025986526,Loss on validation set
REFERENCES,0.848893166506256,"Figure A2: Exemplary MHNfs learning curve on FS-Mol. On the x-axis the number of epochs
is displayed and on the y-axis thee training loss (left) and the validation loss (right) The dashed
line indicates the determined early-stopping point which is determined based on ∆AUC-PR on the
validation set."
REFERENCES,0.8498556304138595,"0.1
0.0
0.1
0.2
0.3
0.4
0.5
Frequent hitter [ AUC-PR] 0.1 0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.8508180943214629,MHNfs [ AUC-PR]
REFERENCES,0.8517805582290664,"Oxidoreductase
Kinase
Hydrolase
Lyase
Isomerase
Ligase
Translocase"
REFERENCES,0.8527430221366699,"Figure A3: Performance comparison of MHNfs with the frequent hitter model. Each point refers to a
task in the test set. Dashed lines indicate variablility across training re-runs and different test support
sets. The most points are located above the dashed line, which indicates that MHNfs performs better
than den FH baseline at this task."
REFERENCES,0.8537054860442733,"Table A5: Hyperparameter space considered for the PAR model selection. The hyperparameters of the
best conﬁguration are marked bold."
REFERENCES,0.8546679499518768,"Hyperparameter
Explored values"
REFERENCES,0.8556304138594802,"Training
• Meta learning rate
1.0 · 10−05, 1.0 · 10−04, 1.0 · 10−03, 1.0 · 10−02
• Inner learning rate
0.01, 0.1
• Update step
1, 2
• Update step test
1, 2
• Weight decay
5.0 · 10−05, 1.0 · 10−03
• Epochs
200000
• Eval. steps
2000
Encoder
• Use pre-trained GNN
yes, no
Attention-based module
• Map dimension
128, 512
• Map layer
2, 3
• Pre fc layer
0, 2
• Map dropout
0.1, 0.5
• Context layer
2, 3, 4
Relation graph
• Hidden dimension
8, 128, 512
• Number of layers
2, 4
• Number of layers for relation edge update
2, 3
• Batch norm
yes, no
• Relation dropout 1
0, 0.25, 0.5
• Relation dropout 2
0.2, 0.25, 0.5"
REFERENCES,0.8565928777670837,"indicate the selected hyperparameters for the ﬁnal model. Notably, we just report hyperparameter
813"
REFERENCES,0.8575553416746872,"choices which were different from standard choices. We used a training script provided by (Wang
814"
REFERENCES,0.8585178055822906,"et al., 2021), which can be found here https://github.com/tata1661/PAR-NeurIPS21.
815"
REFERENCES,0.8594802694898941,"A.2
Domain shift experiment
816"
REFERENCES,0.8604427333974976,"Experimental setup. For the domain shift experiment, we used the Tox21 dataset. This dataset
817"
REFERENCES,0.861405197305101,"consists of 12,707 chemical compounds, for which measurements for up to 12 different toxic effects
818"
REFERENCES,0.8623676612127045,"are reported (Mayr et al., 2016; Huang et al., 2016a). It was published with a ﬁxed training, validation
819"
REFERENCES,0.8633301251203079,"and test split. State-of-the-art supervised learning methods that have access to the full training set
820"
REFERENCES,0.8642925890279115,"reach AUC performance values between 0.845 and 0.871 (Klambauer et al., 2017; Duvenaud et al.,
821"
REFERENCES,0.865255052935515,"2015; Li et al., 2017, 2021; Zaslavskiy et al., 2019; Alperstein et al., 2019). For our evaluation, we
822"
REFERENCES,0.8662175168431184,"re-cast Tox21 as a few-shot learning setting and draw small support sets from the 12 tasks. The
823"
REFERENCES,0.8671799807507219,"compared methods were pre-trained on FS-Mol and obtain small support sets from Tox21. Based
824"
REFERENCES,0.8681424446583254,"on the support sets, the methods had to predict the activities of the Tox21 test set. Note that there is
825"
REFERENCES,0.8691049085659288,"a strong domain shift from drug-like molecules of FS-Mol to environmental chemicals, pesticides,
826"
REFERENCES,0.8700673724735323,"food additives of Tox21. The domain shift also concerns the outputs where a shift from kinases,
827"
REFERENCES,0.8710298363811357,"hydrolases, and oxidoreductases of FS-Mol to nuclear receptors and stress responses of Tox21 is
828"
REFERENCES,0.8719923002887392,"present.
829"
REFERENCES,0.8729547641963427,"Methods compared. We compared the new method MHNfs, the runner-up method IterRefLSTM,
830"
REFERENCES,0.8739172281039461,"and Similarity Search — since it has been widely used for such purposes for decades (Cereto-
831"
REFERENCES,0.8748796920115496,"Massagué et al., 2015).
832"
REFERENCES,0.875842155919153,"Training and evaluation. We followed the procedure of Stanley et al. (2021) for data-cleaning,
833"
REFERENCES,0.8768046198267565,"preprocessing and extraction of the ﬁngerprints and descriptors used in FS-Mol. After running the
834"
REFERENCES,0.87776708373436,"cleanup step, 8,423 molecules remained for the domain shift experiments. From the training set, 8
835"
REFERENCES,0.8787295476419634,"active and 8 inactive molecules per task were randomly selected to build the support set. The test set
836"
REFERENCES,0.8796920115495669,"molecules were used as query molecules. The validation set molecules were not used at all. During
837"
REFERENCES,0.8806544754571703,"test-time, a support set was drawn ten times for each task. Then, the performance of the models were
838"
REFERENCES,0.8816169393647738,"Table A6: Results of the domain shift experiment on Tox21 [AUC,
∆AUC-PR]. The best method is marked bold. Error bars represent
standard deviation across training re-runs and draws of support sets"
REFERENCES,0.8825794032723773,"Method
AUC
∆AUC-PR"
REFERENCES,0.8835418671799807,"Similarity Search (baseline)
.629 ± .015
.061 ± .008
IterRefLSTM (Altae-Tran et al., 2017)
.664 ± .018
.067 ± .008
MHNfs (ours)
.679 ± .018
.073 ± .008"
REFERENCES,0.8845043310875842,"Table A7: Results of the ablation study on FS-Mol [AUC, ∆AUC-PR ]. The error bars represent
standard deviation across training re-runs and draws of support sets. The p-values indicate whether
the difference between two models in consecutive rows is signiﬁcant."
REFERENCES,0.8854667949951877,"Method
AUC
∆AUC-PR
pAUCa
p∆AUC−PRa"
REFERENCES,0.8864292589027911,"MHNfs (CM+CAM+SM)
.739 ± .005
.241 ± .006
MHNfs -CM
.737 ± .004
.240 ± .005
0.030
0.002
MHNfs -CM -CAM
.719 ± .006
.223 ± .006
< 1.0e-8
<1.0e-8
Similarity Search
.604 ± .003
.113 ± .004
<1.0e-8
< 1.0e-8
IterRefLSTM (Altae-Tran et al., 2017)b
.730 ± .005
.234 ± .005
<1.0e-8
8.73e-7"
REFERENCES,0.8873917228103946,"a paired Wilcoxon rank sum test
b IterRefLSTM is compared to MHNfs -CM"
REFERENCES,0.888354186717998,"evaluated for these support sets, using the area under precision-recall curve (AUC-PR), analogously to
839"
REFERENCES,0.8893166506256015,"the FS-Mol benchmarking experiment reported as the difference to a random classiﬁer (∆AUC-PR),
840"
REFERENCES,0.890279114533205,"and the area under receiver operating characteristic curve (AUC) metrics. The performance values
841"
REFERENCES,0.8912415784408084,"report the mean over all combinations regarding the training reruns and the support set sampling
842"
REFERENCES,0.8922040423484119,"iterations. Error bars indicate the standard deviation.
843"
REFERENCES,0.8931665062560153,"Results.
The Hopﬁeld-based context retrieval method has signiﬁcantly outperformed the
844"
REFERENCES,0.8941289701636189,"IterRefLSTM-based model (p∆AUC−PR-value 3.4e−5, pAUC-value 2.5e-6, paired Wilcoxon test)
845"
REFERENCES,0.8950914340712224,"and the Classic Similarity Search (p∆AUC−PR-value 2.4e-9, pAUC-value 7.6e-10, paired Wilcoxon
846"
REFERENCES,0.8960538979788258,"test) and therefore showed robust performance on the toxicity domain, see Table A6. Notably, all
847"
REFERENCES,0.8970163618864293,"models were trained on the FS-Mol dataset and then applied to the Tox21 dataset without adjusting
848"
REFERENCES,0.8979788257940328,"any weight parameter.
849"
REFERENCES,0.8989412897016362,"A.3
Details on the ablation study
850"
REFERENCES,0.8999037536092397,"The MHNfs has two new main elements compared to the previous state-of-the art method Iter-
851"
REFERENCES,0.9008662175168431,"RefLSTM, which are the context module and the cross-attention-module. In this ablation study
852"
REFERENCES,0.9018286814244466,"we aim to investigate i) the importance of all design elements, which are the context module, the
853"
REFERENCES,0.9027911453320501,"cross-attention module, and the similarity module, and ii) the superiority of the cross-attention module
854"
REFERENCES,0.9037536092396535,"compared to the IterRefLSTM module.
855"
REFERENCES,0.904716073147257,"A.3.1
Ablation study A: comparison against IterRefLSTM
856"
REFERENCES,0.9056785370548605,"For a fair comparison between the cross-attention module and the IterRefLSTM we used a pruned
857"
REFERENCES,0.9066410009624639,"MHN version (""MHNfs -CM"") which has no context module and compared it with the IterRefLSTM
858"
REFERENCES,0.9076034648700674,"model. The evaluation includes ﬁve training re-runs each and ten different support set samplings.
859"
REFERENCES,0.9085659287776708,"The results, reported as the mean across training re-runs and support sets, can be seen in Table A7.
860"
REFERENCES,0.9095283926852743,"We performed a paired Wilcoxon rank sum test for both the AUC and the ∆AUC-PR metric. Both
861"
REFERENCES,0.9104908565928778,"p-values indicate high signiﬁcance.
862"
REFERENCES,0.9114533205004812,"A.3.2
Ablation study B: all design elements
863"
REFERENCES,0.9124157844080847,"We evaluate the performance of all main elements within the MHNfs, which are the context module,
864"
REFERENCES,0.9133782483156881,"the cross-attention module, the similarity module and the molecule encoder. For this analysis,
865"
REFERENCES,0.9143407122232916,"we start with the complete MHNfs which includes all modules and report AUC and ∆AUC-PR
866"
REFERENCES,0.9153031761308951,"performance values. Then, we iteratively omit the individual modules, measuring whether there is a
867"
REFERENCES,0.9162656400384985,"Table A8: Results of the ablation study on Tox21 [AUC, ∆AUC-PR ]. The error bars
represent standard deviation across training re-runs and draws of support sets. The
p-values indicate whether a model is signiﬁcantly different to the MHNfs in terms of
the AUC and ∆AUC-PR metric."
REFERENCES,0.917228103946102,"Method
AUC
∆AUC-PR
pAUCa
p∆AUC−PRa"
REFERENCES,0.9181905678537055,"MHNfs (CM+CAM+SM)
.679 ± .018
.073 ± .008
MHNfs -CM
.662 ± .028
.069 ± .012
6.28e-8
0.002
MHNfs -CM -CAM
.640 ± .018
.057 ± .009
<1.0e-8
<1.0e-8
Similarity Search
.629 ± .015
.061 ± .008
<1.0e-8
<1.0e-8
IterRefLSTM
.664 ± .018
.067 ± .008
2.53e-6
3.38e-5"
REFERENCES,0.9191530317613089,a paired Wilcoxon rank sum test
REFERENCES,0.9201154956689124,"signiﬁcant performance difference with and without the module. Table A7 shows the results, where
868"
REFERENCES,0.9210779595765158,"performance values for the full MHNfs, a MHNfs model without the context module (""MHNfs -CM"")
869"
REFERENCES,0.9220404234841193,"and a MHNfs module without the context and the cross-attenion module (""MHNfs -CM -CAM"") is
870"
REFERENCES,0.9230028873917228,"included. Notably, the model without the context module and without the cross-attention module
871"
REFERENCES,0.9239653512993262,"just consists of a learned molecule encoder and the similarity module. We evaluted the impact of
872"
REFERENCES,0.9249278152069298,"the learned molecule encoder by replacing it with a ﬁxed encoder, which maps a molecule to its
873"
REFERENCES,0.9258902791145333,"descriptors. The model with the ﬁxed encoder is a classic chemoinformatics method which is called
874"
REFERENCES,0.9268527430221367,"Similarity Search (Cereto-Massagué et al., 2015).
875"
REFERENCES,0.9278152069297402,"For the evaluation, we performed ﬁve training re-runs for every model and sampled ten different
876"
REFERENCES,0.9287776708373436,"support sets for every task. Table A7 shows the results in terms of AUC and ∆AUC-PR. We performed
877"
REFERENCES,0.9297401347449471,"paired Wilcoxon rank sum tests on both metrics, comparing two methods in consecutive rows in the
878"
REFERENCES,0.9307025986525506,"table. The table shows that every module has a signiﬁcant impact as omitting a module results in
879"
REFERENCES,0.931665062560154,"a signiﬁcant performance drop. The comparison between the MHNfs version without the context
880"
REFERENCES,0.9326275264677575,"module and without the cross-attention module with the Similarity Search showed a signiﬁcant
881"
REFERENCES,0.933589990375361,"superiority of the learned molecule encoder in comparison to the ﬁxed encoder.
882"
REFERENCES,0.9345524542829644,"A.3.3
Ablation study C: Under domain shift on Tox21
883"
REFERENCES,0.9355149181905679,"Referring to Section A.3.2, the context module and the cross-attention module showed their impor-
884"
REFERENCES,0.9364773820981713,"tance for the global architecture. This importance gets even more pronounced for the domain shift
885"
REFERENCES,0.9374398460057748,"experiment on Tox21 as one can see in Table A8.
886"
REFERENCES,0.9384023099133783,"Again, ﬁve training re-runs and ten support set draws are used for evaluation. Including the context
887"
REFERENCES,0.9393647738209817,"module makes a clear and signiﬁcant difference for both metrics AUC and ∆AUC-PR.
888"
REFERENCES,0.9403272377285852,"A.4
Generalization to different support set sizes
889"
REFERENCES,0.9412897016361886,"In this section, we test the ability of MHNfs to generalize to different support set sizes. During
890"
REFERENCES,0.9422521655437921,"training in the FS-Mol benchmarking setting, the MHNfs model has access to support sets of size
891"
REFERENCES,0.9432146294513956,"16. However, at inference, the support set size might be different. Figure A4 provides performance
892"
REFERENCES,0.944177093358999,"estimates of the support-set-size-16 MHNfs models on other support set sizes. Note that the estimates
893"
REFERENCES,0.9451395572666025,"could be seen as approximate lower bounds of the predictive performance on settings with different
894"
REFERENCES,0.9461020211742059,"support set sizes (y-axis labels). For a model used in production or in a real-world drug discovery
895"
REFERENCES,0.9470644850818094,"setting, MHNfs should be trained with varying support set sizes that resemble the distribution of real
896"
REFERENCES,0.9480269489894129,"drug discovery projects.
897"
REFERENCES,0.9489894128970163,"A.5
Generalization to different context sets
898"
REFERENCES,0.9499518768046198,"In this section, we test the ability of MHNfs to generalize to different context sets. While the FS-Mol
899"
REFERENCES,0.9509143407122232,"training split is used as a context during training, we assessed whether our model is robust to different
900"
REFERENCES,0.9518768046198267,"context sets for inference. To this end we preprocessed the GEOM dataset (Axelrod and Gomez-
901"
REFERENCES,0.9528392685274302,"Bombarelli, 2022) from which we used 100,000 molecules that passed all pre-processing checks.
902"
REFERENCES,0.9538017324350336,"From this set, we sample 10,000 molecules as context set for MHNfs. Because GEOM contains
903"
REFERENCES,0.9547641963426372,"drug-like molecules, similar to FS-Mol the predictive performance remains stable (see Table A9).
904"
REFERENCES,0.9557266602502407,"2
4
6
8
10
12
14 16 18 20
32
64
128
Support set size (inference time) 0.14 0.16 0.18 0.20 0.22 0.24 0.26 0.28"
REFERENCES,0.9566891241578441,AUC-PR (test set)
REFERENCES,0.9576515880654476,"Figure A4: Performance of MHNfs for different support set sizes during inference time. The MHNfs
models are trained with support sets of the size 16."
REFERENCES,0.958614051973051,"Table A9: MHNfs performance for different context sets [∆AUC-PR
]. The error bars represent standard deviation across training re-runs
and draws of support sets."
REFERENCES,0.9595765158806545,"Dataset used as a context
∆AUC-PR"
REFERENCES,0.960538979788258,"FS-Mol (Stanley et al., 2021)
.2414 ± .006
GEOM (Axelrod and Gomez-Bombarelli, 2022)
.2415 ± .005"
REFERENCES,0.9615014436958614,"A.6
Details and insights on the context module
905"
REFERENCES,0.9624639076034649,"The context module replaces the initial representations of query and support set molecules by a
906"
REFERENCES,0.9634263715110684,"retrieval from the context set. The context set is a large set of molecules and covers a large chemical
907"
REFERENCES,0.9643888354186718,"space. The context module learns how to replace the initial molecule embeddings such that the
908"
REFERENCES,0.9653512993262753,"context-enriched representations are put in relation to this large chemical space and still contains
909"
REFERENCES,0.9663137632338787,"all necessary information for the similarity-based prediction part. Figure A5 shows the effect of the
910"
REFERENCES,0.9672762271414822,"context module for the MHNfs model. Extreme initial embeddings, such as the purple embedding
911"
REFERENCES,0.9682386910490857,"on the right, are pulled more into the known chemical space, represented by the context molecules.
912"
REFERENCES,0.9692011549566891,"Notably, the replacement described above is a soft replacement, because also the initial embeddings
913"
REFERENCES,0.9701636188642926,"contribute to the context-enriched representations due to skip-connections.
914"
REFERENCES,0.971126082771896,"A.7
Reinforcing the covariance structure in the data using modern Hopﬁeld networks
915"
REFERENCES,0.9720885466794995,"We follow the argumentation of (Fürst et al., 2021, Theorem A3) that retrieval from an associative
916"
REFERENCES,0.973051010587103,"memory of a MHN reinforces the covariance structure.
917"
REFERENCES,0.9740134744947064,"Let us assume that we have one molecule embedding from the query set m ∈Rd and one molecule
918"
REFERENCES,0.9749759384023099,"embedding from the support set x ∈Rd and both have been enriched with the context module with
919"
REFERENCES,0.9759384023099134,"memory C ∈Rd×M(ignoring linear mappings):
920"
REFERENCES,0.9769008662175168,"m′ = C softmax(βCT m)
(A9)"
REFERENCES,0.9778633301251203,"x′ = C softmax(βCT x)
(A10)"
REFERENCES,0.9788257940327237,"Then the similarity of the retrieved representations as measured by the dot product can be expressed
921"
REFERENCES,0.9797882579403272,"in terms of covariances:
922"
REFERENCES,0.9807507218479307,"20
10
0
10
20
pc0 20 15 10 5 0 5 10 15 20 pc1"
REFERENCES,0.9817131857555341,"Context
Initial Embedding
Context-enriched Representation"
REFERENCES,0.9826756496631376,"Figure A5: PCA plot of molecule embeddings. Each dot in the plot represents a molecule embedding,
of which the ﬁrst two principal components are displayed on the x- and y-axis. Blue dots represent
context molecules. Dark purple dots represent initial embeddings for some exemplary molecules,
of which some exhibit extreme characteristics and are thus located away from the center. Arrows
and light purple dots represent the enriched molecule embeddings after the retrieval step. Especially
molecules from extreme positions are moved stronger to the center and thus are more similar to
known molecules after retrieval."
REFERENCES,0.983638113570741,"m′T x′ = softmax(βCT m)T CT Csoftmax(βCT x) =
(A11)"
REFERENCES,0.9846005774783445,"= (c + Cov(C, m)T m)T (c + Cov(C, x)x),
(A12)"
REFERENCES,0.9855630413859481,"where c is the row mean of C and following the weighted covariances are used:
923"
REFERENCES,0.9865255052935515,"Cov(C, m) = CJm(βCm)CT
Cov(C, x) = CJm(βCx)CT .
(A13)"
REFERENCES,0.987487969201155,"Jm : RM 7→RM×M is a mean Jacobian function of the softmax (Fürst et al., 2021, Eq.(A172)).
924"
REFERENCES,0.9884504331087585,"The Jacobian J of p = softmax(βa) is J(βa) = β
 
diag(p) −ppT 
.
925"
REFERENCES,0.9894128970163619,"bT J(βa) b = β bT  
diag(p) −p pT 
b = β  X"
REFERENCES,0.9903753609239654,"i
pi b2
i − X"
REFERENCES,0.9913378248315688,"i
pi bi !2"
REFERENCES,0.9923002887391723,",
(A14)"
REFERENCES,0.9932627526467758,"this is the second moment minus the mean squared, which is the variance. Therefore, bT J(βa)b is β
926"
REFERENCES,0.9942252165543792,"times the covariance of b if component i is drawn with probability pi of the multinomial distribution
927"
REFERENCES,0.9951876804619827,"p. In our case the component i is context sample ci. Jm is the average of J(λa) over λ = 0 to λ = β.
928"
REFERENCES,0.9961501443695862,"Note that we can express the enriched representations using these covariance functions:
929"
REFERENCES,0.9971126082771896,"m′ = (c + Cov(C, m)T m)
(A15)"
REFERENCES,0.9980750721847931,"x′ = (c + Cov(C, x)T x),
(A16)"
REFERENCES,0.9990375360923965,"which connects retrieval from MHNs with reinforcing the covariance structure of the data.
930"
