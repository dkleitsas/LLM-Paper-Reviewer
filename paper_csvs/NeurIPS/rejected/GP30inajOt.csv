Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009718172983479105,"Optimization over the Stiefel manifold has played a significant role in various
1"
ABSTRACT,0.001943634596695821,"machine learning tasks. Many existing algorithms either use the retraction operator
2"
ABSTRACT,0.0029154518950437317,"to keep each iterate staying on the manifold, or solve an unconstrained quadratic
3"
ABSTRACT,0.003887269193391642,"penalized problem. The retraction operator in the former corresponds to orthonor-
4"
ABSTRACT,0.004859086491739553,"malization of matrices and can be computationally costly for large-scale matrices.
5"
ABSTRACT,0.0058309037900874635,"The latter approach usually equips with an unknown large penalty parameter. To
6"
ABSTRACT,0.006802721088435374,"address the above issues, we propose a retraction-free and penalty parameter-free
7"
ABSTRACT,0.007774538386783284,"algorithm, which lands on the manifold. A key component of the analysis is the
8"
ABSTRACT,0.008746355685131196,"convex-like property of the quadratic penalty of the Stiefel manifold, which enables
9"
ABSTRACT,0.009718172983479106,"us to explicitly characterize the penalty parameter. As an application, we introduce
10"
ABSTRACT,0.010689990281827016,"a new algorithm, Manifold-LoRA, which employs the landing technique and a
11"
ABSTRACT,0.011661807580174927,"carefully designed step size strategy to accelerate low-rank adaptation (LoRA)
12"
ABSTRACT,0.012633624878522837,"in fine-tuning large language models. Numerical experiments on the benchmark
13"
ABSTRACT,0.013605442176870748,"datasets demonstrate the efficiency of our proposed method.
14"
INTRODUCTION,0.014577259475218658,"1
Introduction
15"
INTRODUCTION,0.015549076773566569,"Optimization over the Stiefel manifold has attracted considerable attention in the context of machine
16"
INTRODUCTION,0.01652089407191448,"learning, e.g., RNN [3], batch normalization [10], and distributionally robust optimization [8]. The
17"
INTRODUCTION,0.01749271137026239,"mathematical formulation of this class of problems is:
18"
INTRODUCTION,0.0184645286686103,"min
X∈Rd×r f(X) subject to
X ∈St(d, r) := {X ∈Rd×r : X⊤X = Id},
(1)"
INTRODUCTION,0.019436345966958212,"where r ≤d and f : Rd×r →R is a continuously differentiable function. The most popular methods
19"
INTRODUCTION,0.02040816326530612,"for solving (1) are retraction-based algorithms, which have been extensively studied in the context
20"
INTRODUCTION,0.021379980563654033,"of manifold optimization [2, 23, 6]. Recently, to alleviate the possible computational burden of the
21"
INTRODUCTION,0.022351797862001945,"retraction operator, some retraction-free methods have been developed in [19, 18, 41, 1]. The ideas
22"
INTRODUCTION,0.023323615160349854,"in these papers are based on a combination of the manifold geometry and a penalty function for the
23"
INTRODUCTION,0.024295432458697766,"manifold constraint, which involves an unknown but sufficiently large penalty parameter. For large-
24"
INTRODUCTION,0.025267249757045675,"scale machine learning applications, retraction-free algorithms are preferred. However, designing
25"
INTRODUCTION,0.026239067055393587,"retraction-free algorithms with a known penalty parameter for solving (1) remains a challenge.
26"
INTRODUCTION,0.027210884353741496,"Another motivation for studying retraction-free methods arises from its application in the fine-tuning
27"
INTRODUCTION,0.028182701652089408,"of large language models (LLMs). Recently, LLMs have revolutionized the field of natural language
28"
INTRODUCTION,0.029154518950437316,"processing (NLP), achieving unprecedented performance across various applications [33, 32]. To
29"
INTRODUCTION,0.03012633624878523,"tailor pretrained LLMs for specific downstream tasks, the most common approach is full fine-tuning,
30"
INTRODUCTION,0.031098153547133137,"which requires prohibitively large computational resources due to the need to adapt all model weights,
31"
INTRODUCTION,0.03206997084548105,"hindering the deployment of large models. As a result, parameter-efficient fine-tuning (PEFT) has
32"
INTRODUCTION,0.03304178814382896,"gained widespread attention for requiring few trainable parameters while delivering comparable
33"
INTRODUCTION,0.034013605442176874,"or even superior results to full fine-tuning. This paradigm involves inserting learnable modules or
34"
INTRODUCTION,0.03498542274052478,"designating only a small portion of weights as trainable, keeping the main model frozen [21, 26, 44].
35"
INTRODUCTION,0.03595724003887269,"Among fine-tuning methods, low-rank adaptation (LoRA) [22] has become the de facto standard
36"
INTRODUCTION,0.0369290573372206,"among parameter-efficient fine-tuning techniques. It assumes that the change in weights lies in a
37"
INTRODUCTION,0.037900874635568516,"“low intrinsic dimension”, thereby modelling the update ∆W ∈Rd×m by two low-rank (not greater
38"
INTRODUCTION,0.038872691933916424,"than a small integer r) matrices A ∈Rr×m and B ∈Rd×r, i.e., ∆W = BA. Since r ≪d, the
39"
INTRODUCTION,0.03984450923226433,"requirements on both storage and computation are significantly reduced. Due to its decompositional
40"
INTRODUCTION,0.04081632653061224,"nature, there is redundancy in the representation of ∆W. Traditional optimization methods for LoRA
41"
INTRODUCTION,0.04178814382896016,"do not exploit this redundancy, which consequently undermines model performance. Instead, we
42"
INTRODUCTION,0.042759961127308066,"reformulate LoRA fine-tuning as an optimization problem over the product of Stiefel manifolds
43"
INTRODUCTION,0.043731778425655975,"and Euclidean spaces. Therefore, we propose an algorithmic framework called Manifold-LoRA to
44"
INTRODUCTION,0.04470359572400389,"accelerate the fine-tuning process and enhance model performance. Moreover, by exploiting projected
45"
INTRODUCTION,0.0456754130223518,"gradients and incorporating a parameter-free penalty, the overhead that our method incurs is relatively
46"
INTRODUCTION,0.04664723032069971,"negligible. Our contributions are as follows:
47"
INTRODUCTION,0.047619047619047616,"• We first prove the existence of explicit choice for the penalty parameter by establishing a
48"
INTRODUCTION,0.04859086491739553,"strong convexity-like condition of the nonconvex penalty problem associated with the Stiefel
49"
INTRODUCTION,0.04956268221574344,"manifold constraint. Furthermore, for the given penalty parameter, under mild conditions,
50"
INTRODUCTION,0.05053449951409135,"we prove that the iterates of our proposed retraction-free gradient descent method eventually
51"
INTRODUCTION,0.05150631681243926,"land on the Stiefel manifold and achieve the optimality of (1).
52"
INTRODUCTION,0.052478134110787174,"• Building upon the established landing theory of retraction-free and penalty parameter-free
53"
INTRODUCTION,0.05344995140913508,"method and the AdamW framework, we proposed a new method, Manifold-LoRA, which
54"
INTRODUCTION,0.05442176870748299,"employs a carefully designed step size strategy to accelerate the training process of fine-
55"
INTRODUCTION,0.05539358600583091,"tuning. Compared with the conventional AdamW method, we use the penalized gradient
56"
INTRODUCTION,0.056365403304178816,"instead of the usual gradient, and the computational overhead is negligible.
57"
INTRODUCTION,0.057337220602526724,"• Numerical experiments are conducted on a wide range of NLP tasks, demonstrating the
58"
INTRODUCTION,0.05830903790087463,"efficiency of our algorithm. Specifically, compared to the vanilla LoRA, our Manifold-LoRA
59"
INTRODUCTION,0.05928085519922255,"with half the trainable parameters not only delivers fast convergence but also yields improved
60"
INTRODUCTION,0.06025267249757046,"generalization. In particular, Our method converges twice as fast as baseline methods on
61"
INTRODUCTION,0.061224489795918366,"several typical datasets, including the SQuAD 2.0 dataset and the CoLA dataset.
62"
RELATED WORK,0.062196307094266275,"1.1
Related Work
63"
RELATED WORK,0.06316812439261418,"Optimization over the Stiefel manifold. Optimization over the Stiefel manifold has attracted lots of
64"
RELATED WORK,0.0641399416909621,"attention due to its broad applications. Through the use of retraction, known as the generalization of
65"
RELATED WORK,0.06511175898931001,"the exponential map, the Riemannian gradient descent is proposed [2, 6, 23], where all iterates lie on
66"
RELATED WORK,0.06608357628765792,"the manifold. When such retraction is computationally costly, the authors [19] develop a retraction-
67"
RELATED WORK,0.06705539358600583,"free algorithm based on the augmented Lagrangian method. More recently, by defining the constraint
68"
RELATED WORK,0.06802721088435375,"dissolving operator and adding a sufficiently large penalty term, the authors [41] convert the manifold
69"
RELATED WORK,0.06899902818270165,"constrained problem (1) into an unconstrained problem and then apply unconstrained optimization
70"
RELATED WORK,0.06997084548104957,"algorithms. In [1], motivated by the convergence of the Oja’s flow, a landing flow, consisting of the
71"
RELATED WORK,0.07094266277939747,"projected gradient and the gradient of the penalty function, is developed to retraction-free method for
72"
RELATED WORK,0.07191448007774538,"the squared Stiefel manifold, i.e., d = r. All of these methods rely on an unknown penalty parameter
73"
RELATED WORK,0.0728862973760933,"to ensure the convergence. This motivates us to design penalty parameter-free algorithms, which
74"
RELATED WORK,0.0738581146744412,"could significantly reduce the need for tuning parameters in practical implementations.
75"
RELATED WORK,0.07482993197278912,"LoRA. There are numerous variants of LoRA aiming to improve performance or reduce memory
76"
RELATED WORK,0.07580174927113703,"usage. AdaLoRA [46], a well-known successor, introduces the idea of adaptively adjusting the rank
77"
RELATED WORK,0.07677356656948493,"of different layers by incorporating an additional vector g to serve as the diagonal of a singular
78"
RELATED WORK,0.07774538386783285,"value matrix. This approach leverages a revised sensitivity-based importance measure to decide
79"
RELATED WORK,0.07871720116618076,"whether to disable entries in vector g and in matrices A and B. A similar work, SoRA [15],
80"
RELATED WORK,0.07968901846452867,"adopts the same model architecture as AdaLoRA, but proposes a different way to update vector
81"
RELATED WORK,0.08066083576287658,"g after training. This update rule is the proximal gradient of L1 loss, acting as a post-pruning
82"
RELATED WORK,0.08163265306122448,"method. Additionally, a recently emerged method called VeRA [25] significantly reduces memory
83"
RELATED WORK,0.0826044703595724,"overhead while maintaining competitive performance. Based on the idea that networks with random
84"
RELATED WORK,0.08357628765792031,"initialization contain subnetworks that are near-optimal or optimal [17], VeRA only uses two frozen
85"
RELATED WORK,0.08454810495626822,"low-rank matrices shared by all layers, training scaling vectors unique to each layer. Although LoRA
86"
RELATED WORK,0.08551992225461613,"has gained significant popularity and various variants have been developed, the potential for efficient
87"
RELATED WORK,0.08649173955296405,"training through leveraging the manifold geometry to reduce redundancy has not been well-explored.
88"
NOTATION,0.08746355685131195,"1.2
Notation
89"
NOTATION,0.08843537414965986,"For a matrix X ∈Rd×r, we use ∥X∥to denote its Frobenius norm. For a squared matrix A ∈Rd×d,
90"
NOTATION,0.08940719144800778,we define sym(A) = A+A⊤
NOTATION,0.09037900874635568,"2
and use diag(A) ∈Rd to denote its diagonal part. For two matrices
91"
NOTATION,0.0913508260447036,"X, Y ∈Rd×r, we use ⟨X, Y ⟩:= Pd
i=1
Pr
j=1 XijYij to denote their Euclidean inner product. For a
92"
NOTATION,0.0923226433430515,"differential function f : Rd×r →d, we use ∇f(X) to denote its Euclidean gradient at X.
93"
RETRACTION-FREE AND PENALTY PARAMETER-FREE OPTIMIZATION OVER THE STIEFEL,0.09329446064139942,"2
Retraction-free and penalty parameter-free optimization over the Stiefel
94"
RETRACTION-FREE AND PENALTY PARAMETER-FREE OPTIMIZATION OVER THE STIEFEL,0.09426627793974733,"manifold
95"
RETRACTION-FREE AND PENALTY PARAMETER-FREE OPTIMIZATION OVER THE STIEFEL,0.09523809523809523,"In this section, we focus on the design of retraction-free and penalty parameter-free algorithms for
96"
RETRACTION-FREE AND PENALTY PARAMETER-FREE OPTIMIZATION OVER THE STIEFEL,0.09620991253644315,"solving problem (1). We will first present the retraction-free algorithm and then show how the penalty
97"
RETRACTION-FREE AND PENALTY PARAMETER-FREE OPTIMIZATION OVER THE STIEFEL,0.09718172983479106,"parameter can be explicitly determined by characterizing the landscape of the penalty function.
98"
RETRACTION-FREE ALGORITHMS,0.09815354713313897,"2.1
Retraction-free algorithms
99"
RETRACTION-FREE ALGORITHMS,0.09912536443148688,"Inspired by the retraction-free algorithms [19, 41, 1], we consider the following retraction-free
100"
RETRACTION-FREE ALGORITHMS,0.1000971817298348,"gradient descent method for problem (1):
101"
RETRACTION-FREE ALGORITHMS,0.1010689990281827,"Xk+1 = Xk −αgradf(Xk) −µXk(X⊤
k Xk −Id),
(2)
where α, µ
>
0 are step sizes and the projected gradient gradf(Xk)
:=
∇f(Xk) −
102"
RETRACTION-FREE ALGORITHMS,0.10204081632653061,"Xksym(X⊤
k ∇f(Xk)). Note that the tangent space of St(d, r) is TXkSt(d, r) := {ξ ∈Rd×r :
103"
RETRACTION-FREE ALGORITHMS,0.10301263362487852,"X⊤
k ξ + ξ⊤Xk = 0}. Then, for Xk ∈St(d, r), gradf(Xk) is the projection of the Euclidean gra-
104"
RETRACTION-FREE ALGORITHMS,0.10398445092322643,"dient ∇f(Xk) to the tangent space, i.e., gradf(Xk) = PTXk St(d,r)(∇f(Xk)). Note that the term
105"
RETRACTION-FREE ALGORITHMS,0.10495626822157435,"Xk(X⊤
k Xk −Id) is exactly the gradient of the following quadratic penalty function
106"
RETRACTION-FREE ALGORITHMS,0.10592808551992225,φ(X) := 1
RETRACTION-FREE ALGORITHMS,0.10689990281827016,4∥X⊤X −I∥2.
RETRACTION-FREE ALGORITHMS,0.10787172011661808,"As will be shown in our theorem, the use of the projected gradient is essential for landing on the
107"
RETRACTION-FREE ALGORITHMS,0.10884353741496598,"manifold. This differs with the usual penalty method, which optimizes f(X) + µφ(X) using the
108"
RETRACTION-FREE ALGORITHMS,0.1098153547133139,"update Xk+1 = Xk −α∇f(Xk) −µXk(X⊤
k Xk −Id), needs µ →∞to guarantee the feasibility.
109"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.11078717201166181,"2.2
Explicit choice for the penalty parameter
110"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.11175898931000972,"It is known that a large penalty parameter yields better feasibility [29, Chapter 17]. To make the
111"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.11273080660835763,"iterative scheme (2) be penalty parameter-free, we need a careful investigation on the landscape of
112"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.11370262390670553,"the following optimization problem:
113"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.11467444120505345,"min
X∈Rd×r φ(X).
(3)"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.11564625850340136,"It can be easily verified that problem (3) is nonconvex and its the optimal solution set is St(d, r). The
114"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.11661807580174927,"key of obtaining an explicit formula of µ is to establish certain strong convexity-type inequality and
115"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.11758989310009718,"show the gradient descent method with step size µ has linear convergence.
116"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.1185617103984451,"For any X ∈St(d, r), let us denote ¯X := PSt(d,r)(X). Let X = USV ⊤be the singular value
117"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.119533527696793,"decomposition with orthogonal matrices U ∈Rd×r, V ∈Rd×d and diagonal matrix S ∈Rd×d, then
118"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.12050534499514091,"¯X = UV ⊤. Building on these notations, we demonstrate that problem (3) satisfies the restrict secant
119"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.12147716229348883,"inequality (RSI) [45], which serves as an alternative to the strong convexity in the linear convergence
120"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.12244897959183673,"analysis of gradient-type methods.
121"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.12342079689018465,Lemma 1. For any X ∈Rd×r with ∥X −¯X∥≤1
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.12439261418853255,"8, we have
122

∇φ(X), X −¯X

≥∥X −¯X∥2.
(4)"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.12536443148688048,"With the above RSI, we have the linear convergence of the gradient descent update for (3), i.e.,
123"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.12633624878522837,"Xk+1 = Xk −µ∇φ(Xk).
(5)
Lemma 2. Let the sequence {Xk} be generated by (5) with µ = 1"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.12730806608357628,3. Suppose that ∥X0 −¯X0∥≤1
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.1282798833819242,"8.
124"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.1292517006802721,"We have
125"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.13022351797862003,∥Xk+1 −¯Xk+1∥2 ≤2
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.13119533527696792,"3∥Xk −¯Xk∥2.
(6)"
EXPLICIT CHOICE FOR THE PENALTY PARAMETER,0.13216715257531583,"The proofs of Lemmas 1 and 2 can be found in Appendix B.
126"
LANDING ON THE STIEFEL MANIFOLD,0.13313896987366375,"2.3
Landing on the Stiefel manifold
127"
LANDING ON THE STIEFEL MANIFOLD,0.13411078717201166,"Building on the established linear convergence of gradient descent for problem (3), we are now able
128"
LANDING ON THE STIEFEL MANIFOLD,0.13508260447035958,"to show that the iterates generated by (2) will land on the Stiefel manifold eventually, and the limiting
129"
LANDING ON THE STIEFEL MANIFOLD,0.1360544217687075,"point is a stationary point of (1), i.e., gradf(X∞) = 0.
130"
LANDING ON THE STIEFEL MANIFOLD,0.13702623906705538,"Let us start with the Lipschitz continuity of gradf(X). For any X ∈¯USt(d,r)( 1"
LANDING ON THE STIEFEL MANIFOLD,0.1379980563654033,"8), we define
131"
LANDING ON THE STIEFEL MANIFOLD,0.13896987366375121,"PTXSt(d,r)(U) = U −Xsym(X⊤U) for U ∈Rd×r. We first have the following quadratic upper
132"
LANDING ON THE STIEFEL MANIFOLD,0.13994169096209913,"bound on f from its twice differentiability and the compactness of St(d, r).
133"
LANDING ON THE STIEFEL MANIFOLD,0.14091350826044705,"Lemma 3. There exists a constant L > 0 such that for any X, Y ∈St(d, r), the following quadratic
134"
LANDING ON THE STIEFEL MANIFOLD,0.14188532555879493,"upper bound holds:
135"
LANDING ON THE STIEFEL MANIFOLD,0.14285714285714285,"f(Y ) ≤f(X) + ⟨gradf(X), Y −X⟩+ L"
LANDING ON THE STIEFEL MANIFOLD,0.14382896015549076,"2 ∥Y −X∥2.
(7)"
LANDING ON THE STIEFEL MANIFOLD,0.14480077745383868,"In addition, there exists a constant ˆL > 0 such that for any X ∈St(d, r), Y ∈UM( 1"
LANDING ON THE STIEFEL MANIFOLD,0.1457725947521866,"8),
136"
LANDING ON THE STIEFEL MANIFOLD,0.1467444120505345,"∥gradf(X) −gradf(Y )∥≤ˆL∥X −Y ∥.
(8)"
LANDING ON THE STIEFEL MANIFOLD,0.1477162293488824,"By the linear convergence result in Lemma 2, we have the following bound on the feasibility error.
137"
LANDING ON THE STIEFEL MANIFOLD,0.14868804664723032,Lemma 4. Let {Xk} be the sequence generated by (2) with µ = 1
LANDING ON THE STIEFEL MANIFOLD,0.14965986394557823,3 and ∥X0 −¯X0∥≤1
WE HAVE,0.15063168124392615,"8. We have
138"
WE HAVE,0.15160349854227406,∥Xk+1 −¯Xk+1∥≤2
WE HAVE,0.15257531584062195,"3∥Xk −¯Xk∥+ α∥gradf(Xk)∥.
(9)"
WE HAVE,0.15354713313896987,"The following one-step descent lemma on f is crucial in establishing the convergence.
139"
WE HAVE,0.15451895043731778,Lemma 5. Let {Xk} be the sequence generated by (2) with µ = 1
WE HAVE,0.1554907677356657,3 and ∥X0 −¯X0∥≤1
WE HAVE,0.1564625850340136,"8. We have
140"
WE HAVE,0.15743440233236153,f( ¯Xk+1) −f( ¯Xk) ≤−(α −(4ˆL2 + 4L + 1)α2)∥gradf(Xk)∥2 + 1
WE HAVE,0.15840621963070942,2∥Xk+1 −¯Xk+1∥2 + 1 2
WE HAVE,0.15937803692905733,"
4 ˆDf + 16ˆL2 + 16L + 3

∥Xk −¯Xk∥2.
(10)"
WE HAVE,0.16034985422740525,"From the above lemma, the one-step descrease on f is related to both the gradient norm of f and the
141"
WE HAVE,0.16132167152575316,"feasibility error. In terms of convergence, we need both gradf(Xk) and ∥X⊤
k Xk −I∥converge to 0.
142"
WE HAVE,0.16229348882410108,"The following theorem demonstrates that the retraction-free and penalty parameter-free update (2)
143"
WE HAVE,0.16326530612244897,"converges.
144"
WE HAVE,0.16423712342079688,Theorem 1. Let {Xk} be the sequence generated by (2) with µ = 1
WE HAVE,0.1652089407191448,3 and ∥X0 −¯X0∥≤1
IF THE,0.1661807580174927,"8. If the
145"
IF THE,0.16715257531584063,"step size α <
1
2c1 for some c1 large enough, then we have
146"
IF THE,0.16812439261418854,"min
k=0,...,K ∥gradf(Xk)∥2 ≤1"
IF THE,0.16909620991253643,"K ,
min
k=0,...,K ∥X⊤
k Xk −I∥2 ≤1"
IF THE,0.17006802721088435,"K .
(11)"
IF THE,0.17103984450923226,"The proofs of the above lemmas and theorem are presented in Appendix B.
147"
ACCELERATE LORA FINE-TUNING WITH LANDING,0.17201166180758018,"3
Accelerate LoRA fine-tuning with landing
148"
ACCELERATE LORA FINE-TUNING WITH LANDING,0.1729834791059281,"In this section, we will first clarify where the Stiefel manifold constraint comes from in the LoRA
149"
ACCELERATE LORA FINE-TUNING WITH LANDING,0.17395529640427598,"fine-tuning. Then, we will apply the above developed retraction-free and penalty parameter-free
150"
ACCELERATE LORA FINE-TUNING WITH LANDING,0.1749271137026239,"method to enhance LoRA fine-tuning.
151"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.17589893100097181,"3.1
Manifold optimization formulation of LoRA fine-tuning
152"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.17687074829931973,"In neural networks, the dense layers perform matrix multiplication, and the weight matrices in these
153"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.17784256559766765,"layers usually have a full rank. However, when adapting to a specific task, pre-trained language models
154"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.17881438289601556,"have been shown to have a low intrinsic dimension, allowing them to learn efficiently even with a
155"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.17978620019436345,"random projection to a smaller subspace. One possible drawback in the current LoRA fine-tuning
156"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.18075801749271136,"framework is that the low-rank decomposition ∆W into product BA is not unique. Specifically,
157"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.18172983479105928,"for any invertible matrix C, it holds that BA = (BC)(C−1A). Note that BC shares the same
158"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.1827016520894072,"column space with B. This suggests us optimizing the subspace generated by B instead of B itself.
159"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.1836734693877551,"Numerous studies in the field of low-rank optimization, e.g., [7, 13, 12], investigate the manifold
160"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.184645286686103,"geometry of the low-rank decomposition and develop efficient algorithms. However, such geometry
161"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.18561710398445092,"has not been explored in the LoRA fine-tuning.
162"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.18658892128279883,"To address such redundancy (i.e., the non-uniqueness of BA representations), we regard B as the basis
163"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.18756073858114675,"through the manifold constraint and A as the coordinate of ∆W under B. Hence, the optimization
164"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.18853255587949466,"problem can be formulated as
165"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.18950437317784258,"min
A,B
L(BA),
subject to
B ∈St(d, r) or B ∈Ob(d, r),
(12)"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.19047619047619047,"where Ob(d, r) := {B ∈Rd×r : diag(B⊤B) = 1}. Compared to the Stiefel manifold St(d, r),
166"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.19144800777453838,"the oblique manifold Ob(d, r) necessitates that the matrix B has unit norms in its columns, without
167"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.1924198250728863,"imposing requirements for orthogonality between the columns. Problem (12) is an optimization
168"
MANIFOLD OPTIMIZATION FORMULATION OF LORA FINE-TUNING,0.1933916423712342,"problem over the product of manifolds and Euclidean spaces.
169"
MANIFOLD-LORA,0.19436345966958213,"3.2
Manifold-LoRA
170"
MANIFOLD-LORA,0.19533527696793002,"The retraction-free method is well-suited to address (12), simultaneously minimizing the loss function
171"
MANIFOLD-LORA,0.19630709426627793,"L(BA) and constraint violation. To control the constraint violation, we use the quadratic penalties
172"
MANIFOLD-LORA,0.19727891156462585,"Rs(B) := ∥B⊤B −I∥2 and Ro(B) := ∥diag(B⊤B) −1∥2 for the Stiefel manifold and oblique
173"
MANIFOLD-LORA,0.19825072886297376,"manifold, respectively. As shown in the landing theory in Section 2, we shall use the projected
174"
MANIFOLD-LORA,0.19922254616132168,"gradient of the loss part instead of the Euclidean gradient. For the Stiefel manifold and the oblique
175"
MANIFOLD-LORA,0.2001943634596696,"manifold, the respective projected gradients are
176"
MANIFOLD-LORA,0.20116618075801748,"gradBL(BA) = ∇BL(BA) −Bsym(B⊤∇BL(BA))
(13)"
MANIFOLD-LORA,0.2021379980563654,"and
177"
MANIFOLD-LORA,0.2031098153547133,"gradBL(BA) = ∇BL(BA) −Bdiag(diag(B⊤∇BL(BA))),
(14)"
MANIFOLD-LORA,0.20408163265306123,"where sym(X) := (X + X⊤)/2. Thus, the gradients of our retraction-free method for A and B are
178"
MANIFOLD-LORA,0.20505344995140914,"∇AL(BA) and gradBL(BA) + µ∇Rs(B)( or ∇Ro(B)).
179"
MANIFOLD-LORA,0.20602526724975703,"Note that B and A represent the basis and the coordinate of ∆W, respectively. This results in
180"
MANIFOLD-LORA,0.20699708454810495,"different magnitudes and different Lipschitz constants of their gradient function. In fact, let X = BA.
181"
MANIFOLD-LORA,0.20796890184645286,"It follows
182"
MANIFOLD-LORA,0.20894071914480078,"∇AL(BA) = B⊤∇XL(X),
∇BL(BA) = ∇XL(X)A⊤.
Then,
183"
MANIFOLD-LORA,0.2099125364431487,"∥∇AL(BA1) −∇L(BA2)∥≤∥B∥2Lg∥A1 −A2∥,
∥∇BL(B1A) −∇L(B2A)∥≤∥A∥2Lg∥B1 −B2∥,"
MANIFOLD-LORA,0.2108843537414966,"where Lg is the Lipschitz constant of ∇XL(X) and ∥· ∥2 represent the matrix ℓ2 norm (i.e., the
184"
MANIFOLD-LORA,0.2118561710398445,"largest singular value). Note that the step size generally should be propositional to the reciprocal of
185"
MANIFOLD-LORA,0.21282798833819241,"Lipschitz constant for the gradient type algorithms [29, 5]. Hence, we schedule the learning rates for
186"
MANIFOLD-LORA,0.21379980563654033,"the two matrices based on their respective ℓ2 norms. Having prepared the above, we incorporate the
187"
MANIFOLD-LORA,0.21477162293488825,"AdamW optimizer [28] with our manifold-accelerated technique to enhance the LoRA fine-tuning, as
188"
MANIFOLD-LORA,0.21574344023323616,"presented in Algorithm 1.
189"
EXPERIMENTS,0.21671525753158405,"4
Experiments
190"
EXPERIMENTS,0.21768707482993196,"In this section, we delve into the experimental results and their detailed analysis. This discussion is
191"
EXPERIMENTS,0.21865889212827988,"structured around two principal areas: (1) the performance gain compared to other mainstream fine-
192"
EXPERIMENTS,0.2196307094266278,"tuning methods and accelerated convergence achieved through our manifold-constrained optimization
193"
EXPERIMENTS,0.2206025267249757,"approach; (2) the convergence of matrix B onto the manifold, illustrated by the heat map of B⊤B.
194"
EXPERIMENTS,0.22157434402332363,"Baselines We compare our approach against several baseline methods, including full fine-tuning,
195"
EXPERIMENTS,0.22254616132167152,"Adapter [21], BitFit [44] and LoRA [22]. The variants of the Adapter method are excluded from the
196"
EXPERIMENTS,0.22351797862001943,"baselines, as their performance are relatively similar.
197"
EXPERIMENTS,0.22448979591836735,"Implementation Details Our code is based on Pytorch [31], Huggingface Transformers [40] and an
198"
EXPERIMENTS,0.22546161321671526,"open-source plug-and-play library for parameter-efficient fine-tuning opendelta [24]. The bottleneck
199"
EXPERIMENTS,0.22643343051506318,"dimension for the Adapter is set to 16 or 32, ensuring that the number of trainable parameters aligns
200"
EXPERIMENTS,0.22740524781341107,"Algorithm 1: Manifold-LoRA
Input: Initial point A0, B0, µ ∈R, β1 = 0.9, β2 = 0.999, upper bound ≥lower bound > 0,
ϵ = 10−8, γ > 0, λ ∈R, and k = 0.
while Stopping conditions not met do"
EXPERIMENTS,0.22837706511175898,"for C ∈{A, B} do"
EXPERIMENTS,0.2293488824101069,if C = B then
EXPERIMENTS,0.2303206997084548,Set g(Ck) according to (13) or (14) using the stochastic estimate of ∇BL(BkAk)
EXPERIMENTS,0.23129251700680273,"// Projected gradient for matrix B
else"
EXPERIMENTS,0.23226433430515064,"Set g(Ck) to be the stochastic estimate of ∇AL(BkAk)
end
end
m(Ck) ←β1m(Ck) + (1 −β1)g(Ck)
v(Ck) ←β2v(Ck) + (1 −β2)g2
t (Ck)
ˆm(Ck) ←m(Ck)"
EXPERIMENTS,0.23323615160349853,"1−βt
1
ˆv(Ck) ←v(Ck)"
EXPERIMENTS,0.23420796890184645,"1−βt
2
η(Ck) ←clip(normCk, upper bound, lower bound)"
EXPERIMENTS,0.23517978620019436,// Scheduling step size of matrix A and B
EXPERIMENTS,0.23615160349854228,"Ck ←Ck−1 −ηt(Ck)

ˆmt(Ck)/
p"
EXPERIMENTS,0.2371234207968902,"ˆvt(Ck) + ϵ

−λCk−1
if C = B then"
EXPERIMENTS,0.23809523809523808,"Ck ←Ck −µ∇Rs(Ck)( or ∇Ro(Ck)) // Apply penalty gradient for matrix B
end
end
k ←k + 1
end"
EXPERIMENTS,0.239067055393586,"closely with that of the LoRA method and the new layers are inserted into the attention layer and
201"
EXPERIMENTS,0.2400388726919339,"feed-forward layer. The update of LoRA is scaled by a hyper-parameter α. This value is typically left
202"
EXPERIMENTS,0.24101068999028183,"unmodified, as it is usually set as 16 or 32 and never tuned [22, 43]. The exponential moving average
203"
EXPERIMENTS,0.24198250728862974,"parameters β1 and β2 of AdamW [27] are set to their default values of 0.9 and 0.999, respectively. All
204"
EXPERIMENTS,0.24295432458697766,"the experiments are conducted on NVIDIA A800 GPUs. More details are presented in Appendix C.
205"
NATURAL LANGUAGE UNDERSTANDING,0.24392614188532555,"4.1
Natural language understanding
206"
NATURAL LANGUAGE UNDERSTANDING,0.24489795918367346,"We first evaluate our backbone model DeBERTaV3-base [20] on GLUE [37] benchmark containing
207"
NATURAL LANGUAGE UNDERSTANDING,0.24586977648202138,"nine sub datasets, including MNLI [39], SST-2 [36], CoLA [38], QQP [37], QNLI [35], RTE [4],
208"
NATURAL LANGUAGE UNDERSTANDING,0.2468415937803693,"MRPC [16], and STS-B [37].
209"
NATURAL LANGUAGE UNDERSTANDING,0.2478134110787172,"Experimental results of the GLUE dataset are recorded in Table 1. It can be seen that our method
210"
NATURAL LANGUAGE UNDERSTANDING,0.2487852283770651,"is consistently superior to other baselines. Notably, for RTE and STS-B datasets, both sphere-
211"
NATURAL LANGUAGE UNDERSTANDING,0.24975704567541301,"constrained (i.e., oblique manifold-constrained) and Stiefel-constrained have an obvious performance
212"
NATURAL LANGUAGE UNDERSTANDING,0.25072886297376096,"gain even with only half the trainable parameters compared to the LoRA baseline, i.e., Spherer=8 and
213"
NATURAL LANGUAGE UNDERSTANDING,0.25170068027210885,"Stiefelr=8 beat LoRAr=16. In addition, with the help of manifold geometry, the fine-tuning process
214"
NATURAL LANGUAGE UNDERSTANDING,0.25267249757045673,"can be significantly accelerated compared to the vanilla AdamW optimizer, achieving a lower training
215"
NATURAL LANGUAGE UNDERSTANDING,0.2536443148688047,"loss, as shown in Figure 1. Particularly on the CoLA dataset presented in Figure 1a, our approach
216"
NATURAL LANGUAGE UNDERSTANDING,0.25461613216715256,"achieves the same training loss as the standard Adam optimizer but requires nearly half the number
217"
NATURAL LANGUAGE UNDERSTANDING,0.2555879494655005,"of epochs.
218"
QUESTION ANSWERING,0.2565597667638484,"4.2
Question Answering
219"
QUESTION ANSWERING,0.2575315840621963,"We conduct an evaluation on two question answering datasets: SQuAD v1.1 [35] and SQuADv2.0
220"
QUESTION ANSWERING,0.2585034013605442,"[34]. Manifold-LoRA is used to fine-tune DeBERTaV3-base for these tasks, which are treated as
221"
QUESTION ANSWERING,0.2594752186588921,"sequence labeling problems predicting the probability of each token as the start or end of an answer
222"
QUESTION ANSWERING,0.26044703595724006,"span.
223"
QUESTION ANSWERING,0.26141885325558795,"The main experimental results are presented in Table 2. For LoRA and our algorithms, new layers
224"
QUESTION ANSWERING,0.26239067055393583,"are inserted into Wq, Wk, Wv, Wo, FC1, FC2. Notably, both manifold-regularized LoRA variants
225"
QUESTION ANSWERING,0.2633624878522838,"consistently outperform all fine-tuning methods. Additionally, we plot the training loss, evaluation
226"
QUESTION ANSWERING,0.26433430515063167,Table 1: Results with DeBERTaV3-base on GLUE benchmark. We denote the best results in bold.
QUESTION ANSWERING,0.2653061224489796,"Method # Params
MNLI
SST-2
CoLA
QQP
QNLI
RTE
MRPC
STS-B
All
m / mm
Acc
Mcc
Acc / F1
Acc
Acc
Acc
Corr
Ave."
QUESTION ANSWERING,0.2662779397473275,"Full
FT
184.42M
90.45/90.60
95.48
68.17
91.99/89.12
93.60
79.28
88.93
90.92
87.85"
QUESTION ANSWERING,0.26724975704567544,"Adapter
0.61M
90.13/90.16
94.86
69.37
91.38/88.46
93.54
81.87
89.12
91.52
88.06
BitFit
0.06M
87.08/86.39
94.88
69.11
87.96/84.35
92.19
76.52
87.06
90.96
85.65
LoRAr=8
0.30M
90.20/90.08
94.93
68.14
90.78/87.68
93.85
80.15
90.40
90.29
87.60
LoRAr=16 0.59M
90.44/90.12
95.41
68.19
90.92/87.77
94.00
80.58
90.20
90.34
87.74
Spherer=8 0.30M
90.37/90.09
95.48
69.55
91.25/88.34
94.02
82.44
91.55
91.26
88.44
Spherer=16 0.59M
90.52/90.19
95.64
70.14
91.46/88.65
94.29
82.16
91.67
91.59
88.63
Stiefelr=8 0.30M
90.25/89.99
95.46
69.85
91.44/88.60
94.09
83.16
91.18
91.22
88.52
Stiefelr=16 0.59M
90.26/90.28
95.76
68.92
91.71/89.00
94.10
82.16
91.10
91.51
88.48"
QUESTION ANSWERING,0.26822157434402333,"exact match, and evaluation F1 scores against epochs in Figure 2. We conclude that the proposed
227"
QUESTION ANSWERING,0.2691933916423712,"Manifold-LoRA method achieves a 2x speed-up in training epochs compared to AdamW, while
228"
QUESTION ANSWERING,0.27016520894071916,"simultaneously improving model performance. We also illustrate the heat map of B⊤B in Figure 3,
229"
QUESTION ANSWERING,0.27113702623906705,"which indicates that the matrix B lands on the manifold eventually. This supports our assertion that
230"
QUESTION ANSWERING,0.272108843537415,"landing on manifold enhances the performance of LoRA.
231"
NATURAL LANGUAGE GENERATION,0.2730806608357629,"4.3
Natural Language Generation
232"
NATURAL LANGUAGE GENERATION,0.27405247813411077,"The E2E NLG Challenge[30], as introduced by Novikova, provides a dataset for training end-to-end,
233"
NATURAL LANGUAGE GENERATION,0.2750242954324587,"data-driven natural language generation systems, widely used in data-to-text evaluations. The E2E
234"
NATURAL LANGUAGE GENERATION,0.2759961127308066,"dataset comprises approximately 42,000 training examples, 4,600 validation examples, and 4,600
235"
NATURAL LANGUAGE GENERATION,0.27696793002915454,"test examples, all from the restaurant domain. We test our method on the E2E dataset using GPT-2
236"
NATURAL LANGUAGE GENERATION,0.27793974732750243,"Medium and Large models, following the experimental setup outlined by LoRA [22]. For LoRA, we
237"
NATURAL LANGUAGE GENERATION,0.2789115646258503,"set the hyperparameters to match those specified in the original paper.
238"
NATURAL LANGUAGE GENERATION,0.27988338192419826,"The results from the E2E dataset are recorded in Table 3, where we focus on comparing LoRA and
239"
NATURAL LANGUAGE GENERATION,0.28085519922254615,"Manifold-LoRA. The results clearly indicate that our proposed algorithm outperforms the established
240"
NATURAL LANGUAGE GENERATION,0.2818270165208941,"baselines. Also, as shown in Figure 4, the matrix B resides on the manifold even at the early training
241"
NATURAL LANGUAGE GENERATION,0.282798833819242,"stage, validating the feasibility of our method.
242"
NATURAL LANGUAGE GENERATION,0.28377065111758987,"0
5
10
15
20
25
Epoch 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Loss Lora"
NATURAL LANGUAGE GENERATION,0.2847424684159378,Sphere
NATURAL LANGUAGE GENERATION,0.2857142857142857,Stefiel
NATURAL LANGUAGE GENERATION,0.28668610301263364,(a) CoLA train loss
NATURAL LANGUAGE GENERATION,0.28765792031098153,"0
1
2
3
4
5
Epoch 0.150 0.175 0.200 0.225 0.250 0.275 0.300 Loss Lora"
NATURAL LANGUAGE GENERATION,0.2886297376093295,Sphere
NATURAL LANGUAGE GENERATION,0.28960155490767736,Stefiel
NATURAL LANGUAGE GENERATION,0.29057337220602525,(b) QQP train loss
NATURAL LANGUAGE GENERATION,0.2915451895043732,"0
5
10
15
20
25
Epoch 0.0 0.1 0.2 0.3 0.4 0.5 Loss Lora"
NATURAL LANGUAGE GENERATION,0.2925170068027211,Sphere
NATURAL LANGUAGE GENERATION,0.293488824101069,Stefiel
NATURAL LANGUAGE GENERATION,0.2944606413994169,(c) STSB train loss
NATURAL LANGUAGE GENERATION,0.2954324586977648,"5
10
15
20
25
Epoch 0.60 0.62 0.64 0.66 0.68 0.70 0.72"
NATURAL LANGUAGE GENERATION,0.29640427599611274,Eval Matthews Correlation Lora
NATURAL LANGUAGE GENERATION,0.29737609329446063,Sphere
NATURAL LANGUAGE GENERATION,0.2983479105928086,Stefiel
NATURAL LANGUAGE GENERATION,0.29931972789115646,"(d) CoLA evaluation matthews
correlation"
NATURAL LANGUAGE GENERATION,0.30029154518950435,"0
1
2
3
4
5
Epoch 0.86 0.87 0.88 0.89 0.90 0.91 0.92"
NATURAL LANGUAGE GENERATION,0.3012633624878523,Eval Accuracy Lora
NATURAL LANGUAGE GENERATION,0.3022351797862002,Sphere
NATURAL LANGUAGE GENERATION,0.3032069970845481,Stefiel
NATURAL LANGUAGE GENERATION,0.304178814382896,(e) QQP evaluation accuracy
NATURAL LANGUAGE GENERATION,0.3051506316812439,"5
10
15
20
25
Epoch 0.87 0.88 0.89 0.90 0.91 0.92"
NATURAL LANGUAGE GENERATION,0.30612244897959184,Eval Pearson Lora
NATURAL LANGUAGE GENERATION,0.30709426627793973,Sphere
NATURAL LANGUAGE GENERATION,0.3080660835762877,Stefiel
NATURAL LANGUAGE GENERATION,0.30903790087463556,(f) STSB evaluation pearson
NATURAL LANGUAGE GENERATION,0.3100097181729835,"Figure 1: The figures illustrate that both sphere constrained and Stiefel constrained manifold-LoRA
achieve a faster convergence rate and attain a lower training loss within same optimization steps
compared to LoRA method on three distinct datasets CoLA, QQP, STSB."
NATURAL LANGUAGE GENERATION,0.3109815354713314,"Table 2: Results with DeBERTaV3-base on SQuAD v1.1 and SQuADv2.0. We report EM/F1. The
best results in each setting are shown in bold."
NATURAL LANGUAGE GENERATION,0.3119533527696793,"Methods
Params
SQuADv1.1
SQuADv2.0"
NATURAL LANGUAGE GENERATION,0.3129251700680272,"Full FT
184M
86.30 / 92.85
84.30 / 87.58
Adapterr=16
0.61M
87.46 / 93.41
85.30 / 88.23
Adapterr=32
1.22M
87.53 / 93.51
85.42 / 88.36
Bitfit
0.07M
80.26 / 88.79
74.21 / 87.19
LoRAr=8
1.33M
87.90 / 93.88
85.56 / 88.52
LoRAr=16
2.65M
87.94 / 93.75
85.90 / 88.81
Spherer=8
1.33M
88.51 / 94.25
86.33 / 89.20
Spherer=16
2.65M
88.32 / 94.03
86.15 / 89.03
Stiefelr=8
1.33M
88.68 / 94.23
86.35 / 89.09
Stiefelr=16
2.65M
88.25 / 94.04
86.41 / 89.22"
NATURAL LANGUAGE GENERATION,0.3138969873663751,"1
2
3
4
Epoch 0.5 1.0 1.5 2.0 2.5"
NATURAL LANGUAGE GENERATION,0.31486880466472306,Train Loss Lora
NATURAL LANGUAGE GENERATION,0.31584062196307094,Sphere
NATURAL LANGUAGE GENERATION,0.31681243926141883,Stefiel
NATURAL LANGUAGE GENERATION,0.3177842565597668,(a) SQuADv2.0 Train Loss
NATURAL LANGUAGE GENERATION,0.31875607385811466,"1
2
3
4
Epoch 60 65 70 75 80 85"
NATURAL LANGUAGE GENERATION,0.3197278911564626,Exact Match Lora
NATURAL LANGUAGE GENERATION,0.3206997084548105,Sphere
NATURAL LANGUAGE GENERATION,0.3216715257531584,Stefiel
NATURAL LANGUAGE GENERATION,0.3226433430515063,(b) SQuADv2.0 Eval Exact Match
NATURAL LANGUAGE GENERATION,0.3236151603498542,"1
2
3
4
Epoch 65 70 75 80 85 90"
NATURAL LANGUAGE GENERATION,0.32458697764820216,Eval F1 Lora
NATURAL LANGUAGE GENERATION,0.32555879494655005,Sphere
NATURAL LANGUAGE GENERATION,0.32653061224489793,Stefiel
NATURAL LANGUAGE GENERATION,0.3275024295432459,(c) SQuADv2.0 Eval F1
NATURAL LANGUAGE GENERATION,0.32847424684159376,"Figure 2: The figures compare the training loss, evaluation exact match, and evaluation F1 metrics
against the number of epochs for the SQuADv2.0 dataset."
NATURAL LANGUAGE GENERATION,0.3294460641399417,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.3304178814382896,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.33138969873663754,layer.2.value_proj
NATURAL LANGUAGE GENERATION,0.3323615160349854,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.3333333333333333,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.33430515063168126,layer.2.attention.output
NATURAL LANGUAGE GENERATION,0.33527696793002915,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.3362487852283771,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.337220602526725,layer.2.intermediate.dense
NATURAL LANGUAGE GENERATION,0.33819241982507287,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.3391642371234208,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.3401360544217687,layer.2.output.dense
NATURAL LANGUAGE GENERATION,0.34110787172011664,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.34207968901846453,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.3430515063168124,layer.3.query_proj
NATURAL LANGUAGE GENERATION,0.34402332361516036,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.34499514091350825,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.3459669582118562,layer.3.key_proj
NATURAL LANGUAGE GENERATION,0.3469387755102041,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.34791059280855197,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.3488824101068999,layer.3.value_proj
NATURAL LANGUAGE GENERATION,0.3498542274052478,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.35082604470359574,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.35179786200194363,layer.3.attention.output 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
NATURAL LANGUAGE GENERATION,0.35276967930029157,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.35374149659863946,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.35471331389698735,layer.2.value_proj
NATURAL LANGUAGE GENERATION,0.3556851311953353,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.3566569484936832,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.3576287657920311,layer.2.attention.output
NATURAL LANGUAGE GENERATION,0.358600583090379,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.3595724003887269,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.36054421768707484,layer.2.intermediate.dense
NATURAL LANGUAGE GENERATION,0.36151603498542273,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.3624878522837707,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.36345966958211856,layer.2.output.dense
NATURAL LANGUAGE GENERATION,0.36443148688046645,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.3654033041788144,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.3663751214771623,layer.3.query_proj
NATURAL LANGUAGE GENERATION,0.3673469387755102,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.3683187560738581,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.369290573372206,layer.3.key_proj
NATURAL LANGUAGE GENERATION,0.37026239067055394,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.37123420796890183,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.3722060252672498,layer.3.value_proj
NATURAL LANGUAGE GENERATION,0.37317784256559766,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.3741496598639456,"0
1
2
3
4
5
6
7"
NATURAL LANGUAGE GENERATION,0.3751214771622935,layer.3.attention.output 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
NATURAL LANGUAGE GENERATION,0.3760932944606414,"Figure 3: The heat map of B⊤B with the Stiefel manifold (the first and second rows) and the oblique
manifold (the third and fourth rows) at the end of training on SQuADv2.0 dataset."
NATURAL LANGUAGE GENERATION,0.3770651117589893,"Table 3: GPT-2 medium (M) and large (L) models were evaluated on the E2E NLG Challenge. *
denotes results from previously published works."
NATURAL LANGUAGE GENERATION,0.3780369290573372,"Model
Parameters
BLEU
NIST
MET
ROUGE-L
CIDEr"
NATURAL LANGUAGE GENERATION,0.37900874635568516,"GPT-2 M (FT)*
354.92M
68.2
8.62
46.2
71.0
2.47
GPT-2 M (AdapterL)*
0.37M
66.3
8.41
45.0
69.8
2.40
GPT-2 M (AdapterL)*
11.09M
68.9
8.71
46.1
71.3
2.47
GPT-2 M (AdapterH)*
11.09M
67.3±.6
8.50±.07
46.0±.2
70.7±.2
2.44±.01
GPT-2 M (FTTop2)*
25.19M
68.1
8.59
46.0
70.8
2.41
GPT-2 M (PreLayer)*
0.35M
69.7
8.81
46.1
71.4
2.49
GPT-2 M (LoRA)
0.35M
68.9
8.69
46.5
71.5
2.51
GPT-2 M(Stiefel)
0.35M
70.1
8.82
46.8
71.7
2.53
GPT-2 M(Sphere)
0.35M
70.3
8.83
46.7
71.7
2.52"
NATURAL LANGUAGE GENERATION,0.37998056365403304,"GPT-2 L (FT)*
774.03M
68.5
8.78
46.0
69.9
2.45
GPT-2 L (AdapterL)*
0.88M
69.1±.1
8.68±.03
46.3±.0
71.4±.2
2.49±.0
GPT-2 L (AdapterL)*
23.00M
68.9±.3
8.70±.04
46.1±.1
71.3±.2
2.45±.02
GPT-2 L (PreLayer)*
0.77M
70.3
8.85
46.2
71.7
2.47
GPT-2 L (LoRA)
0.77M
70.1
8.82
46.7
72.0
2.53
GPT-2 L(Stiefel)
0.77M
70.4
8.86
46.8
72.1
2.53
GPT-2 L(Sphere)
0.77M
70.9
8.92
46.8
72.5
2.55"
NATURAL LANGUAGE GENERATION,0.38095238095238093,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.3819241982507289,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.38289601554907676,module.transformer.h.0.attn.c_attn.lora_B
NATURAL LANGUAGE GENERATION,0.3838678328474247,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.3848396501457726,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.3858114674441205,module.transformer.h.1.attn.c_attn.lora_B
NATURAL LANGUAGE GENERATION,0.3867832847424684,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.3877551020408163,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.38872691933916426,module.transformer.h.2.attn.c_attn.lora_B
NATURAL LANGUAGE GENERATION,0.38969873663751214,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.39067055393586003,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.391642371234208,module.transformer.h.3.attn.c_attn.lora_B
NATURAL LANGUAGE GENERATION,0.39261418853255586,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.3935860058309038,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.3945578231292517,module.transformer.h.4.attn.c_attn.lora_B
NATURAL LANGUAGE GENERATION,0.39552964042759964,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.3965014577259475,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.3974732750242954,module.transformer.h.5.attn.c_attn.lora_B
NATURAL LANGUAGE GENERATION,0.39844509232264336,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.39941690962099125,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.4003887269193392,module.transformer.h.6.attn.c_attn.lora_B
NATURAL LANGUAGE GENERATION,0.4013605442176871,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.40233236151603496,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.4033041788143829,module.transformer.h.7.attn.c_attn.lora_B
NATURAL LANGUAGE GENERATION,0.4042759961127308,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.40524781341107874,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.4062196307094266,module.transformer.h.8.attn.c_attn.lora_B 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
NATURAL LANGUAGE GENERATION,0.4071914480077745,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.40816326530612246,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.40913508260447035,module.transformer.h.0.attn.c_attn.lora_B
NATURAL LANGUAGE GENERATION,0.4101068999028183,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.4110787172011662,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.41205053449951407,module.transformer.h.1.attn.c_attn.lora_B
NATURAL LANGUAGE GENERATION,0.413022351797862,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.4139941690962099,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.41496598639455784,module.transformer.h.2.attn.c_attn.lora_B
NATURAL LANGUAGE GENERATION,0.4159378036929057,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.41690962099125367,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.41788143828960156,module.transformer.h.3.attn.c_attn.lora_B
NATURAL LANGUAGE GENERATION,0.41885325558794945,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.4198250728862974,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.4207968901846453,module.transformer.h.4.attn.c_attn.lora_B
NATURAL LANGUAGE GENERATION,0.4217687074829932,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.4227405247813411,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.423712342079689,module.transformer.h.5.attn.c_attn.lora_B
NATURAL LANGUAGE GENERATION,0.42468415937803694,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.42565597667638483,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.42662779397473277,module.transformer.h.6.attn.c_attn.lora_B
NATURAL LANGUAGE GENERATION,0.42759961127308066,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.42857142857142855,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.4295432458697765,module.transformer.h.7.attn.c_attn.lora_B
NATURAL LANGUAGE GENERATION,0.4305150631681244,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.4314868804664723,"0
1
2
3"
NATURAL LANGUAGE GENERATION,0.4324586977648202,module.transformer.h.8.attn.c_attn.lora_B 0.2 0.0 0.2 0.4 0.6 0.8 1.0 0.2 0.0 0.2 0.4 0.6 0.8 1.0 0.2 0.0 0.2 0.4 0.6 0.8 1.0 0.2 0.0 0.2 0.4 0.6 0.8 1.0 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0
NATURAL LANGUAGE GENERATION,0.4334305150631681,"Figure 4: The heat map of B⊤B with the Stiefel manifold (left) and the oblique manifold (right) on
E2E dataset."
CONCLUSION,0.43440233236151604,"5
Conclusion
243"
CONCLUSION,0.43537414965986393,"Optimization over the Stiefel manifold has been widely used in machine learning tasks. In this work,
244"
CONCLUSION,0.4363459669582119,"we develop a retraction-free and penalty parameter-free gradient method, and prove that the generated
245"
CONCLUSION,0.43731778425655976,"iterates eventually land on the manifold and achieve the optimality simultaneously. We then apply
246"
CONCLUSION,0.4382896015549077,"this landing theory to avoid the possible redundancy of LoRA fine-tuning in LLMs. Specifically, we
247"
CONCLUSION,0.4392614188532556,"reformulate the LoRA fine-tuning as an optimization problem over the Stiefel manifold, and propose
248"
CONCLUSION,0.4402332361516035,"a new algorithm, Manifold-LoRA, which incorporates a careful analysis of step sizes to enable fast
249"
CONCLUSION,0.4412050534499514,"training using the landing properties. Extensive experimental results demonstrate that our approach
250"
CONCLUSION,0.4421768707482993,"not only accelerates the training process but also yields significant performance improvements.
251"
CONCLUSION,0.44314868804664725,"Our study suggests several potential directions for future research. Although the established landing
252"
CONCLUSION,0.44412050534499514,"theory focuses on the Stiefel manifold, extending this theory to general manifolds is one potential
253"
CONCLUSION,0.44509232264334303,"direction. Additionally, evaluating the performance of Manifold-LoRA on LLMs with billions of
254"
CONCLUSION,0.446064139941691,"parameters would be valuable. Due to the heterogeneity of different layers, incorporating adaptive
255"
CONCLUSION,0.44703595724003886,"ranks for ∆W across different layers is another possible direction. This may be achievable by adding
256"
CONCLUSION,0.4480077745383868,"sparsity regularization to the coordinate matrix A.
257"
REFERENCES,0.4489795918367347,"References
258"
REFERENCES,0.4499514091350826,"[1] Pierre Ablin and Gabriel Peyr´e. Fast and accurate optimization on the orthogonal manifold
259"
REFERENCES,0.4509232264334305,"without retraction. In International Conference on Artificial Intelligence and Statistics, pages
260"
REFERENCES,0.4518950437317784,"5636–5657. PMLR, 2022.
261"
REFERENCES,0.45286686103012636,"[2] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix
262"
REFERENCES,0.45383867832847424,"manifolds. Princeton University Press, 2008.
263"
REFERENCES,0.45481049562682213,"[3] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks.
264"
REFERENCES,0.4557823129251701,"In International conference on machine learning, pages 1120–1128. PMLR, 2016.
265"
REFERENCES,0.45675413022351796,"[4] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth pascal recognizing
266"
REFERENCES,0.4577259475218659,"textual entailment challenge. TAC, 7(8):1, 2009.
267"
REFERENCES,0.4586977648202138,"[5] L´eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
268"
REFERENCES,0.45966958211856174,"learning. SIAM review, 60(2):223–311, 2018.
269"
REFERENCES,0.4606413994169096,"[6] Nicolas Boumal. An introduction to optimization on smooth manifolds. Cambridge University
270"
REFERENCES,0.4616132167152575,"Press, 2023.
271"
REFERENCES,0.46258503401360546,"[7] Nicolas Boumal and Pierre-antoine Absil. Rtrmc: A riemannian trust-region method for
272"
REFERENCES,0.46355685131195334,"low-rank matrix completion. Advances in neural information processing systems, 24, 2011.
273"
REFERENCES,0.4645286686103013,"[8] Robert S Chen, Brendan Lucier, Yaron Singer, and Vasilis Syrgkanis. Robust optimization for
274"
REFERENCES,0.4655004859086492,"non-convex objectives. Advances in Neural Information Processing Systems, 30, 2017.
275"
REFERENCES,0.46647230320699706,"[9] Shixiang Chen, Alfredo Garcia, Mingyi Hong, and Shahin Shahrampour. Decentralized Rie-
276"
REFERENCES,0.467444120505345,"mannian gradient descent on the Stiefel manifold. In International Conference on Machine
277"
REFERENCES,0.4684159378036929,"Learning, pages 1594–1605. PMLR, 2021.
278"
REFERENCES,0.46938775510204084,"[10] Minhyung Cho and Jaehyung Lee. Riemannian approach to batch normalization. Advances in
279"
REFERENCES,0.4703595724003887,"Neural Information Processing Systems, 30, 2017.
280"
REFERENCES,0.4713313896987366,"[11] Francis H Clarke, Ronald J Stern, and Peter R Wolenski. Proximal smoothness and the lower-C2
281"
REFERENCES,0.47230320699708456,"property. Journal of Convex Analysis, 2(1-2):117–144, 1995.
282"
REFERENCES,0.47327502429543244,"[12] Wei Dai, Ely Kerman, and Olgica Milenkovic. A geometric approach to low-rank matrix
283"
REFERENCES,0.4742468415937804,"completion. IEEE Transactions on Information Theory, 58(1):237–247, 2012.
284"
REFERENCES,0.4752186588921283,"[13] Wei Dai, Olgica Milenkovic, and Ely Kerman. Subspace evolution and transfer (set) for low-rank
285"
REFERENCES,0.47619047619047616,"matrix completion. IEEE Transactions on Signal Processing, 59(7):3120–3132, 2011.
286"
REFERENCES,0.4771622934888241,"[14] Kangkang Deng and Jiang Hu. Decentralized projected riemannian gradient method for smooth
287"
REFERENCES,0.478134110787172,"optimization on compact submanifolds. arXiv preprint arXiv:2304.08241, 2023.
288"
REFERENCES,0.47910592808551994,"[15] Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, and
289"
REFERENCES,0.4800777453838678,"Maosong Sun. Sparse low-rank adaptation of pre-trained language models. arXiv preprint
290"
REFERENCES,0.48104956268221577,"arXiv:2311.11696, 2023.
291"
REFERENCES,0.48202137998056366,"[16] Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.
292"
REFERENCES,0.48299319727891155,"In Third international workshop on paraphrasing (IWP2005), 2005.
293"
REFERENCES,0.4839650145772595,"[17] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable
294"
REFERENCES,0.4849368318756074,"neural networks. arXiv preprint arXiv:1803.03635, 2018.
295"
REFERENCES,0.4859086491739553,"[18] Bin Gao, Guanghui Hu, Yang Kuang, and Xin Liu. An orthogonalization-free parallelizable
296"
REFERENCES,0.4868804664723032,"framework for all-electron calculations in density functional theory. SIAM Journal on Scientific
297"
REFERENCES,0.4878522837706511,"Computing, 44(3):B723–B745, 2022.
298"
REFERENCES,0.48882410106899904,"[19] Bin Gao, Xin Liu, Xiaojun Chen, and Ya-xiang Yuan. A new first-order algorithmic framework
299"
REFERENCES,0.4897959183673469,"for optimization problems with orthogonality constraints. SIAM Journal on Optimization,
300"
REFERENCES,0.49076773566569487,"28(1):302–332, 2018.
301"
REFERENCES,0.49173955296404276,"[20] Pengcheng He, Jianfeng Gao, and Weizhu Chen.
Debertav3: Improving deberta using
302"
REFERENCES,0.49271137026239065,"electra-style pre-training with gradient-disentangled embedding sharing.
arXiv preprint
303"
REFERENCES,0.4936831875607386,"arXiv:2111.09543, 2021.
304"
REFERENCES,0.4946550048590865,"[21] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,
305"
REFERENCES,0.4956268221574344,"Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning
306"
REFERENCES,0.4965986394557823,"for nlp. In International conference on machine learning, pages 2790–2799. PMLR, 2019.
307"
REFERENCES,0.4975704567541302,"[22] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
308"
REFERENCES,0.49854227405247814,"Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv
309"
REFERENCES,0.49951409135082603,"preprint arXiv:2106.09685, 2021.
310"
REFERENCES,0.500485908649174,"[23] Jiang Hu, Xin Liu, Zai-Wen Wen, and Ya-Xiang Yuan. A brief introduction to manifold
311"
REFERENCES,0.5014577259475219,"optimization. Journal of the Operations Research Society of China, 8:199–248, 2020.
312"
REFERENCES,0.5024295432458697,"[24] Shengding Hu, Ning Ding, Weilin Zhao, Xingtai Lv, Zhen Zhang, Zhiyuan Liu, and Maosong
313"
REFERENCES,0.5034013605442177,"Sun. Opendelta: A plug-and-play library for parameter-efficient adaptation of pre-trained
314"
REFERENCES,0.5043731778425656,"models. arXiv preprint arXiv:2307.03084, 2023.
315"
REFERENCES,0.5053449951409135,"[25] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki Markus Asano. Vera: Vector-based random
316"
REFERENCES,0.5063168124392614,"matrix adaptation. arXiv preprint arXiv:2310.11454, 2023.
317"
REFERENCES,0.5072886297376094,"[26] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.
318"
REFERENCES,0.5082604470359572,"arXiv preprint arXiv:2101.00190, 2021.
319"
REFERENCES,0.5092322643343051,"[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
320"
REFERENCES,0.5102040816326531,"arXiv:1711.05101, 2017.
321"
REFERENCES,0.511175898931001,"[28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International
322"
REFERENCES,0.5121477162293488,"Conference on Learning Representations, 2018.
323"
REFERENCES,0.5131195335276968,"[29] Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 1999.
324"
REFERENCES,0.5140913508260447,"[30] Jekaterina Novikova, Ondˇrej Duˇsek, and Verena Rieser. The e2e dataset: New challenges for
325"
REFERENCES,0.5150631681243926,"end-to-end generation. arXiv preprint arXiv:1706.09254, 2017.
326"
REFERENCES,0.5160349854227405,"[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
327"
REFERENCES,0.5170068027210885,"Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
328"
REFERENCES,0.5179786200194364,"style, high-performance deep learning library. Advances in neural information processing
329"
REFERENCES,0.5189504373177842,"systems, 32, 2019.
330"
REFERENCES,0.5199222546161322,"[32] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi
331"
REFERENCES,0.5208940719144801,"Yang. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint
332"
REFERENCES,0.521865889212828,"arXiv:2302.06476, 2023.
333"
REFERENCES,0.5228377065111759,"[33] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
334"
REFERENCES,0.5238095238095238,"Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
335"
REFERENCES,0.5247813411078717,"[34] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable
336"
REFERENCES,0.5257531584062196,"questions for squad. arXiv preprint arXiv:1806.03822, 2018.
337"
REFERENCES,0.5267249757045676,"[35] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions
338"
REFERENCES,0.5276967930029155,"for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.
339"
REFERENCES,0.5286686103012633,"[36] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y
340"
REFERENCES,0.5296404275996113,"Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a
341"
REFERENCES,0.5306122448979592,"sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural
342"
REFERENCES,0.531584062196307,"language processing, pages 1631–1642, 2013.
343"
REFERENCES,0.532555879494655,"[37] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
344"
REFERENCES,0.5335276967930029,"Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv
345"
REFERENCES,0.5344995140913509,"preprint arXiv:1804.07461, 2018.
346"
REFERENCES,0.5354713313896987,"[38] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability
347"
REFERENCES,0.5364431486880467,"judgments. Transactions of the Association for Computational Linguistics, 7:625–641, 2019.
348"
REFERENCES,0.5374149659863946,"[39] Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus
349"
REFERENCES,0.5383867832847424,"for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.
350"
REFERENCES,0.5393586005830904,"[40] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony
351"
REFERENCES,0.5403304178814383,"Moi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,
352"
REFERENCES,0.5413022351797862,"Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain
353"
REFERENCES,0.5422740524781341,"Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-
354"
REFERENCES,0.543245869776482,"art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods
355"
REFERENCES,0.54421768707483,"in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020.
356"
REFERENCES,0.5451895043731778,"Association for Computational Linguistics.
357"
REFERENCES,0.5461613216715258,"[41] Nachuan Xiao, Xin Liu, and Kim-Chuan Toh. Dissolving constraints for riemannian optimiza-
358"
REFERENCES,0.5471331389698737,"tion. Mathematics of Operations Research, 49(1):366–397, 2024.
359"
REFERENCES,0.5481049562682215,"[42] Jinming Xu, Shanying Zhu, Yeng Chai Soh, and Lihua Xie. Augmented distributed gradient
360"
REFERENCES,0.5490767735665695,"methods for multi-agent optimization under uncoordinated constant stepsizes. In 2015 54th
361"
REFERENCES,0.5500485908649174,"IEEE Conference on Decision and Control (CDC), pages 2055–2060. IEEE, 2015.
362"
REFERENCES,0.5510204081632653,"[43] Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint
363"
REFERENCES,0.5519922254616132,"arXiv:2011.14522, 2020.
364"
REFERENCES,0.5529640427599611,"[44] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient
365"
REFERENCES,0.5539358600583091,"fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199,
366"
REFERENCES,0.5549076773566569,"2021.
367"
REFERENCES,0.5558794946550049,"[45] Hui Zhang and Wotao Yin. Gradient methods for convex minimization: better rates under
368"
REFERENCES,0.5568513119533528,"weaker conditions. arXiv preprint arXiv:1303.4645, 2013.
369"
REFERENCES,0.5578231292517006,"[46] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen,
370"
REFERENCES,0.5587949465500486,"and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh
371"
REFERENCES,0.5597667638483965,"International Conference on Learning Representations, 2023.
372"
REFERENCES,0.5607385811467445,"A
Proximal smoothness
373"
REFERENCES,0.5617103984450923,"The notion of proximal smoothness, as introduced by [11], refers to the characteristic of a closed set
374"
REFERENCES,0.5626822157434402,"whereby the nearest-point projection becomes a singleton when the point is in close enough to the set.
375"
REFERENCES,0.5636540330417882,"This property facilitates algorithmic and theoretical advancements by endowing nonconvex sets with
376"
REFERENCES,0.564625850340136,"convex-like structural attributes. Specifically, for any positive real number γ, we define the γ-tube
377"
REFERENCES,0.565597667638484,"around M as UM(γ) := {x : dist(x, M) < γ}. We say a closed set M is γ-proximally smooth if
378"
REFERENCES,0.5665694849368319,"the projection operator PM(x) := argminy∈M ∥y −x∥2 is a singleton whenever x ∈UM(γ).
379"
REFERENCES,0.5675413022351797,"Obviously, any closed and convex set is proximally smooth for arbitrary γ ∈(0, ∞). According to
380"
REFERENCES,0.5685131195335277,"[11, Corollary 4.6], a closed set M is convex if and only if it is proximally smooth with a radius of γ
381"
REFERENCES,0.5694849368318756,"for every γ > 0. It is worth noting that the Stiefel manifold is 1-proximally smooth. By following the
382"
REFERENCES,0.5704567541302236,"proof in [11, Theorem 4.8],
383"
REFERENCES,0.5714285714285714,"PSt(d,r)(x) −PSt(d,r)(y)
 ≤2∥x −y∥, ∀x, y ∈¯USt(d,r)(1"
REFERENCES,0.5724003887269193,"2),
(15)"
REFERENCES,0.5733722060252673,"where ¯USt(d,r)( 1"
REFERENCES,0.5743440233236151,"2) := {x : dist(x, St(d, r)) ≤1"
REFERENCES,0.5753158406219631,"2} is the closure of USt(d,r)( 1"
REFERENCES,0.576287657920311,"2). It is worth noting
384"
REFERENCES,0.577259475218659,"that for any closed convex set M ⊂Rd×r, the projection operator PM is 1-Lipschitz continuous
385"
REFERENCES,0.5782312925170068,"over Rd×r.
386"
REFERENCES,0.5792031098153547,"B
Proofs
387"
REFERENCES,0.5801749271137027,"Proof of Lemma 1
388"
REFERENCES,0.5811467444120505,"Proof. Denote the SVD of X by X = USV ⊤. Then, it holds that dist(X, St(d, r)) = ∥X −¯X∥=
389"
REFERENCES,0.5821185617103984,"∥s −1∥2, where s = diag(S). Furthermore, we have
390"
REFERENCES,0.5830903790087464,"∇φ(X), X −¯X

=

USV ⊤(V S2V ⊤−I), USV ⊤−UV ⊤"
REFERENCES,0.5840621963070942,"=

U(S3 −S)V ⊤, U(S −I)V ⊤"
REFERENCES,0.5850340136054422,= tr((S3 −S)(S −I)) ≥3
REFERENCES,0.5860058309037901,"2∥s −1∥2
2 = 3"
REFERENCES,0.586977648202138,"2∥X −¯X∥2,"
REFERENCES,0.5879494655004859,where the last inequality is from mini si(si + 1) ≥105 64 ≥3
REFERENCES,0.5889212827988338,"2. This completes the proof.
391"
REFERENCES,0.5898931000971818,"Proof of Lemma 2
392"
REFERENCES,0.5908649173955296,Proof. Assume that ∥Xk −¯Xk∥≤1
REFERENCES,0.5918367346938775,"8. Denote the SVD of Xk by USV ⊤. Let s = diag(S). Then,
393"
REFERENCES,0.5928085519922255,we have 7
REFERENCES,0.5937803692905733,8 ≤si ≤9
REFERENCES,0.5947521865889213,"8 for any i. This implies
394"
REFERENCES,0.5957240038872692,"∥∇φ(Xk)∥2 = tr((S3 −S)2) ≤6∥Xk −¯Xk∥2.
(16)"
REFERENCES,0.5966958211856171,"Hence, we have
395"
REFERENCES,0.597667638483965,∥Xk+1 −¯Xk+1∥2 ≤∥Xk+1 −¯Xk∥2
REFERENCES,0.5986394557823129,= ∥Xk −1
REFERENCES,0.5996112730806609,3∇φ(Xk) −¯Xk∥2
REFERENCES,0.6005830903790087,= ∥Xk −¯Xk∥2 −2
REFERENCES,0.6015549076773566,"3

Xk −¯Xk, ∇φ(Xk)

+ 1"
REFERENCES,0.6025267249757046,9∥∇φ(Xk)∥2
REFERENCES,0.6034985422740525,≤(1 −1 + 2
REFERENCES,0.6044703595724004,3)∥Xk −¯Xk∥2 = 2
REFERENCES,0.6054421768707483,"3∥Xk −¯Xk∥2,"
REFERENCES,0.6064139941690962,"where the first inequality is from ¯Xk+1 = argminX∈St(d,r) ∥X −Xk∥2 and the second inequality is
396"
REFERENCES,0.6073858114674441,"due to Lemma 1 and (16).
397"
REFERENCES,0.608357628765792,"Proof of Lemma 3
398"
REFERENCES,0.60932944606414,"Proof. Due to the twice differentiability of f and the compactness of St(d, r), the inequality (7)
399"
REFERENCES,0.6103012633624878,"directly follows from [9, Lemma 2.4] and [14, Lemma 4.2], where L := Lf + Df with Lf being the
400"
REFERENCES,0.6112730806608357,"Lipschitz constant of ∇f(X) over St(d, r) and Df := maxX∈St(d,r) ∥∇f(X)∥.
401"
REFERENCES,0.6122448979591837,"For the second argument, we have
402"
REFERENCES,0.6132167152575316,"∥gradf(X) −gradf(Y )∥
≤∥PTXSt(d,r)(∇f(X)) −PTXSt(d,r)(∇f(Y ))∥+ ∥PTXSt(d,r)(∇f(Y )) −gradf(Y )∥"
REFERENCES,0.6141885325558795,≤Lf∥X −Y ∥+ 1
REFERENCES,0.6151603498542274,2∥X(X⊤∇f(Y ) + ∇f(Y )⊤X) −Y (Y ⊤∇f(Y ) + ∇f(Y )⊤Y )∥
REFERENCES,0.6161321671525753,≤Lf∥X −Y ∥+ 1
REFERENCES,0.6171039844509232,2∥X((X −Y )⊤∇f(Y ) + ∇f(Y )⊤(X −Y ))∥ + 1
REFERENCES,0.6180758017492711,2∥(X −Y )(Y ⊤∇f(Y ) + ∇f(Y )⊤Y )∥
REFERENCES,0.6190476190476191,≤Lf∥X −Y ∥+ 1
REFERENCES,0.620019436345967,2(2 ˆDf + 3 ˆDf)∥X −Y ∥
REFERENCES,0.6209912536443148,=(Lf + 5
REFERENCES,0.6219630709426628,"2
ˆDf)∥X −Y ∥,"
REFERENCES,0.6229348882410107,"where ˆDf := maxX∈¯USt(d,r)( 1"
REFERENCES,0.6239067055393586,"8 ) ∥∇f(X)∥, the second inequality is due to the contractive property
403"
REFERENCES,0.6248785228377065,"of PTXSt(d,r), and the last inequality is from the fact that ∥Y ∥2 ≤3"
REFERENCES,0.6258503401360545,2 . By setting ˆL = Lf + 5
REFERENCES,0.6268221574344023,"2 ˆDf,
404"
REFERENCES,0.6277939747327502,"we complete the proof.
405"
REFERENCES,0.6287657920310982,"Proof of Lemma 4
406"
REFERENCES,0.6297376093294461,"Proof. It follows that
407"
REFERENCES,0.630709426627794,∥Xk+1 −¯Xk+1∥≤∥Xk+1 −¯Xk∥
REFERENCES,0.6316812439261419,≤∥Xk −µφ(Xk) −¯Xk∥+ α∥gradf(Xk)∥ ≤2
REFERENCES,0.6326530612244898,3∥Xk −¯Xk∥+ α∥gradf(Xk)∥.
REFERENCES,0.6336248785228377,"We complete the proof.
408"
REFERENCES,0.6345966958211856,"Proof of Lemma 5
409"
REFERENCES,0.6355685131195336,"Proof. It follows from (7) that
410"
REFERENCES,0.6365403304178814,"f( ¯Xk+1) −f( ¯Xk) ≤

gradf( ¯Xk), ¯Xk+1 −¯Xk

+ L"
REFERENCES,0.6375121477162293,2 ∥¯Xk+1 −¯Xk∥2
REFERENCES,0.6384839650145773,"≤

gradf( ¯Xk), ¯Xk+1 −Xk+1 + Xk −¯Xk

+

gradf( ¯Xk), Xk+1 −Xk"
REFERENCES,0.6394557823129252,+ 2L∥Xk+1 −Xk∥2
REFERENCES,0.640427599611273,"≤

gradf( ¯Xk), ¯Xk+1 −Xk+1

+

gradf( ¯Xk), Xk+1 −Xk"
REFERENCES,0.641399416909621,+ 4L(α2∥gradf(Xk)∥2 + µ2∥∇φ(Xk)∥2)
REFERENCES,0.6423712342079689,"=

gradf( ¯Xk) −gradf( ¯Xk+1), ¯Xk+1 −Xk+1

+ ⟨gradf(Xk), Xk+1 −Xk⟩"
REFERENCES,0.6433430515063168,"+

gradf( ¯Xk) −gradf(Xk), Xk+1 −Xk"
REFERENCES,0.6443148688046647,+ 4L(α2∥gradf(Xk)∥2 + µ2∥∇φ(Xk)∥2)
REFERENCES,0.6452866861030127,≤2ˆL2∥Xk+1 −Xk∥2 + 1
REFERENCES,0.6462585034013606,2∥Xk+1 −¯Xk+1∥2 −α∥gradf(Xk)∥2
REFERENCES,0.6472303206997084,"−µ ⟨gradf(Xk), ∇φ(Xk)⟩+ 1"
REFERENCES,0.6482021379980564,2(ˆL2∥Xk −¯Xk∥2 + ∥Xk+1 −Xk∥2)
REFERENCES,0.6491739552964043,+ 4L(α2∥gradf(Xk)∥2 + µ2∥∇φ(Xk)∥2)
REFERENCES,0.6501457725947521,"≤−α∥gradf(Xk)∥2 −µ
D
∇f(Xk), PTXk St(d,r)(∇φ(Xk))
E
+ 1"
REFERENCES,0.6511175898931001,2∥Xk+1 −¯Xk+1∥2 + 1
REFERENCES,0.652089407191448,2∥Xk −¯Xk∥2 + (4ˆL2 + 4L + 1)(α2∥gradf(Xk)∥2 + µ2∥∇φ(Xk)∥2)
REFERENCES,0.6530612244897959,≤−(α −(4ˆL2 + 4L + 1)α2)∥gradf(Xk)∥2 + 1
REFERENCES,0.6540330417881438,2∥Xk+1 −¯Xk+1∥2
REFERENCES,0.6550048590864918,+ (6µ ˆDf + 1
REFERENCES,0.6559766763848397,"2 + 16(4ˆL2 + 4L + 1)µ2)∥Xk −¯Xk∥2, (17)"
REFERENCES,0.6569484936831875,"where the second inequality is from the 2-Lipschitz continuity of PSt(d,r) over ¯USt(d,r)( 1"
REFERENCES,0.6579203109815355,"8), the third
411"
REFERENCES,0.6588921282798834,"inequality is due to the facts that Xk −¯Xk ∈N ¯
XkSt(d, r) and ⟨A, B⟩≤1"
REFERENCES,0.6598639455782312,"2(∥A∥2 + ∥B∥2) for any
412"
REFERENCES,0.6608357628765792,"A, B ∈Rn×d, and the last inequality comes from
413"
REFERENCES,0.6618075801749271,"∥PTXk St(d,r)(∇φ(Xk))∥= ∥Xk(X⊤
k Xk −I)2∥≤6∥Xk −¯Xk∥2."
REFERENCES,0.6627793974732751,Plugging µ = 1
REFERENCES,0.6637512147716229,"3 into (17) gives (10).
414"
REFERENCES,0.6647230320699709,"Proof of Theorem.
415"
REFERENCES,0.6656948493683188,"Proof. Applying [42, Lemma 2] to (9) yields
416 K
X"
REFERENCES,0.6666666666666666,"k=0
∥Xk −¯Xk∥2 ≤18α2
K
X"
REFERENCES,0.6676384839650146,"k=0
∥gradf( ¯Xk)∥2 + 4.
(18)"
REFERENCES,0.6686103012633625,"Then, summing (10) over k = 0, . . . , K gives
417"
REFERENCES,0.6695821185617103,f( ¯Xk+1) −f( ¯X0)
REFERENCES,0.6705539358600583,"≤−(α −(4ˆL2 + 4L + 1)α2) K
X"
REFERENCES,0.6715257531584062,"k=0
∥gradf(Xk)∥2 + 1 2"
REFERENCES,0.6724975704567542,"
4 ˆDf + 16ˆL2 + 16L + 3
 K+1
X"
REFERENCES,0.673469387755102,"k=0
∥Xk −¯Xk∥2"
REFERENCES,0.67444120505345,"≤−(α −(4ˆL2 + 4L + 1)α2 + 9(4 ˆDf + 16ˆL2 + 16L + 3)α2) K
X"
REFERENCES,0.6754130223517979,"k=0
∥gradf(Xk)∥2 + 1 2"
REFERENCES,0.6763848396501457,"
4 ˆDf + 16ˆL2 + 16L + 3

(18α2∥gradf(Xk+1)∥2 + 4). (19)"
REFERENCES,0.6773566569484937,"Define c1 = 148ˆL2 + 148L + 36 ˆDf + 28 and c2 = (9 ˆD2
f + 2)(4 ˆDf + 16ˆL2 + 16L + 4). Then, we
418"
REFERENCES,0.6783284742468416,"have
419"
REFERENCES,0.6793002915451894,"α(1 −c1α) K
X"
REFERENCES,0.6802721088435374,"k=0
∥gradf(Xk)∥2 ≤f( ¯X0) −f( ¯Xk+1) + c2."
REFERENCES,0.6812439261418853,"Therefore, for any α ≤
1
2c1 , taking K →∞gives P∞
k=0 ∥gradf(Xk)∥2 < ∞. Then by (11),
420
P∞
k=0 ∥Xk −¯Xk∥2 < ∞. These lead to (11).
421"
REFERENCES,0.6822157434402333,"C
Hyperparameters
422"
REFERENCES,0.6831875607385811,Table 4: Hyperparameter setup of Manifold-LoRA for question answering tasks.
REFERENCES,0.6841593780369291,"Method
Hyperparamter
SQuADv1.1
SQuADv2.0"
REFERENCES,0.685131195335277,"Warmup Ratio
0.06
LR Schedule
Linear
Weight Decay
0.1
β1
0.9
β2
0.999
Batch Size
64
Learning Rate
3e-3
Epochs
4"
REFERENCES,0.6861030126336248,"Sphere(r=8)
µ
0.85
0.85
Lower
0.25
0.25
Upper
0.75
0.5"
REFERENCES,0.6870748299319728,"Sphere(r=16)
µ
0.9
0.85
Lower
0.25
0.25
Upper
0.5
0.5"
REFERENCES,0.6880466472303207,"Stiefel(r=8)
µ
0.85
0.85
Lower
0.25
0.25
Upper
0.5
0.5"
REFERENCES,0.6890184645286687,"Stiefel(r=16)
µ
0.9
0.85
Lower
0.25
0.25
Upper
0.5
0.5"
REFERENCES,0.6899902818270165,"Table 5: Hyperparameter configurations of Manifold-LoRA for GLUE benchmark
Method
Hyperparameter
MNLI
SST-2
CoLA
QQP
QNLI
RTE
MRPC
STS-B"
REFERENCES,0.6909620991253644,"Warmup Ratio
0.06
LR Schedule
Linear
Max Sequence Length
256
Weight Decay
0.1
β1
0.9
β2
0.999
Batch Size
32
LoRA Layer
Wq, Wv
Epochs
7
24
25
5
5
50
30
25
Learning rate
5e-4
8e-4
5e-4
5e-4
1.2e-3
1.2e-3
1e-3
2.2e-3"
REFERENCES,0.6919339164237124,"Sphere(r=16)
µ
1
0.9
0.8
0.9
0.95
1.2
0.85
0.9
Lower
0.25
0.25
0.5
0.5
0.5
0.5
1
1
Upper
2
2
2
4
2
2
4
4"
REFERENCES,0.6929057337220602,"Sphere(r=8)
µ
0.95
0.95
1
0.9
1
0.9
0.85
1
Lower
2
0.5
1
0.5
0.5
0.25
2
1
Upper
8
2
8
2
2
0.5
4
8"
REFERENCES,0.6938775510204082,"Stiefel(r=16)
µ
0.8
0.85
0.95
0.9
0.95
1.2
0.8
1
Lower
2
0.5
2
0.5
0.5
0.5
1
1
Upper
8
1
8
4
1
2
4
16"
REFERENCES,0.6948493683187561,"Stiefel(r=8)
µ
0.8
0.95
0.95
0.9
0.85
0.9
1
1
Lower
2
0.5
2
0.5
0.5
0.25
1
1
Upper
8
2
8
2
2
1
4
16"
REFERENCES,0.6958211856171039,Table 6: Hyperparameter setup of Manifold-LoRA for E2E benchmark.
REFERENCES,0.6967930029154519,"Method
Hyperparamter
GPT-2(M)
GPT-2(L)"
REFERENCES,0.6977648202137998,"Warmup Steps
500
LR Schedule
Linear
Weight Decay
0.01
β1
0.9
β2
0.999
LoRA dropout
0
Batch Size
8
Learning Rate
2e-4
Epochs
5"
REFERENCES,0.6987366375121478,"Sphere(r=4)
µ
1
0.9
Lower
0.5
0.5
Upper
2
2"
REFERENCES,0.6997084548104956,"Stiefel(r=4)
µ
1
1.1
Lower
0.5
0.5
Upper
4
2"
REFERENCES,0.7006802721088435,"NeurIPS Paper Checklist
423"
CLAIMS,0.7016520894071915,"1. Claims
424"
CLAIMS,0.7026239067055393,"Question: Do the main claims made in the abstract and introduction accurately reflect the
425"
CLAIMS,0.7035957240038873,"paper’s contributions and scope?
426"
CLAIMS,0.7045675413022352,"Answer: [Yes]
427"
CLAIMS,0.7055393586005831,"Justification: Our empirical results in Section 4 justify ours claims.
428"
CLAIMS,0.706511175898931,"Guidelines:
429"
CLAIMS,0.7074829931972789,"• The answer NA means that the abstract and introduction do not include the claims
430"
CLAIMS,0.7084548104956269,"made in the paper.
431"
CLAIMS,0.7094266277939747,"• The abstract and/or introduction should clearly state the claims made, including the
432"
CLAIMS,0.7103984450923226,"contributions made in the paper and important assumptions and limitations. A No or
433"
CLAIMS,0.7113702623906706,"NA answer to this question will not be perceived well by the reviewers.
434"
CLAIMS,0.7123420796890184,"• The claims made should match theoretical and experimental results, and reflect how
435"
CLAIMS,0.7133138969873664,"much the results can be expected to generalize to other settings.
436"
CLAIMS,0.7142857142857143,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
437"
CLAIMS,0.7152575315840622,"are not attained by the paper.
438"
LIMITATIONS,0.7162293488824101,"2. Limitations
439"
LIMITATIONS,0.717201166180758,"Question: Does the paper discuss the limitations of the work performed by the authors?
440"
LIMITATIONS,0.718172983479106,"Answer: [Yes]
441"
LIMITATIONS,0.7191448007774538,"Justification: We discuss our limitations in Section 5.
442"
LIMITATIONS,0.7201166180758017,"Guidelines:
443"
LIMITATIONS,0.7210884353741497,"• The answer NA means that the paper has no limitation while the answer No means that
444"
LIMITATIONS,0.7220602526724975,"the paper has limitations, but those are not discussed in the paper.
445"
LIMITATIONS,0.7230320699708455,"• The authors are encouraged to create a separate ”Limitations” section in their paper.
446"
LIMITATIONS,0.7240038872691934,"• The paper should point out any strong assumptions and how robust the results are to
447"
LIMITATIONS,0.7249757045675413,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
448"
LIMITATIONS,0.7259475218658892,"model well-specification, asymptotic approximations only holding locally). The authors
449"
LIMITATIONS,0.7269193391642371,"should reflect on how these assumptions might be violated in practice and what the
450"
LIMITATIONS,0.7278911564625851,"implications would be.
451"
LIMITATIONS,0.7288629737609329,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
452"
LIMITATIONS,0.7298347910592808,"only tested on a few datasets or with a few runs. In general, empirical results often
453"
LIMITATIONS,0.7308066083576288,"depend on implicit assumptions, which should be articulated.
454"
LIMITATIONS,0.7317784256559767,"• The authors should reflect on the factors that influence the performance of the approach.
455"
LIMITATIONS,0.7327502429543246,"For example, a facial recognition algorithm may perform poorly when image resolution
456"
LIMITATIONS,0.7337220602526725,"is low or images are taken in low lighting. Or a speech-to-text system might not be
457"
LIMITATIONS,0.7346938775510204,"used reliably to provide closed captions for online lectures because it fails to handle
458"
LIMITATIONS,0.7356656948493683,"technical jargon.
459"
LIMITATIONS,0.7366375121477162,"• The authors should discuss the computational efficiency of the proposed algorithms
460"
LIMITATIONS,0.7376093294460642,"and how they scale with dataset size.
461"
LIMITATIONS,0.738581146744412,"• If applicable, the authors should discuss possible limitations of their approach to
462"
LIMITATIONS,0.7395529640427599,"address problems of privacy and fairness.
463"
LIMITATIONS,0.7405247813411079,"• While the authors might fear that complete honesty about limitations might be used by
464"
LIMITATIONS,0.7414965986394558,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
465"
LIMITATIONS,0.7424684159378037,"limitations that aren’t acknowledged in the paper. The authors should use their best
466"
LIMITATIONS,0.7434402332361516,"judgment and recognize that individual actions in favor of transparency play an impor-
467"
LIMITATIONS,0.7444120505344995,"tant role in developing norms that preserve the integrity of the community. Reviewers
468"
LIMITATIONS,0.7453838678328474,"will be specifically instructed to not penalize honesty concerning limitations.
469"
THEORY ASSUMPTIONS AND PROOFS,0.7463556851311953,"3. Theory Assumptions and Proofs
470"
THEORY ASSUMPTIONS AND PROOFS,0.7473275024295433,"Question: For each theoretical result, does the paper provide the full set of assumptions and
471"
THEORY ASSUMPTIONS AND PROOFS,0.7482993197278912,"a complete (and correct) proof?
472"
THEORY ASSUMPTIONS AND PROOFS,0.749271137026239,"Answer: [Yes]
473"
THEORY ASSUMPTIONS AND PROOFS,0.750242954324587,"Justification: We provide complete proofs in Appendix B and full set of assumptions in
474"
THEORY ASSUMPTIONS AND PROOFS,0.7512147716229349,"Section 2
475"
THEORY ASSUMPTIONS AND PROOFS,0.7521865889212828,"Guidelines:
476"
THEORY ASSUMPTIONS AND PROOFS,0.7531584062196307,"• The answer NA means that the paper does not include theoretical results.
477"
THEORY ASSUMPTIONS AND PROOFS,0.7541302235179786,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
478"
THEORY ASSUMPTIONS AND PROOFS,0.7551020408163265,"referenced.
479"
THEORY ASSUMPTIONS AND PROOFS,0.7560738581146744,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
480"
THEORY ASSUMPTIONS AND PROOFS,0.7570456754130224,"• The proofs can either appear in the main paper or the supplemental material, but if
481"
THEORY ASSUMPTIONS AND PROOFS,0.7580174927113703,"they appear in the supplemental material, the authors are encouraged to provide a short
482"
THEORY ASSUMPTIONS AND PROOFS,0.7589893100097181,"proof sketch to provide intuition.
483"
THEORY ASSUMPTIONS AND PROOFS,0.7599611273080661,"• Inversely, any informal proof provided in the core of the paper should be complemented
484"
THEORY ASSUMPTIONS AND PROOFS,0.760932944606414,"by formal proofs provided in appendix or supplemental material.
485"
THEORY ASSUMPTIONS AND PROOFS,0.7619047619047619,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
486"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7628765792031098,"4. Experimental Result Reproducibility
487"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7638483965014577,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
488"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7648202137998056,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
489"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7657920310981535,"of the paper (regardless of whether the code and data are provided or not)?
490"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7667638483965015,"Answer: [Yes]
491"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7677356656948494,"Justification: We specify the training details in Section 4 and Appendix C
492"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7687074829931972,"Guidelines:
493"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7696793002915452,"• The answer NA means that the paper does not include experiments.
494"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7706511175898931,"• If the paper includes experiments, a No answer to this question will not be perceived
495"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.771622934888241,"well by the reviewers: Making the paper reproducible is important, regardless of
496"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7725947521865889,"whether the code and data are provided or not.
497"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7735665694849369,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
498"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7745383867832848,"to make their results reproducible or verifiable.
499"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7755102040816326,"• Depending on the contribution, reproducibility can be accomplished in various ways.
500"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7764820213799806,"For example, if the contribution is a novel architecture, describing the architecture fully
501"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7774538386783285,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
502"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7784256559766763,"be necessary to either make it possible for others to replicate the model with the same
503"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7793974732750243,"dataset, or provide access to the model. In general. releasing code and data is often
504"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7803692905733722,"one good way to accomplish this, but reproducibility can also be provided via detailed
505"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7813411078717201,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
506"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.782312925170068,"of a large language model), releasing of a model checkpoint, or other means that are
507"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.783284742468416,"appropriate to the research performed.
508"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7842565597667639,"• While NeurIPS does not require releasing code, the conference does require all submis-
509"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7852283770651117,"sions to provide some reasonable avenue for reproducibility, which may depend on the
510"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7862001943634597,"nature of the contribution. For example
511"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7871720116618076,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
512"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7881438289601554,"to reproduce that algorithm.
513"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7891156462585034,"(b) If the contribution is primarily a new model architecture, the paper should describe
514"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7900874635568513,"the architecture clearly and fully.
515"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7910592808551993,"(c) If the contribution is a new model (e.g., a large language model), then there should
516"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7920310981535471,"either be a way to access this model for reproducing the results or a way to reproduce
517"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.793002915451895,"the model (e.g., with an open-source dataset or instructions for how to construct
518"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.793974732750243,"the dataset).
519"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7949465500485908,"(d) We recognize that reproducibility may be tricky in some cases, in which case
520"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7959183673469388,"authors are welcome to describe the particular way they provide for reproducibility.
521"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7968901846452867,"In the case of closed-source models, it may be that access to the model is limited in
522"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7978620019436345,"some way (e.g., to registered users), but it should be possible for other researchers
523"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7988338192419825,"to have some path to reproducing or verifying the results.
524"
OPEN ACCESS TO DATA AND CODE,0.7998056365403304,"5. Open access to data and code
525"
OPEN ACCESS TO DATA AND CODE,0.8007774538386784,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
526"
OPEN ACCESS TO DATA AND CODE,0.8017492711370262,"tions to faithfully reproduce the main experimental results, as described in supplemental
527"
OPEN ACCESS TO DATA AND CODE,0.8027210884353742,"material?
528"
OPEN ACCESS TO DATA AND CODE,0.8036929057337221,"Answer: [Yes]
529"
OPEN ACCESS TO DATA AND CODE,0.8046647230320699,"Justification: We specify the code and dataset in Section 4.
530"
OPEN ACCESS TO DATA AND CODE,0.8056365403304179,"Guidelines:
531"
OPEN ACCESS TO DATA AND CODE,0.8066083576287658,"• The answer NA means that paper does not include experiments requiring code.
532"
OPEN ACCESS TO DATA AND CODE,0.8075801749271136,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
533"
OPEN ACCESS TO DATA AND CODE,0.8085519922254616,"public/guides/CodeSubmissionPolicy) for more details.
534"
OPEN ACCESS TO DATA AND CODE,0.8095238095238095,"• While we encourage the release of code and data, we understand that this might not be
535"
OPEN ACCESS TO DATA AND CODE,0.8104956268221575,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
536"
OPEN ACCESS TO DATA AND CODE,0.8114674441205053,"including code, unless this is central to the contribution (e.g., for a new open-source
537"
OPEN ACCESS TO DATA AND CODE,0.8124392614188533,"benchmark).
538"
OPEN ACCESS TO DATA AND CODE,0.8134110787172012,"• The instructions should contain the exact command and environment needed to run to
539"
OPEN ACCESS TO DATA AND CODE,0.814382896015549,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
540"
OPEN ACCESS TO DATA AND CODE,0.815354713313897,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
541"
OPEN ACCESS TO DATA AND CODE,0.8163265306122449,"• The authors should provide instructions on data access and preparation, including how
542"
OPEN ACCESS TO DATA AND CODE,0.8172983479105929,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
543"
OPEN ACCESS TO DATA AND CODE,0.8182701652089407,"• The authors should provide scripts to reproduce all experimental results for the new
544"
OPEN ACCESS TO DATA AND CODE,0.8192419825072886,"proposed method and baselines. If only a subset of experiments are reproducible, they
545"
OPEN ACCESS TO DATA AND CODE,0.8202137998056366,"should state which ones are omitted from the script and why.
546"
OPEN ACCESS TO DATA AND CODE,0.8211856171039844,"• At submission time, to preserve anonymity, the authors should release anonymized
547"
OPEN ACCESS TO DATA AND CODE,0.8221574344023324,"versions (if applicable).
548"
OPEN ACCESS TO DATA AND CODE,0.8231292517006803,"• Providing as much information as possible in supplemental material (appended to the
549"
OPEN ACCESS TO DATA AND CODE,0.8241010689990281,"paper) is recommended, but including URLs to data and code is permitted.
550"
OPEN ACCESS TO DATA AND CODE,0.8250728862973761,"6. Experimental Setting/Details
551"
OPEN ACCESS TO DATA AND CODE,0.826044703595724,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
552"
OPEN ACCESS TO DATA AND CODE,0.827016520894072,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
553"
OPEN ACCESS TO DATA AND CODE,0.8279883381924198,"results?
554"
OPEN ACCESS TO DATA AND CODE,0.8289601554907677,"Answer: [Yes]
555"
OPEN ACCESS TO DATA AND CODE,0.8299319727891157,"Justification: We specify training details in Section 4 and hyperparameters in Appendix C.
556"
OPEN ACCESS TO DATA AND CODE,0.8309037900874635,"Guidelines:
557"
OPEN ACCESS TO DATA AND CODE,0.8318756073858115,"• The answer NA means that the paper does not include experiments.
558"
OPEN ACCESS TO DATA AND CODE,0.8328474246841594,"• The experimental setting should be presented in the core of the paper to a level of detail
559"
OPEN ACCESS TO DATA AND CODE,0.8338192419825073,"that is necessary to appreciate the results and make sense of them.
560"
OPEN ACCESS TO DATA AND CODE,0.8347910592808552,"• The full details can be provided either with the code, in appendix, or as supplemental
561"
OPEN ACCESS TO DATA AND CODE,0.8357628765792031,"material.
562"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8367346938775511,"7. Experiment Statistical Significance
563"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8377065111758989,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
564"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8386783284742468,"information about the statistical significance of the experiments?
565"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8396501457725948,"Answer: [Yes]
566"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8406219630709426,"Justification: We specify these in our Section 4.
567"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8415937803692906,"Guidelines:
568"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8425655976676385,"• The answer NA means that the paper does not include experiments.
569"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8435374149659864,"• The authors should answer ”Yes” if the results are accompanied by error bars, confi-
570"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8445092322643343,"dence intervals, or statistical significance tests, at least for the experiments that support
571"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8454810495626822,"the main claims of the paper.
572"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8464528668610302,"• The factors of variability that the error bars are capturing should be clearly stated (for
573"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.847424684159378,"example, train/test split, initialization, random drawing of some parameter, or overall
574"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8483965014577259,"run with given experimental conditions).
575"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8493683187560739,"• The method for calculating the error bars should be explained (closed form formula,
576"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8503401360544217,"call to a library function, bootstrap, etc.)
577"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8513119533527697,"• The assumptions made should be given (e.g., Normally distributed errors).
578"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8522837706511176,"• It should be clear whether the error bar is the standard deviation or the standard error
579"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8532555879494655,"of the mean.
580"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8542274052478134,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
581"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8551992225461613,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
582"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8561710398445093,"of Normality of errors is not verified.
583"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8571428571428571,"• For asymmetric distributions, the authors should be careful not to show in tables or
584"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.858114674441205,"figures symmetric error bars that would yield results that are out of range (e.g. negative
585"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.859086491739553,"error rates).
586"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8600583090379009,"• If error bars are reported in tables or plots, The authors should explain in the text how
587"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8610301263362488,"they were calculated and reference the corresponding figures or tables in the text.
588"
EXPERIMENTS COMPUTE RESOURCES,0.8620019436345967,"8. Experiments Compute Resources
589"
EXPERIMENTS COMPUTE RESOURCES,0.8629737609329446,"Question: For each experiment, does the paper provide sufficient information on the com-
590"
EXPERIMENTS COMPUTE RESOURCES,0.8639455782312925,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
591"
EXPERIMENTS COMPUTE RESOURCES,0.8649173955296404,"the experiments?
592"
EXPERIMENTS COMPUTE RESOURCES,0.8658892128279884,"Answer: [Yes]
593"
EXPERIMENTS COMPUTE RESOURCES,0.8668610301263362,"Justification: We use the Hugging face and opendelta as our base code and make some
594"
EXPERIMENTS COMPUTE RESOURCES,0.8678328474246841,"modifications. We use GLUE, E2E, and Suqad three dataset.
595"
EXPERIMENTS COMPUTE RESOURCES,0.8688046647230321,"Guidelines:
596"
EXPERIMENTS COMPUTE RESOURCES,0.86977648202138,"• The answer NA means that the paper does not include experiments.
597"
EXPERIMENTS COMPUTE RESOURCES,0.8707482993197279,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
598"
EXPERIMENTS COMPUTE RESOURCES,0.8717201166180758,"or cloud provider, including relevant memory and storage.
599"
EXPERIMENTS COMPUTE RESOURCES,0.8726919339164237,"• The paper should provide the amount of compute required for each of the individual
600"
EXPERIMENTS COMPUTE RESOURCES,0.8736637512147716,"experimental runs as well as estimate the total compute.
601"
EXPERIMENTS COMPUTE RESOURCES,0.8746355685131195,"• The paper should disclose whether the full research project required more compute
602"
EXPERIMENTS COMPUTE RESOURCES,0.8756073858114675,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
603"
EXPERIMENTS COMPUTE RESOURCES,0.8765792031098154,"didn’t make it into the paper).
604"
CODE OF ETHICS,0.8775510204081632,"9. Code Of Ethics
605"
CODE OF ETHICS,0.8785228377065112,"Question: Does the research conducted in the paper conform, in every respect, with the
606"
CODE OF ETHICS,0.8794946550048591,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
607"
CODE OF ETHICS,0.880466472303207,"Answer: [Yes]
608"
CODE OF ETHICS,0.8814382896015549,"Justification: Our research is compatible with the NeurIPS Code of Ethics.
609"
CODE OF ETHICS,0.8824101068999028,"Guidelines:
610"
CODE OF ETHICS,0.8833819241982507,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
611"
CODE OF ETHICS,0.8843537414965986,"• If the authors answer No, they should explain the special circumstances that require a
612"
CODE OF ETHICS,0.8853255587949466,"deviation from the Code of Ethics.
613"
CODE OF ETHICS,0.8862973760932945,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
614"
CODE OF ETHICS,0.8872691933916423,"eration due to laws or regulations in their jurisdiction).
615"
BROADER IMPACTS,0.8882410106899903,"10. Broader Impacts
616"
BROADER IMPACTS,0.8892128279883382,"Question: Does the paper discuss both potential positive societal impacts and negative
617"
BROADER IMPACTS,0.8901846452866861,"societal impacts of the work performed?
618"
BROADER IMPACTS,0.891156462585034,"Answer: [NA]
619"
BROADER IMPACTS,0.892128279883382,"Justification: There is no societal impact of our work performed
620"
BROADER IMPACTS,0.8931000971817298,"Guidelines:
621"
BROADER IMPACTS,0.8940719144800777,"• The answer NA means that there is no societal impact of the work performed.
622"
BROADER IMPACTS,0.8950437317784257,"• If the authors answer NA or No, they should explain why their work has no societal
623"
BROADER IMPACTS,0.8960155490767736,"impact or why the paper does not address societal impact.
624"
BROADER IMPACTS,0.8969873663751214,"• Examples of negative societal impacts include potential malicious or unintended uses
625"
BROADER IMPACTS,0.8979591836734694,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
626"
BROADER IMPACTS,0.8989310009718173,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
627"
BROADER IMPACTS,0.8999028182701652,"groups), privacy considerations, and security considerations.
628"
BROADER IMPACTS,0.9008746355685131,"• The conference expects that many papers will be foundational research and not tied
629"
BROADER IMPACTS,0.901846452866861,"to particular applications, let alone deployments. However, if there is a direct path to
630"
BROADER IMPACTS,0.902818270165209,"any negative applications, the authors should point it out. For example, it is legitimate
631"
BROADER IMPACTS,0.9037900874635568,"to point out that an improvement in the quality of generative models could be used to
632"
BROADER IMPACTS,0.9047619047619048,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
633"
BROADER IMPACTS,0.9057337220602527,"that a generic algorithm for optimizing neural networks could enable people to train
634"
BROADER IMPACTS,0.9067055393586005,"models that generate Deepfakes faster.
635"
BROADER IMPACTS,0.9076773566569485,"• The authors should consider possible harms that could arise when the technology is
636"
BROADER IMPACTS,0.9086491739552964,"being used as intended and functioning correctly, harms that could arise when the
637"
BROADER IMPACTS,0.9096209912536443,"technology is being used as intended but gives incorrect results, and harms following
638"
BROADER IMPACTS,0.9105928085519922,"from (intentional or unintentional) misuse of the technology.
639"
BROADER IMPACTS,0.9115646258503401,"• If there are negative societal impacts, the authors could also discuss possible mitigation
640"
BROADER IMPACTS,0.9125364431486881,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
641"
BROADER IMPACTS,0.9135082604470359,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
642"
BROADER IMPACTS,0.9144800777453839,"feedback over time, improving the efficiency and accessibility of ML).
643"
SAFEGUARDS,0.9154518950437318,"11. Safeguards
644"
SAFEGUARDS,0.9164237123420796,"Question: Does the paper describe safeguards that have been put in place for responsible
645"
SAFEGUARDS,0.9173955296404276,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
646"
SAFEGUARDS,0.9183673469387755,"image generators, or scraped datasets)?
647"
SAFEGUARDS,0.9193391642371235,"Answer: [NA]
648"
SAFEGUARDS,0.9203109815354713,"Justification: Our work poses no such risks.
649"
SAFEGUARDS,0.9212827988338192,"Guidelines:
650"
SAFEGUARDS,0.9222546161321672,"• The answer NA means that the paper poses no such risks.
651"
SAFEGUARDS,0.923226433430515,"• Released models that have a high risk for misuse or dual-use should be released with
652"
SAFEGUARDS,0.924198250728863,"necessary safeguards to allow for controlled use of the model, for example by requiring
653"
SAFEGUARDS,0.9251700680272109,"that users adhere to usage guidelines or restrictions to access the model or implementing
654"
SAFEGUARDS,0.9261418853255587,"safety filters.
655"
SAFEGUARDS,0.9271137026239067,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
656"
SAFEGUARDS,0.9280855199222546,"should describe how they avoided releasing unsafe images.
657"
SAFEGUARDS,0.9290573372206026,"• We recognize that providing effective safeguards is challenging, and many papers do
658"
SAFEGUARDS,0.9300291545189504,"not require this, but we encourage authors to take this into account and make a best
659"
SAFEGUARDS,0.9310009718172984,"faith effort.
660"
LICENSES FOR EXISTING ASSETS,0.9319727891156463,"12. Licenses for existing assets
661"
LICENSES FOR EXISTING ASSETS,0.9329446064139941,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
662"
LICENSES FOR EXISTING ASSETS,0.9339164237123421,"the paper, properly credited and are the license and terms of use explicitly mentioned and
663"
LICENSES FOR EXISTING ASSETS,0.93488824101069,"properly respected?
664"
LICENSES FOR EXISTING ASSETS,0.9358600583090378,"Answer: [Yes]
665"
LICENSES FOR EXISTING ASSETS,0.9368318756073858,"Justification: We correctly cite the code and datasets we used in Section 4.
666"
LICENSES FOR EXISTING ASSETS,0.9378036929057337,"Guidelines:
667"
LICENSES FOR EXISTING ASSETS,0.9387755102040817,"• The answer NA means that the paper does not use existing assets.
668"
LICENSES FOR EXISTING ASSETS,0.9397473275024295,"• The authors should cite the original paper that produced the code package or dataset.
669"
LICENSES FOR EXISTING ASSETS,0.9407191448007775,"• The authors should state which version of the asset is used and, if possible, include a
670"
LICENSES FOR EXISTING ASSETS,0.9416909620991254,"URL.
671"
LICENSES FOR EXISTING ASSETS,0.9426627793974732,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
672"
LICENSES FOR EXISTING ASSETS,0.9436345966958212,"• For scraped data from a particular source (e.g., website), the copyright and terms of
673"
LICENSES FOR EXISTING ASSETS,0.9446064139941691,"service of that source should be provided.
674"
LICENSES FOR EXISTING ASSETS,0.9455782312925171,"• If assets are released, the license, copyright information, and terms of use in the
675"
LICENSES FOR EXISTING ASSETS,0.9465500485908649,"package should be provided. For popular datasets, paperswithcode.com/datasets
676"
LICENSES FOR EXISTING ASSETS,0.9475218658892128,"has curated licenses for some datasets. Their licensing guide can help determine the
677"
LICENSES FOR EXISTING ASSETS,0.9484936831875608,"license of a dataset.
678"
LICENSES FOR EXISTING ASSETS,0.9494655004859086,"• For existing datasets that are re-packaged, both the original license and the license of
679"
LICENSES FOR EXISTING ASSETS,0.9504373177842566,"the derived asset (if it has changed) should be provided.
680"
LICENSES FOR EXISTING ASSETS,0.9514091350826045,"• If this information is not available online, the authors are encouraged to reach out to
681"
LICENSES FOR EXISTING ASSETS,0.9523809523809523,"the asset’s creators.
682"
NEW ASSETS,0.9533527696793003,"13. New Assets
683"
NEW ASSETS,0.9543245869776482,"Question: Are new assets introduced in the paper well documented and is the documentation
684"
NEW ASSETS,0.9552964042759962,"provided alongside the assets?
685"
NEW ASSETS,0.956268221574344,"Answer: [NA]
686"
NEW ASSETS,0.9572400388726919,"Justification: Our paper does not release new asset.
687"
NEW ASSETS,0.9582118561710399,"Guidelines:
688"
NEW ASSETS,0.9591836734693877,"• The answer NA means that the paper does not release new assets.
689"
NEW ASSETS,0.9601554907677357,"• Researchers should communicate the details of the dataset/code/model as part of their
690"
NEW ASSETS,0.9611273080660836,"submissions via structured templates. This includes details about training, license,
691"
NEW ASSETS,0.9620991253644315,"limitations, etc.
692"
NEW ASSETS,0.9630709426627794,"• The paper should discuss whether and how consent was obtained from people whose
693"
NEW ASSETS,0.9640427599611273,"asset is used.
694"
NEW ASSETS,0.9650145772594753,"• At submission time, remember to anonymize your assets (if applicable). You can either
695"
NEW ASSETS,0.9659863945578231,"create an anonymized URL or include an anonymized zip file.
696"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.966958211856171,"14. Crowdsourcing and Research with Human Subjects
697"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.967930029154519,"Question: For crowdsourcing experiments and research with human subjects, does the paper
698"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9689018464528668,"include the full text of instructions given to participants and screenshots, if applicable, as
699"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9698736637512148,"well as details about compensation (if any)?
700"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9708454810495627,"Answer: [NA]
701"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9718172983479106,"Justification: Our study does not involve crowdsourcing nor research with human subjects.
702"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9727891156462585,"Guidelines:
703"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9737609329446064,"• The answer NA means that the paper does not involve crowdsourcing nor research with
704"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9747327502429544,"human subjects.
705"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9757045675413022,"• Including this information in the supplemental material is fine, but if the main contribu-
706"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9766763848396501,"tion of the paper involves human subjects, then as much detail as possible should be
707"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9776482021379981,"included in the main paper.
708"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9786200194363459,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
709"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9795918367346939,"or other labor should be paid at least the minimum wage in the country of the data
710"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9805636540330418,"collector.
711"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9815354713313897,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
712"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9825072886297376,"Subjects
713"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9834791059280855,"Question: Does the paper describe potential risks incurred by study participants, whether
714"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9844509232264335,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
715"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9854227405247813,"approvals (or an equivalent approval/review based on the requirements of your country or
716"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9863945578231292,"institution) were obtained?
717"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9873663751214772,"Answer: [NA]
718"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9883381924198251,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
719"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.989310009718173,"Guidelines:
720"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902818270165209,"• The answer NA means that the paper does not involve crowdsourcing nor research with
721"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912536443148688,"human subjects.
722"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9922254616132167,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
723"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9931972789115646,"may be required for any human subjects research. If you obtained IRB approval, you
724"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941690962099126,"should clearly state this in the paper.
725"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9951409135082604,"• We recognize that the procedures for this may vary significantly between institutions
726"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9961127308066083,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
727"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970845481049563,"guidelines for their institution.
728"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980563654033042,"• For initial submissions, do not include any information that would break anonymity (if
729"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990281827016521,"applicable), such as the institution conducting the review.
730"
