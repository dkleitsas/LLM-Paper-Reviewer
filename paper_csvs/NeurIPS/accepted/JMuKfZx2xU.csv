Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.004,"Measuring dependence between two random variables is of great importance in
various domains but is difficult to compute in today’s complex environments with
high-dimensional data. Recently, slicing methods have shown to be a scalable ap-
proach to measuring mutual information (MI) between high-dimensional variables
by projecting these variables into one-dimensional spaces. Unfortunately, these
methods use uniform distributions of slicing directions, which generally discard
informative features between variables and thus lead to inaccurate quantification of
dependence. In this paper, we propose a principled framework that searches for an
optimal distribution of slices for MI. Importantly, we answer theoretical questions
about finding the optimal slicing distribution in the context of MI and develop
corresponding theoretical analyses. We also develop a practical algorithm, connect-
ing our theoretical results with modern machine learning frameworks. Through
comprehensive experiments in benchmark domains, we demonstrate significant
gains in our information measure than state-of-the-art baselines."
INTRODUCTION,0.008,"1
Introduction"
INTRODUCTION,0.012,"Mutual information (MI) measures statistical dependence between two random variables by quan-
tifying the amount of information gained about one variable from an observation of the other
variable (Shannon, 1948). Despite its popularity in various fields (Marinoni and Gamba, 2017;
Hamma et al., 2016), MI suffers from the curse of dimensionality: the estimation of mutual depen-
dence is often stymied by the large dimension of the variables (Kraskov et al., 2004). Recently,
slicing methods (Bonneel et al., 2015; Nadjahi et al., 2020) have demonstrated a scalable approach for
estimating probability divergences and mutual dependence. Specifically, sliced mutual information
(SI; Goldfeld and Greenewald (2021)) measures the average of MI terms between one-dimensional
projections of the high-dimensional variables, where the projecting slicing directions are sampled
uniformly from a unit sphere. Because slicing methods do not compute the dependence directly in
the high-dimensional space but in the low-dimensional space, these methods significantly reduce
computation costs and are scalable. However, a critical problem of these slicing frameworks is
that they generally require many slices for accurate estimation. As we detail later in the paper, the
root cause of this problem is the uniform sampling of the slicing directions: discarding informative
features about the variables and generating noisy slices by equally favoring important and irrelevant
regions of the variables’ spaces. Hence, the uniform sampling fails to capture complex relation-
ships of variables which renders current slicing methods inefficient and can even lead to inaccurate
dependence quantification."
INTRODUCTION,0.016,"Our contribution.
With this insight, this paper aims to address the uniform sampling issue with
the slicing approaches for improving the dependence measurement. Specifically, motivated by the
success of Nguyen et al. (2021) in the generative modeling domain, we seek to find an optimal
distribution of slicing directions in the context of MI that satisfies two criteria: 1) slices are distributed
over maximally informative regions, and 2) slices are scattered over the unit sphere so that no"
INTRODUCTION,0.02,∗afayad@mit.edu
INTRODUCTION,0.024,"relevant directions are discarded. We formalize these criteria in a regularized optimization problem
and introduce a novel measure of dependence, the optimal sliced mutual information (denoted by
SI∗). SI∗provides a much more efficient performance compared to previous slicing methods and
dependence measures (e.g., improvements of up to 30% in dependence detection power). We show
an important benefit of SI∗, which is that it can transform the fundamental work of Goldfeld and
Greenewald (2021) (i.e. SI) to larger and more complex machine learning problems."
INTRODUCTION,0.028,"Several unique challenges arise when developing our regularized optimization problem. First, MI
requires at least two slicing variables and the optimization should be over the space of the joint
distribution of the slices, but the constraint is enforced over the marginals. We find that this challenge
essentially corresponds to solving optimal transportation (OT) (Villani, 2009) problems: optimization
over a set of couplings (of the distributions of slices) with given constraints. To that end, we leverage
OT properties that enable us to derive many interesting theorems about SI∗connecting it to MI, SI,
and differential entropy. The second challenge is the continuity problem: the cost function in OT is
assumed to be upper semicontinuous in the topology of weak convergence for the maximizing solution
to exist, but this continuity assumption no longer holds when dealing with information-theoretic
quantities. We rigorously prove that, under very mild assumptions, a maximizing solution exists for
any input variables regardless of their dimensions."
INTRODUCTION,0.032,"In summary, this paper bridges the gap between intractable information/dependence measures and
modern computational frameworks by addressing the shortcomings of current slicing approaches. We
investigate the optimality of slicing directions for MI with the following key contributions:"
INTRODUCTION,0.036,"• Formalization of a new dependence measure (Sections 3.1 and 3.2). We introduce a novel and
scalable dependence measure, SI∗with theoretical analyses of its properties and implications.
• Scalable estimator (Sections 3.3 and 4). We construct an optimal estimator with the tightest
bounds and explicitly show that the effect of the variables’ dimensions on the convergence rate is only
up to a constant factor. We then employ deep neural networks to this optimal estimator to acquire an
end-to-end SI∗neural estimator and show that it is computationally efficient.
• Comprehensive evaluation of our approach (Section 6). We demonstrate significant gains of
SI∗in the accuracy of detecting complicated relations over state-of-the-art baselines on an extensive
set of experiments. SI∗also works excellently across challenging domains (e.g., representation and
reinforcement learning), which is an unprecedented advantage and a feature that is largely missing
from current literature on statistical dependence."
BACKGROUND,0.04,"2
Background"
BACKGROUND,0.044,"Notation. We follow standard notations from Villani (2009). Specifically, this paper considers Borel
measures on Polish spaces; the latter are complete, separable metric spaces, equipped with their
Borel σ-algebra. We denote the space of Borel probability measures on X as P(X). If µ is a Borel
measure on X, and T is a Borel map X →Y, then T#µ stands for the push-forward measure of µ
by T: it is a Borel measure on Y, defined by T#µ[A] = µ[T −1(A)] for any A ⊂Y, where δx is the
Dirac delta function at x. If π(dθdφ) is a probability measure in two variables θ ∈X and φ ∈Y,
its marginal (or projection) on X (resp. Y) is the measure pΘ
#π (resp. pΦ
#π), where pΘ(θ, φ) = θ
and pΦ(θ, φ) = φ. We focus on absolutely continuous random variables with bounded density
functions. Let θ∗: x 7→θ⊤x and γγγ(X) is the uniform distribution on X. For any two probability
measures µ, ν, dµ/dν denotes the Radon-Nikodym derivative of µ with respect to ν. Furthermore,
let (ψ × ξ)(x, y) = (ψ(x), ξ(y)) be the Cartesian product of two functions ψ, ξ. Denote by C(X, Y)
the set of continuous Borel maps from X to Y. ∥.∥denotes the Euclidean norm. Finally, Sd−1 is the
unit sphere in Rd, and Ωd,k
def
= Sd−1 × Sk−1."
BACKGROUND,0.048,"Information theory. The mutual information (Shannon, 1948) characterizes the distance between
the joint distribution PX,Y and the product of its marginals PX ⊗PY :"
BACKGROUND,0.052,"I(X; Y ) = KL(PX,Y ||PX ⊗PY )) =
Z"
BACKGROUND,0.056,"X×Y
log

dPX,Y
dPX ⊗PY"
BACKGROUND,0.06,"
dPX,Y ,"
BACKGROUND,0.064,"where KL(·||·) is the Kullback–Leibler divergence (Kullback and Leibler, 1951) which is lower
semicontinuous in the topology of weak convergence (Posner, 1975). The Sliced mutual information
(Goldfeld and Greenewald, 2021) is the mean of MI terms between one-dimensional projections of"
BACKGROUND,0.068,the variables:
BACKGROUND,0.072,"SI(X; Y ) =
I"
BACKGROUND,0.076,"Ωdx,dy
I(θ⊤X; φ⊤Y )dγγγ(θ) ⊗γγγ(φ).
(1)"
BACKGROUND,0.08,"In Equation (1), X ∈Rdx, Y ∈Rdy; θ and φ are the slices corresponding to X and Y , respectively.
We note that slices are independently and uniformly sampled from Sdx−1 and Sdy−1. This uniformity
causes redundancy in slices which deems optimality beneficial and necessary for the slicing process."
DEFINITION AND THEORIES OF SLICING OPTIMALITY,0.084,"3
Definition and Theories of Slicing Optimality"
DEFINITION AND THEORIES OF SLICING OPTIMALITY,0.088,"In this section, we begin by motivating the definition of our novel measure by showing the advantage
of our solution over uniform sampling. We then formalize our proposal as a regularized optimization
problem. Lastly, we discuss our theoretical findings, such as the existence of an optimal slicing
policy and the properties of SI∗. These studies show that SI∗possesses desired characteristics of a
dependence measure."
DEFINITION AND THEORIES OF SLICING OPTIMALITY,0.092,"3.1
Definition of SI∗"
DEFINITION AND THEORIES OF SLICING OPTIMALITY,0.096,"We first motivate our solution in an example, in which we incorporate the following two optimality
criteria into SI:"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.1,"1. The projection directions are mainly concentrated into areas where the one-dimensional variables
contain the maximum mutual information possible.
2. The slicing directions are also diversified over the whole sphere, ensuring that all regions with
relevant information are visited."
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.104,"1
4
16
128
Number of Slices .5 .6 .7 .8 .9 1 AUC"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.108,SI (Optimal)
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.112,"SI (Uniform)
MI θ0 θ1 ϕ0 ϕ1"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.116,"Figure 1: ROC AUC curve (left) and visualization of the
custom distribution the slices (middle and right). Optimal
distribution of slices can capture more information and yields
a more accurate measure of dependence."
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.12,"Motivating example.
Let Z
∼
N(0, I3), Λ ∼N(0, 0.1I2), and let
X =[Z1, Z2]⊤, Y =[Z1, Z3]⊤+ Λ. In
this example, we aim to estimate the
average MI between one-dimensional
projections of the two variables X and
Y and show that our optimal distribu-
tion of slices yields superior perfor-
mance over standard SI. Specifically,
by sampling slicing directions, θ and
φ, uniformly and independently from
the 2-sphere (i.e., unit circle), we ob-
tain SI. To construct the distribution
of slices that meets the former crite-
ria, we first note that the slices which
project only the first entries of X, Y
yield the maximum information because both variables share Z1 as their first entry (up to additive
noise) while their second entries are independent. We leverage this observation and sample two-
dimensional slices such that they are mainly concentrated around |θ0| = 1, and |φ0| = 1, respectively.
We also enforce the second criterion of slices diversity by choosing a few slices away from the cluster
(See Figure 1 middle and right). We refer to Appendix D.1 for details."
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.124,"Figure 1 (left) shows the area under the curve (AUC) of the receiver operating characteristic (ROC)
as a function of the number of slices along with a visualization of the customized distribution of
θ, φ. Note the improvement in the number of slices needed until reaching an ROC AUC = 1, as our
customized measure requires ∼75% fewer slices to reach perfect accuracy. This result shows that
our proposed distribution can extract sufficient information more effectively."
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.128,"Having established the importance of optimal slicing, we now formally define the optimal sliced
mutual information (SI∗) as a weighted average of information stored in one-dimensional projections
of two random variables X, Y :"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.132,"Definition 1 (Optimal sliced mutual information). Given two random variables X ∈Rdx, Y ∈Rdy
and ωX, ωY ∈[0, π/2], define the following collection of probability measures Σd,ω = {µ : µ ∈"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.136,"P(Sd−1), Ex,y∼µ[arccos |x⊤y|] ≥ω}. The Optimal Sliced Mutual Information can be expressed as:"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.14,"SI∗(X; Y ) = sup
 I"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.144,"Ωdx,dy
I(θ⊤X; φ⊤Y )dσ(θ, φ)
: pΘ
#σ ∈Σdx,ωX, pΦ
#σ ∈Σdy,ωY"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.148,"
.
(2)"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.152,"We call the distribution σ a slicing policy. Note that pΘ
#σ refers to the first marginal distribution of σ
(in other words, the law of the random slicing vector of X, Θ, i.e. law(Θ)=pΘ
#σ); law(Θ) ∈Σdx,ωX.
This observation also applies to law(Φ) ∈Σdy,ωY . The supremum is taken to ensure that the slices
convey sufficient information to quantify the relationship between X and Y , or equivalently to ensure
that the slices are prevalent in maximally informative regions (i.e., criterion 1). The set over which the
supremum is taken, thus, refers to the joint measures whose marginals produce slices that are scattered
over the unit sphere of an appropriate dimension (i.e., criterion 2), where we use the arccos |.| to
measure slices diversity. Importantly, without the constraint, the measure might collapse to a Dirac
delta probability measure at the max slicing vector (i.e. collapse to supθ,φ I). As such, the prominent
feature of SI∗is that it reveals the maximum amount of information content stored in X, Y by
searching for a distribution of slices that results in informative, noiseless, and diverse projections.
Remark 1. SI∗is not a mutual information estimator and should not be used as a proxy of MI but
rather as a new dependency measure. In fact, although it shares many properties with MI, SI∗has
multiple advantages such as scalability and efficiency, not to mention the cutting-edge performance
on detecting complex relationships between high-dimensional random variables in nontrivial settings.
Remark 2. If dx = dy = 1, SI∗(X; Y ) = I(X; Y ). Essentially, for one-dimensional r.v.s, SI∗"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.156,boils down to MI.
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.16,"We also consider the optimality of slicing into k-dimensional subspaces by enforcing our same criteria
on distributions over the Stiefel manifold (Chikuse, 2012). Appendix E details this definition."
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.164,"3.2
Theoretical Properties of SI∗"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.168,"We propose optimizing the slicing distribution towards one-dimensional projections with higher MI
values. However, due to the lower semicontinuity of MI in the maximization problem, it is difficult
to establish whether an optimal slicing policy exists. In the following, we prove the existence of a
solution that our problem can converge to by assuming that for a given random variable X with pdf
pX,
R
∥x∥κ pX(x)dx < ∞for some κ > 1:
Theorem 1 (Existence of an optimal slicing policy). For any random variables X and Y and
any ωX, ωY
∈[0, π/2], there exists a slicing policy σ such that the information functional,
H
I(θ⊤X; φ⊤Y )dσ(θ, φ), is maximized among all possible couplings of (law(Θ), law(Φ)) ∈
Σdx,ωX × Σdy,ωY ."
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.172,"Proof. See Appendix A.1 for details.
□"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.176,"We refer to such σ as an optimal slicing policy. Theorem 1 enables us to treat the problem as a
reward-maximizing OT. Roughly speaking, SI∗(X; Y ) can be deemed as a reward-maximizing
optimal transport problem, where the reward induced by slicing vectors θ and φ is the amount of
information they reveal about X and Y . As such, the formula in Equation (2) measures the maximum
information gain possible among slicing policies."
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.18,"Rényi (1959) and Bell (1962) postulate that a measure of dependence should satisfy properties found
in Theorem 2:
Theorem 2 (Properties of SI∗). For random variables X and Y , we have:"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.184,1. SI∗(X; Y ) is nonnegative and symmetric.
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.188,2. SI∗(X; Y ) = 0 if and only if X and Y are independent.
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.192,"3. If Xn, Yn are sequences of random variables with joint distribution P (n)
X,Y that converges pointwise
to the joint distribution PX,Y , then limn→∞SI∗(Xn; Yn) = SI∗(X; Y )."
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.196,"4. Similar to MI, SI∗has a relative entropy form: SI∗(X; Y ) = sup
σ
E
(Θ,Φ)∼σ"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.2,"h
KL
 
(Θ∗×"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.204,"Φ∗)#PX,Y ||Θ∗
#PX ⊗Φ∗
#PY
i
= supσ KL
 
σ ⊗(Θ∗× Φ∗)#PX,Y ||σ ⊗Θ∗
#PX ⊗Φ∗
#PY

."
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.208,"Proof. See Appendix A.2 for details.
□"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.212,"Now that we have established that SI∗possesses the desirable properties of a dependence measure,
we emphasize the importance of using two slices:
Remark 3. Unlike existing slicing methods (Nguyen et al., 2021), SI∗requires two slicing variables
Θ and Φ since using only one slice violates the important property (2) in Theorem 2. To illustrate, Let"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.216,"X ∼N(0, I2) and Y =

0
α
−α
0"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.22,"
X, α ̸= 0, clearly X and Y are dependent but θ⊤X and θ⊤Y"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.224,"are not for all θ ∈S1 (the latter follows from the independence of the entries of X. See (Goldfeld
and Greenewald, 2021)). The observation implies that for the such dependent X, Y, SI∗(X; Y ) = 0.
We thus resort to using two slicing directions."
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.228,"Next, we use the property (4) in Theorem 2 to represent SI∗using a discriminator function.
Corollary 1 (Discriminator-based form). Let µ = (Θ∗× Φ∗)#PX,Y , ν = Θ∗
#PX ⊗Φ∗
#PY . We
write SI∗as:"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.232,"SI∗(X; Y ) = sup
σ E(Θ,Φ)Eµ
h
arg max
h"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.236,"
Eµ[log(ς ◦h(.))] −Eν[1 −log(ς ◦h(.))]
i
,"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.24,"where ς is the Sigmoid function and the discriminator h is defined on R. This GAN-type form may
be used to estimate SI∗in Reproducing Kernel Hilbert Space similarly to Ghimire et al. (2021)’s
approach for estimating KL."
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.244,"Proof. See Appendix A.2.1 for details.
□"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.248,"Moreover, we present a variational representation of SI∗which will be later used to construct a
neural estimator:
Corollary 2 (Variational representation). Let (X, Y ) ∼PX,Y and (Ψ, Υ) ∼γγγ(Ωdx,dy). We have:"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.252,"SI∗(X; Y ) =
sup
T,f1,f2"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.256,"n
E

T(Ψ, Υ, f1(Ψ, Υ)⊤X, f2(Ψ, Υ)⊤Y )

−"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.26,"log E

exp
 
T(Ψ, Υ, f1(Ψ, Υ)⊤X, f2(Ψ, Υ)⊤Y )
 o
,
(3)"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.264,"where (X, Y ) ∼PX ⊗PY . The supremum is taken over T ∈C(Ωdx,dy × R2, R), f1 ∈FωX
def
= {f :
f ∈C(Ωdx,dy, Sdx−1), f#γγγ(Ωdx,dy) ∈Σdx,ωX}, f2 ∈FωY , where FωY is defined analogously to
FωX."
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.268,"Proof. See Appendix A.2.2 for details.
□"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.272,"Corollaries 1 and 2 provide useful representations of SI∗that make it compatible with modern
machine learning algorithms and optimization models. Next, we highlight the concept of data
processing that connects SI∗to I and SI.
Theorem 3 (Data processing). For ωX, ωY ≥π/4,"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.276,"I(X; Y ) ≥SI∗(X; Y ) ≥SI(X; Y ).
(4)"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.28,"Notably, I(X; Y ) ≥supθ,φ I(θ⊤X; φ⊤Y ) ≥SI∗(X; Y ) ≥SI(X; Y ) ≥infθ,φ I(θ⊤X; φ⊤Y )."
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.284,"Proof. See Appendix A.3 for details.
□"
THE PROJECTION DIRECTIONS ARE MAINLY CONCENTRATED INTO AREAS WHERE THE ONE-DIMENSIONAL VARIABLES,0.288,"Theorem 3 shows that SI∗is a generalization of SI because, for the given values of ωX and ωY ,
Σdx,ωX and Σdy,ωY contain the uniform distributions over the respective sphere. On the other hand,
SI∗does not exceed MI as a consequence of the Data Processing Inequality. However, due to
the scalability and sample efficiency issues in MI, SI∗is a much better candidate for quantifying
dependence than MI especially when the data have complex structures."
ESTIMATION,0.292,"3.3
Estimation"
ESTIMATION,0.296,"A key property of SI∗is its scalability which is intrinsic to slicing approaches, where the latter facili-
tate estimation from samples and do not suffer from the curse of dimensionality since computations
are performed in low-dimensional subspaces. Here, we construct an optimal SI∗estimator based on"
ESTIMATION,0.3,"one-dimensional information estimators. Let bIn be a one-dimensional MI estimator over n samples
whose absolute error is uniformly bounded by δ(n):"
ESTIMATION,0.304,"sup
PS,Q
E[|I(S; Q) −bIn(S; Q)|] ≤δ(n)."
ESTIMATION,0.308,"Any optimal slicing policy σ∗that maximizes the information functional can be obtained by applying
an appropriate transformation on the uniform measure on Ωdx,dy; this transformation can be learned
using neural networks. By the definition of the push-forward measure, we express SI∗(X; Y ) as2:"
ESTIMATION,0.312,"SI∗(X; Y ) =
sup
f1∈FωX ,f2∈FωY
E(ψ,υ)∼γγγ(Ωdx,dy )

I(f1(ψ, υ)⊤X; f2(ψ, υ)⊤Y )

,"
ESTIMATION,0.316,"where FωX, FωY are defined in Corollary 2. The approximation then becomes:"
ESTIMATION,0.32,"d
SI∗
n,m(X; Y )
def
=
sup
f1∈FωX ,f2∈FωY"
M,0.324,"1
m m
X j=1"
M,0.328,"bIn(f1(ψj, υj)⊤X; f2(ψj, υj)⊤Y )

.
(5)"
M,0.332,"Theorem 4 (Convergence Rate). The uniform error bound of d
SI∗
n,m(X; Y ) is:"
M,0.336,"sup
PX,Y
E[|SI∗(X; Y ) −d
SI∗
n,m(X; Y )|] ≤δ(n) +
U
2√m,"
M,0.34,"where U ∝(d−1
x
+ d−1
y )1/2 is a constant factor."
M,0.344,"Proof. See Appendix A.4 for details.
□"
M,0.348,"We note that the estimation rate depends on the dimensions of the problem dx, dy only up to the
constant factor U. The explicit dependence of U on dx, dy has been a recent challenge which is
now solved in Theorem 4. By imposing additional regularity on PX,Y , we can obtain δ(n) ≤
C
 
(n log n)−
s
s+2 log n1−
2
p(s+2) + n−1/2
where s, p : 0 < s ≤2 ≤p, and C > 0 (see Han et al.
(2020); Goldfeld and Greenewald (2021)), which is significantly faster than the MI estimation rate,
i.e. n−1/(dx+dy)."
M,0.352,"4
Neural Estimator for SI∗"
M,0.356,"In this section, we introduce a practical implementation of SI∗(based on Corollary 2) by leveraging
a neural network estimator and connect our theoretical results in Section 3 with modern machine
learning frameworks. Let D
def
= {(Xk, Yk)}m
k=1 be a mini-batch with X ∈Rdx, Y ∈Rdy, and n =
⌊m"
M,0.36,"2 ⌋. We take (X, Y ) ∼{(Xk, Yk)}n
k=1, (X, Y ) ∼{(Xk, Yk+n)}n
k=1,3 and (Ψk, Υk) ∼γγγ(Ωdx,dy).
Then, the neural network estimator SI∗
W is computed as:"
M,0.364,"sup
T,f1,f2  1 n n
X"
M,0.368,"k=1
T(Ψk, Υk, f (k)
1"
M,0.372,"⊤Xk, f (k)
2"
M,0.376,"⊤Yk) −log 1 n n
X"
M,0.38,"k=1
exp

T(Ψk, Υk, f (k)
1"
M,0.384,"⊤Xk, f (k)
2"
M,0.388,"⊤Y k)
 + λ1  1 n2
X"
M,0.392,"k,j
arccos |f (k)
1"
M,0.396,"⊤f (j)
1 | −ωX"
M,0.4,"
+ λ2  1 n2
X"
M,0.404,"k,j
arccos |f (k)
2"
M,0.408,"⊤f (j)
2 | −ωY 
, (6)"
M,0.412,"where f (k)
i
= fi(Ψk, Υk) for i = 1, 2; k ∈[n]. λ1, λ2 > 0, and W
def
= [wwwT ,www1,www2] are the
parameters of neural networks T, f1, f2, respectively. We also follow Song and Ermon (2020) and
account for the possible high-variance issues by using a smoothed version of Equation (6), where
we clip the exp(·) term in the second line between exp(−ρ) and exp(ρ) where ρ > 0. We refer
to Appendix C for the pseudocode of SI∗
W. We later analyze the computational complexity of the
estimator."
M,0.416,"2See detailed mathematical explanation in our proof of Corollary 2.
3Alternatively, one can take (X, Y ) ∼{(Xk, Yk)}n
k=1, and (X, Y ) ∼{(Xk, Yσ(k))}n
k=1, σ(k) ̸= k, σ ∈S
(the group of n-permutations)."
M,0.42,".5
.6
.7
.8
.9
1 .5 .6 .7 .8 .9 1 AUC"
M,0.424,Linear
M,0.428,".5
.6
.7
.8
.9
1"
M,0.432,Parabolic
M,0.436,".5
.6
.7
.8
.9
1"
M,0.44,Elliptical
M,0.444,".5
.6
.7
.8
.9
1"
M,0.448,Sinusoidal
M,0.452,".5
.6
.7
.8
.9
1"
M,0.456,Two Sinusoidals
M,0.46,".5
.6
.7
.8
.9
1
Noise Ratio .5 .6 .7 .8 .9 1 AUC"
M,0.464,Common Signal
M,0.468,".5
.6
.7
.8
.9
1
Noise Ratio"
M,0.472,"3
Hypocycloid (k = 4)"
M,0.476,".5
.6
.7
.8
.9
1
Noise Ratio"
M,0.48,"3
Epicycloid (k=1)"
M,0.484,".5
.6
.7
.8
.9
1
Noise Ratio"
M,0.488,Spherical Harmonic 1
M,0.492,".5
.6
.7
.8
.9
1
Noise Ratio"
M,0.496,Spherical Harmonic 2
M,0.5,"Figure 2: Statistical efficiency of dependence measures with structures of varying complexity
X ∈R3, Y ∈R3. The AUC ROC quantifies the measure’s discriminative ability, where a perfect
classifier has AUC ROC of 1.0 while a random classifier has AUC ROC of 0.5. The figures are
computed from 100 random runs."
RELATED WORK,0.504,"5
Related Work"
RELATED WORK,0.508,"The main idea in quantifying dependence between random variables X, Y is to characterize the
distance between their joint distribution PX,Y and the product of its marginals PX ⊗PY . Naturally,
this problem has been studied in the area of optimal transportation theory (Mordant and Segers,
2022; Wiesel, 2021; Chuang et al., 2023). For example, Nies et al. (2021) defined the distance as the
optimal transport cost on the spaces of the aforementioned distributions for a restricted set of cost
functions. Other popular approaches study the distance covariance, which is the weighted Euclidean
distance between the joint characteristic function and the product of the marginals’ characteristic
functions (Székely et al., 2007). Also, Reshef et al. (2011) introduced MIC, a measure of dependence
for the 2-variable relationships based on testing multiple grids with varying dimensions and searching
for the grid with maximum information between the two variables. Notably, (Liu et al., 2021) benefit
from the optimal transportation theory to calculate Squared-loss Mutual Information with a small
number of paired samples (drawn from the joint distribution) and a large number of unpaired samples.
Lastly, MID (Sugiyama and Borgwardt, 2013) uses the concept of fractal dimensions (Ott, 2002) to
define the information dimension and shows benefits in various applications."
RELATED WORK,0.512,"In the context of sliced statistical distances, Nguyen et al. (2021) proposed a similar solution to
alleviate the problem of using a large number of slices and applied it to the Wasserstein (W-) distance
by reformulating the distance as an optimization problem. While we share a similar motivation, the
two slicing approaches have significant differences stemming from the fact that Sliced W-distances
have only been studied in the context of generative modeling; they only require one slicing direction;
and their cost function (1-Wasserstein distance) is continuous, which makes their problem a vanilla
expectation maximization (EM) problem. In a different domain, Nguyen et al. (2020) proposed
an improvement to relational regularized autoencoders by finding an important area of projections
characterized by a von Mises-Fisher (vMF) distribution (Jupp and Mardia, 1979). Optimizing over
the family of vMF distributions helps to identify important directions on the sphere (ϵ parameter)
and how much weight to put there compared to other directions (κ parameter). We, however, do not
restrict our optimization to a particular family of distributions since the optimal slicing policy might
not take the form of vMF."
EXPERIMENTS,0.516,"6
Experiments"
EXPERIMENTS,0.52,"We conduct extensive experiments to demonstrate SI∗’s efficacy. We compare SI∗’s dependence
quantification quality to a diverse set of competitive baselines. Through careful analyses, we show
SI∗’s scalability, computation speed, sample efficiency, and slicing efficiency. We further provide
empirical evidence that SI∗works excellently across challenging representation learning and rein-"
EXPERIMENTS,0.524,"10
102
103 n .5 .6 .7 .8 .9 1 AUC"
EXPERIMENTS,0.528,Common Signal
EXPERIMENTS,0.532,"10
102
103 m"
EXPERIMENTS,0.536,Common Signal
EXPERIMENTS,0.54,"10
102
103
104 n"
EXPERIMENTS,0.544,Elliptical
EXPERIMENTS,0.548,"10
102
103 m"
EXPERIMENTS,0.552,Elliptical
EXPERIMENTS,0.556,"Figure 3: Statistical efficiency of SI∗, SI, and MI with dimension, number of samples (n), and
number of slices (m). The figures with respect to m do not include MI because this method does not
use slices."
EXPERIMENTS,0.56,"forcement learning tasks. We refer to Appendix B for further analysis experiments and Appendix D
for implementation details and hyperparameters provided at https://bit.ly/3foLke2."
EXPERIMENTS,0.564,"6.1
Effectiveness of SI∗as a Dependence Measure"
EXPERIMENTS,0.568,"We first illustrate the superiority of SI∗as a dependence measure by comparing against the following
state-of-the-art measures: Mutual Information (MI) (Shannon, 1948; Kraskov et al., 2004), Sliced Mu-
tual Information (SI) (Goldfeld and Greenewald, 2021), distributional sliced Wasserstein Dependence
(WD) (Nguyen et al., 2021), Transport Dependency Coefficient (TD) (a.k.a Transport Correlation)
(Nies et al., 2021), Mutual Information Dimension (MID) (Sugiyama and Borgwardt, 2013), Maximal
Information Coefficient (Reshef et al., 2011), and Distance Correlation (dCor) (Székely et al., 2007).
We perform ROC curve analysis to examine precision and recall in recognizing complex relationship
types with the existence of uniformly distributed noise. We test on synthetic data that comprises
positive samples that depict extremely complicated structures of PX,Y ∈P(R3 × R3). In this setting,
we conduct 10 statistical tests and cover a wide range of structures, including sinusoidal features,
elliptical surfaces, spherical harmonic surfaces, and hyper-hypocycloids. We provide the full details
(e.g. the mappings formulae) in Appendix D.2."
EXPERIMENTS,0.572,"Results are reported in Figure 2 as a function of the noise ratio. Notably, SI∗surpassed all other
dependency measures in discerning structure from noise and sometimes by a significant margin (up
to 30% improvement in accuracy). Although SI∗has shown similar performance to some of the
baselines in the linear and parabolic settings, a clear superiority of SI∗is manifested in the eight
complicated geometries, such as the spherical harmonics and sinusoidals. As a result, Figure 2 clearly
shows the advantages of our slicing optimality in highly non-trivial settings."
EXPERIMENTS,0.576,"6.2
Sample and Slicing Efficiency of SI∗"
EXPERIMENTS,0.58,"Supporting our theory in Section 3.3, we show that SI∗scales efficiently to higher dimensions. We
test against SI and MI estimators (Kraskov et al., 2004) on two structures of common signal and
elliptical. We calculate the AUC of the ROC while varying the number of samples or projections.
Figure 3 shows that SI∗attains the perfect accuracy with less number of slices compared to the
uniform-sampling-based baselines. As such, this empirical result supports our claim that SI∗"
EXPERIMENTS,0.584,"succeeds in detecting the dependence between the variables with an improved sample and projection
efficiency. Appendix B.2 provides additional experiments in higher dimensions."
RESULT ON REPRESENTATION LEARNING,0.588,"6.3
Result on Representation Learning"
RESULT ON REPRESENTATION LEARNING,0.592,"Representation learning employs MI to discover useful representations by training a neural network
encoder to maximize MI between its inputs and outputs. For example, Hjelm et al. (2019) introduces
Deep InfoMax (DIM), a popular method for learning unsupervised representations, which improves
the learning by including knowledge about the local structure of the input into the objective. We
follow DIM settings and substitute MI with SI and SI∗as the information measure, respectively.
We test these three methods along with BiGAN (Donahue et al., 2016) on the STL-10 (Coates et al.,
2011) and CIFAR10 (Krizhevsky et al., 2009) datasets, which consist of high-dimensional images.
Results in Tables 1 and 2 show that SI∗outperforms the baselines for all classifiers: high-level vector"
RESULT ON REPRESENTATION LEARNING,0.596,"representation (Y ), the output of the previous fully-connected layer (fc) and the last convolutional
layer (conv). As such, this experiment highlights the scalability aspect of SI∗with high-dimensional
variables not to mention its excellent performance on a challenging ML task."
RESULT ON REPRESENTATION LEARNING,0.6,Table 1: Classification accuracy (%) on STL-10
RESULT ON REPRESENTATION LEARNING,0.604,"conv
fc
Y
BiGAN
71.53
67.18
58.48
DIM (MI)
69.15
63.81
61.92
DIM (SI)
74.54
71.34
68.90
DIM (SI∗)
76.89
71.67
70.04"
RESULT ON REPRESENTATION LEARNING,0.608,Table 2: Classification accuracy (%) on CIFAR10
RESULT ON REPRESENTATION LEARNING,0.612,"conv
fc
Y
BiGAN
62.57
62.74
52.54
DIM (MI)
72.66
70.66
64.71
DIM (SI)
74.37
70.23
65.99
DIM (SI∗)
77.01
70.39
69.04"
RESULT ON REINFORCEMENT LEARNING,0.616,"6.4
Result on Reinforcement Learning"
RESULT ON REINFORCEMENT LEARNING,0.62,"Previous frameworks have studied MI objectives as regularizers to the reinforcement learning (RL)
objective that involves high-dimensional variables of states, actions, and rewards (Nachum et al.,
2019; Schwarzer et al., 2021). Since it is practically difficult to estimate MI in high dimensions,
most works resort to learning representations of the high-dimensional data by projecting them into
low-dimensional embeddings. Thanks to the slicing technique, these representations are no longer
needed when using SI∗. We empirically evaluate the scalability of SI∗with an RL task, where we
adapt the forward information objective in Rakelly et al. (2021) as:"
RESULT ON REINFORCEMENT LEARNING,0.624,"Jπ
SI∗= max
π
SI∗(St+1; [At, St]),
(7)"
RESULT ON REINFORCEMENT LEARNING,0.628,"where π denotes the agent’s policy, At ∼π(St), St+1 ∼T(St, At), and T is the transition dynamics.
For SI, we use the same objective (by replacing SI∗with the dependence measure SI(.; .)). We
run tests on three challenging Gym environments (Brockman et al., 2016), where the environment
dimensions (observation dim, action dim) are as follows: Humanoid-v3: (378, 17); Ant-v3: (113, 8);
Hopper-v3: (12, 3). Further details can be found in Appendix D.3. We report the results comparing
SI∗, SI, and the original paper objective (MI). Figure 4 shows clear gains (up to 25%) with a
noticeable learning speed of SI∗compared to the baselines."
RESULT ON REINFORCEMENT LEARNING,0.632,"0.0
0.2
0.4
0.6
0.8
1
Timesteps (1e7) 0 2000 4000 6000 8000 10000"
RESULT ON REINFORCEMENT LEARNING,0.636,Return
RESULT ON REINFORCEMENT LEARNING,0.64,(a) Humanoid-v3
RESULT ON REINFORCEMENT LEARNING,0.644,"0.0
0.2
0.4
0.6
0.8
1
Timesteps (1e7) 0 1000 2000 3000 4000 5000 6000 7000 8000"
RESULT ON REINFORCEMENT LEARNING,0.648,(b) Ant-v3
RESULT ON REINFORCEMENT LEARNING,0.652,"0.0
0.2
0.4
0.6
0.8
1
Timesteps (1e7) 1000 1500 2000 2500 3000 3500 4000 4500"
RESULT ON REINFORCEMENT LEARNING,0.656,(c) Hopper-v3
RESULT ON REINFORCEMENT LEARNING,0.66,"Figure 4: Results on high-dimensional Reinforcement Learning environments show a cutting-edge
performance of SI∗on complex high-dimensional control problems. The mean and variance
computed for 12 seeds are shown in the figures."
BEHAVIOR ANALYSIS,0.664,"6.5
Behavior Analysis"
BEHAVIOR ANALYSIS,0.668,"Convergence of SI∗. We validate the convergence rate of SI∗(Theorem 4). In Figure 5, we show
the RMSE between the ground truth SI∗and the estimated in the setting where X and Y are normal
random variables with 5 overlapping entries: Z ∼N(0, I15), Λ ∼N(0, 0.1I10), X = Z[1,10], Y =
Z[6,15] + Λ. The heat map in Figure 5a shows convergence when n, m (# samples and # slices,
respectively) vary independently, while Figure 5b shows when one parameter is varying and the other
is set to 103. The MI estimator used is the Kozachenko–Leonenko estimator (Kraskov et al., 2004).
We also test with X, Y ∈Rd with linear dependence to study the effect of the dimension on the
convergence. Results are reported in Figure 5c with n and m varying together."
BEHAVIOR ANALYSIS,0.672,"Computational complexity. Results in Figure 6 (left) show that the computational complexity of
SI∗is of an order similar to that of SI. Note that, while SI∗is slower than SI, the difference is"
BEHAVIOR ANALYSIS,0.676,"10
32
100
320
1000
Number of Samples"
BEHAVIOR ANALYSIS,0.68,"10
32
100
320
1000
Number of Slices 0.05 0.10 0.15 0.20 RMSE (a)"
BEHAVIOR ANALYSIS,0.684,"10
102
103"
BEHAVIOR ANALYSIS,0.688,"n (resp. m) 10
3 10
2 10
1 100 RMSE"
BEHAVIOR ANALYSIS,0.692,"SI *  (m=103, n varies)"
BEHAVIOR ANALYSIS,0.696,"SI *  (n=103, m varies) MI (b)"
BEHAVIOR ANALYSIS,0.7,"10
102
103 n=m 10
2 10
1 100 RMSE"
BEHAVIOR ANALYSIS,0.704,"d=10
d=20
d=50
d=100 (c)"
BEHAVIOR ANALYSIS,0.708,"Figure 5: Figure shows the RMSE with varying n, m and variables dimensions."
BEHAVIOR ANALYSIS,0.712,"insignificant not to mention the clear advantage of SI∗over other baselines given its superiority in
detecting complex dependencies. We also analyze the effect of number of gradient update steps of
f1, f2 on the computation speed (Figure 6 (right)). Further results can be found in Appendix B."
BEHAVIOR ANALYSIS,0.716,"29
210
211
212
213
n 2−3 2−2 2−1 20 21"
BEHAVIOR ANALYSIS,0.72,Second
BEHAVIOR ANALYSIS,0.724,SI (m = 700)
BEHAVIOR ANALYSIS,0.728,SI∗(m = 700)
BEHAVIOR ANALYSIS,0.732,SI (m = 1000)
BEHAVIOR ANALYSIS,0.736,SI∗(m = 1000)
BEHAVIOR ANALYSIS,0.74,"29
210
211
212
213
n 2−3 2−2 2−1 20 21 22"
BEHAVIOR ANALYSIS,0.744,Second
ITERATION,0.748,"10 Iteration
20 Iteration
50 Iteration"
ITERATION,0.752,"Figure 6: (Left) Computation speed of measures based on the number of samples and slices (depen-
dency is linear and noise ratio = 0.6). (Right) Measures with respect to the number of update steps
(m = 700)."
CONCLUDING REMARKS,0.756,"7
Concluding Remarks"
CONCLUDING REMARKS,0.76,"We presented a novel dependence measure, called SI∗, that is scalable to high dimensions while
being efficient regarding time, sample, and slicing complexity. We discussed its theoretical properties,
proved that its estimation error depends on the problem dimensions only up to a constant factor,
and empirically validated the competence of SI∗in detecting complicated dependencies against
state-of-the-art dependence measures. We further placed SI∗into modern ML, where we proved the
adequacy of our dependence measure on a more onerous set of tasks."
CONCLUDING REMARKS,0.764,"Limitations. Since the aim of this paper was to introduce optimality into slicing methods, SI∗"
CONCLUDING REMARKS,0.768,"is formalized to use two slicing variables. Although the current setting has proved effective and
rather sufficient in complex machine learning scenarios, the extension into multivariate settings is
encouraged for future works."
CONCLUDING REMARKS,0.772,"Broader impact. The findings and methodologies developed through this work can significantly
improve the way we understand and analyze systems with complex relations. We believe this work
will have noticeable impact on broader communities, since the study of quantifying information
is active in research areas including physics, statistics, computational biology, economics, and
neuroscience. At its current form, this work is mainly theoretical and does not impose any negative
societal impact."
REFERENCES,0.776,References
REFERENCES,0.78,"Bell, C. (1962). Mutual information and maximal correlation as measures of dependence. The Annals
of Mathematical Statistics, pages 587–595."
REFERENCES,0.784,"Billingsley, P. (2013). Convergence of probability measures. John Wiley & Sons."
REFERENCES,0.788,"Bonneel, N., Rabin, J., Peyré, G., and Pfister, H. (2015). Sliced and radon wasserstein barycenters of
measures. Journal of Mathematical Imaging and Vision, 51(1):22–45."
REFERENCES,0.792,"Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W.
(2016). Openai gym. arXiv preprint arXiv:1606.01540."
REFERENCES,0.796,"Chikuse, Y. (2012). Statistics on Special Manifolds, volume 174. Springer Science & Business
Media."
REFERENCES,0.8,"Chuang, C.-Y., Jegelka, S., and Alvarez-Melis, D. (2023). Infoot: Information maximizing optimal
transport. In International Conference on Machine Learning, pages 6228–6242. PMLR."
REFERENCES,0.804,"Coates, A., Ng, A., and Lee, H. (2011). An analysis of single-layer networks in unsupervised feature
learning. In Proceedings of the fourteenth international conference on artificial intelligence and
statistics, pages 215–223. JMLR Workshop and Conference Proceedings."
REFERENCES,0.808,"Donahue, J., Krähenbühl, P., and Darrell, T. (2016). Adversarial feature learning. arXiv preprint
arXiv:1605.09782."
REFERENCES,0.812,"Donsker, M. D. and Varadhan, S. S. (1975). Asymptotic evaluation of certain markov process
expectations for large time, i. Communications on Pure and Applied Mathematics, 28(1):1–47."
REFERENCES,0.816,"Fujimoto, S., Hoof, H., and Meger, D. (2018). Addressing function approximation error in actor-critic
methods. In International conference on machine learning, pages 1587–1596. PMLR."
REFERENCES,0.82,"Ghimire, S., Masoomi, A., and Dy, J. (2021). Reliable estimation of kl divergence using a discrimi-
nator in reproducing kernel hilbert space. Advances in Neural Information Processing Systems,
34:10221–10233."
REFERENCES,0.824,"Ghourchian, H., Gohari, A., and Amini, A. (2017). Existence and continuity of differential entropy
for a class of distributions. IEEE Communications Letters, 21(7):1469–1472."
REFERENCES,0.828,"Godavarti, M. and Hero, A. (2004). Convergence of differential entropies. IEEE Transactions on
Information Theory, 50(1):171–176."
REFERENCES,0.832,"Goldfeld, Z. and Greenewald, K. (2021). Sliced mutual information: A scalable measure of statistical
dependence. Advances in Neural Information Processing Systems, 34."
REFERENCES,0.836,"Goldfeld, Z., Greenewald, K., Nuradha, T., and Reeves, G. (2022). k-sliced mutual information: A
quantitative study of scalability with dimension. arXiv preprint arXiv:2206.08526."
REFERENCES,0.84,"Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. In International conference on
machine learning, pages 1861–1870. PMLR."
REFERENCES,0.844,"Hamma, A., Giampaolo, S. M., and Illuminati, F. (2016). Mutual information and spontaneous
symmetry breaking. Phys. Rev. A, 93:012303."
REFERENCES,0.848,"Han, Y., Jiao, J., Weissman, T., and Wu, Y. (2020). Optimal rates of entropy estimation over lipschitz
balls. The Annals of Statistics, 48(6):3228–3250."
REFERENCES,0.852,"Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., and Bengio,
Y. (2019). Learning deep representations by mutual information estimation and maximization. In
International Conference on Learning Representations."
REFERENCES,0.856,"Ioffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pages 448–456.
PMLR."
REFERENCES,0.86,"Jupp, P. E. and Mardia, K. V. (1979). Maximum likelihood estimators for the matrix von mises-fisher
and bingham distributions. The Annals of Statistics, 7(3):599–606."
REFERENCES,0.864,"Kraskov, A., Stögbauer, H., and Grassberger, P. (2004). Estimating mutual information. Phys. Rev. E,
69:066138."
REFERENCES,0.868,"Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images."
REFERENCES,0.872,"Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classification with deep convolu-
tional neural networks. Advances in neural information processing systems, 25."
REFERENCES,0.876,"Kullback, S. and Leibler, R. A. (1951). On information and sufficiency. The annals of mathematical
statistics, 22(1):79–86."
REFERENCES,0.88,"Levy, P. S. (1955). Théorie de l’addition des variables aléatoires. The Mathematical Gazette, 39:344."
REFERENCES,0.884,"Liu, Y., Yamada, M., Tsai, Y.-H. H., Le, T., Salakhutdinov, R., and Yang, Y. (2021). Lsmi-sinkhorn:
Semi-supervised mutual information estimation with optimal transport. In Machine Learning and
Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021,
Bilbao, Spain, September 13–17, 2021, Proceedings, Part I 21, pages 655–670. Springer."
REFERENCES,0.888,"Loeve, M. (2017). Probability theory. Courier Dover Publications."
REFERENCES,0.892,"Marinoni, A. and Gamba, P. (2017). Unsupervised data driven feature extraction by means of mutual
information maximization. IEEE Transactions on Computational Imaging, 3(2):243–253."
REFERENCES,0.896,"Mescheder, L., Nowozin, S., and Geiger, A. (2017).
Adversarial variational bayes: Unifying
variational autoencoders and generative adversarial networks. In International Conference on
Machine Learning, pages 2391–2400. PMLR."
REFERENCES,0.9,"Mordant, G. and Segers, J. (2022). Measuring dependence between random vectors via optimal
transport. Journal of Multivariate Analysis, 189:104912."
REFERENCES,0.904,"Nachum, O., Gu, S., Lee, H., and Levine, S. (2019). Near-optimal representation learning for
hierarchical reinforcement learning. In International Conference on Learning Representations."
REFERENCES,0.908,"Nadjahi, K., Durmus, A., Chizat, L., Kolouri, S., Shahrampour, S., and Simsekli, U. (2020). Statistical
and topological properties of sliced probability divergences. Advances in Neural Information
Processing Systems, 33:20802–20812."
REFERENCES,0.912,"Nguyen, K., Ho, N., Pham, T., and Bui, H. (2021). Distributional sliced-wasserstein and applications
to generative modeling. In International Conference on Learning Representations."
REFERENCES,0.916,"Nguyen, K., Nguyen, S., Ho, N., Pham, T., and Bui, H. (2020). Improving relational regularized
autoencoders with spherical sliced fused gromov wasserstein. arXiv preprint arXiv:2010.01787."
REFERENCES,0.92,"Nies, T. G., Staudt, T., and Munk, A. (2021). Transport dependency: Optimal transport based
dependency measures. arXiv preprint arXiv:2105.02073."
REFERENCES,0.924,"Ott, E. (2002). Chaos in dynamical systems. Cambridge university press."
REFERENCES,0.928,"Piera, F. J. and Parada, P. (2009). On convergence properties of shannon entropy. Problems of
Information Transmission, 45(2):75–94."
REFERENCES,0.932,"Popoviciu, T. (1935). Sur les équations algébriques ayant toutes leurs racines réelles. Mathematica,
9(129-145):20."
REFERENCES,0.936,"Posner, E. (1975). Random coding strategies for minimum entropy. IEEE Transactions on Information
Theory, 21(4):388–391."
REFERENCES,0.94,"Rakelly, K., Gupta, A., Florensa, C., and Levine, S. (2021). Which mutual-information representation
learning objectives are sufficient for control? Advances in Neural Information Processing Systems,
34."
REFERENCES,0.944,"Rényi, A. (1959). On measures of dependence. Acta Mathematica Academiae Scientiarum Hungarica,
10:441–451."
REFERENCES,0.948,"Reshef, D. N., Reshef, Y. A., Finucane, H. K., Grossman, S. R., McVean, G., Turnbaugh, P. J., Lander,
E. S., Mitzenmacher, M., and Sabeti, P. C. (2011). Detecting novel associations in large data sets.
science, 334(6062):1518–1524."
REFERENCES,0.952,"Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347."
REFERENCES,0.956,"Schwarzer, M., Anand, A., Goel, R., Hjelm, R. D., Courville, A., and Bachman, P. (2021). Data-
efficient reinforcement learning with self-predictive representations. In International Conference
on Learning Representations."
REFERENCES,0.96,"Shannon, C. E. (1948). A mathematical theory of communication. The Bell system technical journal,
27(3):379–423."
REFERENCES,0.964,"Sønderby, C. K., Caballero, J., Theis, L., Shi, W., and Huszár, F. (2016). Amortised map inference
for image super-resolution. arXiv preprint arXiv:1610.04490."
REFERENCES,0.968,"Song, J. and Ermon, S. (2020). Understanding the limitations of variational mutual information
estimators. In International Conference on Learning Representations."
REFERENCES,0.972,"Sugiyama, M. and Borgwardt, K. M. (2013). Measuring statistical dependence via the mutual
information dimension. In Twenty-Third International Joint Conference on Artificial Intelligence.
Citeseer."
REFERENCES,0.976,"Székely, G. J., Rizzo, M. L., and Bakirov, N. K. (2007). Measuring and testing dependence by
correlation of distances. The annals of statistics, 35(6):2769–2794."
REFERENCES,0.98,"Todorov, E., Erez, T., and Tassa, Y. (2012). Mujoco: A physics engine for model-based control. In
2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033."
REFERENCES,0.984,"Villani, C. (2009). Optimal transport: old and new, volume 338. Springer."
REFERENCES,0.988,"Wiesel, J. (2021).
Measuring association with wasserstein distances.
arXiv preprint
arXiv:2102.00356."
REFERENCES,0.992,"Yang, P. and Chen, B. (2018). Robust kullback-leibler divergence and universal hypothesis testing
for continuous distributions. IEEE Transactions on Information Theory, 65(4):2360–2373."
REFERENCES,0.996,"Yoo, A. B., Jette, M. A., and Grondona, M. (2003). Slurm: Simple linux utility for resource
management. In Workshop on job scheduling strategies for parallel processing, pages 44–60.
Springer."
