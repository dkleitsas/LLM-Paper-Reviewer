Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009541984732824427,"Model quantization is widely applied for compressing and accelerating deep neural
1"
ABSTRACT,0.0019083969465648854,"networks (DNNs). However, conventional Quantization-Aware Training (QAT)
2"
ABSTRACT,0.0028625954198473282,"focuses on training DNNs with uniform bit-width. The bit-width settings vary
3"
ABSTRACT,0.003816793893129771,"across different hardware and transmission demands, which induces considerable
4"
ABSTRACT,0.004770992366412214,"training and storage costs. Hence, the scheme of one-shot joint training multiple
5"
ABSTRACT,0.0057251908396946565,"precisions is proposed to address this issue. Previous works either store a larger
6"
ABSTRACT,0.006679389312977099,"FP32 model to switch between different precision models for higher accuracy or
7"
ABSTRACT,0.007633587786259542,"store a smaller INT8 model but compromise accuracy due to using shared quanti-
8"
ABSTRACT,0.008587786259541985,"zation parameters. In this paper, we introduce the Double Rounding quantization
9"
ABSTRACT,0.009541984732824428,"method, which fully utilizes the quantized representation range to accomplish
10"
ABSTRACT,0.01049618320610687,"nearly lossless bit-switching while reducing storage by using the highest integer
11"
ABSTRACT,0.011450381679389313,"precision instead of full precision. Furthermore, we observe a competitive inter-
12"
ABSTRACT,0.012404580152671756,"ference among different precisions during one-shot joint training, primarily due
13"
ABSTRACT,0.013358778625954198,"to inconsistent gradients of quantization scales during backward propagation. To
14"
ABSTRACT,0.01431297709923664,"tackle this problem, we propose an Adaptive Learning Rate Scaling (ALRS) tech-
15"
ABSTRACT,0.015267175572519083,"nique that dynamically adapts learning rates for various precisions to optimize the
16"
ABSTRACT,0.016221374045801526,"training process. Additionally, we extend our Double Rounding to one-shot mixed
17"
ABSTRACT,0.01717557251908397,"precision training and develop a Hessian-Aware Stochastic Bit-switching (HASB)
18"
ABSTRACT,0.01812977099236641,"strategy. Experimental results on the ImageNet-1K classification demonstrate that
19"
ABSTRACT,0.019083969465648856,"our methods have enough advantages to state-of-the-art one-shot joint QAT in both
20"
ABSTRACT,0.020038167938931296,"multi-precision and mixed-precision. Our codes are available at here.
21"
INTRODUCTION,0.02099236641221374,"1
Introduction
22"
INTRODUCTION,0.02194656488549618,"Recently, with the popularity of mobile and edge devices, more and more researchers have attracted
23"
INTRODUCTION,0.022900763358778626,"attention to model compression due to the limitation of computing resources and storage. Model
24"
INTRODUCTION,0.02385496183206107,"quantization [1; 2] has gained significant prominence in the industry. Quantization maps floating-point
25"
INTRODUCTION,0.02480916030534351,"values to integer values, significantly reducing storage requirements and computational resources
26"
INTRODUCTION,0.025763358778625955,"without altering the network architecture.
27"
INTRODUCTION,0.026717557251908396,"Generally, for a given pre-trained model, the quantization bit-width configuration is predefined for a
28"
INTRODUCTION,0.02767175572519084,"specific application scenario. The quantized model then undergoes retraining, i.e., QAT, to mitigate
29"
INTRODUCTION,0.02862595419847328,"the accuracy decline. However, when the model is deployed across diverse scenarios with different
30"
INTRODUCTION,0.029580152671755726,"precisions, it often requires repetitive retraining processes for the same model. A lot of computing
31"
INTRODUCTION,0.030534351145038167,"resources and training costs are wasted. To address this challenge, involving the simultaneous
32"
INTRODUCTION,0.03148854961832061,"training of multi-precision [3; 4] or one-shot mixed-precision [3; 5] have been proposed. Among
33"
INTRODUCTION,0.03244274809160305,"these approaches, some involve sharing weight parameters between low-precision and high-precision
34"
INTRODUCTION,0.033396946564885496,"models, enabling dynamic bit-width switching during inference.
35"
INTRODUCTION,0.03435114503816794,"However, bit-switching from high precision (or bit-width) to low precision may introduce significant
36"
INTRODUCTION,0.035305343511450385,"accuracy degradation due to the Rounding operation in the quantization process. Additionally, there is
37"
INTRODUCTION,0.03625954198473282,"severe competition in the convergence process between higher and lower precisions in multi-precision
38 FP32 INT8 INT6 INT4 INT2"
INTRODUCTION,0.03721374045801527,"First 
Rounding"
INTRODUCTION,0.03816793893129771,High bit
INTRODUCTION,0.039122137404580155,"Second
Rounding"
INTRODUCTION,0.04007633587786259,Low bit
INTRODUCTION,0.04103053435114504,"Adaptive
Learn Rate"
BIT,0.04198473282442748,6bit
BIT,0.042938931297709926,2bit
BIT,0.04389312977099236,4bit
BIT,0.04484732824427481,8bit
BIT,0.04580152671755725,"8bit
6bit"
BIT,0.046755725190839696,2bit
BIT,0.04770992366412214,4bit
BIT,0.04866412213740458,Bit Mixing
BIT,0.04961832061068702,"Selection
Adaptive 
Learning Rate"
BIT,0.05057251908396947,"Double 
Round Bit 
Switch"
BIT,0.05152671755725191,"Multi-Precision 
Mixed-Precision 
Full-Precision"
BIT,0.05248091603053435,"2bit 
4bit"
BIT,0.05343511450381679,"6bit 
8bit layer layer"
BIT,0.05438931297709924,8bit
BIT,0.05534351145038168,2bit 100% 0%
BIT,0.05629770992366412,Probability
BIT,0.05725190839694656,"Stochastic 
Bit-switching"
BIT,0.05820610687022901,Sensitivity
BIT,0.05916030534351145,"(a) Saving only 8-bit representation 
for various low-precisions"
BIT,0.060114503816793896,"(b) Stabilize learning process 
across various precisions"
BIT,0.061068702290076333,"(c) Sensitivity-aware bit selection 
for different layers"
BIT,0.06202290076335878,Figure 1: Overview of our proposed lossless adaptive bit-switching strategy.
BIT,0.06297709923664122,"scheme. In mixed-precision scheme, previous methods often incur vast searching and retraining costs
39"
BIT,0.06393129770992366,"due to decoupling the training and search stages. Due to the above challenges, bit-switching remains
40"
BIT,0.0648854961832061,"a very challenging problem. Our motivation is designing a bit-switching quantization method that
41"
BIT,0.06583969465648855,"doesn’t require storing a full-precision model and achieves nearly lossless switching from high-bits to
42"
BIT,0.06679389312977099,"low-bits. Specifically, for different precisions, we propose unified representation, normalized learning
43"
BIT,0.06774809160305344,"steps, and tuned probability distribution so that an efficient and stable learning process is achieved
44"
BIT,0.06870229007633588,"across multiple and mixed precisions, as depicted in Figure 1.
45"
BIT,0.06965648854961833,"To solve the bit-switching problem, prior methods either store the floating-point parameters [6; 7; 4; 8]
46"
BIT,0.07061068702290077,"to avoid accuracy degradation or abandon some integer values by replacing rounding with floor[3; 9]
47"
BIT,0.0715648854961832,"but leading to accuracy decline or training collapse at lower bit-widths. We propose Double Rounding,
48"
BIT,0.07251908396946564,"which applies the rounding operation twice instead of once, as shown in Figure1 (a). This approach
49"
BIT,0.07347328244274809,"ensures nearly lossless bit-switching and allows storing the highest bit-width model instead of the
50"
BIT,0.07442748091603053,"full-precision model. Specifically, the lower precision weight is included in the higher precision
51"
BIT,0.07538167938931298,"weight, reducing storage constraints.
52"
BIT,0.07633587786259542,"Moreover, we empirically find severe competition between higher and lower precisions, particularly
53"
BIT,0.07729007633587787,"in 2-bit precision, as also noted in [10; 4]. There are two reasons for this phenomenon: The optimal
54"
BIT,0.07824427480916031,"quantization interval itself is different for higher and lower precisions. Furthermore, shared weights
55"
BIT,0.07919847328244274,"are used for different precisions during joint training, but the quantization interval gradients for
56"
BIT,0.08015267175572519,"different precisions exhibit distinct magnitudes during training. Therefore, we introduce an Adaptive
57"
BIT,0.08110687022900763,"Learning Rate Scaling (ALRS) method, designed to dynamically adjust the learning rates across
58"
BIT,0.08206106870229007,"different precisions, which ensures consistent update steps of quantization scales corresponding to
59"
BIT,0.08301526717557252,"different precisions, as shown in the Figure 1 (b).
60"
BIT,0.08396946564885496,"Finally, we develop an efficient one-shot mixed-precision quantization approach based on Double
61"
BIT,0.08492366412213741,"Rounding. Prior mixed-precision approaches first train a SuperNet with predefined bit-width lists,
62"
BIT,0.08587786259541985,"then search for optimal candidate SubNets under restrictive conditions, and finally retrain or fine-tune
63"
BIT,0.0868320610687023,"them, which incurs significant time and training costs. However, we use the Hessian Matrix Trace [11]
64"
BIT,0.08778625954198473,"as a sensitivity metric for different layers to optimize the SuperNet and propose a Hessian-Aware
65"
BIT,0.08874045801526717,"Stochastic Bit-switching (HASB) strategy, inspired by the Roulette algorithm [12]. This strategy
66"
BIT,0.08969465648854962,"enables tuned probability distribution of switching bit-width across layers, assigning higher bits to
67"
BIT,0.09064885496183206,"more sensitive layers and lower bits to less sensitive ones, as shown in Figure 1 (c). And, we add the
68"
BIT,0.0916030534351145,"sensitivity to the search stage as a constraint factor. So, our approach can omit the last stage.
69"
BIT,0.09255725190839695,"In conclusion, our main contributions can be described as:
70"
BIT,0.09351145038167939,"• Double Rounding quantization method for multi-precision is proposed, which stores a single
71"
BIT,0.09446564885496184,"integer weight to enable adaptive precision switching with nearly lossless accuracy.
72"
BIT,0.09541984732824428,"• Adaptive Learning Rate Scaling (ALRS) method for the multi-precision scheme is intro-
73"
BIT,0.09637404580152671,"duced, which effectively narrows the training convergence gap between high-precision
74"
BIT,0.09732824427480916,"and low-precision, enhancing the accuracy of low-precision models without compromising
75"
BIT,0.0982824427480916,"high-precision model accuracy.
76"
BIT,0.09923664122137404,"• Hessian-Aware Stochastic Bit-switching (HASB) strategy for one-shot mixed-precision
77"
BIT,0.10019083969465649,"SuperNet is applied, where the access probability of bit-width for each layer is determined
78"
BIT,0.10114503816793893,"based on the layer’s sensitivity.
79"
BIT,0.10209923664122138,"• Experimental results on the ImageNet1K dataset demonstrate that our proposed methods are
80"
BIT,0.10305343511450382,"comparable to state-of-the-art methods across different mainstream CNN architectures.
81"
RELATED WORKS,0.10400763358778627,"2
Related Works
82"
RELATED WORKS,0.1049618320610687,"Multi-Precision. Multi-Precision entails a single shared model with multiple precisions by one-shot
83"
RELATED WORKS,0.10591603053435114,"joint Quantization-Aware Training (QAT). This approach can dynamically adapt uniform bit-switching
84"
RELATED WORKS,0.10687022900763359,"for the entire model according to computing resources and storage constraints. AdaBits [13] is the
85"
RELATED WORKS,0.10782442748091603,"first work to consider adaptive bit-switching but encounters convergence issues with 2-bit quantization
86"
RELATED WORKS,0.10877862595419847,"on ResNet50 [14]. Bit-Mixer [9] addresses this problem by using the LSQ [2] quantization method
87"
RELATED WORKS,0.10973282442748092,"but discards the lowest state quantized value, resulting in an accuracy decline. Multi-Precision
88"
RELATED WORKS,0.11068702290076336,"joint QAT can also be viewed as a multi-objective optimization problem. Any-precision [6] and
89"
RELATED WORKS,0.11164122137404581,"MultiQuant [4] combine knowledge distillation techniques to improve model accuracy. Among these
90"
RELATED WORKS,0.11259541984732824,"methods, MultiQuant’s proposed ""Online Adaptive Label"" training strategy is essentially a form of
91"
RELATED WORKS,0.11354961832061068,"self-distillation [15]. Similar to our method, AdaBits and Bit-Mixer can save an 8-bit model, while
92"
RELATED WORKS,0.11450381679389313,"other methods rely on 32-bit models for bit switching. Our Double Rounding method can store the
93"
RELATED WORKS,0.11545801526717557,"highest bit-width model (e.g., 8-bit) and achieve almost lossless bit-switching, ensuring a stable
94"
RELATED WORKS,0.11641221374045801,"optimization process. Importantly, this leads to a reduction in training time by approximately 10% [7]
95"
RELATED WORKS,0.11736641221374046,"compared to separate quantization training.
96"
RELATED WORKS,0.1183206106870229,"One-shot Mixed-Precision. Previous works mainly utilize costly approaches, such as reinforcement
97"
RELATED WORKS,0.11927480916030535,"learning [16; 17] and Neural Architecture Search (NAS) [18; 19; 20], or rely on partial prior knowl-
98"
RELATED WORKS,0.12022900763358779,"edge [21; 22] for bit-width allocation, which may not achieve global optimality. In contrast, our
99"
RELATED WORKS,0.12118320610687022,"proposed one-shot mixed-precision method employs Hessian-Aware optimization to refine a SuperNet
100"
RELATED WORKS,0.12213740458015267,"via gradient updates, and then obtain the optimal conditional SubNets with less search cost without
101"
RELATED WORKS,0.12309160305343511,"retraining or fine-tuning. Additionally, Bit-Mixer [9] and MultiQuant [4] implement layer-adaptive
102"
RELATED WORKS,0.12404580152671756,"mixed-precision models, but Bit-Mixer uses a naive search method to attain a sub-optimal solution,
103"
RELATED WORKS,0.125,"while MultiQuant requires 300 epochs of fine-tuning to achieve ideal performance. Unlike NAS
104"
RELATED WORKS,0.12595419847328243,"approaches [20], which focus on altering network architecture (e.g., depth, kernel size, or channels),
105"
RELATED WORKS,0.1269083969465649,"our method optimizes a once-for-all SuperNet using only quantization techniques without altering
106"
RELATED WORKS,0.12786259541984732,"the model architecture.
107"
METHODOLOGY,0.12881679389312978,"3
Methodology
108"
DOUBLE ROUNDING,0.1297709923664122,"3.1
Double Rounding
109"
DOUBLE ROUNDING,0.13072519083969467,"Conventional separate precision quantization using Quantization-Aware Training (QAT) [23] attain
110"
DOUBLE ROUNDING,0.1316793893129771,"a fixed bit-width quantized model under a pre-trained FP32 model. A pseudo-quantization node is
111"
DOUBLE ROUNDING,0.13263358778625955,"inserted into each layer of the model during training. This pseudo-quantization node comprises two
112"
DOUBLE ROUNDING,0.13358778625954199,"operations: the quantization operation quant(x), which maps floating-point (FP32) values to lower-
113"
DOUBLE ROUNDING,0.13454198473282442,"bit integer values, and the dequantization operation dequant(x), which restores the quantized integer
114"
DOUBLE ROUNDING,0.13549618320610687,"value to its original floating-point representation. It can simulate the quantization error incurred
115"
DOUBLE ROUNDING,0.1364503816793893,"when compressing float values into integer values. As quantization involves a non-differentiable
116"
DOUBLE ROUNDING,0.13740458015267176,"Rounding operation, Straight-Through Estimator (STE) [24] is commonly used to handle the non-
117"
DOUBLE ROUNDING,0.1383587786259542,"differentiability.
118"
DOUBLE ROUNDING,0.13931297709923665,"However, for multi-precision quantization, bit-switching can result in significant accuracy loss,
119"
DOUBLE ROUNDING,0.14026717557251908,"especially when transitioning from higher bit-widths to lower ones, e.g., from 8-bit to 2-bit. To
120"
DOUBLE ROUNDING,0.14122137404580154,"1.0
0.5
0.0
0.5
1.0
x 1.0 0.5 0.0 0.5 1.0 y"
DOUBLE ROUNDING,0.14217557251908397,Storage:32bit LSQ
BIT,0.1431297709923664,"2bit
3bit
4bit"
BIT,0.14408396946564886,"1.0
0.5
0.0
0.5
1.0
x 1.0 0.5 0.0 0.5 1.0 y"
BIT,0.1450381679389313,Storage:4bit
BIT,0.14599236641221375,AdaBits
BIT,0.14694656488549618,"2bit
3bit
4bit"
BIT,0.14790076335877864,"1.0
0.5
0.0
0.5
1.0
x 1.0 0.5 0.0 0.5 1.0 y"
BIT,0.14885496183206107,"Storage:4bit
Bit-mixer"
BIT,0.14980916030534353,"2bit
3bit
4bit"
BIT,0.15076335877862596,"1.0
0.5
0.0
0.5
1.0
x 1.0 0.5 0.0 0.5 1.0 y"
BIT,0.15171755725190839,"Storage:4bit
Double Rounding"
BIT,0.15267175572519084,"2bit
3bit
4bit"
BIT,0.15362595419847327,"Figure 2: Comparison of four quantization schemes:(from left to right) used in LSQ [2], AdaBits [3],
Bit-Mixer [9] and Ours Double Rounding. In all cases y = dequant(quant(x))."
BIT,0.15458015267175573,"mitigate this loss, prior works have mainly employed two strategies: one involves bit-switching from
121"
BIT,0.15553435114503816,"a floating-point model (32-bit) to a lower-bit model each time using multiple learnable quantization
122"
BIT,0.15648854961832062,"parameters, and the other substitutes the Rounding operation with the Floor operation, but this
123"
BIT,0.15744274809160305,"results in accuracy decline (especially in 2-bit). In contrast, we propose a nearly lossless bit-
124"
BIT,0.15839694656488548,"switching quantization method called Double Rounding. This method overcomes these limitations by
125"
BIT,0.15935114503816794,"employing a Rounding operation twice. It allows the model to be saved in the highest-bit (e.g., 8-bit)
126"
BIT,0.16030534351145037,"representation instead of full-precision, facilitating seamless switching to other bit-width models. A
127"
BIT,0.16125954198473283,"detailed comparison of Double Rounding with other quantization methods is shown in Figure 2.
128"
BIT,0.16221374045801526,"Unlike AdaBits, which relies on the Dorefa [1] quantization method where the quantization scale is
129"
BIT,0.16316793893129772,"determined based on the given bit-width, the quantization scale of our Double Rounding is learned
130"
BIT,0.16412213740458015,"online and is not fixed. It only requires a pair of shared quantization parameters, i.e., scale and
131"
BIT,0.1650763358778626,"zero-point. Quantization scales of different precisions adhere to a strict ""Power of Two"" relationship.
132"
BIT,0.16603053435114504,"Suppose the highest-bit and the target low-bit are denoted as h-bit and l-bit respectively, and the
133"
BIT,0.16698473282442747,"difference between them is ∆= h −l. The specific formulation of Double Rounding is as follows:
134"
BIT,0.16793893129770993,"f
Wh = clip(
W −zh sh"
BIT,0.16889312977099236,"
, −2h−1, 2h−1 −1)
(1)"
BIT,0.16984732824427481,"f
Wl = clip("
BIT,0.17080152671755724,"$
f
Wh 2∆ '"
BIT,0.1717557251908397,", −2l−1, 2l−1 −1)
(2)"
BIT,0.17270992366412213,"c
Wl = f
Wl × sh × 2∆+ zh
(3)"
BIT,0.1736641221374046,"where the symbol ⌊.⌉denotes the Rounding function, and clip(x, low, upper) means x is limited
135"
BIT,0.17461832061068702,"to the range between low and upper. Here, W represents the FP32 model’s weights, sh ∈R
136"
BIT,0.17557251908396945,"and zh ∈Z denote the highest-bit (e.g., 8-bit) quantization scale and zero-point respectively. f
Wh
137"
BIT,0.1765267175572519,"represent the quantized weights of the highest-bit, while f
Wl and c
Wl represent the quantized weights
138"
BIT,0.17748091603053434,"and dequantized weights of the low-bit respectively.
139"
BIT,0.1784351145038168,"Hardware shift operations can efficiently execute the division and multiplication by 2∆. Note that in
140"
BIT,0.17938931297709923,"our Double Rounding, the model can also be saved at full precision by using unshared quantization
141"
BIT,0.1803435114503817,"parameters to run bit-switching and attain higher accuracy. Because we use symmetric quantization
142"
BIT,0.18129770992366412,"scheme, the zh is 0. Please refer to Section A.4 for the gradient formulation of Double Rounding.
143"
BIT,0.18225190839694658,"Unlike fixed weights, activations change online during inference. So, the corresponding scale and
144"
BIT,0.183206106870229,"zero-point values for different precisions can be learned individually to increase overall accuracy.
145"
BIT,0.18416030534351144,"Suppose X denotes the full precision activation, and f
Xb and c
Xb are the quantized activation and
146"
BIT,0.1851145038167939,"dequantized activation respectively. The quantization process can be formulated as follows:
147"
BIT,0.18606870229007633,"f
Xb = clip(
X −zb sb"
BIT,0.18702290076335878,"
, 0, 2b −1)
(4)"
BIT,0.18797709923664122,"c
Xb = f
Xb × sb + zb
(5)"
BIT,0.18893129770992367,"where sb ∈R and zb ∈Z represent the quantization scale and zero-point of different bit-widths
148"
BIT,0.1898854961832061,"activation respectively. Note that zb is 0 for the ReLU activation function.
149"
ADAPTIVE LEARNING RATE SCALING FOR MULTI-PRECISION,0.19083969465648856,"3.2
Adaptive Learning Rate Scaling for Multi-Precision
150"
ADAPTIVE LEARNING RATE SCALING FOR MULTI-PRECISION,0.191793893129771,"Although our proposed Double Rounding method represents a significant improvement over most
151"
ADAPTIVE LEARNING RATE SCALING FOR MULTI-PRECISION,0.19274809160305342,"previous multi-precision works, the one-shot joint optimization of multiple precisions remains
152"
ADAPTIVE LEARNING RATE SCALING FOR MULTI-PRECISION,0.19370229007633588,"constrained by severe competition between the highest and lowest precisions [10; 4]. Different
153"
ADAPTIVE LEARNING RATE SCALING FOR MULTI-PRECISION,0.1946564885496183,"precisions simultaneously impact each other during joint training, resulting in substantial differences
154"
ADAPTIVE LEARNING RATE SCALING FOR MULTI-PRECISION,0.19561068702290077,"in convergence rates between them, as shown in Figure 3 (c). We experimentally find that this
155"
ADAPTIVE LEARNING RATE SCALING FOR MULTI-PRECISION,0.1965648854961832,"competitive relationship stems from the inconsistent magnitudes of the quantization scale’s gradients
156"
ADAPTIVE LEARNING RATE SCALING FOR MULTI-PRECISION,0.19751908396946566,"between high-bit and low-bit quantization during joint training, as shown in Figure 3 (a) and (b). For
157"
ADAPTIVE LEARNING RATE SCALING FOR MULTI-PRECISION,0.1984732824427481,"other models statistical results please refer to Section A.6 in the appendix.
158"
ADAPTIVE LEARNING RATE SCALING FOR MULTI-PRECISION,0.19942748091603055,1 2 3 4 5 6 7 8 9 101112131415161718 Layer 0.075 0.050 0.025 0.000 0.025 0.050 0.075
ADAPTIVE LEARNING RATE SCALING FOR MULTI-PRECISION,0.20038167938931298,Gradients of weight scale
ADAPTIVE LEARNING RATE SCALING FOR MULTI-PRECISION,0.2013358778625954,1 2 3 4 5 6 7 8 9 101112131415161718 Layer 0.075 0.050 0.025 0.000 0.025 0.050 0.075
ADAPTIVE LEARNING RATE SCALING FOR MULTI-PRECISION,0.20229007633587787,Gradients of weight scale
ADAPTIVE LEARNING RATE SCALING FOR MULTI-PRECISION,0.2032442748091603,"1
11
21
31
61
71
81
41
51
Epoch"
ADAPTIVE LEARNING RATE SCALING FOR MULTI-PRECISION,0.20419847328244276,"5
10
15
20"
ADAPTIVE LEARNING RATE SCALING FOR MULTI-PRECISION,0.20515267175572519,"70
65
60
55
50
45
40
35
30
25"
ADAPTIVE LEARNING RATE SCALING FOR MULTI-PRECISION,0.20610687022900764,Acc1(%)
BIT,0.20706106870229007,"8bit
6bit
4bit
2bit gap"
BIT,0.20801526717557253,"1
11
21
31
41
51
61
71
81
Epoch"
BIT,0.20896946564885496,"5
10
15
20
25
30
35
40
45
50
55
60
65
70"
BIT,0.2099236641221374,Acc1(%)
BIT,0.21087786259541985,"8bit
6bit
4bit
2bit"
BIT,0.21183206106870228,"(a) 2-bit
(b) 4-bit
(c) w/o ALRS
(d) w. ALRS"
BIT,0.21278625954198474,"Figure 3: The statistics of ResNet18 on ImageNet-1K dataset. (a) and (b): The quantization scale
gradients’ statistics for the weights, with outliers removed for clarity. (c) and (d): The multi-precision
training processes of our Double Rounding without and with the ALRS strategy."
BIT,0.21374045801526717,"Motivated by these observations, we introduce a technique termed Adaptive Learning Rate Scaling
159"
BIT,0.21469465648854963,"(ALRS), which dynamically adjusts learning rates for different precisions to optimize the training
160"
BIT,0.21564885496183206,"process. This technique is inspired by the Layer-wise Adaptive Rate Scaling (LARS) [25] optimizer.
161"
BIT,0.21660305343511452,"Specifically, suppose the current batch iteration’s learning rate is λ, we set learning rates λb of
162"
BIT,0.21755725190839695,"different precisions as follows:
163"
BIT,0.21851145038167938,"λb = ηb  λ − L
X i=1"
BIT,0.21946564885496184,"min
 
max_abs
 
clip_grad(∇si
b, 1.0)

, 1.0
 L ! ,
(6) ηb ="
BIT,0.22041984732824427,"(
1 × 10−∆"
BIT,0.22137404580152673,"2 ,
if ∆is even"
BIT,0.22232824427480916,5 × 10−( ∆+1
BIT,0.22328244274809161,"2
),
if ∆is odd
(7)"
BIT,0.22423664122137404,"where the L is the number of layers, clip_grad(.) represents gradient clipping that prevents gradient
164"
BIT,0.22519083969465647,"explosion, max_abs(.) denotes the maximum absolute value of all elements. The ∇si
b denotes the
165"
BIT,0.22614503816793893,"quantization scale’s gradients of layer i and ηb denotes scaling hyperparameter of different precisions,
166"
BIT,0.22709923664122136,"e.g., 8-bit is 1, 6-bit is 0.1, and 4-bit is 0.01. Note that the ALRS strategy is only used for updating
167"
BIT,0.22805343511450382,"quantization scales. It can adaptively update the learning rates of different precisions and ensure
168"
BIT,0.22900763358778625,"that model can optimize quantization parameters at the same pace, ultimately achieving a minimal
169"
BIT,0.2299618320610687,"convergence gap in higher bits and 2-bit, as shown in Figure 3 (d).
170"
BIT,0.23091603053435114,"In multi-precision scheme, different precisions share the same model weights during joint training.
171"
BIT,0.2318702290076336,"For conventional multi-precision, the shared weight computes n forward processes at each training
172"
BIT,0.23282442748091603,"iteration, where n is the number of candidate bit-widths. The losses attained from different precisions
173"
BIT,0.23377862595419846,"are then accumulated, and the gradients are computed. Finally, the shared parameters are updated.
174"
BIT,0.23473282442748092,"For detailed implementation please refer to Algorithm A.1 in the appendix. However, we find that
175"
BIT,0.23568702290076335,"if different precision losses separately compute gradients and directly update shared parameters at
176"
BIT,0.2366412213740458,"each forward process, it attains better accuracy when combined with our ALRS training strategy.
177"
BIT,0.23759541984732824,"Additionally, we use dual optimizers to update the weight parameters and quantization parameters
178"
BIT,0.2385496183206107,"simultaneously. We also set the weight-decay of the quantization scales to 0 to achieve stable
179"
BIT,0.23950381679389313,"convergence. For detailed implementation please refer to Algorithm A.2 in the appendix.
180"
ONE-SHOT MIXED-PRECISION SUPERNET,0.24045801526717558,"3.3
One-Shot Mixed-Precision SuperNet
181"
ONE-SHOT MIXED-PRECISION SUPERNET,0.24141221374045801,"Unlike multi-precision, where all layers uniformly utilize the same bit-width, mixed-precision
182"
ONE-SHOT MIXED-PRECISION SUPERNET,0.24236641221374045,"SuperNet provides finer-grained adaptive by configuring the bit-width at different layers. Previous
183"
ONE-SHOT MIXED-PRECISION SUPERNET,0.2433206106870229,"methods typically decouple the training and search stages, which need a third stage for retraining
184"
ONE-SHOT MIXED-PRECISION SUPERNET,0.24427480916030533,"or fine-tuning the searched SubNets. These approaches generally incur substantial search costs in
185"
ONE-SHOT MIXED-PRECISION SUPERNET,0.2452290076335878,"selecting the optimal SubNets, often employing methods such as greedy algorithms [26; 9] or genetic
186"
ONE-SHOT MIXED-PRECISION SUPERNET,0.24618320610687022,"algorithms [27; 4]. Considering the fact that the sensitivity [28], i.e., importance, of each layer
187"
ONE-SHOT MIXED-PRECISION SUPERNET,0.24713740458015268,"is different, we propose a Hessian-Aware Stochastic Bit-switching (HASB) strategy for one-shot
188"
ONE-SHOT MIXED-PRECISION SUPERNET,0.2480916030534351,"mixed-precision training.
189"
ONE-SHOT MIXED-PRECISION SUPERNET,0.24904580152671757,"Specifically, the Hessian Matrix Trace (HMT) is utilized to measure the sensitivity of each layer. We
190"
ONE-SHOT MIXED-PRECISION SUPERNET,0.25,"first need to compute the pre-trained model’s HMT by around 1000 training images [11], as shown in
191"
BIT,0.25095419847328243,"8bit
2bit
4bit
6bit"
BIT,0.25190839694656486,Probability
BIT,0.25286259541984735,"25%
25%
25%
25%"
BIT,0.2538167938931298,"8bit
2bit
4bit
6bit"
BIT,0.2547709923664122,Probability 10% 20% 30% 40%
BIT,0.25572519083969464,1 2 3 4 5 6 7 8 9 101112131415161718 Layer 0.00 0.01 0.02 0.03 0.04
BIT,0.2566793893129771,Average Hessian trace
BIT,0.25763358778625955,weights
BIT,0.258587786259542,"2
3
4
5
6
7
8
Average bit-width 65 66 67 68 69 70 71"
BIT,0.2595419847328244,Acc1(%)
BIT,0.26049618320610685,"w. HASB
w/o HASB"
BIT,0.26145038167938933,"(a) Unsensitive
(b) Sensitive
(c) Hessian trace
(d) Mixed precision"
BIT,0.26240458015267176,"Figure 4: The HASB stochastic process and Mixed-precision of ResNet18 for {2,4,6,8}-bit."
BIT,0.2633587786259542,"Figure 4 (c). Then, the HMT of different layers is utilized as the probability metric for bit-switching.
192"
BIT,0.2643129770992366,"Higher bits are priority selected for sensitive layers, while all candidate bits are equally selected for
193"
BIT,0.2652671755725191,"unsensitive layers. Our proposed Roulette algorithm is used for bit-switching processes of different
194"
BIT,0.26622137404580154,"layers during training, as shown in the Algorithm 1. If a layer’s HMT exceeds the average HMT of
195"
BIT,0.26717557251908397,"all layers, it is recognized as sensitive, and the probability distribution of Figure 4 (b) is used for bit
196"
BIT,0.2681297709923664,"selection. Conversely, if the HMT is below the average, the probability distribution of Figure 4 (a) is
197"
BIT,0.26908396946564883,"used for selection. Finally, the Integer Linear Programming (ILP) [29] algorithm is employed to find
198"
BIT,0.2700381679389313,"the optimal SubNets. Considering each layer’s sensitivity during training and adding this sensitivity
199"
BIT,0.27099236641221375,"to the ILP’s constraint factors (e.g., model’s FLOPs, latency, and parameters), which depend on
200"
BIT,0.2719465648854962,"the actual deployment requirements. We can efficiently attain a set of optimal SubNets during the
201"
BIT,0.2729007633587786,"search stage without retraining, thereby significant reduce the overall costs. All the searched SubNets
202"
BIT,0.2738549618320611,"collectively constitute the Pareto Frontier optimal solution, as shown in Figure 4 (d). For detailed
203"
BIT,0.2748091603053435,"mixed-precision training and searching process (i.e., ILP) please refer to the Algorithm A.3 and the
204"
BIT,0.27576335877862596,Algorithm 2 respectively.
BIT,0.2767175572519084,"Algorithm 1 Roulette algorithm for bit-switching
Require: Candidate bit-widths set b ∈B, the HMT of
current layer: tl, average HMT: tm;
1: Sample r ∼U(0, 1] from a uniform distribution;
2: if tl < tm then
3:
Compute bit-switching probability of all candi-
date bi with pi = 1/n;
4:
Set s = 0, and i = 0;
5:
while s < r do
6:
i = i + 1;
7:
s = pi + s;
8:
end while
9: else
10:
Compute bit-switching probability of all candi-
date bi with pi = bi/∥B∥1;
11:
Set s = 0, and i = 0;
12:
while s < r do
13:
i = i + 1;
14:
s = pi + s;
15:
end while
16: end if
17: return bi;
Note that n and L represent the number of candidate bit-widths and
model layers respectively, and ∥· ∥1 is L1 norm."
BIT,0.2776717557251908,"Algorithm 2 Our searching process for SubNets
Input: Candidate bit-widths set b ∈B, the HMT of
different layers of FP32 model: tl ∈{T}L
l=1, the
constraint average bit-width: ω, each layer param-
eters: nl ∈{N}L
l=1;
1: Initial searched SubNets’solutions: S = ϕ
2: Minimal objective : O = PL
l=1
tl
nl · bl"
BIT,0.2786259541984733,3: Constraints: ω ≡
BIT,0.27958015267175573,"PL
l=1 bl"
BIT,0.28053435114503816,"L
4: The first solve:
s1
= pulp.solve(O, ω) and
S.append(s1)
5: for ci in s1 do
6:
for b in B[: idenx(max(s1))] do
7:
if b ̸= ci then
8:
Add constraint: b ≡ci
9:
Solve: s = pulp.solve(O, ω, b)
10:
if s not in S then
11:
S.append(s)
12:
end if
13:
Pop last constraint: b ≡ci
14:
end if
15:
end for
16: end for
17: return S 205"
EXPERIMENTAL RESULTS,0.2814885496183206,"4
Experimental Results
206"
EXPERIMENTAL RESULTS,0.2824427480916031,"Setup. In this paper, we mainly focus on ImageNet-1K [30] classification task using both classical
207"
EXPERIMENTAL RESULTS,0.2833969465648855,"networks (ResNet18/50 [14]) and lightweight networks (MobileNetV2 [31]), which same as previous
208"
EXPERIMENTAL RESULTS,0.28435114503816794,"works. Experiments cover joint quantization training for multi-precision and mixed precision. We
209"
EXPERIMENTAL RESULTS,0.28530534351145037,"explore two candidate bit configurations, i.e., {8,6,4,2}-bit and {4,3,2}-bit, each number represents
210"
EXPERIMENTAL RESULTS,0.2862595419847328,"the quantization level of the weight and activation layers. Like previous methods, we exclude batch
211"
EXPERIMENTAL RESULTS,0.2872137404580153,"normalization layers from quantization, and the first and last layers are kept at full precision. We
212"
EXPERIMENTAL RESULTS,0.2881679389312977,"initialize the multi-precision models with a pre-trained FP32 model, and initialize the mixed-precision
213"
EXPERIMENTAL RESULTS,0.28912213740458015,"models with a pre-trained multi-precision model. All models use the Adam optimizer [32] with a batch
214"
EXPERIMENTAL RESULTS,0.2900763358778626,"size of 256 for 90 epochs and use a cosine scheduler without warm-up phase. The initial learning
215"
EXPERIMENTAL RESULTS,0.29103053435114506,"rate is 5e-4 and weight decay is 5e-5. Data augmentation uses the standard set of transformations
216"
EXPERIMENTAL RESULTS,0.2919847328244275,"including random cropping, resizing to 224×224 pixels, and random flipping. Images are resized to
217"
EXPERIMENTAL RESULTS,0.2929389312977099,"256×256 pixels and then center-cropped to 224×224 resolution during evaluation.
218"
MULTI-PRECISION,0.29389312977099236,"4.1
Multi-Precision
219"
MULTI-PRECISION,0.2948473282442748,"Results. For {8,6,4,2}-bit configuration, the Top-1 validation accuracy is shown in Table 1. The
220"
MULTI-PRECISION,0.2958015267175573,"network weights and the corresponding activations are quantized into w-bit and a-bit respectively.
221"
MULTI-PRECISION,0.2967557251908397,"Our double-rounding combined with ALRS training strategy surpasses the previous state-of-the-art
222"
MULTI-PRECISION,0.29770992366412213,"(SOTA) methods. For example, in ResNet18, it exceeds Any-Precision [6] by 2.7%(or 2.83%) under
223"
MULTI-PRECISION,0.29866412213740456,"w8a8 setting without(or with) using KD technique [15], and outperforms MultiQuant [4] by 0.63%(or
224"
MULTI-PRECISION,0.29961832061068705,"0.73%) under w4a4 setting without(or with) using KD technique respectively. Additionally, when
225"
MULTI-PRECISION,0.3005725190839695,"the candidate bit-list includes 2-bit, the previous methods can’t converge on MobileNetV2 during
226"
MULTI-PRECISION,0.3015267175572519,"training. So, they use {8,6,4}-bit precision for MobileNetV2 experiments. For consistency, we
227"
MULTI-PRECISION,0.30248091603053434,"also test {8,6,4}-bit results, as shown in the ""Ours {8,6,4}-bit"" rows of Table 1. Our method achieves
228"
MULTI-PRECISION,0.30343511450381677,"0.25%/0.11%/0.56% higher accuracy than AdaBits [3] under the w8a8/w6a6/w4a4 settings.
229"
MULTI-PRECISION,0.30438931297709926,"Notably, our method exhibits the ability to converge but shows a big decline in accuracy on Mo-
230"
MULTI-PRECISION,0.3053435114503817,"bileNetV2. On the one hand, the compact model exhibits significant differences in the quantization
231"
MULTI-PRECISION,0.3062977099236641,"scale gradients of different channels due to involving DeepWise Convolution [33]. On the other hand,
232"
MULTI-PRECISION,0.30725190839694655,"when the bit-list includes 2-bit, it intensifies competition between different precisions during training.
233"
MULTI-PRECISION,0.30820610687022904,"To improve the accuracy of compact models, we suggest considering the per-layer or per-channel
234"
MULTI-PRECISION,0.30916030534351147,learning rate scaling techniques in future work.
MULTI-PRECISION,0.3101145038167939,"Table 1: Top1 accuracy comparisons on multi-precision of {8,6,4,2}-bit on ImageNet-1K datasets.
’KD’ denotes knowledge distillation. The ""−"" represents the unqueried value."
MULTI-PRECISION,0.3110687022900763,"Model
Method
KD
Storage
Epoch
w8a8
w6a6
w4a4
w2a2
FP"
MULTI-PRECISION,0.31202290076335876,ResNet18
MULTI-PRECISION,0.31297709923664124,"Hot-Swap[34]
✗
32bit
−
70.40
70.30
70.20
64.90
−
L1[35]
✗
32bit
−
69.92
66.39
0.22
−
70.07
KURE[36]
✗
32bit
80
70.20
70.00
66.90
−
70.30
Ours
✗
8bit
90
70.74
70.71
70.43
66.35
69.76
Any-Precision[6]
✓
32bit
80
68.04
−
67.96
64.19
69.27
CoQuant[7]
✓
8bit
100
67.90
67.60
66.60
57.10
69.90
MultiQuant[4]
✓
32bit
90
70.28
70.14
69.80
66.56
69.76
Ours
✓
8bit
90
70.87
70.79
70.53
66.84
69.76"
MULTI-PRECISION,0.3139312977099237,ResNet50
MULTI-PRECISION,0.3148854961832061,"Any-Precision[6]
✗
32bit
80
74.68
−
74.43
72.88
75.95
Hot-Swap[34]
✗
32bit
−
75.60
75.50
75.30
71.90
−
KURE[36]
✗
32bit
80
−
76.20
74.30
−
76.30
Ours
✗
8bit
90
76.51
76.28
75.74
72.31
76.13
Any-Precision[6]
✓
32bit
80
74.91
−
74.75
73.24
75.95
MultiQuant[4]
✓
32bit
90
76.94
76.85
76.46
73.76
76.13
Ours
✓
8bit
90
76.98
76.86
76.52
73.78
76.13"
MULTI-PRECISION,0.31583969465648853,MobileNetV2
MULTI-PRECISION,0.31679389312977096,"AdaBits[3]
✗
8bit
150
72.30
72.30
70.30
−
71.80
KURE[36]
✗
32bit
80
−
70.00
59.00
−
71.30
Ours {8,6,4}-bit
✗
8bit
90
72.42
72.06
69.92
−
71.14
MultiQuant[4]
✓
32bit
90
72.33
72.09
70.59
−
71.88
Ours {8,6,4}-bit
✓
8bit
90
72.55
72.41
70.86
−
71.14
Ours {8,6,4,2}-bit
✗
8bit
90
70.98
70.70
68.77
50.43
71.14
Ours {8,6,4,2}-bit
✓
8bit
90
71.35
71.20
69.85
53.06
71.14 235"
MULTI-PRECISION,0.31774809160305345,"For {4,3,2}-bit configuration, Table 2 demonstrate that our double-rounding consistently surpasses
236"
MULTI-PRECISION,0.3187022900763359,"previous SOTA methods. For instance, in ResNet18, it exceeds Bit-Mixer [9] by 0.63%/0.7%/1.2%(or
237"
MULTI-PRECISION,0.3196564885496183,"0.37%/0.64%/1.02%) under w4a4/w3a3/w2a2 settings without(or with) using KD technique, and
238"
MULTI-PRECISION,0.32061068702290074,"outperforms ABN[10] by 0.87%/0.74%/1.12% under w4a4/w3a3/w2a2 settings with using KD
239"
MULTI-PRECISION,0.32156488549618323,"technique respectively. In ResNet50, Our method outperforms Bit-Mixer [9] by 0.86%/0.63%/0.1%
240"
MULTI-PRECISION,0.32251908396946566,"under w4a4/w3a3/w2a2 settings.
241"
MULTI-PRECISION,0.3234732824427481,"Notably, the overall results of Table 2 are worse than the {8,6,4,2}-bit configuration for joint training.
242"
MULTI-PRECISION,0.3244274809160305,"We analyze that this discrepancy arises from information loss in the shared lower precision model
243"
MULTI-PRECISION,0.32538167938931295,"(i.e., 4-bit) used for bit-switching. In other words, compared with 4-bit, it is easier to directly optimize
244"
MULTI-PRECISION,0.32633587786259544,"8-bit quantization parameters to converge to the optimal value. So, we recommend including 8-bit for
245"
MULTI-PRECISION,0.32729007633587787,"multi-precision training. Furthermore, independently learning the quantization scales for different
246"
MULTI-PRECISION,0.3282442748091603,"precisions, including weights and activations, significantly improves accuracy compared to using
247"
MULTI-PRECISION,0.3291984732824427,"shared scales. However, it requires saving the model in 32-bit format, as shown in ""Ours*"" of Table 2.
248"
MULTI-PRECISION,0.3301526717557252,"Table 2: Top1 accuracy comparisons on multi-precision of {4,3,2}-bit on ImageNet-1K datasets."
MULTI-PRECISION,0.33110687022900764,"Model
Method
KD
Storage
Epoch
w4a4
w3a3
w2a2
FP"
MULTI-PRECISION,0.3320610687022901,ResNet18
MULTI-PRECISION,0.3330152671755725,"Bit-Mixer[9]
✗
4bit
160
69.10
68.50
65.10
69.60
Vertical-layer[37]
✗
4bit
300
69.20
68.80
66.60
70.50
Ours
✗
4bit
90
69.73
69.20
66.30
69.76
Q-DNNs[7]
✓
32bit
45
66.94
66.28
62.91
68.60
ABN[10]
✓
4bit
160
68.90
68.60
65.50
−
Bit-Mixer[9]
✓
4bit
160
69.40
68.70
65.60
69.60
Ours
✓
4bit
90
69.77
69.34
66.62
69.76"
MULTI-PRECISION,0.33396946564885494,ResNet50
MULTI-PRECISION,0.3349236641221374,"Ours
✗
4bit
90
75.81
75.24
71.62
76.13
AdaBits[3]
✗
32bit
150
76.10
75.80
73.20
75.00
Ours*
✗
32bit
90
76.42
75.82
73.28
76.13
Bit-Mixer[9]
✓
4bit
160
75.20
74.90
72.70
−
Ours
✓
4bit
90
76.06
75.53
72.80
76.13"
MIXED-PRECISION,0.33587786259541985,"4.2
Mixed-Precision
249"
MIXED-PRECISION,0.3368320610687023,"Results. We follow previous works to conduct mixed-precision experiments based on the {4,3,2}-bit
250"
MIXED-PRECISION,0.3377862595419847,"configuration. Our proposed one-shot mixed-precision joint quantization method with the HASB tech-
251"
MIXED-PRECISION,0.3387404580152672,"nique comparable to the previous SOTA methods, as presented in Table 3. For example, in ResNet18,
252"
MIXED-PRECISION,0.33969465648854963,"our method exceeds Bit-Mixer [9] by 0.83%/0.72%/0.77%/7.07% under w4a4/w3a3/w2a2/3MP
253"
MIXED-PRECISION,0.34064885496183206,"settings and outperforms EQ-Net[5] by 0.2% under 3MP setting. The results demonstrate the effec-
254"
MIXED-PRECISION,0.3416030534351145,"tiveness of one-shot mixed-precision joint training to consider sensitivity with Hessian Matrix Trace
255"
MIXED-PRECISION,0.3425572519083969,"when randomly allocating bit-widths for different layers. Additionally, Table 3 reveals that our results
256"
MIXED-PRECISION,0.3435114503816794,"do not achieve optimal performance across all settings. We hypothesize that extending the number of
257"
MIXED-PRECISION,0.34446564885496184,"training epochs or combining ILP with other efficient search methods, such as genetic algorithms,
258"
MIXED-PRECISION,0.34541984732824427,"may be necessary to achieve optimal results in mixed-precision optimization.
259"
MIXED-PRECISION,0.3463740458015267,"Table 3: Top1 accuracy comparisons on mixed-precision of {4,3,2}-bit on ImageNet-1K dataset.
""MP"" denotes average bit-width for mixed-precision. The ""−"" represents the unqueried value."
MIXED-PRECISION,0.3473282442748092,"Model
Method
KD Training Searching Fine-tune Epoch w4a4 w3a3 w2a2
3MP
FP"
MIXED-PRECISION,0.3482824427480916,ResNet18
MIXED-PRECISION,0.34923664122137404,"Ours
✗
HASB
ILP
w/o
90
69.80 68.63 64.88 68.85 69.76
Bit-Mixer[9]
✓
Random
Greedy
w/o
160
69.20 68.60 64.40 62.90 69.60
ABN[10]
✓
DRL
DRL
w.
160
69.80 69.00 66.20 67.70
−
MultiQuant[4]
✓
LRH
Genetic
w.
90
−
67.50
−
69.20 69.76
EQ-Net[5]
✓
LRH
Genetic
w.
120
−
69.30 65.90 69.80 69.76
Ours
✓
KD
KD
w/o
90
70.03 69.32 65.17 69.92 69.76"
MIXED-PRECISION,0.3501908396946565,"ResNet50
Ours
✗
HASB
ILP
w/o
90
75.01 74.31 71.47 75.06 76.13
Bit-Mixer[9]
✓
Random
Greedy
w/o
160
75.20 74.80 72.10 73.20
−
EQ-Net[5]
✓
LRH
Genetic
w.
120
−
74.70 72.50 75.10 76.13
Ours
✓
HASB
ILP
w/o
90
75.63 74.36 72.32 75.24 76.13"
ABLATION STUDIES,0.3511450381679389,"4.3
Ablation Studies
260"
ABLATION STUDIES,0.3520992366412214,"ALRS vs. Conventional in Multi-Precision. To verify the effectiveness of our proposed ALRS train-
261"
ABLATION STUDIES,0.3530534351145038,"ing strategy, we conduct an ablation experiment without KD, as shown in Table 4, and observe overall
262"
ABLATION STUDIES,0.35400763358778625,"accuracy improvements, particularly for the 2bit. Like previous works, where MobileNetV2 can’t
263"
ABLATION STUDIES,0.3549618320610687,"achieve stable convergence with {4,3,2}-bit, we also opt for {8,6,4}-bit to keep consistent. However,
264"
ABLATION STUDIES,0.35591603053435117,"our method can achieve stable convergence with {8,6,4,2}-bit quantization. This demonstrates the
265"
ABLATION STUDIES,0.3568702290076336,"superiority of our proposed Double-Rounding and ALRS methods.
266"
ABLATION STUDIES,0.35782442748091603,"Multi-Precision vs. Separate-Precision in Time Cost. We statistic the results regarding the time cost
267"
ABLATION STUDIES,0.35877862595419846,"for multi-precision compared to separate-precision quantization, as shown in Table 5. Multi-precision
268"
ABLATION STUDIES,0.3597328244274809,"training costs stay approximate constant as the number of candidate bit-widths.
269"
ABLATION STUDIES,0.3606870229007634,"Table 4: Ablation studies of multi-precision, ResNet20 on CIFAR-10 dataset and other models on
ImageNet-1K dataset. Note that MobileNetV2 uses {8,6,4}-bit instead of {4,3,2}-bit."
ABLATION STUDIES,0.3616412213740458,"Model
ALRS
{8,6,4,2}-bit
{4,3,2}-bit
FP
w8a8
w6a6
w4a4
w2a2
w4a4
w3a3
w2a2"
ABLATION STUDIES,0.36259541984732824,"ResNet20
w/o
92.17
92.20
92.17
89.67
91.19
90.98
88.62
92.30
w.
92.25
92.32
92.09
90.19
91.79
91.83
88.88
92.30"
ABLATION STUDIES,0.36354961832061067,"ResNet18
w/o
70.05
69.80
69.32
65.83
69.38
68.74
65.62
69.76
w.
70.74
70.71
70.43
66.35
69.73
69.20
66.30
69.76"
ABLATION STUDIES,0.36450381679389315,"ResNet50
w/o
76.18
76.08
75.64
70.28
75.48
74.85
70.64
76.13
w.
76.51
76.28
75.74
72.31
75.81
75.24
71.62
76.13"
ABLATION STUDIES,0.3654580152671756,"MobileNetV2
w/o
70.55
70.65
68.08
45.00
72.06
71.87
69.40
71.14
w.
70.98
70.70
68.77
50.43
72.42
72.06
69.92
71.14"
ABLATION STUDIES,0.366412213740458,Table 5: Training costs for multi-precision and separate-precision are averaged over three runs.
ABLATION STUDIES,0.36736641221374045,"Model
Dataset
Bit-widths
#V100
Epochs
BatchSize
Avg. hours
Save cost (%)"
ABLATION STUDIES,0.3683206106870229,"ResNet20
Cifar10
Separate-bit
1
200
128
0.9
0.0
{4,3,2}-bit
1
200
128
0.7
28.6
{8,6,4,2}-bit
1
200
128
0.8
12.5"
ABLATION STUDIES,0.36927480916030536,"ResNet18
ImageNet
Separate-bit
4
90
256
19.0
0.0
{4,3,2}-bit
4
90
256
15.2
25.0
{8,6,4,2}-bit
4
90
256
16.3
16.6"
ABLATION STUDIES,0.3702290076335878,"ResNet50
ImageNet
Separate-bit
4
90
256
51.6
0.0
{4,3,2}-bit
4
90
256
40.7
26.8
{8,6,4,2}-bit
4
90
256
40.8
26.5"
ABLATION STUDIES,0.3711832061068702,"Pareto Frontier of Different Mixed-Precision Configurations. To verify the effectiveness of our
270"
ABLATION STUDIES,0.37213740458015265,"HASB strategy, we conduct ablation experiments on different bit-lists. Figure 5 shows the search
271"
ABLATION STUDIES,0.37309160305343514,"results of Mixed-precision SuperNet under {8,6,4,2}-bit, {4,3,2}-bit and {8,4}-bit configurations
272"
ABLATION STUDIES,0.37404580152671757,"respectively. Where each point represents a SubNet. These results are obtained directly from ILP
273"
ABLATION STUDIES,0.375,"sampling without retraining or fine-tuning. As the figure shows, the highest red points are higher than
274"
ABLATION STUDIES,0.37595419847328243,"the blue points under the same bit width, indicating that this strategy is effective.
275"
ABLATION STUDIES,0.37690839694656486,"2
3
4
5
6
7
8
Average bit-width 65 66 67 68 69 70 71"
ABLATION STUDIES,0.37786259541984735,Acc1(%)
ABLATION STUDIES,0.3788167938931298,"w. HASB
w/o HASB"
ABLATION STUDIES,0.3797709923664122,2.00 2.25 2.50 2.75 3.00 3.25 3.50 3.75 4.00
ABLATION STUDIES,0.38072519083969464,Average bit-width 65 66 67 68 69 70 71
ABLATION STUDIES,0.3816793893129771,Acc1(%)
ABLATION STUDIES,0.38263358778625955,"w. HASB
w/o HASB"
ABLATION STUDIES,0.383587786259542,"4.0
4.5
5.0
5.5
6.0
6.5
7.0
7.5
8.0
Average bit-width 65 66 67 68 69 70 71"
ABLATION STUDIES,0.3845419847328244,Acc1(%)
ABLATION STUDIES,0.38549618320610685,"w. HASB
w/o HASB"
ABLATION STUDIES,0.38645038167938933,"(a) {8,6,4,2}-bit
(b) {4,3,2}-bit
(c) {8,4}-bit"
ABLATION STUDIES,0.38740458015267176,Figure 5: Comparison of HASB and Baseline approaches for Mixed-Precision on ResNet18.
CONCLUSION,0.3883587786259542,"5
Conclusion
276"
CONCLUSION,0.3893129770992366,"This paper first introduces Double Rounding quantization method used to address the challenges
277"
CONCLUSION,0.3902671755725191,"of multi-precision and mixed-precision joint training. It can store single integer-weight parameters
278"
CONCLUSION,0.39122137404580154,"and attain nearly lossless bit-switching. Secondly, we propose an Adaptive Learning Rate Scaling
279"
CONCLUSION,0.39217557251908397,"(ALRS) method for multi-precision joint training that narrows the training convergence gap between
280"
CONCLUSION,0.3931297709923664,"high-precision and low-precision, enhancing model accuracy of multi-precision. Finally, our proposed
281"
CONCLUSION,0.39408396946564883,"Hessian-Aware Stochastic Bit-switching (HASB) strategy for one-shot mixed-precision SuperNet
282"
CONCLUSION,0.3950381679389313,"and efficient searching method combined with Integer Linear Programming, achieving approximate
283"
CONCLUSION,0.39599236641221375,"Pareto Frontier optimal solution. Our proposed methods aim to achieve a flexible and effective model
284"
CONCLUSION,0.3969465648854962,"compression technique for adapting different storage and computation requirements.
285"
REFERENCES,0.3979007633587786,"References
286"
REFERENCES,0.3988549618320611,"[1] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou, “Dorefa-net: Training low bitwidth convolutional
287"
REFERENCES,0.3998091603053435,"neural networks with low bitwidth gradients,” arXiv preprint arXiv:1606.06160, 2016.
288"
REFERENCES,0.40076335877862596,"[2] S. K. Esser, J. L. McKinstry, D. Bablani, R. Appuswamy, and D. S. Modha, “Learned step size quantization,”
289"
REFERENCES,0.4017175572519084,"arXiv preprint arXiv:1902.08153, 2019.
290"
REFERENCES,0.4026717557251908,"[3] Q. Jin, L. Yang, and Z. Liao, “Adabits: Neural network quantization with adaptive bit-widths,” in Proceed-
291"
REFERENCES,0.4036259541984733,"ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 2146–2156.
292"
REFERENCES,0.40458015267175573,"[4] K.
Xu,
Q.
Feng,
X.
Zhang,
and
D.
Wang,
“Multiquant:
Training
once
for
multi-bit
293"
REFERENCES,0.40553435114503816,"quantization of neural networks,” in IJCAI, L. D. Raedt, Ed.
International Joint Conferences
294"
REFERENCES,0.4064885496183206,"on Artificial Intelligence Organization, 7 2022, pp. 3629–3635, main Track. [Online]. Available:
295"
REFERENCES,0.4074427480916031,"https://doi.org/10.24963/ijcai.2022/504
296"
REFERENCES,0.4083969465648855,"[5] K. Xu, L. Han, Y. Tian, S. Yang, and X. Zhang, “Eq-net: Elastic quantization neural networks,” in
297"
REFERENCES,0.40935114503816794,"Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 1505–1514.
298"
REFERENCES,0.41030534351145037,"[6] H. Yu, H. Li, H. Shi, T. S. Huang, and G. Hua, “Any-precision deep neural networks,” in Proceedings of
299"
REFERENCES,0.4112595419847328,"the AAAI Conference on Artificial Intelligence, vol. 35, no. 12, 2021, pp. 10 763–10 771.
300"
REFERENCES,0.4122137404580153,"[7] K. Du, Y. Zhang, and H. Guan, “From quantized dnns to quantizable dnns,” CoRR, vol. abs/2004.05284,
301"
REFERENCES,0.4131679389312977,"2020. [Online]. Available: https://arxiv.org/abs/2004.05284
302"
REFERENCES,0.41412213740458015,"[8] X. Sun, R. Panda, C.-F. R. Chen, N. Wang, B. Pan, A. Oliva, R. Feris, and K. Saenko, “Improved techniques
303"
REFERENCES,0.4150763358778626,"for quantizing deep networks with adaptive bit-widths,” in Proceedings of the IEEE/CVF Winter Conference
304"
REFERENCES,0.41603053435114506,"on Applications of Computer Vision, 2024, pp. 957–967.
305"
REFERENCES,0.4169847328244275,"[9] A. Bulat and G. Tzimiropoulos, “Bit-mixer: Mixed-precision networks with runtime bit-width selection,”
306"
REFERENCES,0.4179389312977099,"in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 5188–5197.
307"
REFERENCES,0.41889312977099236,"[10] C. Tang, H. Zhai, K. Ouyang, Z. Wang, Y. Zhu, and W. Zhu, “Arbitrary bit-width network:
308"
REFERENCES,0.4198473282442748,"A joint layer-wise quantization and adaptive inference approach,” 2022. [Online]. Available:
309"
REFERENCES,0.4208015267175573,"https://arxiv.org/abs/2204.09992
310"
REFERENCES,0.4217557251908397,"[11] Z. Dong, Z. Yao, D. Arfeen, A. Gholami, M. W. Mahoney, and K. Keutzer, “Hawq-v2: Hessian aware
311"
REFERENCES,0.42270992366412213,"trace-weighted quantization of neural networks,” Advances in neural information processing systems,
312"
REFERENCES,0.42366412213740456,"vol. 33, pp. 18 518–18 529, 2020.
313"
REFERENCES,0.42461832061068705,"[12] Y. Dong, R. Ni, J. Li, Y. Chen, H. Su, and J. Zhu, “Stochastic quantization for learning accurate low-bit
314"
REFERENCES,0.4255725190839695,"deep neural networks,” International Journal of Computer Vision, vol. 127, pp. 1629–1642, 2019.
315"
REFERENCES,0.4265267175572519,"[13] Q. Jin, L. Yang, and Z. Liao, “Towards efficient training for neural network quantization,” arXiv preprint
316"
REFERENCES,0.42748091603053434,"arXiv:1912.10207, 2019.
317"
REFERENCES,0.42843511450381677,"[14] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” CoRR, vol.
318"
REFERENCES,0.42938931297709926,"abs/1512.03385, 2015. [Online]. Available: http://arxiv.org/abs/1512.03385
319"
REFERENCES,0.4303435114503817,"[15] K. Kim, B. Ji, D. Yoon, and S. Hwang, “Self-knowledge distillation with progressive refinement of targets,”
320"
REFERENCES,0.4312977099236641,"in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 6567–6576.
321"
REFERENCES,0.43225190839694655,"[16] K. Wang, Z. Liu, Y. Lin, J. Lin, and S. Han, “Haq: Hardware-aware automated quantization with mixed
322"
REFERENCES,0.43320610687022904,"precision,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
323"
REFERENCES,0.43416030534351147,"2019, pp. 8612–8620.
324"
REFERENCES,0.4351145038167939,"[17] A. Elthakeb, P. Pilligundla, F. Mireshghallah, A. Yazdanbakhsh, S. Gao, and H. Esmaeilzadeh, “Releq: an
325"
REFERENCES,0.4360687022900763,"automatic reinforcement learning approach for deep quantization of neural networks,” in NeurIPS ML for
326"
REFERENCES,0.43702290076335876,"Systems workshop, 2018, 2019.
327"
REFERENCES,0.43797709923664124,"[18] B. Wu, Y. Wang, P. Zhang, Y. Tian, P. Vajda, and K. Keutzer, “Mixed precision quantization of convnets
328"
REFERENCES,0.4389312977099237,"via differentiable neural architecture search,” arXiv preprint arXiv:1812.00090, 2018.
329"
REFERENCES,0.4398854961832061,"[19] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y. Wei, and J. Sun, “Single path one-shot neural architecture
330"
REFERENCES,0.44083969465648853,"search with uniform sampling,” in Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
331"
REFERENCES,0.44179389312977096,"UK, August 23–28, 2020, Proceedings, Part XVI 16.
Springer, 2020, pp. 544–560.
332"
REFERENCES,0.44274809160305345,"[20] M. Shen, F. Liang, R. Gong, Y. Li, C. Li, C. Lin, F. Yu, J. Yan, and W. Ouyang, “Once quantization-aware
333"
REFERENCES,0.4437022900763359,"training: High performance extremely low-bit architecture search,” in Proceedings of the IEEE/CVF
334"
REFERENCES,0.4446564885496183,"International Conference on Computer Vision (ICCV), October 2021, pp. 5340–5349.
335"
REFERENCES,0.44561068702290074,"[21] J. Liu, J. Cai, and B. Zhuang, “Sharpness-aware quantization for deep neural networks,” arXiv preprint
336"
REFERENCES,0.44656488549618323,"arXiv:2111.12273, 2021.
337"
REFERENCES,0.44751908396946566,"[22] Z. Yao, Z. Dong, Z. Zheng, A. Gholami, J. Yu, E. Tan, L. Wang, Q. Huang, Y. Wang, M. Mahoney
338"
REFERENCES,0.4484732824427481,"et al., “Hawq-v3: Dyadic neural network quantization,” in International Conference on Machine Learning.
339"
REFERENCES,0.4494274809160305,"PMLR, 2021, pp. 11 875–11 886.
340"
REFERENCES,0.45038167938931295,"[23] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. G. Howard, H. Adam, and D. Kalenichenko,
341"
REFERENCES,0.45133587786259544,"“Quantization and training of neural networks for efficient integer-arithmetic-only inference,” CoRR, vol.
342"
REFERENCES,0.45229007633587787,"abs/1712.05877, 2017. [Online]. Available: http://arxiv.org/abs/1712.05877
343"
REFERENCES,0.4532442748091603,"[24] Y. Bengio, N. Léonard, and A. Courville, “Estimating or propagating gradients through stochastic neurons
344"
REFERENCES,0.4541984732824427,"for conditional computation,” arXiv preprint arXiv:1308.3432, 2013.
345"
REFERENCES,0.4551526717557252,"[25] Y. You, I. Gitman, and B. Ginsburg, “Large batch training of convolutional networks,” arXiv preprint
346"
REFERENCES,0.45610687022900764,"arXiv:1708.03888, 2017.
347"
REFERENCES,0.4570610687022901,"[26] Z. Cai and N. Vasconcelos, “Rethinking differentiable search for mixed-precision neural networks,” in
348"
REFERENCES,0.4580152671755725,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 2349–
349"
REFERENCES,0.45896946564885494,"2358.
350"
REFERENCES,0.4599236641221374,"[27] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y. Wei, and J. Sun, “Single path one-shot neural architecture
351"
REFERENCES,0.46087786259541985,"search with uniform sampling,” in European conference on computer vision.
Springer, 2020, pp. 544–560.
352"
REFERENCES,0.4618320610687023,"[28] Z. Dong, Z. Yao, A. Gholami, M. W. Mahoney, and K. Keutzer, “Hawq: Hessian aware quantization of
353"
REFERENCES,0.4627862595419847,"neural networks with mixed-precision,” in Proceedings of the IEEE/CVF International Conference on
354"
REFERENCES,0.4637404580152672,"Computer Vision, 2019, pp. 293–302.
355"
REFERENCES,0.46469465648854963,"[29] Y. Ma, T. Jin, X. Zheng, Y. Wang, H. Li, Y. Wu, G. Jiang, W. Zhang, and R. Ji, “Ompq: Orthogonal mixed
356"
REFERENCES,0.46564885496183206,"precision quantization,” in Proceedings of the AAAI conference on artificial intelligence, vol. 37, no. 7,
357"
REFERENCES,0.4666030534351145,"2023, pp. 9029–9037.
358"
REFERENCES,0.4675572519083969,"[30] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image
359"
REFERENCES,0.4685114503816794,"database,” in 2009 IEEE conference on computer vision and pattern recognition.
Ieee, 2009, pp. 248–255.
360"
REFERENCES,0.46946564885496184,"[31] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mobilenetv2: Inverted residuals and
361"
REFERENCES,0.47041984732824427,"linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition,
362"
REFERENCES,0.4713740458015267,"2018, pp. 4510–4520.
363"
REFERENCES,0.4723282442748092,"[32] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980,
364"
REFERENCES,0.4732824427480916,"2014.
365"
REFERENCES,0.47423664122137404,"[33] T. Sheng, C. Feng, S. Zhuo, X. Zhang, L. Shen, and M. Aleksic, “A quantization-friendly separable
366"
REFERENCES,0.4751908396946565,"convolution for mobilenets,” in 2018 1st Workshop on Energy Efficient Machine Learning and Cognitive
367"
REFERENCES,0.4761450381679389,"Computing for Embedded Applications (EMC2).
IEEE, 2018, pp. 14–18.
368"
REFERENCES,0.4770992366412214,"[34] Q. Sun, X. Li, Y. Ren, Z. Huang, X. Liu, L. Jiao, and F. Liu, “One model for all quantization: A quantized
369"
REFERENCES,0.4780534351145038,"network supporting hot-swap bit-width adjustment,” arXiv preprint arXiv:2105.01353, 2021.
370"
REFERENCES,0.47900763358778625,"[35] M. Alizadeh, A. Behboodi, M. van Baalen, C. Louizos, T. Blankevoort, and M. Welling, “Gradient l1
371"
REFERENCES,0.4799618320610687,"regularization for quantization robustness,” arXiv preprint arXiv:2002.07520, 2020.
372"
REFERENCES,0.48091603053435117,"[36] B. Chmiel, R. Banner, G. Shomron, Y. Nahshan, A. Bronstein, U. Weiser et al., “Robust quantization: One
373"
REFERENCES,0.4818702290076336,"model to rule them all,” Advances in neural information processing systems, vol. 33, pp. 5308–5317, 2020.
374"
REFERENCES,0.48282442748091603,"[37] H. Wu, R. He, H. Tan, X. Qi, and K. Huang, “Vertical layering of quantized neural networks for heteroge-
375"
REFERENCES,0.48377862595419846,"neous inference,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 12, pp.
376"
REFERENCES,0.4847328244274809,"15 964–15 978, 2023.
377"
REFERENCES,0.4856870229007634,"[38] Y. Bhalgat, J. Lee, M. Nagel, T. Blankevoort, and N. Kwak, “Lsq+: Improving low-bit quantization through
378"
REFERENCES,0.4866412213740458,"learnable offsets and better initialization,” in Proceedings of the IEEE/CVF Conference on Computer Vision
379"
REFERENCES,0.48759541984732824,"and Pattern Recognition Workshops, 2020, pp. 696–697.
380"
REFERENCES,0.48854961832061067,"[39] J. Yu, L. Yang, N. Xu, J. Yang, and T. Huang, “Slimmable neural networks,” arXiv preprint
381"
REFERENCES,0.48950381679389315,"arXiv:1812.08928, 2018.
382"
REFERENCES,0.4904580152671756,"A
Appendix / supplemental material
383"
REFERENCES,0.491412213740458,"A.1
Overview
384"
REFERENCES,0.49236641221374045,"In this supplementary material, we present more explanations and experimental results.
385"
REFERENCES,0.4933206106870229,"• First, we provide a detailed explanation of the different quantization types under QAT.
386"
REFERENCES,0.49427480916030536,"• We then present a comparison of multi-precision and separate-precision on the ImageNet-1k dataset.
387"
REFERENCES,0.4952290076335878,"• Furthermore, we provide the gradient formulation of Double Rounding.
388"
REFERENCES,0.4961832061068702,"• And, the algorithm implementation of both multi-precision and mixed-precision training approaches.
389"
REFERENCES,0.49713740458015265,"• Finally, we provide more gradient statistics of learnable quantization scales in different networks.
390"
REFERENCES,0.49809160305343514,"A.2
Different Quantization Types
391"
REFERENCES,0.49904580152671757,"In this section, we provide a detailed explanation of the different quantization types during
392"
REFERENCES,0.5,"Quantization-Aware Training (QAT), as is shown in Figure 6."
REFERENCES,0.5009541984732825,"(a) Separate-Precision:  Each bit-width 
requires training a new network with 
separate weights by repeating multi-retrain."
REFERENCES,0.5019083969465649,"(b) Multi-Precision: A shared network can be 
quantized to any bit-width at runtime without 
re-training or finetuning. All layers inside the 
network uniformly share the same bit-width."
REFERENCES,0.5028625954198473,"(c) Mixed-Precision: A SuperNet whose 
individual layers can be quantized to any 
bit-width at runtime, and its searched 
subnets without re-training or fine-tuning."
BIT,0.5038167938931297,"8bit
2bit 𝐿𝐿1 𝐿𝐿2 𝐿𝐿n"
BIT,0.5047709923664122,"32bit
6bit
8bit
2bit 𝐿𝐿1 𝐿𝐿2 𝐿𝐿n"
BIT,0.5057251908396947,"32bit
6bit
8bit
2bit 𝐿𝐿1 𝐿𝐿2 𝐿𝐿n"
BIT,0.5066793893129771,"32bit
6bit"
BIT,0.5076335877862596,"Separate Train
Layer
Input
Bit Mixing
Data Flow
Bit Switching"
BIT,0.5085877862595419,Figure 6: Comparison between different quantization types during quantization-aware training. 393
BIT,0.5095419847328244,"A.3
Multi-Precision vs. Separate-Precision.
394"
BIT,0.5104961832061069,"We provide the comparison of Multi-Precision and Separate-Precision on ImageNet-1K dataset.
395"
BIT,0.5114503816793893,"Table 6 shows that our Multi-Precision joint training scheme has comparable accuracy of different
396"
BIT,0.5124045801526718,"precisions compared to Separate-Precision with multiple re-train. This further proves the effectiveness
397"
BIT,0.5133587786259542,of our proposed One-shot Double Rounding Multi-Precision method.
BIT,0.5143129770992366,"Table 6: Top1 accuracy comparisons on multi-precision of {8,6,4,2}-bit on ImageNet-1K datasets."
BIT,0.5152671755725191,"Model
Method
One-shot
Storage
Epoch
w8a8
w6a6
w4a4
w2a2
FP"
BIT,0.5162213740458015,"ResNet18
LSQ[2]
✗
{8,6,4,2}-bit
90
71.10
−
71.10
67.60
70.50
LSQ+[38]
✗
{8,6,4,2}-bit
90
−
−
70.80
66.80
70.10
Ours
✓
8-bit
90
70.74
70.71
70.43
66.35
69.76"
BIT,0.517175572519084,"ResNet50
LSQ[2]
✗
{8,6,4,2}-bit
90
76.80
−
76.70
73.70
76.90
Ours
✓
8-bit
90
76.51
76.28
75.74
72.31
76.13 398"
BIT,0.5181297709923665,"A.4
The Gradient Formulation of Double Rounding
399"
BIT,0.5190839694656488,"A general formulation for uniform quantization process is as follows:
400"
BIT,0.5200381679389313,"f
W = clip(
W s"
BIT,0.5209923664122137,"
+ z, −2b−1, 2b−1 −1)
(8)"
BIT,0.5219465648854962,"c
W = (f
W −z) × s
(9)"
BIT,0.5229007633587787,"where the symbol ⌊.⌉denotes the Rounding function, clip(x, low, upper) expresses x below low
401"
BIT,0.523854961832061,"are set to low and above upper are set to upper. b denotes the quantization level (or bit-width),
402"
BIT,0.5248091603053435,"s ∈R and z ∈Z represents the quantization scale (or interval) and zero-point associated with each b,
403"
BIT,0.5257633587786259,"respectively. W represents the FP32 model’s weights, f
W signifies the quantized integer weights, and
404"
BIT,0.5267175572519084,"c
W represents the dequantized floating-point weights.
405"
BIT,0.5276717557251909,"The quantization scale of our Double Rounding is learned online and not fixed. And it only needs a
406"
BIT,0.5286259541984732,"pair of shared quantization parameters, i.e., scale and zero-point. Suppose the highest-bit and the
407"
BIT,0.5295801526717557,"low-bit are denoted as h-bit and l-bit respectively, and the difference between them is ∆= h −l.
408"
BIT,0.5305343511450382,"The specific formulation is as follows:
409"
BIT,0.5314885496183206,"f
Wh = clip(
W −zh sh"
BIT,0.5324427480916031,"
, −2h−1, 2h−1 −1)
(10)"
BIT,0.5333969465648855,"f
Wl = clip("
BIT,0.5343511450381679,"$ f
Wh 2∆ '"
BIT,0.5353053435114504,", −2l−1, 2l−1 −1)
(11)"
BIT,0.5362595419847328,"c
Wl = f
Wl × sh × 2∆+ zh
(12)"
BIT,0.5372137404580153,"where sh ∈R and zh ∈Z denote the highest-bit quantization scale and zero-point respectively. f
Wh
410"
BIT,0.5381679389312977,"and f
Wl represent the quantized weights of the highest-bit and low-bit respectively. Hardware shift
411"
BIT,0.5391221374045801,"operations can efficiently execute the division and multiplication by 2∆. And the zh is 0 for the
412"
BIT,0.5400763358778626,"weight quantization in this paper. The gradient formulation of Double Rounding for one-shot joint
413"
BIT,0.541030534351145,"training is represented as follows:
414"
BIT,0.5419847328244275,"∂bY
∂sh
≃"
BIT,0.5429389312977099,"(j
Y −zh sh"
BIT,0.5438931297709924,"m
−Y −zh"
BIT,0.5448473282442748,"sh
if n < Y −zh"
BIT,0.5458015267175572,"sh
< p,"
BIT,0.5467557251908397,"n
or
p
otherwise.
(13)"
BIT,0.5477099236641222,"∂bY
∂zh
≃"
BIT,0.5486641221374046,"(
0
if n < Y −zh"
BIT,0.549618320610687,"sh
< p,
1
otherwise.
(14)"
BIT,0.5505725190839694,"where n and p denote the lower and upper bounds of the integer range [Nmin, Nmax] for quantizing
415"
BIT,0.5515267175572519,"the weights or activations respectively. Y represents the FP32 weights or activations, and bY represents
416"
BIT,0.5524809160305344,"the dequantized weights or activations. Unlike weights, activation quantization scale and zero-point
417"
BIT,0.5534351145038168,"of different precisions are learned independently. However, the gradient formulation is the same.
418"
BIT,0.5543893129770993,"A.5
Algorithms
419"
BIT,0.5553435114503816,"This section provides the algorithm implementations of multi-precision, one-shot mixed-precision
420"
BIT,0.5562977099236641,"joint training, and the search stage of SubNets.
421"
BIT,0.5572519083969466,"A.5.1
Multi-Precision Joint Training
422"
BIT,0.558206106870229,"The multi-precision model with different quantization precisions shares the same model weight(e.g.,
423"
BIT,0.5591603053435115,"the highest-bit) during joint training. In conventional multi-precision, the shared weight (e.g., multi-
424"
BIT,0.5601145038167938,"precision model) computes n forward processes at each training iteration, where n is the number of
425"
BIT,0.5610687022900763,"candidate bit-widths. Then, all attained losses of different precisions perform an accumulation, and
426"
BIT,0.5620229007633588,"update the parameters accordingly. For specific implementation details please refer to Algorithm A.1.
427"
BIT,0.5629770992366412,"However, we find that if separate precision loss and parameter updates are performed directly after
428"
BIT,0.5639312977099237,"calculating a precision at each forward process, it will lead to difficulty convergence during training
429"
BIT,0.5648854961832062,"or suboptimal accuracy. In other words, the varying gradient magnitudes of quantization scales of
430"
BIT,0.5658396946564885,"different precisions make it hard to attain stable convergence during joint training. To address this
431"
BIT,0.566793893129771,"issue, we introduce an adaptive approach (e.g., Adaptive Learning Rate Scaling, ALRS) to alter the
432"
BIT,0.5677480916030534,"learning rate for different precisions during training, aiming to achieve a consistent update pace.
433"
BIT,0.5687022900763359,"This method allows us to directly update the shared parameters after calculating the loss after every
434"
BIT,0.5696564885496184,"forward. We update both the weight parameters and quantization parameters simultaneously using
435"
BIT,0.5706106870229007,"dual optimizers. We also set the weight-decay of the quantization scales to 0 to achieve more stable
436"
BIT,0.5715648854961832,"convergence. For specific implementation details, please refer to Algorithm A.2.
437"
BIT,0.5725190839694656,"Algorithm A.1 Conventional Multi-precision training approach
Require: Candidate bit-widths set b ∈B;"
BIT,0.5734732824427481,"1: Initialize: Pretrained model M with FP32 weights W, the quantization scales s including of weights sw
and activations sx, BatchNorm layers:{BN}n
b=1, optimizer:optim(W, s, wd), learning rate: λ, wd: weight
decay, CE: CrossEntropyLoss, Dtrain: training dataset;
2: For one epoch:
3: Sample mini-batch data (x, y) ∈{Dtrain}
4: for b in B do
5:
forward(M, x, y, b):
6:
for each quantization layer do
7:
c
W b = dequant(quant(W, sb
w))
8:
b
Xb = dequant(quant(X, sb
x))
9:
Ob = Conv(c
W b, b
Xb)
10:
end for
11:
ob = FC(W, Ob)
12:
Update BN b layer
13:
Compute loss: Lb = CE(ob, y)
14:
Compute gradients: Lb.backward()
15: end for
16: Update weights and scales: optim.step(λ)
17: Clear gradient: optim.zero_grad();
Note that n and L represent the number of candidate bit-widths and model layers respectively."
BIT,0.5744274809160306,"Algorithm A.2 Our Multi-precision training approach
Require: Candidate bit-widths set b ∈B"
BIT,0.575381679389313,"1: Initialize: Pretrained model M with FP32 weights W, the quantization scales s including of weights sw and
activations sx, BatchNorm layers: {BN}n
b=1, optimizers: optim1(W, wd), optim2(s, wd = 0), learning
rate: λ, wd: weight decay, CE: CrossEntropyLoss, Dtrain: training dataset;
2: For every epoch:
3: Sample mini-batch data (x, y) ∈{Dtrain}
4: for b in B do
5:
forward(M, x, y, b):
6:
for each quantization layer do
7:
c
W b = dequant(quant(W, sb
w))
8:
b
Xb = dequant(quant(X, sb
x))
9:
Ob = Conv(c
W b, b
Xb)
10:
end for
11:
ob = FC(W, Ob)
12:
Update BN b layer
13:
Compute loss: Lb = CE(ob, y)
14:
Compute gradients: Lb.backward()
15:
Compute learning rate: λb
# please see formula (6) of the main paper
16:
Update weights and quantization scales: optim1.step(λ); optim2.step(λb)
17:
Clear gradient: optim1.zero_grad(); optim2.zero_grad()
18: end for
Note that n and L represent the number of candidate bit-widths and model layers respectively."
BIT,0.5763358778625954,"A.5.2
One-shot Joint Training for Mixed Precision SuperNet
438"
BIT,0.5772900763358778,"Unlike multi-precision joint quantization, the bit-switching of mixed-precision training is more
439"
BIT,0.5782442748091603,"complicated. In multi-precision training, the bit-widths calculated in each iteration are fixed, e.g.,
440"
BIT,0.5791984732824428,"{8,6,4,2}-bit. In mixed-precision training, the bit-widths of different layers are not fixed in each
441"
BIT,0.5801526717557252,"iteration, e.g., {8,random-bit,2}-bit, where ""random-bit"" is any bits of e.g., {7,6,5,4,3,2}-bit, similar
442"
BIT,0.5811068702290076,"to the sandwich strategy of [39]. Therefore, mixed precision training often requires more training
443"
BIT,0.5820610687022901,"epochs to reach convergence compared to multi-precision training. Bit-mixer [9] conducts the same
444"
BIT,0.5830152671755725,"probability of selecting bit-width for different layers. However, we take the sensitivity of each layer
445"
BIT,0.583969465648855,"into consideration which uses sensitivity (e.g. Hessian Matrix Trace [11]) as a metric to identify the
446"
BIT,0.5849236641221374,"selection probability of different layers. For more sensitive layers, preference is given to higher-bit
447"
BIT,0.5858778625954199,"widths, and vice versa. We refer to this training strategy as a Hessian-Aware Stochastic Bit-switching
448"
BIT,0.5868320610687023,"(HASB) strategy for optimizing one-shot mixed-precision SuperNet. Specific implementation details
449"
BIT,0.5877862595419847,"can be found in Algorithm A.3. In additionally, unlike multi-precision joint training, the BN layers
450"
BIT,0.5887404580152672,"are replaced by TBN (Transitional Batch-Norm) [9], which compensates for the distribution shift
451"
BIT,0.5896946564885496,"between adjacent layers that are quantized to different bit-widths. To achieve the best convergence
452"
BIT,0.5906488549618321,"effect, we propose that the threshold of bit-switching (i.e., σ) also increases as the epoch increases.
453"
BIT,0.5916030534351145,Algorithm A.3 Our one-shot Mixed-precision SuperNet training approach
BIT,0.5925572519083969,"Require: Candidate bit-widths set b ∈B, the HMT of different layers of FP32 model: tl ∈{T}L
l=1, average"
BIT,0.5935114503816794,HMT: tm =
BIT,0.5944656488549618,"PL
l=1 tl"
BIT,0.5954198473282443,"L
;
1: Initialize: Pretrained model M with FP32 weights W, the quantization scales s including of weights sw and
activations sx, BatchNorm layers:{BN}n2
b=1, the threshold of bit-switching:σ, optimizer:optim(W, s, wd),
learning rate: λ, wd: weight decay, CE: CrossEntropyLoss, Dtrain: training dataset;
2: For one epoch:
3: Attain the threshold of bit-switching: σ = σ ×
epoch+1
total_epochs
4: Sample mini-batch data (x, y) ∈{Dtrain}
5: for b in B do
6:
forward(M, x, y, b, T, tm):
7:
for each quantization layer do
8:
Sample r ∼U[0, 1];
9:
if r < σ then
10:
b = Roulette(B, tl, tm)
# Please refer to Algorithm 1 of the main paper
11:
end if
12:
c
W b = dequant(quant(W, sb
w))
13:
b
Xb = dequant(quant(X, sb
x))
14:
Ob = Conv(c
W b, b
Xb)
15:
end for
16:
ob = FC(W, Ob)
17:
Update BN b layer
18:
Compute loss: Lb = CE(ob, y)
19:
Compute gradients: Lb.backward()
20:
Update weights and scales: optim.step(λ)
21:
Clear gradient: optim.zero_grad();
22: end for
Note that n and L represent the number of candidate bit-widths and model layers respectively."
BIT,0.5963740458015268,"A.5.3
Efficient one-shot searching for Mixed Precision SuperNet
454"
BIT,0.5973282442748091,"After training the mixed-precision SuperNet, the next step is to select the appropriate optimal SubNets
455"
BIT,0.5982824427480916,"based on conditions, such as model parameters, latency, and FLOPs, for actual deployment and
456"
BIT,0.5992366412213741,"inference. To achieve optimal allocations for candidate bit-width under given conditions, we employ
457"
BIT,0.6001908396946565,"the Iterative Integer Linear Programming (ILP) approach. Since each ILP run can only provide
458"
BIT,0.601145038167939,"one solution, we obtain multiple solutions by altering the values of different average bit widths.
459"
BIT,0.6020992366412213,"Specifically, given a trained SuperNet (e.g., RestNet18), it takes less than two minutes to solve
460"
BIT,0.6030534351145038,"candidate SubNets. It can be implemented through the Python PULP package. Finally, these searched
461"
BIT,0.6040076335877863,"SubNets only need inference to attain final accuracy, which needs a few hours. This forms a Pareto
462"
BIT,0.6049618320610687,"optimal frontier. From this frontier, we can select the appropriate subnet for deployment. Specific
463"
BIT,0.6059160305343512,"implementation details of the searching process by ILP can be found in Algorithm 2.
464"
BIT,0.6068702290076335,"A.6
The Gradient Statistics of Learnable Scale of Quantization
465"
BIT,0.607824427480916,"In this section, we analyze the changes in gradients of the learnable scale for different models during
466"
BIT,0.6087786259541985,"the training process. Figure 7 and Figure 8 display the gradient statistical results for ResNet20 on
467"
BIT,0.6097328244274809,"CIFAR-10. Similarly, Figure 9 and Figure 10 show the gradient statistical results for ResNet18 on
468"
BIT,0.6106870229007634,"ImageNet-1K, and Figure 11 and Figure 12 present the gradient statistical results for ResNet50 on
469"
BIT,0.6116412213740458,"ImageNet-1K. These figures reveal a similarity in the range of gradient changes between higher-bit
470"
BIT,0.6125954198473282,"quantization and 2-bit quantization. Notably, they illustrate that the value range of 2-bit quantization
471"
BIT,0.6135496183206107,"is noticeably an order of magnitude higher than the value ranges of higher-bit quantization.
472"
BIT,0.6145038167938931,1 2 3 4 5 6 7 8 9 1011121314151617181920 Layer 6 4 2 0 2 4
BIT,0.6154580152671756,The gradient of weight scale
E,0.6164122137404581,"1e
2
8bit"
E,0.6173664122137404,1 2 3 4 5 6 7 8 9 1011121314151617181920 Layer 3 2 1 0 1 2 3
E,0.6183206106870229,The gradient of weight scale
E,0.6192748091603053,"1e
2
6bit"
E,0.6202290076335878,1 2 3 4 5 6 7 8 9 1011121314151617181920 Layer 3 2 1 0 1 2 3
E,0.6211832061068703,The gradient of weight scale
E,0.6221374045801527,"1e
2
4bit"
E,0.6230916030534351,1 2 3 4 5 6 7 8 9 1011121314151617181920 Layer 8 6 4 2 0 2 4 6 8
E,0.6240458015267175,The gradient of weight scale
E,0.625,"1e
2
2bit"
E,0.6259541984732825,"Figure 7: The scale gradient statistics of weight of ResNet20 on CIFAR-10 dataset. Note that the
outliers are removed for exhibition."
E,0.6269083969465649,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18
Layer 2.5 2.0 1.5 1.0 0.5 0.0"
E,0.6278625954198473,The gradient of activation scale
E,0.6288167938931297,"1e
3
8bit"
E,0.6297709923664122,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18
Layer 3 2 1 0 1"
E,0.6307251908396947,The gradient of activation scale
E,0.6316793893129771,"1e
4
6bit"
E,0.6326335877862596,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18
Layer 1.2 1.0 0.8 0.6 0.4 0.2 0.0 0.2"
E,0.6335877862595419,The gradient of activation scale
E,0.6345419847328244,"1e
3
4bit"
E,0.6354961832061069,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18
Layer 4 3 2 1 0 1"
E,0.6364503816793893,The gradient of activation scale
E,0.6374045801526718,"1e
3
2bit"
E,0.6383587786259542,"Figure 8: The scale gradient statistics of activation of ResNet20 on CIFAR-10 dataset. Note that the
first and last layers are not quantized."
E,0.6393129770992366,"1 2 3 4 5 6 7 8 9 10111213141516171819
Layer 6 4 2 0 2 4 6"
E,0.6402671755725191,The gradient of weight scale
E,0.6412213740458015,"1e
2
8bit"
E,0.642175572519084,"1 2 3 4 5 6 7 8 9 10111213141516171819
Layer 4 3 2 1 0 1 2 3"
E,0.6431297709923665,The gradient of weight scale
E,0.6440839694656488,"1e
2
6bit"
E,0.6450381679389313,"1 2 3 4 5 6 7 8 9 10111213141516171819
Layer 4 2 0 2 4"
E,0.6459923664122137,The gradient of weight scale
E,0.6469465648854962,"1e
2
4bit"
E,0.6479007633587787,"1 2 3 4 5 6 7 8 9 10111213141516171819
Layer 2 1 0 1 2"
E,0.648854961832061,The gradient of weight scale
E,0.6498091603053435,"1e
1
2bit"
E,0.6507633587786259,"Figure 9: The scale gradient statistics of weight of ResNet18 on ImageNet dataset. Note that the
outliers are removed for exhibition."
E,0.6517175572519084,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16
Layer 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75"
E,0.6526717557251909,The gradient of activation scale
E,0.6536259541984732,"1e
3
8bit"
E,0.6545801526717557,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16
Layer 6 4 2 0 2 4 6"
E,0.6555343511450382,The gradient of activation scale
E,0.6564885496183206,"1e
4
6bit"
E,0.6574427480916031,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16
Layer 4 2 0 2 4"
E,0.6583969465648855,The gradient of activation scale
E,0.6593511450381679,"1e
4
4bit"
E,0.6603053435114504,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16
Layer 4 2 0 2 4"
E,0.6612595419847328,The gradient of activation scale
E,0.6622137404580153,"1e
4
2bit"
E,0.6631679389312977,"Figure 10: The scale gradient statistics of activation of ResNet18 on ImageNet dataset. Note that the
outliers are removed for exhibition."
E,0.6641221374045801,1 2 3 4 5 6 7 8 9 10111213141516171819202122232425262728293031323334353637383940414243444546474849505152 Layer 7.5 5.0 2.5 0.0 2.5 5.0 7.5
E,0.6650763358778626,The gradient of weight scale
BIT,0.666030534351145,8bit
BIT,0.6669847328244275,1 2 3 4 5 6 7 8 9 10111213141516171819202122232425262728293031323334353637383940414243444546474849505152 Layer 4 2 0 2 4
BIT,0.6679389312977099,The gradient of weight scale
BIT,0.6688931297709924,6bit
BIT,0.6698473282442748,1 2 3 4 5 6 7 8 9 10111213141516171819202122232425262728293031323334353637383940414243444546474849505152 Layer 6 4 2 0 2 4 6
BIT,0.6708015267175572,The gradient of weight scale
BIT,0.6717557251908397,4bit
BIT,0.6727099236641222,1 2 3 4 5 6 7 8 9 10111213141516171819202122232425262728293031323334353637383940414243444546474849505152 Layer 3 2 1 0 1 2 3
BIT,0.6736641221374046,The gradient of weight scale
BIT,0.674618320610687,"1e2
2bit"
BIT,0.6755725190839694,"Figure 11: The scale gradient statistics of weight of ResNet50 on ImageNet dataset. Note that the
outliers are removed for exhibition, and the first and last layers are not quantized."
BIT,0.6765267175572519,1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930313233343536373839404142434445464748 Layer 2 1 0 1 2
BIT,0.6774809160305344,The gradient of activation scale
E,0.6784351145038168,"1e
5
8bit"
E,0.6793893129770993,1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930313233343536373839404142434445464748 Layer 7.5 5.0 2.5 0.0 2.5 5.0 7.5
E,0.6803435114503816,The gradient of activation scale
E,0.6812977099236641,"1e
5
6bit"
E,0.6822519083969466,1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930313233343536373839404142434445464748 Layer 1.5 1.0 0.5 0.0 0.5 1.0 1.5
E,0.683206106870229,The gradient of activation scale
E,0.6841603053435115,"1e
3
4bit"
E,0.6851145038167938,1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930313233343536373839404142434445464748 Layer 6 4 2 0 2 4 6 8
E,0.6860687022900763,The gradient of activation scale
E,0.6870229007633588,"1e
2
2bit"
E,0.6879770992366412,"Figure 12: The scale gradient statistics of activation of ResNet50 on ImageNet dataset. Note that the
outliers are removed for exhibition."
E,0.6889312977099237,"NeurIPS Paper Checklist
473"
CLAIMS,0.6898854961832062,"1. Claims
474"
CLAIMS,0.6908396946564885,"Question: Do the main claims made in the abstract and introduction accurately reflect the
475"
CLAIMS,0.691793893129771,"paper’s contributions and scope?
476"
CLAIMS,0.6927480916030534,"Answer: [Yes]
477"
CLAIMS,0.6937022900763359,"Justification: [TODO] Please refer to the Abstract Section and Section 1, where related
478"
CLAIMS,0.6946564885496184,"material for the question can be found.
479"
CLAIMS,0.6956106870229007,"Guidelines:
480"
CLAIMS,0.6965648854961832,"• The answer NA means that the abstract and introduction do not include the claims
481"
CLAIMS,0.6975190839694656,"made in the paper.
482"
CLAIMS,0.6984732824427481,"• The abstract and/or introduction should clearly state the claims made, including the
483"
CLAIMS,0.6994274809160306,"contributions made in the paper and important assumptions and limitations. A No or
484"
CLAIMS,0.700381679389313,"NA answer to this question will not be perceived well by the reviewers.
485"
CLAIMS,0.7013358778625954,"• The claims made should match theoretical and experimental results, and reflect how
486"
CLAIMS,0.7022900763358778,"much the results can be expected to generalize to other settings.
487"
CLAIMS,0.7032442748091603,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
488"
CLAIMS,0.7041984732824428,"are not attained by the paper.
489"
LIMITATIONS,0.7051526717557252,"2. Limitations
490"
LIMITATIONS,0.7061068702290076,"Question: Does the paper discuss the limitations of the work performed by the authors?
491"
LIMITATIONS,0.7070610687022901,"Answer: [TODO][Yes]
492"
LIMITATIONS,0.7080152671755725,"Justification: [TODO] Although our proposed methods have achieved comparable results in
493"
LIMITATIONS,0.708969465648855,"multi-precision and mixed-precision, this paper has several limitations and improvements.
494"
LIMITATIONS,0.7099236641221374,"(1) Due to time and computing resource constraints, our methods are only tested on common
495"
LIMITATIONS,0.7108778625954199,"CNNs-based networks and aren’t tested on ViTs-based networks. (2) For multi-precision,
496"
LIMITATIONS,0.7118320610687023,"compact networks, e.g., MobileNet, still have a big drop in 2bit. We will try to use per-layer
497"
LIMITATIONS,0.7127862595419847,"or per-channel adaptive learning rate adjustment in the future. (3) For mixed precision,
498"
LIMITATIONS,0.7137404580152672,"relying only on one-shot ILP-based SubNets search may yield a suboptimal solution. We
499"
LIMITATIONS,0.7146946564885496,"further need to combine it with other efficient search methods, e.g., genetic algorithms, to
500"
LIMITATIONS,0.7156488549618321,"achieve global optimal.
501"
LIMITATIONS,0.7166030534351145,"Guidelines:
502"
LIMITATIONS,0.7175572519083969,"• The answer NA means that the paper has no limitation while the answer No means that
503"
LIMITATIONS,0.7185114503816794,"the paper has limitations, but those are not discussed in the paper.
504"
LIMITATIONS,0.7194656488549618,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
505"
LIMITATIONS,0.7204198473282443,"• The paper should point out any strong assumptions and how robust the results are to
506"
LIMITATIONS,0.7213740458015268,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
507"
LIMITATIONS,0.7223282442748091,"model well-specification, asymptotic approximations only holding locally). The authors
508"
LIMITATIONS,0.7232824427480916,"should reflect on how these assumptions might be violated in practice and what the
509"
LIMITATIONS,0.7242366412213741,"implications would be.
510"
LIMITATIONS,0.7251908396946565,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
511"
LIMITATIONS,0.726145038167939,"only tested on a few datasets or with a few runs. In general, empirical results often
512"
LIMITATIONS,0.7270992366412213,"depend on implicit assumptions, which should be articulated.
513"
LIMITATIONS,0.7280534351145038,"• The authors should reflect on the factors that influence the performance of the approach.
514"
LIMITATIONS,0.7290076335877863,"For example, a facial recognition algorithm may perform poorly when image resolution
515"
LIMITATIONS,0.7299618320610687,"is low or images are taken in low lighting. Or a speech-to-text system might not be
516"
LIMITATIONS,0.7309160305343512,"used reliably to provide closed captions for online lectures because it fails to handle
517"
LIMITATIONS,0.7318702290076335,"technical jargon.
518"
LIMITATIONS,0.732824427480916,"• The authors should discuss the computational efficiency of the proposed algorithms
519"
LIMITATIONS,0.7337786259541985,"and how they scale with dataset size.
520"
LIMITATIONS,0.7347328244274809,"• If applicable, the authors should discuss possible limitations of their approach to
521"
LIMITATIONS,0.7356870229007634,"address problems of privacy and fairness.
522"
LIMITATIONS,0.7366412213740458,"• While the authors might fear that complete honesty about limitations might be used by
523"
LIMITATIONS,0.7375954198473282,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
524"
LIMITATIONS,0.7385496183206107,"limitations that aren’t acknowledged in the paper. The authors should use their best
525"
LIMITATIONS,0.7395038167938931,"judgment and recognize that individual actions in favor of transparency play an impor-
526"
LIMITATIONS,0.7404580152671756,"tant role in developing norms that preserve the integrity of the community. Reviewers
527"
LIMITATIONS,0.7414122137404581,"will be specifically instructed to not penalize honesty concerning limitations.
528"
THEORY ASSUMPTIONS AND PROOFS,0.7423664122137404,"3. Theory Assumptions and Proofs
529"
THEORY ASSUMPTIONS AND PROOFS,0.7433206106870229,"Question: For each theoretical result, does the paper provide the full set of assumptions and
530"
THEORY ASSUMPTIONS AND PROOFS,0.7442748091603053,"a complete (and correct) proof?
531"
THEORY ASSUMPTIONS AND PROOFS,0.7452290076335878,"Answer: [Yes]
532"
THEORY ASSUMPTIONS AND PROOFS,0.7461832061068703,"Justification: [TODO] We display index numbers wherever formulas and theoretical support
533"
THEORY ASSUMPTIONS AND PROOFS,0.7471374045801527,"are needed. For example, please refer to Section 3.
534"
THEORY ASSUMPTIONS AND PROOFS,0.7480916030534351,"Guidelines:
535"
THEORY ASSUMPTIONS AND PROOFS,0.7490458015267175,"• The answer NA means that the paper does not include theoretical results.
536"
THEORY ASSUMPTIONS AND PROOFS,0.75,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
537"
THEORY ASSUMPTIONS AND PROOFS,0.7509541984732825,"referenced.
538"
THEORY ASSUMPTIONS AND PROOFS,0.7519083969465649,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
539"
THEORY ASSUMPTIONS AND PROOFS,0.7528625954198473,"• The proofs can either appear in the main paper or the supplemental material, but if
540"
THEORY ASSUMPTIONS AND PROOFS,0.7538167938931297,"they appear in the supplemental material, the authors are encouraged to provide a short
541"
THEORY ASSUMPTIONS AND PROOFS,0.7547709923664122,"proof sketch to provide intuition.
542"
THEORY ASSUMPTIONS AND PROOFS,0.7557251908396947,"• Inversely, any informal proof provided in the core of the paper should be complemented
543"
THEORY ASSUMPTIONS AND PROOFS,0.7566793893129771,"by formal proofs provided in appendix or supplemental material.
544"
THEORY ASSUMPTIONS AND PROOFS,0.7576335877862596,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
545"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7585877862595419,"4. Experimental Result Reproducibility
546"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7595419847328244,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
547"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7604961832061069,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7614503816793893,"of the paper (regardless of whether the code and data are provided or not)?
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7624045801526718,"Answer: [Yes] To ensure that our experimental results can be reproduced: (1) we describe
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7633587786259542,"the experimental training settings and algorithm pseudocode in detail in Section 4 and
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7643129770992366,"Section A.5, and (2) we also provide the code related to all experiments in this paper,
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7652671755725191,"allowing the community to improve and conduct further research.
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7662213740458015,"Justification: [TODO]
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.767175572519084,"Guidelines:
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7681297709923665,"• The answer NA means that the paper does not include experiments.
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7690839694656488,"• If the paper includes experiments, a No answer to this question will not be perceived
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7700381679389313,"well by the reviewers: Making the paper reproducible is important, regardless of
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7709923664122137,"whether the code and data are provided or not.
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7719465648854962,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7729007633587787,"to make their results reproducible or verifiable.
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.773854961832061,"• Depending on the contribution, reproducibility can be accomplished in various ways.
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7748091603053435,"For example, if the contribution is a novel architecture, describing the architecture fully
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7757633587786259,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7767175572519084,"be necessary to either make it possible for others to replicate the model with the same
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7776717557251909,"dataset, or provide access to the model. In general. releasing code and data is often
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7786259541984732,"one good way to accomplish this, but reproducibility can also be provided via detailed
567"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7795801526717557,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7805343511450382,"of a large language model), releasing of a model checkpoint, or other means that are
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7814885496183206,"appropriate to the research performed.
570"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7824427480916031,"• While NeurIPS does not require releasing code, the conference does require all submis-
571"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7833969465648855,"sions to provide some reasonable avenue for reproducibility, which may depend on the
572"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7843511450381679,"nature of the contribution. For example
573"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7853053435114504,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
574"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7862595419847328,"to reproduce that algorithm.
575"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7872137404580153,"(b) If the contribution is primarily a new model architecture, the paper should describe
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7881679389312977,"the architecture clearly and fully.
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7891221374045801,"(c) If the contribution is a new model (e.g., a large language model), then there should
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7900763358778626,"either be a way to access this model for reproducing the results or a way to reproduce
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.791030534351145,"the model (e.g., with an open-source dataset or instructions for how to construct
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7919847328244275,"the dataset).
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7929389312977099,"(d) We recognize that reproducibility may be tricky in some cases, in which case
582"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7938931297709924,"authors are welcome to describe the particular way they provide for reproducibility.
583"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7948473282442748,"In the case of closed-source models, it may be that access to the model is limited in
584"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7958015267175572,"some way (e.g., to registered users), but it should be possible for other researchers
585"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7967557251908397,"to have some path to reproducing or verifying the results.
586"
OPEN ACCESS TO DATA AND CODE,0.7977099236641222,"5. Open access to data and code
587"
OPEN ACCESS TO DATA AND CODE,0.7986641221374046,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
588"
OPEN ACCESS TO DATA AND CODE,0.799618320610687,"tions to faithfully reproduce the main experimental results, as described in supplemental
589"
OPEN ACCESS TO DATA AND CODE,0.8005725190839694,"material?
590"
OPEN ACCESS TO DATA AND CODE,0.8015267175572519,"Answer: [Yes]
591"
OPEN ACCESS TO DATA AND CODE,0.8024809160305344,"Justification: [TODO] Our code are available at here and the data is open source dataset.
592"
OPEN ACCESS TO DATA AND CODE,0.8034351145038168,"Guidelines:
593"
OPEN ACCESS TO DATA AND CODE,0.8043893129770993,"• The answer NA means that paper does not include experiments requiring code.
594"
OPEN ACCESS TO DATA AND CODE,0.8053435114503816,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
595"
OPEN ACCESS TO DATA AND CODE,0.8062977099236641,"public/guides/CodeSubmissionPolicy) for more details.
596"
OPEN ACCESS TO DATA AND CODE,0.8072519083969466,"• While we encourage the release of code and data, we understand that this might not be
597"
OPEN ACCESS TO DATA AND CODE,0.808206106870229,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
598"
OPEN ACCESS TO DATA AND CODE,0.8091603053435115,"including code, unless this is central to the contribution (e.g., for a new open-source
599"
OPEN ACCESS TO DATA AND CODE,0.8101145038167938,"benchmark).
600"
OPEN ACCESS TO DATA AND CODE,0.8110687022900763,"• The instructions should contain the exact command and environment needed to run to
601"
OPEN ACCESS TO DATA AND CODE,0.8120229007633588,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
602"
OPEN ACCESS TO DATA AND CODE,0.8129770992366412,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
603"
OPEN ACCESS TO DATA AND CODE,0.8139312977099237,"• The authors should provide instructions on data access and preparation, including how
604"
OPEN ACCESS TO DATA AND CODE,0.8148854961832062,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
605"
OPEN ACCESS TO DATA AND CODE,0.8158396946564885,"• The authors should provide scripts to reproduce all experimental results for the new
606"
OPEN ACCESS TO DATA AND CODE,0.816793893129771,"proposed method and baselines. If only a subset of experiments are reproducible, they
607"
OPEN ACCESS TO DATA AND CODE,0.8177480916030534,"should state which ones are omitted from the script and why.
608"
OPEN ACCESS TO DATA AND CODE,0.8187022900763359,"• At submission time, to preserve anonymity, the authors should release anonymized
609"
OPEN ACCESS TO DATA AND CODE,0.8196564885496184,"versions (if applicable).
610"
OPEN ACCESS TO DATA AND CODE,0.8206106870229007,"• Providing as much information as possible in supplemental material (appended to the
611"
OPEN ACCESS TO DATA AND CODE,0.8215648854961832,"paper) is recommended, but including URLs to data and code is permitted.
612"
OPEN ACCESS TO DATA AND CODE,0.8225190839694656,"6. Experimental Setting/Details
613"
OPEN ACCESS TO DATA AND CODE,0.8234732824427481,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
614"
OPEN ACCESS TO DATA AND CODE,0.8244274809160306,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
615"
OPEN ACCESS TO DATA AND CODE,0.825381679389313,"results?
616"
OPEN ACCESS TO DATA AND CODE,0.8263358778625954,"Answer: [Yes]
617"
OPEN ACCESS TO DATA AND CODE,0.8272900763358778,"Justification: [TODO] Our codes are available here and include all related training and test
618"
OPEN ACCESS TO DATA AND CODE,0.8282442748091603,"details.
619"
OPEN ACCESS TO DATA AND CODE,0.8291984732824428,"Guidelines:
620"
OPEN ACCESS TO DATA AND CODE,0.8301526717557252,"• The answer NA means that the paper does not include experiments.
621"
OPEN ACCESS TO DATA AND CODE,0.8311068702290076,"• The experimental setting should be presented in the core of the paper to a level of detail
622"
OPEN ACCESS TO DATA AND CODE,0.8320610687022901,"that is necessary to appreciate the results and make sense of them.
623"
OPEN ACCESS TO DATA AND CODE,0.8330152671755725,"• The full details can be provided either with the code, in appendix, or as supplemental
624"
OPEN ACCESS TO DATA AND CODE,0.833969465648855,"material.
625"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8349236641221374,"7. Experiment Statistical Significance
626"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8358778625954199,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
627"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8368320610687023,"information about the statistical significance of the experiments?
628"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8377862595419847,"Answer: [Yes]
629"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8387404580152672,"Justification: [TODO] Please refer to the Section A.6 in the appendix.
630"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8396946564885496,"Guidelines:
631"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8406488549618321,"• The answer NA means that the paper does not include experiments.
632"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8416030534351145,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
633"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8425572519083969,"dence intervals, or statistical significance tests, at least for the experiments that support
634"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8435114503816794,"the main claims of the paper.
635"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8444656488549618,"• The factors of variability that the error bars are capturing should be clearly stated (for
636"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8454198473282443,"example, train/test split, initialization, random drawing of some parameter, or overall
637"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8463740458015268,"run with given experimental conditions).
638"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8473282442748091,"• The method for calculating the error bars should be explained (closed form formula,
639"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8482824427480916,"call to a library function, bootstrap, etc.)
640"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8492366412213741,"• The assumptions made should be given (e.g., Normally distributed errors).
641"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8501908396946565,"• It should be clear whether the error bar is the standard deviation or the standard error
642"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.851145038167939,"of the mean.
643"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8520992366412213,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
644"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8530534351145038,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
645"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8540076335877863,"of Normality of errors is not verified.
646"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8549618320610687,"• For asymmetric distributions, the authors should be careful not to show in tables or
647"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8559160305343512,"figures symmetric error bars that would yield results that are out of range (e.g. negative
648"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8568702290076335,"error rates).
649"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.857824427480916,"• If error bars are reported in tables or plots, The authors should explain in the text how
650"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8587786259541985,"they were calculated and reference the corresponding figures or tables in the text.
651"
EXPERIMENTS COMPUTE RESOURCES,0.8597328244274809,"8. Experiments Compute Resources
652"
EXPERIMENTS COMPUTE RESOURCES,0.8606870229007634,"Question: For each experiment, does the paper provide sufficient information on the com-
653"
EXPERIMENTS COMPUTE RESOURCES,0.8616412213740458,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
654"
EXPERIMENTS COMPUTE RESOURCES,0.8625954198473282,"the experiments?
655"
EXPERIMENTS COMPUTE RESOURCES,0.8635496183206107,"Answer: [Yes]
656"
EXPERIMENTS COMPUTE RESOURCES,0.8645038167938931,"Justification: [TODO] Please refer to the Table 5.
657"
EXPERIMENTS COMPUTE RESOURCES,0.8654580152671756,"Guidelines:
658"
EXPERIMENTS COMPUTE RESOURCES,0.8664122137404581,"• The answer NA means that the paper does not include experiments.
659"
EXPERIMENTS COMPUTE RESOURCES,0.8673664122137404,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
660"
EXPERIMENTS COMPUTE RESOURCES,0.8683206106870229,"or cloud provider, including relevant memory and storage.
661"
EXPERIMENTS COMPUTE RESOURCES,0.8692748091603053,"• The paper should provide the amount of compute required for each of the individual
662"
EXPERIMENTS COMPUTE RESOURCES,0.8702290076335878,"experimental runs as well as estimate the total compute.
663"
EXPERIMENTS COMPUTE RESOURCES,0.8711832061068703,"• The paper should disclose whether the full research project required more compute
664"
EXPERIMENTS COMPUTE RESOURCES,0.8721374045801527,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
665"
EXPERIMENTS COMPUTE RESOURCES,0.8730916030534351,"didn’t make it into the paper).
666"
CODE OF ETHICS,0.8740458015267175,"9. Code Of Ethics
667"
CODE OF ETHICS,0.875,"Question: Does the research conducted in the paper conform, in every respect, with the
668"
CODE OF ETHICS,0.8759541984732825,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
669"
CODE OF ETHICS,0.8769083969465649,"Answer: [Yes]
670"
CODE OF ETHICS,0.8778625954198473,"Justification: [TODO] We have read the NeurIPS Code of Ethics and conform to it.
671"
CODE OF ETHICS,0.8788167938931297,"Guidelines:
672"
CODE OF ETHICS,0.8797709923664122,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
673"
CODE OF ETHICS,0.8807251908396947,"• If the authors answer No, they should explain the special circumstances that require a
674"
CODE OF ETHICS,0.8816793893129771,"deviation from the Code of Ethics.
675"
CODE OF ETHICS,0.8826335877862596,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
676"
CODE OF ETHICS,0.8835877862595419,"eration due to laws or regulations in their jurisdiction).
677"
BROADER IMPACTS,0.8845419847328244,"10. Broader Impacts
678"
BROADER IMPACTS,0.8854961832061069,"Question: Does the paper discuss both potential positive societal impacts and negative
679"
BROADER IMPACTS,0.8864503816793893,"societal impacts of the work performed?
680"
BROADER IMPACTS,0.8874045801526718,"Answer: [NA]
681"
BROADER IMPACTS,0.8883587786259542,"Justification: [TODO] Due to space limitations, this social impact aspect is not discussed
682"
BROADER IMPACTS,0.8893129770992366,"in the main paper. This paper doesn’t involve negative societal impacts including potential
683"
BROADER IMPACTS,0.8902671755725191,"malicious or unintended uses. Our proposed methods aim to achieve an efficient and
684"
BROADER IMPACTS,0.8912213740458015,"effective model compression technique to flexible adaptive different storage and computation
685"
BROADER IMPACTS,0.892175572519084,"requirements, which are beneficial to social advancement.
686"
BROADER IMPACTS,0.8931297709923665,"Guidelines:
687"
BROADER IMPACTS,0.8940839694656488,"• The answer NA means that there is no societal impact of the work performed.
688"
BROADER IMPACTS,0.8950381679389313,"• If the authors answer NA or No, they should explain why their work has no societal
689"
BROADER IMPACTS,0.8959923664122137,"impact or why the paper does not address societal impact.
690"
BROADER IMPACTS,0.8969465648854962,"• Examples of negative societal impacts include potential malicious or unintended uses
691"
BROADER IMPACTS,0.8979007633587787,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
692"
BROADER IMPACTS,0.898854961832061,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
693"
BROADER IMPACTS,0.8998091603053435,"groups), privacy considerations, and security considerations.
694"
BROADER IMPACTS,0.9007633587786259,"• The conference expects that many papers will be foundational research and not tied
695"
BROADER IMPACTS,0.9017175572519084,"to particular applications, let alone deployments. However, if there is a direct path to
696"
BROADER IMPACTS,0.9026717557251909,"any negative applications, the authors should point it out. For example, it is legitimate
697"
BROADER IMPACTS,0.9036259541984732,"to point out that an improvement in the quality of generative models could be used to
698"
BROADER IMPACTS,0.9045801526717557,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
699"
BROADER IMPACTS,0.9055343511450382,"that a generic algorithm for optimizing neural networks could enable people to train
700"
BROADER IMPACTS,0.9064885496183206,"models that generate Deepfakes faster.
701"
BROADER IMPACTS,0.9074427480916031,"• The authors should consider possible harms that could arise when the technology is
702"
BROADER IMPACTS,0.9083969465648855,"being used as intended and functioning correctly, harms that could arise when the
703"
BROADER IMPACTS,0.9093511450381679,"technology is being used as intended but gives incorrect results, and harms following
704"
BROADER IMPACTS,0.9103053435114504,"from (intentional or unintentional) misuse of the technology.
705"
BROADER IMPACTS,0.9112595419847328,"• If there are negative societal impacts, the authors could also discuss possible mitigation
706"
BROADER IMPACTS,0.9122137404580153,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
707"
BROADER IMPACTS,0.9131679389312977,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
708"
BROADER IMPACTS,0.9141221374045801,"feedback over time, improving the efficiency and accessibility of ML).
709"
SAFEGUARDS,0.9150763358778626,"11. Safeguards
710"
SAFEGUARDS,0.916030534351145,"Question: Does the paper describe safeguards that have been put in place for responsible
711"
SAFEGUARDS,0.9169847328244275,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
712"
SAFEGUARDS,0.9179389312977099,"image generators, or scraped datasets)?
713"
SAFEGUARDS,0.9188931297709924,"Answer: [NA]
714"
SAFEGUARDS,0.9198473282442748,"Justification: [TODO] This paper doesn’t have any high risk for misuse.
715"
SAFEGUARDS,0.9208015267175572,"Guidelines:
716"
SAFEGUARDS,0.9217557251908397,"• The answer NA means that the paper poses no such risks.
717"
SAFEGUARDS,0.9227099236641222,"• Released models that have a high risk for misuse or dual-use should be released with
718"
SAFEGUARDS,0.9236641221374046,"necessary safeguards to allow for controlled use of the model, for example by requiring
719"
SAFEGUARDS,0.924618320610687,"that users adhere to usage guidelines or restrictions to access the model or implementing
720"
SAFEGUARDS,0.9255725190839694,"safety filters.
721"
SAFEGUARDS,0.9265267175572519,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
722"
SAFEGUARDS,0.9274809160305344,"should describe how they avoided releasing unsafe images.
723"
SAFEGUARDS,0.9284351145038168,"• We recognize that providing effective safeguards is challenging, and many papers do
724"
SAFEGUARDS,0.9293893129770993,"not require this, but we encourage authors to take this into account and make a best
725"
SAFEGUARDS,0.9303435114503816,"faith effort.
726"
LICENSES FOR EXISTING ASSETS,0.9312977099236641,"12. Licenses for existing assets
727"
LICENSES FOR EXISTING ASSETS,0.9322519083969466,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
728"
LICENSES FOR EXISTING ASSETS,0.933206106870229,"the paper, properly credited and are the license and terms of use explicitly mentioned and
729"
LICENSES FOR EXISTING ASSETS,0.9341603053435115,"properly respected?
730"
LICENSES FOR EXISTING ASSETS,0.9351145038167938,"Answer: [Yes]
731"
LICENSES FOR EXISTING ASSETS,0.9360687022900763,"Justification: [TODO] We conform to the CC-BY 4.0 license.
732"
LICENSES FOR EXISTING ASSETS,0.9370229007633588,"Guidelines:
733"
LICENSES FOR EXISTING ASSETS,0.9379770992366412,"• The answer NA means that the paper does not use existing assets.
734"
LICENSES FOR EXISTING ASSETS,0.9389312977099237,"• The authors should cite the original paper that produced the code package or dataset.
735"
LICENSES FOR EXISTING ASSETS,0.9398854961832062,"• The authors should state which version of the asset is used and, if possible, include a
736"
LICENSES FOR EXISTING ASSETS,0.9408396946564885,"URL.
737"
LICENSES FOR EXISTING ASSETS,0.941793893129771,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
738"
LICENSES FOR EXISTING ASSETS,0.9427480916030534,"• For scraped data from a particular source (e.g., website), the copyright and terms of
739"
LICENSES FOR EXISTING ASSETS,0.9437022900763359,"service of that source should be provided.
740"
LICENSES FOR EXISTING ASSETS,0.9446564885496184,"• If assets are released, the license, copyright information, and terms of use in the
741"
LICENSES FOR EXISTING ASSETS,0.9456106870229007,"package should be provided. For popular datasets, paperswithcode.com/datasets
742"
LICENSES FOR EXISTING ASSETS,0.9465648854961832,"has curated licenses for some datasets. Their licensing guide can help determine the
743"
LICENSES FOR EXISTING ASSETS,0.9475190839694656,"license of a dataset.
744"
LICENSES FOR EXISTING ASSETS,0.9484732824427481,"• For existing datasets that are re-packaged, both the original license and the license of
745"
LICENSES FOR EXISTING ASSETS,0.9494274809160306,"the derived asset (if it has changed) should be provided.
746"
LICENSES FOR EXISTING ASSETS,0.950381679389313,"• If this information is not available online, the authors are encouraged to reach out to
747"
LICENSES FOR EXISTING ASSETS,0.9513358778625954,"the asset’s creators.
748"
NEW ASSETS,0.9522900763358778,"13. New Assets
749"
NEW ASSETS,0.9532442748091603,"Question: Are new assets introduced in the paper well documented and is the documentation
750"
NEW ASSETS,0.9541984732824428,"provided alongside the assets?
751"
NEW ASSETS,0.9551526717557252,"Answer: [NA]
752"
NEW ASSETS,0.9561068702290076,"Justification: [TODO] This paper does not release new assets
753"
NEW ASSETS,0.9570610687022901,"Guidelines:
754"
NEW ASSETS,0.9580152671755725,"• The answer NA means that the paper does not release new assets.
755"
NEW ASSETS,0.958969465648855,"• Researchers should communicate the details of the dataset/code/model as part of their
756"
NEW ASSETS,0.9599236641221374,"submissions via structured templates. This includes details about training, license,
757"
NEW ASSETS,0.9608778625954199,"limitations, etc.
758"
NEW ASSETS,0.9618320610687023,"• The paper should discuss whether and how consent was obtained from people whose
759"
NEW ASSETS,0.9627862595419847,"asset is used.
760"
NEW ASSETS,0.9637404580152672,"• At submission time, remember to anonymize your assets (if applicable). You can either
761"
NEW ASSETS,0.9646946564885496,"create an anonymized URL or include an anonymized zip file.
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9656488549618321,"14. Crowdsourcing and Research with Human Subjects
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9666030534351145,"Question: For crowdsourcing experiments and research with human subjects, does the paper
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9675572519083969,"include the full text of instructions given to participants and screenshots, if applicable, as
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9685114503816794,"well as details about compensation (if any)?
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9694656488549618,"Answer: [NA]
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9704198473282443,"Justification: [TODO] This paper does not involve crowdsourcing nor research with human
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9713740458015268,"subjects.
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9723282442748091,"Guidelines:
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9732824427480916,"• The answer NA means that the paper does not involve crowdsourcing nor research with
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9742366412213741,"human subjects.
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9751908396946565,"• Including this information in the supplemental material is fine, but if the main contribu-
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.976145038167939,"tion of the paper involves human subjects, then as much detail as possible should be
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9770992366412213,"included in the main paper.
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9780534351145038,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9790076335877863,"or other labor should be paid at least the minimum wage in the country of the data
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9799618320610687,"collector.
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9809160305343512,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9818702290076335,"Subjects
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.982824427480916,"Question: Does the paper describe potential risks incurred by study participants, whether
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9837786259541985,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9847328244274809,"approvals (or an equivalent approval/review based on the requirements of your country or
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856870229007634,"institution) were obtained?
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9866412213740458,"Answer: [NA]
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9875954198473282,"Justification: [TODO] This paper does not involve crowdsourcing nor research with human
786"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9885496183206107,"subjects.
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9895038167938931,"Guidelines:
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904580152671756,"• The answer NA means that the paper does not involve crowdsourcing nor research with
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9914122137404581,"human subjects.
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923664122137404,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9933206106870229,"may be required for any human subjects research. If you obtained IRB approval, you
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9942748091603053,"should clearly state this in the paper.
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9952290076335878,"• We recognize that the procedures for this may vary significantly between institutions
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9961832061068703,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9971374045801527,"guidelines for their institution.
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980916030534351,"• For initial submissions, do not include any information that would break anonymity (if
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990458015267175,"applicable), such as the institution conducting the review.
798"
