Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.000846740050804403,"OfÔ¨Çine reinforcement learning, which seeks to utilize ofÔ¨Çine/historical data to
optimize sequential decision-making strategies, has gained surging prominence
in recent studies. Due to the advantage that appropriate function approximators
can help mitigate the sample complexity burden in modern reinforcement learning
problems, existing endeavors usually enforce powerful function representation
models (e.g. neural networks) to learn the optimal policies. However, a precise
understanding of the statistical limits with function representations, remains elusive,
even when such a representation is linear.
Towards this goal, we study the statistical limits of ofÔ¨Çine reinforcement learning
with linear model representations. To derive the tight ofÔ¨Çine learning bound, we
design the variance-aware pessimistic value iteration (VAPVI), which adopts the
conditional variance information of the value function for time-inhomogeneous
episodic linear Markov decision processes (MDPs). VAPVI leverages estimated
variances of the value functions to reweight the Bellman residuals in the least-
square pessimistic value iteration and provides improved ofÔ¨Çine learning bounds
over the best-known existing results (whereas the Bellman residuals are equally
weighted by design). More importantly, our learning bounds are expressed in terms
of system quantities, which provide natural instance-dependent characterizations
that previous results are short of. We hope our results draw a clearer picture of
what ofÔ¨Çine learning should look like when linear representations are provided."
INTRODUCTION,0.001693480101608806,"1
INTRODUCTION"
INTRODUCTION,0.002540220152413209,"OfÔ¨Çine reinforcement learning (ofÔ¨Çine RL or batch RL Lange et al. (2012); Levine et al. (2020))
is the framework for learning a reward-maximizing policy in an unknown environment (Markov
Decision Process or MDP)1 using the logged data coming from some behavior policy ¬µ. Function
approximations, on the other hand, are well-known for generalization in the standard supervised
learning. OfÔ¨Çine RL with function representation/approximation, as a result, provides generalization
across large state-action spaces for the challenging sequential decision-making problems when no
iteration is allowed (as opposed to online learning). This paradigm is crucial to the success of modern
RL problems as many deep RL algorithms Ô¨Ånd their prototypes in the literature of ofÔ¨Çine RL. For
example, Xie and Jiang (2020) provides a view that Fitted Q-Iteration (Gordon, 1999; Ernst et al.,
2005) can be considered as the theoretical prototype of the deep Q-networks algorithm (DQN) Mnih
et al. (2015) with neural networks being the function representors. On the empirical side, there are
a huge body of deep RL-based algorithms (Mnih et al., 2015; Silver et al., 2017; Fujimoto et al.,
2019; Kumar et al., 2019; Wu et al., 2019; Kidambi et al., 2020; Yu et al., 2020; Kumar et al., 2020;
Janner et al., 2021; Chen et al., 2021a; Kostrikov et al., 2022) that utilize function approximations to
achieve respective successes in the ofÔ¨Çine regime. However, it is also realized that practical function
approximation schemes can be quite sample inefÔ¨Åcient (e.g. millions of samples are needed for deep
Q-network to solve certain Atari games Mnih et al. (2015))."
INTRODUCTION,0.003386960203217612,"1The environment could have other forms as well, e.g. partially-observed MDP (POMDP) or non-markovian
decision process (NMDP)."
INTRODUCTION,0.004233700254022015,Published as a conference paper at ICLR 2022
INTRODUCTION,0.005080440304826418,"To understand this phenomenon, there are numerous studies consider how to achieve sample efÔ¨Åciency
with function approximation from the theoretical side, as researchers Ô¨Ånd sample efÔ¨Åcient algorithms
are possible with particular model representations, in either online RL (e.g. Yang and Wang (2019;
2020); Modi et al. (2020); Jin et al. (2020); Ayoub et al. (2020); Jiang et al. (2017); Du et al. (2019);
Sun et al. (2019); Zanette et al. (2020); Zhou et al. (2021a); Jin et al. (2021a); Du et al. (2021)) or
ofÔ¨Çine RL (e.g. Munos (2003); Chen and Jiang (2019); Xie and Jiang (2020); Jin et al. (2021b); Xie
et al. (2021a); Min et al. (2021); Duan et al. (2021); Nguyen-Tang et al. (2021); Zanette et al. (2021))."
INTRODUCTION,0.0059271803556308214,"Among them, the linear MDP model (Yang and Wang, 2020; Jin et al., 2020), where the transition is
represented as a linear combinations of the given d-dimensional feature, is (arguably) the most studied
setting in function approximation and there are plenty of extensions based upon it (e.g. generalized
linear model (Wang et al., 2021b), reward-free RL (Wang et al., 2020), gap-dependent analysis (He
et al., 2021) or generative adversarial learning (Liu et al., 2021)). Given its prosperity, however, there
are still unknowns for understanding function representations in RL, especially in the ofÔ¨Çine case."
INTRODUCTION,0.006773920406435224,"‚Ä¢ While there are surging researches in showing provable sample efÔ¨Åciency (polynomial
sample complexity is possible) under a variety of function approximation schemes, how
to improve the sample efÔ¨Åciency for a given class of function representations remains
understudied. For instance, given a neural network approximation class, an algorithm that
learns the optimal policy with complexity O(H10) is far worse than the one that can learn
in O(H3) sample complexity, despite that both algorithms are considered sample efÔ¨Åcient.
Therefore, how to achieve the optimal/tight sample complexity when function approximation
is provided is a valuable question to consider. On the other hand, it is known that tight
sample complexity, due to the limit of the existing statistical analysis tools, can be very
tough to establish when function representation has a very complicated form. However, does
this mean tight analysis is not hopeful even when the representation is linear?
‚Ä¢ Second, in the existing analysis of ofÔ¨Çine RL (with function approximation or simply the
tabular MDPs), the learning bounds depend either explicitly on the data-coverage quantities
(e.g. uniform concentrability coefÔ¨Åcients Chen and Jiang (2019); Xie and Jiang (2020),
uniform visitation measure Yin et al. (2021); Yin and Wang (2021a) and single concen-
trability Rashidinejad et al. (2021); Xie et al. (2021b)) or the horizon length H (Jin et al.,
2021b; Uehara and Sun, 2021). While those results are valuable as they do not depend on
the structure of the particular problem (therefore, remain valid even for pathological MDPs),
in practice, the empirical performances of ofÔ¨Çine reinforcement learning are often far better
than those non-adaptive bounds would indicate. Can the learning bounds reÔ¨Çect the nature
of individual MDP instances when the MDP model has a certain function representation?"
INTRODUCTION,0.007620660457239628,"In this work, we think about ofÔ¨Çine RL from the above two aspects. In particular, we consider the
fundamental linear model representations and ask the following question of interest:"
INTRODUCTION,0.00846740050804403,Can we achieve the statistical limits for ofÔ¨Çine RL when models have linear representations?
RELATED WORKS,0.009314140558848433,"1.1
RELATED WORKS"
RELATED WORKS,0.010160880609652836,"OfÔ¨Çine RL with general function representations. The Ô¨Ånite sample analysis of ofÔ¨Çine RL with
function approximation is initially conducted by Fitted Q-Iteration (FQI) type algorithms and can
be dated back to (Munos, 2003; Szepesv¬¥ari and Munos, 2005; Antos et al., 2008a;b). Later, Chen
and Jiang (2019); Le et al. (2019); Xie and Jiang (2020) follow this line of research and derive the
improved learning results. However, owing to the aim for tackling general function approximation,
those learning bounds are expressed in terms of the stringent concentrability coefÔ¨Åcients (therefore,
are less adaptive to individual instances) and are usually only information-theoretical, due to the
computational intractability of the optimization procedure over the general function classes. Other
works impose weaker assumptions (e.g. partial coverage (Liu et al., 2020; Kidambi et al., 2020;
Uehara and Sun, 2021)), and their Ô¨Ånite sample analysis are generally suboptimal in terms of H or
the effective horizon (1 ‚àíŒ≥)‚àí1."
RELATED WORKS,0.01100762066045724,"OfÔ¨Çine RL with tabular models. For tabular MDPs, tight learning bounds can be achieved under
several data-coverage assumptions. For the class of problems with uniform data-visitation measure
dm, the near-optimal sample complexity bound has the rate O(H3/dmœµ2) for time-inhomogeneous
MDPs (Yin et al., 2021) and O(H2/dmœµ) for time-homogeneous MDPs (Yin and Wang, 2021a; Ren"
RELATED WORKS,0.011854360711261643,Published as a conference paper at ICLR 2022
RELATED WORKS,0.012701100762066046,"et al., 2021). Under the single concentrability assumption, the tight rate O(H3SC‚ãÜ/œµ2) is obtained
by Xie et al. (2021b). In particular, the recent study Yin and Wang (2021b) introduces the intrinsic
ofÔ¨Çine learning bound that is not only instance-dependent but also subsumes previous optimal results."
RELATED WORKS,0.013547840812870448,"OfÔ¨Çine RL with linear model representations. Recently, there is more focus on studying the
provable efÔ¨Åcient ofÔ¨Çine RL under the linear model representations. Jin et al. (2021b) Ô¨Årst shows
ofÔ¨Çine RL with linear MDP is provably efÔ¨Åcient by the pessimistic value iteration. Their analysis
deviates from their lower bound by a factor of d ¬∑ H (check their Theorem 4.4 and 4.6). Later, Xie
et al. (2021a) considers function approximation under the Bellman-consistent assumptions, and,
when realized to linear MDP setting, improves the sample complexity guarantee of Jin et al. (2021b)
by an order O(d) (Theorem 3.2).2 However, their improvement only holds for Ô¨Ånite action space
(due to the dependence log |A|) and by the direct reduction (from Theorem 3.1) their result does not
imply a computationally tractable algorithm with the same guarantee. Concurrently, Zanette et al.
(2021) considers the Linear Bellman Complete model and designs the actor-critic style algorithm
that achieves tight result under the assumption that the value function is bounded by 1. While their
algorithm is efÔ¨Åcient (which is based on solving a sequence of second-order cone programs), the
resulting learning bound requires the action space to be Ô¨Ånite due to the mirror descent updates in
the Actor procedure (Agarwal et al., 2021). Besides, assuming the value function to be less than 1
simpliÔ¨Åes the challenges in dealing with horizon H since when rescaling their result to [0, H], there
is a H factor blow-up, which makes no horizon improvement comparing to Jin et al. (2021b). As a
result, none of the existing algorithms can achieve the statistical limit for the well-structured linear
MDP model with the general (inÔ¨Ånite or continuous) state-action spaces. On the other hand, Wang
et al. (2021a); Zanette (2021) study the statistical hardness of ofÔ¨Çine RL with linear representations
by proving the exponential lower bounds. Recently, Foster et al. (2021) shows realizability and
concentrability are not sufÔ¨Åcient for ofÔ¨Çine learning when state space is arbitrary large."
RELATED WORKS,0.014394580863674851,"Variance-aware studies. Talebi and Maillard (2018) Ô¨Årst incorporates the variance structure in online
tabular MDPs and Zanette and Brunskill (2019) tightens the result. For linear mixture MDPs, Zhou
et al. (2021a) Ô¨Årst uses variance structure to achieve near-optimal result and the Weighted OFUL
incorporates the variance structure explicitly in the regret bound. Recently, Variance-awareness is
also considered in Zhang et al. (2021) for horizon-free setting and for OPE problem (Min et al., 2021).
In particular, We point out that Min et al. (2021) is the Ô¨Årst work that uses variance reweighting
for policy evaluation in ofÔ¨Çine RL, which inspires our study for policy optimization problem. The
guarantee of Min et al. (2021) strictly improves over Duan et al. (2020) for OPE problem."
OUR CONTRIBUTION,0.015241320914479255,"1.2
OUR CONTRIBUTION"
OUR CONTRIBUTION,0.016088060965283656,"In this work, we study ofÔ¨Çine RL for time-inhomogeneous episodic linear Markov decision processes.
Linear MDPs serve as one critical step towards understanding function approximation in RL since: 1.
unlike general function representation, linear MDP representation has the well-structured form by
the given feature representors, which makes delicate statistical analysis hopeful; 2. unlike tabular
representation, which only works for Ô¨Ånite models, linear MDP provides generalization as it adapts to
inÔ¨Ånite or continuous state-action spaces. Especially, we design the variance-aware pessimistic value
iteration (VAPVI, Algorithm 1) which incorporates the conditional variance information of the value
function and, by the variance structure, Theorem 3.2 is able to improve over the aforementioned state-
of-the-art guarantees. In addition, we further improve the state-action guarantee by designing an even
tighter bonus (4). VAPVI-Improved (Theorem 3.3) is near-minimax optimal as indicated by our lower
bound (Theorem 3.5). Importantly, the resulting learning bounds from VAPVI/VAPVI-Improved
are able to characterize the adaptive nature of individual instances and yield different convergence
rates for different problems. Algorithmically, our algorithm builds upon the nice Min et al. (2021)
with pessimism as we use the estimated variances to reweight the Bellman residual learning objective
so that the (training) samples with high uncertainty get less attention (Section 3). This is the key to
obtaining instance-adaptive guarantees."
PRELIMINARIES,0.01693480101608806,"2
PRELIMINARIES"
PRELIMINARIES,0.017781541066892465,2This comparison is based on translating their inÔ¨Ånite horizon discounted setting to the Ô¨Ånite-horizon case.
PRELIMINARIES,0.018628281117696866,Published as a conference paper at ICLR 2022
PROBLEM SETTINGS,0.01947502116850127,"2.1
PROBLEM SETTINGS"
PROBLEM SETTINGS,0.02032176121930567,"Episodic time-inhomogeneous linear Markov decision process. A Ô¨Ånite-horizon Markov Decision
Process (MDP) is denoted as M = (S, A, P, r, H, d1) (Sutton and Barto, 2018), where S is the
arbitrary state space and A is the arbitrary action space which can be inÔ¨Ånite or even continuous.
A time-inhomogeneous transition kernel Ph : S √ó A 7‚Üí‚àÜS (‚àÜS represents a probability simplex)
maps each state action(sh, ah) to a probability distribution Ph(¬∑|sh, ah) and Ph can be different
across time. In addition, r : S √ó A 7‚ÜíR is the mean reward function satisfying 0 ‚â§r ‚â§1. d1
is the initial state distribution. H is the horizon. A policy œÄ = (œÄ1, . . . , œÄH) assigns each state
sh ‚ààS a probability distribution over actions according to the map sh 7‚ÜíœÄh(¬∑|sh) ‚àÄh ‚àà[H] and
induces a random trajectory s1, a1, r1, . . . , sH, aH, rH, sH+1 with s1 ‚àºd1, ah ‚àºœÄ(¬∑|sh), sh+1 ‚àº
Ph(¬∑|sh, ah), ‚àÄh ‚àà[H]. In particular, we adopts the linear MDP protocol from Jin et al. (2020;
2021b), meaning that the transition kernel and the mean reward function admit linear structures in the
feature map.
DeÔ¨Ånition 2.1 (Linear MDPs). 3 An episodic MDP (S, A, H, P, r) is called a linear MDP with
a known (unsigned) feature map œÜ : S √ó A ‚ÜíRd if there exist d unknown (unsigned) measures
ŒΩh = (ŒΩ(1)
h , . . . , ŒΩ(d)
h ) over S and an unknown vector Œ∏h ‚ààRd such that"
PROBLEM SETTINGS,0.021168501270110076,"Ph (s‚Ä≤ | s, a) = ‚ü®œÜ(s, a), ŒΩh (s‚Ä≤)‚ü©,
rh (s, a) = ‚ü®œÜ(x, a), Œ∏h‚ü©,
‚àÄs‚Ä≤, s ‚ààS, a ‚ààA, h ‚àà[H]."
PROBLEM SETTINGS,0.02201524132091448,"where ‚à•ŒΩh(S)‚à•2 ‚â§
‚àö"
PROBLEM SETTINGS,0.02286198137171888,"d and max(‚à•œÜ(s, a)‚à•2 , ‚à•Œ∏h‚à•2) ‚â§1 for all h ‚àà[H] and ‚àÄs, a ‚ààS √ó A.
‚à•¬µh(S)‚à•=
R"
PROBLEM SETTINGS,0.023708721422523286,S ‚à•¬µh(s)‚à•ds.
PROBLEM SETTINGS,0.024555461473327687,"V -values and Q-values. For any policy œÄ, the V -value functions V œÄ
h (¬∑) ‚ààRS and Q-value functions
QœÄ
h(¬∑, ¬∑) ‚ààRS√óA are deÔ¨Åned as: V œÄ
h (s) = EœÄ[PH
t=h rt|sh = s],
QœÄ
h(s, a) = EœÄ[PH
t=h rt|sh, ah ="
PROBLEM SETTINGS,0.02540220152413209,"s, a], ‚àÄs, a, h ‚ààS, A, [H]. The performance measure is deÔ¨Åned as vœÄ := Ed1 [V œÄ
1 ] = EœÄ,d1
hPH
t=1 rt
i
.
The Bellman (optimality) equations follow ‚àÄh ‚àà[H]: QœÄ
h = rh + PhV œÄ
h+1, V œÄ
h = Ea‚àºœÄh[QœÄ
h],
Q‚ãÜ
h =
rh + PhV ‚ãÜ
h+1, V ‚ãÜ
h = maxa Q‚ãÜ
h(¬∑, a) (where Qh, Vh, Ph are vectors). By DeÔ¨Ånition 2.1, the Q-values
also admit linear structures, i.e. QœÄ
h = ‚ü®œÜ, wœÄ
h‚ü©for some wœÄ
h ‚ààRd (Lemma H.9). Lastly, for a policy
œÄ, we denote the induced occupancy measure over the state-action space at any time h ‚àà[H] to be:
for any E ‚äÜS √ó A, dœÄ
h(E) := E[(sh, ah) ‚ààE|s1 ‚àºd1, ai ‚àºœÄ(¬∑|si), si ‚àºPi‚àí1(¬∑|si‚àí1, ai‚àí1), 1 ‚â§
i ‚â§h] and EœÄ,h[f(s, a)] :=
R"
PROBLEM SETTINGS,0.026248941574936496,"S√óA f(s, a)dœÄ
h(s, a)dsda. Here for notation simplicity we abuse dœÄ
h(¬∑)
to denote either probability measure or density function."
PROBLEM SETTINGS,0.027095681625740897,"OfÔ¨Çine learning setting. OfÔ¨Çine RL requires the agent to learn the policy œÄ that maximizes vœÄ,
provided with the historical data D = {(sœÑ
h, aœÑ
h, rœÑ
h, sœÑ
h+1)}h‚àà[H]
œÑ‚àà[K] rolled out from some behavior policy ¬µ.
The ofÔ¨Çine nature requires we cannot change ¬µ and in particular we do not know the data generating
distribution of ¬µ. To sum up, the agent seeks to Ô¨Ånd a policy œÄalg such that v‚ãÜ‚àívœÄalg ‚â§œµ for the
given batch data D and a given targeted accuracy œµ > 0."
ASSUMPTIONS,0.0279424216765453,"2.2
ASSUMPTIONS"
ASSUMPTIONS,0.028789161727349702,"It is known that learning a near-optimal policy from the ofÔ¨Çine data D cannot be sample efÔ¨Åcient
without certain data-coverage assumptions (Wang et al., 2021a; Yin and Wang, 2021b). To begin
with, we deÔ¨Åne the population covariance matrix under the behavior policy ¬µ for all h ‚àà[H]:"
ASSUMPTIONS,0.029635901778154106,"Œ£p
h := E¬µ,h

œÜ(s, a)œÜ(s, a)‚ä§
,
(1)"
ASSUMPTIONS,0.03048264182895851,"since Œ£p
h measure the coverage of state-action space for data D, we make the following assumption.
Assumption 2.2 (Feature Coverage). The data distributions ¬µ satisfy the minimum eigenvalue
condition: ‚àÄh ‚àà[H], Œ∫h := Œªmin(Œ£p
h) > 0 and denote Œ∫ = minh Œ∫h. Note Œ∫ is a system-dependent
(non-universal) quantity as it is upper bounded by 1/d (Assumption 2 in Wang et al. (2021a))."
ASSUMPTIONS,0.03132938187976291,"We make this assumption for the following reasons. First of all, our ofÔ¨Çine learning guarantee
(Theorem 3.2) provides simultaneously comparison to all the policies, which is stronger than only
competing with the optimal policy (whereas relaxed assumption sufÔ¨Åces, e.g. supx‚ààRd xŒ£œÄ‚ãÜx‚ä§"
ASSUMPTIONS,0.03217612193056731,xŒ£¬µx‚ä§< ‚àû
ASSUMPTIONS,0.03302286198137172,"3This deÔ¨Ånition is a standard extension over the tabular MDPs by referencing the similar notions from the
bandit literature, i.e. from Multi-armed Bandit to Linear Bandit (Lattimore and Szepesv¬¥ari, 2020)."
ASSUMPTIONS,0.03386960203217612,Published as a conference paper at ICLR 2022
ASSUMPTIONS,0.03471634208298052,"(Uehara and Sun, 2021)). As a consequence, the behavior distribution ¬µ must be able to explore each
feature dimension for the result to be valid. Second, even if Assumption 2.2 does not hold, we can
always restrict our algorithmic design to the effective subspan of Œ£p
h, which causes the alternative
notion of Œ∫ := minh‚àà[H]{Œ∫h : s.t. Œ∫h = smallest positive eigenvalue at time h} (see Appendix G.1
for detailed discussions). In this scenario, learning the optimal policy cannot be guaranteed as a
constant suboptimality gap needs to be suffered due to the lack of coverage and this is formed as
assumption-free RL in Yin and Wang (2021b). Lastly, previous works analyzing the linear MDPs
impose very similar assumptions, e.g. Xie et al. (2021a) Theorem 3.2 where Œ£‚àí1
D exists and Min et al.
(2021) for the OPE problem."
ASSUMPTIONS,0.03556308213378493,"Next, for any function Vh+1(¬∑) ‚àà[0, H ‚àíh], we deÔ¨Åne the conditional variance œÉVh+1 : S √óA ‚ÜíR+
as œÉVh+1(s, a)2 := max{1, VarPh(Vh+1)(s, a)}.4
Based on this deÔ¨Ånition, we can deÔ¨Åne the
variance-involved population covariance matrices as:Œõp
h := E¬µ,h

œÉVh+1(s, a)‚àí2œÜ(s, a)œÜ(s, a)‚ä§
. In
particular, when Vh = V ‚ãÜ
h , we use the notation Œõ‚ãÜp
h instead."
ALGORITHM,0.03640982218458933,"3
ALGORITHM"
ALGORITHM,0.03725656223539373,"Least square regression is usually considered as one of the ‚Äúdefault‚Äù tools for handling problems
with linear structures (e.g. LinUCB algorithm for linear Bandits) and Ô¨Ånds its popularity in RL as
well since Least-Square Value Iteration (LSVI, Jin et al. (2020)) is shown to be provably efÔ¨Åcient for
linear MDPs, due to that Vh+1(s‚Ä≤) is an unbiased estimator of [PhVh+1](s, a). Concretely, it solves
the ridge regression problems at each time steps (with Œª > 0 being the regularization parameter):"
ALGORITHM,0.03810330228619814,"bwh := argmin
w‚ààRd
Œª‚à•w‚à•2
2 + K
X k=1"
ALGORITHM,0.03895004233700254,"h
‚ü®œÜ(sk
h, ak
h), w‚ü©‚àírk
h ‚àíVh+1(s‚Ä≤k
h+1)
i2
(2)"
ALGORITHM,0.03979678238780694,"and has the closed-form solution bwh = Œ£‚àí1
h
PK
k=1 œÜ(sk
h, ak
h)[rk,h + Vh+1(s‚Ä≤k
h )] with Œ£‚àí1
h
=
PK
k=1 œÜ(sk
h, ak
h)œÜ(sk
h, ak
h)‚ä§+ ŒªI. In ofÔ¨Çine RL, this has also been leveraged in pessimistic value
iteration (Jin et al., 2021b) and Ô¨Åtted Q-evaluation (Duan et al., 2020). Nevertheless, LSVI could
only yield suboptimal guarantees, as illustrated by the following example."
ALGORITHM,0.04064352243861134,"Example 3.1. Instantiate PEVI (Theorem 4.4 in Jin et al. (2021b)) with œÜ(s, a) = 1s,a (i.e. tabular
MDPs)5, by direct calculation the learning bound has the form O(dH¬∑P"
ALGORITHM,0.04149026248941575,"h,s,a dœÄ‚ãÜ
h (s, a)
q"
ALGORITHM,0.04233700254022015,"1
K¬∑d¬µ
h(s,a)) and"
ALGORITHM,0.04318374259102455,the optimal result (Yin and Wang (2021b) Theorem 4.1) gives O(P
ALGORITHM,0.04403048264182896,"h,s,a dœÄ‚ãÜ
h (s, a)
r"
ALGORITHM,0.04487722269263336,"VarPs,a (r+V ‚ãÜ
h+1)"
ALGORITHM,0.04572396274343776,"K¬∑d¬µ
h(s,a)
)."
ALGORITHM,0.04657070279424217,The former has the horizon dependence H2 and the latter is H3/2 by law of total variance.
ALGORITHM,0.04741744284504657,"Motivation. By comparing the above two expressions, it can be seen that PEVI cannot get rid of
the explicit H factor due to missing the variance information (w.r.t V ‚ãÜ). If we go deeper, one could
Ô¨Ånd that it might not be all that ideal to put equal weights on all the training samples in the least
square objective (2), since, unlike linear regression where the randomness coming from one source
distribution, we are regressing over a sequence of distributions in RL (i.e. each sh, ah corresponds
to a different distribution P(¬∑|sh, ah) and there are possibly inÔ¨Ånite many of them). Therefore,
conceptually, the sample piece (sh, ah, sh+1) that has higher variance distribution P(¬∑|sh, ah) tends
to be less ‚Äúreliable‚Äù than the one (s‚Ä≤
h, a‚Ä≤
h, s‚Ä≤
h+1) with lower variance (hence should not have equal
weight in (2)). This suggests reweighting scheme might help improve the learning guarantee and
reweighting over the variance of the value function stands as a natural choice."
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.04826418289585097,"3.1
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.04911092294665537,"Now we explain our framework that incorporates the variance information. Our design is motivated
by previous Zhou et al. (2021a) (for online learning) and Min et al. (2021) (for policy evaluation).
By the ofÔ¨Çine nature, we can use the independent episodic data D‚Ä≤ = {(¬ØsœÑ
h, ¬ØaœÑ
h, ¬ØrœÑ
h, ¬ØsœÑ‚Ä≤
h )}h‚àà[H]
œÑ‚àà[K] (from
¬µ) to estimate the conditional variance of any V -values Vh+1 via the deÔ¨Ånition [VarhVh+1](s, a) ="
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.04995766299745978,"4The max(1, ¬∑) applied here is for technical reason only. In general, it sufÔ¨Åces to think œÉ2
Vh+1 ‚âàVarhVh+1.
5This provides a valid illustration since tabular MDP is a special case of linear MDPs."
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.05080440304826418,Published as a conference paper at ICLR 2022
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.05165114309906858,"[Ph(Vh+1)2](s, a) ‚àí([PhVh+1](s, a))2. For the second order moment, by DeÔ¨Ånition 2.1, it holds

PhV 2
h+1

(s, a) =
Z"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.05249788314987299,"S
V 2
h+1
 
s‚Ä≤
dPh
 
s‚Ä≤ | s, a

= œÜ(s, a)‚ä§
Z"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.05334462320067739,"S
V 2
h+1
 
s‚Ä≤
dŒΩh
 
s‚Ä≤
."
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.05419136325148179,"Denote Œ≤h :=
R"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.0550381033022862,"S V 2
h+1 (s‚Ä≤) dŒΩh (s‚Ä≤), then PhV 2
h+1 = ‚ü®œÜ, Œ≤h‚ü©and we can estimator it via:"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.0558848433530906,"¬ØŒ≤h = argmin
Œ≤‚ààRd K
X k=1"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.056731583403895,"hD
œÜ(¬Øsk
h, ¬Øak
h), Œ≤
E
‚àíV 2
h+1

¬Øsk
h+1
i2
+ Œª‚à•Œ≤‚à•2
2 = ¬ØŒ£‚àí1
h K
X"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.057578323454699404,"k=1
œÜ(¬Øsk
h, ¬Øak
h)V 2
h+1

¬Øsk
h+1
"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.05842506350550381,"and, similarly, the Ô¨Årst order moment PhVh+1 := ‚ü®œÜ, Œ∏h‚ü©can be estimated via:"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.05927180355630821,"¬ØŒ∏h = argmin
Œ∏‚ààRd K
X k=1"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.060118543607112614,"hD
œÜ(¬Øsk
h, ¬Øak
h), Œ∏
E
‚àíVh+1

¬Øsk
h+1
i2
+ Œª‚à•Œ∏‚à•2
2 = ¬ØŒ£‚àí1
h K
X"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06096528365791702,"k=1
œÜ(¬Øsk
h, ¬Øak
h)Vh+1

¬Øsk
h+1
"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06181202370872142,"The Ô¨Ånal estimator is deÔ¨Åned as bœÉ2
Vh(¬∑, ¬∑) := max{1, d
VarhVh+1(¬∑, ¬∑)} with d
VarhVh+1(¬∑, ¬∑) ="
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06265876375952582,"‚ü®œÜ(¬∑, ¬∑), ¬ØŒ≤h‚ü©[0,(H‚àíh+1)2] ‚àí

‚ü®œÜ(¬∑, ¬∑), ¬ØŒ∏h‚ü©[0,H‚àíh+1]
2.6 In particular, when setting Vh+1 = bVh+1,
it recovers bœÉh in Algorithm 1 line 8. Here ¬ØŒ£h = PK
œÑ=1 œÜ(¬ØsœÑ
h, ¬ØaœÑ
h)œÜ(¬ØsœÑ
h, ¬ØaœÑ
h)‚ä§+ ŒªId."
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06350550381033022,"Variance-weighted LSVI. The idea of LSVI (2) is based on approximate the Bellman updates:
Th(V )(s, a) = rh(s, a) + (PhV )(s, a). With variance estimator bœÉh at hand, we can modify (2) to
solve the variance-weighted LSVI instead (Line 10 of Algorithm 1)"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06435224386113463,"b
wh := argmin
w‚ààRd
Œª‚à•w‚à•2
2 + K
X k=1"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06519898391193904,"h
‚ü®œÜ(sk
h, ak
h), w‚ü©‚àírk
h ‚àíbVh+1(s‚Ä≤k
h+1)
i2"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06604572396274344,"bœÉ2
h(sk
h, ak
h)
= bŒõ‚àí1
h K
X k=1"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06689246401354784,"œÜ

sk
h, ak
h

¬∑
h
rk
h + bVh+1

sk
h+1
i"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06773920406435224,"bœÉ2(sk
h, ak
h)"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06858594411515664,"where bŒõh = PK
k=1 œÜ(sk
h, ak
h)œÜ(sk
h, ak
h)‚ä§/bœÉ2
h(sk
h, ak
h) + ŒªId. The estimated Bellman update bTh (acts
on bVh+1) is deÔ¨Åned as: (bTh bVh+1)(¬∑, ¬∑) = œÜ(¬∑, ¬∑)‚ä§bwh and the pessimism Œìh is assigned to update
bQh ‚âàbTh bVh+1 ‚àíŒìh, i.e. Bellman update + Pessimism (Line 10-12 in Algorithm 1)."
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.06943268416596104,"Tighter Pessimistic Design. To improve the learning guarantee, we create a tighter penalty design
that includes bŒõ‚àí1
h
rather than ¬ØŒ£‚àí1
h
and an extra higher order O( 1"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.07027942421676546,K ) term:
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.07112616426756986,"Œìh ‚ÜêO
‚àö"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.07197290431837426,"d ¬∑ (œÜ(¬∑, ¬∑)‚ä§bŒõ‚àí1
h œÜ(¬∑, ¬∑))1/2
+ 2H3‚àö"
VARIANCE-AWARE PESSIMISTIC VALUE ITERATION,0.07281964436917866,"d
K
Note such a design admits no explicit factor in H in the main term (as opposed to Jin et al. (2021b))
therefore is the key for achieving adaptive/problem-dependent results (as we shall discuss later).
The full algorithm VAPVI is stated in Algorithm 1. In particular, we halve the ofÔ¨Çine data into two
independent parts with D = {(sœÑ
h, aœÑ
h, rœÑ
h, sœÑ‚Ä≤
h )}h‚àà[H]
œÑ‚àà[K] and D‚Ä≤ = {(¬ØsœÑ
h, ¬ØaœÑ
h, ¬ØrœÑ
h, ¬ØsœÑ‚Ä≤
h )}h‚àà[H]
œÑ‚àà[K] for different
purposes (estimating variance and updating Q-values)."
MAIN RESULT,0.07366638441998306,"3.2
MAIN RESULT"
MAIN RESULT,0.07451312447078746,"We denote quantities M1, M2, M3, M4 as in the notation list A. Then VAPVI provides the following
result. The complete proof is provided in Appendix C.
Theorem 3.2. Let K be the number of episodes. If K > max{M1, M2, M3, M4} and
‚àö"
MAIN RESULT,0.07535986452159187,"d > Œæ,"
MAIN RESULT,0.07620660457239628,"where Œæ := supV ‚àà[0,H], s‚Ä≤‚àºPh(s,a), h‚àà[H]"
MAIN RESULT,0.07705334462320068,"rh+V (s‚Ä≤)‚àí(ThV )(s,a)"
MAIN RESULT,0.07790008467400508,"œÉV (s,a)"
MAIN RESULT,0.07874682472480948,". Then for any 0 < Œª < Œ∫, with"
MAIN RESULT,0.07959356477561388,"probability 1 ‚àíŒ¥, for all policy œÄ simultaneously, the output bœÄ of Algorithm 1 satisÔ¨Åes"
MAIN RESULT,0.08044030482641829,"vœÄ ‚àívbœÄ ‚â§eO
 ‚àö d ¬∑ H
X"
MAIN RESULT,0.08128704487722269,"h=1
EœÄ q"
MAIN RESULT,0.0821337849280271,"œÜ(¬∑, ¬∑)‚ä§Œõ‚àí1
h œÜ(¬∑, ¬∑)

+ 2H4‚àö d
K"
MAIN RESULT,0.0829805249788315,"where Œõh = PK
k=1
œÜ(sk
h,ak
h)¬∑œÜ(sk
h,ak
h)‚ä§"
MAIN RESULT,0.0838272650296359,"œÉ2
b
Vh+1(sk
h,ak
h)
+ ŒªId. In particular, we have with probability 1 ‚àíŒ¥,"
MAIN RESULT,0.0846740050804403,"v‚ãÜ‚àívbœÄ ‚â§eO
 ‚àö d ¬∑ H
X"
MAIN RESULT,0.0855207451312447,"h=1
EœÄ‚ãÜ
q"
MAIN RESULT,0.0863674851820491,"œÜ(¬∑, ¬∑)‚ä§Œõ‚ãÜ‚àí1
h
œÜ(¬∑, ¬∑)

+ 2H4‚àö"
MAIN RESULT,0.08721422523285352,"d
K
(3)"
MAIN RESULT,0.08806096528365792,"where Œõ‚ãÜ
h = PK
k=1
œÜ(sk
h,ak
h)¬∑œÜ(sk
h,ak
h)‚ä§"
MAIN RESULT,0.08890770533446232,"œÉ2
V ‚ãÜ
h+1(sk
h,ak
h)
+ ŒªId and eO hides universal constants and the Polylog terms."
MAIN RESULT,0.08975444538526672,6The truncation used here is a standard treatment for making the estimator to be within the valid range.
MAIN RESULT,0.09060118543607112,Published as a conference paper at ICLR 2022
MAIN RESULT,0.09144792548687553,Algorithm 1 Variance-Aware Pessimistic Value Iteration (VAPVI)
MAIN RESULT,0.09229466553767993,"1: Input: Dataset D = {(sœÑ
h, aœÑ
h, rœÑ
h)}K,H
œÑ,h=1 D‚Ä≤ = {(¬ØsœÑ
h, ¬ØaœÑ
h, ¬ØrœÑ
h)}K,H
œÑ,h=1. Universal constant C."
MAIN RESULT,0.09314140558848434,"2: Initialization: Set bVH+1(¬∑) ‚Üê0.
3: for h = H, H ‚àí1, . . . , 1 do
4:
‚ãÑPhase1: Regular Least-square Value Iteration for conditional variances
5:
Set ¬ØŒ£h ‚ÜêPK
œÑ=1 œÜ(¬ØsœÑ
h, ¬ØaœÑ
h)œÜ(¬ØsœÑ
h, ¬ØaœÑ
h)‚ä§+ ŒªI
6:
Set ¬ØŒ≤h ‚Üê¬ØŒ£‚àí1
h
PK
œÑ=1 œÜ(¬ØsœÑ
h, ¬ØaœÑ
h) ¬∑ bVh+1(¬ØsœÑ
h+1)2"
MAIN RESULT,0.09398814563928874,"7:
Set ¬ØŒ∏h ‚Üê¬ØŒ£‚àí1
h
PK
œÑ=1 œÜ(¬ØsœÑ
h, ¬ØaœÑ
h) ¬∑ bVh+1(¬ØsœÑ
h+1)
8:
Set
d
Varh bVh+1

(¬∑, ¬∑) =

œÜ(¬∑, ¬∑), ¬ØŒ≤h

[0,(H‚àíh+1)2] ‚àí

œÜ(¬∑, ¬∑), ¬ØŒ∏h"
MAIN RESULT,0.09483488569009314,"[0,H‚àíh+1]
2"
MAIN RESULT,0.09568162574089754,"9:
Set bœÉh(¬∑, ¬∑)2 ‚Üêmax{1, d
VarPh bVh+1(¬∑, ¬∑)}
10:
‚ãÑPhase2: Weighted Least-square Value Iteration for pessimistic updates
11:
Set bŒõh ‚ÜêPK
œÑ=1 œÜ (sœÑ
h, aœÑ
h) œÜ (sœÑ
h, aœÑ
h)‚ä§/bœÉ2(sœÑ
h, aœÑ
h) + Œª ¬∑ I,"
MAIN RESULT,0.09652836579170194,"12:
Set bwh ‚ÜêbŒõ‚àí1
h
PK
œÑ=1 œÜ (sœÑ
h, aœÑ
h) ¬∑

rœÑ
h + bVh+1 (sœÑ
h+1)

/bœÉ2(sœÑ
h, aœÑ
h)
"
MAIN RESULT,0.09737510584250635,"13:
Set Œìh(¬∑, ¬∑) ‚ÜêC
‚àö"
MAIN RESULT,0.09822184589331075,"d ¬∑

œÜ(¬∑, ¬∑)‚ä§bŒõ‚àí1
h œÜ(¬∑, ¬∑)
1/2
+ 2H3‚àö"
MAIN RESULT,0.09906858594411516,"d
K
(Use ŒìI
h for the improved version)"
MAIN RESULT,0.09991532599491956,"14:
Set ¬ØQh(¬∑, ¬∑) ‚ÜêœÜ(¬∑, ¬∑)‚ä§bwh ‚àíŒìh(¬∑, ¬∑)
15:
Set bQh(¬∑, ¬∑) ‚Üêmin
 ¬ØQh(¬∑, ¬∑), H ‚àíh + 1
	+"
MAIN RESULT,0.10076206604572396,"16:
Set bœÄh(¬∑ | ¬∑) ‚Üêarg maxœÄh

 bQh(¬∑, ¬∑), œÄh(¬∑ | ¬∑)"
MAIN RESULT,0.10160880609652836,"A, bVh(¬∑) ‚ÜêmaxœÄh

 bQh(¬∑, ¬∑), œÄh(¬∑ | ¬∑)"
MAIN RESULT,0.10245554614733277,"A
17: end for
18: Output: {bœÄh}H
h=1."
MAIN RESULT,0.10330228619813717,"Theorem 3.2 provides improvements over the existing best-known results and we now explain it.
However, before that, we Ô¨Årst discuss about our theorem condition."
MAIN RESULT,0.10414902624894158,"Comparing to Zhou et al. (2021a). In the online regime, Zhou et al. (2021a) is the Ô¨Årst result
that achieves optimal regret rate with O(dH
‚àö"
MAIN RESULT,0.10499576629974598,"T) in the linear (mixture) MDPs. However, this
result requires the condition d ‚â•H (their Theorem 6 and Remark 7). In ofÔ¨Çine RL, VAPVI only
requires a milder condition
‚àö"
MAIN RESULT,0.10584250635055038,"d > Œæ comparing to d ‚â•H (since for any Ô¨Åxed V ‚àà[0, H], the
standardized quantity r+V (s‚Ä≤)‚àí(ThV )(s,a)"
MAIN RESULT,0.10668924640135478,"œÉV (s,a)
is bounded by constant with high probability, e.g. by
chebyshev inequality), which makes our result apply to a wider range of linear MDPs."
MAIN RESULT,0.10753598645215919,"Comparing to Jin et al. (2021b). Jin et al. (2021b) Ô¨Årst shows pessimistic value iteration (PEVI)
is provably efÔ¨Åcient for Linear MDPs in ofÔ¨Çine RL. VAPVI improves PEVI over O(
‚àö"
MAIN RESULT,0.10838272650296359,"d) on
the feature dimension, and improves the horizon dependence as Œõh ‚âΩ
1
H2 Œ£h implies Œõ‚àí1
h
‚âº
H2Œ£‚àí1
h . In addition, when instantiate to the tabular case, i.e. œÜ(s, a) = 1s,a, VAPVI gives O(
‚àö d P"
MAIN RESULT,0.10922946655376799,"h,s,a dœÄ‚ãÜ
h (s, a)
r"
MAIN RESULT,0.1100762066045724,"VarPs,a (r+V ‚ãÜ
h+1)"
MAIN RESULT,0.1109229466553768,"K¬∑d¬µ
h(s,a)
), which enjoys O(
‚àö"
MAIN RESULT,0.1117696867061812,H) improvement over PEVI (recall
MAIN RESULT,0.1126164267569856,Example 3.1) and the order O(H3/2) is tight (check Section G for the detailed derivation).
MAIN RESULT,0.11346316680779,"Comparing to Xie et al. (2021a). Their linear MDP guarantee in Theorem 3.2. enjoys the same
rate as VAPVI in feature dimension but the horizon dependence is essentially the same as Jin et al.
(2021b) (by translating H ‚âàO(
1
1‚àíŒ≥ )) therefore is not optimal. The general function approximation
scheme in Xie et al. (2021a) provides elegant characterizations for on-support error and off-support
error, but the algorithmic framework is information-theoretical only (and the practical version PSPI
will not yield the same learning guarantee). Also, due to the use Ô¨Ånite function class and policy class,
the reduction to linear MDP only works with Ô¨Ånite action space. As a comparison, VAPVI has no
constraints on any of these."
MAIN RESULT,0.1143099068585944,"Comparing to Zanette et al. (2021). Concurrently, Zanette et al. (2021) considers ofÔ¨Çine RL with
the linear Bellman complete model, which is more general than linear MDPs and, with the assumption
QœÄ ‚â§1, their PACLE algorithm provides near-minimax optimal guarantee in this setting. However,
when recovering to the standard setting QœÄ ‚àà[0, H], their bound will rescale by an H factor,7 which
could be suboptimal due to the variance-unawareness. The reason behind this is: when QœÄ ‚â§1,
lack of variance information encoding will not matter, since in this case VarP (V œÄ) ‚â§1 has constant"
MAIN RESULT,0.11515664690939881,7Check their Footnote 2 in Page 9.
MAIN RESULT,0.11600338696020322,Published as a conference paper at ICLR 2022
MAIN RESULT,0.11685012701100762,"order (therefore will not affect the optimal rate); when QœÄ ‚àà[0, H], VarP (V œÄ) can be as large as
H2, effectively leveraging the variance information can help improve the sample efÔ¨Åciency, e.g. via
law of total variances, just like VAPVI does. On the other hand, their guarantee also requires Ô¨Ånite
action space, due to the mirror descent style analysis. Nevertheless, we do point out Zanette et al.
(2021) has improved state-action measure than VAPVI, as ‚à•EœÄ[œÜ(¬∑, ¬∑)]‚à•M ‚àí1 ‚â§EœÄ[‚à•œÜ(¬∑, ¬∑)‚à•M ‚àí1] by
Jensen‚Äôs inequality and that norm ‚à•¬∑‚à•M ‚àí1 is convex for some positive-deÔ¨Ånite matrix M."
MAIN RESULT,0.11769686706181202,"Adaptive characterization and faster convergence. Comparing to existing works, one major"
MAIN RESULT,0.11854360711261643,"improvement is that the main term for VAPVI
‚àö"
MAIN RESULT,0.11939034716342083,"d PH
h=1 EœÄ‚ãÜq"
MAIN RESULT,0.12023708721422523,"œÜ(¬∑, ¬∑)‚ä§Œõ‚ãÜ‚àí1
h
œÜ(¬∑, ¬∑)

admits no
explicit dependence on H, which provides a more adaptive/instance-dependent characterization. For
instance, if we ignore the technical treatment by taking Œª = 0 and œÉ‚ãÜ
h ‚âàVarP (V ‚ãÜ
h+1), then for the
partially deterministic systems (where there are t stochastic Ph‚Äôs and H ‚àít deterministic Ph‚Äôs), the"
MAIN RESULT,0.12108382726502964,"main term diminishes to
‚àö"
MAIN RESULT,0.12193056731583404,"d Pt
i=1 EœÄ‚ãÜq"
MAIN RESULT,0.12277730736663844,"œÜ(¬∑, ¬∑)‚ä§Œõ‚ãÜ‚àí1
hi œÜ(¬∑, ¬∑)

with hi ‚àà{h : s.t. Ph is stochastic}
and can be a much smaller quantity when t ‚â™H. Furthermore, for the fully deterministic system,
VAPVI automatically provides faster convergence rate O( 1"
MAIN RESULT,0.12362404741744284,"K ) from the higher order term, given that
the main term degenerates to 0. Those adaptive/instance-dependent features are not enjoyed by (Xie
et al., 2021a; Zanette et al., 2021), as they always provide the standard statistical rate O(
1
‚àö"
MAIN RESULT,0.12447078746824725,"K ) (also
check Remark C.9 for a related discussion)."
MAIN RESULT,0.12531752751905165,"3.3
VAPVI-IMPROVED: FURTHER IMPROVEMENT IN STATE-ACTION DIMENSION"
MAIN RESULT,0.12616426756985605,"Can we further improve the VAPVI? Indeed, by deploying a carefully tuned tighter penalty, we are
able to further improve the state-action dependence if the feature is non-negative (œÜ ‚â•0). Concretely,
we replace the following ŒìI
h in Algorithm 1 instead, and call the algorithm VAPVI-Improved (or
VAPVI-I for short). The proof can be found in Appendix D."
MAIN RESULT,0.12701100762066045,"ŒìI
h(s, a) ‚ÜêœÜ(s, a)‚ä§
bŒõ‚àí1
h K
X œÑ=1"
MAIN RESULT,0.12785774767146485,"œÜ (sœÑ
h, aœÑ
h) ¬∑

rœÑ
h + bVh+1 (sœÑ
h+1) ‚àí

bTh bVh+1

(sœÑ
h, aœÑ
h)
"
MAIN RESULT,0.12870448772226925,"bœÉ2
h(sœÑ
h, aœÑ
h)"
MAIN RESULT,0.12955122777307368,+ eO(H3d/Œ∫
MAIN RESULT,0.13039796782387808,"K
) (4)"
MAIN RESULT,0.13124470787468248,"Theorem 3.3. Suppose the feature is non-negative (œÜ ‚â•0). Let K be the number of episodes. If
K > max{M1, M2, M3, M4} and
‚àö"
MAIN RESULT,0.13209144792548688,"d > Œæ. Deploying ŒìI
h (4) in Algorithm 1. Then for any
0 < Œª < Œ∫, with probability 1 ‚àíŒ¥, for all policy œÄ simultaneously, the output bœÄ of Algorithm 1
(VAPVI-I) satisÔ¨Åes"
MAIN RESULT,0.13293818797629128,"vœÄ ‚àívbœÄ ‚â§eO
 ‚àö d ¬∑ H
X h=1 q"
MAIN RESULT,0.13378492802709568,"EœÄ[œÜ(¬∑, ¬∑)]‚ä§Œõ‚àí1
h EœÄ[œÜ(¬∑, ¬∑)]

+ eO(H4d/Œ∫ K
)"
MAIN RESULT,0.13463166807790009,"In particular, when choosing œÄ = œÄ‚ãÜ, the above guarantee holds true with Œõ‚àí1
h
replaced by Œõ‚ãÜ‚àí1
h
.
Here Œõ‚àí1
h , Œõ‚ãÜ‚àí1
h
, Œæ are deÔ¨Åned the same as Theorem 3.2."
MAIN RESULT,0.1354784081287045,"Theorem 3.3 maintains nearly all the features of Theorem 3.2 (except higher order term is slightly
worse) and the dominate term evolves from EœÄ ‚à•œÜ‚à•Œõ‚àí1
h
to ‚à•EœÄ[œÜ]‚à•Œõ‚àí1
h . Clearly, the two bounds
differ by the magnitude of Jensen‚Äôs inequality. To provide a concrete view of how much improvement
is made, we check the parameter dependence in the context of tabular MDPs (where we ignore the
higher order term for conciseness). In particular, we compare the results under the single-policy
concentrability."
MAIN RESULT,0.1363251481795089,"Assumption 3.4 (Rashidinejad et al. (2021); Xie et al. (2021b)). There exists a optimal policy œÄ‚ãÜ, s.t.
suph,s,a dœÄ‚ãÜ
h (s, a)/d¬µ
h(s, a) := C‚ãÜ< ‚àû, where dœÄ is the marginal state-action probability under œÄ."
MAIN RESULT,0.1371718882303133,"In tabular RL, œÜ(s, a) = 1s,a and d = S ¬∑ A (S, A be the Ô¨Ånite state, action cardinality), then"
MAIN RESULT,0.1380186282811177,"Theorem 3.2 ‚Üí
‚àö SA H
X h X"
MAIN RESULT,0.1388653683319221,"s,a
dœÄ‚ãÜ
h (s, a) s"
MAIN RESULT,0.1397121083827265,"VarPs,a(r + V ‚ãÜ
h+1)
K ¬∑ d¬µ
h(s, a)
‚â§ r"
MAIN RESULT,0.14055884843353092,"H3C‚ãÜS2A K
;"
MAIN RESULT,0.14140558848433532,"Theorem 3.3 ‚Üí
‚àö SA H
X h"
MAIN RESULT,0.14225232853513972,"v
u
u
tX"
MAIN RESULT,0.14309906858594412,"s,a
dœÄ‚ãÜ
h (s, a)2 VarPs,a(r + V ‚ãÜ
h+1)
K ¬∑ d¬µ
h(s, a)
‚â§ r"
MAIN RESULT,0.14394580863674852,"H3C‚ãÜSA K
. (5)"
MAIN RESULT,0.14479254868755292,Published as a conference paper at ICLR 2022
MAIN RESULT,0.14563928873835733,"Theorem 3.3 enjoys a S state improvement over Theorem 3.2 and nearly recovers the minimax rate
q H3C‚ãÜS"
MAIN RESULT,0.14648602878916173,"K
(Xie et al., 2021b). The detailed derivation can be found in Appendix G. Also, to show our
result is near-optimal, we provide the corresponding lower bound. The proof is in Appendix E.
Theorem 3.5 (Minimax lower bound). There exist a pair of universal constants c, c‚Ä≤ > 0 such that
given dimension d, horizon H and sample size K > c‚Ä≤d3, one can always Ô¨Ånd a family of linear
MDP instances M such that (where Œõ‚ãÜ
h = PK
k=1
œÜ(sk
h,ak
h)¬∑œÜ(sk
h,ak
h)‚ä§"
MAIN RESULT,0.14733276883996613,"Varh(V ‚ãÜ
h+1)(sk
h,ak
h) satisÔ¨Åes (Œõ‚ãÜ
h)‚àí1 exists and"
MAIN RESULT,0.14817950889077053,"Varh(V ‚ãÜ
h+1)(sk
h, ak
h) > 0 ‚àÄM ‚ààM)"
MAIN RESULT,0.14902624894157493,"inf
bœÄ
sup
M‚ààM
EM

v‚ãÜ‚àívbœÄ‚àö d ¬∑ H
X h=1 q"
MAIN RESULT,0.14987298899237933,"EœÄ‚ãÜ[œÜ]‚ä§(Œõ‚ãÜ
h)‚àí1EœÄ‚ãÜ[œÜ]

‚â•c.
(6)"
MAIN RESULT,0.15071972904318373,Theorem 3.5 nearly matches the main term in VAPVI-I (Theorem 3.3) and certiÔ¨Åes it is near-optimal.
PROOF OVERVIEW,0.15156646909398813,"4
PROOF OVERVIEW"
PROOF OVERVIEW,0.15241320914479256,"Due to the space constraint, we could only provide a brief overview of the key proving ideas of the
theorems. We begin with Theorem 3.2. First, by the extended value difference lemma (Lemma H.7),
we can convert bounding the suboptimality gap of v‚ãÜ‚àívbœÄ to bounding PH
h=1 2 ¬∑ EœÄ [Œìh(sh, ah)],
given that |(Th bVh+1 ‚àíbTh bVh+1)(s, a)| ‚â§Œìh(s, a) for all s, a, h. To bound Th bVh+1 ‚àíbTh bVh+1, by
decomposing it reduces to bounding the key quantity"
PROOF OVERVIEW,0.15325994919559696,"œÜ(s, a)‚ä§bŒõ‚àí1
h  K
X"
PROOF OVERVIEW,0.15410668924640136,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

rœÑ
h + bVh+1
 
sœÑ
h+1

‚àí

Th bVh+1

(sœÑ
h, aœÑ
h)

/bœÉ2
h(sœÑ
h, aœÑ
h)

(7)"
PROOF OVERVIEW,0.15495342929720576,"The term is treated in two steps. First, we bound the gap of
œÉ2
bVh+1 ‚àíbœÉ2
h
 so we can convert bœÉ2
h to"
PROOF OVERVIEW,0.15580016934801016,"œÉ2
bVh+1. Next, since Var
h
rœÑ
h + bVh+1
 
sœÑ
h+1

‚àí

Th bVh+1

(sœÑ
h, aœÑ
h) | sœÑ
h, aœÑ
h
i
‚âàœÉ2
bVh+1, therefore by
the variance-weighted scheme in (equation 7), we can leverage the recent technical development
Bernstein inequality for self-normalized martingale (Lemma H.3) for acquiring the tight result, in
contrast to the previous treatment of Hoeffding inequality for self-normalized martingale + Covering.8
For the second part, one needs to further convert œÉ2
bVh+1 to œÉ‚ãÜ2
h (Œõ‚àí1
h
to Œõ‚ãÜ‚àí1
h
) with appropriate
concentrations. The proof of Theorem 3.3 is similar but with more complicated computations and
relies on using the linear representation of œÜ in ŒìI
h (4), so that the expectation over œÄ is inside the
square root by taking expectation over the linear representation at the beginning. The lower bound
proof uses a simple modiÔ¨Åcation of Zanette et al. (2021) which consists of the reduction from learning
to testing with Assouad‚Äôs method, and the use of standard information inequalities (e.g. from total
variation to KL divergence). For completeness, we provide the full proof in Appendix E."
DISCUSSION AND CONCLUSION,0.15664690939881457,"5
DISCUSSION AND CONCLUSION"
DISCUSSION AND CONCLUSION,0.15749364944961897,"This work studies ofÔ¨Çine RL with linear MDP representation and contributes Variance Aware
Pessimistic Value Iteration (VAPVI) which adopts the conditional variance information of the value
function. VAPVI uses the estimated variances to reweight the Bellman residuals in the least-square
pessimistic value iteration and provides improved ofÔ¨Çine learning bounds over the existing best-known
results. VAPVI-I further improves over VAPVI in the state-action dimension and is near-minimax
optimal. One highlight of the theorems is that our learning bounds are expressed in terms of system
quantities, which automatically provide natural instance-dependent characterizations that previous
results are short of."
DISCUSSION AND CONCLUSION,0.15834038950042337,"On the other hand, while VAPVI/VAPVI-I close the existing gap from previous literature (Jin et al.,
2021b; Xie et al., 2021a), the optimal guarantee is in the minimax sense. Although our upper bounds
possess instance-dependent characterizations, the lower bound only holds true for a class of hard
instances. In this sense, whether ‚Äúinstance-dependent optimality‚Äù can be achieved remains elusive in
the current linear MDP setting (such a discussion is recently initiated in MAB problems (Xiao et al.,
2021)). We leave this as future work."
DISCUSSION AND CONCLUSION,0.15918712955122777,"8Variance-reweighting in (7) is important, since applying Bernstein inequality for self-normalized martingale
(Lemma H.3) without variance-reweighting cannot provide any improvement."
DISCUSSION AND CONCLUSION,0.16003386960203217,Published as a conference paper at ICLR 2022
DISCUSSION AND CONCLUSION,0.16088060965283657,ACKNOWLEDGMENTS
DISCUSSION AND CONCLUSION,0.16172734970364097,"The authors would like to thank Quanquan Gu for explaining Min et al. (2021) and introducing a
couple of related literatures. Ming Yin would like to thank Zhuoran Yang for the helpful suggestions
and Dan Qiao for a careful proofreading. Mengdi Wang gratefully acknowledges funding from OfÔ¨Åce
of Naval Research (ONR) N00014-21-1-2288, Air Force OfÔ¨Åce of ScientiÔ¨Åc Research (AFOSR)
FA9550-19-1-0203, and NSF 19-589, CMMI-1653435. Yu-Xiang Wang gratefully acknowledges
funding from National Science Foundation (NSF) #2007117 and #2003257."
REFERENCES,0.16257408975444537,REFERENCES
REFERENCES,0.1634208298052498,"Yasin Abbasi-Yadkori, D¬¥avid P¬¥al, and Csaba Szepesv¬¥ari. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems, pages 2312‚Äì2320, 2011."
REFERENCES,0.1642675698560542,"Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy
gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning
Research, 22(98):1‚Äì76, 2021."
REFERENCES,0.1651143099068586,"Sanae Amani, Christos Thrampoulidis, and Lin F Yang. Safe reinforcement learning with linear
function approximation. arXiv preprint arXiv:2106.06239, 2021."
REFERENCES,0.165961049957663,"Andras Antos, Remi Munos, and Csaba Szepesvari. Fitted q-iteration in continuous action-space
mdps. In Advances in Neural Information Processing Systems, pages 9‚Äì16, 2008a."
REFERENCES,0.1668077900084674,"Andr¬¥as Antos, Csaba Szepesv¬¥ari, and R¬¥emi Munos. Learning near-optimal policies with bellman-
residual minimization based Ô¨Åtted policy iteration and a single sample path. Machine Learning, 71
(1):89‚Äì129, 2008b."
REFERENCES,0.1676545300592718,"Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement
learning with value-targeted regression. In International Conference on Machine Learning, pages
463‚Äì474. PMLR, 2020."
REFERENCES,0.1685012701100762,"Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efÔ¨Åcient exploration in policy optimiza-
tion. In International Conference on Machine Learning, pages 1283‚Äì1294. PMLR, 2020."
REFERENCES,0.1693480101608806,"Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In
International Conference on Machine Learning, pages 1042‚Äì1051, 2019."
REFERENCES,0.170194750211685,"Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel,
Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence
modeling. arXiv preprint arXiv:2106.01345, 2021a."
REFERENCES,0.1710414902624894,"Zixiang Chen, Dongruo Zhou, and Quanquan Gu. Almost optimal algorithms for two-player markov
games with linear function approximation. arXiv preprint arXiv:2102.07404, 2021b."
REFERENCES,0.1718882303132938,"Fan Chung and Linyuan Lu. Concentration inequalities and martingale inequalities: a survey. Internet
Mathematics, 3(1):79‚Äì127, 2006."
REFERENCES,0.1727349703640982,"Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford.
Provably efÔ¨Åcient rl with rich observations via latent state decoding. In International Conference
on Machine Learning, pages 1665‚Äì1674. PMLR, 2019."
REFERENCES,0.1735817104149026,"Simon S Du, Sham M Kakade, Jason D Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong
Wang. Bilinear classes: A structural framework for provable generalization in rl. International
Conference on Machine Learning, 2021."
REFERENCES,0.17442845046570704,"Yaqi Duan, Zeyu Jia, and Mengdi Wang. Minimax-optimal off-policy evaluation with linear function
approximation. In International Conference on Machine Learning, pages 8334‚Äì8342, 2020."
REFERENCES,0.17527519051651144,"Yaqi Duan, Chi Jin, and Zhiyuan Li. Risk bounds and rademacher complexity in batch reinforcement
learning. International Conference on Machine Learning, 2021."
REFERENCES,0.17612193056731584,"Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6:503‚Äì556, 2005."
REFERENCES,0.17696867061812024,Published as a conference paper at ICLR 2022
REFERENCES,0.17781541066892464,"Dylan J Foster, Akshay Krishnamurthy, David Simchi-Levi, and Yunzong Xu. OfÔ¨Çine reinforcement
learning: Fundamental barriers for value function approximation. arXiv preprint arXiv:2111.10919,
2021."
REFERENCES,0.17866215071972905,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pages 2052‚Äì2062. PMLR, 2019."
REFERENCES,0.17950889077053345,"Minbo Gao, Tianle Xie, Simon S Du, and Lin F Yang. A provably efÔ¨Åcient algorithm for linear
markov decision process with low switching cost. arXiv preprint arXiv:2101.00494, 2021."
REFERENCES,0.18035563082133785,"Geoffrey J Gordon. Approximate solutions to Markov decision processes. Carnegie Mellon University,
1999."
REFERENCES,0.18120237087214225,"Jiafan He, Dongruo Zhou, and Quanquan Gu. Logarithmic regret for reinforcement learning with
linear function approximation. In International Conference on Machine Learning, pages 4171‚Äì
4180. PMLR, 2021."
REFERENCES,0.18204911092294665,"Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence modeling
problem. arXiv preprint arXiv:2106.02039, 2021."
REFERENCES,0.18289585097375105,"Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contex-
tual decision processes with low bellman rank are pac-learnable. In International Conference on
Machine Learning-Volume 70, pages 1704‚Äì1713, 2017."
REFERENCES,0.18374259102455545,"Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efÔ¨Åcient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pages 2137‚Äì2143.
PMLR, 2020."
REFERENCES,0.18458933107535985,"Chi Jin, Qinghua Liu, and Sobhan MiryooseÔ¨Å. Bellman eluder dimension: New rich classes of rl
problems, and sample-efÔ¨Åcient algorithms. arXiv preprint arXiv:2102.00815, 2021a."
REFERENCES,0.18543607112616428,"Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efÔ¨Åcient for ofÔ¨Çine rl? In
International Conference on Machine Learning, pages 5084‚Äì5096. PMLR, 2021b."
REFERENCES,0.18628281117696868,"Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based ofÔ¨Çine reinforcement learning. Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.18712955122777308,"Ilya Kostrikov, Ashvin Nair, and Sergey Levine. OfÔ¨Çine reinforcement learning with in-sample
q-learning. In International Conference on Learning Representations, 2022."
REFERENCES,0.18797629127857748,"Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Pac reinforcement learning with rich
observations. Advances in neural information processing systems, 2016."
REFERENCES,0.18882303132938189,"Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via
bootstrapping error reduction. Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.1896697713801863,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofÔ¨Çine
reinforcement learning. Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.1905165114309907,"Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforcement
learning, pages 45‚Äì73. Springer, 2012."
REFERENCES,0.1913632514817951,"Tor Lattimore and Csaba Szepesv¬¥ari. Bandit algorithms. Cambridge University Press, 2020."
REFERENCES,0.1922099915325995,"Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In Interna-
tional Conference on Machine Learning, pages 3703‚Äì3712, 2019."
REFERENCES,0.1930567315834039,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. OfÔ¨Çine reinforcement learning: Tutorial,
review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.1939034716342083,"Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch reinforce-
ment learning without great exploration. arXiv preprint arXiv:2007.08202, 2020."
REFERENCES,0.1947502116850127,Published as a conference paper at ICLR 2022
REFERENCES,0.1955969517358171,"Zhihan Liu, Yufeng Zhang, Zuyue Fu, Zhuoran Yang, and Zhaoran Wang. Provably efÔ¨Åcient genera-
tive adversarial imitation learning for online and ofÔ¨Çine setting with linear function approximation.
arXiv preprint arXiv:2108.08765, 2021."
REFERENCES,0.1964436917866215,"Yifei Min, Tianhao Wang, Dongruo Zhou, and Quanquan Gu. Variance-aware off-policy evaluation
with linear function approximation. Advances in neural information processing systems, 2021."
REFERENCES,0.19729043183742592,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. nature, 518(7540):529‚Äì533, 2015."
REFERENCES,0.19813717188823032,"Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement
learning using linearly combined model ensembles. In International Conference on ArtiÔ¨Åcial
Intelligence and Statistics, pages 2010‚Äì2020. PMLR, 2020."
REFERENCES,0.19898391193903472,"R¬¥emi Munos. Error bounds for approximate policy iteration. In ICML, volume 3, pages 560‚Äì567,
2003."
REFERENCES,0.19983065198983913,"Thanh Nguyen-Tang, Sunil Gupta, Hung Tran-The, and Svetha Venkatesh. On Ô¨Ånite-sample analysis
of ofÔ¨Çine reinforcement learning with deep relu networks. arXiv preprint arXiv:2103.06671, 2021."
REFERENCES,0.20067739204064353,"Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging ofÔ¨Çine rein-
forcement learning and imitation learning: A tale of pessimism. arXiv preprint arXiv:2103.12021,
2021."
REFERENCES,0.20152413209144793,"Tongzheng Ren, Jialian Li, Bo Dai, Simon S Du, and Sujay Sanghavi. Nearly horizon-free ofÔ¨Çine
reinforcement learning. Advances in neural information processing systems, 2021."
REFERENCES,0.20237087214225233,"Paul D Sampson and Peter Guttorp. Nonparametric estimation of nonstationary spatial covariance
structure. Journal of the American Statistical Association, 87(417):108‚Äì119, 1992."
REFERENCES,0.20321761219305673,"Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Pessimistic q-learning for ofÔ¨Çine
reinforcement learning: Towards optimal sample complexity. arXiv preprint arXiv:2202.13890,
2022."
REFERENCES,0.20406435224386113,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. nature, 550(7676):354‚Äì359, 2017."
REFERENCES,0.20491109229466553,"Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
rl in contextual decision processes: Pac bounds and exponential improvements over model-free
approaches. In Conference on learning theory, pages 2898‚Äì2933. PMLR, 2019."
REFERENCES,0.20575783234546993,"Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018."
REFERENCES,0.20660457239627433,"Csaba Szepesv¬¥ari and R¬¥emi Munos. Finite time bounds for sampling based Ô¨Åtted value iteration. In
Proceedings of the 22nd international conference on Machine learning, pages 880‚Äì887, 2005."
REFERENCES,0.20745131244707873,"Mohammad Sadegh Talebi and Odalric-Ambrym Maillard. Variance-aware regret bounds for undis-
counted reinforcement learning in mdps. In Algorithmic Learning Theory, pages 770‚Äì805. PMLR,
2018."
REFERENCES,0.20829805249788316,"Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational
mathematics, 12(4):389‚Äì434, 2012."
REFERENCES,0.20914479254868756,"Masatoshi Uehara and Wen Sun. Pessimistic model-based ofÔ¨Çine rl: Pac bounds and posterior
sampling under partial coverage. arXiv preprint arXiv:2107.06226, 2021."
REFERENCES,0.20999153259949196,"Ruosong Wang, Simon S Du, Lin F Yang, and Ruslan Salakhutdinov. On reward-free reinforcement
learning with linear function approximation. Advances in neural information processing systems,
2020."
REFERENCES,0.21083827265029637,"Ruosong Wang, Dean P Foster, and Sham M Kakade. What are the statistical limits of ofÔ¨Çine rl with
linear function approximation? International Conference on Learning Representations, 2021a."
REFERENCES,0.21168501270110077,Published as a conference paper at ICLR 2022
REFERENCES,0.21253175275190517,"Yining Wang, Ruosong Wang, Simon Shaolei Du, and Akshay Krishnamurthy. Optimism in rein-
forcement learning with generalized linear function approximation. In International Conference
on Learning Representations, 2021b."
REFERENCES,0.21337849280270957,"Yifan Wu, George Tucker, and OÔ¨År Nachum. Behavior regularized ofÔ¨Çine reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019."
REFERENCES,0.21422523285351397,"Chenjun Xiao, Yifan Wu, Jincheng Mei, Bo Dai, Tor Lattimore, Lihong Li, Csaba Szepesvari, and
Dale Schuurmans. On the optimality of batch policy optimization algorithms. In International
Conference on Machine Learning, pages 11362‚Äì11371. PMLR, 2021."
REFERENCES,0.21507197290431837,"Tengyang Xie and Nan Jiang. Q* approximation schemes for batch reinforcement learning: A
theoretical comparison. In Uncertainty in ArtiÔ¨Åcial Intelligence, pages 550‚Äì559, 2020."
REFERENCES,0.21591871295512277,"Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent
pessimism for ofÔ¨Çine reinforcement learning. Advances in neural information processing systems,
2021a."
REFERENCES,0.21676545300592717,"Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy Ô¨Ånetuning: Bridg-
ing sample-efÔ¨Åcient ofÔ¨Çine and online reinforcement learning. Advances in neural information
processing systems, 2021b."
REFERENCES,0.21761219305673157,"Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features.
In International Conference on Machine Learning, pages 6995‚Äì7004. PMLR, 2019."
REFERENCES,0.21845893310753597,"Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In International Conference on Machine Learning, pages 10746‚Äì10756. PMLR,
2020."
REFERENCES,0.2193056731583404,"Ming Yin and Yu-Xiang Wang. Asymptotically efÔ¨Åcient off-policy evaluation for tabular rein-
forcement learning. In International Conference on ArtiÔ¨Åcial Intelligence and Statistics, pages
3948‚Äì3958. PMLR, 2020."
REFERENCES,0.2201524132091448,"Ming Yin and Yu-Xiang Wang. Optimal uniform ope and model-based ofÔ¨Çine reinforcement learning
in time-homogeneous, reward-free and task-agnostic settings. Advances in neural information
processing systems, 2021a."
REFERENCES,0.2209991532599492,"Ming Yin and Yu-Xiang Wang. Towards instance-optimal ofÔ¨Çine reinforcement learning with
pessimism. Advances in neural information processing systems, 2021b."
REFERENCES,0.2218458933107536,"Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal provable uniform convergence in ofÔ¨Çine policy
evaluation for reinforcement learning. In International Conference on ArtiÔ¨Åcial Intelligence and
Statistics, pages 1567‚Äì1575. PMLR, 2021."
REFERENCES,0.222692633361558,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and
Tengyu Ma. Mopo: Model-based ofÔ¨Çine policy optimization. arXiv preprint arXiv:2005.13239,
2020."
REFERENCES,0.2235393734123624,"Andrea Zanette. Exponential lower bounds for batch reinforcement learning: Batch rl can be
exponentially harder than online rl. International Conference on Machine Learning, 2021."
REFERENCES,0.2243861134631668,"Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement
learning without domain knowledge using value function bounds. In International Conference on
Machine Learning, pages 7304‚Äì7312. PMLR, 2019."
REFERENCES,0.2252328535139712,"Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near
optimal policies with low inherent bellman error. In International Conference on Machine Learning,
pages 10978‚Äì10989. PMLR, 2020."
REFERENCES,0.2260795935647756,"Andrea Zanette, Martin J. Wainwright, and Emma Brunskill. Provable beneÔ¨Åts of actor-critic methods
for ofÔ¨Çine reinforcement learning, 2021."
REFERENCES,0.22692633361558,"Zihan Zhang, Jiaqi Yang, Xiangyang Ji, and Simon S Du. Variance-aware conÔ¨Ådence set: Variance-
dependent bound for linear bandits and horizon-free bound for linear mixture mdp. arXiv preprint
arXiv:2101.12745, 2021."
REFERENCES,0.2277730736663844,Published as a conference paper at ICLR 2022
REFERENCES,0.2286198137171888,"Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement
learning for linear mixture markov decision processes. In Conference on Learning Theory, pages
4532‚Äì4576. PMLR, 2021a."
REFERENCES,0.22946655376799321,"Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efÔ¨Åcient reinforcement learning for discounted
mdps with feature mapping. In International Conference on Machine Learning, pages 12793‚Äì
12802. PMLR, 2021b."
REFERENCES,0.23031329381879762,Published as a conference paper at ICLR 2022
REFERENCES,0.23116003386960204,Appendix
REFERENCES,0.23200677392040644,"A
NOTATION LIST"
REFERENCES,0.23285351397121085,"Œ£p
h
E¬µ,h

œÜ(s, a)œÜ(s, a)‚ä§"
REFERENCES,0.23370025402201525,"Œõp
h
E¬µ,h

œÉVh+1(s, a)‚àí2œÜ(s, a)œÜ(s, a)‚ä§"
REFERENCES,0.23454699407281965,"Œ∫
minh Œªmin(Œ£p
h)"
REFERENCES,0.23539373412362405,"Œπ
minh Œªmin(Œõp
h) ‚â•Œ∫/H2 for any Vh"
REFERENCES,0.23624047417442845,"œÉ2
V (s, a)
max{1, VarPh(V )(s, a)} for any V"
REFERENCES,0.23708721422523285,"œÉ2
Vh+1(s, a)
max{1, VarPh(Vh+1)(s, a)}"
REFERENCES,0.23793395427603725,"bœÉ2
Vh(s, a)
max{1, d
VarhVh+1(s, a)}
¬ØŒ£h
PK
œÑ=1 œÜ(¬ØsœÑ
h, ¬ØaœÑ
h)œÜ(¬ØsœÑ
h, ¬ØaœÑ
h)‚ä§+ ŒªId
bŒõh
PK
k=1 œÜ(sk
h, ak
h)œÜ(sk
h, ak
h)‚ä§/bœÉ2
h(sk
h, ak
h) + ŒªId"
REFERENCES,0.23878069432684165,"M1
max{2Œª, 128 log(2d/Œ¥), 128H4 log(2d/Œ¥)/Œ∫2}"
REFERENCES,0.23962743437764605,"M2
max{
Œª2
Œ∫ log((Œª+K)H/ŒªŒ¥), 962H12d log((Œª + K)H/ŒªŒ¥)/Œ∫5}"
REFERENCES,0.24047417442845045,"M3
max

512H4/Œ∫2 log
  2d"
REFERENCES,0.24132091447925486,"Œ¥

, 4ŒªH2/Œ∫"
REFERENCES,0.24216765453005928,"M4
12
p"
REFERENCES,0.24301439458086369,H4d log((Œª + K)H/ŒªŒ¥)/Œ∫
REFERENCES,0.2438611346316681,"Œ¥
Failure probability"
REFERENCES,0.2447078746824725,"Œæ
supV ‚àà[0,H], s‚Ä≤‚àºPh(s,a), h‚àà[H]"
REFERENCES,0.2455546147332769,"rh+V (s‚Ä≤)‚àí(ThV )(s,a)"
REFERENCES,0.2464013547840813,"œÉV (s,a) "
REFERENCES,0.2472480948348857,"CH,d,Œ∫,K
36
r H4d3"
REFERENCES,0.2480948348856901,"Œ∫
log

(Œª+K)2KdH2"
REFERENCES,0.2489415749364945,"ŒªŒ¥

+ 12Œª H2‚àö d
Œ∫"
REFERENCES,0.2497883149872989,"B
EXTENDED LITERATURE REVIEW"
REFERENCES,0.2506350550381033,"B.1
LINEAR MODEL REPRESENTATION AND ITS EXTENSION IN ONLINE RL"
REFERENCES,0.2514817950889077,"There are numerous works in online RL that study linear model representations. Yang and Wang
(2019; 2020); Jin et al. (2020) propose Linear MDP, which assumes the transition kernel and the
reward are linear in given features. Cai et al. (2020); Ayoub et al. (2020); Modi et al. (2020); Zhou
et al. (2021b) propose Linear mixture MDP, which assumes the transition probability is a linear
combination of some base kernels. Linear Bellman Complete model (Zanette et al., 2020) generalizes
linear MDP model by allowing linear functions to approximate the Q-function and the function class
is closed under the Bellman update. The notion of low Bellman rank (Jiang et al., 2017) subsumes
not only linear MDPs but also models including linear quadratic regulator (LQR), Reactive POMDP
(Krishnamurthy et al., 2016) and Block MDP (Du et al., 2019). There are also other models, e.g.
factored MDP (Sun et al., 2019), Bellman Eluder dimension (Jin et al., 2021a) and Bilinear class (Du
et al., 2021). With the linear MDP model itself, there are also fruitful extensions, e.g. gap-dependent
analysis with logarithmic regret (He et al., 2021), low-switching cost RL (Gao et al., 2021), safe RL
(Amani et al., 2021), reward-free RL (Wang et al., 2020), generalized linear model (GLM) (Wang
et al., 2021b), two-player markov game (Chen et al., 2021b) and generative adversarial learning (Liu
et al., 2021). In particular, Zhou et al. (2021a) shows UCRL-VTR+ is near-minimax optimal when
feature dimension d ‚â•H."
REFERENCES,0.2523285351397121,"B.2
EXISTING RESULTS IN OFFLINE RL WITH MODEL REPRESENTATIONS"
REFERENCES,0.2531752751905165,"OfÔ¨Çine RL with general function representations. The Ô¨Ånite sample analysis of ofÔ¨Çine RL with
function approximation is initially conducted by Fitted Q-Iteration (FQI) type algorithms and can
be dated back to (Munos, 2003; Szepesv¬¥ari and Munos, 2005; Antos et al., 2008a;b). Later, Chen
and Jiang (2019); Le et al. (2019); Xie and Jiang (2020) follow this line of research and derive the"
REFERENCES,0.2540220152413209,Published as a conference paper at ICLR 2022
REFERENCES,0.2548687552921253,"improved learning results. However, owing to the aim for tackling general function approximation,
those learning bounds are expressed in terms of the stringent concentrability coefÔ¨Åcients (therefore,
are less adaptive to individual instances) and are usually only information-theoretical, due to the
computational intractability of the optimization procedure over the general function classes. Other
works impose weaker assumptions (e.g. partial coverage (Liu et al., 2020; Kidambi et al., 2020;
Uehara and Sun, 2021)), and their Ô¨Ånite sample analysis are generally suboptimal in terms of H or
the effective horizon (1 ‚àíŒ≥)‚àí1."
REFERENCES,0.2557154953429297,"OfÔ¨Çine RL with tabular models. For tabular MDPs, tight learning bounds can be achieved under
several data-coverage assumptions. For the class of problems with uniform data-visitation measure
dm, the near-optimal sample complexity bound has the rate O(H3/dmœµ2) for time-inhomogeneous
MDPs (Yin et al., 2021) and O(H2/dmœµ) for time-homogeneous MDPs (Yin and Wang, 2021a; Ren
et al., 2021). Under the single concentrability assumption, the tight rate O(H3SC‚ãÜ/œµ2) is obtained
by Xie et al. (2021b). In particular, the recent study Yin and Wang (2021b) introduces the intrinsic
ofÔ¨Çine learning bound that is not only instance-dependent but also subsumes previous optimal results.
More recently, Shi et al. (2022) uses model-free approach to achieve minimax rate with a larger
œµ-range."
REFERENCES,0.25656223539373413,"OfÔ¨Çine RL with linear model representations. Recently, there are more focus on studying the
provable efÔ¨Åcient ofÔ¨Çine RL under the linear model representations. Jin et al. (2021b) Ô¨Årst shows
ofÔ¨Çine RL with linear MDP is provably efÔ¨Åcient by the pessimistic value iteration (PEVI), which
is an ofÔ¨Çine counterpart of LSVI-UCB in Jin et al. (2020). Their analysis deviates from their lower
bound by a factor of d ¬∑ H (check their Theorem 4.4 and 4.6). Later, Xie et al. (2021a) considers
function approximation under the Bellman-consistent assumptions, and, when realized to linear MDP
setting, improve the sample complexity guarantee of Jin et al. (2021b) by a order O(d) (Theorem 3.2).
However, their improvement only holds for Ô¨Ånite action space (due to the dependence log |A|) and
by the direct reduction (from Theorem 3.1) their result does not imply a computationally tractable
algorithm. In addition, there is no improvement on the horizon dependence. Concurrently, Zanette
et al. (2021) considers the Linear Bellman Complete model (which originates from its online version
Zanette et al. (2020)) and designs the actor-critic style algorithm that achieves tight result under
the assumption that the value function is bounded by 1. While their algorithm is efÔ¨Åcient (which is
based on solving a sequence of second-order cone programs), the resulting learning bound requires
the action space to be Ô¨Ånite due to the mirror descent/natural policy gradient updates in the Actor
procedure (Agarwal et al., 2021). Besides, assuming the value function to be less than 1 simpliÔ¨Åes
the challenges in dealing with horizon H since when rescale their result to [0, H], there is a H factor
blow-up, which makes no improvement in the horizon dependence comparing to Jin et al. (2021b).
On the other hand, Wang et al. (2021a); Zanette (2021) study the statistical hardness of ofÔ¨Çine RL
with linear representations by prooÔ¨Ång the exponential lower bounds. As a result, none of the existing
algorithms can achieve the statistical limit for the well-structured linear MDP model with the general
(inÔ¨Ånite or continuous) state-action spaces in the ofÔ¨Çine regime."
REFERENCES,0.2574089754445385,"C
PROOFS IN SECTION 3.2"
REFERENCES,0.25825571549534293,"Instead of prooÔ¨Ång the result for v‚ãÜ‚àívbœÄ, in most parts of the proof we deal with V ‚ãÜ
1 ‚àíV bœÄ
1 , which is
more general."
REFERENCES,0.25910245554614736,"C.1
SOME PREPARATIONS"
REFERENCES,0.25994919559695173,"DeÔ¨Åne the Bellman update error Œ∂h(s, a) := (Th bVh+1)(s, a) ‚àíbQh(s, a) and recall bœÄh(s) =
arg maxœÄh‚ü®bQh(s, ¬∑), œÄh(¬∑ | s)‚ü©A, then by the direct application of Lemma H.8"
REFERENCES,0.26079593564775616,"V œÄ
1 (s) ‚àíV bœÄ
1 (s) ‚â§ H
X"
REFERENCES,0.26164267569856053,"h=1
EœÄ [Œ∂h(sh, ah) | s1 = s] ‚àí H
X"
REFERENCES,0.26248941574936496,"h=1
EbœÄ [Œ∂h(sh, ah) | s1 = s] .
(8)"
REFERENCES,0.26333615580016934,"The next lemma shows it is sufÔ¨Åcient to bound the pessimistic penalty, which is the key in the proof."
REFERENCES,0.26418289585097376,"Lemma C.1. Suppose with probability 1 ‚àíŒ¥, it holds for all h, s, a ‚àà[H] √ó S √ó A that |(Th bVh+1 ‚àí
bTh bVh+1)(s, a)| ‚â§Œìh(s, a), then it implies ‚àÄs, a, h ‚ààS √ó A √ó [H], 0 ‚â§Œ∂h(s, a) ‚â§2Œìh(s, a)."
REFERENCES,0.26502963590177814,Published as a conference paper at ICLR 2022
REFERENCES,0.26587637595258257,"Furthermore, it holds for any policy œÄ simultaneously, with probability 1 ‚àíŒ¥,"
REFERENCES,0.26672311600338694,"V œÄ
1 (s) ‚àíV bœÄ
1 (s) ‚â§ H
X"
REFERENCES,0.26756985605419137,"h=1
2 ¬∑ EœÄ [Œìh(sh, ah) | s1 = s] ."
REFERENCES,0.26841659610499574,"Proof of Lemma C.1. We Ô¨Årst show given |(Th bVh+1 ‚àíbTh bVh+1)(s, a)| ‚â§Œìh(s, a), then 0 ‚â§
Œ∂h(s, a) ‚â§2Œìh(s, a), ‚àÄs, a, h ‚ààS √ó A √ó [H]."
REFERENCES,0.26926333615580017,"Step1: we Ô¨Årst show 0 ‚â§Œ∂h(s, a), ‚àÄs, a, h ‚ààS √ó A √ó [H]."
REFERENCES,0.2701100762066046,"Indeed, if ¬ØQh(s, a) ‚â§0, then by deÔ¨Ånition bQh(s, a) = 0 and in this case Œ∂h(s, a) :=
(Th bVh+1)(s, a) ‚àíbQh(s, a) = (Th bVh+1)(s, a) ‚â•0; if ¬ØQh(s, a) > 0, then bQh(s, a) ‚â§¬ØQh(s, a)
and"
REFERENCES,0.270956816257409,"Œ∂h(s, a) :=(Th bVh+1)(s, a) ‚àíbQh(s, a) ‚â•(Th bVh+1)(s, a) ‚àí¬ØQh(s, a)"
REFERENCES,0.2718035563082134,"=(Th bVh+1)(s, a) ‚àí(bTh bVh+1)(s, a) + Œìh(s, a) ‚â•0."
REFERENCES,0.2726502963590178,"Step2: next we show Œ∂h(s, a) ‚â§2Œìh(s, a), ‚àÄs, a, h ‚ààS √ó A √ó [H]."
REFERENCES,0.2734970364098222,"Indeed, we have bQh(s, a) = max( ¬ØQh(s, a), 0) and this is because: ¬ØQh(x, a) = (bTh bVh+1)(x, a) ‚àí
Œìh(x, a) ‚â§(Th bVh+1)(x, a) ‚â§H ‚àíh + 1. Therefore, in this case we have:"
REFERENCES,0.2743437764606266,"Œ∂h(s, a) :=(Th bVh+1)(s, a) ‚àíbQh(s, a) ‚â§(Th bVh+1)(s, a) ‚àí¬ØQh(s, a)"
REFERENCES,0.275190516511431,"=(Th bVh+1)(s, a) ‚àí(bTh bVh+1)(s, a) + Œìh(s, a) ‚â§2 ¬∑ Œìh(s, a)."
REFERENCES,0.2760372565622354,"For the last statement, denote F := {0 ‚â§Œ∂h(s, a) ‚â§2Œìh(s, a), ‚àÄs, a, h ‚ààS √ó A √ó [H]}. Note
conditional on F, then by equation 8, V œÄ
1 (s) ‚àíV bœÄ
1 (s) ‚â§PH
h=1 2 ¬∑ EœÄ[Œìh(sh, ah) | s1 = s] holds for
any policy œÄ almost surely. Therefore, P """
REFERENCES,0.2768839966130398,"‚àÄœÄ, V œÄ
1 (s) ‚àíV bœÄ
1 (s) ‚â§ H
X"
REFERENCES,0.2777307366638442,"h=1
2 ¬∑ EœÄ[Œìh(sh, ah) | s1 = s]. # =P """
REFERENCES,0.2785774767146486,"‚àÄœÄ, V œÄ
1 (s) ‚àíV bœÄ
1 (s) ‚â§ H
X"
REFERENCES,0.279424216765453,"h=1
2 ¬∑ EœÄ[Œìh(sh, ah) | s1 = s] F #"
REFERENCES,0.2802709568162574,"¬∑ P[F] +P """
REFERENCES,0.28111769686706184,"‚àÄœÄ, V œÄ
1 (s) ‚àíV bœÄ
1 (s) ‚â§ H
X"
REFERENCES,0.2819644369178662,"h=1
2 ¬∑ EœÄ[Œìh(sh, ah) | s1 = s] Fc
#"
REFERENCES,0.28281117696867064,"¬∑ P[Fc] ‚â•P """
REFERENCES,0.283657917019475,"‚àÄœÄ, V œÄ
1 (s) ‚àíV bœÄ
1 (s) ‚â§ H
X"
REFERENCES,0.28450465707027944,"h=1
2 ¬∑ EœÄ[Œìh(sh, ah) | s1 = s] F #"
REFERENCES,0.2853513971210838,"¬∑ P[F] ‚â•1 ¬∑ P[F] ‚â•1 ‚àíŒ¥,"
REFERENCES,0.28619813717188824,which Ô¨Ånishes the proof.
REFERENCES,0.2870448772226926,"C.2
BOUNDING
(Th bVh+1)(s, a) ‚àí(bTh bVh+1)(s, a)
."
REFERENCES,0.28789161727349705,"By Lemma C.1, it remains to bound |(Th bVh+1)(s, a)‚àí(bTh bVh+1)(s, a)|. Suppose wh is the coefÔ¨Åcient
corresponding to the Th bVh+1 (such wh exists by Lemma H.9), i.e. Th bVh+1 = œÜ‚ä§wh, and recall"
REFERENCES,0.2887383573243014,Published as a conference paper at ICLR 2022
REFERENCES,0.28958509737510585,"(bTh bVh+1)(s, a) = œÜ(s, a)‚ä§bwh, then:"
REFERENCES,0.2904318374259102,"
Th bVh+1

(s, a) ‚àí

bTh bVh+1

(s, a) = œÜ(s, a)‚ä§(wh ‚àíbwh)"
REFERENCES,0.29127857747671465,"=œÜ(s, a)‚ä§wh ‚àíœÜ(s, a)‚ä§bŒõ‚àí1
h K
X"
REFERENCES,0.292125317527519,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

rœÑ
h + bVh+1
 
sœÑ
h+1

/bœÉ2
h(sœÑ
h, aœÑ
h) !"
REFERENCES,0.29297205757832345,"= œÜ(s, a)‚ä§wh ‚àíœÜ(s, a)‚ä§bŒõ‚àí1
h K
X"
REFERENCES,0.2938187976291279,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

Th bVh+1

(sœÑ
h, aœÑ
h) /bœÉ2
h(sœÑ
h, aœÑ
h) !"
REFERENCES,0.29466553767993225,"|
{z
}
(i)"
REFERENCES,0.2955122777307367,"+ œÜ(s, a)‚ä§bŒõ‚àí1
h K
X"
REFERENCES,0.29635901778154106,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

rœÑ
h + bVh+1
 
sœÑ
h+1

‚àí

Th bVh+1

(sœÑ
h, aœÑ
h)

/bœÉ2
h(sœÑ
h, aœÑ
h) !"
REFERENCES,0.2972057578323455,"|
{z
}
(ii) . (9)"
REFERENCES,0.29805249788314986,The term (i) is dealt by the following lemma.
REFERENCES,0.2988992379339543,"Lemma C.2. Recall Œ∫ in Assumption 2.2. Suppose K ‚â•max

512H4/Œ∫2 log
  2d"
REFERENCES,0.29974597798475866,"Œ¥

, 4ŒªH2/Œ∫
	
,
then with probability 1 ‚àíŒ¥, for all s, a, h ‚ààS √ó A √ó [H]"
REFERENCES,0.3005927180355631,"œÜ(s, a)‚ä§wh ‚àíœÜ(s, a)‚ä§bŒõ‚àí1
h K
X"
REFERENCES,0.30143945808636746,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

Th bVh+1

(sœÑ
h, aœÑ
h) /bœÉ2(sœÑ
h, aœÑ
h)"
REFERENCES,0.3022861981371719,! ‚â§2ŒªH3‚àö
REFERENCES,0.30313293818797626,"d/Œ∫
K
."
REFERENCES,0.3039796782387807,"Proof. Recall Th bVh+1 = œÜ‚ä§wh and apply Lemma H.6, we obtain with probability 1 ‚àíŒ¥, for all
s, a, h ‚ààS √ó A √ó [H],"
REFERENCES,0.3048264182895851,"œÜ(s, a)‚ä§wh ‚àíœÜ(s, a)‚ä§bŒõ‚àí1
h K
X"
REFERENCES,0.3056731583403895,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

Th bVh+1

(sœÑ
h, aœÑ
h) /bœÉ2(sœÑ
h, aœÑ
h) !"
REFERENCES,0.3065198983911939,"=œÜ(s, a)‚ä§wh ‚àíœÜ(s, a)‚ä§bŒõ‚àí1
h K
X"
REFERENCES,0.3073666384419983,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑ œÜ(sœÑ
h, aœÑ
h)‚ä§wh/bœÉ2(sœÑ
h, aœÑ
h) !"
REFERENCES,0.3082133784928027,"=œÜ(s, a)‚ä§wh ‚àíœÜ(s, a)‚ä§bŒõ‚àí1
h

bŒõh ‚àíŒªI

wh = Œª ¬∑ œÜ(s, a)‚ä§bŒõ‚àí1
h wh"
REFERENCES,0.3090601185436071,"‚â§Œª ‚à•œÜ(s, a)‚à•bŒõ‚àí1
h ¬∑ ‚à•wh‚à•bŒõ‚àí1
h
‚â§Œª"
REFERENCES,0.3099068585944115,"K ‚à•œÜ(s, a)‚à•(ÀúŒõp
h)‚àí1 ¬∑ ‚à•wh‚à•(ÀúŒõp
h)‚àí1 ‚â§Œª K 1 ¬∑"
REFERENCES,0.3107535986452159,"r(ÀúŒõp
h)‚àí1
 ¬∑ 2H
‚àö d ¬∑"
REFERENCES,0.31160033869602033,"r(ÀúŒõp
h)‚àí1"
REFERENCES,0.3124470787468247,"where ÀúŒõp
h := E¬µ,h

bœÉh(s, a)‚àí2œÜ(s, a)œÜ(s, a)‚ä§
and the second inequality is by Lemma H.6 (with
œÜ‚Ä≤ = œÜ/bœÉh and ‚à•œÜ/bœÉh‚à•‚â§‚à•œÜ‚à•‚â§1 := C) and the third inequality uses
‚àö"
REFERENCES,0.31329381879762913,"a‚ä§¬∑ A ¬∑ a ‚â§
p"
REFERENCES,0.3141405588484335,"‚à•a‚à•2 ‚à•A‚à•2 ‚à•a‚à•2 = ‚à•a‚à•2
p"
REFERENCES,0.31498729889923793,"‚à•A‚à•2 with a to be either œÜ or wh.
Moreover, Œªmin(ÀúŒõp
h) ‚â•"
REFERENCES,0.31583403895004236,"Œ∫/ maxh,s,a bœÉh(s, a)2 ‚â•Œ∫/H2 implies
(ÀúŒõp
h)‚àí1 ‚â§H2/Œ∫, therefore for all s, a, h ‚ààS √ó A √ó [H],
with probability 1 ‚àíŒ¥"
REFERENCES,0.31668077900084673,"œÜ(s, a)‚ä§wh ‚àíœÜ(s, a)‚ä§bŒõ‚àí1
h K
X"
REFERENCES,0.31752751905165116,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

Th bVh+1

(sœÑ
h, aœÑ
h) /bœÉ2(sœÑ
h, aœÑ
h)"
REFERENCES,0.31837425910245554,! ‚â§2ŒªH3‚àö
REFERENCES,0.31922099915325997,"d/Œ∫
K
."
REFERENCES,0.32006773920406434,Published as a conference paper at ICLR 2022
REFERENCES,0.32091447925486877,"For term (ii), denote: xœÑ = œÜ(sœÑ
h,aœÑ
h)
bœÉ(sœÑ
h,aœÑ
h),
Œ∑œÑ =

rœÑ
h + bVh+1
 
sœÑ
h+1

‚àí

Th bVh+1

(sœÑ
h, aœÑ
h)

/bœÉ(sœÑ
h, aœÑ
h),
then by Cauchy inequality it follows"
REFERENCES,0.32176121930567314,"œÜ(s, a)‚ä§bŒõ‚àí1
h K
X"
REFERENCES,0.32260795935647757,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

rœÑ
h + bVh+1
 
sœÑ
h+1

‚àí

Th bVh+1

(sœÑ
h, aœÑ
h)

/bœÉ2
h(sœÑ
h, aœÑ
h) ! ‚â§
q"
REFERENCES,0.32345469940728194,"œÜ(s, a)‚ä§bŒõ‚àí1
h œÜ(s, a) ¬∑ || K
X"
REFERENCES,0.32430143945808637,"œÑ=1
xœÑŒ∑œÑ||bŒõ‚àí1
h
(10)"
REFERENCES,0.32514817950889074,"C.2.1
ANALYZING THE TERM
q"
REFERENCES,0.3259949195596952,"œÜ(s, a)bŒõ‚àí1
h œÜ(s, a)"
REFERENCES,0.3268416596104996,"Recall (in Theorem 3.2) the estimated bŒõh = PK
œÑ=1 œÜ (sœÑ
h, aœÑ
h) œÜ (sœÑ
h, aœÑ
h)‚ä§/bœÉ2(sœÑ
h, aœÑ
h) + Œª ¬∑ I and
Œõh = PK
œÑ=1 œÜ(sœÑ
h, aœÑ
h)‚ä§œÜ(sœÑ
h, aœÑ
h)/œÉ2
bVh+1(sœÑ
h, aœÑ
h) + ŒªI. Then we have the following lemma to"
REFERENCES,0.327688399661304,"control the term
q"
REFERENCES,0.3285351397121084,"œÜ(s, a)bŒõ‚àí1
h œÜ(s, a)."
REFERENCES,0.3293818797629128,"Lemma C.3. Denote the quantities C1 = max{2Œª, 128 log(2d/Œ¥), 128H4 log(2d/Œ¥)/Œ∫2} and
C2 = max{
Œª2
Œ∫ log((Œª+K)H/ŒªŒ¥), 962H12d log((Œª + K)H/ŒªŒ¥)/Œ∫5}. Suppose the number of episode
K satisÔ¨Åes K > max{C1, C2}, then with probability 1 ‚àíŒ¥, q"
REFERENCES,0.3302286198137172,"œÜ(s, a)bŒõ‚àí1
h œÜ(s, a) ‚â§2
q"
REFERENCES,0.3310753598645216,"œÜ(s, a)Œõ‚àí1
h œÜ(s, a),
‚àÄs, a ‚ààS √ó A."
REFERENCES,0.331922099915326,"Proof of Lemma C.3. By deÔ¨Ånition
q"
REFERENCES,0.3327688399661304,"œÜ(s, a)bŒõ‚àí1
h œÜ(s, a) = ‚à•œÜ(s, a)‚à•bŒõ‚àí1
h . Then denote"
REFERENCES,0.3336155800169348,"bŒõ‚Ä≤
h = 1"
REFERENCES,0.3344623200677392,"K
bŒõh,
Œõ‚Ä≤
h = 1 K Œõh,"
REFERENCES,0.3353090601185436,"where Œõh = PK
œÑ=1 œÜ(sœÑ
h, aœÑ
h)‚ä§œÜ(sœÑ
h, aœÑ
h)/œÉ2
bVh+1(sœÑ
h, aœÑ
h) + ŒªI.
Under the condition of K, by
Lemma C.7, with probability 1 ‚àíŒ¥"
REFERENCES,0.336155800169348,"bŒõ‚Ä≤
h ‚àíŒõ‚Ä≤
h
 ‚â§sup
s,a"
REFERENCES,0.3370025402201524,"œÜ(s, a)œÜ(s, a)‚ä§"
REFERENCES,0.33784928027095684,"bœÉ2
h(s, a)
‚àíœÜ(s, a)œÜ(s, a)‚ä§"
REFERENCES,0.3386960203217612,"œÉ2
bVh+1(s, a) "
REFERENCES,0.33954276037256564,"‚â§sup
s,a "
REFERENCES,0.34038950042337,"bœÉ2
h(s, a) ‚àíœÉ2
bVh+1(s, a)"
REFERENCES,0.34123624047417445,"bœÉ2
h(s, a)œÉ2
bVh+1(s, a)"
REFERENCES,0.3420829805249788,"¬∑ ‚à•œÜ(s, a)‚à•2 ‚â§sup
s,a "
REFERENCES,0.34292972057578325,"bœÉ2
h(s, a) ‚àíœÉ2
bVh+1(s, a) 1 ¬∑ 1 ‚â§12 s H4d"
REFERENCES,0.3437764606265876,"Œ∫K log
(Œª + K)H ŒªŒ¥"
REFERENCES,0.34462320067739205,"
+ 12ŒªH2‚àö"
REFERENCES,0.3454699407281964,"d
Œ∫K
. (11)"
REFERENCES,0.34631668077900085,"Next by Lemma H.5 (with œÜ to be œÜ/œÉbVh+1 and C = 1), it holds with probability 1 ‚àíŒ¥,"
REFERENCES,0.3471634208298052,"Œõ‚Ä≤
h ‚àí

E¬µ,h[œÜ(s, a)œÜ(s, a)‚ä§/œÉ2
bVh+1(s, a)] + Œª K Id"
REFERENCES,0.34801016088060965," ‚â§4
‚àö 2
‚àö K"
REFERENCES,0.3488569009314141,"
log 2d Œ¥"
REFERENCES,0.34970364098221846,"1/2
."
REFERENCES,0.3505503810330229,Published as a conference paper at ICLR 2022
REFERENCES,0.35139712108382726,"Therefore
by
Weyl‚Äôs
spectrum
theorem
and
the
condition
K
>
max{2Œª, 128 log(2d/Œ¥), 128H4 log(2d/Œ¥)/Œ∫2}, the above implies"
REFERENCES,0.3522438611346317,"‚à•Œõ‚Ä≤
h‚à•=Œªmax(Œõ‚Ä≤
h) ‚â§Œªmax

E¬µ,h[œÜ(s, a)œÜ(s, a)‚ä§/œÉ2
bVh+1(s, a)]

+ Œª"
REFERENCES,0.35309060118543606,"K + 4
‚àö 2
‚àö K"
REFERENCES,0.3539373412362405,"
log 2d Œ¥ 1/2"
REFERENCES,0.35478408128704486,"=
E¬µ,h[œÜ(s, a)œÜ(s, a)‚ä§/œÉ2
bVh+1(s, a)]

2 + Œª"
REFERENCES,0.3556308213378493,"K + 4
‚àö 2
‚àö K"
REFERENCES,0.35647756138865366,"
log 2d Œ¥ 1/2"
REFERENCES,0.3573243014394581,"‚â§‚à•œÜ(s, a)‚à•2 + Œª"
REFERENCES,0.35817104149026247,"K + 4
‚àö 2
‚àö K"
REFERENCES,0.3590177815410669,"
log 2d Œ¥"
REFERENCES,0.3598645215918713,"1/2
‚â§1 + Œª"
REFERENCES,0.3607112616426757,"K + 4
‚àö 2
‚àö K"
REFERENCES,0.3615580016934801,"
log 2d Œ¥"
REFERENCES,0.3624047417442845,"1/2
‚â§2,"
REFERENCES,0.3632514817950889,"Œªmin(Œõ‚Ä≤
h) ‚â•Œªmin

E¬µ,h[œÜ(s, a)œÜ(s, a)‚ä§/œÉ2
bVh+1(s, a)]

+ Œª"
REFERENCES,0.3640982218458933,"K ‚àí4
‚àö 2
‚àö K"
REFERENCES,0.36494496189669773,"
log 2d Œ¥ 1/2"
REFERENCES,0.3657917019475021,"‚â•Œªmin

E¬µ,h[œÜ(s, a)œÜ(s, a)‚ä§/œÉ2
bVh+1(s, a)]

‚àí4
‚àö 2
‚àö K"
REFERENCES,0.36663844199830653,"
log 2d Œ¥ 1/2 ‚â•Œ∫"
REFERENCES,0.3674851820491109,"H2 ‚àí4
‚àö 2
‚àö K"
REFERENCES,0.36833192209991533,"
log 2d Œ¥"
REFERENCES,0.3691786621507197,"1/2
‚â•
Œ∫
2H2 ."
REFERENCES,0.37002540220152413,"Hence with probability 1 ‚àíŒ¥, ‚à•Œõ‚Ä≤
h‚à•‚â§2 and
Œõ‚Ä≤‚àí1
h
 = 1/Œªmin(Œõ‚Ä≤
h) ‚â§2H2/Œ∫. Similarly, one can"
REFERENCES,0.37087214225232856,"show
bŒõ‚Ä≤‚àí1
h
 ‚â§2H2/Œ∫ with high probability."
REFERENCES,0.37171888230313294,"Now apply Lemma H.4 to bŒõ‚Ä≤
h and Œõ‚Ä≤
h and a union bound, we obtain with probability 1‚àíŒ¥, for all s, a"
REFERENCES,0.37256562235393736,"‚à•œÜ(s, a)‚à•bŒõ‚Ä≤‚àí1
h
‚â§ "" 1 +"
REFERENCES,0.37341236240474174,"rŒõ‚Ä≤‚àí1
h
 ‚à•Œõ‚Ä≤
h‚à•¬∑
bŒõ‚Ä≤‚àí1
h
 ¬∑
bŒõ‚Ä≤
h ‚àíŒõ‚Ä≤
h #"
REFERENCES,0.37425910245554617,"¬∑ ‚à•œÜ(s, a)‚à•Œõ‚Ä≤‚àí1
h ‚â§ "" 1 + r 2H2"
REFERENCES,0.37510584250635054,"Œ∫
¬∑ 1 ¬∑ 2H2"
REFERENCES,0.37595258255715497,"Œ∫
¬∑
bŒõ‚Ä≤
h ‚àíŒõ‚Ä≤
h #"
REFERENCES,0.37679932260795934,"¬∑ ‚à•œÜ(s, a)‚à•Œõ‚Ä≤‚àí1
h ‚â§ Ô£Æ Ô£ØÔ£∞1 +"
REFERENCES,0.37764606265876377,"v
u
u
t48H4 Œ∫2 s H4d"
REFERENCES,0.37849280270956814,"Œ∫K log
(Œª + K)H ŒªŒ¥"
REFERENCES,0.3793395427603726,"
+ ŒªH2‚àö d
Œ∫K !Ô£π"
REFERENCES,0.38018628281117695,"Ô£∫Ô£ª¬∑ ‚à•œÜ(s, a)‚à•Œõ‚Ä≤‚àí1
h ‚â§ Ô£Æ Ô£ØÔ£∞1 +"
REFERENCES,0.3810330228619814,"v
u
u
t96H4 Œ∫2 s H4d"
REFERENCES,0.38187976291278575,"Œ∫K log
(Œª + K)H ŒªŒ¥ 
Ô£π"
REFERENCES,0.3827265029635902,"Ô£∫Ô£ª¬∑ ‚à•œÜ(s, a)‚à•Œõ‚Ä≤‚àí1
h
‚â§2 ‚à•œÜ(s, a)‚à•Œõ‚Ä≤‚àí1
h"
REFERENCES,0.3835732430143946,"where the third inequality uses equation 11 and the last and the second last inequality use
K > max{
Œª2
Œ∫ log((Œª+K)H/ŒªŒ¥), 962H12d log((Œª + K)H/ŒªŒ¥)/Œ∫5}. Note the above is equivalent to
q"
REFERENCES,0.384419983065199,"œÜ(s, a)bŒõ‚àí1
h œÜ(s, a) ‚â§2
q"
REFERENCES,0.3852667231160034,"œÜ(s, a)Œõ‚àí1
h œÜ(s, a) by multiplying 1/
‚àö"
REFERENCES,0.3861134631668078,K on both sides.
REFERENCES,0.3869602032176122,"C.2.2
ANALYZING THE TERM ||PK
œÑ=1 xœÑŒ∑œÑ||bŒõ‚àí1"
REFERENCES,0.3878069432684166,"Lemma C.4. Recall xœÑ = œÜ(sœÑ
h,aœÑ
h)
bœÉ(sœÑ
h,aœÑ
h) and Œ∑œÑ =

rœÑ
h + bVh+1
 
sœÑ
h+1

‚àí

Th bVh+1

(sœÑ
h, aœÑ
h)

/bœÉ(sœÑ
h, aœÑ
h)."
REFERENCES,0.388653683319221,"Let CH,d,Œ∫,K := 36
r H4d3"
REFERENCES,0.3895004233700254,"Œ∫
log

(Œª+K)2KdH2"
REFERENCES,0.3903471634208298,"ŒªŒ¥

+ 12Œª H2‚àö"
REFERENCES,0.3911939034716342,"d
Œ∫
and denote"
REFERENCES,0.3920406435224386,"Œæ :=
sup
V ‚àà[0,H], s‚Ä≤‚àºPh(s,a), h‚àà[H]"
REFERENCES,0.392887383573243,"rh + V (s‚Ä≤) ‚àí(ThV ) (s, a)"
REFERENCES,0.3937341236240474,"œÉV (s, a) ."
REFERENCES,0.39458086367485184,Published as a conference paper at ICLR 2022
REFERENCES,0.3954276037256562,"If K ‚â•4C2
H,d,Œ∫,K and K ‚â•eO(H6d/Œ∫), then with probability 1 ‚àíŒ¥,  K
X"
REFERENCES,0.39627434377646065,"œÑ=1
xœÑŒ∑œÑ"
REFERENCES,0.397121083827265,"bŒõ‚àí1
‚â§16 s"
REFERENCES,0.39796782387806945,"d log

1 + K Œªd"
REFERENCES,0.3988145639288738,"
¬∑ log
4K2 Œ¥"
REFERENCES,0.39966130397967825,"
+ 4Œæ log
4K2 Œ¥"
REFERENCES,0.4005080440304826,"
‚â§eO max
‚àö"
REFERENCES,0.40135478408128705,"d, Œæ
	
,"
REFERENCES,0.4022015241320914,where eO absorbs the constants and Polylog terms.
REFERENCES,0.40304826418289585,"Proof of Lemma C.4. By construction, we have ‚à•xœÑ‚à•‚â§‚à•œÜ/bœÉ‚à•‚â§1 and by Lemma C.7, with
probability 1 ‚àíŒ¥/3,"
REFERENCES,0.4038950042337002,"œÉbVh+1 ‚àíbœÉh

‚àû= sup
s,a"
REFERENCES,0.40474174428450466,"œÉ2
bVh+1(s, a) ‚àíbœÉ2
h(s, a)

œÉbVh+1(s, a) + bœÉh(s, a)

‚â§1 2"
REFERENCES,0.4055884843353091,"œÉ2
bVh+1 ‚àíbœÉ2
h

‚àû‚â§CH,d,Œ∫,K r"
K,0.40643522438611346,"1
K"
K,0.4072819644369179,"Therefore, when K ‚â•4C2
H,d,Œ∫,K, CH,d,Œ∫,K
q"
K,0.40812870448772226,"1
K ‚â§1/2 ‚â§œÉbVh+1(sœÑ
h, aœÑ
h)/2 and hence"
K,0.4089754445385267,|Œ∑œÑ| ‚â§ 
K,0.40982218458933106,"rœÑ
h + bVh+1
 
sœÑ
h+1

‚àí

Th bVh+1

(sœÑ
h, aœÑ
h)"
K,0.4106689246401355,"œÉbVh+1(sœÑ
h, aœÑ
h) ‚àíCH,d,Œ∫,K K1/2 ‚â§2 "
K,0.41151566469093986,"rœÑ
h + bVh+1
 
sœÑ
h+1

‚àí

Th bVh+1

(sœÑ
h, aœÑ
h)"
K,0.4123624047417443,"œÉbVh+1(sœÑ
h, aœÑ
h) "
K,0.41320914479254867,"‚â§2
sup
V ‚àà[0,H], s‚Ä≤‚àºPh(s,a)"
K,0.4140558848433531,"r + V (s‚Ä≤) ‚àí(ThV ) (s, a)"
K,0.41490262489415747,"œÉV (s, a) := Œæ."
K,0.4157493649449619,"Next, for a Ô¨Åxed function V , we deÔ¨Åne the Bellman error as Bh(V )(s, a) = rh+V (s‚Ä≤)‚àí(ThV )(s, a),
then"
K,0.4165961049957663,"Var [Œ∑œÑ|FœÑ‚àí1] =
Var
h
rœÑ
h + bVh+1
 
sœÑ
h+1

‚àí

Th bVh+1

(sœÑ
h, aœÑ
h)
FœÑ‚àí1
i"
K,0.4174428450465707,"bœÉ2(sœÑ
h, aœÑ
h)"
K,0.4182895850973751,"=
Var
h
Bh bVh+1(sœÑ
h, aœÑ
h) ‚àíBhV ‚ãÜ
h+1(sœÑ
h, aœÑ
h) + BhV ‚ãÜ
h+1(sœÑ
h, aœÑ
h)
FœÑ‚àí1
i"
K,0.4191363251481795,"bœÉ2(sœÑ
h, aœÑ
h)"
K,0.41998306519898393,"‚â§
Var

BhV ‚ãÜ
h+1(sœÑ
h, aœÑ
h)
FœÑ‚àí1

+ 8H
Bh bVh+1 ‚àíBhV ‚ãÜ
h+1

‚àû
bœÉ2(sœÑ
h, aœÑ
h)"
K,0.4208298052497883,"‚â§
Var

BhV ‚ãÜ
h+1(sœÑ
h, aœÑ
h)
FœÑ‚àí1

+ 16H
bVh+1 ‚àíV ‚ãÜ
h+1

‚àû
bœÉ2(sœÑ
h, aœÑ
h)"
K,0.42167654530059273,"‚â§
Var

BhV ‚ãÜ
h+1(sœÑ
h, aœÑ
h)
FœÑ‚àí1

+ eO( H3‚àö d
‚àö Œ∫K )"
K,0.4225232853513971,"bœÉ2(sœÑ
h, aœÑ
h)"
K,0.42337002540220153,"=
Var

BhV ‚ãÜ
h+1(sœÑ
h, aœÑ
h)
sœÑ
h, aœÑ
h

+ eO( H3‚àö d
‚àö Œ∫K )"
K,0.4242167654530059,"bœÉ2(sœÑ
h, aœÑ
h)"
K,0.42506350550381033,"=
VarV ‚ãÜ
h+1(sœÑ
h, aœÑ
h) + eO( H3‚àö d
‚àö Œ∫K )"
K,0.4259102455546147,"bœÉ2(sœÑ
h, aœÑ
h)
‚â§
2VarV ‚ãÜ
h+1(sœÑ
h, aœÑ
h) + eO( H3‚àö d
‚àö Œ∫K )"
K,0.42675698560541914,"œÉ‚ãÜ2(sœÑ
h, aœÑ
h)
‚â§2 +
eO( H3‚àö d
‚àö Œ∫K )"
K,0.42760372565622357,"œÉ‚ãÜ2(sœÑ
h, aœÑ
h)"
K,0.42845046570702794,‚â§eO(1)
K,0.42929720575783237,"where the Ô¨Årst inequality is by Lemma H.11, the second inequality is by Th is non-expansive,
the third inequality is by Lemma C.8, the next equality is by Markovian property, and the fourth
inequality is by Lemma C.7 and Lemma C.10. The Ô¨Åfth inequality uses deÔ¨Ånition œÉh,V (s, a)2 :=
max{1, VarPh(V )(s, a)} and the last one is by condition K ‚â•eO(H6d/Œ∫) and œÉh,V ‚ãÜ(s, a)2 :=
max{1, VarPh(V ‚ãÜ)(s, a)} ‚â•1. Thus, by Bernstein inequality for self-normalized martingale"
K,0.43014394580863674,Published as a conference paper at ICLR 2022
K,0.43099068585944117,"(Lemma H.3),9 with probability 1 ‚àíŒ¥, K
X"
K,0.43183742591024554,"œÑ=1
xœÑŒ∑œÑ"
K,0.43268416596104997,"bŒõ‚àí1
‚â§eO s"
K,0.43353090601185434,"d log

1 + K Œªd"
K,0.4343776460626588,"
¬∑ log
4K2 Œ¥ !"
K,0.43522438611346315,"+ 4Œæ log
4K2 Œ¥"
K,0.4360711261642676,"
‚â§eO max
‚àö d, Œæ"
K,0.43691786621507195,where eO absorbs the constants and Polylog terms.
K,0.4377646062658764,"Recall M1, M2, M3, M4 in List A. Based on the above results, we have the following key lemma:"
K,0.4386113463166808,"Lemma C.5. Assume K > max{M1, M2, M3, M4}, for any 0 < Œª < Œ∫, suppose
‚àö"
K,0.4394580863674852,"d > Œæ,"
K,0.4403048264182896,"where Œæ := supV ‚àà[0,H], s‚Ä≤‚àºPh(s,a), h‚àà[H]"
K,0.441151566469094,"rh+V (s‚Ä≤)‚àí(ThV )(s,a)"
K,0.4419983065198984,"œÉV (s,a)"
K,0.4428450465707028,". Then with probability 1 ‚àíŒ¥, for all"
K,0.4436917866215072,"h, s, a ‚àà[H] √ó S √ó A,"
K,0.4445385266723116,"(Th bVh+1 ‚àíbTh bVh+1)(s, a)
 ‚â§eO
‚àö d
q"
K,0.445385266723116,"œÜ(s, a)Œõ‚àí1
h œÜ(s, a)

+ 2H3‚àö d
K
,"
K,0.4462320067739204,"where Œõh = PK
œÑ=1 œÜ(sœÑ
h, aœÑ
h)‚ä§œÜ(sœÑ
h, aœÑ
h)/œÉ2
bVh+1(sœÑ
h, aœÑ
h)+ŒªI and eO absorbs the universal constants
and Polylog terms."
K,0.4470787468247248,"Proof of Lemma C.5. Combing equation 9, Lemma C.2, equation 10, Lemma C.3 and C.4 and a
union bound to Ô¨Ånish the proof."
K,0.4479254868755292,"C.3
PROOF OF THE FIRST PART OF THEOREM 3.2"
K,0.4487722269263336,"Theorem C.6 (First part of Theorem 3.2). Let K be the number of episodes. Suppose
‚àö"
K,0.44961896697713805,"d > Œæ, where"
K,0.4504657070279424,"Œæ := supV ‚àà[0,H], s‚Ä≤‚àºPh(s,a), h‚àà[H]"
K,0.45131244707874685,"rh+V (s‚Ä≤)‚àí(ThV )(s,a)"
K,0.4521591871295512,"œÉV (s,a)"
K,0.45300592718035565,"and K > max{M1, M2, M3, M4}10."
K,0.45385266723116,"Then for any 0 < Œª < Œ∫, with probability 1 ‚àíŒ¥, for all policy œÄ simultaneously, the output bœÄ of
Algorithm 1 satisÔ¨Åes"
K,0.45469940728196445,"vœÄ ‚àívbœÄ ‚â§eO ‚àö d ¬∑ H
X"
K,0.4555461473327688,"h=1
EœÄ
h 
œÜ(¬∑, ¬∑)‚ä§Œõ‚àí1
h œÜ(¬∑, ¬∑)
1/2i!"
K,0.45639288738357325,"+ 2H4‚àö d
K"
K,0.4572396274343776,"where Œõh = PK
œÑ=1
œÜ(sœÑ
h,aœÑ
h)¬∑œÜ(sœÑ
h,aœÑ
h)‚ä§"
K,0.45808636748518206,"œÉ2
b
Vh+1(sœÑ
h,aœÑ
h)
+ ŒªId and eO absorbs the universal constants and the Polylog"
K,0.45893310753598643,terms.
K,0.45977984758679086,"Proof of Theorem C.6. Combing Lemma C.1 and Lemma C.5, we directly have with probability
1 ‚àíŒ¥, for all policy œÄ simultaneously,"
K,0.46062658763759523,"V œÄ
1 (s) ‚àíV bœÄ
1 (s) ‚â§eO ‚àö d ¬∑ H
X"
K,0.46147332768839966,"h=1
EœÄ
h 
œÜ(¬∑, ¬∑)‚ä§Œõ‚àí1
h œÜ(¬∑, ¬∑)
1/2s1 = s
i!"
K,0.4623200677392041,+ 2H4‚àö
K,0.46316680779000846,"d
K
,
(12)"
K,0.4640135478408129,now take the initial distribution d1 on both sides to get the stated result.
K,0.46486028789161726,"C.4
TWO INTERMEDIATE RESULTS"
K,0.4657070279424217,The next two lemmas provide intermediate results in Ô¨Ånishing the whole proofs.
K,0.46655376799322607,"9To be rigorous, Lemma H.3 needs to be modiÔ¨Åed since the absolute value bound and the variance bound
here are in the high probability sense. However, this will not affect the validity of the result as the weaker version
can also be obtained (see Chung and Lu (2006) and a related discussion in Yin et al. (2021) Remark E.7.) To
make the proof more readable, we do not include them here to avoid over-technicality.
10The deÔ¨Ånition of Mi is in List A."
K,0.4674005080440305,Published as a conference paper at ICLR 2022
K,0.46824724809483487,"C.4.1
BOUNDING THE VARIANCE"
K,0.4690939881456393,"Lemma C.7. Recall the deÔ¨Ånition bœÉh(¬∑, ¬∑)2 = max{1, d
VarPh bVh+1(¬∑, ¬∑)} + 1 and œÉbVh+1(¬∑, ¬∑)2 :="
K,0.46994072819644367,"max{1, VarPh bVh+1(¬∑, ¬∑)} + 1.
Moreover,
d
Varh bVh+1

(¬∑, ¬∑)
=

œÜ(¬∑, ¬∑), ¬ØŒ≤h

[0,(H‚àíh+1)2] ‚àí

œÜ(¬∑, ¬∑), ¬ØŒ∏h"
K,0.4707874682472481,"[0,H‚àíh+1]
2
(where
¬ØŒ≤h
and
¬ØŒ∏h
are deÔ¨Åned in Algorithm 1).
Let K
‚â•
max

512(1/Œ∫)2 log
  4Hd"
K,0.47163420829805247,"Œ¥

, 4Œª/Œ∫
	
, then with probability 1 ‚àíŒ¥,"
K,0.4724809483488569,"sup
h
||bœÉ2
h ‚àíœÉ2
bVh+1||‚àû‚â§36 s H4d3"
K,0.47332768839966133,"Œ∫K log
(Œª + K)2KdH2 ŒªŒ¥"
K,0.4741744284504657,"
+ 12ŒªH2‚àö"
K,0.47502116850127013,"d
Œ∫K
."
K,0.4758679085520745,"Proof. Step1: we Ô¨Årst show for all h, s, a ‚àà[H] √ó S √ó A, with probability 1 ‚àíŒ¥"
K,0.47671464860287893,"‚ü®œÜ(s, a), ¬ØŒ≤h‚ü©[0,(H‚àíh+1)2] ‚àíPh(bVh+1)2(s, a)
 ‚â§12 s H4d3"
K,0.4775613886536833,"Œ∫K log
(Œª + K)2KdH2 ŒªŒ¥"
K,0.47840812870448773,"
+4ŒªH2‚àö"
K,0.4792548687552921,"d
Œ∫K
."
K,0.48010160880609654,"Proof of Step1. Note
‚ü®œÜ(s, a), ¬ØŒ≤h‚ü©[0,(H‚àíh+1)2] ‚àíPh(bVh+1)2(s, a)
 ‚â§
‚ü®œÜ(s, a), ¬ØŒ≤h‚ü©‚àíPh(bVh+1)2(s, a) ="
K,0.4809483488569009,"œÜ(s, a)‚ä§¬ØŒ£‚àí1
h K
X"
K,0.48179508890770534,"œÑ=1
œÜ(¬ØsœÑ
h, ¬ØaœÑ
h) ¬∑ bVh+1(¬ØsœÑ
h+1)2 ‚àíPh(bVh+1)2(s, a)  ="
K,0.4826418289585097,"œÜ(s, a)‚ä§¬ØŒ£‚àí1
h K
X"
K,0.48348856900931414,"œÑ=1
œÜ(¬ØsœÑ
h, ¬ØaœÑ
h) ¬∑ bVh+1(¬ØsœÑ
h+1)2 ‚àíœÜ(s, a)‚ä§
Z"
K,0.48433530906011857,"S
(bVh+1)2(s‚Ä≤)dŒΩh(s‚Ä≤)  ="
K,0.48518204911092294,"œÜ(s, a)‚ä§¬ØŒ£‚àí1
h K
X"
K,0.48602878916172737,"œÑ=1
œÜ(¬ØsœÑ
h, ¬ØaœÑ
h) ¬∑ bVh+1(¬ØsœÑ
h+1)2 ‚àíœÜ(s, a)‚ä§¬ØŒ£‚àí1
h ( K
X"
K,0.48687552921253174,"œÑ=1
œÜ(¬ØsœÑ
h, ¬ØaœÑ
h)œÜ(¬ØsœÑ
h, ¬ØaœÑ
h)‚ä§+ ŒªI)
Z"
K,0.4877222692633362,"S
(bVh+1)2(s‚Ä≤)dŒΩh(s‚Ä≤)  ‚â§"
K,0.48856900931414055,"œÜ(s, a)‚ä§¬ØŒ£‚àí1
h K
X"
K,0.489415749364945,"œÑ=1
œÜ(¬ØsœÑ
h, ¬ØaœÑ
h) ¬∑

bVh+1(¬ØsœÑ
h+1)2 ‚àíPh(bVh+1)2(¬ØsœÑ
h, ¬ØaœÑ
h)

|
{z
}
1"
K,0.49026248941574935,"+ Œª
œÜ(s, a)‚ä§¬ØŒ£‚àí1
h Z"
K,0.4911092294665538,"S
(bVh+1)2(s‚Ä≤)dŒΩh(s‚Ä≤)

|
{z
}
2"
K,0.49195596951735815,"For 2 , since K ‚â•max

512(1/Œ∫)2 log
  4Hd"
K,0.4928027095681626,"Œ¥

, 4Œª/Œ∫
	
, by Lemma H.6 and a union bound over
h ‚àà[H], with probability 1 ‚àíŒ¥ for all h, s, a ‚àà[H] √ó S √ó A,"
K,0.49364944961896695,"2 ‚â§Œª ‚à•œÜ(s, a)‚à•¬ØŒ£‚àí1
h  Z"
K,0.4944961896697714,"S
(bVh+1)2(s‚Ä≤)dŒΩh(s‚Ä≤)
¬ØŒ£‚àí1
h"
K,0.4953429297205758,"‚â§Œª 2
‚àö"
K,0.4961896697713802,"K
‚à•œÜ(s, a)‚à•(Œ£p
h)‚àí1
2
‚àö K  Z"
K,0.4970364098221846,"S
(bVh+1)2(s‚Ä≤)dŒΩh(s‚Ä≤)

(Œ£p
h)‚àí1 ‚â§4Œª
(Œ£p
h)‚àí1 H2‚àö"
K,0.497883149872989,"d
K
‚â§4ŒªH2‚àö"
K,0.4987298899237934,"d
Œ∫K
. (13)"
K,0.4995766299745978,"For 1 , we have"
K,0.5004233700254022,"1 ‚â§‚à•œÜ(s, a)‚à•¬ØŒ£‚àí1
h  K
X"
K,0.5012701100762066,"œÑ=1
œÜ(¬ØsœÑ
h, ¬ØaœÑ
h) ¬∑

bVh+1(¬ØsœÑ
h+1)2 ‚àíPh(bVh+1)2(¬ØsœÑ
h, ¬ØaœÑ
h)
¬ØŒ£‚àí1
h (14)"
K,0.502116850127011,"Bounding using covering. Note for any Ô¨Åx Vh+1, we can deÔ¨Åne xœÑ = œÜ(¬ØsœÑ
h, ¬ØaœÑ
h) (‚à•œÜ‚à•2 ‚â§1) and
Œ∑œÑ = Vh+1(¬ØsœÑ
h+1)2 ‚àíPh(Vh+1)2(¬ØsœÑ
h, ¬ØaœÑ
h) is H2-subgaussian, by Lemma H.2 (where t = K and
L = 1) with probability 1 ‚àíŒ¥, K
X"
K,0.5029635901778154,"œÑ=1
œÜ(¬ØsœÑ
h, ¬ØaœÑ
h) ¬∑
 
Vh+1(¬ØsœÑ
h+1)2 ‚àíPh(Vh+1)2(¬ØsœÑ
h, ¬ØaœÑ
h)

¬ØŒ£‚àí1
h ‚â§ s"
K,0.5038103302286198,8H4 ¬∑ d
LOG,0.5046570702794242,"2 log
Œª + K ŒªŒ¥ "
LOG,0.5055038103302286,let Nh(œµ) be the minimal œµ-cover (with respect the supremum norm) of Vh := {Vh : Vh(¬∑) =
LOG,0.506350550381033,maxa‚ààA
LOG,0.5071972904318375,"
min{œÜ(s, a)‚ä§Œ∏ ‚àíC1
q"
LOG,0.5080440304826418,"d ¬∑ œÜ(¬∑, ¬∑)‚ä§bŒõ‚àí1
h œÜ(¬∑, ¬∑) ‚àíC2, H ‚àíh + 1}+}

. That is, for any"
LOG,0.5088907705334462,Published as a conference paper at ICLR 2022
LOG,0.5097375105842507,"V ‚ààVh, there exists a value function V ‚Ä≤ ‚ààNh(œµ) such that sups‚ààS |V (s) ‚àíV ‚Ä≤(s)| < œµ. Now by a
union bound, we obtain with probability 1 ‚àíŒ¥"
LOG,0.5105842506350551,"sup
Vh+1‚ààNh+1(œµ)  K
X"
LOG,0.5114309906858594,"œÑ=1
œÜ(¬ØsœÑ
h, ¬ØaœÑ
h) ¬∑
 
Vh+1(¬ØsœÑ
h+1)2 ‚àíPh(Vh+1)2(¬ØsœÑ
h, ¬ØaœÑ
h)

¬ØŒ£‚àí1
h ‚â§ s"
LOG,0.5122777307366638,8H4 ¬∑ d
LOG,0.5131244707874683,"2 log
Œª + K"
LOG,0.5139712108382727,"ŒªŒ¥
|Nh+1(œµ)|
"
LOG,0.514817950889077,"which implies K
X"
LOG,0.5156646909398814,"œÑ=1
œÜ(¬ØsœÑ
h, ¬ØaœÑ
h) ¬∑

bVh+1(¬ØsœÑ
h+1)2 ‚àíPh(bVh+1)2(¬ØsœÑ
h, ¬ØaœÑ
h)
¬ØŒ£‚àí1
h ‚â§ s"
LOG,0.5165114309906859,8H4 ¬∑ d
LOG,0.5173581710414903,"2 log
Œª + K"
LOG,0.5182049110922947,"ŒªŒ¥
|Nh+1(œµ)|

+ 4H2p"
LOG,0.519051651143099,œµ2K2/Œª
LOG,0.5198983911939035,"choosing œµ = d
‚àö"
LOG,0.5207451312447079,"Œª/K, applying Lemma B.3 of Jin et al. (2021b)11 to the covering number Nh+1(œµ)
w.r.t. Vh+1, we can further bound above by ‚â§ s"
LOG,0.5215918712955123,8H4 ¬∑ d3
LOG,0.5224386113463166,"2 log
Œª + K"
LOG,0.5232853513971211,"ŒªŒ¥
2dHK

+ 4H2‚àö d2 ‚â§6 s"
LOG,0.5241320914479255,"H4 ¬∑ d3 log
Œª + K"
LOG,0.5249788314987299,"ŒªŒ¥
2dHK
"
LOG,0.5258255715495342,"Apply a union bound for h ‚àà[H], we have with probability 1 ‚àíŒ¥, for all h ‚àà[H], K
X"
LOG,0.5266723116003387,"œÑ=1
œÜ(¬ØsœÑ
h, ¬ØaœÑ
h) ¬∑

bVh+1(¬ØsœÑ
h+1)2 ‚àíPh(bVh+1)2(¬ØsœÑ
h, ¬ØaœÑ
h)
¬ØŒ£‚àí1
h ‚â§6 s"
LOG,0.5275190516511431,"H4d3 log
(Œª + K)2KdH2 ŒªŒ¥ "
LOG,0.5283657917019475,"(15)
and similar to 2 , with probability 1 ‚àíŒ¥ for all h, s, a ‚àà[H] √ó S √ó A,"
LOG,0.529212531752752,"‚à•œÜ(s, a)‚à•¬ØŒ£‚àí1
h
‚â§2
(Œ£p
h)‚àí11/2
‚àö"
LOG,0.5300592718035563,"K
‚â§
2
‚àö"
LOG,0.5309060118543607,"Œ∫K
.
(16)"
LOG,0.5317527519051651,"Combing equation 13, equation 14, equation 15 and equation 16 we obtain with probability 1 ‚àíŒ¥ for
all h, s, a ‚àà[H] √ó S √ó A,"
LOG,0.5325994919559696,"‚ü®œÜ(s, a), ¬ØŒ≤h‚ü©[0,(H‚àíh+1)2] ‚àíPh(bVh+1)2(s, a)
 ‚â§12 s H4d3"
LOG,0.5334462320067739,"Œ∫K log
(Œª + K)2KdH2 ŒªŒ¥"
LOG,0.5342929720575783,"
+4ŒªH2‚àö"
LOG,0.5351397121083827,"d
Œ∫K
."
LOG,0.5359864521591872,"Step2: we show for all h, s, a ‚àà[H] √ó S √ó A, with probability 1 ‚àíŒ¥"
LOG,0.5368331922099915,"‚ü®œÜ(s, a), ¬ØŒ∏h‚ü©[0,H‚àíh+1] ‚àíPh(bVh+1)(s, a)
 ‚â§12 s H2d3"
LOG,0.5376799322607959,"Œ∫K log
(Œª + K)2KdH2 ŒªŒ¥"
LOG,0.5385266723116003,"
+ 4ŒªH
‚àö"
LOG,0.5393734123624048,"d
Œ∫K . (17)"
LOG,0.5402201524132092,"The proof of Step2 follows nearly the identical way as Step1 except bV 2
h is replaced by bVh."
LOG,0.5410668924640135,"Step3: We prove suph||bœÉ2
h ‚àíœÉ2
bVh||‚àû‚â§36
r H4d3"
LOG,0.541913632514818,"Œ∫K log

(Œª+K)2KdH2"
LOG,0.5427603725656224,"ŒªŒ¥

+ 12Œª H2‚àö"
LOG,0.5436071126164268,"d
Œ∫K ."
LOG,0.5444538526672311,"Proof of Step3. By equation 17,


œÜ(¬∑, ¬∑), ¬ØŒ∏h"
LOG,0.5453005927180355,"[0,H‚àíh+1]
2 ‚àí

Ph(bVh+1)(s, a)
2"
LOG,0.54614733276884,"=
‚ü®œÜ(s, a), ¬ØŒ∏h‚ü©[0,H‚àíh+1] + Ph(bVh+1)(s, a)
 ¬∑
‚ü®œÜ(s, a), ¬ØŒ∏h‚ü©[0,H‚àíh+1] ‚àíPh(bVh+1)(s, a)"
LOG,0.5469940728196444,"‚â§2H ¬∑
‚ü®œÜ(s, a), ¬ØŒ∏h‚ü©[0,H‚àíh+1] ‚àíPh(bVh+1)(s, a)
 ‚â§24 s H4d3"
LOG,0.5478408128704487,"Œ∫K log
(Œª + K)2KdH2 ŒªŒ¥"
LOG,0.5486875529212532,"
+ 8ŒªH2‚àö"
LOG,0.5495342929720576,"d
Œ∫K
."
LOG,0.550381033022862,11Note the same result in Jin et al. (2021b) applies even though we have an extra constant C2.
LOG,0.5512277730736664,Published as a conference paper at ICLR 2022
LOG,0.5520745131244708,"Combining this with Step1 we receive ‚àÄh, s, a ‚àà[H] √ó S √ó A, with probability 1 ‚àíŒ¥"
LOG,0.5529212531752752,"d
Varh bVh+1(s, a) ‚àíVarPh bVh+1(s, a)
 ‚â§36 s H4d3"
LOG,0.5537679932260796,"Œ∫K log
(Œª + K)2KdH2 ŒªŒ¥"
LOG,0.554614733276884,"
+ 12ŒªH2‚àö"
LOG,0.5554614733276884,"d
Œ∫K
."
LOG,0.5563082133784928,"Finally, by the non-expansiveness of operator max{1, ¬∑}, we have the stated result."
LOG,0.5571549534292972,"C.4.2
A CRUDE BOUND ON suph||V ‚ãÜ
h ‚àíbVh||‚àû."
LOG,0.5580016934801016,"Lemma
C.8.
DeÔ¨Åne
bœÉh(s, a)
=
r"
LOG,0.558848433530906,"max
n
1, d
VarPh bVh+1(s, a)
o
+ 1,
if
K
‚â•"
LOG,0.5596951735817104,"max{M1, M2, M3, M4} and K > C ¬∑ H4Œ∫2, then with probability at least 1 ‚àíŒ¥, sup
h"
LOG,0.5605419136325148,"V ‚ãÜ
h ‚àíbVh

‚àû‚â§eO H2‚àö d
‚àö Œ∫K ! ."
LOG,0.5613886536833192,"Proof. Step1: We show with probability at least 1 ‚àíŒ¥, suph
V ‚ãÜ
h ‚àíV bœÄ
h

‚àû‚â§eO

H2‚àö d
‚àö Œ∫K 
."
LOG,0.5622353937341237,"Indeed, combing Lemma C.1 and Lemma C.5, similar to the proof of Theorem C.6, we directly have
with probability 1 ‚àíŒ¥, for all policy œÄ simultaneously, and for all s ‚ààS, h ‚àà[H]"
LOG,0.563082133784928,"V œÄ
h (s) ‚àíV bœÄ
h (s) ‚â§eO ‚àö d ¬∑ H
X"
LOG,0.5639288738357324,"t=h
EœÄ
h 
œÜ(¬∑, ¬∑)‚ä§Œõ‚àí1
t œÜ(¬∑, ¬∑)
1/2sh = s
i!"
LOG,0.5647756138865369,+ 2H4‚àö
LOG,0.5656223539373413,"d
K
,
(18)"
LOG,0.5664690939881456,"Next, since K ‚â•max

512(1/Œ∫)2 log
  4Hd"
LOG,0.56731583403895,"Œ¥

, 4Œª/Œ∫
	
, by Lemma H.6 and a union bound over
h ‚àà[H], with probability 1 ‚àíŒ¥"
LOG,0.5681625740897545,"sup
s,a ‚à•œÜ(s, a)‚à•bŒõ‚àí1
h
‚â§
2
‚àö"
LOG,0.5690093141405589,"K
sup
s,a ‚à•œÜ(s, a)‚à•Œõp‚àí1
h
‚â§
2H
‚àö"
LOG,0.5698560541913632,"Œ∫K
, ‚àÄh ‚àà[H]."
LOG,0.5707027942421676,"Lastly, taking œÄ = œÄ‚ãÜin equation 18 to obtain"
LOG,0.5715495342929721,"0 ‚â§V œÄ‚ãÜ
h (s) ‚àíV bœÄ
h (s) ‚â§eO ‚àö d ¬∑ H
X"
LOG,0.5723962743437765,"t=h
EœÄ‚ãÜ
h 
œÜ(¬∑, ¬∑)‚ä§Œõ‚àí1
t œÜ(¬∑, ¬∑)
1/2sh = s
i!"
LOG,0.5732430143945809,"+ 2H4‚àö d
K ‚â§eO H2‚àö d
‚àö Œ∫K !"
LOG,0.5740897544453852,"+ 2H4‚àö d
K
. (19)"
LOG,0.5749364944961897,"This implies by using the condition K > C ¬∑ H4Œ∫2, we Ô¨Ånish the proof of Step1."
LOG,0.5757832345469941,"Step2: We show with probability 1 ‚àíŒ¥, suph
bVh ‚àíV bœÄ
h

‚àû‚â§eO

H2‚àö d
‚àö Œ∫K 
."
LOG,0.5766299745977985,"Indeed, applying Extended Value Difference Lemma H.7 for œÄ = œÄ‚Ä≤ = bœÄ, then with probability 1 ‚àíŒ¥,
for all s, h"
LOG,0.5774767146486028,"bVh(s) ‚àíV bœÄ
h (s)
 =  H
X"
LOG,0.5783234546994073,"t=h
EbœÄ
h
bQh(sh, ah) ‚àí

Th bVh+1

(sh, ah)
sh = s
i ‚â§ H
X t=h"
LOG,0.5791701947502117,"(bTh bVh+1 ‚àíTh bVh+1)(s, a)
 + ‚à•Œìh(s, a)‚à•"
LOG,0.5800169348010161,"‚â§eO

H
‚àö d q"
LOG,0.5808636748518204,"œÜ(s, a)Œõ‚àí1
h œÜ(s, a)"
LOG,0.5817104149026249,"
+ 4H4‚àö"
LOG,0.5825571549534293,"d
K
‚â§eO H2‚àö d
‚àö Œ∫K !"
LOG,0.5834038950042337,Published as a conference paper at ICLR 2022
LOG,0.584250635055038,"where the second inequality uses Lemma C.512 and the last inequality follows the same procedure as
Step1."
LOG,0.5850973751058425,"Step3: Combine Step1 and Step2, by triangular inequality and a union bound we Ô¨Ånish the proof of
the lemma."
LOG,0.5859441151566469,"Remark C.9. Note as an intermediate calculation, equation 19 ensures a learning bound with order
eO( H2‚àö d
‚àö"
LOG,0.5867908552074513,"Œ∫K ). Here, the convergence rate is the standard statistical rate
1
‚àö"
LOG,0.5876375952582558,K and the H2 dependence
LOG,0.5884843353090601,"is loose. However, the feature dependence
p"
LOG,0.5893310753598645,"d/Œ∫ is roughly tight, since, in the well-explored case
(Assumption 2 of Wang et al. (2021a)), Œ∫ = 1/d and the
p"
LOG,0.5901778154106689,"d/Œ∫ =
‚àö"
LOG,0.5910245554614734,"d2 recovers the optimal feature
dependence dH
‚àö"
LOG,0.5918712955122777,"T in the online setting (Zhou et al., 2021a). If Œ∫ ‚â™1/d, then doing ofÔ¨Çine learning
requires sample size proportional to d/Œ∫, which reveals ofÔ¨Çine RL is harder when the exploration
of behavior policy is insufÔ¨Åcient. When Œ∫ = 0, learning the optimal policy accurately cannot be
guaranteed even if the sample/episode size K ‚Üí‚àû."
LOG,0.5927180355630821,"C.5
PROOF OF THE SECOND PART OF THEOREM 3.2"
LOG,0.5935647756138865,"Lemma C.10. Recall bœÉh =
r"
LOG,0.594411515664691,"max
n
1, d
VarPh bVh+1
o
+ 1 and œÉ‚ãÜ
h =
q"
LOG,0.5952582557154953,"max

1, VarPhV ‚ãÜ
h+1
	
+ 1."
LOG,0.5961049957662997,"Let K ‚â•max

512(1/Œ∫)2 log
  4Hd"
LOG,0.5969517358171041,"Œ¥

, 4Œª/Œ∫
	
and K ‚â•max{M1, M2, M3, M4}, then with prob-
ability 1 ‚àíŒ¥,"
LOG,0.5977984758679086,"sup
h
||bœÉ2
h ‚àíœÉ‚ãÜ2
h ||‚àû‚â§eO H3‚àö d
‚àö Œ∫K ! ."
LOG,0.598645215918713,"Proof. By deÔ¨Ånition and the non-expansiveness of max{1, ¬∑} + 1, we have
œÉ2
bVh+1 ‚àíœÉ‚ãÜ2
h

‚àû‚â§
VarbVh+1 ‚àíVarV ‚ãÜ
h+1

‚àû"
LOG,0.5994919559695173,"‚â§
Ph

bV 2
h+1 ‚àíV ‚ãÜ2
h+1

‚àû+
(Ph bVh+1)2 ‚àí(PhV ‚ãÜ
h+1)2
‚àû"
LOG,0.6003386960203217,"‚â§
bV 2
h+1 ‚àíV ‚ãÜ2
h+1

‚àû+
(Ph bVh+1 + PhV ‚ãÜ
h+1)(Ph bVh+1 ‚àíPhV ‚ãÜ
h+1)

‚àû"
LOG,0.6011854360711262,"‚â§2H
bVh+1 ‚àíV ‚ãÜ
h+1

‚àû+ 2H
Ph bVh+1 ‚àíPhV ‚ãÜ
h+1

‚àû‚â§eO H3‚àö d
‚àö Œ∫K ! ."
LOG,0.6020321761219306,"with probability 1 ‚àíŒ¥ for all h ‚àà[H], where the last inequality comes from Lemma C.8. Combining
this with Lemma C.7, we have the stated result."
LOG,0.6028789161727349,"Lemma C.11. Denote the quantities C1 = max{2Œª, 128 log(2d/Œ¥), 128H4 log(2d/Œ¥)/Œ∫2} and
C2 = max{
Œª2
Œ∫ log((Œª+K)H/ŒªŒ¥), 962H12d log((Œª + K)H/ŒªŒ¥)/Œ∫5}. Suppose the number of episode
K satisÔ¨Åes K > max{C1, C2}, then with probability 1 ‚àíŒ¥,
q"
LOG,0.6037256562235394,"œÜ(s, a)Œõ‚àí1
h œÜ(s, a) ‚â§2
q"
LOG,0.6045723962743438,"œÜ(s, a)Œõ‚ãÜ‚àí1
h
œÜ(s, a),
‚àÄs, a ‚ààS √ó A,"
LOG,0.6054191363251482,"Proof of Lemma C.11. By deÔ¨Ånition
q"
LOG,0.6062658763759525,"œÜ(s, a)Œõ‚àí1
h œÜ(s, a) = ‚à•œÜ(s, a)‚à•Œõ‚àí1
h . Then denote"
LOG,0.607112616426757,"Œõ‚Ä≤
h = 1"
LOG,0.6079593564775614,"K Œõh,
Œõ‚ãÜ‚Ä≤
h = 1"
LOG,0.6088060965283658,"K Œõ‚ãÜ
h,"
LOG,0.6096528365791702,"12To be absolutely rigorous, we cannot directly apply Lemma C.5 here since the crude bound has already
been used in Lemma C.4. However, this can be resolved completely by Ô¨Årst deriving an even cruder bound for
suph||V ‚ãÜ
h ‚àíbVh||‚àûthat has 1/
‚àö"
LOG,0.6104995766299746,"K rate without using Lemma C.5 (which we call it Lemma C.8‚àó), and we can
use Lemma C.8‚àóto show a similar result Lemma C.5‚àó. Finally, we can use Lemma C.5‚àóhere to Ô¨Ånish the proof
of this Lemma C.8. However, we avoid explicitly doing this to prevent over-technicality."
LOG,0.611346316680779,Published as a conference paper at ICLR 2022
LOG,0.6121930567315834,"where Œõh = PK
œÑ=1 œÜ(sœÑ
h, aœÑ
h)‚ä§œÜ(sœÑ
h, aœÑ
h)/œÉ2
V ‚ãÜ
h+1(sœÑ
h, aœÑ
h) + ŒªI.
Under the condition of K, by
Lemma C.10, with probability 1 ‚àíŒ¥
Œõ‚ãÜ‚Ä≤
h ‚àíŒõ‚Ä≤
h
 ‚â§sup
s,a"
LOG,0.6130397967823878,"œÜ(s, a)œÜ(s, a)‚ä§"
LOG,0.6138865368331922,"œÉ‚ãÜ2
h (s, a)
‚àíœÜ(s, a)œÜ(s, a)‚ä§"
LOG,0.6147332768839966,"œÉ2
bVh+1(s, a) "
LOG,0.615580016934801,"‚â§sup
s,a "
LOG,0.6164267569856055,"œÉ‚ãÜ2
h (s, a) ‚àíœÉ2
bVh+1(s, a)"
LOG,0.6172734970364098,"œÉ‚ãÜ2
h (s, a)œÉ2
bVh+1(s, a)"
LOG,0.6181202370872142,"¬∑ ‚à•œÜ(s, a)‚à•2 ‚â§sup
s,a "
LOG,0.6189669771380186,"œÉ‚ãÜ2
h (s, a) ‚àíœÉ2
bVh+1(s, a) 1 ¬∑ 1 ‚â§eO H3‚àö d
‚àö Œ∫K ! . (20)"
LOG,0.619813717188823,"Next by Lemma H.5 (with œÜ to be œÜ/œÉV ‚ãÜ
h+1 and C = 1), it holds with probability 1 ‚àíŒ¥,
Œõ‚ãÜ‚Ä≤
h ‚àí

E¬µ,h[œÜ(s, a)œÜ(s, a)‚ä§/œÉ2
V ‚ãÜ
h+1(s, a)] + Œª K Id"
LOG,0.6206604572396275," ‚â§4
‚àö 2
‚àö K"
LOG,0.6215071972904318,"
log 2d Œ¥"
LOG,0.6223539373412362,"1/2
."
LOG,0.6232006773920407,"Therefore
by
Weyl‚Äôs
spectrum
theorem
and
the
condition
K
>
max{2Œª, 128 log(2d/Œ¥), 128H4 log(2d/Œ¥)/Œ∫2}, the above implies
Œõ‚ãÜ‚Ä≤
h
 =Œªmax(Œõ‚ãÜ‚Ä≤
h ) ‚â§Œªmax

E¬µ,h[œÜ(s, a)œÜ(s, a)‚ä§/œÉ2
V ‚ãÜ
h+1(s, a)]

+ Œª"
LOG,0.6240474174428451,"K + 4
‚àö 2
‚àö K"
LOG,0.6248941574936494,"
log 2d Œ¥ 1/2"
LOG,0.6257408975444538,"‚â§
E¬µ,h[œÜ(s, a)œÜ(s, a)‚ä§/œÉ2
V ‚ãÜ
h+1(s, a)]
 + Œª"
LOG,0.6265876375952583,"K + 4
‚àö 2
‚àö K"
LOG,0.6274343776460627,"
log 2d Œ¥ 1/2"
LOG,0.628281117696867,"‚â§‚à•œÜ(s, a)‚à•2 + Œª"
LOG,0.6291278577476714,"K + 4
‚àö 2
‚àö K"
LOG,0.6299745977984759,"
log 2d Œ¥"
LOG,0.6308213378492803,"1/2
‚â§1 + Œª"
LOG,0.6316680779000847,"K + 4
‚àö 2
‚àö K"
LOG,0.632514817950889,"
log 2d Œ¥"
LOG,0.6333615580016935,"1/2
‚â§2,"
LOG,0.6342082980524979,"Œªmin(Œõ‚ãÜ‚Ä≤
h ) ‚â•Œªmin

E¬µ,h[œÜ(s, a)œÜ(s, a)‚ä§/œÉ2
V ‚ãÜ
h+1(s, a)]

+ Œª"
LOG,0.6350550381033023,"K ‚àí4
‚àö 2
‚àö K"
LOG,0.6359017781541066,"
log 2d Œ¥ 1/2"
LOG,0.6367485182049111,"‚â•Œªmin

E¬µ,h[œÜ(s, a)œÜ(s, a)‚ä§/œÉ2
V ‚ãÜ
h+1(s, a)]

‚àí4
‚àö 2
‚àö K"
LOG,0.6375952582557155,"
log 2d Œ¥ 1/2 ‚â•Œ∫"
LOG,0.6384419983065199,"H2 ‚àí4
‚àö 2
‚àö K"
LOG,0.6392887383573242,"
log 2d Œ¥"
LOG,0.6401354784081287,"1/2
‚â•
Œ∫
2H2 ."
LOG,0.6409822184589331,"Hence with probability 1 ‚àíŒ¥,
Œõ‚ãÜ‚Ä≤
h
 ‚â§2 and
Œõ‚ãÜ‚Ä≤‚àí1
h
 = 1/Œªmin(Œõ‚ãÜ‚Ä≤
h ) ‚â§2H2/Œ∫. Similarly,
Œõ
‚Ä≤‚àí1
h
 ‚â§2H2/Œ∫ with high probability."
LOG,0.6418289585097375,"Now apply Lemma H.4 to Œõ‚ãÜ‚Ä≤
h and Œõ‚Ä≤
h and a union bound, we obtain with probability 1 ‚àíŒ¥, for all
s, a"
LOG,0.642675698560542,"‚à•œÜ(s, a)‚à•Œõ‚Ä≤‚àí1
h
‚â§

1 +
qŒõ‚ãÜ‚Ä≤‚àí1
h
 Œõ‚ãÜ‚Ä≤
h
 ¬∑
Œõ‚Ä≤‚àí1
h
 ¬∑
Œõ‚ãÜ‚Ä≤
h ‚àíŒõ‚Ä≤
h


¬∑ ‚à•œÜ(s, a)‚à•Œõ‚ãÜ‚Ä≤‚àí1
h ‚â§ "" 1 + r 2H2"
LOG,0.6435224386113463,"Œ∫
¬∑ 1 ¬∑ 2H2"
LOG,0.6443691786621507,"Œ∫
¬∑
Œõ‚ãÜ‚Ä≤
h ‚àíŒõ‚Ä≤
h

#"
LOG,0.6452159187129551,"¬∑ ‚à•œÜ(s, a)‚à•Œõ‚ãÜ‚Ä≤‚àí1
h ‚â§ Ô£Æ Ô£∞1 +"
LOG,0.6460626587637596,"v
u
u
tH4 Œ∫2 ""
eO H3‚àö d
‚àö Œ∫K !#Ô£π"
LOG,0.6469093988145639,"Ô£ª¬∑ ‚à•œÜ(s, a)‚à•Œõ‚ãÜ‚Ä≤‚àí1
h
‚â§2 ‚à•œÜ(s, a)‚à•Œõ‚ãÜ‚Ä≤‚àí1
h"
LOG,0.6477561388653683,"where
the
third
inequality
uses
equation
20
and
the
last
inequality
uses
K
>
max{
Œª2
Œ∫ log((Œª+K)H/ŒªŒ¥), 962H12d log((Œª + K)H/ŒªŒ¥)/Œ∫5}. The claimed result follows straightfor-"
LOG,0.6486028789161727,"wardly by multiplying 1/
‚àö"
LOG,0.6494496189669772,K on both sides of the above.
LOG,0.6502963590177815,Published as a conference paper at ICLR 2022
LOG,0.6511430990685859,"Proof of Theorem 3.2. The Ô¨Årst part of the theorem has been shown in Theorem C.6. For the second
part, apply Theorem C.6 with œÄ = œÄ‚ãÜ, then with probability 1 ‚àíŒ¥,"
LOG,0.6519898391193903,"vœÄ‚ãÜ‚àívbœÄ ‚â§eO ‚àö d ¬∑ H
X"
LOG,0.6528365791701948,"h=1
EœÄ‚ãÜ
h 
œÜ(¬∑, ¬∑)‚ä§Œõ‚àí1
h œÜ(¬∑, ¬∑)
1/2i!"
LOG,0.6536833192209992,"+ 2H4‚àö d
K
,"
LOG,0.6545300592718035,"Now apply Lemma C.11 and a union bound, with probability 1 ‚àíŒ¥,"
LOG,0.655376799322608,"0 ‚â§v‚ãÜ‚àívbœÄ ‚â§eO ‚àö d ¬∑ H
X"
LOG,0.6562235393734124,"h=1
EœÄ‚ãÜ
h 
œÜ(¬∑, ¬∑)‚ä§Œõ‚ãÜ‚àí1
h
œÜ(¬∑, ¬∑)
1/2i!"
LOG,0.6570702794242168,"+ 2H4‚àö d
K
."
LOG,0.6579170194750211,"D
PROOF OF THEOREM 3.3"
LOG,0.6587637595258256,"First of all, we show the following lemma.
Lemma D.1. Suppose K > max{M1, M2, M3, M4}. Plug"
LOG,0.65961049957663,"ŒìI
h(s, a) ‚ÜêœÜ(s, a)‚ä§"
LOG,0.6604572396274344,"bŒõ‚àí1
h K
X œÑ=1"
LOG,0.6613039796782387,"œÜ (sœÑ
h, aœÑ
h) ¬∑

rœÑ
h + bVh+1
 
sœÑ
h+1

‚àí

bTh bVh+1

(sœÑ
h, aœÑ
h)
"
LOG,0.6621507197290432,"bœÉ2
h(sœÑ
h, aœÑ
h)"
LOG,0.6629974597798476,"+ eO(H3d/Œ∫ K
)"
LOG,0.663844199830652,"in Algorithm 1 and let Th be the Bellman operator and bTh be the approximated Bellman operator.
Then we have with probability 1 ‚àíŒ¥:"
LOG,0.6646909398814564,"|(Th bVh+1 ‚àíbTh bVh+1)(s, a)| ‚â§ŒìI
h(s, a),
‚àÄs, a ‚ààS √ó A."
LOG,0.6655376799322608,"Proof of Lemma D.1. Suppose wh is the coefÔ¨Åcient corresponding to the Th bVh+1 (such wh exists by
Lemma H.9), i.e. Th bVh+1 = œÜ‚ä§wh, and recall (bTh bVh+1)(s, a) = œÜ(s, a)‚ä§bwh, then:

Th bVh+1

(s, a) ‚àí

bTh bVh+1

(s, a) = œÜ(s, a)‚ä§(wh ‚àíbwh)"
LOG,0.6663844199830652,"=œÜ(s, a)‚ä§wh ‚àíœÜ(s, a)‚ä§bŒõ‚àí1
h K
X"
LOG,0.6672311600338696,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

rœÑ
h + bVh+1
 
sœÑ
h+1

/bœÉ2
h(sœÑ
h, aœÑ
h) !"
LOG,0.668077900084674,"= œÜ(s, a)‚ä§wh ‚àíœÜ(s, a)‚ä§bŒõ‚àí1
h K
X"
LOG,0.6689246401354784,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

Th bVh+1

(sœÑ
h, aœÑ
h) /bœÉ2
h(sœÑ
h, aœÑ
h) !"
LOG,0.6697713801862828,"|
{z
}
(i)"
LOG,0.6706181202370872,"+ œÜ(s, a)‚ä§bŒõ‚àí1
h K
X"
LOG,0.6714648602878917,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

rœÑ
h + bVh+1
 
sœÑ
h+1

‚àí

bTh bVh+1

(sœÑ
h, aœÑ
h)

/bœÉ2
h(sœÑ
h, aœÑ
h) !"
LOG,0.672311600338696,"|
{z
}
(ii)"
LOG,0.6731583403895004,"+ œÜ(s, a)‚ä§bŒõ‚àí1
h K
X"
LOG,0.6740050804403048,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

bTh bVh+1

(sœÑ
h, aœÑ
h) ‚àí

Th bVh+1

(sœÑ
h, aœÑ
h)

/bœÉ2
h(sœÑ
h, aœÑ
h) !"
LOG,0.6748518204911093,"|
{z
}
(iii)
(21)"
LOG,0.6756985605419137,"For term (i), by Lemma C.2 it is bounded by 2ŒªH3‚àö"
LOG,0.676545300592718,"d/Œ∫
K
with probability 1 ‚àíŒ¥/2.13"
LOG,0.6773920406435224,"For term (ii), it is bounded by"
LOG,0.6782387806943269,"œÜ(s, a)‚ä§"
LOG,0.6790855207451313,"bŒõ‚àí1
h K
X œÑ=1"
LOG,0.6799322607959356,"œÜ (sœÑ
h, aœÑ
h) ¬∑

rœÑ
h + bVh+1
 
sœÑ
h+1

‚àí

bTh bVh+1

(sœÑ
h, aœÑ
h)
"
LOG,0.68077900084674,"bœÉ2
h(sœÑ
h, aœÑ
h) ."
LOG,0.6816257408975445,"13Note Here Lemma C.2 still applies even if the Œìh changes since it works for all bVh ‚àà[0, H] so that
‚à•wh‚à•2 ‚â§2H
‚àö"
LOG,0.6824724809483489,d and the truncation (Line 13 in Algorithm 1) guarantees this.
LOG,0.6833192209991532,Published as a conference paper at ICLR 2022
LOG,0.6841659610499576,"For term (iii), by Cauchy inequality"
LOG,0.6850127011007621,"œÜ(s, a)‚ä§bŒõ‚àí1
h K
X"
LOG,0.6858594411515665,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

bTh bVh+1

(sœÑ
h, aœÑ
h) ‚àí

Th bVh+1

(sœÑ
h, aœÑ
h)

/bœÉ2
h(sœÑ
h, aœÑ
h) !"
LOG,0.6867061812023709,"‚â§‚à•œÜ(s, a)‚à•bŒõ‚àí1
h ¬∑  K
X"
LOG,0.6875529212531752,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

bTh bVh+1

(sœÑ
h, aœÑ
h) ‚àí

Th bVh+1

(sœÑ
h, aœÑ
h)

/bœÉ2
h(sœÑ
h, aœÑ
h)"
LOG,0.6883996613039797,"bŒõ‚àí1
h ‚â§2H
‚àö Œ∫K
¬∑  K
X"
LOG,0.6892464013547841,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

bTh bVh+1

(sœÑ
h, aœÑ
h) ‚àí

Th bVh+1

(sœÑ
h, aœÑ
h)

/bœÉ2
h(sœÑ
h, aœÑ
h)"
LOG,0.6900931414055885,"bŒõ‚àí1
h ‚â§2H
‚àö"
LOG,0.6909398814563928,"Œ∫K
¬∑ eO(H2p d/Œ∫
‚àö"
LOG,0.6917866215071973,"K
) ¬∑
‚àö"
LOG,0.6926333615580017,"d = eO(H3d/Œ∫ K
)"
LOG,0.6934801016088061,"where the Ô¨Årst inequality is by Lemma H.6 (with œÜ‚Ä≤ = œÜ/bœÉh and ‚à•œÜ/bœÉh‚à•‚â§‚à•œÜ‚à•‚â§1 := C) and the
third inequality uses
‚àö"
LOG,0.6943268416596104,"a‚ä§¬∑ A ¬∑ a ‚â§
p"
LOG,0.6951735817104149,"‚à•a‚à•2 ‚à•A‚à•2 ‚à•a‚à•2 = ‚à•a‚à•2
p"
LOG,0.6960203217612193,‚à•A‚à•2 with a to be either œÜ or wh.
LOG,0.6968670618120237,"Moreover, Œªmin(ÀúŒõp
h) ‚â•Œ∫/ maxh,s,a bœÉh(s, a)2 ‚â•Œ∫/H2 implies
(ÀúŒõp
h)‚àí1 ‚â§H2/Œ∫."
LOG,0.6977138018628282,"The second inequality is true by denoting xœÑ = œÜ(sœÑ
h, aœÑ
h)/bœÉ(sœÑ
h, aœÑ
h) and"
LOG,0.6985605419136325,"Œ∑œÑ =

bTh bVh+1

(sœÑ
h, aœÑ
h) ‚àí

Th bVh+1

(sœÑ
h, aœÑ
h)

/bœÉh(sœÑ
h, aœÑ
h)"
LOG,0.6994072819644369,"and use Lemma H.10 as the condition for applying Lemma H.2. By collecting those three terms
together we have the result."
LOG,0.7002540220152413,"D.1
PROOF OF THEOREM 3.3"
LOG,0.7011007620660458,"Proof. Use Lemma D.1 as the condition for Lemma C.1 and average over initial distribution d1, we
obtain with probability 1 ‚àíŒ¥,"
LOG,0.7019475021168501,"vœÄ ‚àívbœÄ ‚â§ H
X"
LOG,0.7027942421676545,"h=1
EœÄh [œÜ(s, a)]‚ä§"
LOG,0.7036409822184589,"bŒõ‚àí1
h K
X œÑ=1"
LOG,0.7044877222692634,"œÜ (sœÑ
h, aœÑ
h) ¬∑

rœÑ
h + bVh+1
 
sœÑ
h+1

‚àí

bTh bVh+1

(sœÑ
h, aœÑ
h)
"
LOG,0.7053344623200677,"bœÉ2
h(sœÑ
h, aœÑ
h)"
LOG,0.7061812023708721,"+ eO(H4d/Œ∫ K
) (22)"
LOG,0.7070279424216765,"Denote Ah := PK
œÑ=1
œÜ(sœÑ
h,aœÑ
h)¬∑(rœÑ
h+bVh+1(sœÑ
h+1)‚àí(Th bVh+1)(sœÑ
h,aœÑ
h))
bœÉ2
h(sœÑ
h,aœÑ
h)
, then"
LOG,0.707874682472481,"EœÄh [œÜ(s, a)]‚ä§"
LOG,0.7087214225232854,"bŒõ‚àí1
h K
X œÑ=1"
LOG,0.7095681625740897,"œÜ (sœÑ
h, aœÑ
h) ¬∑

rœÑ
h + bVh+1
 
sœÑ
h+1

‚àí

bTh bVh+1

(sœÑ
h, aœÑ
h)
"
LOG,0.7104149026248942,"bœÉ2
h(sœÑ
h, aœÑ
h) "
LOG,0.7112616426756986,"‚â§EœÄh [œÜ]‚ä§¬∑
bŒõ‚àí1
h Ah
 + EœÄh [œÜ]‚ä§"
LOG,0.712108382726503,"bŒõ‚àí1
h K
X œÑ=1"
LOG,0.7129551227773073,"œÜ (sœÑ
h, aœÑ
h) ¬∑

Th bVh+1 (sœÑ
h, aœÑ
h) ‚àíbTh bVh+1 (sœÑ
h, aœÑ
h)
"
LOG,0.7138018628281118,"bœÉ2
h(sœÑ
h, aœÑ
h) "
LOG,0.7146486028789162,"For the second term, it can be bounded similar to term (iii) in Lemma D.1 and for the Ô¨Årst term we
have the following:"
LOG,0.7154953429297206,"EœÄh [œÜ]‚ä§¬∑
bŒõ‚àí1
h Ah
 = EœÄh [œÜ]‚ä§¬∑ bŒõ‚àí1
h
¬∑ bŒõh
bŒõ‚àí1
h Ah
 ‚â§‚à•EœÄh[œÜ]‚à•bŒõ‚àí1
h ¬∑
bŒõh|bŒõ‚àí1
h Ah|
bŒõ‚àí1
h
‚â§‚à•EœÄh[œÜ]‚à•bŒõ‚àí1
h ¬∑ ‚à•|Ah|‚à•bŒõ‚àí1
h
‚â§eO(
‚àö"
LOG,0.7163420829805249,"d) ‚à•EœÄh[œÜ]‚à•bŒõ‚àí1
h
‚â§eO(
‚àö"
LOG,0.7171888230313294,"d ‚à•EœÄh[œÜ]‚à•Œõ‚àí1
h ),"
LOG,0.7180355630821338,"where the Ô¨Årst inequality uses Cauchy‚Äôs inequality, the second inequality uses bŒõh is coordinate-wise
positive (since we assume here œÜ ‚â•0), the third inequality is identical to the analysis in Section C.2.2
and the fourth inequality is identical to the analysis in Section C.2.1 with œÜ replaced by E[œÜ]. Plug
this back to equation 22 we Ô¨Ånish the proof for the Ô¨Årst part. For the second part, converting Œõ‚àí1
h
to
Œõ‚ãÜ‚àí1
h
is identical to Section C.5. This Ô¨Ånishes the proof."
LOG,0.7188823031329382,Published as a conference paper at ICLR 2022
LOG,0.7197290431837426,"E
PROOF OF MINIMAX LOWER BOUND THEOREM 3.5"
LOG,0.720575783234547,"The proof follows the lower bound proof of Zanette et al. (2021). For completeness, we provide all
the details in below."
LOG,0.7214225232853514,"E.1
CONSTRUCTION"
LOG,0.7222692633361558,"Similar to the proof of [Zanette et al. (2021), Theorem 2], we construct a family of MDPs, each
parameterized by a Boolean vector u = (u1, . . . , uH) with each uh ‚àà{‚àí1, +1}d‚àí2 for h ‚àà[H].
The MDPs share the same transition kernel and are only different in the reward observations."
LOG,0.7231160033869602,"State space: At each time step h, there are two states S = {+1, ‚àí1}.
Action space: The action space A = {‚àí1, 0, +1}d‚àí2.
Feature map: The feature map œÜ : S √ó A 7‚ÜíRd is given by"
LOG,0.7239627434377646,"œÜ(+1, a) = Ô£´ Ô£≠ a
‚àö"
D,0.724809483488569,"2d
1
‚àö 2
0 Ô£∂"
D,0.7256562235393734,"Ô£∏‚ààRd,
œÜ(‚àí1, a) = Ô£´ Ô£≠ a
‚àö"
D,0.7265029635901779,"2d
0
1
‚àö 2 Ô£∂ Ô£∏‚ààRd."
D,0.7273497036409822,"The construction ensures the condition ‚à•œÜ(s, a)‚à•2 ‚â§1 for any (s, a) ‚ààS √ó A.
Transition kernel: The transition probability Ph(s‚Ä≤ | s, a) is independent of action a. In other
words, the Markov decision process reduces to a homogeneous Markov chain with transition
matrix"
D,0.7281964436917866,"P =
 1"
D,0.729043183742591,"2
1
2
1
2
1
2"
D,0.7298899237933955,"
‚ààR2."
D,0.7307366638441999,By letting
D,0.7315834038950042,ŒΩh(+1) = ŒΩh(‚àí1) = Ô£´
D,0.7324301439458086,"Ô£≠
0d‚àí2
1
‚àö 2
1
‚àö 2 Ô£∂ Ô£∏‚ààRd,"
D,0.7332768839966131,"we have Ph(s‚Ä≤ | s, a) = ‚ü®œÜ(s, a), ŒΩh(s‚Ä≤)‚ü©to be a valid probability transition.
Reward observations: For any MDP Mu, at each times step h, the reward follows a Gaussian
distribution with"
D,0.7341236240474175,"Ru,h(s, a) ‚àºN
 s
‚àö"
D,0.7349703640982218,"6 +
Œ¥
‚àö"
D,0.7358171041490262,"2d
‚ü®a, uh‚ü©, 1

,"
D,0.7366638441998307,"where Œ¥ ‚àà

0,
1
‚àö"
D,0.7375105842506351,"3d

determines to what extent the MDP models are different from each
other. The mean reward function satisÔ¨Åes ru,h(s, a) = ‚ü®œÜ(s, a), Œ∏u,h‚ü©with"
D,0.7383573243014394,"Œ∏u,h = Ô£´"
D,0.7392040643522438,"Ô£≠
Œ¥uh
1
‚àö"
D,0.7400508044030483,"3
‚àí1
‚àö 3 Ô£∂ Ô£∏‚ààRd."
D,0.7408975444538527,"OfÔ¨Çine data collection Scheme: The dataset D = {(sœÑ
h, aœÑ
h, rœÑ
h, sœÑ
h+1)}h‚àà[H]
œÑ‚àà[K] consist of K i.i.d.
trajectories. All the trajectories initiate from uniform distribution. We take a behavior policy
¬µ(¬∑ | s) that is independent of state s. Let {e1, e2, . . . , ed‚àí2} be the canonical bases of
Rd‚àí2 and 0d‚àí2 ‚ààRd‚àí2 be the zero vector. The behavior policy ¬µ is set as"
D,0.7417442845046571,¬µ(ej | s) = 1
D,0.7425910245554614,"d
for any j ‚àà[d ‚àí2]
and
¬µ(0d‚àí2 | s) = 2 d."
D,0.7434377646062659,"E.2
OVERVIEW OF PROOF"
D,0.7442845046570703,"The proof of the theorem is based on Assouad‚Äôs method, where we Ô¨Årst reduce the problem to binary
hypothesis tests (Lemmas E.1 and E.2) and then connect the testing error to the uncertainty quantity
in the upper bound (Lemma E.3)."
D,0.7451312447078747,Published as a conference paper at ICLR 2022
D,0.745977984758679,Lemma E.1 (Reduction to testing). There exists a universal constant c1 > 0 such thata
D,0.7468247248094835,"inf
bœÄ max
u‚ààU Eu

V ‚ãÜ
u ‚àíV bœÄ
u

‚â•c1 Œ¥
‚àö"
D,0.7476714648602879,"d H
min
u,u‚Ä≤‚ààU:DH(u‚Ä≤;u)=1 inf
œà

Pu(œà Ã∏= u) + Pu‚Ä≤(œà Ã∏= u‚Ä≤)

,
(23)"
D,0.7485182049110923,"where bœÄ denotes the output of any algorithm that maps from observations to an estimated policy. œà is
any test function for parameter u and DH is the hamming distance."
D,0.7493649449618967,"Lemma E.2. There exists a universal constant c2 > 0 such that when taking Œ¥ := c2 d
‚àö"
D,0.7502116850127011,"K , we have"
D,0.7510584250635055,"min
u,u‚Ä≤‚ààU:DH(u‚Ä≤;u)=1 inf
œà

Pu(œà Ã∏= u) + Pu‚Ä≤(œà Ã∏= u‚Ä≤)

‚â•1"
D,0.7519051651143099,"2.
(24)"
D,0.7527519051651143,"When K ‚â≥d3, Œ¥ := c2 d
‚àö"
D,0.7535986452159187,"K ensures that Œ¥ ‚â§1/
‚àö"
D,0.7544453852667231,"3d. Combining Lemmas E.1 and E.2 yields a lower
bound"
D,0.7552921253175275,"inf
bœÄ max
u‚ààU Eu

V ‚ãÜ
u ‚àíV bœÄ
u

‚â•c d
‚àö dH
‚àö"
D,0.756138865368332,"K
,
(25)"
D,0.7569856054191363,"where c > 0 is a universal constant. We then use the following Lemma E.3 to connect the above
lower bound to the uncertainty term
‚àö"
D,0.7578323454699407,"d¬∑PH
h=1
p"
D,0.7586790855207451,"EœÄ‚ãÜ[œÜ]‚ä§(Œõ‚ãÜ
h)‚àí1EœÄ‚ãÜ[œÜ] for the chosen linear MDP
instances class M."
D,0.7595258255715496,"Lemma E.3. There exists a universal constant c3 > 0 such that for all M ‚ààM, H
X h=1 q"
D,0.7603725656223539,"EœÄ‚ãÜ[œÜ]‚ä§(Œõ‚ãÜ
h)‚àí1EœÄ‚ãÜ[œÜ] ‚â§c3
d H
‚àö"
D,0.7612193056731583,"K
.
(26)"
D,0.7620660457239627,"Plugging inequality equation E.3 into the bound equation 25, we obtain the minimax lower bound equa-
tion 6 in the statement of theorem."
D,0.7629127857747672,"E.3
REDUCTION TO TESTING VIA ASSOUAD‚ÄôS METHOD"
D,0.7637595258255715,"Proof of Lemma E.1. For any index vector u = (u1, . . . , uH) ‚ààU = {‚àí1, +1}(d‚àí2)√óH, the optimal
policy for MDP instance Mu is simply"
D,0.7646062658763759,"œÄ‚ãÜ
h(¬∑) = uh
for h ‚àà[H]."
D,0.7654530059271804,"Similar to the proof of Lemma 9 in Zanette et al. (2021), we can show that the value suboptimality of
policy œÄ on MDP Mu is given by"
D,0.7662997459779848,"V ‚ãÜ
u ‚àíV œÄ
u =
Œ¥
‚àö"
D,0.7671464860287892,"2d H
X h=1"
D,0.7679932260795935,"uh ‚àíEœÄ[ah]

1."
D,0.768839966130398,"DeÔ¨Åne uœÄ = (uœÄ
1, . . . , uœÄ
H) with uœÄ
h := sign
 
EœÄ[ah]

, then the ‚Ñì1-norm is lower bounded as
uh ‚àíEœÄ[ah]

1 ‚â•DH(uœÄ
h; uh),"
D,0.7696867061812024,where DH(¬∑; ¬∑) denotes the Hamming distance. It follows that
D,0.7705334462320068,"V ‚ãÜ
u ‚àíV œÄ
u ‚â•
Œ¥
‚àö"
D,0.7713801862828111,"2d
DH(uœÄ; u).
(27)"
D,0.7722269263336156,We then apply Assouad‚Äôs method (Lemma 2.12 in Sampson and Guttorp (1992)) and obtain that
D,0.77307366638442,"inf
ÀÜu‚ààU max
u‚ààU Eu

DH(ÀÜu; u)

‚â•(d ‚àí2)H"
MIN,0.7739204064352244,"2
min
u,u‚Ä≤‚ààU:DH(u‚Ä≤;u)=1 inf
œà

Pu(œà Ã∏= u) + Pu‚Ä≤(œà Ã∏= u‚Ä≤)

,
(28)"
MIN,0.7747671464860287,"where œà is any test functions mapping from observations to {u, u‚Ä≤}. Combining inequalities equa-
tion 27 and equation 28, we Ô¨Ånish the proof."
MIN,0.7756138865368332,Published as a conference paper at ICLR 2022
MIN,0.7764606265876376,"E.4
LOWER BOUND ON THE TESTING ERROR"
MIN,0.777307366638442,"Proof of Lemma E.2. The proof of Lemma E.2 is similar to that of Lemma 10 in Zanette et al. (2021).
We Ô¨Årst apply Theorem 2.12 in Sampson and Guttorp (1992) to lower bound the testing error using
Kullback‚ÄìLeibler divergence and obtain"
MIN,0.7781541066892464,"min
u,u‚Ä≤‚ààU:DH(u‚Ä≤;u)=1 inf
œà

Pu(œà Ã∏= u) + Pu‚Ä≤(œà Ã∏= u‚Ä≤)

‚â•1 ‚àí

1
2
max
u,u‚Ä≤‚ààU:DH(u‚Ä≤;u)=1 DKL(Qu‚à•Qu‚Ä≤)
1/2
. (29)"
MIN,0.7790008467400508,It only remains to estimate DKL(Qu‚à•Qu‚Ä≤).
MIN,0.7798475867908552,The probability density Qu takes the form
MIN,0.7806943268416596,"Qu(D) = K
Y"
MIN,0.781541066892464,"k=1
Œæ1(sk
1) H
Y"
MIN,0.7823878069432684,"h=1
¬µ
 
ak
h | sk
h)

Ru,h(sk
h, ak
h)

(rk
h) Ph(sk
h+1 | sk
h, ak
h)"
MIN,0.7832345469940728,"where Œæ1 =
 1 2, 1"
MIN,0.7840812870448772,"2

is the initial distribution. It follows that"
MIN,0.7849280270956817,"DKL(Qu‚à•Qu‚Ä≤) = Eu

log(Qu/Qu‚Ä≤)
 = K ¬∑ H
X"
MIN,0.785774767146486,"h=1
Eu
h
log
 
Ru,h(s1
h, a1
h)

(r1
h)

Ru‚Ä≤,h(s1
h, a1
h)

(r1
h)
i = K d d‚àí2
X"
MIN,0.7866215071972904,"j=1
DKL

N
 
Œ¥
‚àö"
MIN,0.7874682472480948,"2d‚ü®ej, uh‚ü©, 1
  N
 
Œ¥
‚àö"
MIN,0.7883149872988993,"2d‚ü®ej, u‚Ä≤
h‚ü©, 1

."
MIN,0.7891617273497037,"If we take Œ¥ = c2 d
‚àö"
MIN,0.790008467400508,"K , then inequality equation 29 ensures inequality equation 24, as claimed in the
statement of the lemma."
MIN,0.7908552074513124,"E.5
CONNECTION TO THE UNCERTAINTY TERM"
MIN,0.7917019475021169,"Proof of Lemma E.3. We Ô¨Årst calculate the explicit form of the inverse of variance-rescaled covari-
ance matrix Œõ‚ãÜ,p
h . For each time step h ‚àà[H], the value function V ‚ãÜ
u,h+1 takes the form"
MIN,0.7925486875529213,"V ‚ãÜ
u,h+1 = EœÄ‚ãÜru,h+1 +
 
PœÄ‚ãÜ
h+1V ‚ãÜ
u,h+2

."
MIN,0.7933954276037256,"Since
 
Ph+1V ‚ãÜ
u,h+2

(+1) =
 
Ph+1V ‚ãÜ
u,h+2

(‚àí1) and ru,h+1(+1, a) ‚àíru,h+1(‚àí1, a) = 2/
‚àö"
MIN,0.79424216765453,"6, we
have"
MIN,0.7950889077053345,"VarPh(V ‚ãÜ
u,h+1)(+1, a) = VarPh(EœÄ‚ãÜru,h+1)(+1, a) = 1 6."
MIN,0.7959356477561389,"Similarly,"
MIN,0.7967823878069432,"VarPh(V ‚ãÜ
u,h+1)(‚àí1, a) = VarPh(V ‚ãÜ
u,h+1)(+1, a) = 1 6."
MIN,0.7976291278577476,"By routine calculation, we Ô¨Ånd that the population-level rescaled covariance matrix takes the form"
MIN,0.7984758679085521,"Œõ‚ãÜ,p
h
= 3K 2"
MIN,0.7993226079593565,"2
d2 Id‚àí2
1
d
‚àö"
MIN,0.8001693480101609,"d1(d‚àí2)√ó2
1
d
‚àö"
MIN,0.8010160880609652,"d12√ó(d‚àí2)
I2 ! ‚ààRd√ód"
MIN,0.8018628281117697,"for any h ‚àà[H]. Applying Gaussian elimination on Œõ‚ãÜ,p
h , we have"
MIN,0.8027095681625741,"(Œõ‚ãÜ,p
h )‚àí1 =
2
3K Ô£´ Ô£≠ d2"
MIN,0.8035563082133785,"2

Id‚àí2 +
1
d‚àí21(d‚àí2)√ó(d‚àí2)
	
‚àí
d
‚àö"
MIN,0.8044030482641829,"d
2(d‚àí2)1(d‚àí2)√ó2 ‚àí
d
‚àö"
MIN,0.8052497883149873,"d
2(d‚àí2)12√ó(d‚àí2)
1
d‚àí2"
MIN,0.8060965283657917,"
d ‚àí1
1
1
d ‚àí1 
Ô£∂ Ô£∏."
MIN,0.8069432684165961,Published as a conference paper at ICLR 2022
MIN,0.8077900084674005,"For each time step h ‚àà[H], we have (by Jensen‚Äôs inequality)
q"
MIN,0.8086367485182049,"EœÄ‚ãÜ[œÜ]‚ä§(Œõ‚ãÜ
h)‚àí1EœÄ‚ãÜ[œÜ] ‚â§1 2"
MIN,0.8094834885690093,"œÜ(+1, uh)

(Œõ‚ãÜ,p
h
)‚àí1 + 1 2"
MIN,0.8103302286198137,"œÜ(‚àí1, uh)

(Œõ‚ãÜ,p
h
)‚àí1."
MIN,0.8111769686706182,"Recall that by our construction,"
MIN,0.8120237087214225,"œÜ(+1, uh) = Ô£´ Ô£≠ uh
‚àö"
D,0.8128704487722269,"2d
1
‚àö 2
0 Ô£∂"
D,0.8137171888230313,"Ô£∏‚ààRd,
œÜ(‚àí1, uh) = Ô£´ Ô£≠ uh
‚àö"
D,0.8145639288738358,"2d
0
1
‚àö 2 Ô£∂ Ô£∏‚ààRd."
D,0.8154106689246401,"It follows that
œÜ(+1, uh)
2
(Œõ‚ãÜ,p
h
)‚àí1 =
œÜ(‚àí1, uh)
2
(Œõ‚ãÜ,p
h
)‚àí1"
D,0.8162574089754445,"=
2
3K d"
D,0.817104149026249,"4u‚ä§
h

Id‚àí2 +
1
d‚àí21(d‚àí2)√ó(d‚àí2)
	
uh ‚àí
d
2(d ‚àí2)1‚ä§
d‚àí2uh +
d ‚àí1
2(d ‚àí2) "
D,0.8179508890770534,"=
2
3K d2"
D,0.8187976291278577,"4 +
d
4(d ‚àí2)
 
1 ‚àí1‚ä§
d‚àí2uh
2 + 1 4 "
D,0.8196443691786621,"‚â§
2
3K d2"
D,0.8204911092294666,4 + d(d ‚àí1)2
D,0.821337849280271,4(d ‚àí2) + 1 4
D,0.8221845893310754,"
=
2
3K d2"
D,0.8230313293818797,"2 +
d ‚àí1
2(d ‚àí2)"
D,0.8238780694326842,"
‚â≤d2/K."
D,0.8247248094834886,"Therefore,
q"
D,0.825571549534293,"EœÄ‚ãÜ[œÜ]‚ä§(Œõ‚ãÜ
h)‚àí1EœÄ‚ãÜ[œÜ] ‚â≤d/
‚àö K."
D,0.8264182895850973,"Taking the summation over h ‚àà[H], we obtain the bound equation 26 as claimed in the lemma
statement."
D,0.8272650296359018,"F
A NUMERICAL SIMULATION"
D,0.8281117696867062,"F.1
A LINEAR MDP CONSTRUCTION"
D,0.8289585097375106,"We consider a synthetic linear MDP example that is similar to Min et al. (2021) but with some
modiÔ¨Åcations for the ofÔ¨Çine learning task. The MDP instance we use consists of |S| = 2 states and
|A| = 100 actions, and feature dimension d = 100. We set S = {0, 1} and A = {0, 1, . . . , 99}
respectively.
For each action a ‚àà{0, 1, . . . , 99}, we use binary encoding to obtain a vec-
tor a ‚ààR8 using its binary representation (i.e.
each coordinate is either 0 or 1).
we inter-
changebly use a and and its vector representation a for the ease of explanation. We Ô¨Årst deÔ¨Åne"
D,0.8298052497883149,"Œ¥(s, a) =
 1
if 1{s = 0} = 1{a = 0}
0
otherwise
, then the non-stationary linear MDP is speciÔ¨Åed by the"
D,0.8306519898391194,following conÔ¨Åguration
D,0.8314987298899238,"‚Ä¢ Feature mapping:
œÜ(s, a) =
 
a‚ä§, Œ¥(s, a), 1 ‚àíŒ¥(s, a)
‚ä§‚ààR10"
D,0.8323454699407282,‚Ä¢ The true measure ŒΩh
D,0.8331922099915327,"ŒΩh(s) = (0, . . . , 0, (1 ‚àís) ‚äïŒ±h, s ‚äïŒ±h) ,"
D,0.834038950042337,"where {Œ±h}h‚àà[H] is a sequence of integers taking values 0 or 1 and ‚äïis the standard XOR
operator. We deÔ¨Åne
Œ∏h ‚â°(0, . . . , 0, r, 1 ‚àír) ‚ààR10"
D,0.8348856900931414,"with the choice of r = 0.9. The transition follows Ph(s‚Ä≤|s, a) = ‚ü®œÜ(s, a), ŒΩh(s‚Ä≤)‚ü©and the
mean reward function rh(s, a) = ‚ü®œÜ(s, a), Œ∏h‚ü©.
‚Ä¢ Behavior policy: always choose action a = 0 with probability p, and other actions uniformly
with probability (1 ‚àíp)/99. The initial distribution chooses s = 0 and s = 1 with equal
probability 1/2. We use p = 0.6."
D,0.8357324301439458,Published as a conference paper at ICLR 2022
D,0.8365791701947503,"0
200
400
600
800
1000
Episode K 10‚àí3 10‚àí2 10‚àí1 100 101"
D,0.8374259102455546,Suboptimality gap
D,0.838272650296359,Fixed horizon H = 20
D,0.8391193903471634,"PEVI
VAPVI
LSVI
VAVI"
D,0.8399661303979679,"(a) Suboptimality vs. Episode K
(Horizon H = 20)"
D,0.8408128704487722,"0
200
400
600
800
1000
Episode K 10‚àí3 10‚àí2 10‚àí1 100 101"
D,0.8416596104995766,Suboptimality gap
D,0.842506350550381,Fixed horizon H = 30
D,0.8433530906011855,"PEVI
VAPVI
LSVI
VAVI"
D,0.8441998306519899,"(b) Suboptimality vs. Episode K
(Horizon H = 30)"
D,0.8450465707027942,"0
200
400
600
800
1000
Episode K 10‚àí2 10‚àí1 100 101"
D,0.8458933107535986,Suboptimality gap
D,0.8467400508044031,Fixed horizon H = 50
D,0.8475867908552075,"PEVI
VAPVI
LSVI
VAVI"
D,0.8484335309060118,"(c) Suboptimality vs. Episode K
(Horizon H = 50)"
D,0.8492802709568162,"Figure 1: Comparison between PEVI and VAPVI in the non-stationary linear MDP instance described
above. In each Ô¨Ågure, y-axis denotes suboptimality gap v‚ãÜ‚àívbœÄ, x-axis denotes number of episodes
K. The problem horizons are Ô¨Åxed to be H = 20, 30, 50. The solid line denotes the average
suboptimality gap over 50 trials and the error bar area is the corresponding standard deviation. The
range of K is from 5 to 1000."
D,0.8501270110076207,"F.2
EMPIRICAL COMPARISON BETWEEN PEVI AND VAPVI ON THE CONSTRUCTED LINEAR
MDP"
D,0.8509737510584251,"We compare Pessimistic Value Iteration (PEVI) in Jin et al. (2021b) and our VAPVI Algorithm 1 in
Figure 1, with horizon to be H = 20, 30, 50. In addition, we add the non-pessimistic version for both
algorithms, i.e. least-square value iteration (LSVI) and variance-aware value iteration (VAVI). The
true optimal value v‚ãÜis computed via value iteration using the underlying transition kernels. For
the empirical validation of VAPVI, we do not split the data and, in particular, in all the methods we
choose Œª = 0.01 (instead of Œª = 1 used in theory (Jin et al., 2021b) which causes over-regularization
in the simulation)."
D,0.8518204911092294,"We can observe VAPVI outperforms PEVI and the gap becomes larger when horizon H increases.
One main reason for this to happen is due to the bonus used in PEVI (Jin et al., 2021b)"
D,0.8526672311600338,"O

dH ¬∑

œÜ(¬∑, ¬∑)‚ä§Œ£‚àí1
h œÜ(¬∑, ¬∑)
1/2"
D,0.8535139712108383,"is overly pessimistic comparing to our O
‚àö"
D,0.8543607112616427,"d ¬∑

œÜ(¬∑, ¬∑)‚ä§Œõ‚àí1
h œÜ(¬∑, ¬∑)
1/2"
D,0.8552074513124471,"when H becomes larger and this could potentially make the learning less accurate. In addition, both
non-pessimistic algorithms exhibit similar accuracy, and this is partially owing to our truncation
scheme bœÉh(¬∑, ¬∑)2 ‚Üêmax{1, d
VarPh bVh+1(¬∑, ¬∑)} so bœÉh(¬∑, ¬∑)2 will just be 1 when the estimated variance
is small. Lastly, variance-aware pessimism eventually outperforms non-pessimism algorithms when
sample size is large and this might come from that the pessimistic bonus is estimated more accurately
when more samples are collected."
D,0.8560541913632514,"G
SOME MISSING DERIVATIONS AND DISCUSSIONS"
D,0.8569009314140559,"G.1
REGARDING COVERAGE ASSUMPTION"
D,0.8577476714648603,"Now we discuss the feature coverage assumption. Indeed, even if Assumption 2.2 is not satisÔ¨Åed,
we can still learn in the effective subspan of Œ£p
h := E¬µ,h

œÜ(s, a)œÜ(s, a)‚ä§
. Concretely, since Œ£p
h is
symmetric, by orthogonal decomposition we have Œ£p
h = ZhŒõZ‚ä§
h , where Zh (can be estimated using
the samples for practical purpose) consists of orthogonal basis and Œõ consists of eigenvalues of Œ£p
h in
the diagonal. Suppose we do not have a full coverage, i.e.
Œõ = diag[Œª1, Œª2, ..., Œªd‚Ä≤, 0, ..., 0]
with
d‚Ä≤ < d,
then we can create transformed features œÜ‚Ä≤
h(s, a) = Zh ¬∑ œÜh(s, a), and then"
D,0.8585944115156647,"E¬µ,h

œÜ‚Ä≤
h(s, a)œÜ‚Ä≤
h(s, a)‚ä§
= Œõ = diag[Œª1, Œª2, ..., Œªd‚Ä≤, 0, ..., 0]."
D,0.859441151566469,"Then we can do learning w.r.t. the truncated features œÜ‚Ä≤
h|1:d‚Ä≤‚Äôs instead of the original œÜ. It reduces to
the weaker notion of Œ∫ := minh‚àà[H]{Œ∫h : s.t. Œ∫h = smallest positive eigenvalue at time h}."
D,0.8602878916172735,Published as a conference paper at ICLR 2022
D,0.8611346316680779,"G.2
DERIVATION OF EQUATION 5"
D,0.8619813717188823,"When reducing Theorem 3.2,3.3 to the tabular case, set œÜ(s, a) = 1s,a, d = SA, Œª = 0, and recall
by Assumption 3.4 (let‚Äôs assume œÄ‚ãÜis a deterministic policy as it always exists in tabular MDP)
C‚ãÜ:= suph,s,a dœÄ‚ãÜ
h (s, a)/d¬µ
h(s, a), then for Theorem 3.2 ‚àö d ¬∑ H
X"
D,0.8628281117696867,"h=1
EœÄ‚ãÜ
q"
D,0.8636748518204911,"œÜ(¬∑, ¬∑)‚ä§Œõ‚ãÜ‚àí1
h
œÜ(¬∑, ¬∑)

=
‚àö d ¬∑ H
X h=1 X"
D,0.8645215918712955,"s,a
dœÄ‚ãÜ
h (s, a)
q"
D,0.8653683319220999,"1‚ä§
s,aŒõ‚ãÜ‚àí1
h
1s,a =
‚àö SA ¬∑ H
X h=1 X"
D,0.8662150719729044,"s,a
dœÄ‚ãÜ
h (s, a) s"
D,0.8670618120237087,"1‚ä§
s,adiag
VarP¬∑,¬∑(V ‚ãÜ
h+1)
nh,¬∑,¬∑"
D,0.8679085520745131,"
1s,a =
‚àö SA ¬∑ H
X h=1 X"
D,0.8687552921253175,"s,a
dœÄ‚ãÜ
h (s, a) s"
D,0.869602032176122,"VarPs,a(V ‚ãÜ
h+1)
nh,s,a
nh,s,a := K
X"
D,0.8704487722269263,"œÑ=1
1[sœÑ
h, aœÑ
h = s, a] ‚â≤
‚àö SA ¬∑ H
X h=1 X"
D,0.8712955122777307,"s,a
dœÄ‚ãÜ
h (s, a) s"
D,0.8721422523285352,"VarPs,a(V ‚ãÜ
h+1)
K ¬∑ d¬µ
h(s, a)
‚â§
p"
D,0.8729889923793396,"SAC‚ãÜ/K ¬∑ H
X h=1 X s,a q"
D,0.8738357324301439,"dœÄ‚ãÜ
h (s, a)VarPs,a(V ‚ãÜ
h+1) =
p"
D,0.8746824724809483,"SAC‚ãÜ/K ¬∑ H
X h=1 X s q"
D,0.8755292125317528,"dœÄ‚ãÜ
h (s, œÄ‚ãÜ(s))VarPs,œÄ‚ãÜ(s)(V ‚ãÜ
h+1) ‚â§
p"
D,0.8763759525825572,"SAC‚ãÜ/K ¬∑ H
X h=1 s S ¬∑
X"
D,0.8772226926333616,"s
dœÄ‚ãÜ
h (s, œÄ‚ãÜ(s))VarPs,œÄ‚ãÜ(s)(V ‚ãÜ
h+1) ‚â§
p"
D,0.8780694326841659,S2AC‚ãÜ/K ¬∑
D,0.8789161727349704,"v
u
u
tH H
X h=1 X"
D,0.8797629127857748,"s
dœÄ‚ãÜ
h (s, œÄ‚ãÜ(s))VarPs,œÄ‚ãÜ(s)(V ‚ãÜ
h+1) =
p"
D,0.8806096528365792,S2AC‚ãÜ/K ¬∑
D,0.8814563928873835,"v
u
u
tH ¬∑ H
X"
D,0.882303132938188,"h=1
EœÄ‚ãÜ
h[VarP(¬∑,¬∑)(V ‚ãÜ
h+1)] ‚â§
p"
D,0.8831498729889924,H3S2AC‚ãÜ/K
D,0.8839966130397968,"where the Ô¨Årst inequality is by Chernoff bound and the last one is by Lemma 3.4. of Yin and Wang
(2020) (Law of total variances). The rest of them are from Cauchy‚Äôs inequality. Similarly, for
Theorem 3.3, we also have ‚àö d ¬∑ H
X h=1 q"
D,0.8848433530906011,"EœÄ‚ãÜ[œÜ]‚ä§Œõ‚ãÜ‚àí1
h
EœÄ‚ãÜ[œÜ] =
‚àö d ¬∑ H
X h=1 q"
D,0.8856900931414056,"Vec{dœÄ‚ãÜ}Œõ‚ãÜ‚àí1
h
Vec{dœÄ‚ãÜ} =
‚àö d ¬∑ H
X h=1 s"
D,0.88653683319221,"Vec{dœÄ‚ãÜ}diag
VarP¬∑,¬∑(V ‚ãÜ
h+1)
nh,¬∑,¬∑"
D,0.8873835732430144,"
Vec{dœÄ‚ãÜ} =
‚àö SA ¬∑ H
X h=1"
D,0.8882303132938189,"v
u
u
tX"
D,0.8890770533446232,"s,a
dœÄ‚ãÜ
h (s, a)2 VarPs,a(V ‚ãÜ
h+1)
nh,s,a ‚â≤
‚àö SA ¬∑ H
X h=1"
D,0.8899237933954276,"v
u
u
tX"
D,0.890770533446232,"s,a
dœÄ‚ãÜ
h (s, a)2 VarPs,a(V ‚ãÜ
h+1)
K ¬∑ d¬µ
h(s, a) ‚â§
p"
D,0.8916172734970365,"SAC‚ãÜ/K ¬∑ H
X h=1 sX"
D,0.8924640135478408,"s,a
dœÄ‚ãÜ
h (s, a)VarPs,a(V ‚ãÜ
h+1) =
p"
D,0.8933107535986452,"SAC‚ãÜ/K ¬∑ H
X h=1 sX"
D,0.8941574936494496,"s
dœÄ‚ãÜ
h (s, œÄ‚ãÜ(s))VarPs,œÄ‚ãÜ(s)(V ‚ãÜ
h+1) ‚â§
p"
D,0.8950042337002541,SAC‚ãÜ/K ¬∑
D,0.8958509737510584,"v
u
u
tH ¬∑ H
X"
D,0.8966977138018628,"h=1
EœÄ‚ãÜ
h[VarP(¬∑,¬∑)(V ‚ãÜ
h+1)] ‚â§
p"
D,0.8975444538526672,H3SAC‚ãÜ/K.
D,0.8983911939034717,Published as a conference paper at ICLR 2022
D,0.8992379339542761,"H
AUXILIARY LEMMAS"
D,0.9000846740050804,"Lemma H.1 (Matrix McDiarmid inequality / Matrix Chernoff bound (Tropp, 2012)). Let zk, k =
1, . . . , K be independent random vectors in Rd, and let H be a mapping that maps K vectors to a
d √ó d symmetric matrix. Assume there exists a sequence of Ô¨Åxed symmetric matrices {Ak}k‚àà[K] such
that for zk, z‚Ä≤
k ranges over all possible values for each k ‚àà[K], it holds
(H(z1, . . . , zk, . . . , zK) ‚àíH(z1, . . . , z‚Ä≤
k, . . . , zK))2 ‚™ØA2
k.
DeÔ¨Åne œÉ2 :=
P"
D,0.9009314140558848,"k A2
k
. Then for any t > 0,"
D,0.9017781541066893,"P {‚à•H(z1, . . . , zK) ‚àíEH(z1, . . . , zK)‚à•‚â•t} ‚â§d ¬∑ exp
‚àít2 8œÉ2 "
D,0.9026248941574937,"Lemma H.2 (Hoeffding inequality for self-normalized martingales (Abbasi-Yadkori et al., 2011)).
Let {Œ∑t}‚àû
t=1 be a real-valued stochastic process. Let {Ft}‚àû
t=0 be a Ô¨Åltration, such that Œ∑t is Ft-
measurable. Assume Œ∑t also satisÔ¨Åes Œ∑t given Ft‚àí1 is zero-mean and R-subgaussian, i.e."
D,0.903471634208298,"‚àÄŒª ‚ààR,
E

eŒªŒ∑t | Ft‚àí1

‚â§eŒª2R2/2"
D,0.9043183742591024,"Let {xt}‚àû
t=1 be an Rd-valued stochastic process where xt is Ft‚àí1 measurable and ‚à•xt‚à•‚â§L. Let
Œõt = ŒªId + Pt
s=1 xsx‚ä§
s . Then for any Œ¥ > 0, with probability 1 ‚àíŒ¥, for all t > 0, t
X"
D,0.9051651143099069,"s=1
xsŒ∑s  2 Œõ‚àí1
t"
D,0.9060118543607113,‚â§8R2 ¬∑ d
LOG,0.9068585944115156,"2 log
Œª + tL ŒªŒ¥ 
."
LOG,0.90770533446232,"Lemma H.3 (Bernstein inequality for self-normalized martingales (Zhou et al., 2021a)). Let {Œ∑t}‚àû
t=1
be a real-valued stochastic process. Let {Ft}‚àû
t=0 be a Ô¨Åltration, such that Œ∑t is Ft-measurable.
Assume Œ∑t also satisÔ¨Åes
|Œ∑t| ‚â§R, E [Œ∑t | Ft‚àí1] = 0, E

Œ∑2
t | Ft‚àí1

‚â§œÉ2."
LOG,0.9085520745131245,"Let {xt}‚àû
t=1 be an Rd-valued stochastic process where xt is Ft‚àí1 measurable and ‚à•xt‚à•‚â§L. Let
Œõt = ŒªId + Pt
s=1 xsx‚ä§
s . Then for any Œ¥ > 0, with probability 1 ‚àíŒ¥, for all t > 0, t
X"
LOG,0.9093988145639289,"s=1
xsŒ∑s Œõ‚àí1
t ‚â§8œÉ s"
LOG,0.9102455546147333,"d log

1 + tL2 Œªd"
LOG,0.9110922946655376,"
¬∑ log
4t2 Œ¥"
LOG,0.9119390347163421,"
+ 4R log
4t2 Œ¥ "
LOG,0.9127857747671465,"Lemma H.4 (Converting the variance under the matrix norm). Let Œõ1 and Œõ2 ‚ààRd√ód are two
positive semi-deÔ¨Ånite matrices. Then:
Œõ‚àí1
1
 ‚â§
Œõ‚àí1
2
 +
Œõ‚àí1
1
 ¬∑
Œõ‚àí1
2
 ¬∑ ‚à•Œõ1 ‚àíŒõ2‚à•
and"
LOG,0.9136325148179509,"‚à•œÜ‚à•Œõ‚àí1
1
‚â§

1 +
qŒõ‚àí1
2
 ‚à•Œõ2‚à•¬∑
Œõ‚àí1
1
 ¬∑ ‚à•Œõ1 ‚àíŒõ2‚à•

¬∑ ‚à•œÜ‚à•Œõ‚àí1
2
."
LOG,0.9144792548687553,for all œÜ ‚ààRd.
LOG,0.9153259949195597,"Proof. For the Ô¨Årst part, note
Œõ‚àí1
1
 ‚â§
Œõ‚àí1
2
 +
Œõ‚àí1
1
‚àíŒõ‚àí1
2
 ‚â§
Œõ‚àí1
2
 +
Œõ‚àí1
2
 ‚à•Œõ1 ‚àíŒõ2‚à•
Œõ‚àí1
1"
LOG,0.9161727349703641,"For the second one,"
LOG,0.9170194750211685,"‚à•œÜ‚à•Œõ‚àí1
1
=
q"
LOG,0.9178662150719729,"œÜ‚ä§Œõ‚àí1
1 œÜ =
q"
LOG,0.9187129551227773,"œÜ‚ä§ 
Œõ‚àí1
1
‚àíŒõ‚àí1
2

œÜ + œÜ‚ä§Œõ‚àí1
2 œÜ = r"
LOG,0.9195596951735817,"œÜ‚ä§Œõ‚àí1/2
2

Œõ1/2
2
Œõ‚àí1
1 Œõ1/2
2
‚àíI + I

Œõ‚àí1/2
2
œÜ ‚â§ r"
LOG,0.9204064352243861,"‚à•œÜ‚à•Œõ‚àí1
2
¬∑

1 +
Œõ1/2
2
Œõ‚àí1
1 Œõ1/2
2
‚àíI


‚à•œÜ‚à•Œõ‚àí1
2"
LOG,0.9212531752751905,"‚â§

1 +
Œõ1/2
2
Œõ‚àí1
1 Œõ1/2
2
‚àíI

1/2
¬∑ ‚à•œÜ‚à•Œõ‚àí1
2
=

1 +
Œõ1/2
2
Œõ‚àí1
1
(Œõ2 ‚àíŒõ1) Œõ‚àí1
2 Œõ1/2
2

1/2
¬∑ ‚à•œÜ‚à•Œõ‚àí1
2"
LOG,0.9220999153259949,"‚â§

1 +
q"
LOG,0.9229466553767993,"‚à•Œõ2‚à•
Œõ‚àí1
1
 Œõ‚àí1
2
 ‚à•Œõ1 ‚àíŒõ2‚à•

¬∑ ‚à•œÜ‚à•Œõ‚àí1
2"
LOG,0.9237933954276037,Published as a conference paper at ICLR 2022
LOG,0.9246401354784082,"Lemma H.5 (Lemma H.4 of Min et al. (2021)). let œÜ : S √ó A ‚ÜíRd satisÔ¨Åes ‚à•œÜ(s, a)‚à•‚â§C for
all s, a ‚ààS √ó A. For any K > 0, Œª > 0, deÔ¨Åne ¬ØGK = PK
k=1 œÜ(sk, ak)œÜ(sk, ak)‚ä§+ ŒªId where
(sk, ak)‚Äôs are i.i.d samples from some distribution ŒΩ. Then with probability 1 ‚àíŒ¥,

¬ØGK K ‚àíEŒΩ  ¬ØGK K"
LOG,0.9254868755292125," ‚â§4
‚àö 2C2
‚àö K"
LOG,0.9263336155800169,"
log 2d Œ¥"
LOG,0.9271803556308214,"1/2
."
LOG,0.9280270956816258,"Proof of Lemma H.5. For completeness, we provide the proof of Lemma H.5. Let xk = œÜ(sk, ak).
Denote eŒ£h as the matrix obtained by replacing the k-th vector xk in bŒ£h by exk and leaving the rest
K ‚àí1 vectors unchanged. Then
 bŒ£h"
LOG,0.9288738357324301,"K ‚àí
eŒ£h K !2"
LOG,0.9297205757832345,"=
xkx‚ä§
k ‚àíÀúxkÀúx‚ä§
k
K"
LOG,0.930567315834039,"
‚™Ø
1
K2
 
2xkx‚ä§
k xkx‚ä§
k + 2ÀúxkÀúx‚ä§
k ÀúxkÀúx‚ä§
k

‚™Ø4C4"
LOG,0.9314140558848434,"K2 Id := A2
k."
LOG,0.9322607959356477,"Notice that
PK
k A2
k
 = 4C4"
LOG,0.9331075359864521,"K , by Lemma H.1 we have the result."
LOG,0.9339542760372566,"Lemma H.6 (Lemma H.5. of Min et al. (2021)). Let œÜ : S √ó A ‚ÜíRd be a bounded function s.t.
‚à•œÜ‚à•2 ‚â§C. DeÔ¨Åne ¬ØGK = PK
k=1 œÜ(sk, ak)œÜ(sk, ak)‚ä§+ ŒªId where (sk, ak)‚Äôs are i.i.d samples from
some distribution ŒΩ. Let G = EŒΩ[œÜ(s, a)œÜ(s, a)‚ä§]. Then for any Œ¥ ‚àà(0, 1), if K satisÔ¨Åes"
LOG,0.934801016088061,"K ‚â•max

512C4 G‚àí12 log
2d Œ¥"
LOG,0.9356477561388654,"
, 4Œª
G‚àí1

."
LOG,0.9364944961896697,"Then with probability at least 1 ‚àíŒ¥, it holds simultaneously for all u ‚ààRd that"
LOG,0.9373412362404742,"‚à•u‚à•¬Ø
G‚àí1
K ‚â§
2
‚àö"
LOG,0.9381879762912786,"K
‚à•u‚à•G‚àí1 ."
LOG,0.939034716342083,"Lemma H.7 (Extended Value Difference (Section B.1 in Cai et al. (2020))). Let œÄ = {œÄh}H
h=1 and
œÄ‚Ä≤ = {œÄ‚Ä≤
h}H
h=1 be two arbitrary policies and let { bQh}H
h=1 be any given Q-functions. Then deÔ¨Åne
bVh(s) := ‚ü®bQh(s, ¬∑), œÄh(¬∑ | s)‚ü©for all s ‚ààS. Then for all s ‚ààS,"
LOG,0.9398814563928873,"bV1(s) ‚àíV œÄ‚Ä≤
1 (s) = H
X"
LOG,0.9407281964436918,"h=1
EœÄ‚Ä≤
h
‚ü®bQh (sh, ¬∑) , œÄh (¬∑ | sh) ‚àíœÄ‚Ä≤
h (¬∑ | sh)‚ü©| s1 = s
i + H
X"
LOG,0.9415749364944962,"h=1
EœÄ‚Ä≤
h
bQh (sh, ah) ‚àí

Th bVh+1

(sh, ah) | s1 = s
i
(30)"
LOG,0.9424216765453006,"where (ThV )(¬∑, ¬∑) := rh(¬∑, ¬∑) + (PhV )(¬∑, ¬∑) for any V ‚ààRS."
LOG,0.9432684165961049,"Proof. Denote Œæh = bQh ‚àíTh bVh+1. For any h ‚àà[H], we have"
LOG,0.9441151566469094,"bVh ‚àíV œÄ‚Ä≤
h
= ‚ü®bQh, œÄh‚ü©‚àí‚ü®QœÄ‚Ä≤
h , œÄ‚Ä≤
h‚ü©"
LOG,0.9449618966977138,"= ‚ü®bQh, œÄh ‚àíœÄ‚Ä≤
h‚ü©+ ‚ü®bQh ‚àíQœÄ‚Ä≤
h , œÄ‚Ä≤
h‚ü©"
LOG,0.9458086367485182,"= ‚ü®bQh, œÄh ‚àíœÄ‚Ä≤
h‚ü©+ ‚ü®Ph(bVh+1 ‚àíV œÄ‚Ä≤
h+1) + Œæh, œÄ‚Ä≤
h‚ü©"
LOG,0.9466553767993227,"= ‚ü®bQh, œÄh ‚àíœÄ‚Ä≤
h‚ü©+ ‚ü®Ph(bVh+1 ‚àíV œÄ‚Ä≤
h+1), œÄ‚Ä≤
h‚ü©+ ‚ü®Œæh, œÄ‚Ä≤
h‚ü©"
LOG,0.947502116850127,"recursively apply the above for bVh+1 ‚àíV œÄ‚Ä≤
h+1 and use the EœÄ‚Ä≤ notation (instead of the inner product
of Ph, œÄ‚Ä≤
h) we can Ô¨Ånish the prove of this lemma."
LOG,0.9483488569009314,"Lemma H.8. Let bœÄ = {bœÄh}H
h=1 and bQh(¬∑, ¬∑) be the arbitrary policy and Q-function and also
bVh(s) = ‚ü®bQh(s, ¬∑), bœÄh(¬∑|s)‚ü©‚àÄs ‚ààS. and Œ∂h(s, a) := (Th bVh+1)(s, a) ‚àíbQh(s, a) (element-wisely)"
LOG,0.9491955969517358,Published as a conference paper at ICLR 2022
LOG,0.9500423370025403,"to be the Bellman update error. Then for any arbitrary œÄ, we have"
LOG,0.9508890770533446,"V œÄ
1 (s) ‚àíV bœÄ
1 (s) = H
X"
LOG,0.951735817104149,"h=1
EœÄ [Œ∂h(sh, ah) | s1 = s] ‚àí H
X"
LOG,0.9525825571549534,"h=1
EbœÄ [Œ∂h(sh, ah) | s1 = s] + H
X"
LOG,0.9534292972057579,"h=1
EœÄ
h
‚ü®bQh (sh, ¬∑) , œÄh (¬∑|sh) ‚àíbœÄh (¬∑|sh)‚ü©| s1 = x
i"
LOG,0.9542760372565622,"where the expectation are taken over sh, ah."
LOG,0.9551227773073666,Proof. Note the gap can be rewritten as
LOG,0.955969517358171,"V œÄ
1 (s) ‚àíV bœÄ
1 (s) = V œÄ
1 (s) ‚àíbV1(s) + bV1(s) ‚àíV bœÄ
1 (s)."
LOG,0.9568162574089755,"By Lemma H.7 with œÄ = bœÄ, œÄ‚Ä≤ = œÄ, we directly have"
LOG,0.9576629974597799,"V œÄ
1 (s)‚àíbV1(s) = H
X"
LOG,0.9585097375105842,"h=1
EœÄ [Œ∂h(sh, ah) | s1 = s]+ H
X"
LOG,0.9593564775613886,"h=1
EœÄ
h
‚ü®bQh (sh, ¬∑) , œÄh (¬∑|sh) ‚àíbœÄh (¬∑|sh)‚ü©| s1 = s
i"
LOG,0.9602032176121931,"(31)
Next apply Lemma H.7 again with œÄ = œÄ‚Ä≤ = bœÄ, we directly have"
LOG,0.9610499576629975,"bV1(s) ‚àíV bœÄ
1 (s) = ‚àí H
X"
LOG,0.9618966977138018,"h=1
EbœÄ [Œ∂h(sh, ah) | s1 = s] .
(32)"
LOG,0.9627434377646062,Combine the above two results we prove the stated result.
LOG,0.9635901778154107,"Lemma H.9. For a linear MDP, for any 0 ‚â§V (¬∑) ‚â§H, then there exists a wh ‚ààRd s.t. ThV =
‚ü®œÜ, wh‚ü©and ‚à•wh‚à•2 ‚â§2H
‚àö"
LOG,0.9644369178662151,"d for all h ‚àà[H]. Here Th(V )(s, a) = rh(x, a) + (PhV )(s, a).
Similarly, for any œÄ, there exists wœÄ
h ‚ààRd, such that QœÄ
h = ‚ü®œÜ, wœÄ
h‚ü©with ‚à•wœÄ
h‚à•2 ‚â§2(H ‚àíh + 1)
‚àö d."
LOG,0.9652836579170194,"Proof. By deÔ¨Ånition,"
LOG,0.9661303979678239,"ThV = rh + (PhV ) = ‚ü®œÜ, Œ∏h‚ü©+ ‚ü®œÜ,
Z"
LOG,0.9669771380186283,"S
V (s)dŒΩh(s)‚ü©"
LOG,0.9678238780694327,"‚áíwh = Œ∏h +
Z"
LOG,0.9686706181202371,"S
V (s)dŒΩh(s),"
LOG,0.9695173581710415,"therefore ‚à•wh‚à•2 ‚â§‚à•Œ∏h‚à•2 + H ¬∑ ‚à•ŒΩh(S)‚à•‚â§1 + H
‚àö"
LOG,0.9703640982218459,"d ‚â§2H
‚àö"
LOG,0.9712108382726503,"d. The proof of the second part is
similar by backward induction and the fact V œÄ
h ‚â§H ‚àíh + 1 for any œÄ."
LOG,0.9720575783234547,"Lemma H.10. For any pessimistic bonus design Œìh, suppose K > max{M1, M2, M3, M4}, then
with probability 1 ‚àíŒ¥, Algorithm 1 yields"
LOG,0.9729043183742591,"Th bVh+1 ‚àíbTh bVh+1

‚àû‚â§eO(H2p d/Œ∫
‚àö K
)"
LOG,0.9737510584250635,Published as a conference paper at ICLR 2022
LOG,0.9745977984758679,"Proof of Lemma H.10. Suppose wh is the coefÔ¨Åcient corresponding to the Th bVh+1 (such wh exists
by Lemma H.9), i.e. Th bVh+1 = œÜ‚ä§wh, and recall (bTh bVh+1)(s, a) = œÜ(s, a)‚ä§bwh, then:

Th bVh+1

(s, a) ‚àí

bTh bVh+1

(s, a) = œÜ(s, a)‚ä§(wh ‚àíbwh)"
LOG,0.9754445385266723,"=œÜ(s, a)‚ä§wh ‚àíœÜ(s, a)‚ä§bŒõ‚àí1
h K
X"
LOG,0.9762912785774767,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

rœÑ
h + bVh+1
 
sœÑ
h+1

/bœÉ2
h(sœÑ
h, aœÑ
h) !"
LOG,0.9771380186282811,"= œÜ(s, a)‚ä§wh ‚àíœÜ(s, a)‚ä§bŒõ‚àí1
h K
X"
LOG,0.9779847586790855,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

Th bVh+1

(sœÑ
h, aœÑ
h) /bœÉ2
h(sœÑ
h, aœÑ
h) !"
LOG,0.97883149872989,"|
{z
}
(i)"
LOG,0.9796782387806944,"+ œÜ(s, a)‚ä§bŒõ‚àí1
h K
X"
LOG,0.9805249788314987,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

rœÑ
h + bVh+1
 
sœÑ
h+1

‚àí

Th bVh+1

(sœÑ
h, aœÑ
h)

/bœÉ2
h(sœÑ
h, aœÑ
h) !"
LOG,0.9813717188823031,"|
{z
}
(ii) . (33)"
LOG,0.9822184589331076,"For term (i), it is bounded by 2ŒªH3‚àö"
LOG,0.983065198983912,"d/Œ∫
K
with probability 1 ‚àíŒ¥ by Lemma C.2."
LOG,0.9839119390347163,"For term (ii), by Cauchy inequality it is bounded by"
LOG,0.9847586790855207,"‚à•œÜ(s, a)‚à•bŒõ‚àí1
h ¬∑  K
X"
LOG,0.9856054191363252,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

rœÑ
h + bVh+1
 
sœÑ
h+1

‚àí

Th bVh+1

(sœÑ
h, aœÑ
h)

/bœÉ2
h(sœÑ
h, aœÑ
h)"
LOG,0.9864521591871296,"bŒõ‚àí1
h ‚â§2H
‚àö Œ∫K  K
X"
LOG,0.9872988992379339,"œÑ=1
œÜ (sœÑ
h, aœÑ
h) ¬∑

rœÑ
h + bVh+1
 
sœÑ
h+1

‚àí

Th bVh+1

(sœÑ
h, aœÑ
h)

/bœÉ2
h(sœÑ
h, aœÑ
h)"
LOG,0.9881456392887383,"bŒõ‚àí1
h ‚â§2H
‚àö"
LOG,0.9889923793395428,"Œ∫K
¬∑ ÀúO(H
‚àö"
LOG,0.9898391193903472,"d) = eO(H2p d/Œ∫
‚àö K
),"
LOG,0.9906858594411516,"where the Ô¨Årst inequality is by Lemma H.6 (with œÜ‚Ä≤ = œÜ/bœÉh and ‚à•œÜ/bœÉh‚à•‚â§‚à•œÜ‚à•‚â§1 := C)
and the third inequality uses
‚àö"
LOG,0.9915325994919559,"a‚ä§¬∑ A ¬∑ a ‚â§
p"
LOG,0.9923793395427604,"‚à•a‚à•2 ‚à•A‚à•2 ‚à•a‚à•2 = ‚à•a‚à•2
p"
LOG,0.9932260795935648,‚à•A‚à•2 with a to be either
LOG,0.9940728196443692,"œÜ or wh. Moreover, Œªmin(ÀúŒõp
h) ‚â•Œ∫/ maxh,s,a bœÉh(s, a)2 ‚â•Œ∫/H2 implies
(ÀúŒõp
h)‚àí1 ‚â§H2/Œ∫."
LOG,0.9949195596951735,"The second inequality comes from Lemma H.2 with R = H since |Œ∑œÑ| = |(rœÑ
h + bVh+1
 
sœÑ
h+1

‚àí
(Th bVh+1)(sœÑ
h, aœÑ
h))/bœÉh(sœÑ
h, aœÑ
h)| ‚â§H and |xœÑ| = |œÜ(sœÑ
h, aœÑ
h)/bœÉh(sœÑ
h, aœÑ
h)| ‚â§1."
LOG,0.995766299745978,"The
Ô¨Ånal
result
is
obtained
by
absorbing
the
term
(i)
via
the
condition
K
>
max{M1, M2, M3, M4}."
LOG,0.9966130397967824,"Lemma H.11. Suppose random variables ‚à•X‚à•‚àû‚â§2H, ‚à•Y ‚à•‚àû‚â§2H, then"
LOG,0.9974597798475868,|Var(X) ‚àíVar(Y )| ‚â§8H ¬∑ ‚à•X ‚àíY ‚à•‚àû.
LOG,0.9983065198983911,Proof of Lemma H.11.
LOG,0.9991532599491956,"|Var(X) ‚àíVar(Y )| =|E[X2] ‚àíE[Y 2] ‚àí(E[X]2 ‚àíE[Y ]2)| = |E[(X + Y )(X ‚àíY )] ‚àí(E[X + Y ])(E[X ‚àíY ])|
‚â§E[|X + Y | ¬∑ |X ‚àíY |] + 4H ¬∑ ‚à•X ‚àíY ‚à•‚àû
‚â§4HE[|X ‚àíY |] + 4H ¬∑ ‚à•X ‚àíY ‚à•‚àû= 8H ¬∑ ‚à•X ‚àíY ‚à•‚àû."
