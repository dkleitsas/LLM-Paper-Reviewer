Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001040582726326743,"Deep learning-based cortical surface reconstruction (CSR) approaches typically
1"
ABSTRACT,0.002081165452653486,"rely on supervision information provided by pseudo ground truth generated by
2"
ABSTRACT,0.003121748178980229,"conventional CSR methods, subject to errors associated with the supervision in-
3"
ABSTRACT,0.004162330905306972,"formation and also increasing computational cost of training data preparation.We
4"
ABSTRACT,0.005202913631633715,"propose a new method to jointly reconstruct multiple cortical surfaces using weak
5"
ABSTRACT,0.006243496357960458,"supervision from brain MRI ribbon segmentation results. Our approach initializes a
6"
ABSTRACT,0.007284079084287201,"midthickness surface, which is then deformed inward and outward to form the inner
7"
ABSTRACT,0.008324661810613945,"(white matter) and outer (pial) cortical surfaces, respectively, by jointly learning
8"
ABSTRACT,0.009365244536940686,"diffeomorphic flows by minimizing loss functions to optimize the surfaces towards
9"
ABSTRACT,0.01040582726326743,"the boundaries of the cortical ribbon segmentation maps. Specifically, a boundary
10"
ABSTRACT,0.011446409989594173,"surface loss drives the initialization surface to the inner and outer boundaries, while
11"
ABSTRACT,0.012486992715920915,"an inter-surface normal consistency loss regularizes the pial surface in challenging
12"
ABSTRACT,0.013527575442247659,"deep cortical sulci regions. Additional regularization terms are utilized to enforce
13"
ABSTRACT,0.014568158168574402,"edge length uniformity and smoothness of the reconstructed surfaces. Our method
14"
ABSTRACT,0.015608740894901144,"has been evaluated on two large-scale adult brain MRI datasets and one infant brain
15"
ABSTRACT,0.01664932362122789,"MRI dataset, demonstrating comparable or superior performance in CSR in terms
16"
ABSTRACT,0.01768990634755463,"of accuracy and surface regularity compared to alternative supervised deep learning
17"
ABSTRACT,0.018730489073881373,"methods.
18"
INTRODUCTION,0.019771071800208116,"1
Introduction
19"
INTRODUCTION,0.02081165452653486,"Cortical surface reconstruction (CSR) is a crucial step for both qualitative visualization and quan-
20"
INTRODUCTION,0.021852237252861603,"titative characterization of cortical surfaces in imaging studies of brain morphology [15, 51], neu-
21"
INTRODUCTION,0.022892819979188347,"rodegenerative diseases [6, 12, 43], and psychological disorders [42]. Well-established cortical
22"
INTRODUCTION,0.023933402705515087,"analysis pipelines, such as BrainSuite [48], FreeSurfer [17], Connectome Workbench [18], and
23"
INTRODUCTION,0.02497398543184183,"iBEAT V2.0 [52], have achieved significant success in reconstructing cortical surfaces from brain
24"
INTRODUCTION,0.026014568158168574,"MRI data. However, these pipelines typically involve multiple processing steps, including iterative
25"
INTRODUCTION,0.027055150884495317,"surface deformation and topology check and correction, resulting in lengthy processing time (e.g.,
26"
INTRODUCTION,0.02809573361082206,"∼6h/subject). Moreover, each pipeline requires meticulously tuned parameters, posing challenges for
27"
INTRODUCTION,0.029136316337148804,"generalization across diverse data domains, age groups, or acquisition protocols.
28"
INTRODUCTION,0.030176899063475548,"Deep learning (DL) approaches have significantly accelerated CSR, demonstrating orders of magni-
29"
INTRODUCTION,0.031217481789802288,"tude faster inference speeds while maintaining high accuracy and topology correctness [8, 11, 13,
30"
INTRODUCTION,0.03225806451612903,"22, 26, 30–32, 41, 47, 54]. One line of research predicts implicit surface representations, such as
31"
INTRODUCTION,0.03329864724245578,"signed distance functions [13, 21] or level sets [41], from which 3D meshes are extracted using the
32"
INTRODUCTION,0.03433922996878252,"Marching Cube (MC) algorithm [27] and refined with topology correction algorithms [4] to detect
33"
INTRODUCTION,0.03537981269510926,"and rectify topology errors, ensuring that the reconstructed surface conforms to a sphere-like topology.
34"
INTRODUCTION,0.036420395421436005,"Another line of research focuses on learning explicit surface deformations, using methods such as
35"
INTRODUCTION,0.037460978147762745,"flow-based [8, 11, 22, 26, 47, 54] or NODE-based techniques [30, 31]), to deform an initial mesh
36"
INTRODUCTION,0.03850156087408949,"towards target cortical surfaces. However, all these methods heavily rely on supervision information
37"
INTRODUCTION,0.03954214360041623,"provided by pseudo ground truth (pGT) of cortical surfaces generated by conventional CSR methods ,
38"
INTRODUCTION,0.04058272632674298,"regardless of whether they use implicit or explicit surface representations. The prolonged processing
39"
INTRODUCTION,0.04162330905306972,"time for generating pGT surfaces limits the collection of sufficiently large datasets for training,
40"
INTRODUCTION,0.04266389177939646,"and a general pipeline capable of extracting pGT surfaces across various data domains (e.g., age,
41"
INTRODUCTION,0.043704474505723206,"modality) is currently lacking. Conversely, segmentation of brain structures is comparatively simpler,
42"
INTRODUCTION,0.044745057232049947,"inspiring us to explore avenues to eliminate the need for supervised learning in CSR and to generalize
43"
INTRODUCTION,0.045785639958376693,"DL-based CSR approaches to scenarios where ribbon segmentation results are readily available.
44"
INTRODUCTION,0.046826222684703434,"The key challenges for achieving accurate weakly supervised reconstruction of cortical surfaces
45"
INTRODUCTION,0.047866805411030174,"span three primary aspects. First, devising sub-voxel supervision signals presents a formidable
46"
INTRODUCTION,0.04890738813735692,"hurdle. While existing approaches can produce precise segmentations [7, 20, 29, 45, 52], voxel-level
47"
INTRODUCTION,0.04994797086368366,"representations may struggle to capture the intricate morphology of the cerebral cortex, especially
48"
INTRODUCTION,0.05098855359001041,"its thin and highly-folded structure, due to the partial volume effect (PVE) inherent in brain MRI
49"
INTRODUCTION,0.05202913631633715,"scans. This problem becomes particularly prominent in deep cortical sulci [17], where the two banks
50"
INTRODUCTION,0.053069719042663895,"of grooves nearly converge, or in low-resolution images [52], such as under-sampled or infant MRIs.
51"
INTRODUCTION,0.054110301768990635,"Second, effectively modeling the interdependence between multiple surfaces is crucial. Incorporating
52"
INTRODUCTION,0.055150884495317375,"this prior knowledge into the design of models and training algorithms can alleviate the complexity
53"
INTRODUCTION,0.05619146722164412,"of reconstructing both the inner (white matter) and outer (pial) surfaces, ensuring the spherical
54"
INTRODUCTION,0.05723204994797086,"topology of the reconstructed surfaces [8, 54]. However, in the absence of pGT, it becomes more
55"
INTRODUCTION,0.05827263267429761,"challenging to forcibly deform surfaces and less stable to optimize multiple surfaces concurrently.
56"
INTRODUCTION,0.05931321540062435,"Third, maintaining optimal surface topology is paramount. Mesh uniformity, smoothness, and
57"
INTRODUCTION,0.060353798126951096,"topology are susceptible to distortion during large deformations if networks are optimized based on
58"
INTRODUCTION,0.061394380853277836,"randomly sampled vertices in 3D space for dense volumetric fields.
59"
INTRODUCTION,0.062434963579604576,"In this paper, we introduce SegCSR, a novel weakly supervised DL framework aimed at reconstructing
60"
INTRODUCTION,0.06347554630593132,"multiple cortical surfaces using ribbon segmentations derived from brain MRIs. We address the
61"
INTRODUCTION,0.06451612903225806,"diffeomorphic deformation problem in a continues coordinate space, deforming the initialization
62"
INTRODUCTION,0.06555671175858481,"midthickness surface towards the target inner and outer surfaces via innovative loss functions.
63"
INTRODUCTION,0.06659729448491156,"Specifically, the boundary surface loss function based on the ribbon segmentations and the intensity
64"
INTRODUCTION,0.06763787721123829,"gradient loss function based on the raw image facilitate sub-voxel-level surface movement. The
65"
INTRODUCTION,0.06867845993756504,"inter-surface normal consistency loss function explicitly integrates the normal directions of the WM,
66"
INTRODUCTION,0.06971904266389178,"midthickness, and pial surfaces, thereby regularizing the pial surface in challenging deep cortical
67"
INTRODUCTION,0.07075962539021852,"sulci regions. Furthermore, we devise a customized edge length loss, in conjunction with the known
68"
INTRODUCTION,0.07180020811654526,"normal consistency loss, to ensure surface uniformity and smoothness. Our main contributions can
69"
INTRODUCTION,0.07284079084287201,"be summarized as follows:
70"
INTRODUCTION,0.07388137356919876,"• We propose a new weakly supervised paradigm for reconstructing multiple cortical surfaces,
71"
INTRODUCTION,0.07492195629552549,"reducing the dependence on pGT cortical surfaces in training, unlike existing DL methods.
72"
INTRODUCTION,0.07596253902185224,"• We design two loss functions to optimize the surfaces towards the boundary of the cortical
73"
INTRODUCTION,0.07700312174817898,"ribbon segmentation maps, along with regularization terms to enforce regularity of surfaces.
74"
INTRODUCTION,0.07804370447450572,"• We conduct extensive experiments on two large-scale adult brain MRI datasets and one
75"
INTRODUCTION,0.07908428720083246,"infant brain MRI dataset. Our new method achieves comparable or superior performance
76"
INTRODUCTION,0.08012486992715921,"compared to existing supervised DL-based CSR alternatives.
77"
RELATED WORKS,0.08116545265348596,"2
Related Works
78"
RELATED WORKS,0.08220603537981269,"Cortical Surface Reconstruction (CSR). (I) Traditional CSR methods typically rely on empirically
79"
RELATED WORKS,0.08324661810613944,"defined automatic image/surface processing techniques to accomplish tissue segmentation (e.g., WM,
80"
RELATED WORKS,0.08428720083246619,"GM, cerebrospinal fluid (CSF)), hemisphere separation, subcortical filling, topology correction, WM
81"
RELATED WORKS,0.08532778355879292,"surface reconstruction, and pial surface reconstruction sequentially. Established pipelines such as
82"
RELATED WORKS,0.08636836628511967,"FreeSurfer [17], BrainSuite [48], and HCP [18] are tailored for processing adult brain images, while
83"
RELATED WORKS,0.08740894901144641,"dHCP [34] and iBEAT V2.0 [52] are designed for neonatal brain images, which exhibit distinct
84"
RELATED WORKS,0.08844953173777315,"differences in intensity values, size, and shape compared to adult brains. Despite achieving sub-voxel
85"
RELATED WORKS,0.08949011446409989,"accuracy and maintaining spherical topology, the iterative surface deformation and topology check and
86"
RELATED WORKS,0.09053069719042664,"correction procedures lead to lengthy processing times. (II) DL-based CSR methods have significantly
87"
RELATED WORKS,0.09157127991675339,"enhanced reconstruction speed while preserving high accuracy. Approaches like SegRecon [19] and
88"
RELATED WORKS,0.09261186264308012,"DeepCSR [13] predict a signed distance map for implicit surface representation, embedding the target
89"
RELATED WORKS,0.09365244536940687,"surface as the zero level-set and extracting it using MC algorithms. However, these methods require
90"
RELATED WORKS,0.09469302809573361,"topology correction to eliminate artifacts and ensure spherical topology. Alternatively, PialNN [32],
91"
RELATED WORKS,0.09573361082206035,"TopoFit [22], Vox2cortex [8], the CorticalFlow series [26, 47], SurfFlow [11], CortexODE [31],
92"
RELATED WORKS,0.0967741935483871,"and CoCSR [54] leverage explicit representation to maintain good topology and overcome PVE by
93"
RELATED WORKS,0.09781477627471384,"learning volumetric or vertex-wise diffeomorphic deformations and progressively deforming genus-0
94"
RELATED WORKS,0.09885535900104059,"template meshes. However, both implicit and explicit methods heavily rely on the supervision of pGT
95"
RELATED WORKS,0.09989594172736732,"of cortical surfaces generated by traditional pipelines. Our proposed method is based on the explicit
96"
RELATED WORKS,0.10093652445369407,"representation but differs significantly from them by utilizing ribbon segmentation maps for weakly
97"
RELATED WORKS,0.10197710718002082,"supervising the model training process.
98"
RELATED WORKS,0.10301768990634755,"Weakly-/Un-supervised Mesh Reconstruction. Although geometric DL methods for general
99"
RELATED WORKS,0.1040582726326743,"computer vision tasks have been extensively studied, research on mesh reconstruction from 3D
100"
RELATED WORKS,0.10509885535900104,"images under weakly-/un-supervised settings is relatively underexplored. One approach involves
101"
RELATED WORKS,0.10613943808532779,"constructing mesh-to-image rasterizer loss functions, as demonstrated in [36], where 2D projection
102"
RELATED WORKS,0.10718002081165452,"views are extracted from predicted 3D meshes and compared with ground truth segmentations.
103"
RELATED WORKS,0.10822060353798127,"Another line of research, exemplified by [39], focuses on learning the correspondence between
104"
RELATED WORKS,0.10926118626430802,"a template image and a target image, which is then utilized to deform the template mesh to the
105"
RELATED WORKS,0.11030176899063475,"target location. However, these methods have primarily been applied to biomedical tasks involving
106"
RELATED WORKS,0.1113423517169615,"organs with relatively simple shapes, such as the liver and heart. But the cerebral cortex presents a
107"
RELATED WORKS,0.11238293444328824,"highly-folded thin structure with a significantly complex shape, necessitating more advanced methods.
108"
RELATED WORKS,0.11342351716961499,"Diffeomorphic Deformation. Diffeomorphic deformation is a spatial transformation that guarantees
109"
RELATED WORKS,0.11446409989594172,"both smoothness and invertibility in the mapping process [46]. It has been widely used in the
110"
RELATED WORKS,0.11550468262226847,"modeling and analysis of brain morphometry, including image registration and surface reconstruction
111"
RELATED WORKS,0.11654526534859522,"tasks. LDDMM [5] computes diffeomorphic deformation based on a time-dependent velocity vector
112"
RELATED WORKS,0.11758584807492195,"field, while Arsigny et al. [2] employ a stationary velocity field (SVF) in conjunction with the
113"
RELATED WORKS,0.1186264308012487,"scaling and squaring method to reduce computation complexity. Learning-based methods [3, 28,
114"
RELATED WORKS,0.11966701352757544,"38] improve the computation efficiency, with regularizations such as smoothness [3] and inverse-
115"
RELATED WORKS,0.12070759625390219,"consistency [38] enchancing the diffeomorphic property of the deformation. In the CSR task,
116"
RELATED WORKS,0.12174817898022892,"diffeomorphic deformation strategies have been adopted to solve an ordinary differential equation
117"
RELATED WORKS,0.12278876170655567,"(ODE) modeling the trajectories of each vertex of a surface. For instance, CoticalFlow methods [26,
118"
RELATED WORKS,0.12382934443288242,"47] propose solving the ODE vertex-wise and derive a numerical condition to ensure homeomorphism
119"
RELATED WORKS,0.12486992715920915,"of integration by training a chain of diffeomorphic deformation models in sequential stages. Recently,
120"
RELATED WORKS,0.1259105098855359,"with the advances in neural ODE solver [10], CortexODE [31] parameterizes the trajectories of
121"
RELATED WORKS,0.12695109261186263,"vertices on the surface as ODEs and proposes a pipeline to reconstruct WM and pial surfaces
122"
RELATED WORKS,0.1279916753381894,"sequentially. Our method builds upon these works [31, 47, 54] and integrates multiple CSR tasks
123"
RELATED WORKS,0.12903225806451613,"into a single framework, leveraging the efficiency and diffeomorphic properties of these strategies.
124"
METHODOLOGY,0.13007284079084286,"3
Methodology
125"
METHODOLOGY,0.13111342351716962,"Our proposed framework, depicted in Fig. 1, is designed to reconstruct multiple cortical surfaces
126"
METHODOLOGY,0.13215400624349635,"simultaneously, eliminating the dependency on pGT generated by conventional and time-consuming
127"
METHODOLOGY,0.1331945889698231,"CSR pipelines. We leverage as weak supervision the brain ribbon segmentation maps that are less
128"
METHODOLOGY,0.13423517169614985,"accurate than pGT surfaces but more accessible. Section3.1 outlines the network structure that couples
129"
METHODOLOGY,0.13527575442247658,"multiple cortical surfaces to reduce the learning difficulty. Section 3.2 describes the loss functions
130"
METHODOLOGY,0.13631633714880334,"devised to supervise the network optimization, facilitating sub-voxel reconstruction accuracy and
131"
METHODOLOGY,0.13735691987513007,"preserving optimal surface topology.
132"
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.1383975026014568,"3.1
Coupled Cortical Surface Reconstruction
133"
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.13943808532778357,"Existing supervised methods require pGT obtained from traditional CSR pipelines to provide precise
134"
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.1404786680541103,"sub-voxel supervision. They can effectively learn the deformation field, even from distant initial
135"
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.14151925078043703,"locations, to accurately align the initialization surface with the target surfaces [11, 26, 47]. However,
136"
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.1425598335067638,"brain ribbon segmentation maps are inherently discrete voxel grids, offering much coarser supervision.
137"
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.14360041623309053,"Consequently, the selection of the initialization surface becomes more critical. Moreover, given the
138"
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.14464099895941726,"intricate folded patterns of the cerebral cortex, the proximity of the two banks of grooves in deep
139"
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.14568158168574402,"cortical sulci often poses a considerable risk of generating topology errors (e.g., handles, holes) in the
140"
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.14672216441207075,"reconstructed surfaces. Conversely, voxels closer to the WM surface exhibit clearer contrast, enabling
141"
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.14776274713839752,"a distinct separation between sulci (Fig. 2 (b)). Thus, following [54], we opt for the midthickness
142"
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.14880332986472425,"layer, positioned midway between the WM and pial surfaces, to serve as a connection for coupling
143"
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.14984391259105098,"the reconstructions of both surfaces and achieve a balanced performance for both surfaces.
144"
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.15088449531737774,"As illustrated in Fig. 1, SegCSR employs a neural network to jointly model three diffeomorphic flows:
145"
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.15192507804370448,"Fθ(I, S0) = (vm, vo, vi). Here, I represents a multi-channel input consisting of brain MRI, cortical
146 Mesh loss"
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.1529656607700312,Multi-channel input
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.15400624349635797,Velocity Field 𝑉𝐹!
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.1550468262226847,"Velocity Field 𝑉𝐹"""
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.15608740894901144,"Initial 
mid surf."
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.1571279916753382,"Mid 
surf."
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.15816857440166493,WM surf. 𝑆!
COUPLED CORTICAL SURFACE RECONSTRUCTION,0.15920915712799166,"GM surf. 𝑆"" S S S 𝑉𝐹#
S S
𝑉𝐹"" 𝑳𝒄𝒚𝒄 𝑳𝒄𝒚𝒄 𝑆! "" 𝑆! """""
D CNN,0.16024973985431842,3D CNN
D CNN,0.16129032258064516,Velocity Field 𝑉𝐹#
D CNN,0.16233090530697192,Deform
D CNN,0.16337148803329865,Deform
D CNN,0.16441207075962538,Deform
D CNN,0.16545265348595214,Deform
D CNN,0.16649323621227888,"Deform 𝑆%
C"
D CNN,0.1675338189386056,"Brain MRI
Ribbon seg.
SDFs"
D CNN,0.16857440166493237,"Weakly 
supervised loss"
D CNN,0.1696149843912591,"Weakly 
supervised loss"
D CNN,0.17065556711758584,Intensity grad. loss
D CNN,0.1716961498439126,"Inter-
mesh 
NC loss"
D CNN,0.17273673257023933,"Mesh 
quality loss"
D CNN,0.17377731529656607,"S
Concatenation
Data flow
C
Vertex-wise sampling"
D CNN,0.17481789802289283,"pGT surf
Weakly supervised loss terms:"
D CNN,0.17585848074921956,"Figure 1: The SegCSR framework overview. SegCSR takes as input a brain MRI image, cortical
ribbon segmentation maps, and signed distance maps of cortical surfaces, and simultaneously learns
three diffeomorphic deformations to optimize the initial midthickness surface S0 to align with the
target midthickness surface SM, and then deform SM outwards and inwards to the pial surface SG and
the WM surface SW , respectively. The model is optimized using weakly supervised loss functions:
the mesh loss guides the surfaces towards the boundaries of the cortical ribbon segmentation maps; the
inter-surface normal consistency loss regularizes the pial surface in deep cortical sulci; the intensity
gradient loss facilitates sub-voxel-level movement; and additional regularization terms control the
deformation trajectories of multiple surfaces as well as the uniformity and smoothness of the surfaces."
D CNN,0.1768990634755463,"ribbon masks, and signed distance functions (SDFs); S0 denotes the initialization midthickness
147"
D CNN,0.17793964620187305,"surface; and vm, vo, vi correspond to the velocity fields that drive S0 towards the true midthickness
148"
D CNN,0.17898022892819979,"surface SM, outward to the pial surface SG, and inward to the WM surface SW , respectively. The
149"
D CNN,0.18002081165452655,"SegCSR establishes an explicit one-to-one mapping between multiple surfaces and is trained by
150"
D CNN,0.18106139438085328,"minimizing weakly supervised losses between the predicted mesh and the ribbon segmentations.
151"
D CNN,0.18210197710718,"The diffeomorphic deformation between the initialization surface and the target surface can be
152"
D CNN,0.18314255983350677,"computed as the integration of an ODE [1] based on the velocity field v:
153"
D CNN,0.1841831425598335,"dΦ(x, t)"
D CNN,0.18522372528616024,"dt
= v(Φ(x, t), t) s.t. Φ(x, 0) = x(0), and thus Φ(x, t) = x(0) +
Z t"
D CNN,0.186264308012487,"o
v(Φ(x, s), s)ds, (1)"
D CNN,0.18730489073881373,"where Φ(x, t) defines a trajectory from the source position x(0) = Φ(x, 0) to the target position
154"
D CNN,0.18834547346514047,"x(1) = Φ(x, 1). According to the Cauchy-Lipschitz theorem [50], if the velocity field is Lipschitz
155"
D CNN,0.18938605619146723,"continuous, the resulting mapping Φ is bijective with continuous inverse (i.e., a diffeomorphism).
156"
D CNN,0.19042663891779396,"To solve this initial value problem, we perform the integration on the predicted velocity fields
157"
D CNN,0.1914672216441207,"using standard numerical integration techniques, such as the Euler method and the Runge-Kutta
158"
D CNN,0.19250780437044746,"method [9]. Specifically, for each integration step t ∈[0, 1], each vertex’s coordinates can be updated
159"
D CNN,0.1935483870967742,"by x(t+1) = x(t) + hv(Φ(x, t), t), where h = 1"
D CNN,0.19458896982310095,"T is the step size and T is the total time steps, and
160"
D CNN,0.19562955254942768,"the velocity vector v for a vertex is trilinearly interpolated from its neighboring velocity vectors [54].
161"
WEAK SUPERVISION LOSS FUNCTIONS,0.19667013527575442,"3.2
Weak Supervision Loss Functions
162"
WEAK SUPERVISION LOSS FUNCTIONS,0.19771071800208118,"Mesh Loss. Weak supervision for SegCSR is derived from cortical ribbon segmentation maps of
163"
WEAK SUPERVISION LOSS FUNCTIONS,0.1987513007284079,"WM and GM (see Fig. 1, the filled interior area of WM and pial surfaces), which can be obtained
164"
WEAK SUPERVISION LOSS FUNCTIONS,0.19979188345473464,"from existing segmentation approaches [7, 20, 29, 45, 52]. Although these ribbon segmentation
165"
WEAK SUPERVISION LOSS FUNCTIONS,0.2008324661810614,"maps do not perfectly represent the intricate pial surface, the WM surface is relatively easier to
166"
WEAK SUPERVISION LOSS FUNCTIONS,0.20187304890738814,"recognize due to its clear local intensity contrast, providing a better-separable boundary (see Fig. 2
167"
WEAK SUPERVISION LOSS FUNCTIONS,0.20291363163371487,"(a-b)). Therefore, we use the boundary of the pGT WM segmentation to supervise the WM surface
168"
WEAK SUPERVISION LOSS FUNCTIONS,0.20395421436004163,"reconstruction. Inspired by [31, 54], we generate an SDF for the WM surface by using a distance
169"
WEAK SUPERVISION LOSS FUNCTIONS,0.20499479708636836,"transform algorithm, where voxels with values of zero represent the surface boundaries and voxels
170"
WEAK SUPERVISION LOSS FUNCTIONS,0.2060353798126951,"with negative or positive values encode their distances to the surface boundaries inward or outward,
171"
WEAK SUPERVISION LOSS FUNCTIONS,0.20707596253902186,"(a) Raw image
(b) Cortical surfaces
(c-1) 
Bi-directional"
WEAK SUPERVISION LOSS FUNCTIONS,0.2081165452653486,Chamfer loss
WEAK SUPERVISION LOSS FUNCTIONS,0.20915712799167535,"(c-2) 
Uni-directional"
WEAK SUPERVISION LOSS FUNCTIONS,0.21019771071800208,Chamfer loss
WEAK SUPERVISION LOSS FUNCTIONS,0.21123829344432882,"(d) Inter-mesh 
normal consistency loss"
WEAK SUPERVISION LOSS FUNCTIONS,0.21227887617065558,"(e) Intensity gradient loss
(f) Cycle consistency loss 𝐴
𝜇
𝜇 𝜇= 2 !"
WEAK SUPERVISION LOSS FUNCTIONS,0.2133194588969823,"""
(g) Customized 
edge length loss !!"" !! !"""
WEAK SUPERVISION LOSS FUNCTIONS,0.21436004162330904,Pial Surf.
WEAK SUPERVISION LOSS FUNCTIONS,0.2154006243496358,WM Surf.
WEAK SUPERVISION LOSS FUNCTIONS,0.21644120707596254,"Mid Surf. !""#$"
WEAK SUPERVISION LOSS FUNCTIONS,0.21748178980228927,"!""#$′
!!"" !""#$"
WEAK SUPERVISION LOSS FUNCTIONS,0.21852237252861603,"!""#$′′"
WEAK SUPERVISION LOSS FUNCTIONS,0.21956295525494277,Pial Surf.
WEAK SUPERVISION LOSS FUNCTIONS,0.2206035379812695,Mid Surf.
WEAK SUPERVISION LOSS FUNCTIONS,0.22164412070759626,"WM Surf. !! !"""
WEAK SUPERVISION LOSS FUNCTIONS,0.222684703433923,"−𝐾
+𝐾
0 0"
WEAK SUPERVISION LOSS FUNCTIONS,0.22372528616024975,Intensity gradient
WEAK SUPERVISION LOSS FUNCTIONS,0.2247658688865765,"Sample location −𝐾
+𝐾"
WEAK SUPERVISION LOSS FUNCTIONS,0.22580645161290322,"Figure 2: (a) A brain MRI region. (b)-(g) are illustration of loss terms. (b) WM, midthickness,
pial surfaces in a deep sulcus region. (c-1) Bi-directional Chamfer loss for the WM surface; (c-2)
Uni-directional Chamfer loss for the pGT pial surface generated from the GM segmentation. (d)
Normal consistency between three reconstructed surfaces. (e) Intensity gradient along the normal
direction of a vertex in the surface. (f) The symmetric deformation trajectory. vo and vi are outward
and inward velocity fields respectively. (g) The customized edge length loss. A: area; µ: edge length."
WEAK SUPERVISION LOSS FUNCTIONS,0.22684703433922998,"respectively. We then apply a fast topology check and correction algorithm [4] to the SDF to ensure
172"
WEAK SUPERVISION LOSS FUNCTIONS,0.2278876170655567,"the surface maintains spherical topology. The WM surface SW∗is extracted using the Marching
173"
WEAK SUPERVISION LOSS FUNCTIONS,0.22892819979188345,"Cubes algorithm [27]. The distance of the vertices between the predicted surface SW and the pGT
174"
WEAK SUPERVISION LOSS FUNCTIONS,0.2299687825182102,"surface SW∗is minimized using the bi-directional Chamfer distance [26]:
175"
WEAK SUPERVISION LOSS FUNCTIONS,0.23100936524453694,"LchW =
1
|SW | X"
WEAK SUPERVISION LOSS FUNCTIONS,0.23204994797086367,"p∈SW
min
p∗∈SW∗
∥p −p∗∥2
2 +
1
|SW∗| X"
WEAK SUPERVISION LOSS FUNCTIONS,0.23309053069719043,"p∗∈SW∗
min
p∈SW∥p∗−p∥2
2,
(2)"
WEAK SUPERVISION LOSS FUNCTIONS,0.23413111342351717,"where p and p∗are the coordinates of vertices on meshes. See Fig. 2 (c-1) for illustration.
176"
WEAK SUPERVISION LOSS FUNCTIONS,0.2351716961498439,"For the pial surface, GM segmentation may fail to delineate the boundary in deep cortical sulci.
177"
WEAK SUPERVISION LOSS FUNCTIONS,0.23621227887617066,"As shown in Fig. 2 (c-2), using a similar pGT surface generation protocol as the WM surface to
178"
WEAK SUPERVISION LOSS FUNCTIONS,0.2372528616024974,"generate the pial surface SG∗fail to capture cortical folding accurately. Directly fitting to SG∗with
179"
WEAK SUPERVISION LOSS FUNCTIONS,0.23829344432882413,"bi-directional Chamfer loss causes the model to predict similarly inaccurate cortical sulci. To address
180"
WEAK SUPERVISION LOSS FUNCTIONS,0.2393340270551509,"this issue, we propose the boundary surface loss, which uses a uni-directional Chamfer distance to
181"
WEAK SUPERVISION LOSS FUNCTIONS,0.24037460978147762,"compute the shortest distance from the pGT pial surface SG∗to the predicted pial surface SG:
182"
WEAK SUPERVISION LOSS FUNCTIONS,0.24141519250780438,"LchG =
1
|SG| X"
WEAK SUPERVISION LOSS FUNCTIONS,0.24245577523413112,"p∈SG
min
p∗∈SG∗
∥p −p∗∥2
2.
(3)"
WEAK SUPERVISION LOSS FUNCTIONS,0.24349635796045785,"In this way, the deformed surface is not influenced by the inaccuracies of SG∗and does not move
183"
WEAK SUPERVISION LOSS FUNCTIONS,0.2445369406867846,"outward from the deep sulci. The overall mesh loss is computed as Lmesh = LchW + LchG.
184"
WEAK SUPERVISION LOSS FUNCTIONS,0.24557752341311134,"Inter-Mesh Normal Consistency Loss. To further alleviate the difficulty of constraining the pial
185"
WEAK SUPERVISION LOSS FUNCTIONS,0.24661810613943808,"surface using the WM and midthickness surfaces, we propose leveraging the prior knowledge that
186"
WEAK SUPERVISION LOSS FUNCTIONS,0.24765868886576484,"the cerebral cortex has a sheet-like topology (i.e., the inner, middle, and outer surfaces are locally
187"
WEAK SUPERVISION LOSS FUNCTIONS,0.24869927159209157,"parallel to each other). As shown in Fig. 2 (d), this loss is defined to ensure that the deformation
188"
WEAK SUPERVISION LOSS FUNCTIONS,0.2497398543184183,"of the midthickness surface aligns with its normal direction, thereby maintaining similar normal
189"
WEAK SUPERVISION LOSS FUNCTIONS,0.25078043704474506,"directions on the target surfaces:
190"
WEAK SUPERVISION LOSS FUNCTIONS,0.2518210197710718,"Limnc =
1
|SM| X"
WEAK SUPERVISION LOSS FUNCTIONS,0.25286160249739853,"p∈SM
(1 −cos(npG, npW )),
(4)"
WEAK SUPERVISION LOSS FUNCTIONS,0.25390218522372526,"where npG and npW are the normal vectors of the deformed vertex p on SM and SG respectively.
191"
WEAK SUPERVISION LOSS FUNCTIONS,0.25494276795005205,"Intensity Gradient Loss. In addition to ribbon segmentaions, inspired by the fact that traditional
192"
WEAK SUPERVISION LOSS FUNCTIONS,0.2559833506763788,"methods utilize raw image intensity contrast to define and optimize the target surfaces, we propose to
193"
WEAK SUPERVISION LOSS FUNCTIONS,0.2570239334027055,"adjust the nuance between GT target surface and the pGT segmentation boundaries. By definition [17,
194"
WEAK SUPERVISION LOSS FUNCTIONS,0.25806451612903225,"52], the WM (or pial) surface lies at the WM/GM (or GM/CSF) interface where image intensity change
195"
WEAK SUPERVISION LOSS FUNCTIONS,0.259105098855359,"most drastically. We sample K points along the extended lines on each side of the normal direction at
196"
WEAK SUPERVISION LOSS FUNCTIONS,0.2601456815816857,"vertex p, and compute the gradients of neighboring points: Lgrad =
1
|SW |
P"
WEAK SUPERVISION LOSS FUNCTIONS,0.2611862643080125,"p∈SM
PK
i=1 gradi(p)+
197"
WEAK SUPERVISION LOSS FUNCTIONS,0.26222684703433924,"1
|SG|
P
p∈SG
PK
i=1 gradi(p).
198"
WEAK SUPERVISION LOSS FUNCTIONS,0.26326742976066597,"Cycle Consistency Loss. We utilize the midthickness layer to establish a correspondence between
199"
WEAK SUPERVISION LOSS FUNCTIONS,0.2643080124869927,"the inner and outer surfaces, thereby reducing the difficulty of learning large deformations. However,
200"
WEAK SUPERVISION LOSS FUNCTIONS,0.26534859521331944,"there is no true midthickness surface available for supervision, nor a definitive criterion for choosing
201"
WEAK SUPERVISION LOSS FUNCTIONS,0.2663891779396462,"between bi-directional or uni-directional approaches for different regions on the midthickness surface.
202"
WEAK SUPERVISION LOSS FUNCTIONS,0.26742976066597296,"Additionally, the learned velocity fields vo and vi could potentially cause non-inverse transformations
203"
WEAK SUPERVISION LOSS FUNCTIONS,0.2684703433922997,"at the midthickness surface. To address these issues, we propose a loss function that enforces the
204"
WEAK SUPERVISION LOSS FUNCTIONS,0.2695109261186264,"midthickness surface resides halfway between the WM and pial surfaces and maintains consistency
205"
WEAK SUPERVISION LOSS FUNCTIONS,0.27055150884495316,"along the entire trajectory:
206"
WEAK SUPERVISION LOSS FUNCTIONS,0.2715920915712799,"Lcyc =
1
|SM| X"
WEAK SUPERVISION LOSS FUNCTIONS,0.2726326742976067,"p∈SM
∥pΦW ◦ΦG −p∥2
2+∥pΦG◦ΦW −p∥2
2+∥LMid→GM(p)−LMid→W M(p)∥2
2, (5)"
WEAK SUPERVISION LOSS FUNCTIONS,0.2736732570239334,"where pΦb◦Φa represents deforming a vertex p ∈SM with velocity field va and vb sequentially, and
207"
WEAK SUPERVISION LOSS FUNCTIONS,0.27471383975026015,"LMid→GM(p) is the accumulated trajectory length over T steps of deformation. For example, as
208"
WEAK SUPERVISION LOSS FUNCTIONS,0.2757544224765869,"shown in Fig. 2 (f), the deformations move a vertex pMid outward to pGM using vo and then inward
209"
WEAK SUPERVISION LOSS FUNCTIONS,0.2767950052029136,"to p′
Mid using vi, in which the two trajectories are aligned by minimizing the distance between pMid
210"
WEAK SUPERVISION LOSS FUNCTIONS,0.2778355879292404,"and p′
Mid. Similarly, we enforce the consistency between pΦG◦ΦW and p. Furthermore, starting
211"
WEAK SUPERVISION LOSS FUNCTIONS,0.27887617065556713,"from the midthickness layer, the trajectory lengths of the vertex moving to the WM and pial surfaces
212"
WEAK SUPERVISION LOSS FUNCTIONS,0.27991675338189387,"should be equal, which is regularized by the third term in the equation above.
213"
WEAK SUPERVISION LOSS FUNCTIONS,0.2809573361082206,"Mesh Quality Loss. First, the reconstructed surface should be composed of uniformally distributed
214"
WEAK SUPERVISION LOSS FUNCTIONS,0.28199791883454733,"triangles. To accommodate various sizes of brain volume and image resolution, we devise a cus-
215"
WEAK SUPERVISION LOSS FUNCTIONS,0.28303850156087407,"tomized edge length loss to constrain the size of triangles in the predicted meshes for each subject.
216"
WEAK SUPERVISION LOSS FUNCTIONS,0.28407908428720086,"Specifically, we assume an ideal prediction where the faces are equilateral and of the same area A
217"
WEAK SUPERVISION LOSS FUNCTIONS,0.2851196670135276,"and drive the edge length to the target edge length µel = 2
q A
√"
WEAK SUPERVISION LOSS FUNCTIONS,0.2861602497398543,"3 (see Fig. 2 (g)). Second, we employ
218"
WEAK SUPERVISION LOSS FUNCTIONS,0.28720083246618106,"a normal consistency loss to promote the surfaces’ smoothness. The mesh quality loss is defined as:
219"
WEAK SUPERVISION LOSS FUNCTIONS,0.2882414151925078,Lqua = 1 |S|  X p∈S
WEAK SUPERVISION LOSS FUNCTIONS,0.2892819979188345,"1
|N(p)| X"
WEAK SUPERVISION LOSS FUNCTIONS,0.2903225806451613,"k∈N(p)
(µel −∥p −k∥2)2 +
X"
WEAK SUPERVISION LOSS FUNCTIONS,0.29136316337148804,"e∈S,f0∩f1=e
(1 −cos(nf0, nf1)) "
WEAK SUPERVISION LOSS FUNCTIONS,0.2924037460978148,",
(6)"
WEAK SUPERVISION LOSS FUNCTIONS,0.2934443288241415,"where S denotes the predicted mesh, N(p) are the neighbors of vertex p, e is an edge, f0 and f1 are
220"
WEAK SUPERVISION LOSS FUNCTIONS,0.29448491155046824,"e’s two neighboring faces with their unit normals nf0 and nf1.
221"
WEAK SUPERVISION LOSS FUNCTIONS,0.29552549427679503,"In summary, we combine all the losses to jointly optimize our SegCSR model: L = λ1Lmesh +
222"
WEAK SUPERVISION LOSS FUNCTIONS,0.29656607700312176,"λ2Limnc + λ3Lgrad + λ4Lcyc + λ5Lqua, where {λi}i=1,··· ,5 are weights to balance the loss terms.
223"
EXPERIMENTS,0.2976066597294485,"4
Experiments
224"
EXPERIMENTAL SETUPS,0.29864724245577523,"4.1
Experimental Setups
225"
EXPERIMENTAL SETUPS,0.29968782518210196,"Datasets. We evaluate our method on two large-scale adult datasets and one infant dataset of low
226"
EXPERIMENTAL SETUPS,0.3007284079084287,"resolution. The ADNI-1 [24] dataset consists of 817 subjects aged 55 to 90. We randomly split it into
227"
EXPERIMENTAL SETUPS,0.3017689906347555,"subsets of 654, 50, and 113 subjects for training, validation, and testing, respectively. The OASIS-
228"
EXPERIMENTAL SETUPS,0.3028095733610822,"1 [35] dataset consists of 413 subjects aged 18 to 96. We randomly split it into subsets of 330, 25, and
229"
EXPERIMENTAL SETUPS,0.30385015608740895,"58 subjects for training, validation, and testing, respectively. We followed a pre-processing protocol
230"
EXPERIMENTAL SETUPS,0.3048907388137357,"used in previous works [8, 13, 26, 31] for fair comparison. The T1-weighted MRI scans were aligned
231"
EXPERIMENTAL SETUPS,0.3059313215400624,"to the MNI152 template and clipped to the size of 192 × 224 × 192 at 1mm3 isotropic resolution.
232"
EXPERIMENTAL SETUPS,0.30697190426638915,"The pseudo ground-truth (pGT) of ribbon segmentation and cortical surfaces were generated using
233"
EXPERIMENTAL SETUPS,0.30801248699271594,"FreeSurfer v7.2.0 [17]. The BCP [23] dataset consists of 121 subjects ranging in age from 2 weeks
234"
EXPERIMENTAL SETUPS,0.3090530697190427,"to 12 months. We randomly allocate 90, 12, and 19 subjects for training, validation, and testing,
235"
EXPERIMENTAL SETUPS,0.3100936524453694,"Table 1: Quantitative analysis of cortical surface reconstruction on geometric accuracy and self-intersections.
The Chamfer distance (CD), average symmetric surface distance (ASSD), Hausdorff distance (HD), and the ratio
of the self-intersecting faces (SIF) were measured for WM and pial surfaces on three datasets. The mean value
and standard deviation are reported. Lower scores indicate better results for all metrics. “S” denotes the use of
pGT surfaces from conventional pipelines, while “W” represents weak supervision by pGT ribbon segmentations.
In each supervision setting, the best results are in bold, and the second best results are underlined. Data Sup."
EXPERIMENTAL SETUPS,0.31113423517169614,"Method
L-Pial Surface
L-WM Surface
CD (mm)
ASSD (mm)
HD (mm)
SIF (%)
CD (mm)
ASSD (mm)
HD (mm)
SIF (%) ADNI S"
EXPERIMENTAL SETUPS,0.31217481789802287,"CorticalFlow++ [47]
0.545±0.036
0.410±0.033
0.886±0.069
0.098±0.067
0.544±0.034
0.401±0.030
0.878±0.066
0.069±0.042
cortexODE [31]
0.476±0.017
0.214±0.020
0.455±0.058
0.022±0.012
0.458±0.016
0.192±0.015
0.436±0.014
0.015±0.011
Vox2Cortex [8]
0.582±0.028
0.370±0.025
0.746±0.057
0.059±0.039
0.577±0.027
0.353±0.022
0.722±0.055
0.043±0.023
CoCSR [54]
0.322±0.021
0.123±0.010
0.267±0.022
0.013±0.011
0.303±0.018
0.117±0.010
0.254±0.021
0.005±0.002 W"
EXPERIMENTAL SETUPS,0.31321540062434966,"DeepCSR [13]
0.945±0.078
0.593±0.065
1.149±0.203
\
0.938±0.076
0.587±0.064
1.137±0.193
\
3D U-Net [44]
0.598±0.049
0.341±0.037
0.782±0.163
\
0.473±0.013
0.265±0.015
0.558±0.028
\
SegCSR (Ours)
0.578±0.019
0.324±0.019
0.749±0.049
0.008±0.009
0.467±0.014
0.258±0.019
0.545±0.036
0.009±0.009 OASIS S"
EXPERIMENTAL SETUPS,0.3142559833506764,"CorticalFlow++ [47]
0.531±0.035
0.399±0.030
0.812±0.057
0.088±0.045
0.529±0.033
0.398±0.030
0.810±0.055
0.086±0.042
cortexODE [31]
0.481±0.019
0.218±0.021
0.461±0.062
0.026±0.015
0.463±0.018
0.207±0.017
0.435±0.015
0.018±0.010
Vox2Cortex [8]
0.588±0.032
0.381±0.030
0.750±0.063
0.061±0.037
0.581±0.028
0.375±0.027
0.731±0.059
0.046±0.027
CoCSR [54]
0.410±0.034
0.142±0.016
0.281±0.024
0.016±0.012
0.349±0.024
0.128±0.019
0.266±0.022
0.007±0.002 W"
EXPERIMENTAL SETUPS,0.3152965660770031,"DeepCSR [13]
0.986±0.085
0.617±0.070
1.331±0.212
\
0.975±0.081
0.594±0.067
1.151±0.197
\
3D U-Net [44]
0.611±0.069
0.332±0.050
0.774±0.267
\
0.454±0.013
0.245±0.017
0.489±0.031
\
SegCSR (Ours)
0.581±0.016
0.321±0.018
0.725±0.040
0.010±0.010
0.449±0.011
0.223±0.016
0.461±0.027
0.010±0.009 BCP S"
EXPERIMENTAL SETUPS,0.31633714880332986,"CorticalFlow++ [47]
0.927±0.271
0.731±0.036
1.943±0.175
1.114±0.385
0.895±0.242
0.722±0.034
1.880±0.151
0.533±0.107
cortexODE [31]
0.759±0.082
0.396±0.032
0.823±0.103
0.124±0.061
0.678±0.071
0.349±0.031
0.816±0.099
0.101±0.034
CoCSR [54]
0.576±0.041
0.216±0.023
0.468±0.063
0.064±0.040
0.544±0.038
0.199±0.020
0.447±0.049
0.058±0.033 W"
EXPERIMENTAL SETUPS,0.3173777315296566,"DeepCSR [13]
2.673±1.131
1.224±0.215
3.112±1.218
\
1.440±0.521
0.428±0.051
0.933±0.118
\
3D U-Net [44]
1.175±0.314
0.793±0.059
2.140±1.021
\
0.688±0.120
0.377±0.041
0.791±0.064
\
SegCSR (Ours)
0.927±0.070
0.497±0.061
1.287±0.144
0.061±0.058
0.876±0.067
0.478±0.052
1.206±0.132
0.055±0.057"
EXPERIMENTAL SETUPS,0.3184183142559833,"respectively. Rigid registration was applied to the T1w and T2w image pairs. The pGT of ribbon
236"
EXPERIMENTAL SETUPS,0.3194588969823101,"segmentation and cortical surfaces were generated by the iBEAT v2.0 [52]. The intensity values of
237"
EXPERIMENTAL SETUPS,0.32049947970863685,"MRI scans, ribbon segmentation maps, and SDFs were normalized to [0, 1] and the coordinates of the
238"
EXPERIMENTAL SETUPS,0.3215400624349636,"vertices were normalized to [−1, 1]. All the models were trained on the training set until they reached
239"
EXPERIMENTAL SETUPS,0.3225806451612903,"a loss plateau on the validation set and evaluated on the test set.
240"
EXPERIMENTAL SETUPS,0.32362122788761705,"Implementation Details Our framework was implemented in PyTorch [40] and trained on a worksta-
241"
EXPERIMENTAL SETUPS,0.32466181061394384,"tion with 12 GB NVIDIA P100 GPU. The 3D U-Net [44] for segmentation of ribbons was trained for
242"
EXPERIMENTAL SETUPS,0.32570239334027057,"200 epochs using Adam [25] optimization and achieved an average Dice index of 0.96 on the testing
243"
EXPERIMENTAL SETUPS,0.3267429760665973,"set. The SegCSR model utilized T = 5 steps (i.e., step size is 0.2) in Euler solver. We trained our
244"
EXPERIMENTAL SETUPS,0.32778355879292403,"SegCSR model using Adam optimizer (β1 = 0.9, β2 = 0.999, ϵ = 1e−10, learning rate 1e−4) for
245"
EXPERIMENTAL SETUPS,0.32882414151925077,"400 epochs to reconstruct both WM, midthickness, and pial surfaces of both brain hemispheres. We
246"
EXPERIMENTAL SETUPS,0.3298647242455775,"set λ1 = λ4 = 1 and λ2 = λ3 = λ5 = 0.1. The surface meshes had ∼130k vertices. More details
247"
EXPERIMENTAL SETUPS,0.3309053069719043,"can be found in the Supplementary Materials.
248"
EXPERIMENTAL SETUPS,0.331945889698231,"Evaluation Metrics We utilized three distance-based metrics to measure the CSR accuracy: Chamfer
249"
EXPERIMENTAL SETUPS,0.33298647242455776,"distance (CD), average symmetric surface distance (ASSD), and 90th-percentile Hausdorff distance
250"
EXPERIMENTAL SETUPS,0.3340270551508845,"(HD). CD [16, 53] measures the mean distance between two sets of vertices. ASSD [13] and
251"
EXPERIMENTAL SETUPS,0.3350676378772112,"HD [13, 49] measure the average and maximum distance between two surfaces. They were computed
252"
EXPERIMENTAL SETUPS,0.33610822060353795,"bidirectionally over ∼130k points uniformly sampled from the predicted and target surfaces. A lower
253"
EXPERIMENTAL SETUPS,0.33714880332986474,"distance means a better result. Since topology is also important in CSR, we utilized the ratio of
254"
EXPERIMENTAL SETUPS,0.3381893860561915,"self-intersection faces (SIF) [13, 14, 31, 54] to measure reconstructed surface quality.
255"
COMPARISON WITH RELATED WORKS,0.3392299687825182,"4.2
Comparison with Related Works
256"
COMPARISON WITH RELATED WORKS,0.34027055150884494,"We compare SegCSR with both implicit and explicit learning-based cortical surface reconstruction
257"
COMPARISON WITH RELATED WORKS,0.3413111342351717,"approaches described in Section 1 and summarize the experimental results in Table 5.
258"
COMPARISON WITH RELATED WORKS,0.34235171696149846,"On Adult Datasets. (I) Comparison with Implicit Approaches. We compare SegCSR with two
259"
COMPARISON WITH RELATED WORKS,0.3433922996878252,"representative implicit representation approaches on the ADNI and OASIS datasets. As shown
260"
COMPARISON WITH RELATED WORKS,0.34443288241415193,"in Table 5, SegCSR achieves superior geometric accuracy. Note that both DeepCSR [13] and 3D
261"
COMPARISON WITH RELATED WORKS,0.34547346514047866,"U-Net [44] require post-processing to correct topology and extract a mesh, resulting in SIFs of 0.
262"
COMPARISON WITH RELATED WORKS,0.3465140478668054,"Without post-processing, the SIFs for 3D U-Net’s WM and pial surfaces range from 3% to 15%.
263"
COMPARISON WITH RELATED WORKS,0.34755463059313213,"SegCSR produces a negligible number of self-intersecting faces, ∼0.3% on average for both white
264"
COMPARISON WITH RELATED WORKS,0.3485952133194589,"and pial surfaces. Fig. 3 shows that SegCSR effectively deforms the pial surface into deep sulci,
265"
COMPARISON WITH RELATED WORKS,0.34963579604578565,"while the baseline approaches exhibit large geometric errors due to the PVE problem of brain MRI.
266"
COMPARISON WITH RELATED WORKS,0.3506763787721124,"(a) DeepCSR
(b) CortexODE
(c) SegCSR (S0 setting)
(d) SegCSR (S0* setting)
(e) pGT from FreeSurfer"
COMPARISON WITH RELATED WORKS,0.3517169614984391,"Figure 3: Visualization of reconstructed pial surfaces compared to DeepCSR and CortexODE. CortexODE is
trained with pGT from FreeSurfer; DeepCSR and ours are trained with pGT ribbon segmentations."
COMPARISON WITH RELATED WORKS,0.35275754422476585,"Additionally, SegCSR requires only 0.37s of runtime per brain hemisphere, orders of magnitude
267"
COMPARISON WITH RELATED WORKS,0.3537981269510926,"faster than traditional FreeSurfer pipelines. (II) Comparison with Explicit Approaches. We compare
268"
COMPARISON WITH RELATED WORKS,0.3548387096774194,"SegCSR with explicit learning-based approaches, including CorticalFlow++ [47], Vox2Cortex [8],
269"
COMPARISON WITH RELATED WORKS,0.3558792924037461,"CortexODE [31], and CoCSR [54]. These methods are trained with pGT surfaces generated by
270"
COMPARISON WITH RELATED WORKS,0.35691987513007284,"conventional pipelines, providing more accurate supervision than pGT ribbon segmentations. For
271"
COMPARISON WITH RELATED WORKS,0.35796045785639957,"a fair comparison, we employ the same network structure for the current best CoCSR [54] and our
272"
COMPARISON WITH RELATED WORKS,0.3590010405827263,"SegCSR, with CoCSR serving as an upper-bound performance benchmark for our weakly supervised
273"
COMPARISON WITH RELATED WORKS,0.3600416233090531,"SegCSR. As shown in Table 5, SegCSR surprisingly surpasses some supervised baselines in terms of
274"
COMPARISON WITH RELATED WORKS,0.3610822060353798,"both geometric and morphological accuracy, demonstrating its potential to replace existing methods
275"
COMPARISON WITH RELATED WORKS,0.36212278876170656,"when accurate surface supervision is not available.
276"
COMPARISON WITH RELATED WORKS,0.3631633714880333,"On Infant Dataset. Infant brain MRIs present additional challenges due to the smaller size of fetal
277"
COMPARISON WITH RELATED WORKS,0.36420395421436,"brains, limited image resolution, and lower image contrast, which together make the reconstruction
278"
COMPARISON WITH RELATED WORKS,0.36524453694068676,"task more difficult. Consequently, overall performance is inferior compared to adult datasets. We
279"
COMPARISON WITH RELATED WORKS,0.36628511966701355,"compare SegCSR with both implicit and explicit representation approaches. The results in Table 5
280"
COMPARISON WITH RELATED WORKS,0.3673257023933403,"show that SegCSR achieves superior performance than the implicit DeepCSR and 3D U-Net methods,
281"
COMPARISON WITH RELATED WORKS,0.368366285119667,"and comparable performance to explicit methods like CorticalFlow++, CortexODE, and CoCSR.
282"
ABLATION STUDIES,0.36940686784599375,"4.3
Ablation Studies
283"
ABLATION STUDIES,0.3704474505723205,"Table 2: Ablation studies on the ADNI dataset. The setting S0 refers to our complete setting (cf. Table 5). Top:
The impact of loss functions. Bottom: The impact of initialization surface location."
ABLATION STUDIES,0.37148803329864727,"Setting
Loss
L-Pial Surface
L-WM Surface
Lmesh Limnc Lgrad Lcyc Lqua CD (mm) ASSD (mm) HD (mm)
SIF (%)
CD (mm) ASSD (mm) HD (mm)
SIF (%)
S0
✓
✓
✓
✓
✓
0.578±0.019 0.324±0.019 0.749±0.049 0.008±0.009 0.467±0.014 0.258±0.019 0.545±0.036 0.009±0.009
S1
✓
✓
✓
✓
0.576±0.019 0.323±0.019 0.747±0.046 0.012±0.011 0.467±0.015 0.257±0.020 0.542±0.036 0.011±0.011
S2
✓
✓
✓
0.579±0.019 0.325±0.019 0.748±0.047 0.014±0.013 0.469±0.016 0.248±0.019 0.544±0.042 0.015±0.014
S3
✓
✓
0.579±0.020 0.325±0.021 0.749±0.050 0.018±0.014 0.473±0.013 0.249±0.018 0.544±0.039 0.017±0.013
S4
✓
0.589±0.034 0.356±0.039 0.764±0.067 0.015±0.012 0.473±0.012 0.256±0.020 0.564±0.042 0.014±0.013
S0⋆
✓⋆
✓
✓
✓
✓
0.607±0.034 0.327±0.024 0.752±0.077 0.026±0.016 0.469±0.015 0.258±0.020 0.547±0.038 0.020±0.015
S4⋆
✓⋆
0.626±0.053 0.321±0.039 0.773±0.168 0.034±0.025 0.476±0.013 0.256±0.018 0.562±0.034 0.031±0.017"
ABLATION STUDIES,0.372528616024974,"Init. Surface
Location"
ABLATION STUDIES,0.37356919875130074,"L-Pial Surface
L-WM Surface
CD (mm)
ASSD (mm)
HD (mm)
SIF(%)
CD (mm)
ASSD (mm)
HD (mm)
SIF(%)
WM
0.878±0.077
0.587±0.060
1.084±0.097
0.012±0.011
0.439±0.011
0.211±0.013
0.430±0.028
0.007±0.008
Mid
0.578±0.019
0.324±0.019
0.749±0.049
0.008±0.009
0.467±0.014
0.258±0.019
0.545±0.036
0.009±0.009
GM
0.489±0.016
0.317±0.018
0.567±0.044
0.008±0.008
0.889±0.085
0.597±0.059
1.211±0.104
0.020±0.018"
ABLATION STUDIES,0.37460978147762747,"Loss Functions. We evaluated the contribution of different losses of our method to the surface
284"
ABLATION STUDIES,0.3756503642039542,"reconstruction performance in terms of both accuracy (CD, ASSD, HD) and topological correctness
285"
ABLATION STUDIES,0.37669094693028093,"(SIF). The results are summarized in Table 2 (Top). The setting S4 represents using our proposed
286"
ABLATION STUDIES,0.3777315296566077,"Chamfer loss (i.e., uni-directional for the pial surface) alone, while S4⋆referes to using existing
287"
ABLATION STUDIES,0.37877211238293446,"bi-directional Chamfer loss for both WM and pial surfaces. The results of S4 and S4⋆indicated
288"
ABLATION STUDIES,0.3798126951092612,"that the model using bi-directional Chamfer loss overfitted to the pGT segmentation boundary and
289"
ABLATION STUDIES,0.3808532778355879,"failed to fit the deep cortical sulci. Another pair of comparison, S0 and S0⋆, showed a similar
290"
ABLATION STUDIES,0.38189386056191466,"phenomenon. Enforcing the inter-mesh normal consistency of the WM and pial surfaces (S3, Limnc)
291"
ABLATION STUDIES,0.3829344432882414,"improved geometric accuracy by explicitly constraining the nromal direction of two surfaces but
292"
ABLATION STUDIES,0.3839750260145682,"slightly worsened the topology, which might be caused by the discrepancy between the midthickness
293"
ABLATION STUDIES,0.3850156087408949,"and the WM (and pial) surface. The proposed intensity gradient loss (S2, Lgrad) helped adjust the
294"
ABLATION STUDIES,0.38605619146722164,"deformed surfaces locally, leading to slightly improved geometric accuracy and reduced topology
295"
ABLATION STUDIES,0.3870967741935484,"error. Enforcing equality of the trajectories from the midthickness surface to the WM and pial surfaces
296"
ABLATION STUDIES,0.3881373569198751,"and symmetric cycle consistency of two trajectories (S1, Lcyc) helped optimize the midthickness
297"
ABLATION STUDIES,0.3891779396462019,"surface and promoted the invertibility of deformations. Moreover, the inclusion of regularization
298"
ABLATION STUDIES,0.39021852237252863,"terms on the uniformity and smoothness of the reconstructed surfaces (S0, Lqua) enhanced the
299"
ABLATION STUDIES,0.39125910509885536,"surface quality and significantly reduce the self-intersection face ratio. Overall, our proposed method
300"
ABLATION STUDIES,0.3922996878251821,"struck a balance between geometric accuracy and topology quality, with each component playing a
301"
ABLATION STUDIES,0.39334027055150883,"complementary role.
302"
ABLATION STUDIES,0.39438085327783556,"Initialization Surface Location. Table 2 (Bottom) shows the impact of the initialization surface
303"
ABLATION STUDIES,0.39542143600416235,"location. Starting from either the WM or midthickness surfaces leads to satisfactory results. Con-
304"
ABLATION STUDIES,0.3964620187304891,"versely, initializing from the GM surface introduced more difficulty in learning large deformations
305"
ABLATION STUDIES,0.3975026014568158,"into deep sulci due to the severe partial volume effect, resulting in worse average geometric accuracy
306"
ABLATION STUDIES,0.39854318418314255,"for both surfaces. The results also indicated that the closer the initial surface was to its target surface,
307"
ABLATION STUDIES,0.3995837669094693,"the higher the reconstruction accuracy achieved. Therefore, starting from the midthickness surface
308"
ABLATION STUDIES,0.4006243496357961,"strikes a balance between WM and pial surface reconstruction outcomes.
309"
REPRODUCIBILITY,0.4016649323621228,"4.4
Reproducibility
310
Table 3: Reproducibility analysis."
REPRODUCIBILITY,0.40270551508844954,"Method
L-WM Surface
CD (mm)
ASSD (mm) HD (mm)
SegCSR (Ours) 0.473±0.016 0.254±0.024 0.520±0.062
DeepCSR
0.505±0.047 0.297±0.053 0.610±0.100
CoCSR
0.451±0.019 0.235±0.030 0.492±0.059
CortexODE
0.457±0.021 0.238±0.031 0.504±0.071
FreeSurfer
0.476±0.015 0.253±0.022 0.519±0.048"
REPRODUCIBILITY,0.4037460978147763,"Method
L-Pial Surface
CD (mm)
ASSD (mm) HD (mm)
SegCSR (Ours) 0.529±0.023 0.285±0.033 0.622±0.066
DeepCSR
0.560±0.055 0.341±0.060 0.668±0.118
CoCSR
0.493±0.024 0.276±0.036 0.573±0.070
CortexODE
0.506±0.029 0.272±0.034 0.581±0.079
FreeSurfer
0.526±0.021 0.283±0.032 0.595±0.068"
REPRODUCIBILITY,0.404786680541103,"We conducted an experiment on the Test-Retest
311"
REPRODUCIBILITY,0.40582726326742974,"dataset [33], which comprises 40 MRIs collected within
312"
REPRODUCIBILITY,0.4068678459937565,"a short period for each of the 3 subjects. The cor-
313"
REPRODUCIBILITY,0.40790842872008326,"tical surfaces of the same subject should be nearly
314"
REPRODUCIBILITY,0.40894901144641,"identical. Following the experimental setup outlined
315"
REPRODUCIBILITY,0.4099895941727367,"in [8, 13, 31, 54], we utilized the iterative closest-point
316"
REPRODUCIBILITY,0.41103017689906346,"algorithm to align image pairs and computed the ge-
317"
REPRODUCIBILITY,0.4120707596253902,"ometric distance between surfaces. The results for
318"
REPRODUCIBILITY,0.413111342351717,"the left hemisphere are presented in Table 3, showing
319"
REPRODUCIBILITY,0.4141519250780437,"that SegCSR obtained superior reproducibility com-
320"
REPRODUCIBILITY,0.41519250780437045,"pared with DeepCSR (implicit representation; weakly
321"
REPRODUCIBILITY,0.4162330905306972,"supervised) and was comparable to the conventional
322"
REPRODUCIBILITY,0.4172736732570239,"FreeSurfer pipeline and supervised DL-based CSR
323"
REPRODUCIBILITY,0.4183142559833507,"methods. This implied that the results generated by SegCSR can be reliably used for downstream
324"
REPRODUCIBILITY,0.41935483870967744,"analyses, such as investigating cortical thickness changes in patients.
325"
CONCLUSIONS,0.42039542143600417,"5
Conclusions
326"
CONCLUSIONS,0.4214360041623309,"We introduce SegCSR, a novel approach to jointly reconstruct multiple cortical surfaces using
327"
CONCLUSIONS,0.42247658688865763,"weak supervision from ribbon segmentations derived from brain MRIs. Our method initializes a
328"
CONCLUSIONS,0.42351716961498437,"midthickness surface and then deforms it inward and outward to the inner and outer cortical surfaces by
329"
CONCLUSIONS,0.42455775234131116,"jointly learning diffeomorphic flows. The new boundary loss function optimizes the surfaces toward
330"
CONCLUSIONS,0.4255983350676379,"the boundaries of the cortical ribbon segmentation maps while the inter-surface normal consistency
331"
CONCLUSIONS,0.4266389177939646,"loss regularizes the pial surface in complex and challenging cortical sulci regions. Additional
332"
CONCLUSIONS,0.42767950052029136,"regularization terms are incorporated to enforce reconstructed surfaces’ uniformity, smoothness,
333"
CONCLUSIONS,0.4287200832466181,"and topology. Extensive experiments conducted on large-scale adult and infant brain MRI datasets
334"
CONCLUSIONS,0.4297606659729448,"demonstrate superior performance in terms of accuracy and surface regularity compared to existing
335"
CONCLUSIONS,0.4308012486992716,"supervised DL-based alternatives.
336"
CONCLUSIONS,0.43184183142559834,"Limitations and Future Directions. The efficacy of SegCSR is influenced by the quality of pGT
337"
CONCLUSIONS,0.4328824141519251,"segmentations. Also, We can utilize brain tissue segmentation as auxiliary functions to supervise the
338"
CONCLUSIONS,0.4339229968782518,"model training. SegCSR constrains the inter-mesh consistency of the deformation on the midthickness
339"
CONCLUSIONS,0.43496357960457854,"surface, potentially affecting anatomical fidelity of pial surfaces. The method should be tested on
340"
CONCLUSIONS,0.43600416233090533,"more diverse cohorts of subjects to demonstrate its efficacy on real world neuroimage analysis tasks.
341"
CONCLUSIONS,0.43704474505723206,"Societal Impact. Our proposed method has been rigorously evaluated on four real-world brain MRI
342"
CONCLUSIONS,0.4380853277835588,"datasets, showcasing its capacity to assist doctors and scientists in both quantitative and qualitative
343"
CONCLUSIONS,0.43912591050988553,"analyses of the cerebral cortex. Nonetheless it is imperative to conduct more thorough evaluation on
344"
CONCLUSIONS,0.44016649323621226,"a larger cohort of subjects and across various imaging qualities. And the deployment of the model in
345"
CONCLUSIONS,0.441207075962539,"clinical settings should be approached with caution and under human supervision.
346"
REFERENCES,0.4422476586888658,"References
347"
REFERENCES,0.4432882414151925,"[1] V. I. Arnold. Ordinary differential equations. Springer Science & Business Media, 1992.
348"
REFERENCES,0.44432882414151925,"[2] V. Arsigny, O. Commowick, X. Pennec, and N. Ayache. A log-euclidean framework for statistics
349"
REFERENCES,0.445369406867846,"on diffeomorphisms. In International Conference on Medical Image Computing and Computer
350"
REFERENCES,0.4464099895941727,"Assisted Intervention, pages 924–931. Springer, 2006.
351"
REFERENCES,0.4474505723204995,"[3] G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, and A. V. Dalca. Voxelmorph: a learning
352"
REFERENCES,0.44849115504682624,"framework for deformable medical image registration. IEEE Transactions on Medical Imaging,
353"
REFERENCES,0.449531737773153,"38(8):1788–1800, 2019.
354"
REFERENCES,0.4505723204994797,"[4] P.-L. Bazin and D. L. Pham. Topology correction of segmented medical images using a fast
355"
REFERENCES,0.45161290322580644,"marching algorithm. Computer Methods and Programs in Biomedicine, 88(2):182–190, 2007.
356"
REFERENCES,0.45265348595213317,"[5] M. F. Beg, M. I. Miller, A. Trouvé, and L. Younes. Computing large deformation metric
357"
REFERENCES,0.45369406867845996,"mappings via geodesic flows of diffeomorphisms. International Journal of Computer Vision,
358"
REFERENCES,0.4547346514047867,"61:139–157, 2005.
359"
REFERENCES,0.4557752341311134,"[6] M. Bertoux, J. Lagarde, F. Corlier, L. Hamelin, J.-F. Mangin, O. Colliot, M. Chupin, M. N.
360"
REFERENCES,0.45681581685744016,"Braskie, P. M. Thompson, M. Bottlaender, et al. Sulcal morphology in alzheimer’s disease: an
361"
REFERENCES,0.4578563995837669,"effective marker of diagnosis and cognition. Neurobiology of Aging, 84:41–49, 2019.
362"
REFERENCES,0.4588969823100936,"[7] B. Billot, D. N. Greve, O. Puonti, A. Thielscher, K. Van Leemput, B. Fischl, A. V. Dalca, J. E.
363"
REFERENCES,0.4599375650364204,"Iglesias, et al. Synthseg: Segmentation of brain mri scans of any contrast and resolution without
364"
REFERENCES,0.46097814776274715,"retraining. Medical image analysis, 86:102789, 2023.
365"
REFERENCES,0.4620187304890739,"[8] F. Bongratz, A.-M. Rickmann, S. Pölsterl, and C. Wachinger. Vox2Cortex: Fast explicit
366"
REFERENCES,0.4630593132154006,"reconstruction of cortical surfaces from 3D MRI scans with geometric deep neural networks. In
367"
REFERENCES,0.46409989594172735,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
368"
REFERENCES,0.46514047866805414,"20773–20783, 2022.
369"
REFERENCES,0.46618106139438087,"[9] R. L. Burden, J. D. Faires, and A. M. Burden. Numerical analysis. Cengage learning, 2015.
370"
REFERENCES,0.4672216441207076,"[10] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential
371"
REFERENCES,0.46826222684703434,"equations. Advances in Neural Information Processing Systems, 31:6572–6583, 2018.
372"
REFERENCES,0.46930280957336107,"[11] X. Chen, J. Zhao, S. Liu, S. Ahmad, and P.-T. Yap. SurfFlow: A flow-based approach for rapid
373"
REFERENCES,0.4703433922996878,"and accurate cortical surface reconstruction from infant brain mri. In International Conference
374"
REFERENCES,0.4713839750260146,"on Medical Image Computing and Computer-Assisted Intervention, pages 380–388. Springer,
375"
REFERENCES,0.4724245577523413,"2023.
376"
REFERENCES,0.47346514047866806,"[12] S. J. Crutch, M. Lehmann, J. M. Schott, G. D. Rabinovici, M. N. Rossor, and N. C. Fox.
377"
REFERENCES,0.4745057232049948,"Posterior cortical atrophy. The Lancet Neurology, 11(2):170–178, 2012.
378"
REFERENCES,0.4755463059313215,"[13] R. S. Cruz, L. Lebrat, P. Bourgeat, C. Fookes, J. Fripp, and O. Salvado. DeepCSR: A 3D deep
379"
REFERENCES,0.47658688865764826,"learning approach for cortical surface reconstruction. In Proceedings of the IEEE/CVF Winter
380"
REFERENCES,0.47762747138397504,"Conference on Applications of Computer Vision, pages 806–815, 2021.
381"
REFERENCES,0.4786680541103018,"[14] R. Dahnke, R. A. Yotter, and C. Gaser. Cortical thickness and central surface estimation.
382"
REFERENCES,0.4797086368366285,"Neuroimage, 65:336–348, 2013.
383"
REFERENCES,0.48074921956295524,"[15] A. M. Dale, B. Fischl, and M. I. Sereno. Cortical surface-based analysis: I. segmentation and
384"
REFERENCES,0.481789802289282,"surface reconstruction. Neuroimage, 9(2):179–194, 1999.
385"
REFERENCES,0.48283038501560877,"[16] H. Fan, H. Su, and L. J. Guibas. A point set generation network for 3d object reconstruction
386"
REFERENCES,0.4838709677419355,"from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and
387"
REFERENCES,0.48491155046826223,"Pattern Recognition, pages 605–613, 2017.
388"
REFERENCES,0.48595213319458896,"[17] B. Fischl. Freesurfer. Neuroimage, 62(2):774–781, 2012.
389"
REFERENCES,0.4869927159209157,"[18] M. F. Glasser, S. N. Sotiropoulos, J. A. Wilson, T. S. Coalson, B. Fischl, J. L. Andersson, J. Xu,
390"
REFERENCES,0.48803329864724243,"S. Jbabdi, M. Webster, J. R. Polimeni, et al. The minimal preprocessing pipelines for the human
391"
REFERENCES,0.4890738813735692,"connectome project. Neuroimage, 80:105–124, 2013.
392"
REFERENCES,0.49011446409989595,"[19] K. Gopinath, C. Desrosiers, and H. Lombaert. SegRecon: Learning joint brain surface re-
393"
REFERENCES,0.4911550468262227,"construction and segmentation from images. In International Conference on Medical Image
394"
REFERENCES,0.4921956295525494,"Computing and Computer Assisted Intervention, pages 650–659. Springer, 2021.
395"
REFERENCES,0.49323621227887615,"[20] L. Henschel, S. Conjeti, S. Estrada, K. Diers, B. Fischl, and M. Reuter. Fastsurfer-a fast and
396"
REFERENCES,0.49427679500520294,"accurate deep learning based neuroimaging pipeline. NeuroImage, 219:117012, 2020.
397"
REFERENCES,0.4953173777315297,"[21] Y. Hong, S. Ahmad, Y. Wu, S. Liu, and P.-T. Yap. Vox2Surf: Implicit surface reconstruction
398"
REFERENCES,0.4963579604578564,"from volumetric data. In Intl. Workshop on Machine Learning in Medical Imaging, pages
399"
REFERENCES,0.49739854318418314,"644–653. Springer, 2021.
400"
REFERENCES,0.4984391259105099,"[22] A. Hoopes, J. E. Iglesias, B. Fischl, D. Greve, and A. V. Dalca. Topofit: Rapid reconstruction of
401"
REFERENCES,0.4994797086368366,"topologically-correct cortical surfaces. In Medical Imaging with Deep Learning, 2022.
402"
REFERENCES,0.5005202913631633,"[23] B. R. Howell, M. A. Styner, W. Gao, P.-T. Yap, L. Wang, K. Baluyot, E. Yacoub, G. Chen,
403"
REFERENCES,0.5015608740894901,"T. Potts, A. Salzwedel, et al. The unc/umn baby connectome project (bcp): An overview of the
404"
REFERENCES,0.5026014568158168,"study design and protocol development. NeuroImage, 185:891–905, 2019.
405"
REFERENCES,0.5036420395421436,"[24] C. R. Jack Jr, M. A. Bernstein, N. C. Fox, P. Thompson, G. Alexander, D. Harvey, B. Borowski,
406"
REFERENCES,0.5046826222684704,"P. J. Britson, J. L. Whitwell, C. Ward, et al. The alzheimer’s disease neuroimaging initiative
407"
REFERENCES,0.5057232049947971,"(ADNI): MRI methods. Journal of Magnetic Resonance Imaging: An Official Journal of the
408"
REFERENCES,0.5067637877211238,"International Society for Magnetic Resonance in Medicine, 27(4):685–691, 2008.
409"
REFERENCES,0.5078043704474505,"[25] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International
410"
REFERENCES,0.5088449531737773,"Conference on Learning Representations, 2015.
411"
REFERENCES,0.5098855359001041,"[26] L. Lebrat, R. Santa Cruz, F. de Gournay, D. Fu, P. Bourgeat, J. Fripp, C. Fookes, and O. Salvado.
412"
REFERENCES,0.5109261186264308,"CorticalFlow: A diffeomorphic mesh transformer network for cortical surface reconstruction.
413"
REFERENCES,0.5119667013527576,"Advances in Neural Information Processing Systems, 34:29491–29505, 2021.
414"
REFERENCES,0.5130072840790842,"[27] T. Lewiner, H. Lopes, A. W. Vieira, and G. Tavares. Efficient implementation of marching
415"
REFERENCES,0.514047866805411,"cubes’ cases with topological guarantees. Journal of Graphics Tools, 8(2):1–15, 2003.
416"
REFERENCES,0.5150884495317378,"[28] H. Li, Y. Fan, and A. D. N. Initiative. MDReg-Net: Multi-resolution diffeomorphic image
417"
REFERENCES,0.5161290322580645,"registration using fully convolutional networks with deep self-supervision. Human Brain
418"
REFERENCES,0.5171696149843913,"Mapping, 43(7):2218–2231, 2022.
419"
REFERENCES,0.518210197710718,"[29] Y. Li, H. Li, and Y. Fan. ACEnet: Anatomical context-encoding network for neuroanatomy
420"
REFERENCES,0.5192507804370448,"segmentation. Medical image analysis, 70:101991, 2021.
421"
REFERENCES,0.5202913631633714,"[30] Q. Ma, L. Li, V. Kyriakopoulou, J. V. Hajnal, E. C. Robinson, B. Kainz, and D. Rueckert. Condi-
422"
REFERENCES,0.5213319458896982,"tional temporal attention networks for neonatal cortical surface reconstruction. In International
423"
REFERENCES,0.522372528616025,"Conference on Medical Image Computing and Computer-Assisted Intervention, pages 312–322.
424"
REFERENCES,0.5234131113423517,"Springer, 2023.
425"
REFERENCES,0.5244536940686785,"[31] Q. Ma, L. Li, E. C. Robinson, B. Kainz, D. Rueckert, and A. Alansary. CortexODE: Learning
426"
REFERENCES,0.5254942767950052,"cortical surface reconstruction by neural ODEs. IEEE Trans. on Medical Imaging, 42(2):430–
427"
REFERENCES,0.5265348595213319,"443, 2022.
428"
REFERENCES,0.5275754422476587,"[32] Q. Ma, E. C. Robinson, B. Kainz, D. Rueckert, and A. Alansary. PialNN: A fast deep learning
429"
REFERENCES,0.5286160249739854,"framework for cortical pial surface reconstruction. In International Workshop on Machine
430"
REFERENCES,0.5296566077003122,"Learning in Clinical Neuroimaging, pages 73–81. Springer, 2021.
431"
REFERENCES,0.5306971904266389,"[33] J. Maclaren, Z. Han, S. B. Vos, N. Fischbein, and R. Bammer. Reliability of brain volume
432"
REFERENCES,0.5317377731529657,"measurements: a test-retest dataset. Scientific Data, 1(1):1–9, 2014.
433"
REFERENCES,0.5327783558792925,"[34] A. Makropoulos, E. C. Robinson, A. Schuh, R. Wright, S. Fitzgibbon, J. Bozek, S. J. Counsell,
434"
REFERENCES,0.5338189386056191,"J. Steinweg, K. Vecchiato, J. Passerat-Palmbach, et al. The developing human connectome
435"
REFERENCES,0.5348595213319459,"project: A minimal processing pipeline for neonatal cortical surface reconstruction. Neuroimage,
436"
REFERENCES,0.5359001040582726,"173:88–112, 2018.
437"
REFERENCES,0.5369406867845994,"[35] D. S. Marcus, T. H. Wang, J. Parker, J. G. Csernansky, J. C. Morris, and R. L. Buckner. Open
438"
REFERENCES,0.5379812695109261,"access series of imaging studies (OASIS): cross-sectional MRI data in young, middle aged,
439"
REFERENCES,0.5390218522372529,"nondemented, and demented older adults. Journal of Cognitive Neuroscience, 19(9):1498–1507,
440"
REFERENCES,0.5400624349635796,"2007.
441"
REFERENCES,0.5411030176899063,"[36] Q. Meng, W. Bai, D. P. O’Regan, and D. Rueckert. Deepmesh: Mesh-based cardiac motion
442"
REFERENCES,0.5421436004162331,"tracking using deep learning. IEEE Transactions on Medical Imaging, 2023.
443"
REFERENCES,0.5431841831425598,"[37] M. Modat, D. M. Cash, P. Daga, G. P. Winston, J. S. Duncan, and S. Ourselin. Global
444"
REFERENCES,0.5442247658688866,"image registration using a symmetric block-matching approach. Journal of medical imaging,
445"
REFERENCES,0.5452653485952134,"1(2):024003–024003, 2014.
446"
REFERENCES,0.54630593132154,"[38] T. C. Mok and A. Chung. Fast symmetric diffeomorphic image registration with convolutional
447"
REFERENCES,0.5473465140478668,"neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
448"
REFERENCES,0.5483870967741935,"Recognition, pages 4644–4653, 2020.
449"
REFERENCES,0.5494276795005203,"[39] D. H. Pak, M. Liu, S. S. Ahn, A. Caballero, J. A. Onofrey, L. Liang, W. Sun, and J. S. Duncan.
450"
REFERENCES,0.5504682622268471,"Weakly supervised deep learning for aortic valve finite element mesh generation from 3D CT
451"
REFERENCES,0.5515088449531738,"images. In International Conference on Information Processing in Medical Imaging, pages
452"
REFERENCES,0.5525494276795005,"637–648. Springer, 2021.
453"
REFERENCES,0.5535900104058272,"[40] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
454"
REFERENCES,0.554630593132154,"N. Gimelshein, L. Antiga, et al. PyTorch: An imperative style, high-performance deep learning
455"
REFERENCES,0.5556711758584808,"library. Advances in Neural Information Processing Systems, 32:8026–8037, 2019.
456"
REFERENCES,0.5567117585848075,"[41] J. Ren, Q. Hu, W. Wang, W. Zhang, C. S. Hubbard, P. Zhang, N. An, Y. Zhou, L. Dahmani,
457"
REFERENCES,0.5577523413111343,"D. Wang, et al. Fast cortical surface reconstruction from MRI using deep learning. Brain
458"
REFERENCES,0.558792924037461,"Informatics, 9(1):1–16, 2022.
459"
REFERENCES,0.5598335067637877,"[42] L. M. Rimol, R. Nesvåg, D. J. Hagler Jr, Ø. Bergmann, C. Fennema-Notestine, C. B. Hartberg,
460"
REFERENCES,0.5608740894901144,"U. K. Haukvik, E. Lange, C. J. Pung, A. Server, et al. Cortical volume, surface area, and
461"
REFERENCES,0.5619146722164412,"thickness in schizophrenia and bipolar disorder. Biological Psychiatry, 71(6):552–560, 2012.
462"
REFERENCES,0.562955254942768,"[43] J. M. Roe, D. Vidal-Piñeiro, Ø. Sørensen, A. M. Brandmaier, S. Düzel, H. A. Gonzalez, R. A.
463"
REFERENCES,0.5639958376690947,"Kievit, E. Knights, S. Kühn, U. Lindenberger, et al. Asymmetric thinning of the cerebral
464"
REFERENCES,0.5650364203954215,"cortex across the adult lifespan is accelerated in alzheimer’s disease. Nature Communications,
465"
REFERENCES,0.5660770031217481,"12(1):721, 2021.
466"
REFERENCES,0.5671175858480749,"[44] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolutional networks for biomedical image
467"
REFERENCES,0.5681581685744017,"segmentation. In International Conference on Medical Image Computing and Computer-Assisted
468"
REFERENCES,0.5691987513007284,"Intervention, pages 234–241, 2015.
469"
REFERENCES,0.5702393340270552,"[45] A. G. Roy, S. Conjeti, N. Navab, C. Wachinger, A. D. N. Initiative, et al. Quicknat: A fully
470"
REFERENCES,0.5712799167533819,"convolutional network for quick and accurate segmentation of neuroanatomy. NeuroImage,
471"
REFERENCES,0.5723204994797086,"186:713–727, 2019.
472"
REFERENCES,0.5733610822060354,"[46] D. Ruelle and D. Sullivan. Currents, flows and diffeomorphisms. Topology, 14(4):319–327,
473"
REFERENCES,0.5744016649323621,"1975.
474"
REFERENCES,0.5754422476586889,"[47] R. Santa Cruz, L. Lebrat, D. Fu, P. Bourgeat, J. Fripp, C. Fookes, and O. Salvado. Corti-
475"
REFERENCES,0.5764828303850156,"calFlow++: Boosting cortical surface reconstruction accuracy, regularity, and interoperability.
476"
REFERENCES,0.5775234131113424,"In International Conferences on Medical Image Computing and Computer Assisted Intervention,
477"
REFERENCES,0.578563995837669,"pages 496–505. Springer, 2022.
478"
REFERENCES,0.5796045785639958,"[48] D. W. Shattuck and R. M. Leahy. Brainsuite: an automated cortical surface identification tool.
479"
REFERENCES,0.5806451612903226,"Medical Image Analysis, 6(2):129–142, 2002.
480"
REFERENCES,0.5816857440166493,"[49] A. A. Taha and A. Hanbury. Metrics for evaluating 3D medical image segmentation: analysis,
481"
REFERENCES,0.5827263267429761,"selection, and tool. BMC Medical Imaging, 15(1):1–28, 2015.
482"
REFERENCES,0.5837669094693028,"[50] G. Teschl. Ordinary differential equations and dynamical systems, volume 140. American
483"
REFERENCES,0.5848074921956296,"Mathematical Soc., 2012.
484"
REFERENCES,0.5858480749219563,"[51] D. C. Van Essen, H. A. Drury, S. Joshi, and M. I. Miller. Functional and structural mapping of
485"
REFERENCES,0.586888657648283,"human cerebral cortex: solutions are in the surfaces. Proceedings of the National Academy of
486"
REFERENCES,0.5879292403746098,"Sciences, 95(3):788–795, 1998.
487"
REFERENCES,0.5889698231009365,"[52] L. Wang, Z. Wu, L. Chen, Y. Sun, W. Lin, and G. Li. ibeat v2.0: a multisite-applicable, deep
488"
REFERENCES,0.5900104058272633,"learning-based pipeline for infant cerebral cortical surface reconstruction. Nature protocols,
489"
REFERENCES,0.5910509885535901,"18(5):1488–1509, 2023.
490"
REFERENCES,0.5920915712799167,"[53] N. Wang, Y. Zhang, Z. Li, Y. Fu, H. Yu, W. Liu, X. Xue, and Y.-G. Jiang. Pixel2Mesh: 3D
491"
REFERENCES,0.5931321540062435,"mesh model generation via image guided deformation. IEEE Transactions on Pattern Analysis
492"
REFERENCES,0.5941727367325702,"and Machine Intelligence, 43(10):3600–3613, 2020.
493"
REFERENCES,0.595213319458897,"[54] H. Zheng, H. Li, and Y. Fan. Coupled reconstruction of cortical surfaces by diffeomorphic mesh
494"
REFERENCES,0.5962539021852237,"deformation. Advances in Neural Information Processing Systems, 37, 2023.
495"
REFERENCES,0.5972944849115505,"A
Model Details
496"
REFERENCES,0.5983350676378772,"A.1
Cortical Ribbon Segmentation Network Architecture
497"
REFERENCES,0.5993756503642039,"Fig. 4 (Left) shows the detailed network architecture of our cortical ribbon segmentation network,
498"
REFERENCES,0.6004162330905307,"which is a 5-level hierarchical encoder-decoder with skip connections. The network processes a 3D
499"
REFERENCES,0.6014568158168574,"brain MRI to produce a cortical ribbon segmentation map. The white matter (WM) segmentation
500"
REFERENCES,0.6024973985431842,"includes the interior of the WM surface, encompassing cortical WM, deep gray matter, ventricles,
501"
REFERENCES,0.603537981269511,"hippocampus, and other tissues within the surface. Similarly, the gray matter (GM) segmentation
502"
REFERENCES,0.6045785639958376,"includes the interior of the pial surface. The output map has five classes: left hemisphere WM and
503"
REFERENCES,0.6056191467221644,"GM, right hemisphere WM and GM, and background. In the encoder, each level uses a 3 × 3 × 3
504"
REFERENCES,0.6066597294484911,"convolutional layer with a stride of 2 to downsample the features. In the decoder, features are
505"
REFERENCES,0.6077003121748179,"upsampled by 2× at each scale, concatenated with the corresponding features from the encoder via
506"
REFERENCES,0.6087408949011447,"skip connections, and then fused using a 3 × 3 × 3 convolutional layer with a stride of 1. For feature
507"
REFERENCES,0.6097814776274714,"extraction at the input, a 3 × 3 × 3 convolutional layer with a stride of 1 is used. Before the final
508"
REFERENCES,0.6108220603537982,"prediction, three consecutive convolutional layers are applied. Each convolutional layer is followed
509"
REFERENCES,0.6118626430801248,"by a leaky ReLU activation function, except for the last one, which uses a Softmax function before
510"
REFERENCES,0.6129032258064516,"computing the cross-entropy loss with the ground truth.
511 32 64 128 128 128 128 64 64 32 32 16"
REFERENCES,0.6139438085327783,"Input
(1-channel) 64 32 16 16
16 16 16 5"
REFERENCES,0.6149843912591051,"3×3×3 conv, stride = 2
2× upsampling 32"
REFERENCES,0.6160249739854319,Input (4-channel) 64 128 256 128 128 64 64 32 32 32
REFERENCES,0.6170655567117586,"Skip connection
Velocity field
2×2×2 deconv, stride = 2
3×3×3 conv, stride = 1"
REFERENCES,0.6181061394380853,"Figure 4: Left: 3D U-Net architecture for ribbon segmentation. The output, i.e., the cortical ribbon
map, is overlaid on the input image for illustration. Right: 3D U-Net architecture for cortical surface
reconstruction. The learned velocity fields are used to calculate deformations."
REFERENCES,0.619146722164412,"A.2
Cortical Surface Reconstruction Network Architecture and Training details
512"
REFERENCES,0.6201873048907388,"As shown in Fig.4 (Right), our cortical surface reconstruction (CSR) network operates at five scales.
513"
REFERENCES,0.6212278876170656,"To conserve memory, we downsample the input image using a 3 × 3 × 3 convolution with a stride of
514"
REFERENCES,0.6222684703433923,"2 and skip complex feature fusion via skip connections in the decoding path at this scale. To improve
515"
REFERENCES,0.6233090530697191,"the accuracy of the velocity fields (VFs), we use 2 × 2 × 2 deconvolutions with a stride of 2 in the
516"
REFERENCES,0.6243496357960457,"decoding path instead of 2× trilinear upsampling. At the output stage, we employ three parallel
517"
REFERENCES,0.6253902185223725,"3 × 3 × 3 convolutional layers to generate VFs for the white matter (WM), midthickness, and pial
518"
REFERENCES,0.6264308012486993,"surfaces, respectively. ReLU activation functions are used after each convolutional layer, except for
519"
REFERENCES,0.627471383975026,"the three parallel layers, where Softsign functions are applied. The VFs are then utilized to compute
520"
REFERENCES,0.6285119667013528,"diffeomorphic deformations.
521"
REFERENCES,0.6295525494276795,"B
Experimental Settings
522"
REFERENCES,0.6305931321540063,"B.1
Dataset Preprocessing
523"
REFERENCES,0.631633714880333,"We preprocessed all the MRIs of the ADNI-1 [24] and OASIS-1 [35] datasets with the same protocols
524"
REFERENCES,0.6326742976066597,"as following: Based on the standard processing protocol in FreeSurfer V7.2.0 [17], the original
525"
REFERENCES,0.6337148803329865,"images were conformed and normalized (saved as orig.mgz), affinely registered to the MNI152
526"
REFERENCES,0.6347554630593132,"template [8] using the NiftyReg toolbox [37]. The respective ribbon segmentation maps, SDFs, and
527"
REFERENCES,0.63579604578564,"pseudo-ground-truth surfaces were also transformed using the computed transformation. Similarly,
528"
REFERENCES,0.6368366285119667,"we utilize iBEAT V2.0 [52] to process the BCP [23] dataset and merge the brain tissue segmentation
529"
REFERENCES,0.6378772112382934,"results as the ribbon segmentation maps.
530"
REFERENCES,0.6389177939646202,"B.2
Baselines
531"
REFERENCES,0.6399583766909469,"We compared our SegCSR with representatives from the two categories of existing DL-based CSR
532"
REFERENCES,0.6409989594172737,"methods and evaluated their performance for both WM and pial surface reconstruction. DeepCSR [13]
533"
REFERENCES,0.6420395421436004,"and 3D U-Net [44] represent implicit surface reconstruction methods, while others fall into the
534"
REFERENCES,0.6430801248699272,"category of explicit methods. Note that we modify the 3D U-Net method to first generate SDFs
535"
REFERENCES,0.644120707596254,"based on the ribbon segmentation results, then perform topology correction, and finally utilize
536"
REFERENCES,0.6451612903225806,"the Marching Cubes algorithm to extract the mesh. Since it does not require pGT surfaces from
537"
REFERENCES,0.6462018730489074,"FreeSurfer for training supervision, it can be treated as a weakly supervised learning-based baseline.
538"
REFERENCES,0.6472424557752341,"CorticalFlow++[47] utilizes smoothed convex hulls as the initialization template, trains a chain of
539"
REFERENCES,0.6482830385015609,"deformation fields, and employs a fourth-order Runge-Kutta (RK4) solver to compute the integration
540"
REFERENCES,0.6493236212278877,"for the initial value problem. CortexODE[31] uses WM segmentation for surface initialization and
541"
REFERENCES,0.6503642039542143,"Neural ODE for deformation computation. Vox2cortex [8] deforms averaged surface templates with
542"
REFERENCES,0.6514047866805411,"a GNN-based network to reconstruct multiple surfaces. CoCSR [54] integrates multiple cortical
543"
REFERENCES,0.6524453694068678,"surface reconstructions into a single network. A summary of the state-of-the-art CSR methods is
544"
REFERENCES,0.6534859521331946,"provided in Table 4.
545"
REFERENCES,0.6545265348595213,"Table 4: Summary of baseline methods in terms of surface representation, supervision in training,
and loss functions."
REFERENCES,0.6555671175858481,"Method
Representation
Supervision
Primary Loss function
3D U-Net [44]
Implicit
Ribbon segmentation
Cross Entropy
DeepCSR [13]
SDFs
L1 Loss
CorticalFlow++ [47]"
REFERENCES,0.6566077003121749,Explicit
REFERENCES,0.6576482830385015,"Mesh
Bi-directional Chamfer Loss
cortexODE [31]
Mesh
Bi-directional Chamfer Loss
Vox2Cortex [8]
Mesh
Bi-directional Chamfer Loss
CoCSR [54]
Mesh
Bi-directional Chamfer Loss
SegCSR (Ours)
Explicit
Ribbon segmentation
Weak Supervision"
REFERENCES,0.6586888657648283,"C
More Experimental Results
546"
REFERENCES,0.659729448491155,"C.1
Quantitative comparison of our methods with Related Works
547"
REFERENCES,0.6607700312174818,"Due to space limit, we only showcase the quantitative results on left hemisphere in the main paper.
548"
REFERENCES,0.6618106139438086,"Quantitative comparison results on the right hemisphere are summarized as a supplement to Table 1
549"
REFERENCES,0.6628511966701353,"in the main paper.
550"
REFERENCES,0.663891779396462,"Table 5: Quantitative analysis of cortical surface reconstruction on geometric accuracy and self-
intersections. The Chamfer distance (CD), average symmetric surface distance (ASSD), Hausdorff
distance (HD), and the ratio of the self-intersecting faces (SIF) were measured for WM and pial
surfaces on three datasets. The mean value and standard deviation are reported. Lower scores indicate
better results for all metrics. “S” denotes the use of pGT surfaces from conventional pipelines, while
“W” represents weak supervision by pGT ribbon segmentations. In each supervision setting, the best
results are in bold, and the second best results are underlined. Data Sup."
REFERENCES,0.6649323621227887,"Method
R-Pial Surface
R-WM Surface
CD (mm)
ASSD (mm)
HD (mm)
SIF (%)
CD (mm)
ASSD (mm)
HD (mm)
SIF (%) ADNI S"
REFERENCES,0.6659729448491155,"CorticalFlow++ [47]
0.550±0.038
0.413±0.034
0.891±0.071
0.101±0.069
0.548±0.035
0.403±0.032
0.883±0.068
0.071±0.042
cortexODE [31]
0.482±0.019
0.220±0.022
0.461±0.060
0.033±0.017
0.470±0.020
0.207±0.019
0.444±0.018
0.023±0.016
Vox2Cortex [8]
0.593±0.032
0.382±0.029
0.755±0.061
0.071±0.045
0.588±0.029
0.363±0.024
0.741±0.057
0.059±0.035
CoCSR [54]
0.326±0.023
0.126±0.012
0.271±0.024
0.015±0.013
0.320±0.020
0.124±0.012
0.265±0.022
0.006±0.003 W"
REFERENCES,0.6670135275754423,"DeepCSR [13]
0.948±0.080
0.597±0.068
1.154±0.207
\
0.942±0.077
0.589±0.065
1.140±0.195
\
3D U-Net [44]
0.601±0.048
0.342±0.037
0.784±0.166
\
0.476±0.014
0.268±0.016
0.563±0.031
\
SegCSR (Ours)
0.582±0.021
0.328±0.022
0.751±0.050
0.009±0.009
0.470±0.015
0.261±0.021
0.548±0.038
0.011±0.010 OASIS S"
REFERENCES,0.668054110301769,"CorticalFlow++ [47]
0.540±0.037
0.405±0.032
0.834±0.060
0.095±0.052
0.536±0.035
0.402±0.031
0.830±0.058
0.088±0.049
cortexODE [31]
0.497±0.023
0.225±0.024
0.473±0.065
0.038±0.027
0.481±0.021
0.214±0.021
0.450±0.022
0.025±0.019
Vox2Cortex [8]
0.598±0.033
0.386±0.031
0.761±0.064
0.072±0.040
0.592±0.031
0.379±0.028
0.752±0.061
0.061±0.037
CoCSR [54]
0.411±0.034
0.144±0.017
0.284±0.022
0.018±0.015
0.353±0.026
0.130±0.021
0.272±0.024
0.009±0.004 W"
REFERENCES,0.6690946930280958,"DeepCSR [13]
0.989±0.086
0.619±0.071
1.336±0.215
\
0.980±0.082
0.601±0.069
1.175±0.202
\
3D U-Net [44]
0.613±0.070
0.333±0.050
0.777±0.268
\
0.456±0.014
0.249±0.020
0.493±0.033
\
SegCSR (Ours)
0.584±0.018
0.323±0.019
0.728±0.041
0.012±0.011
0.452±0.012
0.224±0.016
0.465±0.030
0.012±0.010 BCP S"
REFERENCES,0.6701352757544224,"CorticalFlow++ [47]
0.926±0.271
0.729±0.035
1.940±0.174
1.113±0.374
0.892±0.240
0.721±0.033
1.877±0.148
0.531±0.105
cortexODE [31]
0.758±0.081
0.394±0.032
0.820±0.102
0.121±0.060
0.676±0.069
0.346±0.029
0.814±0.098
0.098±0.033
CoCSR [54]
0.575±0.038
0.214±0.022
0.464±0.059
0.060±0.037
0.542±0.038
0.198±0.020
0.446±0.049
0.056±0.030 W"
REFERENCES,0.6711758584807492,"DeepCSR [13]
2.672±1.131
1.222±0.214
3.101±1.209
\
1.437±0.519
0.426±0.049
0.927±0.116
\
3D U-Net [44]
1.174±0.312
0.790±0.058
2.136±1.020
\
0.687±0.118
0.376±0.039
0.788±0.063
\
SegCSR (Ours)
0.926±0.070
0.497±0.060
1.287±0.142
0.058±0.056
0.875±0.067
0.476±0.050
1.203±0.130
0.054±0.055"
REFERENCES,0.6722164412070759,"NeurIPS Paper Checklist
551"
CLAIMS,0.6732570239334027,"1. Claims
552"
CLAIMS,0.6742976066597295,"Question: Do the main claims made in the abstract and introduction accurately reflect the
553"
CLAIMS,0.6753381893860562,"paper’s contributions and scope?
554"
CLAIMS,0.676378772112383,"Answer: [Yes]
555"
CLAIMS,0.6774193548387096,"Justification: We clearly summarize the contributions in Section 1 Introduction.
556"
CLAIMS,0.6784599375650364,"Guidelines:
557"
CLAIMS,0.6795005202913632,"• The answer NA means that the abstract and introduction do not include the claims
558"
CLAIMS,0.6805411030176899,"made in the paper.
559"
CLAIMS,0.6815816857440167,"• The abstract and/or introduction should clearly state the claims made, including the
560"
CLAIMS,0.6826222684703434,"contributions made in the paper and important assumptions and limitations. A No or
561"
CLAIMS,0.6836628511966701,"NA answer to this question will not be perceived well by the reviewers.
562"
CLAIMS,0.6847034339229969,"• The claims made should match theoretical and experimental results, and reflect how
563"
CLAIMS,0.6857440166493236,"much the results can be expected to generalize to other settings.
564"
CLAIMS,0.6867845993756504,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
565"
CLAIMS,0.6878251821019771,"are not attained by the paper.
566"
LIMITATIONS,0.6888657648283039,"2. Limitations
567"
LIMITATIONS,0.6899063475546305,"Question: Does the paper discuss the limitations of the work performed by the authors?
568"
LIMITATIONS,0.6909469302809573,"Answer: [Yes]
569"
LIMITATIONS,0.6919875130072841,"Justification: We discuss the limitations of the work in Section 5 Conclusions.
570"
LIMITATIONS,0.6930280957336108,"Guidelines:
571"
LIMITATIONS,0.6940686784599376,"• The answer NA means that the paper has no limitation while the answer No means that
572"
LIMITATIONS,0.6951092611862643,"the paper has limitations, but those are not discussed in the paper.
573"
LIMITATIONS,0.696149843912591,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
574"
LIMITATIONS,0.6971904266389178,"• The paper should point out any strong assumptions and how robust the results are to
575"
LIMITATIONS,0.6982310093652445,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
576"
LIMITATIONS,0.6992715920915713,"model well-specification, asymptotic approximations only holding locally). The authors
577"
LIMITATIONS,0.700312174817898,"should reflect on how these assumptions might be violated in practice and what the
578"
LIMITATIONS,0.7013527575442248,"implications would be.
579"
LIMITATIONS,0.7023933402705516,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
580"
LIMITATIONS,0.7034339229968782,"only tested on a few datasets or with a few runs. In general, empirical results often
581"
LIMITATIONS,0.704474505723205,"depend on implicit assumptions, which should be articulated.
582"
LIMITATIONS,0.7055150884495317,"• The authors should reflect on the factors that influence the performance of the approach.
583"
LIMITATIONS,0.7065556711758585,"For example, a facial recognition algorithm may perform poorly when image resolution
584"
LIMITATIONS,0.7075962539021852,"is low or images are taken in low lighting. Or a speech-to-text system might not be
585"
LIMITATIONS,0.708636836628512,"used reliably to provide closed captions for online lectures because it fails to handle
586"
LIMITATIONS,0.7096774193548387,"technical jargon.
587"
LIMITATIONS,0.7107180020811654,"• The authors should discuss the computational efficiency of the proposed algorithms
588"
LIMITATIONS,0.7117585848074922,"and how they scale with dataset size.
589"
LIMITATIONS,0.7127991675338189,"• If applicable, the authors should discuss possible limitations of their approach to
590"
LIMITATIONS,0.7138397502601457,"address problems of privacy and fairness.
591"
LIMITATIONS,0.7148803329864725,"• While the authors might fear that complete honesty about limitations might be used by
592"
LIMITATIONS,0.7159209157127991,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
593"
LIMITATIONS,0.7169614984391259,"limitations that aren’t acknowledged in the paper. The authors should use their best
594"
LIMITATIONS,0.7180020811654526,"judgment and recognize that individual actions in favor of transparency play an impor-
595"
LIMITATIONS,0.7190426638917794,"tant role in developing norms that preserve the integrity of the community. Reviewers
596"
LIMITATIONS,0.7200832466181062,"will be specifically instructed to not penalize honesty concerning limitations.
597"
THEORY ASSUMPTIONS AND PROOFS,0.7211238293444329,"3. Theory Assumptions and Proofs
598"
THEORY ASSUMPTIONS AND PROOFS,0.7221644120707597,"Question: For each theoretical result, does the paper provide the full set of assumptions and
599"
THEORY ASSUMPTIONS AND PROOFS,0.7232049947970863,"a complete (and correct) proof?
600"
THEORY ASSUMPTIONS AND PROOFS,0.7242455775234131,"Answer: [NA]
601"
THEORY ASSUMPTIONS AND PROOFS,0.7252861602497399,"Justification: This is not a theoretical paper.
602"
THEORY ASSUMPTIONS AND PROOFS,0.7263267429760666,"Guidelines:
603"
THEORY ASSUMPTIONS AND PROOFS,0.7273673257023934,"• The answer NA means that the paper does not include theoretical results.
604"
THEORY ASSUMPTIONS AND PROOFS,0.72840790842872,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
605"
THEORY ASSUMPTIONS AND PROOFS,0.7294484911550468,"referenced.
606"
THEORY ASSUMPTIONS AND PROOFS,0.7304890738813735,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
607"
THEORY ASSUMPTIONS AND PROOFS,0.7315296566077003,"• The proofs can either appear in the main paper or the supplemental material, but if
608"
THEORY ASSUMPTIONS AND PROOFS,0.7325702393340271,"they appear in the supplemental material, the authors are encouraged to provide a short
609"
THEORY ASSUMPTIONS AND PROOFS,0.7336108220603538,"proof sketch to provide intuition.
610"
THEORY ASSUMPTIONS AND PROOFS,0.7346514047866806,"• Inversely, any informal proof provided in the core of the paper should be complemented
611"
THEORY ASSUMPTIONS AND PROOFS,0.7356919875130072,"by formal proofs provided in appendix or supplemental material.
612"
THEORY ASSUMPTIONS AND PROOFS,0.736732570239334,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
613"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7377731529656608,"4. Experimental Result Reproducibility
614"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7388137356919875,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
615"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7398543184183143,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
616"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.740894901144641,"of the paper (regardless of whether the code and data are provided or not)?
617"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7419354838709677,"Answer: [Yes]
618"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7429760665972945,"Justification: We clearly illustrate the methodology in Section 3, the experimental setups in
619"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7440166493236212,"Section 4.1, and more implementation details in the Supplementary Materials.
620"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.745057232049948,"Guidelines:
621"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7460978147762747,"• The answer NA means that the paper does not include experiments.
622"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7471383975026015,"• If the paper includes experiments, a No answer to this question will not be perceived
623"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7481789802289281,"well by the reviewers: Making the paper reproducible is important, regardless of
624"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7492195629552549,"whether the code and data are provided or not.
625"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7502601456815817,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
626"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7513007284079084,"to make their results reproducible or verifiable.
627"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7523413111342352,"• Depending on the contribution, reproducibility can be accomplished in various ways.
628"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7533818938605619,"For example, if the contribution is a novel architecture, describing the architecture fully
629"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7544224765868887,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
630"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7554630593132154,"be necessary to either make it possible for others to replicate the model with the same
631"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7565036420395421,"dataset, or provide access to the model. In general. releasing code and data is often
632"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7575442247658689,"one good way to accomplish this, but reproducibility can also be provided via detailed
633"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7585848074921956,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
634"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7596253902185224,"of a large language model), releasing of a model checkpoint, or other means that are
635"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7606659729448492,"appropriate to the research performed.
636"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7617065556711758,"• While NeurIPS does not require releasing code, the conference does require all submis-
637"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7627471383975026,"sions to provide some reasonable avenue for reproducibility, which may depend on the
638"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7637877211238293,"nature of the contribution. For example
639"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7648283038501561,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
640"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7658688865764828,"to reproduce that algorithm.
641"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7669094693028096,"(b) If the contribution is primarily a new model architecture, the paper should describe
642"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7679500520291364,"the architecture clearly and fully.
643"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.768990634755463,"(c) If the contribution is a new model (e.g., a large language model), then there should
644"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7700312174817898,"either be a way to access this model for reproducing the results or a way to reproduce
645"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7710718002081165,"the model (e.g., with an open-source dataset or instructions for how to construct
646"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7721123829344433,"the dataset).
647"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7731529656607701,"(d) We recognize that reproducibility may be tricky in some cases, in which case
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7741935483870968,"authors are welcome to describe the particular way they provide for reproducibility.
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7752341311134235,"In the case of closed-source models, it may be that access to the model is limited in
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7762747138397502,"some way (e.g., to registered users), but it should be possible for other researchers
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.777315296566077,"to have some path to reproducing or verifying the results.
652"
OPEN ACCESS TO DATA AND CODE,0.7783558792924038,"5. Open access to data and code
653"
OPEN ACCESS TO DATA AND CODE,0.7793964620187305,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
654"
OPEN ACCESS TO DATA AND CODE,0.7804370447450573,"tions to faithfully reproduce the main experimental results, as described in supplemental
655"
OPEN ACCESS TO DATA AND CODE,0.7814776274713839,"material?
656"
OPEN ACCESS TO DATA AND CODE,0.7825182101977107,"Answer: [No]
657"
OPEN ACCESS TO DATA AND CODE,0.7835587929240374,"Justification: (1) We used and cited public datasets and gave pre-processing steps in Sec-
658"
OPEN ACCESS TO DATA AND CODE,0.7845993756503642,"tion 4.1 and more details in the Supplementary Materials. (2) We provided code links for
659"
OPEN ACCESS TO DATA AND CODE,0.785639958376691,"public baselines in the Supplementary Materials. (3) Code of our proposed method will be
660"
OPEN ACCESS TO DATA AND CODE,0.7866805411030177,"made public upon acceptance of the paper.
661"
OPEN ACCESS TO DATA AND CODE,0.7877211238293444,"Guidelines:
662"
OPEN ACCESS TO DATA AND CODE,0.7887617065556711,"• The answer NA means that paper does not include experiments requiring code.
663"
OPEN ACCESS TO DATA AND CODE,0.7898022892819979,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
664"
OPEN ACCESS TO DATA AND CODE,0.7908428720083247,"public/guides/CodeSubmissionPolicy) for more details.
665"
OPEN ACCESS TO DATA AND CODE,0.7918834547346514,"• While we encourage the release of code and data, we understand that this might not be
666"
OPEN ACCESS TO DATA AND CODE,0.7929240374609782,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
667"
OPEN ACCESS TO DATA AND CODE,0.7939646201873048,"including code, unless this is central to the contribution (e.g., for a new open-source
668"
OPEN ACCESS TO DATA AND CODE,0.7950052029136316,"benchmark).
669"
OPEN ACCESS TO DATA AND CODE,0.7960457856399584,"• The instructions should contain the exact command and environment needed to run to
670"
OPEN ACCESS TO DATA AND CODE,0.7970863683662851,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
671"
OPEN ACCESS TO DATA AND CODE,0.7981269510926119,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
672"
OPEN ACCESS TO DATA AND CODE,0.7991675338189386,"• The authors should provide instructions on data access and preparation, including how
673"
OPEN ACCESS TO DATA AND CODE,0.8002081165452654,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
674"
OPEN ACCESS TO DATA AND CODE,0.8012486992715921,"• The authors should provide scripts to reproduce all experimental results for the new
675"
OPEN ACCESS TO DATA AND CODE,0.8022892819979188,"proposed method and baselines. If only a subset of experiments are reproducible, they
676"
OPEN ACCESS TO DATA AND CODE,0.8033298647242456,"should state which ones are omitted from the script and why.
677"
OPEN ACCESS TO DATA AND CODE,0.8043704474505723,"• At submission time, to preserve anonymity, the authors should release anonymized
678"
OPEN ACCESS TO DATA AND CODE,0.8054110301768991,"versions (if applicable).
679"
OPEN ACCESS TO DATA AND CODE,0.8064516129032258,"• Providing as much information as possible in supplemental material (appended to the
680"
OPEN ACCESS TO DATA AND CODE,0.8074921956295525,"paper) is recommended, but including URLs to data and code is permitted.
681"
OPEN ACCESS TO DATA AND CODE,0.8085327783558793,"6. Experimental Setting/Details
682"
OPEN ACCESS TO DATA AND CODE,0.809573361082206,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
683"
OPEN ACCESS TO DATA AND CODE,0.8106139438085328,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
684"
OPEN ACCESS TO DATA AND CODE,0.8116545265348595,"results?
685"
OPEN ACCESS TO DATA AND CODE,0.8126951092611863,"Answer: [Yes]
686"
OPEN ACCESS TO DATA AND CODE,0.813735691987513,"Justification: We specify all the training and test details in Section 4.1 and the Supplementary
687"
OPEN ACCESS TO DATA AND CODE,0.8147762747138397,"Materials.
688"
OPEN ACCESS TO DATA AND CODE,0.8158168574401665,"Guidelines:
689"
OPEN ACCESS TO DATA AND CODE,0.8168574401664932,"• The answer NA means that the paper does not include experiments.
690"
OPEN ACCESS TO DATA AND CODE,0.81789802289282,"• The experimental setting should be presented in the core of the paper to a level of detail
691"
OPEN ACCESS TO DATA AND CODE,0.8189386056191468,"that is necessary to appreciate the results and make sense of them.
692"
OPEN ACCESS TO DATA AND CODE,0.8199791883454735,"• The full details can be provided either with the code, in appendix, or as supplemental
693"
OPEN ACCESS TO DATA AND CODE,0.8210197710718002,"material.
694"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8220603537981269,"7. Experiment Statistical Significance
695"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8231009365244537,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
696"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8241415192507804,"information about the statistical significance of the experiments?
697"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8251821019771072,"Answer: [Yes]
698"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.826222684703434,"Justification: We reported mean and standard deviation of each experimental setting and
699"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8272632674297606,"computed the statistical significance.
700"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8283038501560874,"Guidelines:
701"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8293444328824141,"• The answer NA means that the paper does not include experiments.
702"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8303850156087409,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
703"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8314255983350677,"dence intervals, or statistical significance tests, at least for the experiments that support
704"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8324661810613944,"the main claims of the paper.
705"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8335067637877211,"• The factors of variability that the error bars are capturing should be clearly stated (for
706"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8345473465140478,"example, train/test split, initialization, random drawing of some parameter, or overall
707"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8355879292403746,"run with given experimental conditions).
708"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8366285119667014,"• The method for calculating the error bars should be explained (closed form formula,
709"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8376690946930281,"call to a library function, bootstrap, etc.)
710"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8387096774193549,"• The assumptions made should be given (e.g., Normally distributed errors).
711"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8397502601456815,"• It should be clear whether the error bar is the standard deviation or the standard error
712"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8407908428720083,"of the mean.
713"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.841831425598335,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
714"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8428720083246618,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
715"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8439125910509886,"of Normality of errors is not verified.
716"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8449531737773153,"• For asymmetric distributions, the authors should be careful not to show in tables or
717"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8459937565036421,"figures symmetric error bars that would yield results that are out of range (e.g. negative
718"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8470343392299687,"error rates).
719"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8480749219562955,"• If error bars are reported in tables or plots, The authors should explain in the text how
720"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8491155046826223,"they were calculated and reference the corresponding figures or tables in the text.
721"
EXPERIMENTS COMPUTE RESOURCES,0.850156087408949,"8. Experiments Compute Resources
722"
EXPERIMENTS COMPUTE RESOURCES,0.8511966701352758,"Question: For each experiment, does the paper provide sufficient information on the com-
723"
EXPERIMENTS COMPUTE RESOURCES,0.8522372528616025,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
724"
EXPERIMENTS COMPUTE RESOURCES,0.8532778355879292,"the experiments?
725"
EXPERIMENTS COMPUTE RESOURCES,0.854318418314256,"Answer: [Yes]
726"
EXPERIMENTS COMPUTE RESOURCES,0.8553590010405827,"Justification: We specify the computation resources for training and testing in Section 4.1
727"
EXPERIMENTS COMPUTE RESOURCES,0.8563995837669095,"and more details in the Supplementary Materials.
728"
EXPERIMENTS COMPUTE RESOURCES,0.8574401664932362,"Guidelines:
729"
EXPERIMENTS COMPUTE RESOURCES,0.858480749219563,"• The answer NA means that the paper does not include experiments.
730"
EXPERIMENTS COMPUTE RESOURCES,0.8595213319458896,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
731"
EXPERIMENTS COMPUTE RESOURCES,0.8605619146722164,"or cloud provider, including relevant memory and storage.
732"
EXPERIMENTS COMPUTE RESOURCES,0.8616024973985432,"• The paper should provide the amount of compute required for each of the individual
733"
EXPERIMENTS COMPUTE RESOURCES,0.8626430801248699,"experimental runs as well as estimate the total compute.
734"
EXPERIMENTS COMPUTE RESOURCES,0.8636836628511967,"• The paper should disclose whether the full research project required more compute
735"
EXPERIMENTS COMPUTE RESOURCES,0.8647242455775234,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
736"
EXPERIMENTS COMPUTE RESOURCES,0.8657648283038502,"didn’t make it into the paper).
737"
CODE OF ETHICS,0.8668054110301769,"9. Code Of Ethics
738"
CODE OF ETHICS,0.8678459937565036,"Question: Does the research conducted in the paper conform, in every respect, with the
739"
CODE OF ETHICS,0.8688865764828304,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
740"
CODE OF ETHICS,0.8699271592091571,"Answer: [Yes]
741"
CODE OF ETHICS,0.8709677419354839,"Justification: We review and conform with the NeurIPS Code of Ethics.
742"
CODE OF ETHICS,0.8720083246618107,"Guidelines:
743"
CODE OF ETHICS,0.8730489073881373,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
744"
CODE OF ETHICS,0.8740894901144641,"• If the authors answer No, they should explain the special circumstances that require a
745"
CODE OF ETHICS,0.8751300728407908,"deviation from the Code of Ethics.
746"
CODE OF ETHICS,0.8761706555671176,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
747"
CODE OF ETHICS,0.8772112382934444,"eration due to laws or regulations in their jurisdiction).
748"
BROADER IMPACTS,0.8782518210197711,"10. Broader Impacts
749"
BROADER IMPACTS,0.8792924037460979,"Question: Does the paper discuss both potential positive societal impacts and negative
750"
BROADER IMPACTS,0.8803329864724245,"societal impacts of the work performed?
751"
BROADER IMPACTS,0.8813735691987513,"Answer: [Yes]
752"
BROADER IMPACTS,0.882414151925078,"Justification: We discuss the societal impacts of the work in Section 5 Conclusions.
753"
BROADER IMPACTS,0.8834547346514048,"Guidelines:
754"
BROADER IMPACTS,0.8844953173777316,"• The answer NA means that there is no societal impact of the work performed.
755"
BROADER IMPACTS,0.8855359001040582,"• If the authors answer NA or No, they should explain why their work has no societal
756"
BROADER IMPACTS,0.886576482830385,"impact or why the paper does not address societal impact.
757"
BROADER IMPACTS,0.8876170655567117,"• Examples of negative societal impacts include potential malicious or unintended uses
758"
BROADER IMPACTS,0.8886576482830385,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
759"
BROADER IMPACTS,0.8896982310093653,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
760"
BROADER IMPACTS,0.890738813735692,"groups), privacy considerations, and security considerations.
761"
BROADER IMPACTS,0.8917793964620188,"• The conference expects that many papers will be foundational research and not tied
762"
BROADER IMPACTS,0.8928199791883454,"to particular applications, let alone deployments. However, if there is a direct path to
763"
BROADER IMPACTS,0.8938605619146722,"any negative applications, the authors should point it out. For example, it is legitimate
764"
BROADER IMPACTS,0.894901144640999,"to point out that an improvement in the quality of generative models could be used to
765"
BROADER IMPACTS,0.8959417273673257,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
766"
BROADER IMPACTS,0.8969823100936525,"that a generic algorithm for optimizing neural networks could enable people to train
767"
BROADER IMPACTS,0.8980228928199792,"models that generate Deepfakes faster.
768"
BROADER IMPACTS,0.899063475546306,"• The authors should consider possible harms that could arise when the technology is
769"
BROADER IMPACTS,0.9001040582726326,"being used as intended and functioning correctly, harms that could arise when the
770"
BROADER IMPACTS,0.9011446409989594,"technology is being used as intended but gives incorrect results, and harms following
771"
BROADER IMPACTS,0.9021852237252862,"from (intentional or unintentional) misuse of the technology.
772"
BROADER IMPACTS,0.9032258064516129,"• If there are negative societal impacts, the authors could also discuss possible mitigation
773"
BROADER IMPACTS,0.9042663891779397,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
774"
BROADER IMPACTS,0.9053069719042663,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
775"
BROADER IMPACTS,0.9063475546305931,"feedback over time, improving the efficiency and accessibility of ML).
776"
SAFEGUARDS,0.9073881373569199,"11. Safeguards
777"
SAFEGUARDS,0.9084287200832466,"Question: Does the paper describe safeguards that have been put in place for responsible
778"
SAFEGUARDS,0.9094693028095734,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
779"
SAFEGUARDS,0.9105098855359001,"image generators, or scraped datasets)?
780"
SAFEGUARDS,0.9115504682622269,"Answer: [NA]
781"
SAFEGUARDS,0.9125910509885536,"Justification: Our paper poses no such risks.
782"
SAFEGUARDS,0.9136316337148803,"Guidelines:
783"
SAFEGUARDS,0.9146722164412071,"• The answer NA means that the paper poses no such risks.
784"
SAFEGUARDS,0.9157127991675338,"• Released models that have a high risk for misuse or dual-use should be released with
785"
SAFEGUARDS,0.9167533818938606,"necessary safeguards to allow for controlled use of the model, for example by requiring
786"
SAFEGUARDS,0.9177939646201873,"that users adhere to usage guidelines or restrictions to access the model or implementing
787"
SAFEGUARDS,0.918834547346514,"safety filters.
788"
SAFEGUARDS,0.9198751300728408,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
789"
SAFEGUARDS,0.9209157127991675,"should describe how they avoided releasing unsafe images.
790"
SAFEGUARDS,0.9219562955254943,"• We recognize that providing effective safeguards is challenging, and many papers do
791"
SAFEGUARDS,0.922996878251821,"not require this, but we encourage authors to take this into account and make a best
792"
SAFEGUARDS,0.9240374609781478,"faith effort.
793"
LICENSES FOR EXISTING ASSETS,0.9250780437044746,"12. Licenses for existing assets
794"
LICENSES FOR EXISTING ASSETS,0.9261186264308012,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
795"
LICENSES FOR EXISTING ASSETS,0.927159209157128,"the paper, properly credited and are the license and terms of use explicitly mentioned and
796"
LICENSES FOR EXISTING ASSETS,0.9281997918834547,"properly respected?
797"
LICENSES FOR EXISTING ASSETS,0.9292403746097815,"Answer: [Yes]
798"
LICENSES FOR EXISTING ASSETS,0.9302809573361083,"Justification: We properly cite relevant baseline methods and datasets.
799"
LICENSES FOR EXISTING ASSETS,0.931321540062435,"Guidelines:
800"
LICENSES FOR EXISTING ASSETS,0.9323621227887617,"• The answer NA means that the paper does not use existing assets.
801"
LICENSES FOR EXISTING ASSETS,0.9334027055150884,"• The authors should cite the original paper that produced the code package or dataset.
802"
LICENSES FOR EXISTING ASSETS,0.9344432882414152,"• The authors should state which version of the asset is used and, if possible, include a
803"
LICENSES FOR EXISTING ASSETS,0.9354838709677419,"URL.
804"
LICENSES FOR EXISTING ASSETS,0.9365244536940687,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
805"
LICENSES FOR EXISTING ASSETS,0.9375650364203955,"• For scraped data from a particular source (e.g., website), the copyright and terms of
806"
LICENSES FOR EXISTING ASSETS,0.9386056191467221,"service of that source should be provided.
807"
LICENSES FOR EXISTING ASSETS,0.9396462018730489,"• If assets are released, the license, copyright information, and terms of use in the
808"
LICENSES FOR EXISTING ASSETS,0.9406867845993756,"package should be provided. For popular datasets, paperswithcode.com/datasets
809"
LICENSES FOR EXISTING ASSETS,0.9417273673257024,"has curated licenses for some datasets. Their licensing guide can help determine the
810"
LICENSES FOR EXISTING ASSETS,0.9427679500520292,"license of a dataset.
811"
LICENSES FOR EXISTING ASSETS,0.9438085327783559,"• For existing datasets that are re-packaged, both the original license and the license of
812"
LICENSES FOR EXISTING ASSETS,0.9448491155046826,"the derived asset (if it has changed) should be provided.
813"
LICENSES FOR EXISTING ASSETS,0.9458896982310093,"• If this information is not available online, the authors are encouraged to reach out to
814"
LICENSES FOR EXISTING ASSETS,0.9469302809573361,"the asset’s creators.
815"
NEW ASSETS,0.9479708636836629,"13. New Assets
816"
NEW ASSETS,0.9490114464099896,"Question: Are new assets introduced in the paper well documented and is the documentation
817"
NEW ASSETS,0.9500520291363164,"provided alongside the assets?
818"
NEW ASSETS,0.951092611862643,"Answer: [Yes]
819"
NEW ASSETS,0.9521331945889698,"Justification: We communicate the details of the dataset/code/model in our main paper and
820"
NEW ASSETS,0.9531737773152965,"Supplementary Materials. We will make code and model public upon the acceptance of the
821"
NEW ASSETS,0.9542143600416233,"paper.
822"
NEW ASSETS,0.9552549427679501,"Guidelines:
823"
NEW ASSETS,0.9562955254942768,"• The answer NA means that the paper does not release new assets.
824"
NEW ASSETS,0.9573361082206036,"• Researchers should communicate the details of the dataset/code/model as part of their
825"
NEW ASSETS,0.9583766909469302,"submissions via structured templates. This includes details about training, license,
826"
NEW ASSETS,0.959417273673257,"limitations, etc.
827"
NEW ASSETS,0.9604578563995838,"• The paper should discuss whether and how consent was obtained from people whose
828"
NEW ASSETS,0.9614984391259105,"asset is used.
829"
NEW ASSETS,0.9625390218522373,"• At submission time, remember to anonymize your assets (if applicable). You can either
830"
NEW ASSETS,0.963579604578564,"create an anonymized URL or include an anonymized zip file.
831"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9646201873048907,"14. Crowdsourcing and Research with Human Subjects
832"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9656607700312175,"Question: For crowdsourcing experiments and research with human subjects, does the paper
833"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9667013527575442,"include the full text of instructions given to participants and screenshots, if applicable, as
834"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.967741935483871,"well as details about compensation (if any)?
835"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9687825182101977,"Answer: [NA]
836"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9698231009365245,"Justification: Our paper does not involve crowdsourcing nor research with human subjects.
837"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9708636836628513,"Guidelines:
838"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9719042663891779,"• The answer NA means that the paper does not involve crowdsourcing nor research with
839"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9729448491155047,"human subjects.
840"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9739854318418314,"• Including this information in the supplemental material is fine, but if the main contribu-
841"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9750260145681582,"tion of the paper involves human subjects, then as much detail as possible should be
842"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760665972944849,"included in the main paper.
843"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9771071800208116,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
844"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9781477627471384,"or other labor should be paid at least the minimum wage in the country of the data
845"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9791883454734651,"collector.
846"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9802289281997919,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9812695109261186,"Subjects
848"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9823100936524454,"Question: Does the paper describe potential risks incurred by study participants, whether
849"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9833506763787722,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
850"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9843912591050988,"approvals (or an equivalent approval/review based on the requirements of your country or
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9854318418314256,"institution) were obtained?
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9864724245577523,"Answer: [NA]
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9875130072840791,"Justification: Our paper does not involve crowdsourcing nor research with human subjects.
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9885535900104059,"Guidelines:
855"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9895941727367326,"• The answer NA means that the paper does not involve crowdsourcing nor research with
856"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9906347554630593,"human subjects.
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991675338189386,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
858"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9927159209157128,"may be required for any human subjects research. If you obtained IRB approval, you
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9937565036420395,"should clearly state this in the paper.
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947970863683663,"• We recognize that the procedures for this may vary significantly between institutions
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9958376690946931,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9968782518210197,"guidelines for their institution.
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979188345473465,"• For initial submissions, do not include any information that would break anonymity (if
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989594172736732,"applicable), such as the institution conducting the review.
865"
