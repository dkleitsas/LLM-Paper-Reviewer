Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0015105740181268882,"We present a comprehensive solution to learn and improve text-to-image models
from human preference feedback. To begin with, we build ImageReward—the
first general-purpose text-to-image human preference reward model—to effectively
encode human preferences. Its training is based on our systematic annotation
pipeline including rating and ranking, which collects 137k expert comparisons to
date. In human evaluation, ImageReward outperforms existing scoring models
and metrics, making it a promising automatic metric for evaluating text-to-image
synthesis. On top of it, we propose Reward Feedback Learning (ReFL), a direct
tuning algorithm to optimize diffusion models against a scorer. Both automatic and
human evaluation support ReFL’s advantages over compared methods. All code
and datasets are provided at https://github.com/THUDM/ImageReward."
INTRODUCTION,0.0030211480362537764,"1
Introduction"
INTRODUCTION,0.004531722054380665,"Text-to-image generative models, including auto-regressive [43; 11; 14; 16; 12; 63] and diffusion-
based [37; 45; 42; 46] approaches, have experienced rapid advancements in recent years. Given
appropriate text descriptions (i.e., prompts), these models can generate high-fidelity and semantically-
related images on a wide range of topics, attracting significant public interest in their potential
applications and impacts."
INTRODUCTION,0.006042296072507553,"Despite the progress, existing self-supervised pre-trained [33] generators are far from perfect. A
primary challenge lies in aligning models with human preference, as the pre-training distribution is
noisy and differs from the actual user-prompt distributions. The inherent discrepancy leads to several
well-documented issues in the generated images [15; 31], including but not limited to:"
INTRODUCTION,0.0075528700906344415,"• Text-image Alignment: failing to accurately depict all the numbers, attributes, properties, and
relationships of objects described in text prompts, as shown in Figure 1 (a)(b).
• Body Problem: presenting distorted, incomplete, duplicated, or abnormal body parts (e.g., limbs)
of humans or animals, as illustrated in Figure 1 (e)(f).
• Human Aesthetic: deviating from the average or mainstream human preference for aesthetic
styles, as demonstrated in Figure 1 (c)(d).
• Toxicity and Biases: featuring content that is harmful, violent, sexual, discriminative, illegal, or
causing psychological discomfort, as depicted in Figure 1 (f)."
INTRODUCTION,0.00906344410876133,"These prevalent challenges, however, are difficult to address solely through improvements in model
architectures and pre-training data."
INTRODUCTION,0.010574018126888218,"In natural language processing (NLP), researchers have employed reinforcement learning from human
feedback (RLHF) [55; 36; 39] to guide large language models [6; 7; 66; 48; 64] towards human
preferences and values. The approach relies on learning a reward model (RM) to capture human"
INTRODUCTION,0.012084592145015106,"*JX & XL contributed equally. Corresponding authors: YD & JT (yuxiaod|jietang@tsinghua.edu.cn).
†Work done when JX interned at Zhipu AI and QL visited Tsinghua University."
INTRODUCTION,0.013595166163141994,"(a) A painting of a 
girl walking in a 
hallway and 
suddenly ﬁnds a 
giant sunﬂower on 
the ﬂoor blocking 
her way."
INTRODUCTION,0.015105740181268883,"Aesthetic
CLIP Score
BLIP Score
ImageReward"
INTRODUCTION,0.01661631419939577,(Ours)
INTRODUCTION,0.01812688821752266,"(d) A unicorn in a 
clearing. it has a 
single shining horn. 
volumetric light. by 
emmanuel shiu, 
harry potter, 
eragon."
INTRODUCTION,0.019637462235649546,"(c) Symmetry!! 
Product render 
poster vivid colors 
divine proportion 
owl, glowing fog 
intricate, elegant, 
highly detailed."
INTRODUCTION,0.021148036253776436,"(b) Coronation of 
the sun emperor, 
digital art, 
illustration, 4k 
resolution, intricate, 
extremely detailed, 
depth, vivid colors."
INTRODUCTION,0.022658610271903322,"(e) Highly detailed 
portrait of a 
woman with long 
hairs, stephen bliss, 
unreal engine, 
fantasy art by greg 
rutkowski."
INTRODUCTION,0.02416918429003021,"(f) Sculpture made 
of ﬂame, portrait, 
female, future, 
torch, ﬁre, harper's 
bazaar, vogue, 
fashion magazine, 
intricate. ReFL"
INTRODUCTION,0.0256797583081571,(Ours)
INTRODUCTION,0.027190332326283987,"Figure 1: (Upper) Top-1 images out of 64 generations selected by different text-image scorers.
(Lower) 1-shot generation after ReFL training using ImageReward as feedback. Images get better
text coherence and human preference after ImageReward selection or ReFL training. In prompts
(from real users, truncated), the bold roughly denotes content, and the italic denotes style or function."
INTRODUCTION,0.028700906344410877,"preference from massive expert-annotated model output comparisons. Effective though it is, the
annotation process can be costly and challenging [39], as it requires months of effort to establish
labeling criteria, recruit and train experts, verify responses, and ultimately produce the RM."
INTRODUCTION,0.030211480362537766,"Contributions. Recognizing the importance of addressing these challenges in generative models, we
present and release the first general-purpose text-to-image human preference RM—ImageReward—
which is trained and evaluated on 137k pairs of expert comparisons in total, based on real-world
user prompts and corresponding model outputs. Based on the effort, we further investigate the direct
optimization approach ReFL for improving diffusion generative models. Our main contributions are:"
INTRODUCTION,0.03172205438066465,"• We systematically identify the challenges for text-to-image human preference annotation, and
consequently design a pipeline tailored for it, establishing criteria for quantitative assessment and
annotator training, optimizing labeling experience, and ensuring quality validation. We build the
text-to-image comparison dataset for training the ImageReward model based on the pipeline. The
overall architecture is depicted in Figure 2.
• We demonstrate that ImageReward outperforms existing text-image scoring methods, such
as CLIP [41] (by 38.6%), Aesthetic [50] (by 39.6%), and BLIP [26] (by 31.6%), in terms
of understanding human preference in text-to-image synthesis through extensive analysis and
experiments. ImageReward is also proven to significantly mitigate the aforementioned issues,
providing valuable insights into how human preference can be integrated into generative models.
• We suggest that ImageReward could serve as a promising automatic text-to-image evaluation
metric. Compared to FID [18] and CLIP scores on prompts from real users and MS-COCO 2014,
ImageReward aligns consistently to human preference ranking and presents higher distinguisha-
bility across models and samples."
INTRODUCTION,0.03323262839879154,Rating
INTRODUCTION,0.03474320241691843,"Ranking
＞
＞
＞"
INTRODUCTION,0.03625377643504532,"Prompt
A painting of an ocean with clouds and birds, day time, low depth ﬁeld effect."
INTRODUCTION,0.0377643504531722,"2) Annotation
3) Preference Learning"
INTRODUCTION,0.03927492447129909,1) Prompts and Generated Images
INTRODUCTION,0.04078549848942598,Sample
INTRODUCTION,0.04229607250755287,"Overall Rating
(1=worst, 7=best)"
INTRODUCTION,0.04380664652567976,Image-Text Alignment
INTRODUCTION,0.045317220543806644,Fidelity (Image Quality)
INTRODUCTION,0.04682779456193353,"A
B
C
D 0.6 0.3"
INTRODUCTION,0.04833836858006042,"-1.4 -2.0
A
B C
D"
INTRODUCTION,0.04984894259818731,Reward
INTRODUCTION,0.0513595166163142,"xT
xT
xtxt
xt−1
xt−1 x′"
INTRODUCTION,0.052870090634441085,"0x′ 
0"
INTRODUCTION,0.054380664652567974,"x0x0
xt+1
xt+1"
INTRODUCTION,0.055891238670694864,"Image
Reward"
INTRODUCTION,0.05740181268882175,4) Back Propagation
INTRODUCTION,0.05891238670694864,1) A random t
INTRODUCTION,0.06042296072507553,"to denoise 
with gradient"
INTRODUCTION,0.061933534743202415,2) Predict directly
INTRODUCTION,0.0634441087613293,"3) Reward
Feedback
ReFL"
INTRODUCTION,0.0649546827794562,Inference after ReFL training -1.49 1.20
INTRODUCTION,0.06646525679758308,ImageReward
INTRODUCTION,0.06797583081570997,"Figure 2: An overview of the ImageReward and ReFL. (Upper) ImageReward’s annotation and
training, consisting of data collection, annotation, and preference learning. (Lower) ReFL leverages
ImageReward’s feedback to directly optimize diffusion models at a random latter denoising step."
INTRODUCTION,0.06948640483383686,"• We propose Reward Feedback Learning (ReFL) for tuning diffusion models regarding human
preference scorers. Our unique insight on ImageReward’s quality identifiability at latter denoising
steps allows the direct feedback learning on diffusion models, which offer no likelihood for their
generations. Extensive automatic and human evaluations demonstrate ReFL’s advantages over
existing approaches including data augmentation [61; 13] and loss reweighing [23]."
INTRODUCTION,0.07099697885196375,"2
ImageReward: Learning to Score and Evaluate Human Preferences"
INTRODUCTION,0.07250755287009064,"ImageReward is constructed using a systematic pipeline involving data collection and human annota-
tion from experts. Based on the pipeline, we implement the RM training and derive the ImageReward."
ANNOTATION PIPELINE DESIGN,0.07401812688821752,"2.1
Annotation Pipeline Design"
ANNOTATION PIPELINE DESIGN,0.0755287009063444,"Prompt Selection and Image Collection. The dataset utilizes a diverse selection of real user
prompts from DiffusionDB [58], an open-sourced dataset. To ensure diversity in selected prompts, we
employ a graph-based algorithm that leverages language model-based prompt similarity [56; 44; 53].
This selection yields 10,000 candidate prompts, each accompanied by 4 to 9 sampled images from
DiffusionDB, resulting in 177,304 candidate pairs for labeling (Cf. Appendix A.1 for details)."
ANNOTATION PIPELINE DESIGN,0.0770392749244713,"Human Annotation Design. Our annotation pipeline involves a prompt annotation stage, which
includes categorizing prompts and identifying problematic ones, and a text-image rating stage, where
images are rated based on alignment, fidelity, and harmlessness. Subsequently, annotators rank
the images in order of preference. To manage potential contradictions in the ranking, we provide
trade-offs in our annotation document (completely attached in Appendix B). Our annotation system is
composed of three stages: Prompt Annotation, Text-Image Rating, and Image Ranking. Screenshots
of our system are provided in Figure 8. Annotators were recruited in collaboration with a professional
data annotation company, with a majority having at least college-level education. They are trained
using documents that describe the labeling process and criteria. To ensure quality, we employ quality
inspectors to double-check each annotation, with invalid annotations reassigned for relabeling. Due
to the page limits, please refer to Appendix A.3, A.2, B for comprehensive details and discussion."
ANNOTATION PIPELINE DESIGN,0.07854984894259819,"Human Annotation Analysis. After 2 months of annotation, we collected valid annotations for
8,878 prompts, resulting in 136,892 compared pairs. A comprehensive analysis of these prompts,
annotations, and challenges discovered is discussed in detail in Appendix A.4."
ANNOTATION PIPELINE DESIGN,0.08006042296072508,"Table 1: Text-to-image model ranking by humans and automatic metrics (ImageReward, CLIP, and
FID). ∗Zero-shot FID (30k) scores of DALL-E 2 is taken from [42]; others are evaluated in 256×256
resolution on MS-COCO 2014 validation set following prior practices."
ANNOTATION PIPELINE DESIGN,0.08157099697885196,"Dataset & Model
Real User Prompts
MS-COCO 2014"
ANNOTATION PIPELINE DESIGN,0.08308157099697885,"Human Eval.
ImageReward
CLIP
ImageReward Zero-shot FID∗"
ANNOTATION PIPELINE DESIGN,0.08459214501510574,"Rank
#Win
Rank
Score
Rank
Score
Rank
Score
Rank
Score"
ANNOTATION PIPELINE DESIGN,0.08610271903323263,"Openjourney
1
507
1
0.2614
2
0.2726
3
-0.0455
5
20.7
Stable Diffusion 2.1-base
2
463
2
0.2458
4
0.2683
2
0.1553
4
18.8
DALL-E 2
3
390
3
0.2114
3
0.2684
1
0.5387
1
10.9∗
Stable Diffusion 1.4
4
362
4
0.1344
1
0.2763
4
-0.0857
2
17.9
Versatile Diffusion
5
340
5
-0.2470
5
0.2606
5
-0.5485
3
18.4
CogView 2
6
74
6
-1.2376
6
0.2044
6
-0.8510
6
26.2"
ANNOTATION PIPELINE DESIGN,0.08761329305135952,"Spearman ρ to Human Eval.
-
1.00
0.60
0.77
0.09"
RM TRAINING,0.0891238670694864,"2.2
RM Training"
RM TRAINING,0.09063444108761329,"Admittedly, human evaluation is after all the touchstone for human preference for synthesized images;
but it is limited by labor costs and hard to scale up. We aim to model human preference based on
annotations, which can lead to a virtual evaluator free from dependence on humans."
RM TRAINING,0.09214501510574018,"Similar to RM training for language model of previous works [55; 39], we formulate the preference
annotations as rankings. We have k ∈[4, 9] images ranked for the same prompt T (the best to the
worst are denoted as x1, x2, ..., xk) and get at most C2
k comparison pairs if no ties between two
images. For each comparison, if xi is better and xj is worse, the loss function can be formulated as:"
RM TRAINING,0.09365558912386707,"loss(θ) = −E(T,xi,xj)∼D[log(σ(fθ(T, xi) −fθ(T, xj)))]
(1)"
RM TRAINING,0.09516616314199396,"where fθ(T, x) is a scalar value of preference model for prompt T and generated image x."
RM TRAINING,0.09667673716012085,"Training Techniques. We use BLIP [26] as the backbone of ImageReward, as it outperforms
conventional CLIP (Cf. Table 2b) in our preliminary experiments. We extract image and text features,
combine them with cross attention, and use an MLP to generate a scalar for preference comparison."
RM TRAINING,0.09818731117824774,"Training ImageReward is of no ease. We observe rapid convergence and consequent overfitting, which
harms its performance. To address this, we freeze some backbone transformer layers’ parameters,
finding that a proper number of fixed layers improves ImageReward’s performance (Cf. Section 4.1).
ImageReward also exhibits sensitivity to training hyperparameters, such as learning rate and batch
size. We perform a careful grid search based on the validation set to determine optimal values."
RM TRAINING,0.09969788519637462,"2.3
As Metric: Re-Evaluating Human Preferences on Text-to-Image Models"
RM TRAINING,0.10120845921450151,"Training text-to-image generative models is hard, but evaluating these models reasonably is even
harder. In literature [11; 42; 12; 46], it has been a de facto practice to evaluate text-to-image generative
models on MS-COCO [28] image-caption dataset against the real images, using fine-tuned or zero-
shot FID [18] scores following DALL-E [43] setting. Nevertheless, it remains quite dubious whether
the FID really fits the current need [38], especially from the following aspects:"
RM TRAINING,0.1027190332326284,"1. Zero-shot Usage: As generative models are now dominantly used by the public in a zero-shot
manner without fine-tuning, fine-tuned FID may not honestly reflect models’ actual performance
in real use. In addition, despite the adoption of zero-shot FID in recent trends, the possible leak
of MS-COCO in some models’ pre-training data would make it a potentially unfair setting.
2. Human Preference: FID measures the average distance between generated images and reference
real images, and thus fails to encompass human preference that is crucial to text-to-image synthesis
in evaluation. Moreover, FID’s relies on average over the whole dataset to provide an accurate
assessment, whereas in many cases we need the metric to serve as a selector over single images."
RM TRAINING,0.1042296072507553,"Seeing these challenges, we propose ImageReward as a promising zero-shot automatic evaluation
metric for text-to-image model comparison and individual sample selection."
RM TRAINING,0.10574018126888217,"Better Human Alignment Across Models. We conduct researcher annotation (i.e., by authors)
across 6 popular high-resolution (around 512×512) available text-to-image models: CogView 2 [12],"
RM TRAINING,0.10725075528700906,"Versatile Diffusion (VD) [62], Stable Diffusion (SD) 1.4 and 2.1-base [45], DALL-E 2 (via OpenAI
API) [42], and Openjourney1, to identify the alignment of different metrics to human."
RM TRAINING,0.10876132930513595,"We sample 100 real-user test prompts for the alignment test, with each model generating 10 outputs as
candidates. To compare these models, we first pick the best image out of 10 outputs by each model on
each prompt. Then, the annotators rank the images from different models for each prompt, following
the disciplines for ranking described in Section 2.1. We aggregate all annotators’ annotations, and
compute the final win count of each model to all others (Cf. Table 1)."
RM TRAINING,0.11027190332326284,"For ImageReward and CLIP scores, we report their average for 1,000 text-image pairs per model. We
also document all models’ zero-shot FID and ImageReward score (30k) on MS-COCO 2014 valid
set following prior practices [42; 12], where outputs are unified to 256×256 resolution and optimal
classifier-free guidance values are selected by grid search (i.e., [1.5, 2.0, 3.0, 4.0, 5.0]). As shown in
Table 1, ImageReward aligns well with human ranking, whereas zero-shot FID and CLIP are not."
RM TRAINING,0.11178247734138973,"0.0
0.2
0.4
0.6
0.8
1.0"
RM TRAINING,0.11329305135951662,CogView 2 VD
RM TRAINING,0.1148036253776435,SD 1.4
RM TRAINING,0.1163141993957704,DALL·E 2
RM TRAINING,0.11782477341389729,SD 2.1-base
RM TRAINING,0.11933534743202417,Openjourney
RM TRAINING,0.12084592145015106,"ImageReward Score
CLIP Score"
RM TRAINING,0.12235649546827794,"Figure 3: Normalized distribution of ImageRe-
ward and CLIP scores of different generative
models (outliers are discarded).
ImageRe-
ward’s scores align well with human prefer-
ence and present higher distinguishability."
RM TRAINING,0.12386706948640483,"Better Distinguishability Across Models and Sam-
ples. Another highlight is that, compared to CLIP,
we observe that ImageReward can better distinguish
the quality between individual samples. Figure 3
presents a box plot of ImageReward and CLIP’s
score distributions on the 1,000 generations per
model. The distributions are normalized to 0.0 to
1.0 using minimum and maximum values of Im-
ageReward and CLIP scores per model, and outliers
are discarded. As it demonstrates, ImageReward’s
scores in each model have a much larger interquartile
range than that of CLIP, which means ImageReward
can well distinguish the quality of images from each
other. Besides, in terms of comparison across mod-
els, we discover that the medians of the ImageRe-
ward scores are also roughly in line with human
ranking in Table 1. On the contrary, CLIP’s medians
fail to present the property."
RM TRAINING,0.12537764350453173,"3
ReFL: Reward Feedback Learning Improves Text-to-Image Diffusion"
RM TRAINING,0.1268882175226586,"Though ImageReward can pick out highly human-preferred images from many generations of
a prompt, the generate-and-then-filter paradigm could be expensive and inefficient in practical
applications. Therefore, we seek to improve text-to-image generative models, particularly for the
popular latent diffusion models, for allowing high-quality generation in single or very few trials."
RM TRAINING,0.1283987915407855,"0.35
0.59"
RM TRAINING,0.1299093655589124,"-2.15
-1.62"
RM TRAINING,0.13141993957703926,"step=30
step=40"
RM TRAINING,0.13293051359516617,"Prompt: Landscape photography by marc adamus, mountains with some forests, 
small lake in the center, fog in the background, sunrays, golden hour, high quality."
RM TRAINING,0.13444108761329304,"Figure 4: ImageReward scores of a prompt with different
generation seeds along denoising steps. Final image quali-
ties become identifiable after 30 out of 40 steps."
RM TRAINING,0.13595166163141995,"Challenge. In NLP, researchers have
reported using reinforcement learning
algorithms (e.g., PPO [51]) to steer lan-
guage models to align to human pref-
erence [55; 36; 39], which depends on
the likelihood of a whole generation to
update the model."
RM TRAINING,0.13746223564954682,"However, unlike language models, la-
tent diffusion models (LDMs)’s multi-
step denoising generation cannot yield
likelihoods for their generations, and
thus fail to adopt the same RLHF
approaches.
A potentially similar
approach is classifier-guidance [54;
9] technique during LDM inference.
Nonetheless, it is for inference only"
RM TRAINING,0.13897280966767372,1https://openjourney.art/
RM TRAINING,0.1404833836858006,"and employs a classifier necessarily trained on noisy intermediate latents, which naturally con-
tradicts RMs’ annotation where images need to be completely denoised for humans to mark correct
preference. Some concurrent works propose some alternative indirect solutions, such as using RMs
to filter dataset for fine-tuning [61; 13], or to re-weight losses of training samples according to their
qualities [23]. Nevertheless, these data-oriented approaches are virtually indirect. They could rely
heavily on proper fine-tuning data distributions and finally only improve the LDMs mildly."
RM TRAINING,0.1419939577039275,Algorithm 1 Reward Feedback Learning (ReFL) for LDMs
RM TRAINING,0.14350453172205438,"1: Dataset: Prompt set Y = {y1, y2, ..., yn}
2: Pre-training Dataset:
Text-image pairs dataset D
=
{(txt1, img1), ...(txtn, imgn)}
3: Input: LDM with pre-trained parameters w0, reward model r,
reward-to-loss map function ϕ, LDM pre-training loss function
ψ, reward re-weight scale λ
4: Initialization: The number of noise scheduler time steps T,
and time step range for fine-tuning [T1, T2]
5: for yi ∈Y and (txti, imgi) ∈D do
6:
Lpre ←ψwi(txti, imgi)
7:
wi ←wi // Update LDMwi using Pre-training Loss
8:
t ←rand(T1, T2) // Pick a random time step t ∈[T1, T2]
9:
xT ∼N(0, I) // Sample noise as latent
10:
for j = T, ..., t + 1 do
11:
no grad: xj−1 ←LDMwi{xj}
12:
end for
13:
with grad: xt−1 ←LDMwi{xt}
14:
x0 ←xt−1 // Predict the original latent by noise scheduler
15:
zi ←x0 // From latent to image
16:
Lreward ←λϕ(r(yi, zi)) // ReFL loss
17:
wi+1 ←wi // Update LDMwi using ReFL loss
18: end for"
RM TRAINING,0.14501510574018128,"ReFL: Insight and Solution. We en-
deavor to develop a direct optimiza-
tion method for improving LDMs ac-
cording to an RM (e.g., ImageRe-
ward).
Looking into ImageReward
scores along denoising steps (i.e., 40
in our case), we derive an intriguing
insight (Cf. Figure 4) that when we
directly predict xt →x′
0 at a step t
(different from the real latent x0 which
experiences xt →xt+1 →... →x0):"
RM TRAINING,0.14652567975830816,"• When t
≤
15:
ImageReward
scores for all generations are unan-
imously low.
• When 15
≤
t
≤
30:
High-
quality generations begin to stand
out, but overall we still cannot
clearly judge all generations’ final
qualities based on the current Im-
ageReward scores.
• When t ≥30: Generations of
different ImageReward scores are
generally distinguishable."
RM TRAINING,0.14803625377643503,"In light of the observation, we conclude that ImageReward scores for generations x′
0 after 30 steps of
denoising, unnecessarily the final step, could serve as reliable feedback for improving LDMs."
RM TRAINING,0.14954682779456194,"We thus propose an algorithm to directly fine-tune LDMs by viewing the scores of an RM as human
preference losses to back-propagate gradients (Cf. Algorithm 1) to a randomly-picked latter step t
(in our case t ∈[30, 40]) in the denoising process. The reason for the random selection of t instead
of using the last step is that, if only the gradient of the last denoising step is retained, the training
is proved very unstable and the results are bad. In practice, to avoid rapid overfitting and stabilize
the fine-tuning, we re-weight ReFL loss and regularize with pre-training loss. The final loss form is
written as"
RM TRAINING,0.1510574018126888,"Lreward = λEyi∼Y(ϕ(r(yi, gθ(yi))))
(2)"
RM TRAINING,0.15256797583081572,"Lpre = E(yi,xi)∼D(EE(xi),yi,ϵ∼N(0,1),t[∥ϵ −ϵθ(zt, t, τθ(yi))∥2
2])
(3)"
RM TRAINING,0.1540785498489426,"where θ denotes the parameters of the LDM, gθ(yi) denotes the generated image of LDM with
parameters θ corresponding to prompt yi. Meanings of other symbols are detailed in Algorithm 1,
while the loss function of Lpre is taken from [45]."
EXPERIMENT,0.1555891238670695,"4
Experiment"
EXPERIMENT,0.15709969788519637,"4.1
ImageReward: On Human Preference Prediction"
EXPERIMENT,0.15861027190332327,"Dataset & Training Setting. Rankings of annotated images are collected to train ImageReward,
which contains 8,878 prompts and 136,892 pairs of image comparisons. We divide the dataset
according to prompts annotated by different annotators and select 466 prompts from annotators who
have a higher agreement with researchers to consist for the model test. Except for prompts for testing,
other more than 8k prompts of annotation are collected for training."
EXPERIMENT,0.16012084592145015,"Table 2: Data annotation agreement and ablation study on model backbones and dataset sizes.
(a) Agreement between different annotators, researchers, and models. Espe-
cially, ""annotator ensemble"" means, for each pair of images, we use the image
considered better by most people as the better one."
EXPERIMENT,0.16163141993957703,"researcher annotator annotator
ensemble"
EXPERIMENT,0.16314199395770393,"CLIP
Score Aesthetic BLIP
Score
Ours"
EXPERIMENT,0.1646525679758308,"researcher
71.2%
± 11.1%
65.3%
± 8.5%
73.4%
± 6.2%"
EXPERIMENT,0.1661631419939577,"57.8%
± 3.6%
55.6%
± 3.1%
57.0%
± 3.0%
64.5%
± 2.5%"
EXPERIMENT,0.16767371601208458,"annotator
65.3%
± 8.5%
65.3%
± 5.6%
53.9%
± 5.8%"
EXPERIMENT,0.1691842900302115,"54.3%
± 3.2%
55.9%
± 3.1%
57.4%
± 2.7%
65.3%
± 3.7%"
EXPERIMENT,0.17069486404833836,"annotator
ensemble"
EXPERIMENT,0.17220543806646527,"73.4%
± 6.2%
53.9%
± 5.8%
-
54.4%
± 21.1%
57.5%
± 15.9%
62.0%
± 16.1%
70.5%
± 18.6%"
EXPERIMENT,0.17371601208459214,"(b) Ablation study for different
backbones (i.e., CLIP and BLIP)
used in ImageReward."
EXPERIMENT,0.17522658610271905,Backbone Training Preference
EXPERIMENT,0.17673716012084592,"Set Size
Acc."
EXPERIMENT,0.1782477341389728,"CLIP
4k
61.87
8k
62.98 BLIP"
K,0.1797583081570997,"1k
63.07
2k
63.18
4k
64.71
8k
65.14"
K,0.18126888217522658,"Table 3: Results of ImageReward and comparison methods on human preference prediction. Prefer-
ence accuracy is from the test set of 466 prompts (6,399 comparisons); Recall and Filter’s scores are
from another test set of 371 prompts with 8 images each. All scores are averaged per prompt."
K,0.18277945619335348,"Model
Preference
Recall
Filter
Acc.
@1
@2
@4
@1
@2
@4"
K,0.18429003021148035,"CLIP Score
54.82
27.22
48.52
78.17
29.65
51.75
76.82
Aesthetic Score
57.35
30.73
53.91
75.74
32.08
54.45
76.55
BLIP Score
57.76
30.73
50.67
77.63
33.42
56.33
80.59"
K,0.18580060422960726,"ImageReward (Ours)
65.14
39.62
63.07
90.84
49.06
70.89
88.95"
K,0.18731117824773413,"We load the pre-trained checkpoint of BLIP (ViT-L for image encoder, 12-layers transformer for text
encoder) as the backbone of ImageReward, and initialize MLP head according to N(0, 1/(dmodel+1))
decaying the learning rate with a cosine schedule. We sweep over several value settings of learning
rate and batch size and fix different rates of backbone transformer layers. We find that fixing 70% of
transformer layers with a learning rate of 1e-5 and batch size of 64 can reach up to the best preference
accuracy. ImageReward is trained on 4 40GB NVIDIA A100 GPUs, with a per-GPU batch size of 16."
K,0.18882175226586104,"We use the CLIP score, Aesthetic score, and BLIP score as baselines to compare with the ImageRe-
ward. CLIP score and BLIP score are calculated directly as cosine similarity between text and image
embedding, while the Aesthetic score is given by an aesthetic predictor introduced by LAION[50]."
K,0.1903323262839879,"Agreement Analysis. Agreement assesses the likelihood of two individuals sharing consistent
preferences for superior images. While most people generally agree on image quality, variations in
model-generated images may lead to divergent judgments. Before assessing model performance, it’s
crucial to measure the likelihood of consensus in selecting superior images. We use other 40 prompts
(778 pairs) to calculate preference agreement between researchers, annotators, annotator ensemble,
and models. Table 2a shows the result."
K,0.19184290030211482,"Main Results: Preference Accuracy. Preference accuracy is the correctness of a scorer choosing
the same one from two different images of one prompt with a human. As Table 3 shows, our model
outperforms all the baselines. The preference accuracy of ImageReward reaches up to 65.14%, which
is 15.14% more than 50% (random), about twice as much as 7.76% (that of BLIP score)."
K,0.1933534743202417,"Main Results: Human Evaluation. To evaluate the ability of ImageReward to select the more
preferred images among large amounts of generated images, we produce another dataset, collecting
prompts with 9/25/64 generated images from DiffusionDB, and use different methods to select from
those images to get top3 results. Then three annotators rank these selected top-3 images. Figure 5
shows the win rates. Qualitative results can be seen in Appendix G, showing that ImageReward can
select images that are more aligned to text and with higher fidelity and avoid toxic contents."
K,0.19486404833836857,"Ablation Study: Training dataset size. To investigate the effect of training dataset sizes on the
performance of the model, comparative experiments are conducted. Table 2b shows that adding up the
scale of the dataset significantly improves the preference accuracy of ImageReward. It’s promising
that if we collect more annotation data in the future, ImageReward will get better performance."
K,0.19637462235649547,"0%
20%
40%
60%
80% 100%"
K,0.19788519637462235,"Ours v.s.
BLIP Score"
K,0.19939577039274925,"Ours v.s.
Aesthetic"
K,0.20090634441087613,"Ours v.s.
CLIP Score"
K,0.20241691842900303,"Ours v.s.
Random 50%"
K,0.2039274924471299,Top 3 selected from 9 images
K,0.2054380664652568,"0%
20%
40%
60%
80% 100% 50%"
K,0.20694864048338368,Top 3 selected from 25 images
K,0.2084592145015106,"0%
20%
40%
60%
80% 100% 50%"
K,0.20996978851963746,Top 3 selected from 64 images
K,0.21148036253776434,"Win
Tie
Lose"
K,0.21299093655589124,"Figure 5: Win rates of ImageReward compared to other models. ImageReward wins most of the time.
On average, 77.1% to random, 69.3% to CLIP, 69.8% to Aesthetic, and 65.8% to BLIP."
K,0.21450151057401812,"Methods
Real User Prompts MT Bench [40]"
K,0.21601208459214502,"#Win
WinRate
#Win WinRate"
K,0.2175226586102719,"SD v1.4 (baseline) [45] 1315
-
718
-"
K,0.2190332326283988,"Dataset Filtering [61]
1394
55.17
735
51.72
Reward Weighted [23] 1075
39.52
585
43.33
RAFT [13] (iter=1)
1341
49.86
578
42.31
RAFT (iter=2)
753
30.85
452
33.02
RAFT (iter=3)
398
20.97
355
26.19"
K,0.22054380664652568,"ReFL (Ours)
1508
58.79
808
58.49"
K,0.22205438066465258,"Table 4: Human evaluation on different LDM optimiza-
tion methods. ReFL performs the best with regard to
total win count and WinRate against SD v1.4 baseline."
K,0.22356495468277945,SD v1.4
K,0.22507552870090636,Dataset
K,0.22658610271903323,Filtering
K,0.2280966767371601,Reward
K,0.229607250755287,Weighted
K,0.2311178247734139,RAFT-1
K,0.2326283987915408,RAFT-2
K,0.23413897280966767,RAFT-3 ReFL
K,0.23564954682779457,SD v1.4
K,0.23716012084592145,"Dataset
Filtering"
K,0.23867069486404835,"Reward
Weighted"
K,0.24018126888217523,RAFT-1
K,0.24169184290030213,RAFT-2
K,0.243202416918429,RAFT-3 ReFL
K,0.24471299093655588,"0.5
0.45
0.6
0.5
0.69
0.79
0.41"
K,0.24622356495468278,"0.55
0.5
0.65
0.54
0.72
0.83
0.48"
K,0.24773413897280966,"0.4
0.35
0.5
0.32
0.71
0.84
0.33"
K,0.24924471299093656,"0.5
0.46
0.68
0.5
0.78
0.87
0.4"
K,0.25075528700906347,"0.31
0.28
0.29
0.22
0.5
0.83
0.24"
K,0.25226586102719034,"0.21
0.17
0.16
0.13
0.17
0.5
0.15"
K,0.2537764350453172,"0.59
0.52
0.67
0.6
0.76
0.85
0.5
0.2 0.3 0.4 0.5 0.6 0.7 0.8"
K,0.2552870090634441,Figure 6: Win rates between all methods.
K,0.256797583081571,"Ablation Study: RM backbone. ImageReward adopts BLIP as the backbone, which may raise
curiosity about how well BLIP compares to CLIP. We add MLP to CLIP, training in a similar way,
and the result is also shown in Table 2b. Even if CLIP uses a relatively larger training data set, its
preference is still inferior to that of BLIP. The difference between these two as backbone may partly
be because BLIP used bootstrapping of its training set. Moreover, we use BLIP’s image-grounded
text encoder as a feature encoder different from the separate encoder for text/image as CLIP."
K,0.2583081570996979,"4.2
ReFL: On Improving Diffusion Models with Human Preference"
K,0.2598187311178248,"Training Settings. We use Stable Diffusion v1.4[45] as the baseline generative model and fine-tune
it for experiments. For the dataset, the pre-training dataset is from a 625k subset of LAION-5B[50]
selected by aesthetic score, while the prompt set for ReFL is sampled from DiffusionDB. The
model is fine-tuned in half-precision on 8 40GB NVIDIA A100 GPUs, with a learning rate of 1e-5
and batch size of 128 in total (64 for pre-training and 64 for ReFL). For ReFL algorithm, we set
ϕ = ReLU, λ = 1e −3 and T = 40, [T1, T2] = [1, 10]."
K,0.26132930513595165,"Evaluation Settings. We collect 466 real user prompts from DiffusionDB and 90 designed challeng-
ing prompts from multi-task benchmark (MT Bench) [40] for evaluation. All fine-tuning methods use
the same dataset as the pre-training dataset or generated dataset (both contain 20,000 samples), and
train for one epoch with the same training settings (the same learning rate and batch size) for a fair
comparison. The human evaluation is consistent with Section 2.3 and the form of dataset labeling,
which involves humans sorting multiple images under a prompt. Table 4 and Figure 6 show the
comparison results. All methods use the same pre-trained Stable Diffusion v1.4 and the same reward
model ImageReward, using PNDM [30] noise scheduler and default classifier free guidance scale of
7.5 for inference."
K,0.2628398791540785,"We compare several important related methods for improving text-to-image generation [61; 23; 13],
whose implementation details are provided in Appendix E. Compared to ReFL’s direct tuning, these
previous methods are all based on indirect data augmentation or loss reweighing."
K,0.26435045317220546,"Portrait of a female elf 
warlock, long pointy 
ears, glowing green 
eyes, bushy red hair 
and freckles + medieval 
setting, detailed face, 
highly detailed, digital 
painting, artstation."
K,0.26586102719033233,"Mountains range with 
waterfall, purple haze, 
art by greg rutkowski 
and magali villeneuve, 
artstation."
K,0.2673716012084592,"An concept art 
illustration, 
photorealistic tribal 
people working, 
fantasy street with 
huts, large insect and 
plant biomes, ultra 
realistic, style by wlop.
A half - masked rugged 
laboratory engineer 
man with cybernetic 
enhancements as seen 
from a distance, sciﬁ 
character portrait by 
greg rutkowski, esuthio, 
craig mullins."
K,0.2688821752265861,"ReFL 
(Ours)
Reward 
Weighted
Dataset 
Filtering
Original
RAFT"
K,0.270392749244713,"Figure 7: Qualitative comparison between ReFL and other fine-tuning methods. ReFL fine-tuned
model can produce images that are more preferred overall. For example, in the second row of prompts
containing ""long pointy ears,"" only the model fine-tuned with ReFL generates correct ears, while
other images either lack ears or have inaccurate representations."
K,0.2719033232628399,"Results and analysis. When compared to the original version, ReFL fine-tuned model is mostly
preferred with the most win rate and the highest win rate. When compared to each other, ReFL is
always the preferred one."
K,0.27341389728096677,"Note that in our evaluation, neither RAFT [13] nor Reward Weighted [23] has become better compared
to the baseline, although they have been verified in their own experiments. Note that both RAFT
and Reward Weighted do not collect the prompts used by users in real scenarios at finetune, whereas
Reward Weighted manually constructs a dataset to address the alignment issue by combining colors,
numbers, backgrounds, and objects. The prompts used in our review are more widely distributed and
complex, so the problems with their methods are more clearly exposed."
K,0.27492447129909364,"RAFT [13] suffers from over-fitting as the number of iterations increases. [13] propose using an
expert generator as a regularizer to avoid overfitting the reward model. However, RAFT is constrained
by the quality of the constructed dataset. It is important to note that even expert generators have
limitations, and when fine-tuning is performed using prompts sampled from real user data, which
can be challenging, there may be instances where the expert generator fails to generate high-quality
images."
K,0.2764350453172205,"In the case of the Reward Weighted method [23], although real images are used for regularization,
there is a problem with the coefficient used for the rewards, which is constrained within the [0, 1]
range. This implies that while preferred images are given larger weights and poor images are given
smaller weights, the influence of the non-preferred images is not completely eliminated. Similarly,
when utilizing real user prompts, it is likely that there will be non-preferred images (even those
relatively the best) in the dataset, which can introduce interference. The failure to eliminate the
impact of non-preferred images hinders the effectiveness of the Reward Weighted method."
K,0.27794561933534745,"Dataset Filtering [61], on the other hand, considers real images and handles non-preferred images
by labeling them as ""Weird image."" However, this influence is indirect. In contrast, our proposed
algorithm provides direct gradient feedback through rewards, allowing for guidance toward a ""better""
direction, which enables more effective problem-solving."
K,0.2794561933534743,"In summary, RAFT is constrained by the limited ability of the generator, and the Reward Weighted
method suffers from the influence of non-preferred images due to the choice of reward coefficients."
K,0.2809667673716012,"Dataset Filtering partially addresses the problem by considering real images and labeling abnormal
images, but it is still indirect and limited. By directly incorporating rewards into the gradient feedback,
our proposed algorithm ReFL offers a more effective solution to these challenges. Qualitative
examples are in Figure 7."
RELATED WORK,0.2824773413897281,"5
Related Work"
RELATED WORK,0.283987915407855,"Text-to-image Generation and Evaluation. Text-to-image generation has come a long way since
the popularization of GANs [17], with key developments including models like DALL-E [43] and
CogView [11]. Recently, diffusion models [52; 19; 10; 47] have achieved remarkable results, with
Stable Diffusion [45] being particularly popular. Evaluation metrics such as Inception Score (IS) [4]
and Fréchet Inception Distance (FID) [18] are commonly used to assess model performance after
fine-tuning, but they cannot evaluate either single image generations or text-image coherence."
RELATED WORK,0.2854984894259819,"For evaluating individual generated images based on a prompt, prior works [42; 46; 63] often use
CLIP [41] to calculate text-image similarity. While these metrics are useful, they don’t capture
human preference comprehensively. Other predictors, like Aesthetic from LAION [50], partially
contribute to this holistic evaluation by scoring image aesthetics using a CLIP-based architecture.
RM in RLHF, on the other hand, considers a mixture of elements such as text-image alignment,
fidelity, and aesthetics. Overall, RM such as ImageReward provides a more complete evaluation for
individual text-to-image generations, making it better aligned with human preferences."
RELATED WORK,0.28700906344410876,"Learning from Human Feedback. There is often a gap between generative models’ pre-training
objectives and human intent. Thus human feedback has been utilized to align model performance
with intent in various language applications [1; 22; 36; 67; 65] via training an RM [35; 8; 59; 20; 24]
to learn human preference. Researchers have explored RL for language models to achieve more
truthful, helpful, and harmless outcomes [39; 2; 68; 60; 55; 29; 49; 3]. Previous work [5; 55] used
human feedback to train reward functions for summarization tasks, while InstructGPT [39] applied
RLHF to GPT-3 for multi-task NLP, yielding significant improvements."
RELATED WORK,0.28851963746223563,"In text-to-image generation, however, there have been few studies on the topic. One concurrent
work [23] has focused on text-image coherence in the closed domain using simple synthetic prompts
based on templates, and propose to improve models using loss re-weighing. Other concurrent
works [61; 21; 13] collects 1-of-n selection from noisy online user clicking, and thus do not enforce
consistent standards and prompt diversity. Their optimization methods are based on indirect data
filtering and augmentation. On the contrary, ImageReward serves as a general-purpose human
preference scorer with quality ensured by rigorous annotation pipeline, and corresponding ReFL is
the first direct tuning method for optimize diffusion models from scorer feedback."
CONCLUSION,0.29003021148036257,"6
Conclusion"
CONCLUSION,0.29154078549848944,"In this work, we have presented ImageReward and ReFL, the first general-purpose text-to-image
human preference reward model, and a direct fine-tuning approach for optimizing diffusion models by
ImageReward feedback. Through our systematic pipeline for human preference annotation, we curate
a dataset of 137k expert comparisons to train ImageReward, and build ReFL algorithm on top of it.
They together address prevalent issues in generative models and help to better align text-to-image
generation with human values and preferences."
CONCLUSION,0.2930513595166163,Acknowledgement
CONCLUSION,0.2945619335347432,"We would like to thank the data annotators for their help and support. This research was supported by
the Technology and Innovation Major Project of the Ministry of Science and Technology of China
under Grant 2022ZD0118600 and 2022ZD0118601, Natural Science Foundation of China (NSFC) for
Distinguished Young Scholars No. 61825602, NSFC No. 62276148, a research fund from Zhipu.AI,
and the New Cornerstone Science Foundation through the XPLORER PRIZE."
REFERENCES,0.29607250755287007,References
REFERENCES,0.297583081570997,"[1] D. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A. Courville, and Y. Bengio.
An actor-critic algorithm for sequence prediction. In International Conference on Learning
Representations, 2016."
REFERENCES,0.2990936555891239,"[2] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli,
T. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from
human feedback. arXiv preprint arXiv:2204.05862, 2022."
REFERENCES,0.30060422960725075,"[3] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho-
seini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint
arXiv:2212.08073, 2022."
REFERENCES,0.3021148036253776,"[4] S. Barratt and R. Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973,
2018."
REFERENCES,0.30362537764350456,"[5] F. Böhm, Y. Gao, C. M. Meyer, O. Shapira, I. Dagan, and I. Gurevych. Better rewards yield
better summaries: Learning to summarise without references. In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3110–3120, 2019."
REFERENCES,0.30513595166163143,"[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural
information processing systems, 33:1877–1901, 2020."
REFERENCES,0.3066465256797583,"[7] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.
Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv
preprint arXiv:2204.02311, 2022."
REFERENCES,0.3081570996978852,"[8] P. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement
learning from human preferences, Jun 2017."
REFERENCES,0.30966767371601206,"[9] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in Neural
Information Processing Systems, 34:8780–8794, 2021."
REFERENCES,0.311178247734139,"[10] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis, Dec 2021."
REFERENCES,0.31268882175226587,"[11] M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin, J. Lin, X. Zou, Z. Shao, H. Yang,
et al. Cogview: Mastering text-to-image generation via transformers. Advances in Neural
Information Processing Systems, 34:19822–19835, 2021."
REFERENCES,0.31419939577039274,"[12] M. Ding, W. Zheng, W. Hong, and J. Tang. Cogview2: Faster and better text-to-image generation
via hierarchical transformers. In Advances in Neural Information Processing Systems, 2022."
REFERENCES,0.3157099697885196,"[13] H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum, and T. Zhang. Raft: Reward
ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767,
2023."
REFERENCES,0.31722054380664655,"[14] P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image synthesis.
In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages
12873–12883, 2021."
REFERENCES,0.3187311178247734,"[15] W. Feng, X. He, T.-J. Fu, V. Jampani, A. Akula, P. Narayana, S. Basu, X. E. Wang, and W. Y.
Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis.
arXiv preprint arXiv:2212.05032, 2022."
REFERENCES,0.3202416918429003,"[16] O. Gafni, A. Polyak, O. Ashual, S. Sheynin, D. Parikh, and Y. Taigman. Make-a-scene: Scene-
based text-to-image generation with human priors. In Computer Vision–ECCV 2022: 17th
European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XV, pages
89–106. Springer, 2022."
REFERENCES,0.3217522658610272,"[17] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–144,
2020."
REFERENCES,0.32326283987915405,"[18] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two
time-scale update rule converge to a local nash equilibrium. Advances in neural information
processing systems, 30, 2017."
REFERENCES,0.324773413897281,"[19] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in Neural
Information Processing Systems, 33:6840–6851, 2020."
REFERENCES,0.32628398791540786,"[20] B. Ibarz, J. Leike, T. Pohlen, G. Irving, S. Legg, and D. Amodei. Reward learning from human
preferences and demonstrations in atari, Nov 2018."
REFERENCES,0.32779456193353473,"[21] Y. Kirstain, A. Polyak, U. Singer, S. Matiana, J. Penna, and O. Levy. Pick-a-pic: An open
dataset of user preferences for text-to-image generation. 2023."
REFERENCES,0.3293051359516616,"[22] J. Kreutzer, S. Khadivi, E. Matusov, and S. Riezler. Can neural machine translation be improved
with user feedback? In Proceedings of the 2018 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, Volume 3
(Industry Papers), pages 92–105, 2018."
REFERENCES,0.33081570996978854,"[23] K. Lee, H. Liu, M. Ryu, O. Watkins, Y. Du, C. Boutilier, P. Abbeel, M. Ghavamzadeh, and S. S.
Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192,
2023."
REFERENCES,0.3323262839879154,"[24] K. Lee, L. M. Smith, and P. Abbeel. Pebble: Feedback-efficient interactive reinforcement
learning via relabeling experience and unsupervised pre-training. In International Conference
on Machine Learning, pages 6152–6163. PMLR, 2021."
REFERENCES,0.3338368580060423,"[25] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt
tuning. arXiv preprint arXiv:2104.08691, 2021."
REFERENCES,0.33534743202416917,"[26] J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for
unified vision-language understanding and generation. In International Conference on Machine
Learning, pages 12888–12900. PMLR, 2022."
REFERENCES,0.3368580060422961,"[27] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and
the 11th International Joint Conference on Natural Language Processing (Volume 1: Long
Papers), pages 4582–4597, 2021."
REFERENCES,0.338368580060423,"[28] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick.
Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European
Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755.
Springer, 2014."
REFERENCES,0.33987915407854985,"[29] H. Liu, C. Sferrazza, and P. Abbeel. Chain of hindsight aligns language models with feedback,
Feb 2023."
REFERENCES,0.3413897280966767,"[30] L. Liu, Y. Ren, Z. Lin, and Z. Zhao. Pseudo numerical methods for diffusion models on
manifolds. In International Conference on Learning Representations, 2022."
REFERENCES,0.3429003021148036,"[31] N. Liu, S. Li, Y. Du, A. Torralba, and J. Tenenbaum. Compositional visual generation with
composable diffusion models, Jun 2022."
REFERENCES,0.34441087613293053,"[32] X. Liu, K. Ji, Y. Fu, Z. Du, Z. Yang, and J. Tang. P-tuning v2: Prompt tuning can be comparable
to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021."
REFERENCES,0.3459214501510574,"[33] X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, and J. Tang. Self-supervised learning:
Generative or contrastive. IEEE Transactions on Knowledge and Data Engineering, 35(1):857–
876, 2021."
REFERENCES,0.3474320241691843,"[34] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, and J. Tang. Gpt understands, too. arXiv
preprint arXiv:2103.10385, 2021."
REFERENCES,0.34894259818731116,"[35] J. MacGlashan, M. Ho, R. Loftin, B. Peng, G. Wang, D. Roberts, M. Taylor, and M. Littman.
Interactive learning from policy-dependent human feedback, Aug 2017."
REFERENCES,0.3504531722054381,"[36] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju,
W. Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv
preprint arXiv:2112.09332, 2021."
REFERENCES,0.35196374622356497,"[37] A. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mcgrew, I. Sutskever, and
M. Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion
models. In International Conference on Machine Learning, pages 16784–16804. PMLR, 2022."
REFERENCES,0.35347432024169184,"[38] M. Otani, R. Togashi, Y. Sawai, R. Ishigami, Y. Nakashima, E. Rahtu, J. Heikkilä, and S. Satoh.
Toward verifiable and reproducible human evaluation for text-to-image generation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023."
REFERENCES,0.3549848942598187,"[39] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,
K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.
Advances in Neural Information Processing Systems, 35:27730–27744, 2022."
REFERENCES,0.3564954682779456,"[40] V. Petsiuk, A. E. Siemenn, S. Surbehera, Z. Chin, K. Tyser, G. Hunter, A. Raghavan, Y. Hicke,
B. A. Plummer, O. Kerret, et al. Human evaluation of text-to-image models on a multi-task
benchmark. arXiv preprint arXiv:2211.12112, 2022."
REFERENCES,0.3580060422960725,"[41] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision.
In International conference on machine learning, pages 8748–8763. PMLR, 2021."
REFERENCES,0.3595166163141994,"[42] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image
generation with clip latents. arXiv preprint arXiv:2204.06125, 2022."
REFERENCES,0.3610271903323263,"[43] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever.
Zero-shot text-to-image generation. In International Conference on Machine Learning, pages
8821–8831. PMLR, 2021."
REFERENCES,0.36253776435045315,"[44] N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, 11 2019."
REFERENCES,0.3640483383685801,"[45] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis
with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 10684–10695, 2022."
REFERENCES,0.36555891238670696,"[46] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gon-
tijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models
with deep language understanding. Advances in Neural Information Processing Systems,
35:36479–36494, 2022."
REFERENCES,0.36706948640483383,"[47] C. Saharia, J. Ho, W. Chan, T. Salimans, D. J. Fleet, and M. Norouzi. Image super-resolution
via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence, page
1–14, Sep 2022."
REFERENCES,0.3685800604229607,"[48] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´c, D. Hesslow, R. Castagné, A. S. Luccioni,
F. Yvon, M. Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100, 2022."
REFERENCES,0.37009063444108764,"[49] J. Scheurer, J. Campos, J. Chan, A. Chen, K. Cho, and E. Perez. Training language models with
natural language feedback, Apr 2022."
REFERENCES,0.3716012084592145,"[50] C. Schuhmann, R. Beaumont, R. Vencu, C. W. Gordon, R. Wightman, M. Cherti, T. Coombes,
A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next
generation image-text models. In Thirty-sixth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track, 2022."
REFERENCES,0.3731117824773414,"[51] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.37462235649546827,"[52] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning
using nonequilibrium thermodynamics. In International Conference on Machine Learning,
pages 2256–2265. PMLR, 2015."
REFERENCES,0.37613293051359514,"[53] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu. Mpnet: Masked and permuted pre-training for
language understanding. Advances in Neural Information Processing Systems, 33:16857–16867,
2020."
REFERENCES,0.3776435045317221,"[54] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based
generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456,
2020."
REFERENCES,0.37915407854984895,"[55] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F.
Christiano. Learning to summarize with human feedback. Advances in Neural Information
Processing Systems, 33:3008–3021, 2020."
REFERENCES,0.3806646525679758,"[56] H. Su, J. Kasai, C. H. Wu, W. Shi, T. Wang, J. Xin, R. Zhang, M. Ostendorf, L. Zettlemoyer,
N. A. Smith, et al. Selective annotation makes language models better few-shot learners. arXiv
preprint arXiv:2209.01975, 2022."
REFERENCES,0.3821752265861027,"[57] L. van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning
Research, 9(86):2579–2605, 2008."
REFERENCES,0.38368580060422963,"[58] Z. J. Wang, E. Montoya, D. Munechika, H. Yang, B. Hoover, and D. H. Chau. DiffusionDB: A
large-scale prompt gallery dataset for text-to-image generative models. arXiv:2210.14896 [cs],
2022."
REFERENCES,0.3851963746223565,"[59] G. Warnell, N. Waytowich, V. Lawhern, and P. Stone. Deep tamer: Interactive agent shaping in
high-dimensional state spaces. In Proceedings of the AAAI conference on artificial intelligence,
volume 32, 2018."
REFERENCES,0.3867069486404834,"[60] J. Wu, L. Ouyang, D. Ziegler, N. Stiennon, R. Lowe, J. Leike, and P. Christiano. Recursively
summarizing books with human feedback, Sep 2021."
REFERENCES,0.38821752265861026,"[61] X. Wu, K. Sun, F. Zhu, R. Zhao, and H. Li. Better aligning text-to-image models with human
preference. arXiv preprint arXiv:2303.14420, 2023."
REFERENCES,0.38972809667673713,"[62] X. Xu, Z. Wang, E. Zhang, K. Wang, and H. Shi. Versatile diffusion: Text, images and variations
all in one diffusion model. arXiv preprint arXiv:2211.08332, 2022."
REFERENCES,0.39123867069486407,"[63] J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan, A. Ku, Y. Yang, B. K. Ayan,
et al. Scaling autoregressive models for content-rich text-to-image generation. Transactions on
Machine Learning Research, 2022."
REFERENCES,0.39274924471299094,"[64] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, et al.
Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022."
REFERENCES,0.3942598187311178,"[65] J. Zhang, Y. Liu, J. Mao, W. Ma, J. Xu, S. Ma, and Q. Tian. User behavior simulation for search
result re-ranking. ACM Transactions on Information Systems, 41(1):1–35, 2023."
REFERENCES,0.3957703927492447,"[66] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V.
Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068,
2022."
REFERENCES,0.3972809667673716,"[67] W. Zhou and K. Xu. Learning to compare for better training and evaluation of open domain nat-
ural language generation models. Proceedings of the AAAI Conference on Artificial Intelligence,
34(05):9717–9724, Jun 2020."
REFERENCES,0.3987915407854985,"[68] D. Ziegler, N. Stiennon, J. Wu, T. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving.
Fine-tuning language models from human preferences., Sep 2019."
REFERENCES,0.4003021148036254,"Part I
Appendix"
REFERENCES,0.40181268882175225,Table of Contents
REFERENCES,0.4033232628398791,"A Details on ImageReward’s Comparison Data Annotation Pipeline
16
A.1
Prompt Selection
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
A.2 Annotation Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
A.3
Human Annotation Design
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
A.4
Human Annotation Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17"
REFERENCES,0.40483383685800606,"B Annotation Document
21
B.1
Annotate the input text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
B.2
Annotate generated images
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
B.3
Rank generated images
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
B.4
Frequently asked questions
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
24"
REFERENCES,0.40634441087613293,"C More Analysis on ImageReward’s Performance and Properties
24
C.1
Average Scores of the Highest/Lowest Ranked Images . . . . . . . . . . . . . .
24
C.2
Recall/Filter the Best/Worst Image
. . . . . . . . . . . . . . . . . . . . . . . .
25"
REFERENCES,0.4078549848942598,"D Comparison between ImageReward and Other Reward Models
26
D.1
Human Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
D.2 Train Set Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27"
REFERENCES,0.4093655589123867,"E
Implementation Details of Related LDM Optimization Methods
27"
REFERENCES,0.4108761329305136,"F
More Results of ReFL
28
F.1
Demonstrations of ReFL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
F.2
ReFL compares other fine-tuning methods
. . . . . . . . . . . . . . . . . . . .
28"
REFERENCES,0.4123867069486405,"G Additional Results of ImageReward Compared to Other Typical Image Scorers
28"
REFERENCES,0.41389728096676737,"H Limitations
33"
REFERENCES,0.41540785498489424,"I
Broader Impact
33"
REFERENCES,0.4169184290030212,"J
Reproducibility
33"
REFERENCES,0.41842900302114805,"A
Details on ImageReward’s Comparison Data Annotation Pipeline"
REFERENCES,0.4199395770392749,"A.1
Prompt Selection"
REFERENCES,0.4214501510574018,"Training human preference RM requires a diverse prompt distribution that could cover and represent
users’ authentic usage. DiffusionDB has 1.8M prompts which are far beyond the number we plan to
annotate. To ensure the diversity and representativeness of topic distribution in selected prompts, we
adopt the similar method introduced in [56] for selection. During the graph-based selection, every
sample, which is prompt in our case, is represented by a vector calculated by Sentence-BERT [44].
Then a graph is constructed with represented samples as vertices and every vertex is connected to k
nearest neighbors, where k is a hyper-parameter in the algorithm and k = 150 is found to perform
well. The distance between two vertices is the cosine similarity between vertex representations. With
the graph constructed, the score is calculated for every vertex, which is related to the number of
neighbors that have not yet been selected. Vertex selection is based on calculated scores, which are
calculated repeatedly after every selection until the required number is reached."
REFERENCES,0.4229607250755287,"Note that the complexity of the algorithm is of the squared order of the number of samples. For
computational feasibility efficiency, we grouped all prompts for 100 sets with about 20k prompts
per set. We use the method to select 100 prompts among every set and get a total of 10k prompts for
annotation."
REFERENCES,0.4244712990936556,"A.2
Annotation Management"
REFERENCES,0.4259818731117825,"We cooperate with a professional annotation company to complete professional annotation. Our
process of hiring annotators strictly complies with labor laws and other laws and regulations, and we
pay annotators wages at legal market prices."
REFERENCES,0.42749244712990936,"Before annotators are hired, they first learn the annotation documents and examples provided by
researchers. Then, they are asked to take a test and we calculate their agreement with researchers
and the annotator ensemble. Those who get low agreement scores would not be employed for the
annotation. To ensure quality, we hire quality inspectors to double-check each annotation, and those
invalid ones will be assigned to other annotators for relabeling."
REFERENCES,0.42900302114803623,"In the final list of annotating experts, 95.8% of experts have finished at least college-level education.
Although a thousand readers have a thousand Hamlets, the rating and ranking of generated images
can reach a consensus, especially when associated with objective criteria and social ethics. We write
and compile documents describing the labeling process and quality (Cf. Appendix B), which serve
as the standard for training annotators. For scoring and ranking, we design the criteria for giving
different scores/comparisons and offer specific examples."
REFERENCES,0.43051359516616317,"A.3
Human Annotation Design"
REFERENCES,0.43202416918429004,"Although an individual can easily identify his or her preference for a pair of images, a group of
people can hardly reach consensual criteria over a massive number of comparisons in a pragmatic
annotation. In this section, we discuss our efforts spanning months to design and build an effective
yet simple-to-use pipeline for collecting human preference in text-to-image generation."
REFERENCES,0.4335347432024169,"Prompt Annotation. Prompt annotation includes prompt categorization and problem identification.
We adopt prompt category schema from Parti [63] and require our annotators to decide the category
for each prompt. The category information helps us to better understand problems and per-category
features in the later investigation."
REFERENCES,0.4350453172205438,"In addition, some prompts are problematic and need pre-annotation identification. For example, some
are identified as ambiguous and unclear (e.g., ""a brand new medium"", ""low quality"", etc). Others may
contain different kinds of toxic content, such as pornographic, violent, and discriminatory words,
although they have been filtered in DiffusionDB processing. Therefore, we design several checkboxes
concerning these latent issues for annotators in the pipeline (Cf. Appendix B)."
REFERENCES,0.43655589123867067,"Text-Image Rating. Before diving into ranking model outputs, we also design an annotation stage
for each text-image pair to identify its properties and potential problems. From an overall perspective,
we take into account the following measurements: alignment, fidelity, and harmlessness."
REFERENCES,0.4380664652567976,"• Alignment: which requires generated images faithfully show accurate objects of accurate at-
tributes, with relationships between objects and events described in prompts being correct.
• Fidelity: which concentrates on the quality of images, and especially whether objects in generated
images are realistic, aesthetically pleasing, and with no error of the image itself.
• Harmlessness: which means images should not have toxic, illegal, and biased content, or cause
psychological discomfort."
REFERENCES,0.4395770392749245,"The criteria correspond to other binary checkboxes dedicated to image problem identification and
three seven-level quantitative measures concerning 1) Overall Rating, 2) Image-Text Alignment, and
3) Fidelity (Cf. Figure 8 (a))."
REFERENCES,0.44108761329305135,"Image Ranking. After rating each text-image pair, annotators will finally come to the ranking stage,
where they express their preference by ranking a series of generated images conditioned on a prompt
from best to worst. The ranking generally follows the criteria mentioned in Image Rating."
REFERENCES,0.4425981873111782,"However, it is common that sometimes these criteria contradict each other in the ranking given certain
comparisons. We identify some common contradictions observed in the preliminary test, and specify
the trade-offs one should adopt on our annotation document (Cf. Appendix B). For example, in
comparison, if an image is more aligned to prompt but also more toxic, the less toxic one should
outweigh it since we regard toxicity as a more unacceptable property."
REFERENCES,0.44410876132930516,"Annotation System Design. Considering the criteria above, our annotation system consists of three
stages: Prompt Annotation, Text-Image, and Image Ranking. The screenshots of our system are
shown in Figure 8. The procedures for annotators to go through a prompt are as follows:"
REFERENCES,0.44561933534743203,"1. Label the checkboxes and enter the category for the text prompt.
2. Annotate each image one by one. Rate the image from aspects of alignment, fidelity, and overall
satisfaction using a seven-point Likert scale. If the generated image has certain issues such as
body problems or psychological discomfort, point them out.
3. Rank all images generated from the same prompt. There are 5 slots that can be filled, the first slot
corresponds to the best one among images, and the last slot is placed for the worst one. Ties are
allowed when two images are hard to discriminate for which one is better, but one slot allows two
images at most to enforce distinguishing different qualities."
REFERENCES,0.4471299093655589,"A.4
Human Annotation Analysis"
REFERENCES,0.4486404833836858,"Among 10k prompts selected for annotation, after the expert annotation mentioned in Section 2.1, we
finally collected 8,878 pieces of valid prompts, which comprise a total of 136,892 compared pairs."
REFERENCES,0.4501510574018127,"People
37.85%"
REFERENCES,0.4516616314199396,"Arts
27.11%"
REFERENCES,0.45317220543806647,Outdoor Scenes
REFERENCES,0.45468277945619334,13.39%
REFERENCES,0.4561933534743202,Artifacts 9.36%
REFERENCES,0.45770392749244715,Animals 4.65%
REFERENCES,0.459214501510574,Indoor Scenes 2.05%
REFERENCES,0.4607250755287009,Vehicles 1.46%
REFERENCES,0.4622356495468278,"Food
1.18%"
ABSTRACT,0.4637462235649547,Abstract
ABSTRACT,0.4652567975830816,"1.06%
Plants
0.73%"
ABSTRACT,0.46676737160120846,Illustrations 0.65%
ABSTRACT,0.46827794561933533,"World 
Knowledge 0.50%"
ABSTRACT,0.4697885196374622,"Others
7.64%"
ABSTRACT,0.47129909365558914,"Figure 9: Prompt distribution in annotation data,
which comprises 12 categories and 8,878 prompts."
ABSTRACT,0.472809667673716,"Prompt categories distribution. As we men-
tioned before, we have required the annotators
to classify the prompt before scoring the images.
According to the prompt classification standard
of Parti [63], we divided all prompts into 12 cate-
gories: Abstract, Animals, Artifacts, Arts, Food,
Illustrations, Indoor Scenes, Outdoor Scenes,
People, Plants, Vehicles, and World Knowledge.
The distribution of the prompts in our annotation
data is shown in Figure 9. As we can see, the
distribution is diverse yet representative. Most
prompts fall into common topics such as People
(3,360), Arts (2,407), Outdoor Scenes (1,189), Artifacts (831), and Animals (413). Yet, rare categories
such as Plants, Illustrations, and World Knowledge are also considered in the prompt selection."
ABSTRACT,0.4743202416918429,"Average score distribution of different prompt categories. We have scored the images in three
dimensions including text-image alignment, fidelity, and overall satisfaction. The average scores of
each category are shown in Figure 10. Across the three scoring aspects, scores for each category
present roughly the same pattern. We find that generated images of the Abstract prompts get the
lowest scores. We speculate that Stable Diffusion does not comprehend abstract and vague prompts
well, which often lack the description of concrete objects. Besides, we notice that more low-quality"
ABSTRACT,0.47583081570996977,"(a) Stage 1: Text-Image Rating. Three seven-point Likert scales are rated for text-image alignment,
fidelity, and overall quality. Several options are to be checked and filled in to identify problems."
ABSTRACT,0.4773413897280967,"(b) Stage 2: Image Ranking. There are 4-9 generated images to be ranked by being dragged into 5
slots below that represent the different levels of preference."
ABSTRACT,0.4788519637462236,"Figure 8: Screenshots of our annotation system. Annotators first annotate each text-image pair and
then finish ranking all images conditioned on the text prompt."
ABSTRACT,0.48036253776435045,"prompts exist in the Abstract category than in others, which may also affect the performance of
the text-to-image generation. Images that get higher scores are in the categories of Plants, Outdoor
Scenes, Indoor Scenes, and Plants, whose prompts are usually describing landscapes, non-living
objects, and other common concrete things."
ABSTRACT,0.4818731117824773,"Problem distribution of different prompt categories. In addition to the scores, to understand
common problems presented in generated images is of great importance. We have required the
annotators to identify seven problems of the image, including unrealisticness caused by repeated
generation, body problems, fuzziness, toxicity, pornographic content, or violence. We report the 4.6 4.8 5.0 5.2 5.4"
ABSTRACT,0.48338368580060426,Average Score 5.0
ABSTRACT,0.48489425981873113,5.05.05.05.1
ABSTRACT,0.486404833836858,5.15.25.25.2
ABSTRACT,0.4879154078549849,5.35.3 5.5 5.11
ABSTRACT,0.48942598187311176,(a) Overall Satisfaction 4.6 4.8 5.0 5.2 5.4 5.6 4.9
ABSTRACT,0.4909365558912387,5.15.15.15.15.15.2
ABSTRACT,0.49244712990936557,5.35.3
ABSTRACT,0.49395770392749244,5.35.4 5.5 5.15
ABSTRACT,0.4954682779456193,(b) Text-image Alignment 5.0 5.2 5.4 5.6 5.8 6.0
ABSTRACT,0.49697885196374625,5.35.35.45.4
ABSTRACT,0.4984894259818731,5.55.55.55.6
ABSTRACT,0.5,5.75.7
ABSTRACT,0.5015105740181269,5.95.9 5.45
ABSTRACT,0.5030211480362538,(c) Image Fidelity
ABSTRACT,0.5045317220543807,"People (3360)
Abstract (94)"
ABSTRACT,0.5060422960725075,"Arts (2407)
World Knowledge (44)"
ABSTRACT,0.5075528700906344,"Illustrations (58)
Animals (413)"
ABSTRACT,0.5090634441087614,"Vehicles (130)
Artifacts (831)"
ABSTRACT,0.5105740181268882,"Food (105)
Indoor Scenes (182)"
ABSTRACT,0.5120845921450151,"Plants (65)
Outdoor Scenes (1189)"
ABSTRACT,0.513595166163142,"Figure 10: Average scores of each category. Each image is scored in three aspects of text-image
alignment, fidelity, and overall satisfaction. For each category, we calculated the average scores of
corresponding images in these three aspects. Moreover, the average scores of all images are shown as
the horizontal dotted line in the figure. The number in the legend shows the number of prompts in
each category. 0 2 4"
ABSTRACT,0.5151057401812689,Percentage / %
ABSTRACT,0.5166163141993958,0.1 0.2 0.2
ABSTRACT,0.5181268882175226,0.7 0.8 1.3
ABSTRACT,0.5196374622356495,2.4 2.4 3.1 3.8 4.3 5.0 3.16
ABSTRACT,0.5211480362537765,(a) Repeated Generation 0 10 20 30
ABSTRACT,0.5226586102719033,0.5 1.1 1.8 2.8
ABSTRACT,0.5241691842900302,5.5 6.8 7.0
ABSTRACT,0.525679758308157,12.512.614.2 21.0 33.9 21.14
ABSTRACT,0.527190332326284,(b) Body Problem 0.0 0.5 1.0 1.5 2.0
ABSTRACT,0.5287009063444109,0.2 0.3 0.4 0.5
ABSTRACT,0.5302114803625377,0.6 0.8 0.8
ABSTRACT,0.5317220543806647,1.2 1.2 1.8 0.79
ABSTRACT,0.5332326283987915,(c) Fuzzy Image 0 1 2 3
ABSTRACT,0.5347432024169184,Percentage / % 0.0
ABSTRACT,0.5362537764350453,0.3 0.3 0.4 0.6
ABSTRACT,0.5377643504531722,1.1 1.1 1.1 1.8 3.1 1.36
ABSTRACT,0.5392749244712991,(d) Toxic Problem 0.0 0.2 0.4 0.6 0.8 1.0
ABSTRACT,0.540785498489426,0.1 0.1
ABSTRACT,0.5422960725075529,0.2 0.2 0.6 0.7 0.8 0.52
ABSTRACT,0.5438066465256798,(e) Pornographic 0.00 0.05 0.10 0.15 0.20 0.1 0.2 0.09
ABSTRACT,0.5453172205438066,(f) Violence Problem
ABSTRACT,0.5468277945619335,"People (3360)
Abstract (94)"
ABSTRACT,0.5483383685800605,"Arts (2407)
World Knowledge (44)"
ABSTRACT,0.5498489425981873,"Illustrations (58)
Animals (413)"
ABSTRACT,0.5513595166163142,"Vehicles (130)
Artifacts (831)"
ABSTRACT,0.552870090634441,"Food (105)
Indoor Scenes (182)"
ABSTRACT,0.554380664652568,"Plants (65)
Outdoor Scenes (1189)"
ABSTRACT,0.5558912386706949,"Figure 11: Problem frequency in each category. For each image, annotators are required to identify
whether there are problems including unrealistic caused by repeated generation, body problems, fuzzy
images, toxic, pornographic, violent, or violating protected groups. For images in each category,
we calculated the frequency of all these seven problems. Especially, no images are found violating
protected groups, so we omit this figure. Moreover, the frequency of these problems among all
images is shown as the horizontal dotted line."
ABSTRACT,0.5574018126888217,"frequency of each problem in each category in Figure 11. It shows that the most severe one lies in
the body problem, whose average frequency among all prompt categories is 21.14%. The problem
appears most frequently in the categories of People and Arts. The Animals, Abstract, and Artifacts
categories take second place. Body problems may indicate a lack of knowledge of precise body and
limb structures. And this may also explain why the People category get the lowest fidelity score."
ABSTRACT,0.5589123867069486,"The second severe problem is repeated generation with an average frequency of 3.16%. The problem
mostly appears in the categories of Word Knowledge, People, Arts, and Vehicles. In contrast, we
observe little of this problem occurring in the categories of Indoor Scenes, Food, Abstract, Outdoor
Scenes, and Illustrations, which usually have loose quantity requirements."
ABSTRACT,0.5604229607250756,"Another important problem is fuzzy images, which are mostly found in the category of Abstract, and
then in the category of Animal and Arts. It may further imply that the text-to-image model may also
perform poorly when encountering prompts that are too simple (like ""a cat""), or prompts that are
unreal (like ""an anthropomorphic duck in a blue shirt in the style of zootopia"")."
ABSTRACT,0.5619335347432024,"Besides the three problems that we mentioned above, toxic, pornographic, and violent content is also
found in some images due to related descriptions in their prompts (like ""monster peering out of a
cave, dark lighting, horror, realistic""). This indicates that the current text-to-image model cannot
identify these problems in prompts and consequently cannot avoid them in a generation."
ABSTRACT,0.5634441087613293,"0
0-0.2
0.2-0.4 0.4-0.6 0.6-0.8 0.8-1.0
Function Word Proportion 0 1000 2000 3000 4000 5000 6000 Count 5224 1076 1568 686"
ABSTRACT,0.5649546827794562,"253
71
5.0 5.2 5.4 5.6 5.8"
ABSTRACT,0.5664652567975831,Average Score
ABSTRACT,0.56797583081571,"(a) Overall Satisfaction
(b) Text-image Alignment"
ABSTRACT,0.5694864048338368,(c) Image Fidelity
ABSTRACT,0.5709969788519638,"Figure 12: Prompts distribution & average score.
We divided phrases in prompts into three categories:
content, style, and function. For each prompt, we
calculated the proportion of function phrases, and
we divided all prompts into six categories according
to the proportion. The number of prompts and
average scores in each category are marked."
ABSTRACT,0.5725075528700906,"“Function” words distribution.
When an-
alyzing the prompts, we find an interesting
phenomenon that many prompts not only de-
scribe the content and style but also contain
some “function” words, like ""8k"" and ""highly
detailed"", trying to improve the quality of gen-
erated images. Therefore, we decide to un-
derstand how the existence of these function
phrases influences the performance of the text-
to-image model. To study the question, we first
fine-tuned a token-classification model based
on BERT, which can classify words or phrases
in the prompt into three categories of Content,
Style, and Function. For each prompt, we used
our classification model to classify the words
and phrases in the prompt, and then we calcu-
lated the proportion of function phrases. We
evenly divide the proportion from 0% to 100%
into five buckets. Considering the huge number
of prompts without function phrases, we assign
them to a single group. The distribution of prompts is shown in Figure 12. Most prompts do not
contain any function phrases, and very few prompts contain more than 60% function words."
ABSTRACT,0.5740181268882175,"Average score distribution of different proportions of “function” phrases. For each category,
we calculate the average scores of text-image alignment, fidelity, and overall satisfaction again, and
the result is shown in Figure 12. As it indicates, when the proportion of function phrases is low,
the prompt itself mainly contains the description of concrete content, and the generated pictures get
relatively low scores. As the proportion of function phrases increases, the three scores generally
grow. The increasing trend reflects that the existence of proper function phrases does improve the
text-image alignment, fidelity, and overall satisfaction of images to a certain extent. 0 2 4"
ABSTRACT,0.5755287009063444,Percentage / % 2.35
ABSTRACT,0.5770392749244713,"4.49
4.65 4.15 2.74"
ABSTRACT,0.5785498489425982,"3.76
3.18"
ABSTRACT,0.5800604229607251,(a) Repeated Generation 0 5 10 15 20 25
ABSTRACT,0.581570996978852,21.94 21.53 21.72 19.69 14.86 12.3 21.40
ABSTRACT,0.5830815709969789,(b) Body Problem 0.0 0.2 0.4 0.6 0.8 1.0 0.81 0.90 0.82 0.73 0.31 0.80
ABSTRACT,0.5845921450151057,(c) Fuzzy Image 0.0 0.5 1.0 1.5 2.0 2.5
ABSTRACT,0.5861027190332326,Percentage / %
ABSTRACT,0.5876132930513596,"1.33
1.48
1.47 1.09 2.18 0.42 1.37"
ABSTRACT,0.5891238670694864,(d) Toxic Problem 0.0 0.2 0.4 0.6 0.8 0.41
ABSTRACT,0.5906344410876133,"0.73
0.78 0.66 0.37 0.53"
ABSTRACT,0.5921450151057401,(e) Pornographic 0.00 0.05 0.10 0.15
ABSTRACT,0.5936555891238671,"0.09
0.09 0.13 0.07 0.09"
ABSTRACT,0.595166163141994,(f) Violence Problem
ABSTRACT,0.5966767371601208,"0 (5224)
0-0.2 (1076)
0.2-0.4 (1568)
0.4-0.6 (686)
0.6-0.8 (253)
0.8-1.0 (71)"
ABSTRACT,0.5981873111782477,"Figure 13: Problem frequency in each category (divided by function phrases). Like Figure 11, we
calculated the frequency of all image problems in each category as well as the frequency among all
generated images. The legend shows the six categories of prompts, divided by the proportion of
function phrases, and the number in the legend shows there is how many prompts in each category."
ABSTRACT,0.5996978851963746,"Problem distribution of different proportions of “function” words. The frequency of image
problems is shown in Figure 13. The proportion of function words also influences the problem
distributions. With the increase of function phrases, the frequency of the repeated generation problem
shows a trend of first increasing and then decreasing in the range of 0% to 80%, and then increasing
again. As function words increase in prompts, they initially cause issues, then help, but ultimately,
when dominating, they decrease the model’s understanding and increase problems."
ABSTRACT,0.6012084592145015,"The body and fuzzy problems exhibit similar trends. For the body problem, a high proportion of
function phrases can cause the main object to disappear, leading to a general decrease in frequency."
ABSTRACT,0.6027190332326284,"The fuzzy image problem, mainly tied to image quality, is less influenced by the model’s understanding
of prompts, so its frequency also declines as function phrases increase. Other issues like toxic,
pornographic, and violent content mainly stem from the prompts themselves, so there is no strong
correlation with the proportion of function phrases."
ABSTRACT,0.6042296072507553,"B
Annotation Document"
ABSTRACT,0.6057401812688822,"You will be given a number of prompts and there are several generated images according to the
prompt."
ABSTRACT,0.6072507552870091,"Your annotation requirement is to evaluate these outputs to ensure that they have high quality
in terms of image-text alignment, fidelity, and harmlessness."
ABSTRACT,0.6087613293051359,"What we mean by “image-text alignment” is that the generated image should match the text entered
by the user. Some examples of inconsistent image generation are:"
ABSTRACT,0.6102719033232629,"• The subject described in the text does not appear in the image generated, for example, ""A
cat dressed as Napoleon Bonaparte"" generates an image without the word ""cat""."
ABSTRACT,0.6117824773413897,"• The object properties generated in the image are different from the text description, for
example, generating an image of “a little girl sitting in front of a sewing machine” with
many little girls is incorrect."
ABSTRACT,0.6132930513595166,"What we mean by “fidelity” is that the generated image should be true to the shape and characteristics
of the object, and not generated haphazardly. Some examples of low-fidelity images are:"
ABSTRACT,0.6148036253776435,"• “Spider-Man” should only have two legs, generating an image with more legs is incorrect."
ABSTRACT,0.6163141993957704,"• “Unicorn” should only have one horn, generating an image with multiple horns is incorrect."
ABSTRACT,0.6178247734138973,"• People eat noodles with utensils instead of grabbing them with their hands, generating an
image of someone eating noodles with their hands is incorrect."
ABSTRACT,0.6193353474320241,"What we mean by “harmlessness” is that the generated images should not cause physical, psychologi-
cal, or social harm to people; damage or loss of equipment or property; damage to the environment;
or harm to institutions or resources necessary for human well-being. Some examples of harmful
images are:"
ABSTRACT,0.620845921450151,"• Images that are pornographic, violent, prejudicial or even denigrating specific groups are
harmful."
ABSTRACT,0.622356495468278,• Images that cause psychological discomfort when seen are harmful.
ABSTRACT,0.6238670694864048,"Evaluating the output of the model may involve making trade-offs between these criteria. These
trade-offs will depend on the task. When making these trade-offs, use the following guidelines to
help choose between outputs."
ABSTRACT,0.6253776435045317,"1. For most tasks, fidelity and harmlessness are more important than image-text alignment. So,
in most cases, the image having a higher fidelity and harmlessness is rated higher than an
output that is more image-text alignment."
ABSTRACT,0.6268882175226587,"2. However, if: one output image clearly matches the text better than the other; is only slightly
lacking in the requirements of truthfulness and harmlessness; the content does not fall into
""sensitive areas"" (e.g.), the body of the person generating it cannot go wrong, etc.); then the
more consistent results are rated higher."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6283987915407855,"3. When selecting outputs that are having equal image-text alignment but are harmful in
different ways, then ask: Which output is most likely to cause harm to the users (the person
most affected by the task in the real world), then the corresponding output should be ranked
lower. If this is not clear from the task, then mark these outputs as tied."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6299093655589124,"4. There is a kind of low fidelity due to repeated generation, which we consider to be less
low fidelity, but if there are more realistic images with about the same degree of image-text
alignment, the images with more fidelity are at least a notch higher."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6314199395770392,"Guidelines for deciding boundary cases: Which generated images would you prefer to receive
from AI painters?"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6329305135951662,"Ultimately, making these trade-offs can be challenging, and you should use your best judgment. We
give three specific examples of trade-offs in Figure 14."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6344410876132931,"Output A
Output B"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6359516616314199,"Prompt: 
Beautiful portrait of female model Irakli Nadar with intricate 
details of color crushed ﬂuid oil and acrylic on the 
shoulders, berries and dried moss and dried autumn leaves 
headdress."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6374622356495468,"Reasoning (Preferred output B):
Generating images of the human body with errors in 
important details such as the face and eyes would 
obviously cause psychological discomfort in the real world. 
The output A image better ﬁts the description of ""intricate 
details of color crushed ﬂuid painting and acrylic on the 
shoulders"" about the shoulders, but the generated eﬀect 
on the face and eyes of the image is likely to cause 
discomfort."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6389728096676737,"Output A
Output B"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6404833836858006,"Prompt: 
High quality neoclassical portrait of a cat dressed as 
Napoleon Bonaparte, very detailed, 1860, oil on canvas."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6419939577039275,"Reasoning (Preferred output A):
Output A is a bit unrealistic (the cat's head looks slightly 
unnatural). However, output A is clearly a better match than 
output B for text that has a subject ""cat"", and given that 
generating a cat is not highly sensitive, output A should be 
preferred."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6435045317220544,"Output A
Output B"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6450151057401813,"Prompt: 
Michelangelo's statue of David driving a blue Rolls Royce 
convertible, A night at Caesars Palace in Las Vegas in the 
background."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6465256797583081,"Reasoning (parallelism):
Both output pairs and text match only ""Michelangelo's 
statue of David"" and ""A night at Caesars Palace in Las 
Vegas in the background"", but ""Driving a blue Rolls-Royce 
convertible "" does not appear in the image. Both images 
generate potentially harmful information about the statue of 
David, such as the fact that the details of the statue are 
unforgiving. Since it is not clear which output is more 
harmful than the other, these should be marked as a tie."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.648036253776435,Figure 14: Examples of how to trade-off when selecting images.
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.649546827794562,"For each input text, the annotation will consist of the following three parts."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6510574018126888,"1. Annotate the input text.You will describe the input with several characteristics, including
whether the input text is harmful, etc."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6525679758308157,"2. Annotate AI model output. For each output of an AI model, you will annotate the output
images based on several different dimensional questions, including giving a score of 1-7,
determining whether the image contains pornographic/violent content, etc."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6540785498489426,"3. Rank AI model output.You will rank the output images from best to worst, including ties."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6555891238670695,"B.1
Annotate the input text"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6570996978851964,"Based on the input text, annotate the following questions."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6586102719033232,Label (drop-down menu)
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6601208459214502,"- What is the main object of this text description? Choose the label that seems most appropri-
ate."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6616314199395771,The user’s intent is unclear(yes/no)
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6631419939577039,"- If it is really unclear what the user is asking the AI system to do, then mark ""yes"" here. This
may be because the input text is really confusing, or there seems to be no instruction. If the
input text is a little unclear, but you can still understand what the text means, leave this box
unchecked."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6646525679758308,"- One way to think about it is: If you are given this command, can you understand what the
user is asking and give a reasonable corresponding image? If the answer is ""no"", then mark
the box ""yes""."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6661631419939577,(Multiple choice) Does the input text have any of the following issues?
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6676737160120846,- Contains sexual content
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6691842900302115,- Contains violent content
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6706948640483383,"- Contains content that defames certain groups: By "" defame "" we mean unfair criticism. Does
the output portray members of a protected class in a derogatory manner, or as superior/in-
ferior to others? See Appendix B for a list of protected classes (including race, religion,
etc.)."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6722054380664653,"- Contains Personally Identifiable Information (PII): Does this input text contain information
that can be used to identify a person?"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6737160120845922,- There are no such problems
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.675226586102719,"B.2
Annotate generated images"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.676737160120846,"For each AI model output, please give the annotation of the following questions."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6782477341389728,Overall Rating (Likert scale 1-7)
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6797583081570997,"- How good is the generated image of this text (i.e., has an image-text alignment, high fidelity,
and harmlessness.)? How happy would you be if you gave an AI assistant this text and
received this image result?"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6812688821752266,"- 1 = Output is terrible, it is completely useless and/or can cause real damage."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6827794561933535,- 7 = The output is almost perfect and I cannot think of a way to make it better.
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6842900302114804,Image-text alignment (Likert scale 1-7)
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6858006042296072,- How well do the generated images match the given text?
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6873111782477341,"- 1 = The picture and the text do not match at all, it does not contain the object described in
the text at all, and the picture does not match at all."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6888217522658611,"- 7 = The image and text match almost perfectly, including all objects described in the text,
and all object properties are also consistent with the text."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6903323262839879,Fidelity (Likert scale 1-7)
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6918429003021148,"- The output image should be true to the shape and characteristics that the object should have
and will not be generated haphazardly. Some counter-examples of low-fidelity images."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6933534743202417,"• “Spider-Man” should only have two legs, but generating an image with more legs is
incorrect.
• “Unicorn” should only have one horn, but generating an image with multiple horns is
incorrect.
• People eat noodles with utensils instead of grabbing them with their hands, but generat-
ing an image of someone eating noodles with their hands is incorrect."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6948640483383686,"- 1 = There is an obvious error in the above image, and seriously affects the degree of beauty
or coordination of the picture"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6963746223564955,"- 4 = There are obvious mistakes but no harm, the tires in the picture below are obviously
separated from the car body is not right, but the overall picture is coordinated."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6978851963746223,"- 7 = No image errors, beautiful and coordinated pictures"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.6993957703927492,(Multiple choice) Does the image have any of the following issues?
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7009063444108762,"- Obvious ’repeated generation’ resulting in unreality: If the output image repeatedly generates
certain images, such as ""a little girl’s buttocks directly under another little girl’s head"" or
""a horse’s buttocks with a new body continues to be attached to a new body"", then this is a
problem."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.702416918429003,"- Existence of body problems: including apparently incoherent or abnormally shaped body
parts of humans and animals, etc."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7039274924471299,- Too blurred to see objects
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7054380664652568,"- Causes psychological discomfort: If you feel discomfort when you see this picture, you can
mark this question and answer it according to how you feel."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7069486404833837,- Output contains sexual content
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7084592145015106,- The output contains violent content
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7099697885196374,"- Output contains content that defames certain groups: By ""defame"", we mean unfair crit-
icism. Does the output portray members of a protected class in a demeaning way, or as
superior/inferior to others? A list of classes are protected (including race, color, religion,
national origin, sex, age, physical or mental disability, etc.)."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7114803625377644,- There are no such problems
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7129909365558912,"B.3
Rank generated images"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7145015105740181,"Given all AI model output images together, rank them from good to bad, including relationships. The
best output is the one that is the most image-text alignment, high fidelity, and harmless for the given
text. If the images are close in quality, do not hesitate to mark them as tied. Attention: Your ranking
order must be consistent with the previous scoring for each picture, otherwise it cannot be submitted."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.716012084592145,"B.4
Frequently asked questions"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7175226586102719,"Q: Should the exposure of some special primary and secondary sexual characteristics be classified
as sexual content? Nudity of breasts, for example, is common in some sculptures and paintings and
may need to be generated, but is also generally considered inappropriate, especially in the Chinese
cultural environment."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7190332326283988,"A: If the text requires artistic creation then nudity is allowed, in the case of real people it is considered
a violation of the rules for sex-related content."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7205438066465257,"Q: Many of the prompts use words from Western culture and various artists’ styles, what should I do
if I do not understand?"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7220543806646526,"A: Although the Chinese results of machine translation are attached, you can directly use the search
engine to search the image for reference when you encounter words you do not understand. If you
think some of the vocabulary semantics do not understand does not affect the scoring, you can also
keep part of the unknown semantics to do the scoring of the image as a whole."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7235649546827795,"Q: The input contains a command, but it is confusing/obscure. What should I do?"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7250755287009063,A: You may encounter the following tasks.
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7265861027190332,• The task seems confusing
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7280966767371602,• You do not feel you know exactly what it means to do this task well
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.729607250755287,• There are two possible plausible explanations for this task
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7311178247734139,"In these cases, we again encourage you to use your best judgment to infer the intent of the user
submitting this text and judge the output accordingly."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7326283987915407,Q: When should I skip a mission?
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7341389728096677,A: There is an option to skip a task if
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7356495468277946,"• You are uncomfortable with the task, e.g. it involves gore, horror, pornography, etc."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7371601208459214,"• You do not think you can do the task well, e.g. it requires some expertise you do not have,
or is very confusing, or requires specific life experience, etc. (Note: there is a limit to the
number of skips)"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7386706948640483,"C
More Analysis on ImageReward’s Performance and Properties"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7401812688821753,"C.1
Average Scores of the Highest/Lowest Ranked Images"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7416918429003021,"When evaluating several images, we are also concerned about which one is the best or worst, and
whether the preference model can pick. Figure 15 shows scores of the highest-ranked and lowest-
ranked images picked by different methods. For image fidelity, the Aesthetic score performs better
than CLIP/BLIP score, which is trivial because image fidelity is more about aesthetics. ImageReward
still performs quite better than the Aesthetic score, indicating that human preference for image fidelity
is far more complex than aesthetics. It’s interesting that the lowest-ranked images’ average score"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.743202416918429,"of Aesthetic is lower than that of CLIP/BLIP scores, which may be because that images with too
low quality may affect human judgment about whether the object drawn corresponds to certain
text. Overall, our ImageReward model performs the best. Among the highest-ranked images, the
average score of images picked by our models gets the highest score, while the average score among
lowest-ranked images gets the lowest score. Our ImageReward model maximizes the difference
between superior and inferior images."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7447129909365559,"4.0
4.5
5.0
5.5
Overall Satisfaction"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7462235649546828,CLIP Score
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7477341389728097,Aesthetic
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7492447129909365,BLIP Score Ours
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7507552870090635,"5.31
4.85"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7522658610271903,"4.0
4.5
5.0
5.5
Text-image Alignment"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7537764350453172,"5.54
5.05"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7552870090634441,"4.0
4.5
5.0
5.5
Image Fidelity"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.756797583081571,"5.47
5.1"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7583081570996979,"The highest-ranked image
The lowest-ranked image"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7598187311178247,"Figure 15: Average scores for the highest and lowest ranked images of each model. Models are
expected to rank the image with the highest human rating at the top rank and the one with the lowest
rating last. In each figure, two dashed lines denote the mean scores of the four models’ scoring the
last images and the top images, respectively."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7613293051359517,"C.2
Recall/Filter the Best/Worst Image"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7628398791540786,"To further evaluate the models’ ability to select the best image while filtering out the worst image,
we collect 371 other prompts with 8 images per prompt and require annotators to select the best and
worst one among 8 images. Then we use different methods to rank these 8 images and calculate the
rate they recall the best one or filter the worst one human annotated when selecting 1/2/4 images.
These statistics are also shown in Table 3. Figure 16 shows the bucket distribution of the best or
worst image humans selected when being ranked by different methods, our model significantly has
the largest proportion to pick precisely and the minimum ratio to rank incorrectly."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7643504531722054,"1
2
3
4
5
6
7
8
Bucket 0 10 20 30 40"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7658610271903323,Best / % 39.6 23.5 16.7 10.8 5.1
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7673716012084593,1.3 2.4 0.5 Ours
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7688821752265861,"1
2
3
4
5
6
7
8
Bucket 0 10 20 30 40 30.7 19.9 16.7"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.770392749244713,10.28.6 8.9
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7719033232628398,2.4 2.4
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7734138972809668,BLIP Score
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7749244712990937,"1
2
3
4
5
6
7
8
Bucket 0 10 20 30 40 30.7 23.2"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7764350453172205,11.610.29.2
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7779456193353474,5.9 7.5 1.6
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7794561933534743,Aesthetic
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7809667673716012,"1
2
3
4
5
6
7
8
Bucket 0 10 20 30 40 27.2 21.3 13.2 16.4"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7824773413897281,6.7 8.4
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.783987915407855,4.0 2.7
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7854984894259819,CLIP Score
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7870090634441088,"1
2
3
4
5
6
7
8
Bucket 0 20 40"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7885196374622356,Worst / %
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7900302114803626,0.5 1.9 3.5 5.1 5.4 12.9 21.6 49.1
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7915407854984894,"1
2
3
4
5
6
7
8
Bucket 0 20 40"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7930513595166163,3.2 3.8 5.1 7.3 9.7 14.6 22.9 33.4
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7945619335347432,"1
2
3
4
5
6
7
8
Bucket 0 20 40"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7960725075528701,3.5 5.7 4.6
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.797583081570997,9.7 9.212.9 22.4 32.1
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.7990936555891238,"1
2
3
4
5
6
7
8
Bucket 0 20 40"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.8006042296072508,1.9 4.9 8.1 8.410.814.3 22.1 29.6
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.8021148036253777,"Figure 16: Bucket distribution of the best and the worst images in the human annotation. We collect
prompts(except those for training) each with 8 images, among which annotators pick the best/worst
one. Then different methods are applied to rank these images, where buckets 1-8 correspond to
ranks 1-8. The figure shows the distribution of human-annotated best/worst images through these
model-ranked buckets."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.8036253776435045,"Interpolation Analysis Between Different Scorers When humans evaluate images, the selection
process contains multiple elements such as fidelity, image-text alignment, harmlessness, etc. We are
curious about the performance of a combination of different models. We test interpolation among"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.8051359516616314,"Table 5: Results of ImageReward and other reward models on human preference evaluation. Prefer-
ence accuracy is calculated on the test set of 466 prompts (6,399 comparison pairs in total); Recall
and Filter’s scores are evaluated on another test set of 371 prompts with 8 images per prompt. All
scores are averaged per prompt."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.8066465256797583,"Model
Preference
Recall
Filter
Acc.
@1
@2
@4
@1
@2
@4"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.8081570996978852,"HPS
60.79
39.89
58.76
83.29
47.17
65.50
84.10
PickScore
62.78
38.27
63.07
84.10
46.36
65.77
84.91"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.8096676737160121,"ImageReward (Ours)
65.14
39.62
63.07
90.84
49.06
70.89
88.95"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.8111782477341389,"CLIP score, Aesthetic score, and our ImageReward model, their accuracy on the test set can be seen
in Figure 17."
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.8126888217522659,"0.0
0.2
0.4
0.6
0.8
1.0
Mix rate 50 55 60 65"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.8141993957703928,Accuracy / % 65.4
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.8157099697885196,Ours mixed with CLIP
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.8172205438066465,"0.0
0.2
0.4
0.6
0.8
1.0
Mix rate 50 55 60 65 58.9"
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.8187311178247734,CLIP mixed with Aesthetic
WHEN SELECTING OUTPUTS THAT ARE HAVING EQUAL IMAGE-TEXT ALIGNMENT BUT ARE HARMFUL IN,0.8202416918429003,"0.0
0.2
0.4
0.6
0.8
1.0
Mix rate 50 55 60 65"
AESTHETIC MIXED WITH OURS,0.8217522658610272,"65.9
Aesthetic mixed with Ours"
AESTHETIC MIXED WITH OURS,0.823262839879154,"Figure 17: Accuracy of interpolation between different models. Interpolation between CLIP score and
Aesthetic score can get higher accuracy, but still much lower than ImageReward. When ImageReward
is interpolated with CLIP score or Aesthetic score, it reaches only a bit better performance."
AESTHETIC MIXED WITH OURS,0.824773413897281,"D
Comparison between ImageReward and Other Reward Models"
AESTHETIC MIXED WITH OURS,0.8262839879154078,"Besides ImageReward, other reward models aimed at alignment with human preferences have also
emerged recently, such as HPS [61] and PickScore [21]. To facilitate a comprehensive evaluation of
these reward models, we have undertaken a series of analyses, yielding the following results."
AESTHETIC MIXED WITH OURS,0.8277945619335347,"Table 6: Comparison between different reward models. ""# Win"" is counted from all comparisons in
the human evaluation, while ""WinRate"" is calculated from comparisons against the baseline."
AESTHETIC MIXED WITH OURS,0.8293051359516617,"Real User Prompts
Multi-task Benchmark[40]"
AESTHETIC MIXED WITH OURS,0.8308157099697885,"Methods
Human Eval.
Image
Human Eval.
Image"
AESTHETIC MIXED WITH OURS,0.8323262839879154,"# Win WinRate Reward
# Win WinRate
Reward"
AESTHETIC MIXED WITH OURS,0.8338368580060423,"SD v1.4 (baseline)
399
-
0.1058
459
-
0.1859"
AESTHETIC MIXED WITH OURS,0.8353474320241692,"Bo64
HPS
572
67.24
0.6274
662
69.15
0.6788
PickScore
620
72.16
0.7033
773
72.73
0.7579
ImageReward (Ours)
676
73.33
1.3374
824
74.42
1.4098"
AESTHETIC MIXED WITH OURS,0.8368580060422961,"ReFL
HPS
428
52.86
0.4749
426
52.86
0.4646
PickScore
472
56.91
0.4618
454
55.09
0.4908
ImageReward (Ours)
512
58.38
0.6072
492
58.67
0.4822"
AESTHETIC MIXED WITH OURS,0.8383685800604229,"D.1
Human Evaluation"
AESTHETIC MIXED WITH OURS,0.8398791540785498,"Human evaluation requires annotators to rank all the images generated from the same prompt in
different datasets, and the comparisons were analyzed in Table 5 and 6, where ""Bo64"" means ""the
Best of 64 (Images)"", i.e., every image was assigned the highest reward by the corresponding reward
model out of a pool of 64 images, and ""ReFL"" means that every image was generated using a model
tuned through ReFL with the respective reward model."
AESTHETIC MIXED WITH OURS,0.8413897280966768,"300
200
100
0
100
200
300 300 200 100 0 100 200 300"
AESTHETIC MIXED WITH OURS,0.8429003021148036,"Perplexity = 5, Num of Neighbors = 15"
AESTHETIC MIXED WITH OURS,0.8444108761329305,"ImageReward/Train
Pick-a-Pic/Train"
AESTHETIC MIXED WITH OURS,0.8459214501510574,Figure 18: Prompts in train sets of PickScore and ImageReward visualized by t-SNE.
AESTHETIC MIXED WITH OURS,0.8474320241691843,"D.2
Train Set Distribution"
AESTHETIC MIXED WITH OURS,0.8489425981873112,"We employed t-SNE[57] to visualize the prompt distribution of training sets of PickScore and
ImageReward. Specifically, we initiated the process by randomly selecting prompts from PickScore’s
training set, consisting of 583,747 comparisons, to match the size of our own training set prompts
(8,000). Subsequently, we utilized CLIP to extract the features from these two prompt sets. Finally,
we used t-SNE with several parameter configs to visualize the feature vectors as two-dimensional
scatter plots, which can be seen in Figure 18."
AESTHETIC MIXED WITH OURS,0.850453172205438,"We observed that our training set is slightly more evenly distributed compared to the training set of
PickScore."
AESTHETIC MIXED WITH OURS,0.851963746223565,"E
Implementation Details of Related LDM Optimization Methods"
AESTHETIC MIXED WITH OURS,0.8534743202416919,"Optimization methods that are now available that use human feedback to fine-tune LDM can be
basically divided into two categories, one is acquiring new datasets [61; 13], and the other is changing
the coefficients of loss function [23]. We have selected three methods as baseline models. To have a
fair comparison, all methods use half-precision on 8 40GB NVIDIA A100 GPUs, keeping training
settings the same (such as a learning rate of 1e-5). If a pre-trained dataset is required, all fine-tuning
methods use the same subset of LAION-AES."
AESTHETIC MIXED WITH OURS,0.8549848942598187,"Dataset Filtering. [61] use a reward model to filter the dataset. Specifically, they use the reward
model to score multiple images for the same prompt, and then select the images with the highest
or lowest scores. Those images with the highest scores consist of a new dataset, and those with
the lowest scores are paired with a prompt prefix (they choose “Weird image.”) to indicate that
the image is relatively non-preferred. These two newly constructed datasets of model-generated
images are then used to fine-tune the LDM, together with the pre-training dataset. After constructing
the dataset, there is no longer different from the normal fine-tuning process (the reward model is
not used again). We replicated this fine-tuning process with ImageReward as described in [61].
Specifically, the constructed dataset contains 20,000 pre-training samples and 20,000 filtered samples
from DiffusionDB (10,000 preferred samples and 10,000 non-preferred samples)."
AESTHETIC MIXED WITH OURS,0.8564954682779456,"Reward Weighted. [23] also added a dataset of model-generated images to the pre-trained dataset,
but they used the reward model for the coefficients of the loss function instead of using the reward
model when constructing the dataset. Specifically, their loss function is shown in 4."
AESTHETIC MIXED WITH OURS,0.8580060422960725,"L(θ) = E(x,z)∼Dmodel[−rϕ(x, z)log(pθ(x|z))] + βE(x,z)∼Dpre[−log(pθ(x|z))]
(4)"
AESTHETIC MIXED WITH OURS,0.8595166163141994,"where θ denotes parameters of pre-trained LDM, ϕ denotes parameters of the reward model, and β is
a penalty parameter. During fine-tuning, the dataset includes 20,000 pre-training samples and 20,000
generated samples (2,000 prompts from DiffusionDB, 10 generated images per prompt). In [23],
the reward is constrained within the range [0, 1], while our model follows an approximately normal
distribution with a mean of 0 and a variance of 1. Therefore, in order to replicate their methodology,
we need to map the scores to the range [0, 1]. We adopted the approach of min-max normalization,
selecting the maximum and minimum scores from the sample and calculating the corresponding
mapping values. Additionally, we set the penalty coefficient β = 0.5."
AESTHETIC MIXED WITH OURS,0.8610271903323263,"RAFT. [13] proposes a fine-tuning method by constructing a dataset of generated images with higher
rewards. The process consists of three steps: data collection of generated images, data ranking,
and model fine-tuning, which can be repeatedly performed. In every iteration, we generate 100,000
images (10,000 prompts, 10 images per prompt) and use ImageReward to rank generated images,
getting 10,000 selected images to fine-tune LDM."
AESTHETIC MIXED WITH OURS,0.8625377643504532,"F
More Results of ReFL"
AESTHETIC MIXED WITH OURS,0.8640483383685801,"F.1
Demonstrations of ReFL"
AESTHETIC MIXED WITH OURS,0.8655589123867069,Figure 19 shows the insight of ImageReward score during denoising.
AESTHETIC MIXED WITH OURS,0.8670694864048338,"0
20
40
60
80
100
Denoising Step Rate / % 0.2 0.4 0.6 0.8 1.0"
AESTHETIC MIXED WITH OURS,0.8685800604229608,Spearman
AESTHETIC MIXED WITH OURS,0.8700906344410876,"(77.50%, 0.80)"
AESTHETIC MIXED WITH OURS,0.8716012084592145,"0
20
40
60
80
100
Denoising Step Rate / % 2.0 1.5 1.0 0.5"
AESTHETIC MIXED WITH OURS,0.8731117824773413,ImageReward
AESTHETIC MIXED WITH OURS,0.8746223564954683,77.50%
AESTHETIC MIXED WITH OURS,0.8761329305135952,"Original Reward
Trained Reward"
AESTHETIC MIXED WITH OURS,0.877643504531722,"Figure 19: (Left) Correlation between Spearman ρ and denoising step rate. (Right) Correlation
between ImageReward score and denoising step rate."
AESTHETIC MIXED WITH OURS,0.879154078549849,"F.2
ReFL compares other fine-tuning methods"
AESTHETIC MIXED WITH OURS,0.8806646525679759,"0%
20%
40%
60%
80%
100%"
AESTHETIC MIXED WITH OURS,0.8821752265861027,"Ours v.s.
Dataset Filtering"
AESTHETIC MIXED WITH OURS,0.8836858006042296,"Ours v.s.
RAFT (iteration=1)"
AESTHETIC MIXED WITH OURS,0.8851963746223565,"Ours v.s.
Reward Weighted"
AESTHETIC MIXED WITH OURS,0.8867069486404834,"Ours v.s.
RAFT (iteration=2)"
AESTHETIC MIXED WITH OURS,0.8882175226586103,"Ours v.s.
RAFT (iteration=3) 50%"
AESTHETIC MIXED WITH OURS,0.8897280966767371,"Win
Tie
Lose"
AESTHETIC MIXED WITH OURS,0.8912386706948641,Figure 20: Win rate between different fine-tuning methods.
AESTHETIC MIXED WITH OURS,0.8927492447129909,"Figure 20 shows the comparison be-
tween ReFL and other fine-tuning
methods. ReFL gets the highest win
rate compared to any other method."
AESTHETIC MIXED WITH OURS,0.8942598187311178,"More qualitative examples of ReFL are
provided in Figure 21 - 22."
AESTHETIC MIXED WITH OURS,0.8957703927492447,"G
Additional Results
of ImageReward Compared to
Other Typical Image Scorers"
AESTHETIC MIXED WITH OURS,0.8972809667673716,"More qualitative examples of ImageRe-
ward can be seen in Figure 23 - 26."
AESTHETIC MIXED WITH OURS,0.8987915407854985,"Spaceship and waves, 3d, unreal engine 5, octane render, detailed, cinematograﬁc, cinema 4 d, nvidia ray tracing graphics, 
artstation, greg rutkowski style, style rene magritte."
AESTHETIC MIXED WITH OURS,0.9003021148036254,"ReFL 
(Ours)
Reward 
Weighted
Dataset 
Filtering
Original
RAFT"
AESTHETIC MIXED WITH OURS,0.9018126888217523,"Epic painting portrait photography beautiful goddess wearing white silk blouse with golden glow background, drawing, 
intricate, symmetrical front, by amy leibowitz, wlop."
AESTHETIC MIXED WITH OURS,0.9033232628398792,"Medieval old king, rpg character, hearthstone, fantasy, elegant, highly detailed, digital painting, artstation, concept art, 
matte, sharp focus, illustration, global illumination."
AESTHETIC MIXED WITH OURS,0.904833836858006,"Small red wooden cottage by the lake, lanterns on the porch, smoke coming out of the chimney, dusk, birch trees, 
tranquility, two swans swimming on the lake."
AESTHETIC MIXED WITH OURS,0.9063444108761329,"Portrait of a man who looks like a bulldog, oil on canvas by william sidney mont, 1 8 8 3, trending on art station."
AESTHETIC MIXED WITH OURS,0.9078549848942599,Figure 21: More examples for comparing fine-tuning methods.
AESTHETIC MIXED WITH OURS,0.9093655589123867,"ReFL 
(Ours)
Reward 
Weighted
Dataset 
Filtering
Original
RAFT"
AESTHETIC MIXED WITH OURS,0.9108761329305136,"A serene landscape with a singular building near a lake at sunset, anime style, 8k, low saturation, high quality, high detail, 
cartoon."
AESTHETIC MIXED WITH OURS,0.9123867069486404,"Ultra realistic illustration, young man with gray skin, short white hair, intricate, with dark clothes, elegant, highly 
detailed, digital painting, artstation."
AESTHETIC MIXED WITH OURS,0.9138972809667674,"Dslr photo still of a house on ﬁre under water at the bottom of the ocean, 85 mm f 1. 8."
AESTHETIC MIXED WITH OURS,0.9154078549848943,"Portrait of ﬂower goddess, full body, cute detailed face, bay leafes, tendrils, intricate, elegant, highly detailed, digital 
painting, artstation, concept art, smooth, sharp focus, illustration."
AESTHETIC MIXED WITH OURS,0.9169184290030211,Bat eagle lizard hybrid.
AESTHETIC MIXED WITH OURS,0.918429003021148,Figure 22: More examples for comparing fine-tuning methods.
AESTHETIC MIXED WITH OURS,0.9199395770392749,"Random
Aesthetic
CLIP Score
BLIP Score
Ours"
AESTHETIC MIXED WITH OURS,0.9214501510574018,"An anthropomorphic fox man wearing a long 
coat walking across a glacier, hands in pockets,"
AESTHETIC MIXED WITH OURS,0.9229607250755287,"character illustration by Aaron Miller, Greg"
AESTHETIC MIXED WITH OURS,0.9244712990936556,"Rutkowski, thomas kinkade, Howard Pyle."
AESTHETIC MIXED WITH OURS,0.9259818731117825,"Hyperrealism close - up mythological portrait 
of a beautiful medieval woman's shattered face 
partially made of coral color ﬂowers in style of"
AESTHETIC MIXED WITH OURS,0.9274924471299094,classicism using the ﬁbonacci golden ratio.
AESTHETIC MIXED WITH OURS,0.9290030211480362,"A portrait painting of rachel lane / sabrina 
lloyd / perdita weeks / rachel mcadams / nicole"
AESTHETIC MIXED WITH OURS,0.9305135951661632,"de boer hybrid oil painting, gentle expression,"
AESTHETIC MIXED WITH OURS,0.93202416918429,"smiling, elegant clothing, scenic background."
AESTHETIC MIXED WITH OURS,0.9335347432024169,"Figure 23: Qualitative comparison with previous typical methods. Each method selects the top 3
images based on corresponding scores/rewards. Prompts are sampled from DiffusionDB except for
annotated dataset, which has more than 64 generated images to be picked."
AESTHETIC MIXED WITH OURS,0.9350453172205438,"Random
Aesthetic
CLIP Score
BLIP Score
Ours"
AESTHETIC MIXED WITH OURS,0.9365558912386707,"Key anime visual portrait of an 
anthropomorphic anthro wolf fursona, in a 
jacket, with handsome eyes, ofﬁcial modern"
AESTHETIC MIXED WITH OURS,0.9380664652567976,anime art.
AESTHETIC MIXED WITH OURS,0.9395770392749244,"A humanoid german shepherd beast - man,"
AESTHETIC MIXED WITH OURS,0.9410876132930514,"sitting and watching a soccer match in his 
house on television, he has hurt his knee and is"
AESTHETIC MIXED WITH OURS,0.9425981873111783,"a dad, by erin hanson, alexi zaitsev."
AESTHETIC MIXED WITH OURS,0.9441087613293051,"Beautiful dark landscape, a black cat dressed as"
AESTHETIC MIXED WITH OURS,0.945619335347432,"an astronaut, in the style of beeple and Mike 
Winkelmann, intricate, epic lighting, cinematic"
AESTHETIC MIXED WITH OURS,0.947129909365559,"composition, hyper realistic, 8k resolution."
AESTHETIC MIXED WITH OURS,0.9486404833836858,Figure 24: Qualitative comparison of ImageReward to typical image scoring methods.
AESTHETIC MIXED WITH OURS,0.9501510574018127,"Random
Aesthetic
CLIP Score
BLIP Score
Ours"
AESTHETIC MIXED WITH OURS,0.9516616314199395,Hyperrealism close-up mythological portrait of
AESTHETIC MIXED WITH OURS,0.9531722054380665,a huge number of lavender ﬂowers merged
AESTHETIC MIXED WITH OURS,0.9546827794561934,"with female, turquoise palette, pale skin, 
wearing fuchsia silk robe, style of classicism."
AESTHETIC MIXED WITH OURS,0.9561933534743202,"A cute young woman smiling, long shiny 
bronze brown hair, full round face, green eyes,"
AESTHETIC MIXED WITH OURS,0.9577039274924471,"medium skin tone, light cute freckles, smiling"
AESTHETIC MIXED WITH OURS,0.959214501510574,"softly, wearing casual clothing."
AESTHETIC MIXED WITH OURS,0.9607250755287009,"Pink hand, wearing cyberpunk intricate 
streetwear, beautiful, detailed portrait, intricate"
AESTHETIC MIXED WITH OURS,0.9622356495468278,"complexity, ilya kuvshinov, cell shaded, 4 k,"
AESTHETIC MIXED WITH OURS,0.9637462235649547,"concept art, by wlop, ilya kuvshinov."
AESTHETIC MIXED WITH OURS,0.9652567975830816,Figure 25: Qualitative comparison of ImageReward to typical image scoring methods.
AESTHETIC MIXED WITH OURS,0.9667673716012085,"Random
Aesthetic
CLIP Score
BLIP Score
Ours"
AESTHETIC MIXED WITH OURS,0.9682779456193353,"Sculpture made of ﬂame, portrait, female,"
AESTHETIC MIXED WITH OURS,0.9697885196374623,"future, torch, ﬁre, harper's bazaar, vogue,"
AESTHETIC MIXED WITH OURS,0.9712990936555891,"fashion magazine, intricate."
AESTHETIC MIXED WITH OURS,0.972809667673716,"A wlop 3d render of highly detailed beautiful 
mystic portrait of a phantom priestess'female"
AESTHETIC MIXED WITH OURS,0.974320241691843,ape'with whirling galaxy around.
AESTHETIC MIXED WITH OURS,0.9758308157099698,Highly detailed portrait of a woman with long
AESTHETIC MIXED WITH OURS,0.9773413897280967,"hairs, stephen bliss, unreal engine, fantasy art"
AESTHETIC MIXED WITH OURS,0.9788519637462235,by greg rutkowski.
AESTHETIC MIXED WITH OURS,0.9803625377643505,Figure 26: Qualitative comparison of ImageReward to typical image scoring methods.
AESTHETIC MIXED WITH OURS,0.9818731117824774,"H
Limitations"
AESTHETIC MIXED WITH OURS,0.9833836858006042,"In this section, we discuss some limitations we realize during the development of ImageReward."
AESTHETIC MIXED WITH OURS,0.9848942598187311,"Annotation scale, diversity, and quality. Although our annotation data has reached up to about 9k
prompts and 137k pairs of expert comparisons, the larger scale of the annotation dataset is still needed
for better RM training. In addition, our current prompts are all sampled from DiffusionDB, which is
an abundant collection of human real use but still exists some bias. Despite these prompts may close
to many real cases, biases exist since the real application when people use the text-to-image model
are far beyond trying strange prompts. It’s worth exploring more diverse prompts distribution to meet
the more abundant need of humans. Last but not least, our annotation uses a single-person annotation
plus quality control strategy for each prompt annotation, but multi-person fitting annotation may
achieve better annotation consistency and is worth trying in the future."
AESTHETIC MIXED WITH OURS,0.986404833836858,"RM training techniques. As we mentioned in Section 2.2, overfitting dose affects the RM training,
and fixing part of transformer layers helps a lot. Nevertheless, we speculate that more advanced
techniques (e.g., parameter-efficient tuning [27; 34; 25; 32]) could be helpful for the problem. On the
other hand, since BLIP improves over CLIP substantially in ImageReward training, we also expect a
stronger and larger text-image backbone model may contribute to additional gains."
AESTHETIC MIXED WITH OURS,0.9879154078549849,"Using RM to improve generative models. Though we have proposed ReFL as an effective method to
utilize human preference scorers’ feedback to optimize LDMs, it remains an approximation of original
RLHF algorithms and could be improved fundamentally. It is necessary to develop corresponding
unbiased and efficient feedback learning algorithms with solid theory groundings to allow better
human alignment."
AESTHETIC MIXED WITH OURS,0.9894259818731118,"I
Broader Impact"
AESTHETIC MIXED WITH OURS,0.9909365558912386,"The aim of this paper is to introduce human preference feedback to improve text-to-image generation,
which will help image generation to better match the needs of human life and to conform to social
norms. Fine-tuning the model with human feedback helps to avoid researchers from over-relying on
various types of data with copyright issues for training, and can instead directly improve performance
with reward model feedback. A downside is that the preferences of a single reward model are not
representative of the multiplicity of human aesthetics, and we can address this by training a variety
of reward models and limiting the use of individual reward models. We believe that these benefits
outweigh the drawbacks."
AESTHETIC MIXED WITH OURS,0.9924471299093656,"J
Reproducibility"
AESTHETIC MIXED WITH OURS,0.9939577039274925,"We have made substantial efforts to guarantee the reproducibility of our assessments. The code and
detailed information for the ImageReward model and ReFL algorithm are openly accessible in our
repository (Cf. Abstract). This availability covers the entire training and evaluation processes."
AESTHETIC MIXED WITH OURS,0.9954682779456193,"Training. For specifics regarding the objective function and dataset of ImageReward, please refer to
the hyperparameters and cluster configurations in Section 2.2. Detailed information concerning the
model architecture, hyperparameter settings, and experimental setups can be found in Section 4.1."
AESTHETIC MIXED WITH OURS,0.9969788519637462,"Evaluation. We have organized all evaluations, including text-to-image model ranking and human
preference prediction results of ImageReward, into bash scripts that can be executed with a single
command in the code repository. Further details regarding ImageReward as a metric can be found in
Section 2.3, while details regarding ImageReward as a reward model are provided in Section 4.1."
AESTHETIC MIXED WITH OURS,0.9984894259818731,"ReFL. The ReFL algorithm has also been organized into one-command-to-run bash scripts in our
code repository. Detailed insights into ReFL can be located in Section 4.2."
