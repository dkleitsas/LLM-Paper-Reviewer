Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002617801047120419,"Stochastic gradient descent (SGD) undergoes complicated multiplicative noise for
the mean-square loss. We use this property of the SGD noise to derive a stochastic
differential equation (SDE) with simpler additive noise by performing a random
time change. In the SDE, the loss gradient is replaced by the logarithmized loss
gradient. By using this formalism, we obtain the escape rate formula from a local
minimum, which is determined not by the loss barrier height ∆L = L(θs) −
L(θ∗) between a minimum θ∗and a saddle θs but by the logarithmized loss barrier
height ∆log L = log[L(θs)/L(θ∗)]. Our escape-rate formula strongly depends
on the typical magnitude h∗and the number n of the outlier eigenvalues of the
Hessian. This result explains an empirical fact that SGD prefers ﬂat minima with
low effective dimensions, which gives an insight into implicit biases of SGD."
INTRODUCTION,0.005235602094240838,"1
INTRODUCTION"
INTRODUCTION,0.007853403141361256,"Deep learning has achieved breakthroughs in various applications in artiﬁcial intelligence such as
image classiﬁcation (Krizhevsky et al., 2012; LeCun et al., 2015), speech recognition (Hinton et al.,
2012), natural language processing (Collobert & Weston, 2008), and natural sciences (Iten et al.,
2020; Bapst et al., 2020; Seif et al., 2021). Such unparalleled success of deep learning hinges
crucially on stochastic gradient descent (SGD) or its variants as an efﬁcient training algorithm."
INTRODUCTION,0.010471204188481676,"Although the loss landscape is highly nonconvex, the SGD often succeeds in ﬁnding a global
minimum. It has been argued that the SGD noise plays a key role in escaping from local min-
ima (Jastrze¸bski et al., 2017; Wu et al., 2018; 2020; Zhu et al., 2019; Meng et al., 2020; Xie et al.,
2021; Liu et al., 2021). It has also been suggested that SGD has an implicit bias that is beneﬁcial
for generalization. That is, SGD may help the network to ﬁnd ﬂat minima, which are considered to
imply good generalization (Keskar et al., 2017; Hoffer et al., 2017; Wu et al., 2018). How and why
does SGD help the network escape from bad local minima and ﬁnd ﬂat minima? These questions
have been addressed in several works, and it is now recognized that the SGD noise strength and
structure importantly affect the efﬁciency of escape from local minima. Our work follows this line
of research, and add new theoretical perspectives."
INTRODUCTION,0.013089005235602094,"In physics and chemistry, escape from a local minimum of the (free) energy landscape due to thermal
noise at temperature T has been thoroughly discussed (Kramers, 1940; Langer, 1969). When the
(free) energy barrier is given by ∆E, the escape rate is proportional to e−∆E/T , which is known
as the Arrhenius law. By analogy, in machine learning, escape from a local minimum of the loss
function is considered to be determined by the loss barrier height ∆L = L(θs)−L(θ∗), where L(θ)
denotes the loss function at the network parameters θ, θ∗stands for a local minimum of L(θ), and
θs denotes a saddle point that separates θ∗from other minima. If we assume that the SGD noise
is uniform and isotropic, which is often assumed in machine-learning literature (Jastrze¸bski et al.,
2017), the escape rate is proportional to e−∆L/D, where D denotes the SGD noise strength."
INTRODUCTION,0.015706806282722512,"In this paper, we show that inhomogeneity of the SGD noise strength brings about drastic modiﬁ-
cation for the mean-square loss. It turns out that the escape rate is determined by the logarithmized
loss barrier height ∆log L = log L(θs)−log L(θ∗) = log[L(θs)/L(θ∗)]. In other words, the escape
rate is determined by not the difference but the ratio of L(θs) and L(θ∗). This result means that even
if the loss barrier height ∆L is the same, minima with smaller values of L(θ∗) are more stable."
INTRODUCTION,0.01832460732984293,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.020942408376963352,"Moreover, given the fact that the eigenvalue spectrum of the Hessian at a minimum consists of a bulk
of almost zero eigenvalues and outliers (Sagun et al., 2017; Papyan, 2019), our escape-rate formula
implies that SGD prefers ﬂat minima with a low effective dimension, where the effective dimension
is deﬁned as the number of outliers (MacKay, 1992) and ﬂatness is measured by a typical magnitude
of outlier eigenvalues (Keskar et al., 2017). The previous theories (Jastrze¸bski et al., 2017; Wu et al.,
2018; Zhu et al., 2019; Meng et al., 2020; Xie et al., 2021; Liu et al., 2021) have also successfully
explained that SGD prefers ﬂat minima, but not shown the preference of small effective dimensions.
The logarithmized loss naturally explains the latter, and sheds light on implicit biases of SGD."
INTRODUCTION,0.02356020942408377,Main contributions: We obtain the following main results:
INTRODUCTION,0.02617801047120419,"• We derive an equation for approximating the SGD noise in Eq. (6). Remarkably, the SGD
noise strength in the mean-square loss is shown to be proportional to the loss function,
which is experimentally veriﬁed in Sec. 5.2. A key ingredient in deriving Eq. (6) is the
decoupling approximation given in Eq. (7). This is a novel approximate method introduced
in our analysis, and hence we experimentally verify it in Sec. 5.1.
• We derive a novel stochastic differential equation (SDE) in Eq. (14) via a random time
change introduced in Eq. (13). Although the original SDE (4) has a multiplicative noise,
the transformed SDE (14) has a simple additive noise with the gradient of the logarithmic
loss. This shows the convenience of the logarithmic loss landscape for understanding SGD.
• We derive a novel form of SGD escape rate from a local minimum in Eq. (16). Remarkably,
the escape rate depends on the ratio between L(θ∗) and L(θs). In Sec. 5.3, we experimen-
tally test the validity of this result for linear regressions.
• Our escape rate crucially depends on the ﬂatness and the effective dimension, which shows
that SGD has implicit biases towards ﬂat minima with low effective dimension. We also
show in Eq. (18) that a local minimum with an effective dimension n greater than a certain
critical value nc becomes unstable."
INTRODUCTION,0.028795811518324606,"Related works:
The role of the SGD noise structure has been discussed in some previous
works (Zhu et al., 2019; Xie et al., 2021; Liu et al., 2021; Meng et al., 2020; Wojtowytsch, 2021). It
was pointed out that the anisotropic nature of the SGD noise is important: the SGD noise covariance
matrix is aligned with the Hessian of the loss function, which is beneﬁcial for escape from sharp
minima (Zhu et al., 2019; Xie et al., 2021; Liu et al., 2021). These previous works, however, do
not take the inhomogeneity of the SGD noise strength into account, and consequently, escape rates
derived there depend exponentially on the loss barrier height, which differs from our formula."
INTRODUCTION,0.031413612565445025,"Compared with the anisotropy of the SGD noise, the inhomogeneity of the SGD noise strength has
been less explored. In (Meng et al., 2020; Wojtowytsch, 2021), the SGD dynamics under a state-
dependent noise is discussed. However, in these previous works, the connection between the noise
strength and the loss function was not theoretically established, and the logarithmized loss landscape
was not discussed. The instability due to large effective dimensions was also not shown. Another
recent work (Pesme et al., 2021) observed that the noise is proportional to the loss for speciﬁc simple
models. In our paper, such a result is derived for more generic models. G¨urb¨uzbalaban et al. (2021)
showed that SGD will converge to a heavy-tailed stationary distribution due to a multiplicative nature
of the SGD noise in a simple linear regression problem. Our paper strengthens this result: we argue
that such a heavy-tailed distribution generically appears for the mean-square loss."
BACKGROUND,0.034031413612565446,"2
BACKGROUND"
SETUP,0.03664921465968586,"2.1
SETUP"
SETUP,0.03926701570680628,"We consider supervised learning. Let D = {(x(µ), y(µ)) : µ = 1, 2, . . . , N} be the training dataset,
where x(µ) ∈Rd denotes a data vector and y(µ) ∈R be its label. The network output for a given
input x is denoted by f(θ, x) ∈R, where θ ∈RP stands for a set of trainable parameters with P
being the number of trainable parameters (Extension to the multi-dimensional label and output is
straightforward). In this work, we focus on the mean-square loss"
SETUP,0.041884816753926704,"L(θ) =
1
2N N
X µ=1"
SETUP,0.04450261780104712,"h
f(θ, x(µ)) −y(µ)i2
=: 1 N N
X"
SETUP,0.04712041884816754,"µ=1
ℓµ(θ).
(1)"
SETUP,0.049738219895287955,Under review as a conference paper at ICLR 2022
SETUP,0.05235602094240838,"The training proceeds through optimization of L(θ). In most machine-learning applications, the
optimization is done via SGD or its variants. In SGD, the parameter θk+1 at the time step k + 1 is
determined by"
SETUP,0.0549738219895288,"θk+1 = θk −η∇LBk(θk),
LBk(θ) =
1
2B X"
SETUP,0.05759162303664921,"µ∈Bk
ℓµ(θ),
(2)"
SETUP,0.060209424083769635,"where η > 0 is the learning rate, Bk ⊂{1, 2, . . . , N} with |Bk| = B is a mini-batch used at the kth
time step, and LBk denotes the mini-batch loss. Since the training dataset D is randomly divided
into mini-batches, the dynamics deﬁned by Eq. (2) is stochastic. When B = N, the full training
data samples are used for every iteration. In this case, the dynamics is deterministic and called
gradient descent (GD). SGD is interpreted as GD with stochastic noise. By introducing the SGD
noise ξk = −[∇LBk(θk) −∇L(θk)], Eq. (2) is rewritten as"
SETUP,0.06282722513089005,"θk+1 = θk −η∇L(θk) + ηξk.
(3)"
SETUP,0.06544502617801047,"Obviously, ⟨ξk⟩= 0, where the brackets denote the average over possible choices of mini-batches.
The noise covariance matrix is deﬁned as Σ(θk) := ⟨ξkξT
k ⟩. The covariance structure of the SGD
noise is important in analyzing the SGD dynamics, which will be discussed in Sec. 3.1."
STOCHASTIC DIFFERENTIAL EQUATION FOR SGD,0.06806282722513089,"2.2
STOCHASTIC DIFFERENTIAL EQUATION FOR SGD"
STOCHASTIC DIFFERENTIAL EQUATION FOR SGD,0.07068062827225131,"When the parameter update for each iteration is small, which is typically the case when the learning
rate η is small enough, we can consider the continuous-time approximation (Li et al., 2017; Smith
& Le, 2018). By introducing a continuous time variable t ∈R and regarding η as an inﬁnitesimal
time step dt, we have a SDE"
STOCHASTIC DIFFERENTIAL EQUATION FOR SGD,0.07329842931937172,"dθt = −∇L(θt)dt +
p"
STOCHASTIC DIFFERENTIAL EQUATION FOR SGD,0.07591623036649214,"ηΣ(θt) · dWt,
(4)"
STOCHASTIC DIFFERENTIAL EQUATION FOR SGD,0.07853403141361257,"where dWt ∼N(0, IP dt) with In being the n-by-n identity matrix, and the multiplicative noise
p"
STOCHASTIC DIFFERENTIAL EQUATION FOR SGD,0.08115183246073299,"ηΣ(θt) · dWt is interpreted as Itˆo since the noise ξk in Eq. (3) depends on θk but not on θk+1.
Throughout this work, we consider the continuous-time approximation (4) with Gaussian noise."
STOCHASTIC DIFFERENTIAL EQUATION FOR SGD,0.08376963350785341,"In machine learning, the gradient Langevin dynamics (GLD) is also considered, in which the
isotropic and uniform Gaussian noise is injected into the GD as"
STOCHASTIC DIFFERENTIAL EQUATION FOR SGD,0.08638743455497382,"dθt = −∇L(θt)dt +
√"
STOCHASTIC DIFFERENTIAL EQUATION FOR SGD,0.08900523560209424,"2DdWt,
(5)"
STOCHASTIC DIFFERENTIAL EQUATION FOR SGD,0.09162303664921466,"where D > 0 corresponds to the noise strength (it is also called the diffusion coefﬁcient) (Sato
& Nakagawa, 2014; Zhang et al., 2017b; Zhu et al., 2019). The stationary probability distribution
PGLD(θ) of θ for GLD is given by the Gibbs distribution PGLD(θ) ∝e−L(θ)/D. We will see in
Sec. 4 that the SGD noise structure, which is characterized by Σ(θ), drastically alters the stationary
distribution and the escape rate from a local minimum."
THEORETICAL FORMULATION,0.09424083769633508,"3
THEORETICAL FORMULATION"
STRUCTURE OF THE SGD NOISE COVARIANCE,0.0968586387434555,"3.1
STRUCTURE OF THE SGD NOISE COVARIANCE"
STRUCTURE OF THE SGD NOISE COVARIANCE,0.09947643979057591,"The SGD noise covariance matrix Σ(θ) signiﬁcantly affects the dynamics (Jastrze¸bski et al., 2017;
Smith & Le, 2018; Zhu et al., 2019; Ziyin et al., 2021). In this section, under some approxima-
tions, we derive the following expression of Σ(θ) for the mean-square loss within the the basin of
attractions (or the “valley”) of a local minimum θ∗:"
STRUCTURE OF THE SGD NOISE COVARIANCE,0.10209424083769633,Σ(θ) ≈2L(θ)
STRUCTURE OF THE SGD NOISE COVARIANCE,0.10471204188481675,"NB H(θ∗),
(6)"
STRUCTURE OF THE SGD NOISE COVARIANCE,0.10732984293193717,"where H(θ) = ∇2L(θ) is the Hessian.
It should be noted that ∇L(θ∗) = 0 and H(θ∗) is
positive semideﬁnite at any local minima θ∗.
We give a derivation below and the list of the
approximations and their justiﬁcations in Appendix A. In particular, the decoupling approxima-
tion is a new tool and plays a key role in the derivation.
It states that the quantities ℓµ and
C(µ)
f
(θ) := ∇f(θ, x(µ))∇f(θ, x(µ))T are uncorrelated, which implies"
N,0.1099476439790576,"1
N N
X"
N,0.112565445026178,"µ=1
ℓµC(µ)
f
(θ) ≈"
N,0.11518324607329843,"1
N N
X"
N,0.11780104712041885,"µ=1
ℓµ ! ·"
N,0.12041884816753927,"1
N N
X"
N,0.12303664921465969,"µ=1
C(µ)
f !"
N,0.1256544502617801,"= L(θ) 1 N N
X"
N,0.12827225130890052,"µ=1
C(µ)
f
(θ).
(7)"
N,0.13089005235602094,Under review as a conference paper at ICLR 2022
N,0.13350785340314136,"This approximation is promising for large networks in which ∇f looks a random vector. In Sec. 5.1,
we experimentally verify the decoupling approximation for the entire training dynamics."
N,0.13612565445026178,"Our formula (6) possesses two important properties. First, the noise is aligned with the Hessian,
which has been well known and pointed out in the literature (Jastrze¸bski et al., 2017; Zhu et al.,
2019; Xie et al., 2021; Liu et al., 2021). If the loss landscape has ﬂat directions, which correspond
to the directions of the Hessian eigenvectors belonging to vanishingly small eigenvalues, the SGD
noise does not work along those directions. Consequently, SGD dynamics is frozen along those
ﬂat directions, which effectively reduces the dimension of the parameter space explored by SGD
dynamics. This plays an important role in the escape efﬁciency. Indeed, we will see that the escape
rate crucially depends on the effective dimension of a given local minimum."
N,0.1387434554973822,"Second, the noise is proportional to the loss function, which is indeed experimentally conﬁrmed
in Sec. 5.2. This property has not been pointed out and not been taken into account in previous
studies (Jastrze¸bski et al., 2017; Zhu et al., 2019; Xie et al., 2021; Liu et al., 2021) and therefore
gives new insights into the SGD dynamics. Indeed, this property allows us to formulate the Langevin
equation on the logarithmized loss landscape with simple additive noise as discussed in Sec. 3.2.
This new formalism yields the power-law escape rate, i.e. Eqs. (15) and (16), and the importance of
the effective dimension of local minima for their stability."
N,0.14136125654450263,"Derivation of Eq. (6): We start from an analytic expression of Σ(θ), which reads"
N,0.14397905759162305,Σ(θ) = 1
N,0.14659685863874344,"B
N −B N −1"
N,0.14921465968586387,"1
N N
X"
N,0.1518324607329843,"µ=1
∇ℓµ∇ℓT
µ −∇L∇LT
! ≃1 B"
N,0.1544502617801047,"1
N N
X"
N,0.15706806282722513,"µ=1
∇ℓµ∇ℓT
µ −∇L∇LT
! ,
(8)"
N,0.15968586387434555,"where B ≪N is assumed in the second equality. The derivation of Eq. (8) is found in Jastrze¸bski
et al. (2017); Smith & Le (2018). Usually, the gradient noise variance dominates the square of the
gradient noise mean, and hence the term ∇L∇LT in Eq. (8) is negligible."
N,0.16230366492146597,"For the mean-square loss, we have ∇ℓµ = [f(θ, x(µ)) −y(µ)]∇f(θ, x(µ)), and hence"
N,0.1649214659685864,"Σ(θ) ≈
2
BN N
X"
N,0.16753926701570682,"µ=1
ℓµ∇f(θ, x(µ))∇f(θ, x(µ))T =
2
BN N
X"
N,0.17015706806282724,"µ=1
ℓµC(µ)
f
(θ) ≈2L(θ) NB N
X"
N,0.17277486910994763,"µ=1
C(µ)
f
(θ), (9)"
N,0.17539267015706805,"where the decoupling approximation (7) is used in the last equality. Equation (9) is directly related
to the Hessian of the loss function near a (local or global) minimum. The Hessian is written as"
N,0.17801047120418848,"H(θ) = ∇2L(θ) = 1 N N
X"
N,0.1806282722513089,"µ=1
C(µ)
f
(θ) + 1 N N
X µ=1"
N,0.18324607329842932,"h
f(θ, x(µ)) −y(µ)i
∇2f(θ, x(µ)).
(10)"
N,0.18586387434554974,"It is shown by Papyan (2018) that the last term of Eq. (10) does not contribute to outliers (i.e. large
eigenvalues) of the Hessian. Dynamics near a local minimum is governed by outliers, and hence we
can ignore this term. At θ = θ∗, we therefore obtain"
N,0.18848167539267016,"H(θ∗) ≈1 N N
X"
N,0.19109947643979058,"µ=1
C(µ)
f
(θ∗).
(11)"
N,0.193717277486911,"Let us assume C(µ)
f
(θ) ≈C(µ)
f
(θ∗) for θ within the valley of a local minimum θ∗. We then obtain
the desired expression (6) by substituting it into Eq. (9)."
LOGARITHMIZED LOSS LANDSCAPE,0.19633507853403143,"3.2
LOGARITHMIZED LOSS LANDSCAPE"
LOGARITHMIZED LOSS LANDSCAPE,0.19895287958115182,"Let us consider the Itˆo SDE (4) with the SGD noise covariance (6) near a local minimum θ∗, which
is written as"
LOGARITHMIZED LOSS LANDSCAPE,0.20157068062827224,dθt = −∇L(θt)dt + r
LOGARITHMIZED LOSS LANDSCAPE,0.20418848167539266,2ηL(θt)
LOGARITHMIZED LOSS LANDSCAPE,0.20680628272251309,"B
H(θ∗) · dWt.
(12)"
LOGARITHMIZED LOSS LANDSCAPE,0.2094240837696335,Let us consider a stochastic time t(τ) for τ ≥0 as
LOGARITHMIZED LOSS LANDSCAPE,0.21204188481675393,"τ =
Z t(τ)"
LOGARITHMIZED LOSS LANDSCAPE,0.21465968586387435,"0
dt′ L(θt′),
(13)"
LOGARITHMIZED LOSS LANDSCAPE,0.21727748691099477,Under review as a conference paper at ICLR 2022
LOGARITHMIZED LOSS LANDSCAPE,0.2198952879581152,"and perform a random time change from t to τ (Øksendal, 1998). Correspondingly, we introduce
the Wiener process d ˜Wτ ∼N(0, IP dτ). Since dτ = L(θt)dt, we have d ˜Wτ =
p"
LOGARITHMIZED LOSS LANDSCAPE,0.22251308900523561,"L(θt) · dWt. In
terms of the notation ˜θτ = θt, Eq. (12) is expressed as"
LOGARITHMIZED LOSS LANDSCAPE,0.225130890052356,"d˜θτ = −
1
L(˜θτ)
∇L(˜θτ)dτ + r"
LOGARITHMIZED LOSS LANDSCAPE,0.22774869109947643,2ηH(θ∗)
LOGARITHMIZED LOSS LANDSCAPE,0.23036649214659685,"B
d ˜Wτ = −
h
∇log L(˜θτ)
i
dτ + r"
LOGARITHMIZED LOSS LANDSCAPE,0.23298429319371727,2ηH(θ∗)
LOGARITHMIZED LOSS LANDSCAPE,0.2356020942408377,"B
d ˜Wτ.
(14)"
LOGARITHMIZED LOSS LANDSCAPE,0.23821989528795812,"We should note that at a global minimum with L(θ) = 0, which is realized in an overparameterized
regime (Zhang et al., 2017a), the random time change through Eq. (13) is ill-deﬁned since τ is
frozen at a ﬁnite value once the model reaches a global minimum. We can overcome this difﬁculty
by adding an inﬁnitesimal constant ϵ > 0 to the loss, which makes the loss function positive without
changing the ﬁnite-time dynamics like the escape from a local minimum θ∗with L(θ∗) > 0."
LOGARITHMIZED LOSS LANDSCAPE,0.24083769633507854,"In this way, the Langevin equation on the loss landscape L(θ) with multiplicative noise is trans-
formed to that on the logarithmic loss landscape U(θ) = log L(θ) with simpler additive noise.
This formulation indicates the importance of considering the logarithmized loss landscape U(θ) =
log L(θ). In the following, we use Eq. (14) to discuss the escape efﬁciency from local minima."
ESCAPE RATE FROM LOCAL MINIMA,0.24345549738219896,"4
ESCAPE RATE FROM LOCAL MINIMA"
ESCAPE RATE FROM LOCAL MINIMA,0.24607329842931938,"Let us consider a local minimum θ∗and its basin of attraction Aθ∗, which is the set of all the
starting points θ0 such that θt tends to θ∗as t →∞if there is no noise. In order to escape from
the basin of θ∗, θt must go through a saddle θs with the help of noise. The escape time is deﬁned
as τ = inf{t > 0 : θt /∈Aθ∗, θ0 = θ∗}. The escape rate κ is then deﬁned as the inverse of the
mean escape time: κ = ⟨τ⟩−1. Our idea for evaluating the escape rate is applying the Kramers
formula (Kramers, 1940) or its high-dimensional generalization (Langer, 1969; Bovier et al., 2004;
Berglund, 2013) to Eq. (14). It should be noted that the Kramers formula is applicable only for
additive noise, and hence it cannot be used for the original SDE (4) with multiplicative noise."
ESCAPE RATE FROM LOCAL MINIMA,0.2486910994764398,"First, we present main results [Eqs. (15), (16), and (17)], and give their derivations later. For P = 1
(i.e. the single-variable case θ ∈R), the escape rate through a saddle θs is given by κ = 1 2π p"
ESCAPE RATE FROM LOCAL MINIMA,0.2513089005235602,"h∗|hs|
L(θs) L(θ∗) −( 1"
ESCAPE RATE FROM LOCAL MINIMA,0.25392670157068065,"2 +
B
ηh∗)
,
(15)"
ESCAPE RATE FROM LOCAL MINIMA,0.25654450261780104,"where h∗= H(θ∗). This is a variant of the Kramers formula (Kramers, 1940), which is accurate
when κ is small. We can derive Eq. (15) from Eq. (14) without any further approximations."
ESCAPE RATE FROM LOCAL MINIMA,0.2591623036649215,"For θ ∈RP with P > 1, the analysis is more involved and needs some assumptions and approx-
imations, which are listed in Appendix A. We assume that the eigenvalue spectrum of the Hessian
H(θ∗) at the local minimum θ∗consists of bulk of almost zero eigenvalues and outliers, which is
indeed empirically correct (Sagun et al., 2017; Papyan, 2019). We denote by h∗and n a typical
magnitude of outlier eigenvalues (i.e. the ﬂatness) and the number of outlier eigenvalues (i.e. the ef-
fective dimension), respectively. As we discussed in Sec. 3.1, the SGD dynamics is frozen along ﬂat
directions. Therefore SGD dynamics around the local minimum θ∗is restricted to the n-dimensional
manifold spanned by the outlier eigenvectors v1, v2, . . . , vn ∈RP of H(θ∗). Now we parameterize
θ by using n parameters z1, z2, . . . , zn ∈R as θ = θ∗+ Pn
i=1 zivi. The Hessian restricted to this
outlier subspace is then written as ˆH(θ) := ∇2
zL(θ) ∈Rn×n."
ESCAPE RATE FROM LOCAL MINIMA,0.2617801047120419,We then obtain the following escape rate formula for P > 1:
ESCAPE RATE FROM LOCAL MINIMA,0.2643979057591623,"κ = |hs
e|
2π s"
ESCAPE RATE FROM LOCAL MINIMA,0.2670157068062827,det ˆH(θ∗)
ESCAPE RATE FROM LOCAL MINIMA,0.2696335078534031,| det ˆH(θs)|
ESCAPE RATE FROM LOCAL MINIMA,0.27225130890052357,L(θs) L(θ∗)
ESCAPE RATE FROM LOCAL MINIMA,0.27486910994764396,"−(
B
ηh∗+1−n"
ESCAPE RATE FROM LOCAL MINIMA,0.2774869109947644,"2 )
,
(16)"
ESCAPE RATE FROM LOCAL MINIMA,0.2801047120418848,"where hs
e is the negative eigenvalue of H(θs) corresponding to the escape direction.1 Again, Eq. (16)
is valid when κ is small enough. It should be noted that Eq. (16) is reduced to Eq. (15) when n = 1."
IF EIGENVALUES OF,0.28272251308900526,"1If eigenvalues of
ˆH(θ∗) and those of
ˆH(θs) coincide with each other except for the es-
cape direction e,
det ˆH(θ∗)/ det ˆH(θs)
=
h∗
e/hs
e holds,
and Eq. (16) is simpliﬁed as κ
="
IF EIGENVALUES OF,0.28534031413612565,"(1/2π)
p"
IF EIGENVALUES OF,0.2879581151832461,"h∗e|hse|[L(θs)/L(θ∗)]
−

B
ηh∗+1−n 2 
."
IF EIGENVALUES OF,0.2905759162303665,Under review as a conference paper at ICLR 2022
IF EIGENVALUES OF,0.2931937172774869,"For any P, the quasi-stationary distribution Pss(θ) within the valley of a local minimum θ∗, which
can be identiﬁed as the stationary distribution restricted to the valley (Bianchi & Gaudilli`ere, 2016),
is written as
Pss(θ) ∝L(θ)−φ,
φ = 1 + B"
IF EIGENVALUES OF,0.29581151832460734,"ηh∗.
(17)"
IF EIGENVALUES OF,0.29842931937172773,"Remarkably, it depends on L(θ) polynomially rather than exponentially as in the standard
GLD (Jastrze¸bski et al., 2017; Sato & Nakagawa, 2014; Zhang et al., 2017b). This polynomial
dependence on L(θ) is a key feature leading to the escape rate formula."
IF EIGENVALUES OF,0.3010471204188482,"From Eq. (16) we can obtain some implications. The factor [L(θs)/L(θ∗)]−(
B
ηh∗+1−n"
IF EIGENVALUES OF,0.3036649214659686,"2 ) increases
with h∗and n, which indicates that sharp minima (i.e. minima with large h∗) or minima with large
n are unstable. This fact explains why SGD ﬁnds ﬂat minima with a low effective dimension n.
Equation (16) also implies that the effective dimension of any stable minima must satisfy"
IF EIGENVALUES OF,0.306282722513089,"n < nc := 2
 B"
IF EIGENVALUES OF,0.3089005235602094,"ηh∗+ 1

.
(18)"
IF EIGENVALUES OF,0.31151832460732987,"The instability due to a large effective dimension is a new insight naturally explained by the picture
of the logarithmized loss landscape. It arises from the ratio of the determinants of the logarithmized-
loss Hessian: det ∇2
zU(θ∗)/| det ∇2
zU(θs)| = [det ˆH(θ∗)/| det ˆH(θs)|] · [L(θs)/L(θ∗)]n/2."
IF EIGENVALUES OF,0.31413612565445026,"Derivation of Eq. (16) and Eq. (17): Now we give a derivation of Eqs. (16) and (17). Equation (15)
is straightforwardly obtained by putting P = 1 in the derivation below (some approximations are
introduced below, but all of them are not necessary for P = 1)."
IF EIGENVALUES OF,0.31675392670157065,"As we already noted, SGD dynamics is restricted to the n-dimensional outlier subspace. First, we
assume that the anisotropy of the SGD noise within this n-dimensional space is not relevant, and
approximate the SGD noise in Eq. (14) as an isotropic one:
p"
IF EIGENVALUES OF,0.3193717277486911,"2ηH(θ∗)/Bd ˜Wτ ≈
p"
IF EIGENVALUES OF,0.3219895287958115,"2ηh∗/Bd ˜Wτ,
where h∗∈R+ characterizes the magnitude of the Hessian outliers. This assumption is justiﬁed
when the loss landscape is isotropic within the n-dimensional subspace near the minimum. Even
if the Hessian at the minimum is not isotropic, this approximation is justiﬁed when the directions
of the Hessian eigenvectors do not change within the valley. In the latter case, the escape path is a
straight line along the direction of a Hessian eigenvector ve, where e ∈{1, 2, . . . , n} identiﬁes the
escape direction, and h∗corresponds to the Hessian eigenvalue at the minimum along the escape
direction, i.e. h∗= h∗
e. See Appendix C for the details."
IF EIGENVALUES OF,0.32460732984293195,"Under this approximation, Eq. (14) becomes dzτ = −∇zUdτ +
√"
IF EIGENVALUES OF,0.32722513089005234,"2Td ˜Wτ, where T = ηh∗/B. Its
quasi-stationary distribution ˜Pss(θ) within the valley including the local minimum θ = θ∗(i.e. z =
0) is then given by a Gibbs distribution with respect to U(θ): ˜Pss(θ) ∝e−U(θ)/T ∝L(θ)−B/(ηh∗).
This is the distribution function of ˜θτ for a ﬁxed τ. However, what we want is the quasi-stationary
distribution of θt for a ﬁxed t. In Appendix B, by using Eq. (13), we show that the two distributions
are related with each other as Pss(θ) ∝L(θ)−1 ˜Pss(θ). We thus obtain Eq. (17)."
IF EIGENVALUES OF,0.3298429319371728,"The escape rate under the Langevin equation with isotropic additive noise is evaluated by using
celebrated Kramers formula (Kramers, 1940) or its high-dimensional generalization (Langer, 1969;
Bovier et al., 2004; Berglund, 2013). According to it, the escape rate κτ per unit τ is given by"
IF EIGENVALUES OF,0.3324607329842932,"κτ = |us
e|
2π s"
IF EIGENVALUES OF,0.33507853403141363,"det ∇2zU(θ∗)
| det ∇2zU(θs)|e−∆U/T ,
(19)"
IF EIGENVALUES OF,0.337696335078534,"where us
e is the negative eigenvalue of ∇2
zU(θs) corresponding to the escape direction, ∆U =
U(θs) −U(θ∗) is called the potential barrier, and ∆U/T is assumed to be large enough. What we
really want is the escape rate per unit time t. It is a reasonable assumption that θt stays close to
θ∗for most times before escape, and hence τ is approximately given by τ ≃L(θ∗)t. The escape
rate κ per unit t is then given by κ = L(θ∗)κτ. By combining it with Eq. (19), and substituting
U(θ) = log L(θ), we ﬁnally obtain Eq. (16)."
EXPERIMENTS,0.3403141361256545,"5
EXPERIMENTS"
EXPERIMENTS,0.34293193717277487,"Our key theoretical observation is that the SGD noise strength is proportional to the loss function,
which is obtained as a result of the decoupling approximation. This property leads us to the Langevin"
EXPERIMENTS,0.34554973821989526,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.3481675392670157,"(a) at initialization
(b) at 50 epochs
(c) at 500 epochs"
EXPERIMENTS,0.3507853403141361,"20
10
0
10
log(eigenvalues) 100 101 102"
"EXACT
DECOUPLING",0.35340314136125656,"103
exact
decoupling"
"EXACT
DECOUPLING",0.35602094240837695,"20
10
0
10
log(eigenvalues) 100 101 102"
"EXACT
DECOUPLING",0.3586387434554974,"103
exact
decoupling"
"EXACT
DECOUPLING",0.3612565445026178,"20
10
0
10
log(eigenvalues) 100 101 102"
"EXACT
DECOUPLING",0.36387434554973824,"103
exact
decoupling"
"EXACT
DECOUPLING",0.36649214659685864,"Figure 1: Comparison of the eigenvalue distributions of the left-hand side (exact expression) and the right-
hand side (decoupled one) of Eq. (12) in the main text. They agree with each other except for very small
eigenvalues during the entire training dynamics."
"EXACT
DECOUPLING",0.36910994764397903,"0
20
40
60
80
epoch 10
2"
"EXACT
DECOUPLING",0.3717277486910995,"10
1
loss"
"EXACT
DECOUPLING",0.3743455497382199,(SGD noise strength)/(1.3 × 103)
"EXACT
DECOUPLING",0.3769633507853403,"0
20
40
60
80
100
epoch 10
3 10
2 10
1 loss"
"EXACT
DECOUPLING",0.3795811518324607,(SGD noise strength)/(1.5 × 103)
"EXACT
DECOUPLING",0.38219895287958117,"10
3
10
2
10
1 loss 100 101 102"
"EXACT
DECOUPLING",0.38481675392670156,SGD noise strength
"EXACT
DECOUPLING",0.387434554973822,"Figure 2: Training dynamics of the loss and the SGD noise strength N. In the ﬁgure, we multiplied N by a
numerical factor to emphasize that N is actually proportional to the loss in a later stage of the training. (Left)
Fully-connected network trained by the Fashion-MNIST dataset. (Middle) Convolutional network trained by
the CIFAR-10 dataset. (Right) the loss vs N in the training of the convolutional network. The dashed line is a
straight line of slope 1, which implies N ∝L(θ)."
"EXACT
DECOUPLING",0.3900523560209424,"equation (14) with the logarithmized loss gradient and an additive noise through a random time
change (13). Equation (14) implies the stationary distribution (17) and the escape rate (16)."
"EXACT
DECOUPLING",0.39267015706806285,"In Sec. 5.1, we show that the decoupling approximation is valid during entire training dynamics.
In Sec. 5.2, we measure the SGD noise strength and conﬁrm that it is indeed proportional to the
loss function near a minimum. In Sec. 5.3, we experimentally test the validity of Eq. (17) for the
stationary distribution and Eq. (16) for the escape rate. We will see that numerical results for a linear
regression and for a non-linear neural network agree with our theoretical results."
EXPERIMENTAL VERIFICATION OF THE DECOUPLING APPROXIMATION,0.39528795811518325,"5.1
EXPERIMENTAL VERIFICATION OF THE DECOUPLING APPROXIMATION"
EXPERIMENTAL VERIFICATION OF THE DECOUPLING APPROXIMATION,0.39790575916230364,"Let us compare the eigenvalue distribution of the exact matrix (1/N) PN
µ=1 ℓ(µ)C(µ)
f
with that of"
EXPERIMENTAL VERIFICATION OF THE DECOUPLING APPROXIMATION,0.4005235602094241,"the decoupled one L(θ) · (1/N) PN
µ=1 C(µ)
f
with C(µ)
f
= ∇f(θ, x(µ))∇f(θ, x(µ))T. We consider a
binary classiﬁcation problem using the ﬁrst 104 samples of the MNIST dataset such that we classify
each image into even (its label is y = +1) or odd number (its label is y = −1). The network has two
hidden layers, each of which has 100 units and the ReLU activation, followed by the output layer of
a single unit with no activation. Starting from the Glorot initialization, the training is performed via
SGD with the mean-square loss, where we ﬁx η = 0.01 and B = 100."
EXPERIMENTAL VERIFICATION OF THE DECOUPLING APPROXIMATION,0.4031413612565445,"Figure 1 shows histograms of their eigenvalues at different stages of the training: (a) at initialization,
(b) after 50 epochs, and (c) after 500 epochs. We see that the exact matrix and the approximate
one have statistically similar eigenvalue distributions except for very small eigenvalues during the
training dynamics. This means that the decoupling approximation always holds during training."
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.40575916230366493,"5.2
MEASUREMENTS OF THE SGD NOISE STRENGTH"
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.4083769633507853,"As a measure of the SGD noise strength, let us consider the norm of the noise vector ξ given by
⟨ξTξ⟩= Tr Σ ≈N/B, where N := (1/N) PN
µ=1 ∇ℓT
µ∇ℓµ −∇LT∇L. Here we present experi-
mental results for two architectures and datasets. First, we consider training of the Fashion-MNIST
dataset by using a fully connected neural network with three hidden layers, each of which has 2×103
units and the ReLU activation, followed by the output layer of 10 units with no activation (classiﬁca-"
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.4109947643979058,Under review as a conference paper at ICLR 2022
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.41361256544502617,"(a) Exponent φ in Pss(θ)
(b) tp for the linear regression
(c) tp for the neural network 0.1 1 10 100 1000 10000"
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.4162303664921466,100000
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.418848167539267,"1
 10
 100
 1000"
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.4214659685863874,exponent φ B
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.42408376963350786,η=0.01 η=0.1 η=1 1 10 100 1000
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.42670157068062825,"1
 2
 3
 4
 5
 6  7"
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.4293193717277487,mean first passage time c
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.4319371727748691,"d=1
d=5
d=10
 0.1 1 10 100"
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.43455497382198954,"1
 2
 5
 10"
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.43717277486910994,mean first passage time c
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.4397905759162304,η=0.005
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.4424083769633508,"η=0.01
η=0.015"
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.44502617801047123,"Figure 3: (a) Exponent φ for the stationary distribution Pss(θ) ∝L(θ)−φ for d = 1 in the linear regression.
Dashed lines show the theoretical prediction φ = 1 + B/(ηh∗). (b) Log-log plot of the mean ﬁrst-passage
time tp vs c = L(θs)/L(θ∗) for B = 1 and η = 0.1 in the linear regression. Dashed lines show the theoretical
prediction, tp ∝κ−1 ∝cφ−d/2. (c) Log-log plot of tp vs c for B = 10 and various η in the neural network.
Dashed lines show the theoretical prediction, tp ∝κ−1 ∝cB/(ηh∗)+1−n/2 with h∗= 95.6 and n = 9."
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.4476439790575916,"tion labels are given in the one-hot representation). Second, we consider training of the CIFAR-10
dataset by using a convolutional neural network. Following Keskar et al. (2017), let us denote a
stack of n convolutional layers of a ﬁlters and a kernel size of b × c with the stride length of d by
n × [a, b, c, d]. We use the conﬁguration: 3 × [64, 3, 3, 1], 3 × [128, 3, 3, 1], 3 × [256, 3, 3, 1], where
a MaxPool(2) is applied after each stack. To all layers, the ReLU activation is applied. Finally, an
output layer consists of 10 units with no activation."
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.450261780104712,"Starting from the Glorot initialization, the network is trained by SGD of the mini-batch size B = 100
and η = 0.1 for the mean-square loss. During the training, we measure the training loss and the noise
strength N for every epoch. Numerical results are given in Fig. 2. We see that roughly N ∝L at a
later stage of the training, which agrees with our theoretical prediction."
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.45287958115183247,"Although N is not proportional to L at an early stage of training, it does not mean that Eq. (9) is
invalid there. Since the decoupling approximation is valid for the entire training dynamics, Eq. (9)
always holds. The reason why the SGD noise strength does not decrease with the loss function in the
early-stage dynamics is that N ≈2L(θ)×(1/N) PN
µ=1 ∇f(θ, x(µ))T∇f(θ, x(µ)), but the quantity"
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.45549738219895286,"(1/N) PN
µ=1 ∇f(θ, x(µ))T∇f(θ, x(µ)) also changes during training."
MEASUREMENTS OF THE SGD NOISE STRENGTH,0.4581151832460733,"Although Eq. (9) is derived for the mean-square loss, the relation N ∝L(θ) holds in more general
loss functions; see Appendix D for general argument and experiments on the cross entropy loss."
EXPERIMENTAL TEST OF STATIONARY DISTRIBUTION AND ESCAPE RATE FORMULA,0.4607329842931937,"5.3
EXPERIMENTAL TEST OF STATIONARY DISTRIBUTION AND ESCAPE RATE FORMULA"
EXPERIMENTAL TEST OF STATIONARY DISTRIBUTION AND ESCAPE RATE FORMULA,0.46335078534031415,"We experimentally verify our theoretical predictions. Below, we ﬁrst present numerical results for a
simple linear regression problem, and then for a nonlinear model, i.e., a neural network."
EXPERIMENTAL TEST OF STATIONARY DISTRIBUTION AND ESCAPE RATE FORMULA,0.46596858638743455,"Let us start from the following linear regression problem: each entry of x(µ) ∈Rd and its la-
bel y(µ) ∈R are i.i.d.
Gaussian random variables of zero mean and unit variance.
We fo-
cus on the case of d ≪N.
The output for an input x is given by f(θ, x) = θTx, where
θ ∈Rd is the trainable network parameter.
We optimize θ via SGD. The mean-square loss
L(θ) = (1/2N) PN
µ=1
 
θx(µ) −y(µ)2 is quadratic and has a unique minimum at θ ≈0."
EXPERIMENTAL TEST OF STATIONARY DISTRIBUTION AND ESCAPE RATE FORMULA,0.468586387434555,"First, we test Eq. (17), i.e. the stationary distribution, for d = 1 and N = 105. We sampled the
value of θk at every 100 iterations (k = j × 100, j = 1, 2, . . . , 104) and made a histogram. We then
ﬁt the histogram to the form Pss(θ) ∝L(θ)−φ and determine the exponent φ. Our theory predicts
φ = 1 + B/(ηh∗). Numerical results for the exponent φ are presented in Fig. 3 (a) against B for
three ﬁxed learning rates η. In the same ﬁgure, theoretical values of φ are plotted in dashed lines.
The agreement between theory and experiment is fairly well. For a large learning rate η = 1, the
exponent slightly deviates from its theoretical value. This is due to the effect of a ﬁnite learning rate
(recall that η is assumed to be small in deriving the continuous-time stochastic differential equation)."
EXPERIMENTAL TEST OF STATIONARY DISTRIBUTION AND ESCAPE RATE FORMULA,0.4712041884816754,"Next, we test our formula on the escape rate, Eq. (16). Although the mean-square loss is quadratic
and no barrier crossing occurs, we can measure the ﬁrst passage time, which imitates the escape
time for a non-convex loss landscape. Let us ﬁx a threshold value of the loss function. The ﬁrst"
EXPERIMENTAL TEST OF STATIONARY DISTRIBUTION AND ESCAPE RATE FORMULA,0.4738219895287958,Under review as a conference paper at ICLR 2022
EXPERIMENTAL TEST OF STATIONARY DISTRIBUTION AND ESCAPE RATE FORMULA,0.47643979057591623,"passage time tp is deﬁned as the shortest time at which the loss exceeds the threshold value. Here,
time t is identiﬁed as ηk, where k denotes the number of iterations in discrete SGD (2). We identify
the threshold value as L(θs), i.e., the loss at the saddle in the escape problem. It is expected that tp
is similar to the escape time and proportional to κ−1."
EXPERIMENTAL TEST OF STATIONARY DISTRIBUTION AND ESCAPE RATE FORMULA,0.4790575916230366,"The Hessian H = (1/N) PN
µ=1 x(µ)x(µ)T has d nonzero eigenvalues, all of which are close to unity.
We can therefore identify h∗= 1 and n = d. The mean ﬁrst passage time over 100 independent runs
is measured for varying threshold values which are speciﬁed by c = L(θs)/L(θ∗) > 1. Experimen-
tal results for N = 104 are presented in Fig. 3 (b). Dashed straight lines have slope B/(ηh∗) + 1 −
n/2. Experiments show that the ﬁrst passage time behaves as tp ∝[L(θs)/L(θ∗)]B/(ηh∗)+1−n/2,
which agrees with our theoretical evaluation of κ−1 [see Eq. (16)]. We conclude that the escape rate
crucially depends on the effective dimension n, which is not explained by the previous results (Zhu
et al., 2019; Xie et al., 2021; Liu et al., 2021; Meng et al., 2020)."
EXPERIMENTAL TEST OF STATIONARY DISTRIBUTION AND ESCAPE RATE FORMULA,0.4816753926701571,"Our escape-rate formula (16) is also veriﬁed in a non-linear model. As in Sec. 5.1, we consider the
binary classiﬁcation problem using the ﬁrst 104 samples of MNINST dataset such that we classify
each image into even or odd. The network has one hidden layer with 10 units activated by ReLU,
followed by the output layer of a single unit with no activation. We always use the mean-square
loss. Starting from thee Glorot initialization, the network is pre-trained via SGD with η = 0.01
and B = 100 for 105 iterations. We ﬁnd that after pre-training, the loss becomes almost stationary
around at L(θ) ≈0.035. We regard that the pre-trained network is near a local minimum. We then
further train the pre-trained network via SGD with a new choice of η and B (here we ﬁx B = 10),
and measure the ﬁrst passage time tp. The mean ﬁrst-passage time over 100 independent runs is
measured for varying threshold values which are speciﬁed by c = L(θs)/L(θ∗) > 1. Experimental
results are presented in Fig. 3 (c). We see the power-law behavior, which is consistent with our
theory."
EXPERIMENTAL TEST OF STATIONARY DISTRIBUTION AND ESCAPE RATE FORMULA,0.48429319371727747,"To further verify our theoretical formula (16), we also compare the power-law exponent for the mean
ﬁrst-passage time with our theoretical prediction B/(ηh∗) + 1 −n/2. Here, h∗and n are estimated
by the Hessian eigenvalues. In Appendix E, we present a numerical result for eigenvalues of the
approximate Hessian given by the right-hand side of Eq. (11). By identifying the largest eigenvalue
of the Hessian as h∗, we have h∗≈95.6. On the other hand, it is difﬁcult to precisely determine
the effective dimension n, but it seems reasonable to estimate n ∼10. It turns out that theory and
experiment agree with each other by choosing n = 9. Dashed lines in Fig. 3 (c) correspond to our
theoretical prediction κ−1 ∝cB/(ηh∗)+1−n/2 with h∗= 95.6 and n = 9. This excellent agreement
shows that our theoretical formula (16) is also valid in non-linear models."
CONCLUSION,0.4869109947643979,"6
CONCLUSION"
CONCLUSION,0.4895287958115183,"In this work, we have investigated SGD dynamics via a Langevin approach. With several approx-
imations listed in Appendix A, we have derived Eq. (6), which shows that the SGD noise strength
is proportional to the loss function. This SGD noise covariance structure yields the stochastic dif-
ferential equation (14) with additive noise near a minimum via a random time change (13). The
original multiplicative noise is reduced to simpler additive noise, but instead the gradient of the loss
function is replaced by that of the logarithmized loss function U(θ) = log L(θ). This stochastic
differential equation has a quasi-stationary distribution that decays polynomially with L(θ) near a
minimum (17), not exponentially as in the usual Gibbs distribution. This new formalism yields the
power-law escape rate formula (16) whose exponent depends on η, B, h∗, and n."
CONCLUSION,0.49214659685863876,"Our escape-rate formula explains an empirical fact that SGD favors ﬂat minima with low effective
dimensions. The effective dimension of a minimum must satisfy Eq. (18) for its stability. This result
as well as the formulation of SGD dynamics using the logarithmized loss landscape should help
understand more deeply the SGD dynamics and its implicit biases in machine learning problems."
CONCLUSION,0.49476439790575916,"Although the present work focuses on the Gaussian noise, the non-Gaussianity can also play an
important role. For example, S¸ims¸ekli et al. (2019) approximated SGD as a L´evy-driven SDE,
which explains why SGD ﬁnds wide minima. It would be an interesting future problem to take the
non-Gaussian effect into account."
CONCLUSION,0.4973821989528796,Under review as a conference paper at ICLR 2022
REFERENCES,0.5,REFERENCES
REFERENCES,0.5026178010471204,"Victor Bapst, Thomas Keck, A Grabska-Barwi´nska, Craig Donner, Ekin Dogus Cubuk, Samuel S
Schoenholz, Annette Obika, Alexander WR Nelson, Trevor Back, Demis Hassabis, and Pushmeet
Kohli. Unveiling the predictive power of static structure in glassy systems. Nature Physics, 16
(4):448–454, 2020."
REFERENCES,0.5052356020942408,"Nils Berglund. Kramers’ Law: Validity , Derivations and Generalisations. Markov Processes and
Related Fields, 19:459–490, 2013."
REFERENCES,0.5078534031413613,"Alessandra Bianchi and Alexandre Gaudilli`ere. Metastable states, quasi-stationary distributions and
soft measures. Stochastic Processes and their Applications, 126:1622–1680, 2016."
REFERENCES,0.5104712041884817,"Anton Bovier, Michael Eckhoff, V´eronique Gayrard, and Markus Klein. Metastability in reversible
diffusion processes I. Sharp asymptotics for capacities and exit times. Journal of the European
Mathematical Society, 6:399–424, 2004."
REFERENCES,0.5130890052356021,"Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep
neural networks with multitask learning. In International Conference on Machine Learning, 2008."
REFERENCES,0.5157068062827225,"Mert G¨urb¨uzbalaban, Umut S¸ims¸ekli, and Lingjiong Zhu. The Heavy-Tail Phenomenon in SGD. In
International Conference on Machine Learning, 2021."
REFERENCES,0.518324607329843,"Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, and Others. Deep neural
networks for acoustic modeling in speech recognition: The shared views of four research groups.
IEEE Signal processing magazine, 29(6):82–97, 2012."
REFERENCES,0.5209424083769634,"Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gap in large batch training of neural networks. In Advances in Neural Information Processing
Systems, 2017."
REFERENCES,0.5235602094240838,"Raban Iten, Tony Metger, Henrik Wilming, L´ıdia Del Rio, and Renato Renner. Discovering Physical
Concepts with Neural Networks. Physical Review Letters, 124(1):10508, 2020."
REFERENCES,0.5261780104712042,"Stanisław Jastrze¸bski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Ben-
gio, and Amos Storkey. Three Factors Inﬂuencing Minima in SGD. arXiv:1711.04623, 2017."
REFERENCES,0.5287958115183246,"Nitish Shirish Keskar, Jorge Nocedal, Ping Tak Peter Tang, Dheevatsa Mudigere, and Mikhail
Smelyanskiy. On large-batch training for deep learning: Generalization gap and sharp minima.
In International Conference on Learning Representations, 2017."
REFERENCES,0.5314136125654451,"Hendrik Anthony Kramers. Brownian motion in a ﬁeld of force and the diffusion model of chemical
reactions. Physica, 7(4):284–304, 1940."
REFERENCES,0.5340314136125655,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097–1105,
2012."
REFERENCES,0.5366492146596858,"James S. Langer. Statistical theory of the decay of metastable states. Annals of Physics, 54(2):
258–275, 1969."
REFERENCES,0.5392670157068062,"Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444,
2015."
REFERENCES,0.5418848167539267,"Qianxiao Li, Cheng Tai, and E. Weinan. Stochastic modiﬁed equations and adaptive stochastic
gradient algorithms. In International Conference on Machine Learning, 2017."
REFERENCES,0.5445026178010471,"Kangqiao Liu, Liu Ziyin, and Masahito Ueda.
Noise and Fluctuation of Finite Learning Rate
Stochastic Gradient Descent. In International Conference on Machine Learning, 2021."
REFERENCES,0.5471204188481675,"Djc MacKay. Bayesian model comparison and backprop nets. In Advances in Neural Information
Processing Systems, 1992."
REFERENCES,0.5497382198952879,Under review as a conference paper at ICLR 2022
REFERENCES,0.5523560209424084,"Qi Meng, Shiqi Gong, Wei Chen, Zhi Ming Ma, and Tie Yan Liu. Dynamic of Stochastic Gradient
Descent with State-Dependent Noise. arXiv:2006.13719, 2020."
REFERENCES,0.5549738219895288,"Bernt Øksendal. Stochastic differential equations: an introduction with applications. Springer,
Berlin, 1998."
REFERENCES,0.5575916230366492,"Vardan Papyan. The Full Spectrum of Deepnet Hessians at Scale: Dynamics with SGD Training
and Sample Size. arXiv:1811.07062, 2018."
REFERENCES,0.5602094240837696,"Vardan Papyan. Measurements of three-level hierarchical structure in the outliers in the spectrum of
deepnet hessians. In International Conference on Machine Learning, 2019."
REFERENCES,0.56282722513089,"Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit Bias of SGD for Diagonal
Linear Networks: a Provable Beneﬁt of Stochasticity. arXiv:2106.09524, 2021."
REFERENCES,0.5654450261780105,"Levent Sagun, Utku Evci, V. Ugur G¨uney, Yann Dauphin, and L´eon Bottou. Empirical analysis of
the hessian of over-parametrized neural networks. arXiv:1706.04454, 2017."
REFERENCES,0.5680628272251309,"Issei Sato and Hiroshi Nakagawa. Approximation analysis of stochastic gradient langevin dynam-
ics by using fokker-planck equation and ito process. In International Conference on Machine
Learning, 2014."
REFERENCES,0.5706806282722513,"Alireza Seif, Mohammad Hafezi, and Christopher Jarzynski. Machine learning the thermodynamic
arrow of time. Nature Physics, 17:105–113, 2021."
REFERENCES,0.5732984293193717,"Umut S¸ims¸ekli, Levent Sagun, and Mert Giirbiizbalaban. A tail-index analysis of stochastic gradient
noise in deep neural networks. In International Conference on Machine Learning, 2019."
REFERENCES,0.5759162303664922,"Samuel L. Smith and Quoc V. Le. A Bayesian perspective on generalization and stochastic gradient
descent. In International Conference on Learning Representations, 2018."
REFERENCES,0.5785340314136126,"Stephan Wojtowytsch. Stochastic gradient descent with noise of machine learning type. Part II:
Continuous time analysis. arXiv:2106.02588, 2021."
REFERENCES,0.581151832460733,"Jingfeng Wu, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, and Zhanxing Zhu. On
the Noisy Gradient Descent that Generalizes as SGD. In International Conference on Machine
Learning, 2020."
REFERENCES,0.5837696335078534,"Lei Wu, Chao Ma, and E. Weinan. How SGD selects the global minima in over-parameterized learn-
ing: A dynamical stability perspective. In Advances in Neural Information Processing Systems,
2018."
REFERENCES,0.5863874345549738,"Zeke Xie, Issei Sato, and Masashi Sugiyama. A Diffusion Theory For Deep Learning Dynamics:
Stochastic Gradient Descent Exponentially Favors Flat Minima. In International Conference on
Learning Representations, 2021."
REFERENCES,0.5890052356020943,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
Deep Learning Requires Rethinking of Generalization. In International Conference on Learning
Representations, 2017a."
REFERENCES,0.5916230366492147,"Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient
Langevin dynamics. In Proceedings of Machine Learning Research, 2017b."
REFERENCES,0.5942408376963351,"Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochas-
tic gradient descent: Its behavior of escaping from sharp minima and regularization effects. In
International Conference on Machine Learning, 2019."
REFERENCES,0.5968586387434555,"Liu Ziyin, Kangqiao Liu, Takashi Mori, and Masahito Ueda. On Minibatch Noise: Discrete-Time
SGD, Overparametrization, and Bayes. arXiv:2102.05375, 2021."
REFERENCES,0.599476439790576,Under review as a conference paper at ICLR 2022
REFERENCES,0.6020942408376964,"A
LIST OF APPROXIMATIONS AND THEIR JUSTIFICATIONS"
REFERENCES,0.6047120418848168,"In Sec. 3.1, we made several approximations to derive the result (6). For clarity, we list the approxi-
mations made and their justiﬁcations below."
REFERENCES,0.6073298429319371,"• In Eq. (8), we make the approximation of B << N, which simpliﬁes the expression but is
not essential. The main conclusion is not affected by this approximation."
REFERENCES,0.6099476439790575,"• We ignore ∇L∇LT in Eq. (8), which is justiﬁed near a local minimum."
REFERENCES,0.612565445026178,"• In Eq. (9), we make the decoupling approximation (7), which is one of the key heuristic
approximations in our work. This approximation is veriﬁed experimentally in Sec. 5.1."
REFERENCES,0.6151832460732984,"• We ignore the last term of Eq. (10), which is a common approximation (Sagun et al., 2017).
This approximation is justiﬁed if we are only interested in the outliers of the Hessian eigen-
values. Indeed, outliers of the Hessian eigenvalues, which play dominant roles in escape
from local minima, are known to be attributable to the ﬁrst term of the right-hand side of
Eq. (10) (Papyan, 2018)."
REFERENCES,0.6178010471204188,"• In Eq. (11), we assume that the matrix (1/N) PN
µ=1 ∇f(θ, x(µ))∇f(θ, x(µ))T does not
change so much in a valley with a given local minimum. This approximation is indirectly
veriﬁed in Fig. 2 (the proportionality between the loss and the noise strength implies this
matrix is actually constant)."
REFERENCES,0.6204188481675392,"In deriving the escape rate for multi-variable cases in Sec. 4, we further make the following assump-
tions and approximations:"
REFERENCES,0.6230366492146597,"• First of all, we discard a bulk of near-zero eigenvalues of the Hessian and restricted learning
dynamics within the subspace spanned by the outlier eigenvectors. This approximation
is justiﬁed because the SGD dynamics is almost frozen along ﬂat directions of the loss
landscape."
REFERENCES,0.6256544502617801,"• We approximate the SGD noise as an isotropic one within the n-dimensional outlier sub-
space. This approximation is justiﬁed either when the loss landscape in the outlier subspace
is isotropic near the minimum or when the directions of the Hessian eigenvectors do not
change within the valley."
REFERENCES,0.6282722513089005,"• At the last part of the derivation, we approximate τ ≈L(θ∗)t, which is justiﬁed because
the model stays near a local minimum until it escapes from that minimum."
REFERENCES,0.6308900523560209,"B
STATIONARY DISTRIBUTION"
REFERENCES,0.6335078534031413,Since ˜θτ obeys a simple Langevin equation
REFERENCES,0.6361256544502618,"d˜θτ = −U ′(˜θτ) +
√"
REFERENCES,0.6387434554973822,"2Td ˜Wτ,
(20)"
REFERENCES,0.6413612565445026,"the stationary distribution of ˜θτ is given by the Gibbs distribution ˜Pss(θ) ∝e−U(θ)/T . On the other
hand, what we want is the stationary distribution Pss(θ) of θt, where θt = ˜θτ with τ =
R t
0 dt′ L(θt′).
In this section, we show the relation between the two distributions: Pss(θ) ∝L(θ)−1 ˜Pss(θ)."
REFERENCES,0.643979057591623,We express the stationary distributions in terms of the long-time average of the delta function:
REFERENCES,0.6465968586387435,"Pss(θ) = lim
s→∞
1
s Z s"
REFERENCES,0.6492146596858639,"0
dt δ(θt −θ),
˜Pss(θ) = lim
s→∞
1
s Z s"
REFERENCES,0.6518324607329843,"0
dτ δ(˜θτ −θ).
(21)"
REFERENCES,0.6544502617801047,"By using the relation τ =
R t
0 dt′ L(θt′), we have dτ = L(θt)dt. For a sufﬁciently large t, we also
obtain τ ∼t¯L, where ¯L := lims→∞(1/s)
R s
0 dt′ L(θt′) denotes the long-time average of L(θt). By"
REFERENCES,0.6570680628272252,Under review as a conference paper at ICLR 2022
REFERENCES,0.6596858638743456,"using them, Pss(θ) is rewritten as"
REFERENCES,0.662303664921466,"Pss(θ) ≈lim
s→∞
1
s Z s¯L"
REFERENCES,0.6649214659685864,"0
dτ δ(˜θτ −θ)"
REFERENCES,0.6675392670157068,L(˜θτ)
REFERENCES,0.6701570680628273,"=
1
L(θ) lim
s→∞"
REFERENCES,0.6727748691099477,"¯L
s¯L Z s¯L"
REFERENCES,0.675392670157068,"0
dτ δ(˜θτ −θ)"
REFERENCES,0.6780104712041884,"=
¯L
L(θ)
˜Pss(θ).
(22)"
REFERENCES,0.680628272251309,"We thus obtain the desired relation, Pss(θ) ∝L(θ)−1 ˜Pss(θ)."
REFERENCES,0.6832460732984293,"C
JUSTIFICATION OF THE ISOTROPIC-NOISE APPROXIMATION WITHIN THE
n-DIMENSIONAL SUBSPACE"
REFERENCES,0.6858638743455497,"We now show that the isotropic-noise approximation is valid when the directions of the eigen-
vectors of the Hessian do not change within the valley of a given local minimum. In this case,
∂2U/∂zi∂zj = 0 for any i ̸= j, where the Hessian at the minimum is given by H(θ∗) ≈
Pn
i=1 h∗
i vivT
i (hi is an eigenvalue and vi is the corresponding eigenvector of H(θ∗)) and the dis-
placement vector z ∈Rn is deﬁned by θ = θ∗+ Pn
i=1 zivi. The stochastic differential equation
dz = −∇zUdτ +
p"
REFERENCES,0.6884816753926701,"2ηH(θ∗)/Bd ˜Wτ is then equivalent to the following Fokker-Planck equation
for the distribution function P(z, τ) of z at τ:"
REFERENCES,0.6910994764397905,"∂P(z, τ) ∂τ
= n
X i=1  ∂ ∂zi ∂U"
REFERENCES,0.693717277486911,"∂zi
P

+ ηh∗
i
B
∂2"
REFERENCES,0.6963350785340314,"∂z2
i
P

.
(23)"
REFERENCES,0.6989528795811518,"Let us assume that the direction of eth eigenvector ve corresponds to the escape direction. We denote
by z⊥∈Rn−1 the displacement perpendicular to the escape direction, and write z = (ze, z⊥). At
the saddle zs, ∇zU(zs) = 0, hs
e < 0 and hs
i > 0 for all i ̸= e, where {hs
i} is the set of eigenvectors
of the Hessian at zs."
REFERENCES,0.7015706806282722,"Under the above setting, we derive the escape rate formula following Kramers (1940). The steady
current J ∈Rn is aligned to the escape direction, and hence Je ̸= 0 and J⊥= 0. Let us denote by
P ∗the total probability within the valley of a given minimum θ∗and by J the total current ﬂowing
to outside of the valley through the saddle θs = θ∗+ zs. It is assumed that J is small and P ∗is
almost stationary. The escape rate κτ is then given by"
REFERENCES,0.7041884816753927,κτ = J
REFERENCES,0.7068062827225131,"P ∗.
(24)"
REFERENCES,0.7094240837696335,The escape rate κ per unit time is given by
REFERENCES,0.7120418848167539,κ = L(θ∗)κτ = L(θ∗) J
REFERENCES,0.7146596858638743,"P ∗.
(25)"
REFERENCES,0.7172774869109948,"We now evaluate P ∗and J . To evaluate P ∗, it is necessary to know about the stationary distribution
near the minimum θ∗. Near the minimum θ∗(i.e. small z),"
REFERENCES,0.7198952879581152,U(θ) = log L(θ) ≈U(θ∗) + (θ −θ∗)TH(θ∗)(θ −θ∗)
REFERENCES,0.7225130890052356,"2L(θ∗)
= U(θ∗) + zTH(θ∗)z"
REFERENCES,0.725130890052356,"2L(θ∗)
(26)"
REFERENCES,0.7277486910994765,"and
∂U
∂zi
≈
1
L(θ∗) n
X"
REFERENCES,0.7303664921465969,"j=1
H∗
ijzj,
(27)"
REFERENCES,0.7329842931937173,"where H∗= H(θ∗). By substituting it into the Fokker-Planck equation (23), we obtain"
REFERENCES,0.7356020942408377,"∂P(z, τ) ∂τ
= n
X i=1 ∂
∂zi  
n
X"
REFERENCES,0.7382198952879581,"j=1
H∗
ij"
REFERENCES,0.7408376963350786,"
zj
L(θ∗) + η"
REFERENCES,0.743455497382199,"B
∂
∂zj 
P "
REFERENCES,0.7460732984293194,".
(28)"
REFERENCES,0.7486910994764397,Under review as a conference paper at ICLR 2022 ze
REFERENCES,0.7513089005235603,U(θ) −U(θ*) : z⊥= 0
ZS,0.7539267015706806,"0
zs e
zc"
ZS,0.756544502617801,"e
(θ = θ*)
(θ = θs)"
ZS,0.7591623036649214,"P(θ)
Jpath"
ZS,0.7617801047120419,Figure 4: Schematic illustration of the escape from a potential barrier.
ZS,0.7643979057591623,"If for all j

zj
L(θ∗) + η"
ZS,0.7670157068062827,"B
∂
∂zj"
ZS,0.7696335078534031,"
Pss(z) = 0,
(29)"
ZS,0.7722513089005235,"Pss(z) is a stationary distribution near the minimum θ∗. Indeed,"
ZS,0.774869109947644,"Pss(z) = P(θ∗)e−
B
2ηL(θ∗)
Pn
i=1 z2
i
(30)"
ZS,0.7774869109947644,"satisﬁes this condition. It should be noted that the stationary distribution is independent of the
Hessian eigenvalues {h∗
i }. By using Eq. (30), P ∗is evaluated as"
ZS,0.7801047120418848,"P ∗≈
Z
dz1dz2 . . . dznPss(z) = P(θ∗)
2ηL(θ∗) B"
ZS,0.7827225130890052,"n/2
.
(31)"
ZS,0.7853403141361257,"Next, let us evaluate J . The probability current J along the escape direction e is given by"
ZS,0.7879581151832461,Je = −∂U
ZS,0.7905759162303665,"∂ze
P −ηh∗
e
B
∂
∂ze
P.
(32)"
ZS,0.7931937172774869,"At the saddle, the current vector J⊥perpendicular to the escape direction is zero, and hence the
probability distribution near the saddle with ze = zs
e and z⊥̸= 0 is also evaluated in a similar way
as in Eq. (30):
P(ze = zs
e, z⊥) ≈P(ze = zs
e, z⊥= 0)e−
B
2ηL(θs) z2
⊥.
(33)
By substituting it into Eq. (32), we obtain"
ZS,0.7958115183246073,"Je(zs
e, z⊥) = Jpathe−
B
2ηL(θs) z2
⊥,
(34)"
ZS,0.7984293193717278,"where the current along the escape path (z⊥= 0) is denoted by Jpath := J(zs
e, z⊥= 0). The total
current through the saddle is then evaluated as"
ZS,0.8010471204188482,"J =
Z
dz⊥Je(zs
e, z⊥) = Jpath"
ZS,0.8036649214659686,2πηL(θs) B
ZS,0.806282722513089,"(n−1)/2
.
(35)"
ZS,0.8089005235602095,"When the distribution function is almost stationary, Eq. (23) yields ∂Je(ze, z⊥= 0)/∂ze ≈0, and
hence the current along the escape path is approximately constant Je(ze, z⊥= 0) ≈Jpath. Since
Je(ze, z⊥= 0) is given by Eq. (32) by putting z⊥= 0, we have"
ZS,0.8115183246073299,Jpath = −∂U ∂ze
ZS,0.8141361256544503,"z⊥=0
P −ηh∗
e
B
∂P
∂ze
= −ηh∗
e
B e
−
B
ηh∗e U ∂ ∂ze 
e"
ZS,0.8167539267015707,"B
ηh∗e UP

.
(36)"
ZS,0.819371727748691,By multiplying e
ZS,0.8219895287958116,"B
ηh∗e U in both sides and integrating over ze from 0 to zc
e, where zc
e deﬁned as
U(θ∗+ zc
eve) = U(θ∗) (see Fig. 4), we obtain Jpath"
ZS,0.824607329842932,"Z zc
e"
DZE E,0.8272251308900523,"0
dze e"
DZE E,0.8298429319371727,"B
ηh∗e U = ηh∗
e
B e"
DZE E,0.8324607329842932,"B
ηh∗e U(θ∗)P(θ∗),
(37)"
DZE E,0.8350785340314136,Under review as a conference paper at ICLR 2022
DZE E,0.837696335078534,"where it is assumed that the probability at zc
e is small and negligible, P(ze = zc
e, z⊥= 0) ≈0. By
using the saddle-point method, the integral in the left-hand side of Eq. (37) is evaluated as
Z zc
e"
DZE E,0.8403141361256544,"0
dze e"
DZE E,0.8429319371727748,"B
ηh∗e U ≈
Z ∞"
DZE E,0.8455497382198953,"−∞
dze exp
 B ηh∗e"
DZE E,0.8481675392670157,"
U(θs) +
hs
e
2L(θs)(ze −zs
e)2
"
DZE E,0.8507853403141361,"=

2πηh∗
e
B|hse|L(θs)"
DZE E,0.8534031413612565,"1/2
e"
DZE E,0.856020942408377,"B
ηh∗e U(θs).
(38)"
DZE E,0.8586387434554974,"By substituting this result in Eq. (37), we obtain"
DZE E,0.8612565445026178,"Jpath =
 ηh∗
e|hs
e|
2πBL(θs)"
DZE E,0.8638743455497382,"1/2
e
−
B
ηh∗e ∆UP(θ∗),
(39)"
DZE E,0.8664921465968587,where ∆U = U(θs) −U(θ∗). The total current J in Eq. (35) is then expressed as J = p
DZE E,0.8691099476439791,"h∗e|hse|
2πL(θs)"
DZE E,0.8717277486910995,2πηL(θs) B
DZE E,0.8743455497382199,"n/2
e
−
B
ηh∗e ∆UP(θ∗).
(40)"
DZE E,0.8769633507853403,"By using Eqs. (31) and (40), the escape rate κ in Eq. (25) is evaluated as"
DZE E,0.8795811518324608,κ = L(θ∗) J P ∗= p
DZE E,0.8821989528795812,h∗e|hse| 2π
DZE E,0.8848167539267016,L(θs) L(θ∗)  n
DZE E,0.887434554973822,"2 −1
e−
B
ηhe∗∆U.
(41)"
DZE E,0.8900523560209425,"Since ∆U = log[L(θs)/L(θ∗)], we ﬁnally obtain κ = p"
DZE E,0.8926701570680629,h∗e|hse| 2π
DZE E,0.8952879581151832,L(θs) L(θ∗)
DZE E,0.8979057591623036,"−

B
ηh∗e +1−n 2
"
DZE E,0.900523560209424,",
(42)"
DZE E,0.9031413612565445,"which is exactly identical to the escape rate formula derived in the main text using the isotropic-noise
approximation with h∗= h∗
e."
DZE E,0.9057591623036649,"In this way, the isotropic-noise approximation is justiﬁed even when the loss landscape is not
isotropic near the minimum. We have assumed that the directions of the eigenvectors of the Hessian
do not change within the valley of a given local minimum. Under this assumption, the Fokker-Planck
equation along the escape path is decoupled from the perpendicular directions z⊥. Moreover, we
have seen that the stationary distribution near a minimum is independent of the Hessian eigenvalues.
These properties explain the reason why the isotropic-noise approximation is justiﬁed in this case."
DZE E,0.9083769633507853,"D
OTHER LOSS FUNCTIONS"
DZE E,0.9109947643979057,"In our paper, we mainly focus on the mean-square loss, for which we can analytically derive the
relation between the loss L(θ) and the SGD noise covariance Σ(θ). An important observation is that
the SGD noise strength N is proportional to the loss, i.e., N ∝L(θ) (see Sec. 5.2 of the main text
for the deﬁnition of N)."
DZE E,0.9136125654450262,"Here, we argue that the relation N ∝L(θ) also holds in more general situations. During the
training, the value of ℓµ will ﬂuctuate from sample to sample. At a certain time step of SGD, let
us suppose that N −M samples in the training dataset are already classiﬁed correctly and hence
ℓµ ∼0, whereas the other M samples are not and hence ℓµ ∼1. The loss function is then given
by L(θ) = (1/N) PN
µ=1 ℓµ ∼M/N. When ℓµ is small, ∇ℓµ will also be small. Therefore,
for N −M samples with ℓµ ∼0, ∇ℓµ ∼0 also holds. The other M samples will have non-
small gradients: ∥∇ℓµ∥2 ∼g, where g > 0 is a certain positive constant. We thus estimate N as
N ≈(1/N) PN
µ=1 ∇ℓT
µ∇ℓµ ∼gM/N ∼gL(θ). In this way, N ∝L(θ) will hold, irrespective of
the loss function."
DZE E,0.9162303664921466,"However, we emphasize that this is a crude argument. In particular, the above argument will not
hold near a global minimum because all the samples are correctly classiﬁed there, which implies
that ℓµ is small for all µ, in contrast to the above argument relying on the existence of M wrongly
classiﬁed samples with ℓµ ∼1 and ∥∇ℓµ∥2 ∼g."
DZE E,0.918848167539267,Under review as a conference paper at ICLR 2022
DZE E,0.9214659685863874,"(a)
(b)"
DZE E,0.9240837696335078,"0
20
40
60
80
100
epoch 10
5 10
4 10
3 10
2 10
1 loss"
DZE E,0.9267015706806283,(SGD noise strength)/(5 × 103)
DZE E,0.9293193717277487,"0
20
40
60
80
100
epoch 10
8 10
6 10
4 10
2 100 loss"
DZE E,0.9319371727748691,(SGD noise strength)/(6 × 104)
DZE E,0.9345549738219895,"(c)
(d)"
DZE E,0.93717277486911,"10
3
10
2
10
1 loss 10
1 100 101 102 103"
DZE E,0.9397905759162304,SGD noise strength
DZE E,0.9424083769633508,"10
4
10
2
100 loss 10
3 10
1 101 103"
DZE E,0.9450261780104712,SGD noise strength
DZE E,0.9476439790575916,"Figure 5: Training dynamics of the loss function and the SGD noise strength N for (a) a fully
connected network trained by the Fashion-MNIST dataset and (b) a convolutional network trained
by the CIFAR-10 dataset. In the ﬁgure, we multiplied N by a numerical factor to emphasize that
N is actually proportional to the loss in an intermediate stage of the training. Loss vs N for (c) a
fully connected network trained by the Fashion-MNIST and (d) a convolutional network trained by
CIFAR-10. Dashed lines in (c) and (d) are straight lines of slope 1, which imply N ∝L(θ)."
DZE E,0.9502617801047121,"We now experimentally test the relation N ∝L(θ) for the cross-entropy loss. We consider the
same architectures and datasets in Sec. 5.2 of the main text: a fully connected neural network
trained by Fashion-MNIST and a convolutional neural network trained by CIFAR-10 (see Sec. 5.2
for the detail). We ﬁx B = 100 in both cases, and η = 0.1 for the fully connected network and
η = 0.05 for the convolutional network. Experimental results are presented in Fig. 5. We ﬁnd that
the relation N ∝L(θ) seems to hold true at an intermediate stage of the training dynamics, although
the proportionality is less clear compared with Fig. 2 in the main text for the mean-square loss."
DZE E,0.9528795811518325,"We also ﬁnd that for sufﬁciently small values of the loss, N ∝L(θ)2 [see Fig. 5 (c) and (d)], whose
implications should merit further investigation in future studies."
DZE E,0.9554973821989529,"E
HESSIAN EIGENVALUES FOR A NEURAL NETWORK IN SEC. 5.3"
DZE E,0.9581151832460733,"We present numerical results on Hessian eigenvalues in a pre-trained neural network studied in
Sec. 5.3. Instead of the exact Hessian, we consider an approximate Hessian given on the right-hand
side of Eq. (11), i.e.,"
DZE E,0.9607329842931938,"H(θ∗) ≈1 N N
X"
DZE E,0.9633507853403142,"µ=1
∇f(θ, x(µ))∇f(θ, x(µ))T.
(43)"
DZE E,0.9659685863874345,"Eigenvalues {hi} are arranged in descending order as h1 ≥h2 ≥· · · ≥hP (in our model P =
7861)."
DZE E,0.9685863874345549,"A histogram of the Hessian eigenvalues is presented in Fig. 6 (a). We see that most eigenvalues are
close to zero, which corresponds to the bulk, but there are some large eigenvalues, which correspond
to the outliers. The largest eigenvalue is λ1 = 95.6, which is identiﬁed as h∗in our theoretical
formula (16)."
DZE E,0.9712041884816754,Under review as a conference paper at ICLR 2022
DZE E,0.9738219895287958,"(a)
(b)"
DZE E,0.9764397905759162,"0
20
40
60
80
100
eigenvalues 100 101 102 103 104"
DZE E,0.9790575916230366,"1
4
7
10
13
16
19
22
25
28
i 0 20 40 60 80 100 hi"
DZE E,0.981675392670157,"Figure 6: Hessian eigenvalues for a pre-trained neural network studied in Sec. 5.3. (a) The histogram
of the Hessian eigenvalues. Most eigenvalues are close to zero, but there are some large eigenvalues,
which correspond to outliers."
DZE E,0.9842931937172775,"Another important quantity is the effective dimension n corresnding to the number of outliers. Since
the outliers and the bulk are not sharply separated, it is difﬁcult to precisely determine n. In Fig. 6
(b), we plot hi up to i = 30. From this ﬁgure, it seems reasonable to estimate n ∼10."
DZE E,0.9869109947643979,"As a heuristic method of determining n, we can consider the following identiﬁcation: ﬁrst we deﬁne
the weight pi for ith eigenmode as pi = hi/ PP
j=1 hj. We then determine n as n = P
X"
DZE E,0.9895287958115183,"i=1
p2
i !−1"
DZE E,0.9921465968586387,",
(44)"
DZE E,0.9947643979057592,which gives n = 12.7 in our case.
DZE E,0.9973821989528796,"In Sec. 5.3, our formula (16) with n = 9 explains numerical results on the ﬁrst-passage time."
