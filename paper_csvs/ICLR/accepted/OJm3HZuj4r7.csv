Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0016339869281045752,"Despite the empirical success of the deep Q network (DQN) reinforcement learn-
ing algorithm and its variants, DQN is still not well understood and it does not
guarantee convergence. In this work, we show that DQN can indeed diverge and
cease to operate in realistic settings. Although there exist gradient-based con-
vergent methods, we show that they actually have inherent problems in learning
dynamics which cause them to fail even in simple tasks. To overcome these prob-
lems, we propose a convergent DQN algorithm (C-DQN) that is guaranteed to
converge and can work with large discount factors (∼0.9998). It learns robustly
in difﬁcult settings and can learn several difﬁcult games in the Atari 2600 bench-
mark that DQN fails to solve."
INTRODUCTION,0.0032679738562091504,"1
INTRODUCTION"
INTRODUCTION,0.004901960784313725,"With the development of deep learning, reinforcement learning (RL) that utilizes deep neural net-
works has demonstrated great success recently, ﬁnding applications in various ﬁelds including
robotics, games, and scientiﬁc research (Levine et al., 2018; Berner et al., 2019; Fösel et al., 2018;
Wang et al., 2020). One of the most efﬁcient RL strategy is Q-learning (Watkins, 1989), and the
combination of Q-learning and deep learning leads to the DQN algorithms (Mnih et al., 2015; Hes-
sel et al., 2018; Riedmiller, 2005), which hold records on many difﬁcult RL tasks (Badia et al.,
2020). However, unlike supervised learning, Q-learning, or more generally temporal difference
(TD) learning, does not guarantee convergence when function approximations such as neural net-
works are used, and as a result, their success is actually empirical, and the performance relies heavily
on hyperparameter tuning and technical details involved. This happens because the agent uses its
own prediction to construct the learning objective, a.k.a. bootstrapping, and as it generalizes, its pre-
dictions over different data interfere with each other, which can make its learning objective unstable
in the course of training and potentially lead to instability and divergence."
INTRODUCTION,0.006535947712418301,"This non-convergence problem was pointed out decades ago by the pioneering works of Baird (1995)
and Tsitsiklis & Van Roy (1997), and it has been empirically investigated for DQN by Van Hasselt
et al. (2018). We have also observed the divergence of DQN in our experiments, as in Fig. 6. The
non-convergence problem often shows up as instability in practice and it places signiﬁcant obstacles
to the application of DQN to complicated tasks. It makes the training with deeper neural networks
more difﬁcult, limits the time horizon for planning, and makes the results sometimes unstable and
sensitive to hyperparameters. This state of affairs is not satisfactory especially for those scientiﬁc
applications that require convergence and generality. Although convergent gradient-based methods
have also been proposed (Sutton et al., 2009; Bhatnagar et al., 2009; Feng et al., 2019; Ghiassian
et al., 2020), they cannot easily be used with deep non-linear neural networks as they either re-
quire linearity or involve computationally heavy operations, and they often show worse empirical
performance compared with TD methods."
INTRODUCTION,0.008169934640522876,"In this work, we show that the above-mentioned gradient-based methods actually have inherent
problems in learning dynamics which hamper efﬁcient learning, and we propose a convergent DQN"
INTRODUCTION,0.00980392156862745,"∗RIKEN Center for Emergent Matter Science (CEMS), Wako, Saitama 351-0198, Japan"
INTRODUCTION,0.011437908496732025,Published as a conference paper at ICLR 2022
INTRODUCTION,0.013071895424836602,"(C-DQN) algorithm by modifying the loss of DQN. Because an increase of loss upon updating the
target network of DQN is a necessary condition for its divergence, we construct a loss that does not
increase upon the update of the target network, and therefore, the proposed algorithm converges in
the sense that the loss monotonically decreases. In Sec. 2 we present the background. In Sec. 3
we discuss the inefﬁciency issues in the previous gradient-based methods and demonstrate using
toy problems. In Sec. 4 we propose C-DQN and show its convergence. In Sec. 5 we show the
results of C-DQN on the Atari 2600 benchmark (Bellemare et al., 2013) and in Sec. 6 we present
the conclusion and future prospect. To our knowledge, the proposed C-DQN algorithm is the ﬁrst
convergent RL method that is sufﬁciently efﬁcient and scalable to obtain successful results on the
standard Atari 2600 benchmark using deep neural networks, showing its efﬁcacy in dealing with
realistic and complicated problems."
BACKGROUND,0.014705882352941176,"2
BACKGROUND"
BACKGROUND,0.016339869281045753,"Reinforcement learning involves a Markov decision process (MDP), where the state st of an envi-
ronment at time step t makes a transition to the next state st+1 conditioned on the action of the agent
at at time t, producing a reward rt depending on the states. The process can terminate at terminal
states sT , and the transition of states can be either probabilistic or deterministic. The goal is to ﬁnd
a policy π(s) to determine the actions at+i ∼π(st+i) in order to maximizes the return PT −t
i=0 rt+i,
i.e., the sum of future rewards. In practice, a discounted return PT −t
i=0 γirt+i is often used instead,
with the discount factor γ < 1 and γ ≈1, so that the expression is convergent for T →∞and that
rewards far into the future can be ignored, giving an effective time horizon
1
1−γ . The value function
is deﬁned as the expected return for a state st following a policy π, and the Q function is deﬁned as
the expected return for a state-action pair (st, at):"
BACKGROUND,0.017973856209150325,"Vπ(st) = Eat,{(st+i,at+i)}T −t
i=1"
BACKGROUND,0.0196078431372549,"""T −t
X"
BACKGROUND,0.021241830065359478,"i=0
γirt+i #"
BACKGROUND,0.02287581699346405,",
Qπ(st, at) = E{(st+i,at+i)}T −t
i=1"
BACKGROUND,0.024509803921568627,"""T −t
X"
BACKGROUND,0.026143790849673203,"i=0
γirt+i # , (1)"
BACKGROUND,0.027777777777777776,"with at+i ∼π(st+i) in the evaluation of the expectation. When the Q function is maximized by a
policy, we say that the policy is optimal and denote the Q function and the policy by Q∗and π∗,
respectively. The optimality implies that Q∗satisﬁes the Bellman equation (Sutton & Barto, 2018)"
BACKGROUND,0.029411764705882353,"Q∗(st, at) = rt + γEst+1
h
max
a′ Q∗(st+1, a′)
i
.
(2)"
BACKGROUND,0.03104575163398693,"The policy π∗is greedy with respect to Q∗, i.e. π∗(s) = arg maxa′ Q∗(s, a′). Q-learning uses this
recursive relation to learn Q∗. In this work we only consider the deterministic case and drop the
notation Est+1 [·] where appropriate."
BACKGROUND,0.032679738562091505,"When the space of state-action pairs is small and ﬁnite, we can write down the values of an arbitrarily
initialized Q function for all state-action pairs into a table, and iterate over the values using"
BACKGROUND,0.03431372549019608,"∆Q(st, at) = α

rt + γ max
a′ Q(st+1, a′) −Q(st, at)

,
(3)"
BACKGROUND,0.03594771241830065,"where α is the learning rate. This is called Q-table learning and it guarantees convergence to Q∗.
If the space of (s, a) is large and Q-table learning is impossible, a function approximation is used
instead, representing the Q function as Qθ with learnable parameter θ. The learning rule is"
BACKGROUND,0.03758169934640523,"∆θ = α∇θQθ(st, at)

rt + γ max
a′ Qθ(st+1, a′) −Qθ(st, at)

,
(4)"
BACKGROUND,0.0392156862745098,"which can be interpreted as modifying the value of Qθ(st, at) following the gradient so that
Qθ(st, at) approaches the target value rt + γ maxa′ Qθ(st+1, a′).
However, this iteration
may not converge, because the term maxa′ Qθ(st+1, a′) is also θ-dependent and may change
together with Qθ(st, at).
Speciﬁcally, an exponential divergence occurs if γ∇θQθ(st, at) ·
∇θ maxa′ Qθ(st+1, a′) > ||∇θQθ(st, at)||2 is always satisﬁed and the value of maxa′ Qθ(st+1, a′)
is not constrained by other means.1 This can be a serious issue for realistic tasks, because the adja-
cent states st and st+1 often have similar representations and ∇θQθ(st, ·) is close to ∇θQθ(st+1, ·)."
BACKGROUND,0.04084967320261438,"1This can be shown by checking the Bellman error δt := rt + γ maxa′ Qθ(st+1, a′) −Qθ(st, at), for
which we have ∆δt = αδt
 
γ∇θQθ(st, at) · maxa′ Qθ(st+1, a′) −||∇θQθ(st, at)||2
up to the ﬁrst order
of ∆θ following Eq. (4). As ∆δt is proportional to δt with the same sign, it can increase exponentially."
BACKGROUND,0.042483660130718956,Published as a conference paper at ICLR 2022
BACKGROUND,0.04411764705882353,"The DQN algorithm uses a deep neural network with parameters θ as Qθ (Mnih et al., 2015),
and to stabilize learning, it introduces a target network with parameters ˜θ, and replace the term
maxa′ Qθ(st+1, a′) by maxa′ Q˜θ(st+1, a′), so that the target value rt + γ maxa′ Q˜θ(st+1, a′) does
not change simultaneously with Qθ. The target network ˜θ is then updated by copying from θ for ev-
ery few thousand iterations of θ. This technique reduces ﬂuctuations in the target value and dramati-
cally improves the stability of learning, and with the use of ofﬂine sampling and adaptive optimizers,
it can learn various tasks such as video games and simulated robotic control (Mnih et al., 2015; Lil-
licrap et al., 2015). Nevertheless, the introduction of the target network ˜θ is not well-principled,
and it does not really preclude the possibility of divergence. As a result, DQN sometimes requires
a signiﬁcant amount of hyperparameter tuning in order to work well for a new task, and in some
cases, the instability in learning can be hard to diagnose or remove, and usually one cannot use a
discount factor γ that is very close to 1. In an attempt to solve this problem, Durugkar & Stone
(2017) considered only updating θ in a direction that is perpendicular to ∇θ maxa′ Qθ(st+1, a′);
however, this strategy is not satisfactory in general and can lead to poor performance, as shown in
Pohlen et al. (2018)."
BACKGROUND,0.0457516339869281,"One way of approaching this problem is to consider the mean squared Bellman error (MSBE), which
is originally proposed by Baird (1995) and called the residual gradient (RG) algorithm. The Bellman
error, or Bellman residual, TD error, is given by δt(θ) := rt + γ maxa′ Qθ(st+1, a′) −Qθ(st, at).
Given a dataset S of state-action data, δt is a function of θ, and we can minimize the MSBE loss"
BACKGROUND,0.04738562091503268,"LMSBE(θ) := E
h
|δ(θ)|2i
= 1 |S| X"
BACKGROUND,0.049019607843137254,"(st,at,rt,st+1)∈S"
BACKGROUND,0.05065359477124183,"Qθ(st, at) −rt −γ max
a′ Qθ(st+1, a′)

2
,
(5)"
BACKGROUND,0.05228758169934641,"and in practice the loss is minimized via gradient descent. If LMSBE becomes zero, we have δt ≡0
and the Bellman equation is satisﬁed, implying Qθ = Q∗. Given a ﬁxed dataset S, the convergence
of the learning process simply follows the convergence of the optimization of the loss. This strategy
can be used with neural networks straightforwardly. There have also been many improvements on
this strategy including Sutton et al. (2008; 2009); Bhatnagar et al. (2009); Dai et al. (2018); Feng
et al. (2019); Ghiassian et al. (2020); Touati et al. (2018), and they are often referred to as gradient-
TD methods. Many of them have focused on how to evaluate the expectation term in Eq. (2) and
make it converge to the same solution found by TD, or Q-learning. However, most of these methods
often do not work well for difﬁcult problems, and few of them have been successfully demonstrated
on standard RL benchmarks, especially the Atari 2600 benchmark. In the following we refer to the
strategy of simply minimizing LMSBE as the RG algorithm, or RG learning."
INEFFICIENCY OF MINIMIZING THE MSBE,0.05392156862745098,"3
INEFFICIENCY OF MINIMIZING THE MSBE"
ILL-CONDITIONNESS OF THE LOSS,0.05555555555555555,"3.1
ILL-CONDITIONNESS OF THE LOSS"
ILL-CONDITIONNESS OF THE LOSS,0.05718954248366013,"In the following we show that minimizing LMSBE may not lead to efﬁcient learning by considering the
case of deterministic tabular problems, for which all the gradient-TD methods mentioned above re-
duce to RG learning. For a tabular problem, the Q function values for different state-action pairs can
be regarded as independent variables. Suppose we have a trajectory of experience {(st, at, rt)}N−1
t=0
obtained by following the greedy policy with respect to Q, i.e. at = arg maxa′ Q(st, a′), and sN is
a terminal state. Taking γ = 1, the MSBE loss is given by"
ILL-CONDITIONNESS OF THE LOSS,0.058823529411764705,LMSBE = 1 N
ILL-CONDITIONNESS OF THE LOSS,0.06045751633986928,"""N−2
X"
ILL-CONDITIONNESS OF THE LOSS,0.06209150326797386,"t=0
(Q(st, at) −rt −Q(st+1, at+1))2 + (Q(sN−1, aN−1) −rN−1)2
# .
(6)"
ILL-CONDITIONNESS OF THE LOSS,0.06372549019607843,"Despite the simple quadratic form, the Hessian matrix of LMSBE with variables {Q(st, at)}N−1
t=0 is
ill-conditioned, and therefore does not allow efﬁcient gradient descent optimization. The condition
number κ of a Hessian matrix is deﬁned by κ := |λmax|"
ILL-CONDITIONNESS OF THE LOSS,0.06535947712418301,"|λmin| , where λmax and λmin are the largest and
smallest eigenvalues. We have numerically found that κ of the Hessian of LMSBE in Eq. (6) grows as
O(N 2). To ﬁnd an analytic expression, we add an additional term Q(s0, a0)2 to LMSBE, so that the"
ILL-CONDITIONNESS OF THE LOSS,0.06699346405228758,Published as a conference paper at ICLR 2022 Goal
ILL-CONDITIONNESS OF THE LOSS,0.06862745098039216,"101
102
103
104
105"
ILL-CONDITIONNESS OF THE LOSS,0.07026143790849673,update steps 10-1 100 101 102 103 104 105
ILL-CONDITIONNESS OF THE LOSS,0.0718954248366013,MSBE of Q-table
ILL-CONDITIONNESS OF THE LOSS,0.07352941176470588,|Q −Q ∗|2 of Q-table
ILL-CONDITIONNESS OF THE LOSS,0.07516339869281045,MSBE of RG
ILL-CONDITIONNESS OF THE LOSS,0.07679738562091504,|Q −Q ∗|2 of RG
ILL-CONDITIONNESS OF THE LOSS,0.0784313725490196,"10-2
10-1
100
101
102
103
104 MSBE 10-2 10-1 100 101 102 103 104"
ILL-CONDITIONNESS OF THE LOSS,0.08006535947712418,|Q −Q ∗|2
ILL-CONDITIONNESS OF THE LOSS,0.08169934640522876,"Q-table
RG"
ILL-CONDITIONNESS OF THE LOSS,0.08333333333333333,"101
102
103
104
105
106"
ILL-CONDITIONNESS OF THE LOSS,0.08496732026143791,update steps 10-1 100 101 102 103 104 105
ILL-CONDITIONNESS OF THE LOSS,0.08660130718954248,MSBE of Q-table
ILL-CONDITIONNESS OF THE LOSS,0.08823529411764706,|Q −Q ∗|2 of Q-table
ILL-CONDITIONNESS OF THE LOSS,0.08986928104575163,MSBE of RG
ILL-CONDITIONNESS OF THE LOSS,0.0915032679738562,|Q −Q ∗|2 of RG
ILL-CONDITIONNESS OF THE LOSS,0.09313725490196079,"10-2
100
102
104 MSBE 10-2 100 102 104"
ILL-CONDITIONNESS OF THE LOSS,0.09477124183006536,|Q −Q ∗|2
ILL-CONDITIONNESS OF THE LOSS,0.09640522875816994,"Q-table
RG"
ILL-CONDITIONNESS OF THE LOSS,0.09803921568627451,"Figure 1: Left: the cliff walking task, where the agent is supposed to go to the lower right corner
as quickly as possible and avoid the grey region. The red arrows show the optimal policy. In this
example the system has the height of 4 and the width of 10. Right: results of learning the cliff
walking task in a tabular setting, using randomly sampled state-action pair data. The upper plots
show the result with width 10 and γ = 0.9, and the lower show the result with width 20 and
γ = 0.95. |Q −Q∗|2 is the squared distance between the learned Q function and the optimal Q∗.3
Both Q-table learning and RG use the learning rate of 0.5, averaged over 10 repeated runs."
ILL-CONDITIONNESS OF THE LOSS,0.09967320261437909,"Hessian matrix becomes
"
ILL-CONDITIONNESS OF THE LOSS,0.10130718954248366,"


 4
−2"
ILL-CONDITIONNESS OF THE LOSS,0.10294117647058823,"−2
4
...
...
...
−2
−2
4 "
ILL-CONDITIONNESS OF THE LOSS,0.10457516339869281,"


 N×N .
(7)"
ILL-CONDITIONNESS OF THE LOSS,0.10620915032679738,"The eigenvectors of this Hessian matrix that have the form of standing waves are given by
(sin
kπ
N+1, sin 2kπ"
ILL-CONDITIONNESS OF THE LOSS,0.10784313725490197,"N+1, ... sin Nkπ"
ILL-CONDITIONNESS OF THE LOSS,0.10947712418300654,"N+1)T for k ∈{1, 2, ...N}, and the corresponding eigenvalues are given"
ILL-CONDITIONNESS OF THE LOSS,0.1111111111111111,"by 4 −4 cos
kπ
N+1. Therefore, we have κ =
1+cos
π
N+1
1−cos
π
N+1 ∼O(N 2). See appendix for the details."
ILL-CONDITIONNESS OF THE LOSS,0.11274509803921569,"With γ < 1, if the states form a cycle, i.e., if sN−1 makes a transition to s0, the loss becomes
LMSBE =
1
N [PN−2
t=0 (Qt −rt −γQt+1)2 + (QN−1 −rN−1 −γQ0)2], where Q(st, at) is denoted
by Qt. Then, the Hessian matrix is cyclic and the eigenvectors have the form of periodic waves:
(sin 2kπ"
ILL-CONDITIONNESS OF THE LOSS,0.11437908496732026,"N , sin 4kπ"
ILL-CONDITIONNESS OF THE LOSS,0.11601307189542484,"N , ... sin 2Nkπ"
ILL-CONDITIONNESS OF THE LOSS,0.11764705882352941,"N
)T and (cos 2kπ"
ILL-CONDITIONNESS OF THE LOSS,0.119281045751634,"N , cos 4kπ"
ILL-CONDITIONNESS OF THE LOSS,0.12091503267973856,"N , ... cos 2Nkπ"
ILL-CONDITIONNESS OF THE LOSS,0.12254901960784313,"N
)T for k ∈{1, 2, ... N"
ILL-CONDITIONNESS OF THE LOSS,0.12418300653594772,"2 }, with
corresponding eigenvalues given by 2(1 + γ2) −4γ cos 2kπ"
ILL-CONDITIONNESS OF THE LOSS,0.12581699346405228,"N . At the limit of N →∞, we have"
ILL-CONDITIONNESS OF THE LOSS,0.12745098039215685,κ = (1+γ)2
ILL-CONDITIONNESS OF THE LOSS,0.12908496732026145,"(1−γ)2 . Using γ ≈1, we have κ ∼O

1
(1−γ)2

. By interpreting
1
1−γ as the effective time
horizon, or the effective size of the problem, we see that κ is quadratic in the size of the problem,
which is the same as its dependence on N in the case of γ = 1 above. In practice, κ is usually
104 ∼105, and we therefore conclude that the loss is ill-conditioned."
ILL-CONDITIONNESS OF THE LOSS,0.13071895424836602,"The ill-conditionedness has two important implications. First, as gradient descent converges at a
rate of O(κ),2 the required learning time is quadratic in the problem size, i.e. O(N 2), for the RG
algorithm. In contrast, Q-learning only requires O(N) steps as it straightforwardly propagates re-
ward information from si+1 to si at each iteration step. Therefore, the RG algorithm is signiﬁcantly
less computationally efﬁcient than Q-learning. Secondly, the ill-conditionedness implies that a small
LMSBE does not necessarily correspond to a small distance between the learned Q and the optimal
Q∗, which may explain why LMSBE is not a useful indicator of performance (Geist et al., 2017)."
ILL-CONDITIONNESS OF THE LOSS,0.1323529411764706,"Cliff walking
By way of illustration, we consider a tabular task, the cliff walking problem in
Sutton & Barto (2018), as illustrated in Fig. 1. The agent starts at the lower left corner in the
grid and can move into nearby blocks. If it enters a white block, it receives a reward −1; if it
enters a grey block which is the cliff, it receives a reward −100 and the process terminates; if it"
ALTHOUGH IN THE DETERMINISTIC CASE MOMENTUM CAN BE USED TO ACCELERATE GRADIENT DESCENT TO A CONVERGENCE,0.13398692810457516,"2Although in the deterministic case momentum can be used to accelerate gradient descent to a convergence
rate of O(√κ), it cannot straightforwardly be applied to stochastic gradient descent since it requires a large
momentum factor (
√κ−1
√κ+1)2 (Polyak, 1964) which results in unacceptably large noise.
3|Q −Q∗|2 is deﬁned by P"
ALTHOUGH IN THE DETERMINISTIC CASE MOMENTUM CAN BE USED TO ACCELERATE GRADIENT DESCENT TO A CONVERGENCE,0.13562091503267973,"(s,a) |Q(s, a) −Q∗(s, a)|2, where the sum is taken over all state-action pairs."
ALTHOUGH IN THE DETERMINISTIC CASE MOMENTUM CAN BE USED TO ACCELERATE GRADIENT DESCENT TO A CONVERGENCE,0.13725490196078433,Published as a conference paper at ICLR 2022
ALTHOUGH IN THE DETERMINISTIC CASE MOMENTUM CAN BE USED TO ACCELERATE GRADIENT DESCENT TO A CONVERGENCE,0.1388888888888889,"enters the goal, it terminates with a zero reward. To learn this task, we initialize Q for all state-
action pairs to be zero, and we randomly sample a state-action pair as (st, at) and ﬁnd the next
state st+1 to update Q via Eq. (3), which is the Q-table learning, or to minimize the associated loss
(Qθ(st, at) −rt −γ maxa′ Qθ(st+1, a′))2 following the gradient, which is RG learning. As shown
in Fig. 1, RG learns signiﬁcantly more slowly than Q-table learning, and as shown in the right plots,
given a ﬁxed value of LMSBE, RG’s distance to the optimal solution Q∗is also larger than that of
Q-table learning, showing that Q-table learning approaches Q∗more efﬁciently. To investigate the
dependence on the size of the problem, we consider a width of 10 of the system with γ = 0.9, and
its doubled size—a width of 20 with γ = 0.95. The results are presented in the upper and lower
plots in Fig. 1. Notice that the agent learns by random sampling from the state-action space, and
doubling the system size reduces the sampling efﬁciency by a factor of 2. As the learning time
of Q-learning is linear and RG is quadratic in the deterministic case, their learning time should
respectively become 4 times and 8 times for the double-sized system. We have conﬁrmed that the
experimental results approximately coincide with this prediction and therefore support our analysis
of the scaling property above."
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.14052287581699346,"3.2
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION"
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.14215686274509803,"Besides the issue of the ill-conditionedness, the update rule of RG learning still has a serious problem
which can lead to unsatisfactory learning behaviour. To show this, we ﬁrst denote Qt ≡Q(st, at)
and Qt+1 ≡maxa′ Q(st+1, a′), and given that they initially satisfy Qt = γQt+1, with an observed
transition from st to st+1 with a non-zero reward rt, repeatedly applying the Q-table learning rule
in Eq. (3) leads to ∆Qt = rt and ∆Qt+1 = 0, and thus ∆(Qt + Qt+1) = rt. On the other hand, in
the case of RG, minimizing LMSBE using the gradient results in the following learning rule"
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.1437908496732026,"∆Q(st, at) = α

rt + γ max
a′ Q(st+1, a′) −Q(st, at)

,
∆max
a′ Q(st+1, a′) = −γ∆Q(st, at), (8)"
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.1454248366013072,"and therefore whenever Qt is modiﬁed, Qt+1 changes simultaneously, and we have"
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.14705882352941177,"∆(Qt + Qt+1) = (1 −γ)rt.
(9)"
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.14869281045751634,"Due to the condition γ ≈1, ∆(Qt + Qt+1) can be very small, which is different from Q-learning,
since the sum of the predicted Q, i.e. P"
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.1503267973856209,"t Qt, almost does not change. This occurs because there is
an additional degree of freedom when one modiﬁes Qt and Qt+1 to satisfy the Bellman equation:
Q-learning tries to keep Qt+1 ﬁxed, while RG follows the gradient and keeps Qt + 1"
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.15196078431372548,"γ Qt+1 ﬁxed,
except for the case where st+1 is terminal and Qt+1 is a constant. This has important implications
on the learning behaviour of RG as heuristically explained below."
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.15359477124183007,"If the average of Q(s, a) is initialized above that of Q∗(s, a) and the transitions among the states
can form loops, the learning time of RG additionally scales as O(
1
1−γ ) due to Eq. (9), regardless of
the ﬁnite size of the problem. As shown in Fig. 2, in the cliff walking task with width 10, Q-table
learning has roughly the same learning time for different γ, while RG scales roughly as O(
1
1−γ ) and
does not learn for γ = 1. This is because the policy arg maxa′ Q(s, a′) of RG prefers non-terminal
states and goes into loops, since the transitions to terminal states are associated with Q values below
what the agent initially expects. Then, Eq. (9) controls the learning dynamics of the sum of all
Q values, i.e. P"
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.15522875816993464,"(s,a) Q(s, a), with the learning target being P"
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.1568627450980392,"(s,a) Q∗(s, a), and the learning time
scales as O(
1
1−γ ). For γ = 1, ∆P
(s,a) Q(s,a) is always zero and P
(s,a) Q∗(s, a) cannot be learned,
which results in failure of learning."
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.15849673202614378,"A more commonly encountered failure mode appears when Q is initialized below Q∗, in which case
the agent only learns to obtain a small amount of reward and faces difﬁculties in consistently improv-
ing its performance. A typical example is shown in Fig. 3, where Q is initialized to be zero and the
agent learns from its observed state transitions in an online manner following the ϵ-greedy policy.4
We see that while Q-table learning can solve the problem easily without the help of a non-zero ϵ, RG
cannot ﬁnd the optimal policy with ϵ = 0, and it learns slowly and relies on non-zero ϵ values. This
is because when RG ﬁnds rewards, Q(s, a) for some states increase while the other Q(s, a) values"
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.16013071895424835,"4ϵ-greedy means that for probability ϵ a random action is used; otherwise arg maxa′ Q(s, a′) is used."
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.16176470588235295,Published as a conference paper at ICLR 2022
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.16339869281045752,"102
103
104
105
106
107"
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.1650326797385621,update steps 10-1 100 101 102 103 104 105 MSBE
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.16666666666666666,Q-table γ=0.9
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.16830065359477125,Q-table γ=0.99
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.16993464052287582,Q-table γ=1
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.1715686274509804,RG γ=0.9
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.17320261437908496,RG γ=0.99
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.17483660130718953,RG γ=1
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.17647058823529413,"Figure 2:
Results of cliff
walking in Fig. 1 with dif-
ferent γ, with system’s width
of 10, averaged over 10 runs.
RG with γ = 1 does not
converge within reasonable
computational budgets. Goal 2.5 5.0 7.5 10.0 12.5"
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.1781045751633987,total reward
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.17973856209150327,Q-table ϵ = 0
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.18137254901960784,"RG ϵ = 0
RG ϵ = 0.01
RG ϵ = 0.1"
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.1830065359477124,"100
101
102
103
104
105"
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.184640522875817,number of steps 2.0 1.5 1.0 0.5
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.18627450980392157,"min Q(s, a)"
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.18790849673202614,"Figure 3: Left: one-way cliff walking task, where the agent starts at
the lower left corner, and at each step it is allowed to move to the
right to obtain a reward of 2, or move up and terminate with a reward
of −1. It terminates upon reaching the goal. Right: performance
of the learned greedy policy and min Q(s, a) for online Q-table
learning and RG, following the ϵ-greedy policy for different values
of ϵ, with γ = 1 and a learning rate of 0.5, averaged over 100 trials."
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.1895424836601307,"decrease and may become negative, and if the negative values become smaller than the reward asso-
ciated with the termination, i.e. −1, it will choose termination as its best action. Therefore, it relies
on the exploration strategy to correct its behaviour at those states with low Q(s, a) values and to ﬁnd
the optimal policy. Generally, when an unexpected positive reward rt is found in learning, accord-
ing to the learning rule in Eq. (8), with an increase of rt in Q(st, at), maxa′ Q(st+1, a′) decreases
simultaneously by rt, and the action at st+1, i.e. arg maxa′ Q(st+1, a′), is perturbed, which may
make the agent choose a worse action that leads to rt less reward, and therefore the performance
may not improve on the whole. In such cases, the performance of RG crucially relies on the explo-
ration strategy so that the appropriate action at st+1 can be rediscovered. In practice, especially for
large-scale problems, efﬁcient exploration is difﬁcult in general and one cannot enhance exploration
easily without compromising the performance, and therefore, RG often faces difﬁculties in learning
and performs worse than Q-learning for difﬁcult and realistic problems."
TENDENCY OF MAINTAINING THE AVERAGE PREDICTION,0.19117647058823528,"Remark
Although the above discussion is based on the tabular case, we believe that the situation
is not generally better when function approximations are involved. With the above issues in mind,
it can be understood why most of the currently successful examples of gradient-TD methods have
tunable hyperparameters that can reduce the methods to conventional TD learning, which has a Q-
learning-style update rule. If the methods get closer to conventional TD without divergence, they
typically achieve better efﬁciency and better quality of the learned policy. When the agent simply
learns from the gradient of the loss without using techniques like target networks, the performance
can be much worse. This probably explains why the performance of the PCL algorithm (Nachum
et al., 2017) sometimes deteriorates when the value and the policy neural networks are combined
into a uniﬁed Q network, and it has been reported that the performance of PCL can be improved by
using a target network (Gao et al., 2018). Note that although ill-conditionedness may be resolved
by a second-order optimizer or the Retrace loss (Munos et al., 2016; Badia et al., 2020), the issue in
Sec. 3.2 may not be resolved, because it will likely converge to the same solution as the one found
by gradient descent and thus have the same learning behaviour. A rigorous analysis of the issue in
Sec. 3.2 is deﬁnitely desired and is left for future work."
CONVERGENT DQN ALGORITHM,0.19281045751633988,"4
CONVERGENT DQN ALGORITHM"
INTERPRETING DQN AS FITTED VALUE ITERATION,0.19444444444444445,"4.1
INTERPRETING DQN AS FITTED VALUE ITERATION"
INTERPRETING DQN AS FITTED VALUE ITERATION,0.19607843137254902,"As we ﬁnd that Q-learning and the related conventional TD methods and DQN have learning dynam-
ics that is preferable to RG, we wish to minimally modify DQN so that it can maintain its learning
dynamics while being convergent. To proceed, we ﬁrst cast DQN into the form of ﬁtted value itera-
tion (FVI) (Ernst et al., 2005; Munos & Szepesvári, 2008). With initial parameters ˜θ0 of the target
network, the DQN loss for a transition (st, at, rt, st+1) and network parameters θ is deﬁned as"
INTERPRETING DQN AS FITTED VALUE ITERATION,0.1977124183006536,"ℓDQN(θ; ˜θi) :=

Qθ(st, at) −rt −γ max
a′ Q˜θi(st+1, a′)
2
,
(10)"
INTERPRETING DQN AS FITTED VALUE ITERATION,0.19934640522875818,Published as a conference paper at ICLR 2022
INTERPRETING DQN AS FITTED VALUE ITERATION,0.20098039215686275,and DQN learns by iterating over the target network
INTERPRETING DQN AS FITTED VALUE ITERATION,0.20261437908496732,"LDQN(θ; ˜θi) := E
h
lDQN(θ; ˜θi)
i
,
˜θi+1 = arg min
θ
LDQN(θ; ˜θi),
(11)"
INTERPRETING DQN AS FITTED VALUE ITERATION,0.2042483660130719,"and ˜θi is used as the parameter of the trained network for a sufﬁciently large i. In practice, the
minimum in Eq. (11) is found approximately by stochastic gradient descent, but for simplicity, here
we consider the case where the minimum is exact. When DQN diverges, the loss is supposed to
diverge with iterations, which means we have minθ LDQN(θ; ˜θi+1) > minθ LDQN(θ; ˜θi) for some i."
CONSTRUCTING A NON-INCREASING SERIES,0.20588235294117646,"4.2
CONSTRUCTING A NON-INCREASING SERIES"
CONSTRUCTING A NON-INCREASING SERIES,0.20751633986928106,Theorem 1. The minimum of LDQN(θ; ˜θi) with target network ˜θi is upper bounded by LMSBE(˜θi).
CONSTRUCTING A NON-INCREASING SERIES,0.20915032679738563,"This relation can be derived immediately from LDQN(˜θi; ˜θi) = LMSBE(˜θi) and minθ LDQN(θ; ˜θi) ≤
LDQN(˜θi; ˜θi), giving minθ LDQN(θ; ˜θi) ≤LMSBE(˜θi)."
CONSTRUCTING A NON-INCREASING SERIES,0.2107843137254902,"When DQN diverges, minθ LDQN(θ; ˜θi) diverges with increasing i, and therefore LMSBE(˜θi) must
also diverge. Therefore at each iteration, while the minimizer ˜θi+1 minimizes the i-th DQN loss
LDQN(θ; ˜θi), it can increase the upper bound of the (i+1)-th DQN loss, i.e. LMSBE(˜θi+1). We want
both LDQN and LMSBE to decrease in learning and we deﬁne the convergent DQN (C-DQN) loss as"
CONSTRUCTING A NON-INCREASING SERIES,0.21241830065359477,"LCDQN(θ; ˜θi) := E
h
max
n
ℓDQN(θ; ˜θi), ℓMSBE(θ)
oi
,
(12)"
CONSTRUCTING A NON-INCREASING SERIES,0.21405228758169934,"where ℓMSBE(θ) := (Qθ(st, at) −rt −γ maxa′ Qθ(st+1, a′))2 and LMSBE = E [ℓMSBE]."
CONSTRUCTING A NON-INCREASING SERIES,0.21568627450980393,"Theorem 2. The C-DQN loss satisﬁes minθ LCDQN(θ; ˜θi+1) ≤minθ LCDQN(θ; ˜θi), given ˜θi+1 =
arg minθ LCDQN(θ; ˜θi)."
CONSTRUCTING A NON-INCREASING SERIES,0.2173202614379085,We have
CONSTRUCTING A NON-INCREASING SERIES,0.21895424836601307,"min
θ
LCDQN(θ; ˜θi+1) ≤LCDQN(˜θi+1; ˜θi+1) = LMSBE(˜θi+1),
(13)"
CONSTRUCTING A NON-INCREASING SERIES,0.22058823529411764,"LMSBE(˜θi+1) ≤E
h
max
n
ℓDQN(˜θi+1; ˜θi), ℓMSBE(˜θi+1)
oi
= LCDQN(˜θi+1; ˜θi) ≤min
θ
LCDQN(θ; ˜θi). (14)"
CONSTRUCTING A NON-INCREASING SERIES,0.2222222222222222,"Therefore,
we
obtain
the
desired
non-increasing
condition
minθ LCDQN(θ; ˜θi+1)
≤
minθ LCDQN(θ; ˜θi), which means that the iteration ˜θ ←arg minθ LCDQN(θ; ˜θ) is convergent,
in the sense that the loss is bounded from below and non-increasing."
CONSTRUCTING A NON-INCREASING SERIES,0.2238562091503268,"C-DQN as deﬁned above is convergent for a given ﬁxed dataset. Although the analysis starts from
the assumption that ˜θi exactly minimizes the loss, in fact, it is not necessary. In practice, at the
moment when the target network ˜θ is updated by θ, the loss immediately becomes equal to LMSBE(θ)
which is bounded from above by the loss LCDQN(θ; ˜θ) before the target network update. Therefore,
as long as the loss is consistently optimized during the optimization process, the non-increasing
property of the loss holds throughout training. We ﬁnd that it sufﬁces to simply replace the loss used
in DQN by Eq. (12) to implement C-DQN. As we empirically ﬁnd that LMSBE in RG is always much
smaller than LDQN, we expect C-DQN to put more emphasis on the DQN loss and to have learning
behaviour similar to DQN. C-DQN can also be augmented by various extensions of DQN, such
as double Q-learning, distributional DQN and soft Q-learning (Van Hasselt et al., 2016; Bellemare
et al., 2017; Haarnoja et al., 2017), by modifying the losses ℓDQN and ℓMSBE accordingly. The mean
squared loss can also be replaced by the Huber loss (smooth ℓ1 loss) as commonly used in DQN
implementations. More discussions on the properties of C-DQN are provided in the appendix."
EXPERIMENTS,0.22549019607843138,"5
EXPERIMENTS"
EXPERIMENTS,0.22712418300653595,"5.1
COMPARISON OF C-DQN, DQN AND RG"
EXPERIMENTS,0.22875816993464052,"We focus on the Atari 2600 benchmark as in Mnih et al. (2015), and use the dueling network ar-
chitecture and prioritized sampling, with double Q-learning where applicable (Wang et al., 2016;"
EXPERIMENTS,0.23039215686274508,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.23202614379084968,"0.0
0.2
0.4
0.6
0.8
1.0
frames
×108 20 10 0 10 20"
EXPERIMENTS,0.23366013071895425,reward
EXPERIMENTS,0.23529411764705882,"C-DQN
DQN
RG"
EXPERIMENTS,0.2369281045751634,"106
107
108"
EXPERIMENTS,0.238562091503268,frames 10-4 10-3 10-2 loss
EXPERIMENTS,0.24019607843137256,"C-DQN
DQN
RG"
EXPERIMENTS,0.24183006535947713,"0.0
0.5
1.0
1.5
2.0
frames
×108 500 1000 1500 2000"
EXPERIMENTS,0.2434640522875817,reward
EXPERIMENTS,0.24509803921568626,"C-DQN
DQN
RG"
EXPERIMENTS,0.24673202614379086,"106
107
108"
EXPERIMENTS,0.24836601307189543,frames 10-3 10-2 10-1 loss
EXPERIMENTS,0.25,"C-DQN
DQN
RG"
EXPERIMENTS,0.25163398692810457,Figure 4: Training performance and training loss on games Pong (left) and Space Invaders (right).
EXPERIMENTS,0.25326797385620914,"0
2
4
6
frames
×107 0 250 500 750"
EXPERIMENTS,0.2549019607843137,reward
EXPERIMENTS,0.2565359477124183,"C-DQN
DQN 10-3 103 loss"
EXPERIMENTS,0.2581699346405229,"Figure 5:
Training perfor-
mance and loss on Space In-
vaders when half of the data
are randomly discarded."
EXPERIMENTS,0.25980392156862747,"0
2
4
6
frames
×107 0 200 400 600 800"
EXPERIMENTS,0.26143790849673204,reward
EXPERIMENTS,0.2630718954248366,DQN random 10-3 101 105 loss
EXPERIMENTS,0.2647058823529412,"0.0
0.2
0.4
0.6
0.8
1.0
frames
×108 200 400 600 800 1000"
EXPERIMENTS,0.26633986928104575,reward
EXPERIMENTS,0.2679738562091503,"C-DQN random
DQN random
DQN FIFO"
EXPERIMENTS,0.2696078431372549,"0.2
0.4
0.6
0.8
1.0
frames
×108 0.025 0.050 0.075 0.100 0.125 loss"
EXPERIMENTS,0.27124183006535946,"Figure 6: Training performance and training loss on Space In-
vaders when the memory adopts a random replacement strategy
(left) and when the memory is smaller and adopts different strate-
gies (middle and right)."
EXPERIMENTS,0.272875816993464,"Schaul et al., 2015; Van Hasselt et al., 2016). We refer to the combination of the original DQN and
these techniques as DQN, and similarly for C-DQN and RG. Details of experimental settings are
given in the appendix and our codes are available in the supplementary material."
EXPERIMENTS,0.27450980392156865,"As C-DQN, DQN and RG only differ in their loss functions, we follow the hyperparameter settings
in Hessel et al. (2018) for all the three algorithms and compare them on two well-known games,
Pong and Space Invaders, and the learning curves for performance and loss are shown in Fig. 4. We
see that both C-DQN and DQN can learn the tasks, while RG almost does not learn, despite that
RG has a much smaller loss. This coincides with our prediction in Sec. 3, which explains why there
are very few examples of successful applications of RG to realistic problems. The results show that
C-DQN as a convergent method indeed performs well in practice and has performance comparable
to DQN for standard tasks. Results for a few other games are given in the appendix."
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.2761437908496732,"5.2
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE"
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.2777777777777778,"To give an example in which DQN is prone to diverge, we consider learning from incomplete tra-
jectories of experience, i.e. given a transition (st, at, st+1) in the dataset, the subsequent transition
(st+1, at+1, st+2) may be absent from the dataset. This makes DQN prone to diverge because while
DQN learns Qθ(st, at) based on maxa′ Qθ(st+1, a′), there is a possibility that maxa′ Qθ(st+1, a′)
has to be inferred and cannot be directly learned from any data. To create such a setting, we ran-
domly discard half of the transition data collected by the agent and keep the other experimental
settings unchanged, except that the mean squared error is used instead of the Huber loss to allow
for divergence in gradient. We ﬁnd that whether or not DQN diverges or not is task-dependent in
general, with a larger probability to diverge for more difﬁcult tasks, and the result for Space Invaders
is shown in Fig. 5. We see that while DQN diverges, C-DQN learns stably and the speed of its learn-
ing is only slightly reduced. This conﬁrms that C-DQN is convergent regardless of the structure of
the learned data, and implies that C-DQN may be potentially more suitable for ofﬂine learning and
learning from observation when compared with DQN."
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.27941176470588236,"A similar situation arises when one does not use the ﬁrst-in-ﬁrst-out (FIFO) strategy to replace old
data in the replay memory (i.e. the dataset) with new data when the dataset is full, but replaces
old data randomly with new data. In Fig. 6, we show that conventional DQN can actually diverge
in this simple setting. In the existing DQN literature, this replacement strategy is often ignored,
while here it can been seen to be an important detail that affects the results, and in practice, FIFO
is almost always used. However, FIFO makes the memory data less diverse and less informative,
and it increases the possibility of the oscillation of the co-evolvement of the policy and the replay
memory, and as a result, a large size of the replay memory is often necessary for learning. In Fig. 6,
we show that when the size of the replay memory is reduced by a factor of 10, C-DQN can beneﬁt
from utilizing the random replacement strategy while DQN cannot, and C-DQN can reach a higher
performance. Note that DQN does not diverge in this case, probably because the replay memory is"
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.28104575163398693,Published as a conference paper at ICLR 2022
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.2826797385620915,"0.0
0.5
1.0
1.5
2.0
frames
×108 30000 20000 10000 0"
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.28431372549019607,reward
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.28594771241830064,Skiing (γ ≈0.9998)
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.2875816993464052,"C-DQN
DQN"
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.28921568627450983,"0.0
0.5
1.0
1.5
2.0
×108 20 0 20"
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.2908496732026144,Tennis (γ ≈0.9982)
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.29248366013071897,"0.0
0.5
1.0
1.5
2.0
×108 0 5000 10000 15000"
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.29411764705882354,PrivateEye (γ ≈0.9998)
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.2957516339869281,"0.0
0.5
1.0
1.5
2.0
×108 0 250 500 750"
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.2973856209150327,Venture (γ = 0.9998)
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.29901960784313725,"Figure 7: Training performance on several difﬁcult games in Atari 2600, with learning rate 4×10−5.
Each line represents a single run and the shaded regions show the standard deviation. The discount
factors are shown in the titles and all DQN agents have signiﬁcant instabilities or divergence in loss."
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.3006535947712418,"less off-policy when it is small, which alleviates divergence. The result opens up a new possibility
of RL of only storing and learning important data to improve efﬁciency, which cannot be realized
stably with DQN but is possible with C-DQN."
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.3022875816993464,"5.3
DIFFICULT GAMES IN ATARI 2600"
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.30392156862745096,"In this section we consider difﬁcult games in Atari 2600. While DQN often becomes unstable
when the discount factor γ gets increasingly close to 1, in principle, C-DQN can work with any γ.
However, we ﬁnd that a large γ does not always result in better performance in practice, because
a large γ requires the agent to learn to predict rewards that are far in the future, which are often
irrelevant for learning the task. We also notice that when γ is larger than 0.9999, the order of
magnitude of (1 −γ)Qθ gets close to the intrinsic noise caused by the ﬁnite learning rate and
learning can stagnate. Therefore, we require γ to satisfy 0.99 ≤γ ≤0.9998, and use a simple
heuristic algorithm to evaluate how frequent reward signals appear so as to determine γ for each
task, which is discussed in detail in the appendix. We also take this opportunity to evaluate the mean
µ of Q and the scale σ of the reward signal using sampled trajectories, and make our agent learn
the normalized value Q−µ"
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.3055555555555556,"σ
instead of the original Q. We do not clip the reward and follow Pohlen
et al. (2018) to make the neural network learn a transformed function which squashes the Q function
approximately by the square root."
LEARNING FROM INCOMPLETE TRAJECTORIES OF EXPERIENCE,0.30718954248366015,"With C-DQN and large γ values, several difﬁcult tasks which previously could not be solved by
simple DQN variants can now be solved, as shown in Fig. 7. We ﬁnd that especially for Skiing,
Private Eye and Venture, the agent signiﬁcantly beneﬁts from large γ and achieves a higher best
performance in training, even though Private Eye and Venture are partially observable tasks and not
fully learnable, which leads to unstable training performance. Evaluation of the test performance
and details of the settings are given in the appendix. Notably, we ﬁnd that C-DQN achieves the
state-of-the-art test performance on Skiing despite the simplicity of the algorithm."
CONCLUSION AND FUTURE PERSPECTIVES,0.3088235294117647,"6
CONCLUSION AND FUTURE PERSPECTIVES"
CONCLUSION AND FUTURE PERSPECTIVES,0.3104575163398693,"We have discussed the inefﬁciency issues regarding RG and gradient-TD methods, and addressed the
long-standing problem of convergence in Q-learning by proposing a convergent DQN algorithm, and
we have demonstrated the effectiveness of C-DQN on the Atari 2600 benchmark. With the stability
of C-DQN, we can now consider the possibility of tuning γ freely without sacriﬁcing stability, and
consider the possibility of learning only important state transitions to improve efﬁciency. C-DQN
can be applied to difﬁcult tasks for which DQN suffers from instability. It may also be combined
with other strategies that involve target networks and potentially improve their stability."
CONCLUSION AND FUTURE PERSPECTIVES,0.31209150326797386,"There are many outstanding issues concerning C-DQN. The loss used in C-DQN is non-smooth,
and it is not clear how this affects the optimization and the learning dynamics. In our experiments
this does not appear to be a problem, but deserves further investigation. When the transitions are
stochastic, the loss LMSBE used in C-DQN does not converge exactly to the solution of the Bellman
equation, and therefore it would be desirable if C-DQN can be improved so that stochastic transitions
can be learned without bias. It would be interesting to investigate how it interplays with DQN
extensions such as distributional DQN and soft Q-learning, and it is not clear whether the target
network in C-DQN can be updated smoothly as in Lillicrap et al. (2015)."
CONCLUSION AND FUTURE PERSPECTIVES,0.3137254901960784,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.315359477124183,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.31699346405228757,"All our experimental results can be reproduced exactly by our codes provided in the supplementary
material, where the scripts and commands are organised according to the section numbers. We also
present and discuss our implementation details in the appendix so that one can reproduce our results
without referring to the codes."
REFERENCES,0.31862745098039214,REFERENCES
REFERENCES,0.3202614379084967,"Joshua Achiam, Ethan Knight, and Pieter Abbeel. Towards characterizing divergence in deep Q-
learning. arXiv preprint arXiv:1903.08894, 2019."
REFERENCES,0.32189542483660133,"András Antos, Csaba Szepesvári, and Rémi Munos. Learning near-optimal policies with Bellman-
residual minimization based ﬁtted policy iteration and a single sample path. Machine Learning,
71(1):89–129, 2008."
REFERENCES,0.3235294117647059,"Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi,
Zhaohan Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark.
In International Conference on Machine Learning, pp. 507–517. PMLR, 2020."
REFERENCES,0.32516339869281047,"Leemon Baird.
Residual algorithms: Reinforcement learning with function approximation.
In
Machine Learning Proceedings 1995, pp. 30–37. Elsevier, 1995."
REFERENCES,0.32679738562091504,"Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:
253–279, 2013."
REFERENCES,0.3284313725490196,"Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement
learning. In International Conference on Machine Learning, pp. 449–458. PMLR, 2017."
REFERENCES,0.3300653594771242,"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D˛ebiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large
scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019."
REFERENCES,0.33169934640522875,"Shalabh Bhatnagar, Doina Precup, David Silver, Richard S Sutton, Hamid Maei, and Csaba
Szepesvári. Convergent temporal-difference learning with arbitrary smooth function approxi-
mation. Advances in neural information processing systems, 22:1204–1212, 2009."
REFERENCES,0.3333333333333333,"Aditya Bhatt, Max Argus, Artemij Amiranashvili, and Thomas Brox. Crossnorm: Normalization
for off-policy TD reinforcement learning. arXiv preprint arXiv:1902.05605, 2019."
REFERENCES,0.3349673202614379,"Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Sbeed:
Convergent reinforcement learning with nonlinear function approximation. In International Con-
ference on Machine Learning, pp. 1125–1134. PMLR, 2018."
REFERENCES,0.3366013071895425,"Ishan Durugkar and Peter Stone. TD learning with constrained gradients. In Proceedings of the
Deep Reinforcement Learning Symposium, NIPS 2017, Long Beach, CA, USA, December 2017.
URL http://www.cs.utexas.edu/users/ai-lab?NIPS17-ishand."
REFERENCES,0.3382352941176471,"Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. First return, then
explore. Nature, 590(7847):580–586, 2021."
REFERENCES,0.33986928104575165,"Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6:503–556, 2005."
REFERENCES,0.3415032679738562,"Yihao Feng, Lihong Li, and Qiang Liu. A kernel loss for solving the Bellman equation. arXiv
preprint arXiv:1905.10506, 2019."
REFERENCES,0.3431372549019608,"Thomas Fösel, Petru Tighineanu, Talitha Weiss, and Florian Marquardt. Reinforcement learning
with neural networks for quantum feedback. Physical Review X, 8(3):031084, 2018."
REFERENCES,0.34477124183006536,"Scott Fujimoto, David Meger, and Doina Precup. An equivalence between loss functions and non-
uniform sampling in experience replay. arXiv preprint arXiv:2007.06049, 2020."
REFERENCES,0.3464052287581699,Published as a conference paper at ICLR 2022
REFERENCES,0.3480392156862745,"Yang Gao, Huazhe Xu, Ji Lin, Fisher Yu, Sergey Levine, and Trevor Darrell. Reinforcement learning
from imperfect demonstrations. arXiv preprint arXiv:1802.05313, 2018."
REFERENCES,0.34967320261437906,"Matthieu Geist, Bilal Piot, and Olivier Pietquin. Is the Bellman residual a bad proxy? In Advances
in Neural Information Processing Systems, pp. 3205–3214, 2017."
REFERENCES,0.35130718954248363,"Sina Ghiassian, Andrew Patterson, Shivam Garg, Dhawal Gupta, Adam White, and Martha White.
Gradient temporal-difference learning with regularized corrections. In International Conference
on Machine Learning, pp. 3524–3534. PMLR, 2020."
REFERENCES,0.35294117647058826,"Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017."
REFERENCES,0.3545751633986928,"Alexander Hans and Steffen Udluft. Efﬁcient uncertainty propagation for reinforcement learning
with limited data. In International Conference on Artiﬁcial Neural Networks, pp. 70–79. Springer,
2009."
REFERENCES,0.3562091503267974,"Alexander Hans and Steffen Udluft. Ensemble usage for more reliable policy identiﬁcation in rein-
forcement learning. In ESANN, 2011."
REFERENCES,0.35784313725490197,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international
conference on computer vision, pp. 1026–1034, 2015."
REFERENCES,0.35947712418300654,"Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018."
REFERENCES,0.3611111111111111,"Tobias Johannink, Shikhar Bahl, Ashvin Nair, Jianlan Luo, Avinash Kumar, Matthias Loskyll,
Juan Aparicio Ojea, Eugen Solowjow, and Sergey Levine. Residual reinforcement learning for
robot control. In 2019 International Conference on Robotics and Automation (ICRA), pp. 6023–
6029, 2019. doi: 10.1109/ICRA.2019.8794127."
REFERENCES,0.3627450980392157,"Seungchan Kim, Kavosh Asadi, Michael Littman, and George Konidaris. Deepmellow: removing
the need for a target network in deep Q-learning. In Proceedings of the Twenty Eighth Interna-
tional Joint Conference on Artiﬁcial Intelligence, 2019."
REFERENCES,0.36437908496732024,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.3660130718954248,"Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre Quillen. Learning hand-
eye coordination for robotic grasping with deep learning and large-scale data collection. The
International Journal of Robotics Research, 37(4-5):421–436, 2018."
REFERENCES,0.36764705882352944,"Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015."
REFERENCES,0.369281045751634,"Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and
Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open
problems for general agents. Journal of Artiﬁcial Intelligence Research, 61:523–562, 2018."
REFERENCES,0.3709150326797386,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529–533, 2015."
REFERENCES,0.37254901960784315,"Rémi Munos and Csaba Szepesvári. Finite-time bounds for ﬁtted value iteration. Journal of Ma-
chine Learning Research, 9(27):815–857, 2008.
URL http://jmlr.org/papers/v9/
munos08a.html."
REFERENCES,0.3741830065359477,"Rémi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G Bellemare. Safe and efﬁcient off-
policy reinforcement learning. arXiv preprint arXiv:1606.02647, 2016."
REFERENCES,0.3758169934640523,Published as a conference paper at ICLR 2022
REFERENCES,0.37745098039215685,"Oﬁr Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between
value and policy based reinforcement learning. In Advances in Neural Information Processing
Systems, pp. 2775–2785, 2017."
REFERENCES,0.3790849673202614,"Shota Ohnishi, Eiji Uchibe, Yotaro Yamaguchi, Kosuke Nakanishi, Yuji Yasui, and Shin Ishii. Con-
strained deep Q-learning gradually approaching ordinary Q-learning. Frontiers in neurorobotics,
13:103, 2019."
REFERENCES,0.380718954248366,"Tobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan, David Budden,
Gabriel Barth-Maron, Hado Van Hasselt, John Quan, Mel Veˇcerík, et al. Observe and look further:
Achieving consistent performance on atari. arXiv preprint arXiv:1805.11593, 2018."
REFERENCES,0.38235294117647056,"Boris T Polyak. Some methods of speeding up the convergence of iteration methods. Ussr compu-
tational mathematics and mathematical physics, 4(5):1–17, 1964."
REFERENCES,0.3839869281045752,"Martin Riedmiller. Neural ﬁtted q iteration–ﬁrst experiences with a data efﬁcient neural reinforce-
ment learning method. In European Conference on Machine Learning, pp. 317–328. Springer,
2005."
REFERENCES,0.38562091503267976,"Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv
preprint arXiv:1511.05952, 2015."
REFERENCES,0.3872549019607843,"Ralf Schoknecht and Artur Merke. TD (0) converges provably faster than the residual gradient
algorithm. In Proceedings of the 20th International Conference on Machine Learning (ICML-
03), pp. 680–687, 2003."
REFERENCES,0.3888888888888889,"Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018."
REFERENCES,0.39052287581699346,"Richard S Sutton, Csaba Szepesvári, and Hamid Reza Maei. A convergent O(N) temporal-difference
algorithm for off-policy learning with linear function approximation. In NIPS, 2008."
REFERENCES,0.39215686274509803,"Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba
Szepesvári, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning
with linear function approximation. In Proceedings of the 26th Annual International Conference
on Machine Learning, pp. 993–1000, 2009."
REFERENCES,0.3937908496732026,"Ahmed Touati, Pierre-Luc Bacon, Doina Precup, and Pascal Vincent. Convergent tree backup and re-
trace with function approximation. In International Conference on Machine Learning, pp. 4955–
4964. PMLR, 2018."
REFERENCES,0.3954248366013072,"Volker Tresp. The wet game of chicken. Siemens AG, CT IC 4, Technical Report, 1994."
REFERENCES,0.39705882352941174,"John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function
approximation. IEEE transactions on automatic control, 42(5):674–690, 1997."
REFERENCES,0.39869281045751637,"Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Q-
learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 30, 2016."
REFERENCES,0.40032679738562094,"Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Mo-
dayil. Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648, 2018."
REFERENCES,0.4019607843137255,"Zhikang T Wang, Yuto Ashida, and Masahito Ueda. Deep reinforcement learning control of quantum
cartpoles. Physical Review Letters, 125(10):100401, 2020."
REFERENCES,0.4035947712418301,"Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling
network architectures for deep reinforcement learning. In International conference on machine
learning, pp. 1995–2003. PMLR, 2016."
REFERENCES,0.40522875816993464,Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. 1989.
REFERENCES,0.4068627450980392,"Shangtong Zhang, Wendelin Boehmer, and Shimon Whiteson. Deep residual reinforcement learn-
ing. In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent
Systems, AAMAS ’20, pp. 1611–1619, Richland, SC, 2020. International Foundation for Au-
tonomous Agents and Multiagent Systems. ISBN 9781450375184."
REFERENCES,0.4084967320261438,"Shangtong Zhang, Hengshuai Yao, and Shimon Whiteson. Breaking the deadly triad with a target
network. arXiv preprint arXiv:2101.08862, 2021."
REFERENCES,0.41013071895424835,Published as a conference paper at ICLR 2022
REFERENCES,0.4117647058823529,"A
CONVERGENCE OF C-DQN IN STOCHASTIC SETTINGS"
REFERENCES,0.4133986928104575,"When the transition of states is stochastic, it is well-known that the minimum of LMSBE does not
exactly correspond to the solution of the Bellman equation, because we have"
REFERENCES,0.4150326797385621,LMSBE(θ) = Est+1
REFERENCES,0.4166666666666667,"
Qθ(st, at) −rt −γ max
a′ Qθ(st+1, a′)
2"
REFERENCES,0.41830065359477125,"=

Qθ(st, at) −rt −γEst+1
h
max
a′ Qθ(st+1, a′)
i2
+ γ2Varst+1(max
a′ Qθ(st+1, a′)), (15)"
REFERENCES,0.4199346405228758,"where Var(·) represents the variance, and it can be seen that only the ﬁrst term on the last line corre-
sponds to the solution of the Bellman equation. Because C-DQN involves LMSBE, if the underlying
task is stochastic, C-DQN may not converge to the optimal solution due to the bias in LMSBE. In
fact, both the minimum of LMSBE and the solution of the Bellman equation are stationary points for
C-DQN. To show this, we ﬁrst assume that the minimum of LDQN and that of LMSBE are unique
and different. If parameters θ and ˜θi satisfy θ = ˜θi = arg min LDQN(·; ˜θi), i.e., if they are at the
converging point of DQN, then we consider an inﬁnitesimal change δθ of θ following the gradient
of LMSBE in an attempt to reduce LMSBE. Because we have LMSBE(θ) = LDQN(θ; θ) = LDQN(θ; ˜θi),
as θ + δθ moves away from the minimum LDQN(θ; ˜θi), we have"
REFERENCES,0.4215686274509804,"LDQN(θ + δθ; ˜θi) > LDQN(θ; ˜θi) = LMSBE(θ) > LMSBE(θ + δθ),
(16)"
REFERENCES,0.42320261437908496,"and therefore for θ + δθ, LDQN is larger than LMSBE, and C-DQN will choose to optimize LDQN
instead of LMSBE and the parameter will return to the minimum θ, which is the converging point
of DQN. On the other hand, given θ = ˜θi = arg min LMSBE(·), if we change θ by an inﬁnitesimal
amount δθ in an attempt to reduce LDQN(θ; ˜θi), we similarly have"
REFERENCES,0.42483660130718953,"LMSBE(θ + δθ) > LMSBE(θ) = LDQN(θ; ˜θi) > LDQN(θ + δθ; ˜θi),
(17)"
REFERENCES,0.4264705882352941,"and therefore, for the same reason C-DQN will choose to optimize LMSBE and the parameter will
return to θ. Therefore, C-DQN can converge to both the converging points of DQN and RG. More
generally, C-DQN may converge somewhere between the converging points of DQN and RG. It
tries to minimize both the loss functions simultaneously, and it stops if this goal cannot be achieved,
i.e., if a decrease of one loss increases the other. Interestingly, this does not seem to be a severe
problem as demonstrated by the successful application of C-DQN to the Atari 2600 benchmark
(see Sec. 5), because the tasks include a large amount of noise-like behaviour and subtitles such as
partially observable states."
REFERENCES,0.42810457516339867,"A.1
A CASE STUDY: THE WET-CHICKEN BENCHMARK"
REFERENCES,0.4297385620915033,"To investigate the behaviour of C-DQN more closely, we consider a stochastic toy prob-
lem, which is known as the wet-chicken benchmark (Tresp, 1994; Hans & Udluft, 2009;
2011).
In the problem, a canoeist paddles on a river starting at position x
=
0, and
there is a waterfall at position x
=
l
=
20,
and the goal of the canoeist is to
get as close as possible to the waterfall without reaching it.
The canoeist can choose
to paddle back, hold the position, or drift forward, which corresponds to a change of
-1, 0, or +1 in his/her position x, and there is random turbulence z ∼Uniform(−2.5, +2.5), a
uniformly distributed random number, that also contributes to the change in x and stochastically
perturbs x at each step. The reward is equal to the position x, and x is reset back to 0 if he/she
reaches the waterfall at x = 20. The task does not involve the end of an episode, and the per-
formance is evaluated as the average reward per step. This task is known to be highly stochastic,
because the effect of the stochastic perturbation is often stronger than the effect of the action of the
agent, and the stochasticity can lead to states that have dramatically different Q function values."
REFERENCES,0.43137254901960786,"To learn this task, we generate a dataset of 20000 transitions using random actions, and we train a
neural network on this dataset using the DQN, the C-DQN and the RG algorithms to learn the Q
values. The results are shown in Fig. 8. In the left panel of Fig. 8, it can be seen that while RG
signiﬁcantly underperforms DQN, the performance of C-DQN lies between DQN and RG and is
only slightly worse than DQN. This shows that when the task is highly stochastic, although C-DQN"
REFERENCES,0.43300653594771243,Published as a conference paper at ICLR 2022
REFERENCES,0.434640522875817,"0
500
1000
1500
2000
epoch 7 8 9 10 11 12 13 14 15"
REFERENCES,0.4362745098039216,average reward
REFERENCES,0.43790849673202614,"C-DQN
DQN (NFQ)
RG"
REFERENCES,0.4395424836601307,"0
500
1000
1500
2000
epoch 0 25 50 75 100 125 150 175"
REFERENCES,0.4411764705882353,distance
REFERENCES,0.44281045751633985,|QCDQN −QDQN|
REFERENCES,0.4444444444444444,|QCDQN −QRG|
REFERENCES,0.44607843137254904,|QDQN −QRG|
REFERENCES,0.4477124183006536,"|QCDQN −QDQN| + |QCDQN −QRG|
−|QDQN −QRG|"
REFERENCES,0.4493464052287582,"Figure 8: Performance on the wet-chicken benchmark training on a dataset generated by the random
policy (left) and the distances among the learned Q functions (right). The experiment is repeated for
10 times, and the standard error of the performance and the standard deviation of the distances are
shown as the shaded regions."
REFERENCES,0.45098039215686275,"may not reach the optimal solution as DQN can do, C-DQN still behaves robustly and produces
reasonably satisfactory results, while RG fails dramatically."
REFERENCES,0.4526143790849673,"To obtain further details of the learned Q functions, we estimate the distance between the
Q functions.
The distance |Q1 −Q2| between two Q functions Q1 and Q2 is estimated as
qP"
REFERENCES,0.4542483660130719,"(x,a)(Q1(x, a) −Q2(x, a))2, where the summation on the position x is taken over the dis-"
REFERENCES,0.45588235294117646,"crete set {0, 1, 2, ...19}. In the right panel of Fig. 8, we show the estimated distances among the
learned Q functions of DQN, C-DQN and RG. We see that the distance between QDQN and QCDQN
increases rapidly in the beginning and then slowly decreases, implying that DQN learns quickly at
the beginning, and C-DQN catches up later and reduces its distance to DQN. Notably, we ﬁnd that
the value |QCDQN −QDQN| + |QCDQN −QRG| −|QDQN −QRG| always converges to zero, indicating
that the solution found by C-DQN, i.e. QCDQN, lies exactly on the line from QDQN to QRG, which is
consistent with our argument that C-DQN converges somewhere between the converging points of
DQN and RG."
REFERENCES,0.45751633986928103,"Concerning the experimental details, the neural network includes 4 hidden layers, each of which has
128 hidden units and uses the ReLU as the activation function, and the network is optimized using
the Adam optimizer (Kingma & Ba, 2014) with default hyperparameters. We use the batch size of
200, and the target network is updated after each epoch, which makes the DQN algorithm essentially
the same as the neural ﬁtted Q (NFQ) iteration algorithm (Riedmiller, 2005). The training includes
2000 epochs, and the learning rate is reduced by a factor of 10 and 100 at the 1000th and the 1500th
epochs. The discount factor γ is set to be 0.97. The position x and the reward are normalized by
20 before training, and the evaluation of the performance is done every 5 epochs, using 300 time
steps and repeated for 200 trials. The entire experiment is repeated for 10 times including the data
generation process."
REFERENCES,0.4591503267973856,"B
ADDITIONAL EXPERIMENTAL RESULTS"
REFERENCES,0.46078431372549017,"In this section we present additional experimental results. In Sec. B.1, we present the results of
applying C-DQN to the problem of measurement feedback cooling of quantum quartic oscillators
in Wang et al. (2020), where we show that the ﬁnal performances are more consistent regarding
different random seeds and have a smaller variance compared with the results of DQN. Concerning
the Atari 2600 benchmark, in Sec. B.2, results for several other games are presented. In Sec. B.3
we show that C-DQN allows for more ﬂexible update periods of the target network. In Sec. B.4 we
report the test performance of C-DQN on the difﬁcult Atari 2600 games shown in Sec. 5.3, and in
Sec. B.5 we discuss the results of C-DQN on the game Skiing."
REFERENCES,0.4624183006535948,Published as a conference paper at ICLR 2022
REFERENCES,0.46405228758169936,"B.1
RESULTS ON MEASUREMENT FEEDBACK COOLING OF A QUANTUM QUARTIC
OSCILLATOR"
REFERENCES,0.46568627450980393,"To show the stability of C-DQN compared with DQN for problems with practical signiﬁcance, we
reproduce the results in Wang et al. (2020), which trains a RL controller to do measurement feed-
back cooling of a one-dimensional quantum quartic oscillator in numerical simulation. Details of
the problem setting are given in Wang et al. (2020) and in our supplementary codes. The training
performances for C-DQN and DQN are shown in Fig. 9, where each curve represents a different
experiment with a different random seed. As shown in Fig. 9, different C-DQN experiments have
similar learning curves and ﬁnal performances; however, in sharp contrast, those of the DQN exper-
iment have apparently different ﬂuctuating learning curves and the performances are unstable, and
some of the repetitions cannot reach a ﬁnal performance that is comparable to the best-performing
ones. The results show that compared with DQN, the outcome of the training procedure of C-DQN
is highly stable and reproducible, which can greatly beneﬁt practical applications."
REFERENCES,0.4673202614379085,"0
200000
400000
simulation time 0.0 2.5 5.0 7.5 10.0 12.5"
REFERENCES,0.46895424836601307,energy
REFERENCES,0.47058823529411764,Convergent DQN
REFERENCES,0.4722222222222222,"0
200000 400000 600000 800000"
REFERENCES,0.4738562091503268,simulation time 0.0 2.5 5.0 7.5 10.0 12.5
REFERENCES,0.47549019607843135,energy DQN
REFERENCES,0.477124183006536,"Figure 9: Training performance of C-DQN and DQN on the task of measurement feedback cooling
of quartic oscillators. The vertical axis shows the energy of the cooled quartic oscillator, and a
smaller energy represents better performance. The horizontal axis shows the simulated time of
the oscillator system that is used to train the agent. Each curve represents a separate trial of the
experiment."
REFERENCES,0.47875816993464054,"B.2
EXPERIMENTAL RESULTS ON OTHER GAMES"
REFERENCES,0.4803921568627451,"In addition to the results in Sec. 5.1, we present results on 6 other Atari 2600 games comparing
C-DQN and DQN, which are shown in Fig. 10."
REFERENCES,0.4820261437908497,"In general, we ﬁnd that the loss value of C-DQN is almost always smaller than DQN, and for
relatively simple tasks, C-DQN has performance comparable to DQN, but for more difﬁcult tasks,
they show different performances with relatively large variance. Speciﬁcally, we ﬁnd that C-DQN
has better performance for tasks that require more precise learning and control such as Atlantis, and
for tasks that are unstable and irregular such as Video Pinball. However, for tasks that are highly
stochastic and partially observable such as Fishing Derby and Time Pilot, C-DQN may perform less
well compared with DQN, probably because the term LMSBE in LCDQN does not properly account for
stochastic transitions."
REFERENCES,0.48366013071895425,"B.3
MORE FLEXIBLE UPDATE PERIODS FOR THE TARGET NETWORK"
REFERENCES,0.4852941176470588,"As C-DQN has a better convergence property, it allows for shorter update periods of the target
network. Speciﬁcally, convergence of C-DQN is obtained as long as the loss decreases during the
optimization process after an update of the target network. One period of the update of the target
network consists of N˜θ iterations of gradient descent on θ minimizing LCDQN(θ; ˜θi) or LDQN(θ; ˜θi)
with the target network ˜θi, and then using θ as the next target network ˜θi+1, where N˜θ represents
the update period of the target network. In previous works on DQN, N˜θ is set to be 2000 or 2500
(Hessel et al., 2018; Mnih et al., 2015), and DQN may experience instability for a too small N˜θ.
However, we empirically ﬁnd that for many tasks in Atari 2600, N˜θ can be reduced to 200 or even
20 without instability for C-DQN. Therefore, we ﬁnd that C-DQN requires less ﬁne tuning on the"
REFERENCES,0.4869281045751634,Published as a conference paper at ICLR 2022
REFERENCES,0.48856209150326796,"0.0
0.5
1.0
1.5
2.0
frames
×108 0"
REFERENCES,0.49019607843137253,200000
REFERENCES,0.4918300653594771,400000
REFERENCES,0.4934640522875817,600000
REFERENCES,0.4950980392156863,reward
REFERENCES,0.49673202614379086,"C-DQN
DQN"
REFERENCES,0.49836601307189543,"0.5
1.0
1.5
2.0
frames
×108 10-2 loss"
REFERENCES,0.5,Atlantis
REFERENCES,0.5016339869281046,"0.0
0.5
1.0
1.5
2.0
frames
×108 0 100 200 300 400"
REFERENCES,0.5032679738562091,reward
REFERENCES,0.5049019607843137,"0.5
1.0
1.5
2.0
frames
×108 10-2 10-1 loss"
REFERENCES,0.5065359477124183,Breakout
REFERENCES,0.5081699346405228,"0.0
0.5
1.0
1.5
2.0
frames
×108 100 75 50 25 0 25"
REFERENCES,0.5098039215686274,reward
REFERENCES,0.511437908496732,"0.5
1.0
1.5
2.0
frames
×108 10-1 loss"
REFERENCES,0.5130718954248366,Fishing Derby
REFERENCES,0.5147058823529411,"0.0
0.5
1.0
1.5
2.0
frames
×108 0 10 20 30"
REFERENCES,0.5163398692810458,reward
REFERENCES,0.5179738562091504,"0.5
1.0
1.5
2.0
frames
×108 10-5 10-4 10-3 10-2 loss"
REFERENCES,0.5196078431372549,Freeway
REFERENCES,0.5212418300653595,"0.0
0.5
1.0
1.5
2.0
frames
×108 2000 4000 6000 8000 10000"
REFERENCES,0.5228758169934641,reward
REFERENCES,0.5245098039215687,"0.5
1.0
1.5
2.0
frames
×108 10-2 loss"
REFERENCES,0.5261437908496732,Time Pilot
REFERENCES,0.5277777777777778,"0.0
0.5
1.0
1.5
2.0
frames
×108 0"
REFERENCES,0.5294117647058824,100000
REFERENCES,0.5310457516339869,200000
REFERENCES,0.5326797385620915,300000
REFERENCES,0.5343137254901961,400000
REFERENCES,0.5359477124183006,500000
REFERENCES,0.5375816993464052,reward
REFERENCES,0.5392156862745098,"0.5
1.0
1.5
2.0
frames
×108 10-1 100 loss"
REFERENCES,0.5408496732026143,Video Pinball
REFERENCES,0.5424836601307189,"Figure 10: Training performance and training loss of C-DQN and DQN on several other Atari 2600
games, using the same experimental settings as in Sec. 5.1."
REFERENCES,0.5441176470588235,"hyperparameter N˜θ compared with DQN. The experimental results on Space Invaders and Hero are
shown in Fig. 11, using the experimental settings in Sec. 5.1. We see that C-DQN has a generally
higher performance compared to DQN when N˜θ becomes small, and the performance of DQN is
sometimes unstable and is sensitive to the value of N˜θ."
REFERENCES,0.545751633986928,"0.0
0.2
0.4
0.6
0.8
1.0
frames
×108 0 250 500 750 1000 1250"
REFERENCES,0.5473856209150327,reward
REFERENCES,0.5490196078431373,C-DQN N˜θ = 200
REFERENCES,0.5506535947712419,"N˜θ = 20
DQN N˜θ = 200"
REFERENCES,0.5522875816993464,N˜θ = 20
REFERENCES,0.553921568627451,"0.2
0.4
0.6
0.8
1.0
frames
×108 10-1 loss"
REFERENCES,0.5555555555555556,"0.0
0.2
0.4
0.6
0.8
1.0
frames
×108 0 5000 10000 15000 20000 25000"
REFERENCES,0.5571895424836601,reward
REFERENCES,0.5588235294117647,C-DQN N˜θ = 200
REFERENCES,0.5604575163398693,"N˜θ = 20
DQN N˜θ = 200"
REFERENCES,0.5620915032679739,N˜θ = 20
REFERENCES,0.5637254901960784,"0.2
0.4
0.6
0.8
1.0
frames
×108 100 102 104 106 108 loss"
REFERENCES,0.565359477124183,"Figure 11: Training performance using different update periods of the target network on games
Space Invaders (left) and Hero (right). In the game Hero, there appears to be a local optimum with
reward 13000 where the learning can fail to make progress, which is also seen in Fig. 15."
REFERENCES,0.5669934640522876,"B.4
TEST PERFORMANCE ON DIFFICULT ATARI GAMES"
REFERENCES,0.5686274509803921,"In this section we report the test performance of the agents in Sec. 5.3 and compare with existing
works. As the learned policy of the agents has large ﬂuctuations in performance due to noise,
local optima, and insufﬁcient learning when the task is difﬁcult, instead of using the agent at the
end of training, we use the best-performing agent during training to evaluate the test performance.
Speciﬁcally, we save a checkpoint of the parameter θ of the agent every 104 steps, and we choose the
three best-performing agents by comparing their training performances, computed as the average of
the 40 nearby episodes around each of the checkpoints. After the training, we carry out a separate
validation process using 400 episodes to ﬁnd the best-performing one among the three agents, and
then, we evaluate the test performance of the validated best agent by another 400 episodes, using a
different random seed. The policy during evaluation is the ϵ-greedy policy with ϵ = 0.01„ with no-"
REFERENCES,0.5702614379084967,Published as a conference paper at ICLR 2022
REFERENCES,0.5718954248366013,"Table 1: Test performance on difﬁcult Atari 2600 games, corresponding to the results in Sec. 5.3
in the main text, evaluated using no-op starts and without sticky actions (Machado et al., 2018).
The DQN results are produced using the same experimental settings as the C-DQN experiments
except for the loss function. Human results and results for Agent57 are due to Badia et al. (2020),
and results for Rainbow DQN are due to Hessel et al. (2018). The human results only represent
the performance of an average person, not a human expert, and the human results correspond to
reasonably adequate performance instead of the highest possible performance of human."
REFERENCES,0.5735294117647058,"Task
C-DQN
DQN
Human
Rainbow DQN
Agent57 (SOTA)"
REFERENCES,0.5751633986928104,"Skiing
-3697 ± 157
-29751 ± 224
-4337
-12958
-4203 ± 608
Tennis
10.9 ± 6.3
-2.6 ± 1.4
-8.3
0.0
23.8 ± 0.1
Private Eye
14730 ± 37
7948 ± 749
69571
4234
79716 ± 29545
Venture
893 ± 51
386 ± 85
1188
5.5
2624 ± 442"
REFERENCES,0.576797385620915,"Figure 12: A screenshot of the game
Skiing in Atari 2600."
REFERENCES,0.5784313725490197,"0.0
0.5
1.0
1.5
2.0
frames
×108 30000 25000 20000 15000 10000 5000 0"
REFERENCES,0.5800653594771242,reward
REFERENCES,0.5816993464052288,Skiing (γ ≈0.9998) C-DQN
REFERENCES,0.5833333333333334,"Figure 13: Training performance of C-DQN on Skiing
with learning rate 2×10−5, following the experimental
procedure in Sec. 5.3. The standard deviation among
the three runs are shown as the shaded region."
REFERENCES,0.5849673202614379,"op starts5 (Mnih et al., 2015). The average of the test performances of the 3 runs in our experiments
are shown in Table 1 together with the standard error, compared with existing works and the human
performance."
REFERENCES,0.5866013071895425,"As we have basically followed the conventional way of training DQN on Atari 2600 as in Mnih et al.
(2015) and Hessel et al. (2018), our C-DQN and the Rainbow DQN in Hessel et al. (2018) allow for
a fair comparison because they use the same amount of computational budget and a similar neural
network architecture.6 In Table 1, we see that in these four difﬁcult Atari 2600 games Rainbow
DQN fails to make progress in learning, and C-DQN can achieve performances higher than Rainbow
DQN and show non-trivial learning behaviour. The results of Agent57 is for reference only, which
represents the currently known best performance on Atari 2600 in general and does not allow for
a fair comparison with C-DQN, as it involves considerably more computation, more sophisticated
methods and larger neural networks. We ﬁnd that our result on the game Skiing is exceptional, which
is discussed in the next section."
REFERENCES,0.5882352941176471,"B.5
THE ATARI GAME Skiing"
REFERENCES,0.5898692810457516,"In Table 1, one exceptional result is that C-DQN achieves a performance higher than Agent57 on
the game Skiing, actually, using an amount of computation that is less than 0.1% of that of Agent57.
We ﬁnd that this performance is higher than any other known result so far and thus achieves the
state-of-the-art (SOTA) for this speciﬁc game. To elucidate the underlying reason, we describe this
game ﬁrst."
REFERENCES,0.5915032679738562,"5No-op starts mean that at the start of each episode the no-operation action is executed randomly for 1 to
30 frames.
6In fact, a fair comparison with Rainbow DQN can be made except for the case of Skiing, because reward
clipping adopted by Rainbow DQN does not permit the learning of Skiing. Nevertheless, this does not affect
our conclusion."
REFERENCES,0.5931372549019608,Published as a conference paper at ICLR 2022
REFERENCES,0.5947712418300654,"A screenshot of the game Skiing is shown in Fig. 12. This game is similar to a racing game. The
player is required to go downhill and reach the goal as fast as possible, and the time elapsed before
reaching the goal is the minus reward. At each time step, the player receives a small minus reward
which represents the accumulated time, until the goal is reached and the game ends. In addition,
the player is required to pass through the gates along his/her way, which are represented by the two
small ﬂags shown in Fig. 12, and whenever the player fails to pass a gate, a 5-second penalty is
added to the elapsed time when the player reaches the ﬁnal goal. The number of gates that have
not been passed are shown at the top of the game screen. Using the standard setting in Mnih et al.
(2015), the number of state transitions for an episode of this game is ∼1300 for the random policy,
∼4500 when the player slows down signiﬁcantly, and ∼500 when the policy is near-optimal."
REFERENCES,0.5964052287581699,"Since the penalty for not passing a gate is given only at the end of the game, the agent needs to
relate the penalty at the end of the game to the events that happen early in the game, and therefore
the discount factor γ should be at least around 1 −
1
500 to make learning effective. However, the
learning may still stagnate if γ is not larger, because when γ is small, the agent prefers taking a
longer time before reaching the goal, so that the penalty at the end is delayed and the Q function for
the states in the early game is increased, which will increase the episode length and make a larger
γ necessary. Therefore, we have chosen to tune our hyperparameter setting so that γ ≈1 −
1
5000 is
obtained on this game (see Sec. E.3), and we ﬁnd that our C-DQN agent successfully learns with the
γ and produces a new record on this game.7 The large ﬂuctuations in its training performance shown
in Sec. 5.3 are mostly due to the noise coming from the ﬁnite learning rate, which can be conﬁrmed
by repeating the experiments with a smaller learning rate, shown in Fig. 13. However, in this case
the learning easily gets trapped in local optima and the ﬁnal test performance is worse. Note that in
fact, we cannot fairly compare our result with Agent57, because we have tuned our hyperparameters
so that the obtained γ is in favour of this game, while Agent57 uses a more general bandit algorithm
to adaptively determine γ."
REFERENCES,0.5980392156862745,"C
RELATED WORKS"
REFERENCES,0.5996732026143791,"In this section, we present some related works for further references. The slow convergence of RG
compared with TD methods has been shown in Schoknecht & Merke (2003). The O(N 2) scaling
property can also be derived by considering speciﬁc examples of Markov chains, such as the “Hall”
problem as pointed out in Baird (1995). The convergence property of RG-like algorithms has been
analysed in Antos et al. (2008), assuming that the underlying Markov process is β-mixing, i.e. it
converges to a stable distribution exponentially fast. However, this assumption is often impractical,
which may have underlain the discrepancy between the theoretical results of RG and the experi-
mental effectiveness. There is an improved version of RG proposed in Zhang et al. (2020), and RG
has been applied to robotics in Johannink et al. (2019). Concerning DQN, there have been many
attempts to stabilize the learning, to remove the target network, and to use a larger γ. Pohlen et al.
(2018) introduces a temporal consistency loss to reduce the difference between Qθ and Q˜θ on st+1,
and the authors showed that the resulting algorithm can learn with γ = 0.999. A variant of it is
proposed in Ohnishi et al. (2019). Kim et al. (2019) and Bhatt et al. (2019) propose extensions for
DQN and show that the resulting DQN variants can sometimes operate without a target network
when properly tuned. Achiam et al. (2019) gives an analysis of the divergence and proposes to use
a method similar to natural gradient descent to stabilize Q-learning; however, it is computationally
heavy as it uses second-order information, and therefore it cannot be used efﬁciently with large neu-
ral networks. Recently, the strategy of using target networks in DQN has been shown to be useful
for TD learning as well by Zhang et al. (2021). Some other works have been discussed in the main
text and we do not repeat them here."
REFERENCES,0.6013071895424836,"D
CALCULATION DETAILS OF THE CONDITION NUMBER"
REFERENCES,0.6029411764705882,In this section we provide the calculation details of Sec. 3.1. Given the loss function L = 1 N
REFERENCES,0.6045751633986928,"""N−2
X"
REFERENCES,0.6062091503267973,"t=0
(Qt −rt −Qt+1)2 + (QN−1 −rN−1)2
#"
REFERENCES,0.6078431372549019,",
(18)"
REFERENCES,0.6094771241830066,"7The single highest performance we observed was around −3350, and the optimal performance in this game
is reported to be −3272 in Badia et al. (2020)."
REFERENCES,0.6111111111111112,Published as a conference paper at ICLR 2022
REFERENCES,0.6127450980392157,"where Q(st, at) is denoted by Qt, we add an additional term (Q0)2 to it, and the loss function
becomes L = 1"
REFERENCES,0.6143790849673203,"N [Q2
0 + N−2
X"
REFERENCES,0.6160130718954249,"t=0
(Qt −rt −Qt+1)2 + (QN−1 −rN−1)2] = 1"
REFERENCES,0.6176470588235294,"N [Q2
0 + N−2
X t=0"
REFERENCES,0.619281045751634," 
Q2
t −2rtQt + 2rtQt+1 −2QtQt+1 + r2
t + Q2
t+1
"
REFERENCES,0.6209150326797386,"+
 
Q2
N−1 −2rN−1QN−1 + 2r2
N−1

]. (19)"
REFERENCES,0.6225490196078431,"To calculate the condition number of the Hessian matrix, we ignore the prefactor 1"
REFERENCES,0.6241830065359477,"N and only evalu-
ate the second-order derivatives. From Eq. (19), it is straightforward to obtain"
REFERENCES,0.6258169934640523,"∂2L
∂Q2
t
= 4,
t ∈{0, 1, ...N −1}"
REFERENCES,0.6274509803921569,"∂2L
∂Qt∂Qt+1
= −2,
t ∈{0, 1, ...N −2}
(20)"
REFERENCES,0.6290849673202614,"and therefore the Hessian matrix is
"
REFERENCES,0.630718954248366,"


 4
−2"
REFERENCES,0.6323529411764706,"−2
4
...
...
...
−2
−2
4 "
REFERENCES,0.6339869281045751,"


 N×N"
REFERENCES,0.6356209150326797,".
(21)"
REFERENCES,0.6372549019607843,"The eigenvectors of this matrix can be explicitly obtained due to the special structure of the matrix,
which are given by (sin
kπ
N+1, sin 2kπ"
REFERENCES,0.6388888888888888,"N+1, ... sin Nkπ"
REFERENCES,0.6405228758169934,"N+1)T for k ∈{1, 2, ...N}, and the corresponding
eigenvalues are given by 4 −4 cos
kπ
N+1. To show this, we ﬁrst simplify the Hessian matrix by
removing its constant diagonal 4, which only contributes to a constant 4 in the eigenvalues, and then
we multiply the eigenvectors by the eigenvalues

sin
kπ
N + 1, sin 2kπ"
REFERENCES,0.6421568627450981,"N + 1, ... sin Nkπ N + 1"
REFERENCES,0.6437908496732027,"T
· (−4 cos
kπ
N + 1) =
(22)"
REFERENCES,0.6454248366013072,"−2

(sin 0 · kπ"
REFERENCES,0.6470588235294118,N + 1 + sin 2kπ
REFERENCES,0.6486928104575164,"N + 1), (sin
kπ
N + 1 + sin 3kπ"
REFERENCES,0.6503267973856209,"N + 1), ...(sin (N −1)kπ"
REFERENCES,0.6519607843137255,"N + 1
+ sin (N + 1)kπ"
REFERENCES,0.6535947712418301,"N + 1
)
T (23)"
REFERENCES,0.6552287581699346,due to sin nkπ
REFERENCES,0.6568627450980392,"N+1 cos
kπ
N+1
=
1
2

sin (n−1)kπ"
REFERENCES,0.6584967320261438,"N+1
+ sin (n+1)kπ"
REFERENCES,0.6601307189542484,"N+1

.
As we also have sin 0·kπ N+1
="
REFERENCES,0.6617647058823529,sin (N+1)kπ
REFERENCES,0.6633986928104575,"N+1
) = 0, the product of the vector and the eigenvalue is exactly equal to the product
of the vector and the Hessian matrix. As the vectors (sin
kπ
N+1, sin 2kπ"
REFERENCES,0.6650326797385621,"N+1, ... sin Nkπ"
REFERENCES,0.6666666666666666,"N+1)T are linearly
independent for k ∈{1, 2, ...N}, they are all the eigenvectors of the Hessian matrix, and therefore"
REFERENCES,0.6683006535947712,"the eigenvalues are 4−4 cos
kπ
N+1. The condition number is then κ =
4−4 cos
Nπ
N+1
4−4 cos
π
N+1 . Using the Taylor"
REFERENCES,0.6699346405228758,"series expansion, the denominator in the expression of κ is 4 −4 cos
π
N+1 =
2π2
(N+1)2 + O( 1"
REFERENCES,0.6715686274509803,"N4 ), and
therefore for large N, κ ≈
4−4 cos π
4−4 cos
π
N+1 ∼O(N 2)."
REFERENCES,0.673202614379085,"When the state sN−1 makes a transition to s0, with the discount factor γ, the loss is given by L = 1 N"
REFERENCES,0.6748366013071896,"""N−2
X"
REFERENCES,0.6764705882352942,"t=0
(Qt −rt −γQt+1)2 + (QN−1 −rN−1 −γQ0)2
#"
REFERENCES,0.6781045751633987,".
(24)"
REFERENCES,0.6797385620915033,"Using the same calculation, the non-zero second-order derivatives are given by"
REFERENCES,0.6813725490196079,"∂2L
∂Q2
t
= 2 + 2γ2,
t ∈{0, 1, ...N −1}"
REFERENCES,0.6830065359477124,"∂2L
∂Qt∂Qt+1
= −2γ,
t ∈{0, 1, ...N −1},
QN = Q0,
(25)"
REFERENCES,0.684640522875817,Published as a conference paper at ICLR 2022
REFERENCES,0.6862745098039216,"and the Hessian matrix is cyclic.
Assuming that N is even, the eigenvectors are given by
(sin 2kπ"
REFERENCES,0.6879084967320261,"N , sin 4kπ"
REFERENCES,0.6895424836601307,"N , ... sin 2Nkπ"
REFERENCES,0.6911764705882353,"N
)T and (cos 2kπ"
REFERENCES,0.6928104575163399,"N , cos 4kπ"
REFERENCES,0.6944444444444444,"N , ... cos 2Nkπ"
REFERENCES,0.696078431372549,"N
)T for k ∈{1, 2, ... N"
REFERENCES,0.6977124183006536,"2 }, with
eigenvalues given by 2(1 + γ2) −4γ cos 2kπ"
REFERENCES,0.6993464052287581,"N . The result can be similarly conﬁrmed by notic-
ing the relation sin 2nkπ"
REFERENCES,0.7009803921568627,"N
· (−4γ cos 2kπ"
REFERENCES,0.7026143790849673,N ) = −2γ(sin 2(n−1)kπ
REFERENCES,0.704248366013072,"N
+ sin 2(n+1)kπ"
REFERENCES,0.7058823529411765,"N
) and the period-
icity sin 2(N+1)kπ"
REFERENCES,0.7075163398692811,"N
= sin 2kπ"
REFERENCES,0.7091503267973857,"N , and similarly cos 2nkπ"
REFERENCES,0.7107843137254902,"N
· (−4γ cos 2kπ"
REFERENCES,0.7124183006535948,N ) = −2γ(cos 2(n−1)kπ
REFERENCES,0.7140522875816994,"N
+
cos 2(n+1)kπ"
REFERENCES,0.7156862745098039,"N
) and cos 2(N+1)kπ"
REFERENCES,0.7173202614379085,"N
= cos 2kπ"
REFERENCES,0.7189542483660131,"N , which proves that they are indeed the eigenvectors"
REFERENCES,0.7205882352941176,"and eigenvalues. At the limit of N →∞, we have κ =
2(1+γ2)−4γ cos π
2(1+γ2)−4γ cos 2π"
REFERENCES,0.7222222222222222,N ≈2(1+γ2)+4γ
REFERENCES,0.7238562091503268,2(1+γ2)−4γ = (1+γ)2
REFERENCES,0.7254901960784313,(1−γ)2 .
REFERENCES,0.7271241830065359,"Using γ ≈1, we obtain κ ∼O

1
(1−γ)2

."
REFERENCES,0.7287581699346405,"E
EXPERIMENTAL DETAILS ON ATARI 2600"
REFERENCES,0.7303921568627451,"E.1
GENERAL SETTINGS"
REFERENCES,0.7320261437908496,"We follow Mnih et al. (2015) to preprocess the frames of the games by taking the maximum of
the recent two frames and changing them to the grey scale. However, instead of downscaling them
to 84×84 images, we downscale exactly by a factor of 2, which results in 80×105 images as in
Ecoffet et al. (2021). This is to preserve the sharpness of the objects in the images and to preserve
translational invariance of the objects. For each step of the agent, the agent stacks the frames seen in
the recent 4 steps as the current observation, i.e. state st, and decides an action at and executes the
action repeatedly for 4 frames in the game and accumulates the rewards during the 4 frames as rt.
Thus, the number of frames is 4 times the number of steps of the agent. One iteration of the gradient
descent is performed for every 4 steps of the agent, and the agent executes the random policy for
50000 steps to collect some initial data before starting the gradient descent. The replay memory
stores 1 million transition data, using the ﬁrst-in-ﬁrst-out strategy unless otherwise speciﬁed. The
neural network architecture is the same as the one in Wang et al. (2016), except that we use additional
zero padding of 2 pixels at the edges of the input at the ﬁrst convolutional layer, so that all pixels in
the 80×105 images are connected to the output. Following to Hessel et al. (2018), we set the update
period of the target network to be 8000 steps, i.e. 2000 gradient descent iterations, using the Adam
optimizer (Kingma & Ba, 2014) with a mini-batch size of 32 and default β1, β2 hyperparameters,
and we make the agent regard the loss of one life in the game as the end of an episode. The discount
factor γ is set to be 0.99 unless otherwise speciﬁed, and the reward clipping to [−1, 1] is applied
except in Sec. 5.3 and E.3."
REFERENCES,0.7336601307189542,"Gradient of LCDQN upon updating the target network
Although we use gradient descent to
minimize LCDQN, when we update the target network by copying from θ to ˜θ, ℓDQN(θ; ˜θi) is exactly
equal to ℓMSBE(θ) and the gradient of LCDQN is undeﬁned. In this case, we ﬁnd that one may simply
use the gradient computed from ℓDQN without any problem, and one may rely on the later gradient
descent iterations to reduce the loss LCDQN. In our experiments, we further bypass this issue by
using the parameters θ at the previous gradient descent step instead of the current step to update ˜θ,
so that θ does not become exactly equal to ˜θ. This strategy is valid because the consecutive gradient
descent steps are supposed to give parameters that minimize the loss almost equally well, and the
parameters should have similar values. In our implementation of DQN, we also update the target
network in this manner to have a fair comparison with C-DQN."
REFERENCES,0.7352941176470589,"Loss
As mentioned in Sec. 4.2, either the mean squared error or the Huber loss can be used to
compute the loss functions ℓDQN and ℓMSBE. In Sec. 5.2 we use one half of the mean squared error,
and the loss functions are given by"
REFERENCES,0.7369281045751634,ℓDQN(θ; ˜θ) = 1 2
REFERENCES,0.738562091503268,"
Qθ(st, at) −rt −γ max
a′ Q˜θ(st+1, a′)
2
,"
REFERENCES,0.7401960784313726,ℓMSBE(θ) = 1 2
REFERENCES,0.7418300653594772,"
Qθ(st, at) −rt −γ max
a′ Qθ(st+1, a′)
2
.
(26)"
REFERENCES,0.7434640522875817,Published as a conference paper at ICLR 2022
REFERENCES,0.7450980392156863,"In Sec. 5.1 we use the Huber loss, and the loss functions are given by"
REFERENCES,0.7467320261437909,"ℓDQN(θ; ˜θ) = ℓHuber

Qθ(st, at), rt + γ max
a′ Q˜θ(st+1, a′)

,"
REFERENCES,0.7483660130718954,"ℓMSBE(θ) = ℓHuber

Qθ(st, at), rt + γ max
a′ Qθ(st+1, a′)

,"
REFERENCES,0.75,"ℓHuber(x, y) ="
REFERENCES,0.7516339869281046,"(
1
2 (x −y)2 ,
if |x −y| < 1,
|x −y| −1"
REFERENCES,0.7532679738562091,"2,
if |x −y| ≥1. (27)"
REFERENCES,0.7549019607843137,"In Sec. 5.3, we let the agents learn a normalized Q function ˆQ ≡Q−µ"
REFERENCES,0.7565359477124183,"σ , and we use the strategy in
Pohlen et al. (2018) to squash the Q function approximately by the square root before learning. The
relevant transformation function T is deﬁned by"
REFERENCES,0.7581699346405228,"T ( ˆQ) := sign( ˆQ)
q"
REFERENCES,0.7598039215686274,"| ˆQ| + 1 −1

+ ϵT ˆQ,
(28)"
REFERENCES,0.761437908496732,and its inverse is given by
REFERENCES,0.7630718954248366,T −1(f) = sign(f)   p
REFERENCES,0.7647058823529411,1 + 4ϵT (|f| + 1 + ϵT ) −1 2ϵT !2 −1 
REFERENCES,0.7663398692810458,".
(29)"
REFERENCES,0.7679738562091504,"In our experiments we set ϵT = 0.01 as in Pohlen et al. (2018), and the loss functions are"
REFERENCES,0.7696078431372549,"ℓDQN(θ; ˜θ) = ℓHuber

fθ(st, at), T

ˆrt + γT −1 
max
a′ f˜θ(st+1, a′)

,"
REFERENCES,0.7712418300653595,"ℓMSBE(θ) = ℓHuber

fθ(st, at), T

ˆrt + γT −1 
max
a′ fθ(st+1, a′)

,
(30)"
REFERENCES,0.7728758169934641,"where fθ is the neural network, and T −1(fθ(st, at)) represents the learned normalized Q function
ˆQθ(st, at), and ˆrt is the reward that is modiﬁed together with the normalization of the Q function,
which is discussed in Sec. E.3."
REFERENCES,0.7745098039215687,"When we plot the ﬁgures, for consistency, we always report the mean squared errors as the loss
functions, which are given by"
REFERENCES,0.7761437908496732,"ℓDQN(θ; ˜θ) =

Qθ(st, at) −rt −γ max
a′ Q˜θ(st+1, a′)
2
,"
REFERENCES,0.7777777777777778,"ℓMSBE(θ) =

Qθ(st, at) −rt −γ max
a′ Qθ(st+1, a′)
2
,
(31) or,"
REFERENCES,0.7794117647058824,"ℓDQN(θ; ˜θ) =

fθ(st, at) −T

ˆrt + γT −1 
max
a′ f˜θ(st+1, a′)
2
,"
REFERENCES,0.7810457516339869,"ℓMSBE(θ) =

fθ(st, at) −T

ˆrt + γT −1 
max
a′ fθ(st+1, a′)
2
.
(32)"
REFERENCES,0.7826797385620915,"When we use double Q-learning (Van Hasselt et al., 2016) in our experiments,
all
maxa′ Q˜θ(st+1, a′) and maxa′ f˜θ(st+1, a′) terms in the above equations are actually replaced by
Q˜θ (st+1, arg maxa′ Qθ(st+1, a′)) and f˜θ (st+1, arg maxa′ fθ(st+1, a′)). This modiﬁcation do not
change the non-increasing property of LCDQN, which can be conﬁrmed easily."
REFERENCES,0.7843137254901961,"Other details
We ﬁnd that the exploration can often be insufﬁcient when the game is difﬁcult, and
we follow Van Hasselt et al. (2016) and use a slightly more complicated schedule for the ϵ parameter
in the ϵ-greedy policy, slowly annealing ϵ from 0.1 to 0.01. We have a total computation budget of
5 × 107 steps for each agent, and at the j-th step of the agent, for j ≤50000, we set ϵ = 1 since
the initial policy is random; for 50000 > j ≥106, ϵ is exponentially annealed to 0.1 following
ϵ = ej/τ, with τ = 106/ ln(0.1); for 106 > j ≥4 × 107, ϵ is linearly decreased from 0.1 to 0.01;
for j > 4 × 107 we set ϵ = 0.01. This strategy allows ϵ to stay above 0.01 for a fairly long time
and facilitates exploration to mitigate the effects of local optima. We use this ϵ schedule in all of our
experiments."
REFERENCES,0.7859477124183006,Published as a conference paper at ICLR 2022
REFERENCES,0.7875816993464052,"We set the learning rate to be 6.25 × 10−5 following Hessel et al. (2018) unless otherwise speciﬁed.
We use gradient clipping in the gradient descent iterations, using the maximal ℓ2 norm of 10 in
Sec. 5.1 and 5.2, and 5 in Sec. 5.3. The ϵa hyperparameter for the Adam optimizer follows Hessel
et al. (2018) and is set to be 1.5 × 10−4 in Sec. 5.2, but in Sec. 5.1 it is set to be 1.5 × 10−4 for
DQN, and 5 × 10−5 for C-DQN and 5 × 10−6 for RG, becaue we observe that the sizes of the
gradients are different for DQN, C-DQN and RG. ϵa is set to be 10−6 in Sec. 5.3 and E.3. The
weight parameters in the neural networks are initialized following He et al. (2015), and the bias
parameters are initialized to be zero."
REFERENCES,0.7892156862745098,"E.2
PRIORITIZED SAMPLING"
REFERENCES,0.7908496732026143,"In our experiments we have slightly modiﬁed the original prioritized sampling scheme proposed by
Schaul et al. (2015). In the original proposal, in gradient descent optimization, a transition data
di = (st, at, rt, st+1) ∈S is sampled with priority pi, which is set to be"
REFERENCES,0.7924836601307189,"pi = (|δi| + ϵp)α ,
(33)"
REFERENCES,0.7941176470588235,"where ϵp is a small positive number, α is a sampling hyperparameter, and |δi| is the evaluated
Bellman error when di was sampled last time in gradient descent, which is"
REFERENCES,0.795751633986928,"|δi(DQN)| =
Qθ(st, at) −rt −γ max
a′ Q˜θ(st+1, a′)

(34)"
REFERENCES,0.7973856209150327,for DQN and
REFERENCES,0.7990196078431373,"|δi(RG)| =
Qθ(st, at) −rt −γ max
a′ Qθ(st+1, a′)
 ,
(35)"
REFERENCES,0.8006535947712419,"|δi(C-DQN)| = max

|δi(DQN)|, |δi(RG)|
	
,
(36)"
REFERENCES,0.8022875816993464,"as we have chosen for RG and C-DQN, respectively. The probability for di to be sampled is Pi =
pi
P"
REFERENCES,0.803921568627451,"j pj
. To correct the bias that results from prioritized sampling, an importance sampling weight"
REFERENCES,0.8055555555555556,"wi is multiplied to the loss computed on di, which is given by"
REFERENCES,0.8071895424836601,"wi =
P"
REFERENCES,0.8088235294117647,"j pj
N
· 1 pi"
REFERENCES,0.8104575163398693,"β
,
(37)"
REFERENCES,0.8120915032679739,"where N is the total number of data and β is an importance sampling hyperparameter. The bias
caused by prioritized sampling is fully corrected when β is set to be 1."
REFERENCES,0.8137254901960784,"Schaul et al. (2015) propose using ˜wi :=
wi
maxj wj
instead of wi, so that the importance sampling"
REFERENCES,0.815359477124183,"weight only reduces the size of the gradient. However, we ﬁnd that this strategy would make the
learning highly dependent on the hyperparameter ϵp in Eq. (33), because given a data with vanish-
ingly small |δi|, its corresponding priority is pi ≈ϵα
p , and therefore the term maxj wj becomes"
REFERENCES,0.8169934640522876,"maxj wj ≈
 P"
REFERENCES,0.8186274509803921,"j pj
N
· ϵ−α
p
β
∝ϵ−αβ
p
. As a result, the gradient in learning is scaled by the term
maxj wj which is controlled by α, β and ϵp, and maxj wj changes throughout training and typi-
cally increases when the average of |δi| becomes large. For a given ϵa hyperparameter in the Adam
optimizer, the overall decrease of the gradient caused by maxj wj is equivalent to an increase of
ϵa, which effectively anneals the size of the update steps of the gradient descent. This makes ϵp an
important learning hyperparameter, as also noted by Fujimoto et al. (2020), although this hyperpa-
rameter has been ignored in most of the relevant works including the original proposal. The results
on Space Invaders for different values of ϵp are plotted in Fig. 14, which use the experimental set-
tings in Sec. 5.1. It can be seen that the performance is strongly dependent on ϵp. This issue may
partially explain the difﬁculties one usually encounters when trying to reproduce published results."
REFERENCES,0.8202614379084967,"Lower bounded prioritization
To remove this subtlety, we use the original importance sampling
weight wi instead of ˜wi. As |δi| is heavily task-dependent, to remove the dependence of pi on ϵp
for all the tasks, we make use of the average ¯p := P"
REFERENCES,0.8218954248366013,"j pj
N
to bound pi from below instead of simply
using ϵp so as to prevents pi from vanishing. Speciﬁcally, we set pi to be"
REFERENCES,0.8235294117647058,"pi = max

(|δi| + ϵp)α , ¯p ˜cp"
REFERENCES,0.8251633986928104,"
,
(38)"
REFERENCES,0.826797385620915,Published as a conference paper at ICLR 2022
REFERENCES,0.8284313725490197,"0.0
0.5
1.0
1.5
frames
×108 0 1000 2000 3000"
REFERENCES,0.8300653594771242,reward
REFERENCES,0.8316993464052288,DQN ϵp = 10−8
REFERENCES,0.8333333333333334,ϵp = 10−6
REFERENCES,0.8349673202614379,ϵp = 10−4
REFERENCES,0.8366013071895425,"0.0
0.5
1.0
1.5
frames
×108 10-5 10-4 10-3 10-2 loss"
REFERENCES,0.8382352941176471,DQN ϵp = 10−8
REFERENCES,0.8398692810457516,ϵp = 10−6
REFERENCES,0.8415032679738562,ϵp = 10−4
REFERENCES,0.8431372549019608,"Figure 14: Training performance and loss for DQN on Space Invaders, with different hyperparam-
eters ϵp and following the prioritization scheme in Schaul et al. (2015). The loss is calculated by
multiplying ˜wi and ℓDQN for each sampled data."
REFERENCES,0.8447712418300654,"where we set ϵp to be a vanishingly small number 10−10, and ˜cp > 1 is a prioritization hyperparam-
eter that controls the lower bound relative to the average. In our experiments we set ˜cp = 10. This
scheme makes sure that regardless of the size of |δi|, a data is always sampled with a probability
that is at least
1
˜cpN , and wi is bounded by ˜cβ
p from above provided that the total priority P"
REFERENCES,0.8464052287581699,"j pj does
not change too quickly. We adopt this prioritization scheme in all of our experiments except for the
experiments in Fig. 14 above. Compared to Fig. 14, it can be seen that our DQN loss and C-DQN
loss on Space Invaders in Fig. 4 do not change as much during training."
REFERENCES,0.8480392156862745,"Details of setting
When a new data is put into the replay memory, it is assigned a priority that is
equal to the maximum of all priorities pi in the memory that have been calculated using Eq. (38),
and at the beginning of learning when gradient descent has not started, we assign the collected data a
priority equal to 100. We also additionally bound wi from above by 2˜cp, so that wi does not become
too large even if the total priority P"
REFERENCES,0.8496732026143791,"j pj ﬂuctuates. The hyperparameter β is linearly increased from
0.4 to 1 during the 5 × 107 steps of the agent, following Schaul et al. (2015), and α is 0.9 in Sec. 5.3
and E.3 and is 0.6 otherwise. We did not try other values of α."
REFERENCES,0.8513071895424836,"In an attempt to improve efﬁciency, whenever we use |δi| to update the priority pi of a transition
(st, at, rt, st+1), we also use its half |δi|"
REFERENCES,0.8529411764705882,"2
to compute ( |δi|"
REFERENCES,0.8545751633986928,"2 + ϵp)α, and use it as a lower bound to
update the priority of the preceding transition (st−1, at−1, rt−1, st). This accelerates learning by
facilitating the propagation of information. We adopt this strategy in all of our experiments except
in Sec. 5.2 and in Fig. 14, in order to make sure that the results are not due to this additional strategy."
REFERENCES,0.8562091503267973,"E.3
EVALUATION OF THE DISCOUNT FACTOR AND NORMALIZATION OF THE LEARNED Q
FUNCTION"
REFERENCES,0.8578431372549019,"Evaluation of the discount factor
As discussed in Sec. B.5, some tasks require a large discount
factor γ, while for many other tasks, a large γ slows down the learning signiﬁcantly and make the
optimization difﬁcult. Therefore, we wish to ﬁnd a method to automatically determine a suitable
γ for a given task. As an attempt, we propose a heuristic algorithm to approximately estimate the
frequency of the reward signal in an episode, based on which we can determine γ. The algorithm is
described in the following."
REFERENCES,0.8594771241830066,"Given an episode Ek in the form of an ordered sequence of rewards Ek = (ri)Tk−1
i=0 , for which sTk
is a terminal state,8 we wish to have an estimate of the average number of steps needed to observe
the next non-negligible reward when one starts from i = 0 and moves to i = Tk. Suppose that all
rewards {ri} are either 0 or a constant r(1) ̸= 0; then, one may simply count the average number of
steps before encountering the next reward r(1) when one moves from i = 0 to Tk. However, such
a simple strategy does not correctly respect the different kinds of distributions of rewards in time,
such as equispaced rewards and clustered rewards, and this strategy is symmetric with regard to the
time reversal 0 ↔Tk, which is not satisfactory.9 Therefore, we use a weighted average instead. To"
REFERENCES,0.8611111111111112,"8We consider rt to be the reward that is obtained when moving from state st to st+1.
9This is because if one almost encounters no reward at the beginning but encounters many rewards at the
end of an episode, the time horizon for planning should be large; however, if one encounters many rewards at
the beginning but almost encounters no reward at the end, the time horizon is not necessarily large. Therefore,"
REFERENCES,0.8627450980392157,Published as a conference paper at ICLR 2022
REFERENCES,0.8643790849673203,this end we deﬁne the return
REFERENCES,0.8660130718954249,Ri(Ek) :=
REFERENCES,0.8676470588235294,"Tk−1
X"
REFERENCES,0.869281045751634,"t=i
rt,
(39)"
REFERENCES,0.8709150326797386,"which is the sum of the rewards that are to be obtained starting from time step i, and we compute
the average number of steps to see the next reward weighted by this return as"
REFERENCES,0.8725490196078431,"ˆl(Ek) =
PTk−1
i=0
Ri(Ek) li
PTk−1
i′=0 Ri′(Ek)
,
li := min{t | t ≥i, rt = r(1)} −i + 1
(40)"
REFERENCES,0.8741830065359477,"where li is the number of time steps to encounter the next reward starting from the time step i. This
strategy does not respect the time reversal symmetry as it involves Ri, and as the sum is taken over
the time steps {0, 1, ...Tk −1}, it can distinguish between clustered rewards and equispaced rewards,
and it also properly takes the distance between consecutive clusters of rewards into account."
REFERENCES,0.8758169934640523,"The next problem is how we should deal rewards that have various different magnitudes. As we
can deal with the case of the rewards being either 0 or a constant, we may decompose a trajectory
containing different magnitudes of rewards into a few sub-trajectories, each of which contains re-
wards that are either 0 or a constant, so that we can deal with each of these sub-trajectories using
Eq. (40), and ﬁnally use a weighted average of ˆl over those sub-trajectories as our result. With a
set of episodes {Ek}, we treat each of the episodes separately, and again, use a weighted average
over the episodes as our estimate. The algorithm is given by Alg. 1. The weights in line 3 in Alg. 1
ensure that when computing the ﬁnal result, episodes with a zero return can be ruled out, and that an
episode with a large return does not contribute too much compared with the episodes with smaller
returns. We have intentionally used the inverse in line 10 in Alg. 1 and used the root mean square in
line 12 in order to put more weight on episodes that have frequent observations of rewards. Taking
the absolute values of rewards in line 2 is customary, and in fact, one can also separate the negative
and positive parts of rewards and treat them separately. Note that the output f should be divided
by a factor of 2 to correctly represent the frequency of rewards. We also notice that the above ap-
proach can actually be generalized to the case of continuous variables of reward and time, for which
integration can be used instead of decomposition and summation."
REFERENCES,0.8774509803921569,"To obtain the discount factor γ, we set the time horizon to be ˜cγ"
REFERENCES,0.8790849673202614,"f , with a time-horizon hyperparameter"
REFERENCES,0.880718954248366,"˜cγ, and then we set γ = 1 −
f
˜cγ . To make γ close to 0.9998 for the difﬁcult games discussed in
Sec. 5.3, we have set ˜cγ = 15. However, later we noticed that the agent actually learns more
efﬁciently with a smaller γ, and ˜cγ may be set to range from 5 to 15. In Sec. 5.3, ˜cγ = 15 is used,
but in the following experiments we use ˜cγ = 10."
REFERENCES,0.8823529411764706,"We make use of the complete episodes in the initial 50000 transitions collected at the beginning of
training to evaluate γ, and here we also regard the loss of a life in the game as the end of an episode.
We clip the obtained γ so that it lies between 0.99 and 0.9998, and γ is set to be 0.9998 if no reward
is observed."
REFERENCES,0.8839869281045751,"Normalization of the Q function
In addition to using the transformation function T (·) to squash
the Q function as described in Eq. (28) and (30), we normalize the Q function by the scale of the
reward since the tasks in Atari 2600 have vastly different magnitudes of rewards. For an episode Ek,
the Q function, or the value function, as the discounted return in the episode is given by"
REFERENCES,0.8856209150326797,Qi;k =
REFERENCES,0.8872549019607843,"Tk−1
X"
REFERENCES,0.8888888888888888,"t=i
γt−irt,
(41)"
REFERENCES,0.8905228758169934,"and we compute its mean µ by taking the average of Qi;k over all states in given sample episodes.
The standard deviation of Qi;k, however, has a dependence on ˜cγ. If we simply normalize Qi;k by
its standard deviation, the magnitude of the reward signal after normalization becomes dependent
on the hyperparameter ˜cγ, which we wish to avoid. To obtain a normalization that is independent of
˜cγ, we assume that rewards are i.i.d. variables with mean µr and standard deviation σr. Focusing on"
REFERENCES,0.8921568627450981,"the discount factor should not be evaluated by a method that respects the time reversal. Note that the Fourier
transform also respects the time reversal and thus does not sufﬁce for our purpose."
REFERENCES,0.8937908496732027,Published as a conference paper at ICLR 2022
REFERENCES,0.8954248366013072,Algorithm 1 Estimation of the expected frequency of observing a next reward signal
REFERENCES,0.8970588235294118,"Input: sample episodes {Ek}
Output: an estimate of the inverse of the number of time steps f
1: for Ek ∈{Ek} do
2:
Ek = (ri)Tk−1
i=0
←(|ri|)Tk−1
i=0
▷Taking the absolute value of reward
3:
wk ←
p"
REFERENCES,0.8986928104575164,"R0(Ek)
▷For computing a weighted average over different episodes
4:
j ←0
5:
while rewards Ek = (ri)Tk−1
i=0
are not all zero do
6:
j ←j + 1
7:
E(j)
k , Ek ←DECOMPOSESEQUENCE(Ek)
8:
end while"
REFERENCES,0.9003267973856209,"9:
Compute lk ← P"
REFERENCES,0.9019607843137255,"j R0(E(j)
k
) ˆl(E(j)
k
)
P"
REFERENCES,0.9035947712418301,"j′ R0(E(j′)
k
)
▷Weighting the results by the contribution of the rewards"
REFERENCES,0.9052287581699346,"10:
fk ←1"
REFERENCES,0.9068627450980392,"lk
11: end for"
REFERENCES,0.9084967320261438,"12: Compute f ←
q P"
REFERENCES,0.9101307189542484,"k wk f 2
k
P"
REFERENCES,0.9117647058823529,"k′ wk′
▷We use RMS to have more emphasis on episodes with larger fk
13: return f
14:
15: procedure DECOMPOSESEQUENCE(E)
16:
r′ ←min{ri}T −1
i=0 = min E"
REFERENCES,0.9133986928104575,"17:
E′ ←(r′′
i )T −1
i=0 ,
r′′
i :=
0,
if ri = 0
r′,
otherwise"
REFERENCES,0.9150326797385621,"18:
E ←(ri −r′′
i )T −1
i=0
19:
return E′, E
20: end procedure"
REFERENCES,0.9166666666666666,"the Q function at the initial states, i.e. Q0;k, we obtain the relation"
REFERENCES,0.9183006535947712,E [Q0;k] = 1 −γTk
REFERENCES,0.9199346405228758,"1 −γ µr,
(42)"
REFERENCES,0.9215686274509803,"and therefore we estimate µr by µr =
1
NE
P"
REFERENCES,0.923202614379085,"k Q0;k
1−γ
1−γTk , with the number of sample episodes NE.
Also, we have"
REFERENCES,0.9248366013071896,"Var

Q0;k −1 −γTk"
REFERENCES,0.9264705882352942,1 −γ µr
REFERENCES,0.9281045751633987,"
= 1 −γ2Tk"
REFERENCES,0.9297385620915033,"1 −γ2 σ2
r,
(43)"
REFERENCES,0.9313725490196079,"and therefore σr can be estimated by the variance of
n
Q0;k −1−γTk"
REFERENCES,0.9330065359477124,"1−γ µr

·
q"
REFERENCES,0.934640522875817,"1−γ2
1−γ2Tk
o k."
REFERENCES,0.9362745098039216,"After obtaining the standard deviation of rewards σr, we need to compute a scale σ to normalize
the learned Q function. To avoid ˜cγ dependence of σ, we use σr to roughly predict the standard
deviation of Q0;k if γ0 ≡1 −f"
REFERENCES,0.9379084967320261,"2 is used as the discount factor, and we use the obtained standard
deviation as the normalization factor σ. For simplicity we ignore the variance of Tk and obtain"
REFERENCES,0.9395424836601307,σ = σr · 1 NE X k s
REFERENCES,0.9411764705882353,"1 −γ2Tk
0
1 −γ2
0
.
(44)"
REFERENCES,0.9428104575163399,"Finally, the function we let the agent learn is ˆQ ≡Q−µ σ ."
REFERENCES,0.9444444444444444,"The normalized function ˆQ also obeys the Bellman equation as well, but with a slightly modiﬁed
reward. It can be easily shown that"
REFERENCES,0.946078431372549,"ˆQ∗(st, at) = rt −(1 −γ)µ"
REFERENCES,0.9477124183006536,"σ
+ γ max
a′
ˆQ∗(st+1, a′)
if st+1 is non-terminal,
(45)"
REFERENCES,0.9493464052287581,"and
ˆQ∗(st, at) = rt −µ"
REFERENCES,0.9509803921568627,"σ
if st+1 is terminal.
(46)"
REFERENCES,0.9526143790849673,Published as a conference paper at ICLR 2022
REFERENCES,0.954248366013072,"0.0
0.5
1.0
1.5
2.0
frames
×108 30000 20000 10000"
REFERENCES,0.9558823529411765,reward
REFERENCES,0.9575163398692811,Skiing (γ ≈0.9997)
REFERENCES,0.9591503267973857,"C-DQN
Human
Double DQN"
REFERENCES,0.9607843137254902,"0.0
0.5
1.0
1.5
2.0
×108 20 0 20"
REFERENCES,0.9624183006535948,Tennis (γ ≈0.9972)
REFERENCES,0.9640522875816994,"0.0
0.5
1.0
1.5
2.0
×108 50 100 150"
REFERENCES,0.9656862745098039,Bowling (γ ≈0.9995)
REFERENCES,0.9673202614379085,"0.0
0.5
1.0
1.5
2.0
×108 1000 2000"
REFERENCES,0.9689542483660131,SpaceInvaders (γ ≈0.9969)
REFERENCES,0.9705882352941176,"0.0
0.5
1.0
1.5
2.0
×108 0 10000 20000 30000"
REFERENCES,0.9722222222222222,Hero (γ ≈0.9976)
REFERENCES,0.9738562091503268,"0.0
0.5
1.0
1.5
2.0
×108 0 2000 4000 6000"
REFERENCES,0.9754901960784313,MsPacman (γ ≈0.9948)
REFERENCES,0.9771241830065359,"Figure 15: Training performance for C-DQN on several games in Atari 2600 compared with the
human performance (Badia et al., 2020) and the double DQN (Hessel et al., 2018), using the same
experimental setting as in Sec. 5.3, except for using ˜cγ = 10."
REFERENCES,0.9787581699346405,"Therefore, the effect of normalization amounts to modifying the reward rt to ˆrt :=
rt−(1−γ)µ"
REFERENCES,0.9803921568627451,"σ
,
and then assigning an additional terminal reward −γµ"
REFERENCES,0.9820261437908496,"σ . This is easy to implement and we use this
normalization in our experiments in Sec.5.3. Similarly to the evaluation of γ, we use the episodes in
the initial 50000 transitions to compute µ and σ; however, here we do not regard the loss of a life as
the end of an episode, so that the lengths of the episodes do not become too short. We also do not
take episodes that have a zero return into account."
REFERENCES,0.9836601307189542,"Results on other Atari games
To demonstrate the generality of the above strategy, we report our
results on several games in Atari 2600, using a learning rate of 4 × 10−5 and ˜cγ = 10. The results
are shown in Fig. 15. We ﬁnd that for some games, especially Hero, the learning sometimes gets
trapped in a local optimum and learning may stagnate, which deserves further investigation. We did
not do a ﬁne grid search on the learning rate and we simply selected from 6 × 10−5, 4 × 10−5 and
2 × 10−5, and we chose 4 × 10−5, because it produces reasonable results for most of the tasks. We
notice that it is difﬁcult to ﬁnd a learning rate that works well for all the tasks, as the tasks have
drastically different levels of stochasticity and are associated with different time horizons."
REFERENCES,0.9852941176470589,"We also notice that there are several cases where our strategy of the normalization and the evaluation
of γ does not give satisfactory results. This is mainly because we have assumed that the time
scales of obtaining rewards are similar for the random policy and for a learned policy. A typical
counterexample is the game Breakout, where the random policy almost obtains no reward in an
episode but a learned policy frequently obtains rewards. Therefore, our strategy above is still not
general enough to deal with all kinds of scenarios, and it cannot replace the bandit algorithm in
Badia et al. (2020) which is used to select γ, and therefore a better strategy is still desired."
REFERENCES,0.9869281045751634,"F
EXPERIMENTAL DETAILS ON CLIFF WALKING"
REFERENCES,0.988562091503268,"In the cliff walking experiments, we store all state-action pairs into a table, excluding the states
of goal positions and cliff positions, and excluding the actions that go into the walls. We plot the
data on a log scale of iteration steps by explicitly evaluating the loss over all state-action pairs in
Sec. 3.1, and evaluating the reward of the greedy policy in Sec. 3.2. Because we do evaluations at
equal intervals on a log scale of the x-axis, fewer evaluations are made when the number of iteration
steps is larger, and as a consequence, the scatter plots in the right of Fig. 1 do not have equally many
data points along the curves. The learning rate α is always 0.5, and ϵ in the ϵ-greedy policy is always
ﬁxed. Speciﬁcally for the one-way cliff walking task in Fig. 3, when two actions a1 and a2 have the
same Q function value, i.e. Q(st, a1) = Q(st, a2), the greedy policy randomly chooses a1 or a2 at
state st, and when maxa′ Q(st+1, a′) = Q(st+1, a1) = Q(st+1, a2), we modify the learning rule"
REFERENCES,0.9901960784313726,Published as a conference paper at ICLR 2022
REFERENCES,0.9918300653594772,"of RG for transition (st, at, rt, st+1) to be"
REFERENCES,0.9934640522875817,"∆Q(st, at) = α

rt + γ max
a′ Q(st+1, a′) −Q(st, at)

,"
REFERENCES,0.9950980392156863,"∆Q(st+1, a1) = ∆Q(st+1, a2) = −γ"
REFERENCES,0.9967320261437909,"2 ∆Q(st, at),
(47)"
REFERENCES,0.9983660130718954,so that the greedy policy at st+1 is not changed after learning from the transition.
