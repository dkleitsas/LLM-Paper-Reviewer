Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0025188916876574307,"Decision forests are popular tools for classiﬁcation and regression. These forests
1"
ABSTRACT,0.005037783375314861,"naturally generate proximity matrices that measure the frequency of observations
2"
ABSTRACT,0.007556675062972292,"appearing in the same leaf node. While other kernels are known to have strong the-
3"
ABSTRACT,0.010075566750629723,"oretical properties such as being characteristic, there is no similar result available
4"
ABSTRACT,0.012594458438287154,"for decision forest-based kernels. In addition, existing approaches to independence
5"
ABSTRACT,0.015113350125944584,"and k-sample testing may require unfeasibly large sample sizes and are not inter-
6"
ABSTRACT,0.017632241813602016,"pretable. In this manuscript, we prove that the decision forest induced proximity
7"
ABSTRACT,0.020151133501259445,"is a characteristic kernel, enabling consistent independence and k-sample testing
8"
ABSTRACT,0.022670025188916875,"via decision forests. We leverage this to introduce kernel mean embedding random
9"
ABSTRACT,0.02518891687657431,"forest (KMERF), which is a valid and consistent method for independence and
10"
ABSTRACT,0.027707808564231738,"k-sample testing. Our extensive simulations demonstrate that KMERF outperforms
11"
ABSTRACT,0.030226700251889168,"other tests across a variety of independence and two-sample testing scenarios.
12"
ABSTRACT,0.0327455919395466,"Additionally, the test is interpretable, and its key features are readily discernible.
13"
ABSTRACT,0.03526448362720403,"This work therefore demonstrates the existence of a test that is both more powerful
14"
ABSTRACT,0.037783375314861464,"and more interpretable than existing methods, ﬂying in the face of conventional
15"
ABSTRACT,0.04030226700251889,"wisdom of the trade-off between the two.
16"
INTRODUCTION,0.042821158690176324,"1
Introduction
17"
INTRODUCTION,0.04534005037783375,"Decision forests are ensemble method popularized by Breiman [3]. It is highly effective in classiﬁca-
18"
INTRODUCTION,0.04785894206549118,"tion and regression tasks, particularly in high-dimensional settings [5, 6, 37]. This is achieved by
19"
INTRODUCTION,0.05037783375314862,"randomly partitioning the feature set and using subsampling techniques to construct multiple decision
20"
INTRODUCTION,0.05289672544080604,"trees from the training data. To measure the similarity between two observations, a proximity matrix
21"
INTRODUCTION,0.055415617128463476,"can be constructed, deﬁned as the percentage of trees in which both observations lie in the same leaf
22"
INTRODUCTION,0.05793450881612091,"node [4]. This proximity matrix serves as an induced kernel or similarity matrix for the decision
23"
INTRODUCTION,0.060453400503778336,"forest. In general, any random partition algorithm may produce such a kernel matrix.
24"
INTRODUCTION,0.06297229219143577,"As the complexity of datasets grow, it becomes increasingly necessary to develop methods that can
25"
INTRODUCTION,0.0654911838790932,"efﬁciently perform independence and k-sample testing. We also desire methods that are interpretable,
26"
INTRODUCTION,0.06801007556675064,"lending insight into how and why statistically signiﬁcant results were determined. Parametric methods
27"
INTRODUCTION,0.07052896725440806,"are often highly interpretable, such as Pearson’s correlation and its rank variants [22, 33, 16]. These
28"
INTRODUCTION,0.07304785894206549,"methods are still popular to detect linear and monotonic relationships in univariate settings, but
29"
INTRODUCTION,0.07556675062972293,"they are not consistent for detecting more complicated nonlinear relationships. Nonparametric
30"
INTRODUCTION,0.07808564231738035,"methods can be very powerful. The more recent distance correlation (Dcorr) [36, 35] and the
31"
INTRODUCTION,0.08060453400503778,"kernel correlation (HSIC) [10, 11] are consistent for testing independence against any distribution of
32"
INTRODUCTION,0.08312342569269521,"ﬁnite second moments for any ﬁnite dimensionality; moreover, the energy-based statistics (such as
33"
INTRODUCTION,0.08564231738035265,"Dcorr) and kernel-based statistics (such as HSIC) are known to be exactly equivalent for all ﬁnite
34"
INTRODUCTION,0.08816120906801007,"samples [21, 30]. The theory supporting universal consistency of these methods (which we refer to as
35"
INTRODUCTION,0.0906801007556675,"kernel methods hereafter, without loss of generality) depends on those kernels being characteristic
36"
INTRODUCTION,0.09319899244332494,"kernel [28, 18, 19, 30]. Unfortunately, the above tests do not attempt to further characterize the
37"
INTRODUCTION,0.09571788413098237,"dependency structure. To the best of our knowledge, very few tests exist[38, 15].
38"
INTRODUCTION,0.0982367758186398,"In addition, although these methods all have asymptotic guarantees, for ﬁnite samples, performance
39"
INTRODUCTION,0.10075566750629723,"can be impaired by poorly choosing a particular characteristic kernel. Choosing an appropriate
40"
INTRODUCTION,0.10327455919395466,"kernel that properly summarize geometries within the data is often times non-obvious [27]. High-
41"
INTRODUCTION,0.10579345088161209,"dimensional data is particularly vexing [25, 38], and a number of extensions have been proposed to
42"
INTRODUCTION,0.10831234256926953,"achieve better power such as adaptive metric kernel choice [12], low-dimensional projections [14],
43"
INTRODUCTION,0.11083123425692695,"and marginal correlations [29].
44"
INTRODUCTION,0.11335012594458438,"In this paper, we leverage the popular random forest method [3] and a recent chi-square test [32]
45"
INTRODUCTION,0.11586901763224182,"for a more powerful and interpretable method for hypothesis testing. We prove that the random
46"
INTRODUCTION,0.11838790931989925,"forest induced kernel is a characteristic kernel, and the resulting kernel mean embedding random
47"
INTRODUCTION,0.12090680100755667,"forest (KMERF) is a valid and consistent method for independence and k-sample testing. We then
48"
INTRODUCTION,0.12342569269521411,"demonstrate its empirical advantage over existing tools for high-dimensional testing in a variety
49"
INTRODUCTION,0.12594458438287154,"of dependence settings, suggesting that it will often be more powerful than existing approaches in
50"
INTRODUCTION,0.12846347607052896,"real data. As random forest can directly estimate feature importances [3], the outputs of KMERF
51"
INTRODUCTION,0.1309823677581864,"are also interpretable, KMERF therefore ﬂies in the face of conventional wisdom that one must
52"
INTRODUCTION,0.13350125944584382,"choose between power and interpretability: KMERF is both empirically more powerful and more
53"
INTRODUCTION,0.13602015113350127,"interpretable than existing approaches.
54"
PRELIMINARIES,0.1385390428211587,"2
Preliminaries
55"
HYPOTHESIS TESTING,0.14105793450881612,"2.1
Hypothesis Testing
56"
HYPOTHESIS TESTING,0.14357682619647355,"The testing independence hypothesis is formulated as follows: suppose xi ∈Rp and yi ∈Rq, and
57"
HYPOTHESIS TESTING,0.14609571788413098,"n samples of (xi, yi)
iid∼FXY , i.e., xi and yi are realizations of random variables X and Y . The
58"
HYPOTHESIS TESTING,0.1486146095717884,"hypothesis for testing independence is
59"
HYPOTHESIS TESTING,0.15113350125944586,"H0 : FXY = FXFY ,
HA : FXY ̸= FXFY ."
HYPOTHESIS TESTING,0.15365239294710328,"Given any kernel function k(·, ·), we can formulate the kernel induced correlation measure as cn
k(x, y)
60"
HYPOTHESIS TESTING,0.1561712846347607,"using the sample kernel matrices [10, 30], where x = {xi} and y = {yi}. When the kernel function
61"
HYPOTHESIS TESTING,0.15869017632241814,"k(·, ·) is characteristic, it has been shown that cn
k(x, y) →0 if and only if x and y are independent
62"
HYPOTHESIS TESTING,0.16120906801007556,"[10].
63"
HYPOTHESIS TESTING,0.163727959697733,"The k-sample hypothesis is formulated as follows: let uj
i ∈Rp be the realization of random variable
64"
HYPOTHESIS TESTING,0.16624685138539042,"Uj for j = 1, . . . , l and i = 1, . . . , nj. Suppose the l datasets that are sampled i.i.d. from F1, . . . , Fl
65"
HYPOTHESIS TESTING,0.16876574307304787,"and independently from one another. Then,
66"
HYPOTHESIS TESTING,0.1712846347607053,"H0 : F1 = F2 = · · · = Fl,"
HYPOTHESIS TESTING,0.17380352644836272,"HA : ∃j ̸= j′ s.t. Fj ̸= Fj′.
By concatenating the l datasets and introducing an auxiliary random variable, the kernel correlation
67"
HYPOTHESIS TESTING,0.17632241813602015,"measure can be used for k-sample testing [21].
68"
CHARACTERISTIC KERNEL,0.17884130982367757,"2.2
Characteristic Kernel
69"
CHARACTERISTIC KERNEL,0.181360201511335,"Deﬁnition 1. Let X be a separable metric space, such as Rp. A kernel function k(·, ·) : X × X →R
70"
CHARACTERISTIC KERNEL,0.18387909319899245,"measures the similarity between two observations in X, and an n × n kernel matrix for {xi ∈X, i =
71"
CHARACTERISTIC KERNEL,0.18639798488664988,"1, . . . , n} is deﬁned by K(i, j) = k(xi, xj).
72"
CHARACTERISTIC KERNEL,0.1889168765743073,"• A kernel k(·, ·) : X × X →R is positive deﬁnite if, for any n ≥2, x1, . . . , xn ∈X and
73"
CHARACTERISTIC KERNEL,0.19143576826196473,"a1, . . . , an ∈R, it satisﬁes
74 n
X"
CHARACTERISTIC KERNEL,0.19395465994962216,"i,j=1
aiajk(xi, xj) ≥0."
CHARACTERISTIC KERNEL,0.1964735516372796,"• A characteristic kernel is a positive deﬁnite kernel that has the following property: for any
75"
CHARACTERISTIC KERNEL,0.19899244332493704,"two random variables X1 and X2 with distributions FX1 and FX2,
76"
CHARACTERISTIC KERNEL,0.20151133501259447,"E[k(·, X1)] = E[k(·, X2)] if and only if FX1 = FX2.
(1)"
KMERF,0.2040302267002519,"3
KMERF
77"
KMERF,0.20654911838790932,"The proposed approach for hypothesis testing, KMERF, involves the following steps:
78"
KMERF,0.20906801007556675,"1. Run random forest with m trees, with independent bootstrap samples of size nb ≤n used
79"
KMERF,0.21158690176322417,"to construct each tree. The tree structures (partitions) within the forest P are denoted as
80"
KMERF,0.2141057934508816,"φw ∈P, where w ∈1, . . . , m and φw(xi) represents the partition assigned to xi.
81"
CALCULATE THE PROXIMITY KERNEL BY,0.21662468513853905,"2. Calculate the proximity kernel by
82"
CALCULATE THE PROXIMITY KERNEL BY,0.21914357682619648,"Kx
ij = 1 m m
X"
CALCULATE THE PROXIMITY KERNEL BY,0.2216624685138539,"w=1
[I(φw(xi) = φw(xj))],"
CALCULATE THE PROXIMITY KERNEL BY,0.22418136020151133,"where I(·) is the indicator function that checks whether the two observations lie in the same
83"
CALCULATE THE PROXIMITY KERNEL BY,0.22670025188916876,"partition in each tree.
84"
CALCULATE THE PROXIMITY KERNEL BY,0.22921914357682618,"3. Compute the unbiased kernel transformation [34, 32] on Kx. Namely, let
85"
CALCULATE THE PROXIMITY KERNEL BY,0.23173803526448364,"Lx
ij = 
"
CALCULATE THE PROXIMITY KERNEL BY,0.23425692695214106,"
Kx
ij −
1
n−2 nP"
CALCULATE THE PROXIMITY KERNEL BY,0.2367758186397985,"t=1
Kx
it −
1
n−2 nP"
CALCULATE THE PROXIMITY KERNEL BY,0.23929471032745592,"s=1
Kx
sj +
1
(n−1)(n−2) nP"
CALCULATE THE PROXIMITY KERNEL BY,0.24181360201511334,"s,t=1
Kx
st
i ̸= j"
CALCULATE THE PROXIMITY KERNEL BY,0.24433249370277077,"0
i = j"
CALCULATE THE PROXIMITY KERNEL BY,0.24685138539042822,"4. Let Ky be the Euclidean distance induced kernel by Shen and Vogelstein [30], or the
86"
CALCULATE THE PROXIMITY KERNEL BY,0.24937027707808565,"proximity kernel in the case that dimensions of x and y is the same, that is p = q, and
87"
CALCULATE THE PROXIMITY KERNEL BY,0.2518891687657431,"compute Ly using the same unbiased transformation. Then the KMERF statistic for the
88"
CALCULATE THE PROXIMITY KERNEL BY,0.25440806045340053,"induced kernel k is,
89"
CALCULATE THE PROXIMITY KERNEL BY,0.25692695214105793,"cn
k(x, y) =
1
n(n −3)trace(LxLy)."
CALCULATE THE PROXIMITY KERNEL BY,0.2594458438287154,"5. Compute the p-value via the following chi-square test [32]:
90"
CALCULATE THE PROXIMITY KERNEL BY,0.2619647355163728,"p = 1 −Fχ2
1−1 "
CALCULATE THE PROXIMITY KERNEL BY,0.26448362720403024,"n ·
cn
k(x, y)
p"
CALCULATE THE PROXIMITY KERNEL BY,0.26700251889168763,"cn
k(x, x) · cn
k(y, y) ! ,"
CALCULATE THE PROXIMITY KERNEL BY,0.2695214105793451,"where χ2
1 is the chi-square distribution of degree 1. Reject the independence hypothesis if
91"
CALCULATE THE PROXIMITY KERNEL BY,0.27204030226700254,"the p-value is less than a speciﬁed typer 1 error level, say 0.05.
92"
CALCULATE THE PROXIMITY KERNEL BY,0.27455919395465994,"In the numerical implementation, the standard supervised random forest is used with m = 500 (which
93"
CALCULATE THE PROXIMITY KERNEL BY,0.2770780856423174,"is also applicable to the unsupervised version or other random forest variants [2, 1, 37]). In the second
94"
CALCULATE THE PROXIMITY KERNEL BY,0.2795969773299748,"step, we simply compute the proximity kernel deﬁned by the random forest induced kernel. In the
95"
CALCULATE THE PROXIMITY KERNEL BY,0.28211586901763225,"third step, we normalize the proximity kernel to ensure it obtains a consistent dependence measure;
96"
CALCULATE THE PROXIMITY KERNEL BY,0.28463476070528965,"this is the KMERF test statistic. We found that utilizing the multiscale version of the kernel correlation
97"
CALCULATE THE PROXIMITY KERNEL BY,0.2871536523929471,"[38, 31], which is equivalent for linear relationships while being better for nonlinear relationships,
98"
CALCULATE THE PROXIMITY KERNEL BY,0.28967254408060455,"produced similar results to using distance correlation, but substantially increased runtimes.
99"
CALCULATE THE PROXIMITY KERNEL BY,0.29219143576826195,"Note that one could also compute a p-value for KMERF via the permutation test, which is a standard
100"
CALCULATE THE PROXIMITY KERNEL BY,0.2947103274559194,"procedure for testing independence [9]. Speciﬁcally, ﬁrst compute a kernel on the observed {xi} and
101"
CALCULATE THE PROXIMITY KERNEL BY,0.2972292191435768,"{yi}. Then randomly permute the index of {yi}, repeat the kernel generation process for {yi} for
102"
CALCULATE THE PROXIMITY KERNEL BY,0.29974811083123426,"each permutation. This process involves training a new random forest for each permutation. Finally,
103"
CALCULATE THE PROXIMITY KERNEL BY,0.3022670025188917,"compute the test statistic for each of the permutations, and the p-value equals the percentage the
104"
CALCULATE THE PROXIMITY KERNEL BY,0.3047858942065491,"permuted statistics that are larger than the observed statistic. However, the permutation test is very
105"
CALCULATE THE PROXIMITY KERNEL BY,0.30730478589420657,"slow for large sample size and almost always yields similar results as the chi-square test.
106"
THEORETICAL PROPERTIES,0.30982367758186397,"4
Theoretical Properties
107"
THEORETICAL PROPERTIES,0.3123425692695214,"Here, we show that the random forest kernel characteristic, and the induced test statistic used in
108"
THEORETICAL PROPERTIES,0.3148614609571788,"KMERF allows for valid and universally consistent independence and k-sample testing. All proofs
109"
THEORETICAL PROPERTIES,0.31738035264483627,"are in appendix.
110"
THEORETICAL PROPERTIES,0.3198992443324937,"For a kernel to be characteristic, it ﬁrst needs to be positive deﬁnite, which is indeed the case for the
111"
THEORETICAL PROPERTIES,0.3224181360201511,"forest-induced kernel:
112"
THEORETICAL PROPERTIES,0.3249370277078086,"Theorem 1. The random forest induced kernel Kx is always positive deﬁnite.
113"
THEORETICAL PROPERTIES,0.327455919395466,"This theorem holds because the forest-induced kernel is a summation of a permuted block diagonal
114"
THEORETICAL PROPERTIES,0.32997481108312343,"matrix, with each matrix coming from individual tree, that is positive deﬁnite [7]; and a summation
115"
THEORETICAL PROPERTIES,0.33249370277078083,"of positive deﬁnite matrices is still positive deﬁnite.
116"
THEORETICAL PROPERTIES,0.3350125944584383,"Next, we show the kernel is characteristic when the tree partition area converges to zero. A similar
117"
THEORETICAL PROPERTIES,0.33753148614609574,"property is also used for proving classiﬁcation consistency for k-nearest-neighbors [8], and we shall
118"
THEORETICAL PROPERTIES,0.34005037783375314,"denote N(φw) as the maximum area of each part.
119"
THEORETICAL PROPERTIES,0.3425692695214106,"Theorem 2. Suppose as n, m →∞, N(φw) →0 for each tree φw ∈P and each observation xi.
120"
THEORETICAL PROPERTIES,0.345088161209068,"Then the random forest induced kernel Kx is asymptotically characteristic.
121"
THEORETICAL PROPERTIES,0.34760705289672544,"Intuitively, for sufﬁciently many trees and sufﬁciently small leaf region, observations generated by
122"
THEORETICAL PROPERTIES,0.3501259445843829,"two different distributions cannot always be in the same leaf region.
123"
THEORETICAL PROPERTIES,0.3526448362720403,"This leads to the validity and consistency result of KMERF:
124"
THEORETICAL PROPERTIES,0.35516372795969775,"Corollary 2.1. KMERF satisﬁes
125"
THEORETICAL PROPERTIES,0.35768261964735515,"lim
n→∞cn
k(x, y) = c ≥0,"
THEORETICAL PROPERTIES,0.3602015113350126,"with equality to 0 if and only if FXY = FXFY . Moreover, for sufﬁciently large n and sufﬁciently
126"
THEORETICAL PROPERTIES,0.36272040302267,"small type 1 error level α, this method is valid and consistent for independence and k-sample testing.
127"
THEORETICAL PROPERTIES,0.36523929471032746,"By Gretton et al. [10], any characteristic-kernel based dependence measure converges to 0 if and
128"
THEORETICAL PROPERTIES,0.3677581863979849,"only if X and Y are independent. Moreover, Shen et al. [32] showed that the chi-square distribution
129"
THEORETICAL PROPERTIES,0.3702770780856423,"χ2
1 −1 approximates and upper-tail dominates the true null distribution of any unbiased kernel when
130"
THEORETICAL PROPERTIES,0.37279596977329976,"using distance correlation, making it a valid and consistent test.
131"
SIMULATIONS,0.37531486146095716,"5
Simulations
132"
SIMULATIONS,0.3778337531486146,"In this section we exhibit the consistency and validity of KMERF, and compare its testing power
133"
SIMULATIONS,0.380352644836272,"with other competitors in a comprehensive simulation set-up. We utilize the hyppo package in
134"
SIMULATIONS,0.38287153652392947,"Python [20], which uses scikit-learn [23] random forest with 500 trees and otherwise default
135"
SIMULATIONS,0.3853904282115869,"hyper-parameters, and calculate the proximity matrix from this. The KMERF statistic and p-value
136"
SIMULATIONS,0.3879093198992443,"then computed via the process in Section 3. The mathematical details for each simulation type is in
137"
SIMULATIONS,0.3904282115869018,"the Appendix C.
138"
TESTING INDEPENDENCE,0.3929471032745592,"5.1
Testing Independence
139"
TESTING INDEPENDENCE,0.3954659949622166,"In this section we compare KMERF to Multiscale Graph Correlation (MGC), Distance Correlation
140"
TESTING INDEPENDENCE,0.3979848866498741,"(Dcorr), Hilbert-Schmidt Independence Criterion (Hsic), and Heller-Heller-Gorﬁne (HHG) method,
141"
TESTING INDEPENDENCE,0.4005037783375315,"Canonical Correlation Analysis (CCA), and the RV coefﬁcient. The HHG method has been shown
142"
TESTING INDEPENDENCE,0.40302267002518893,"to work extremely well against nonlinear dependencies [13]. The MGC method has been shown
143"
TESTING INDEPENDENCE,0.40554156171284633,"to work well against linear, nonlinear, and high-dimensional dependencies [31]. The CCA and RV
144"
TESTING INDEPENDENCE,0.4080604534005038,"coefﬁcients are popular multivariant extensions of Pearson correlation. For each method, we use the
145"
TESTING INDEPENDENCE,0.4105793450881612,"corresponding implementation in hyppo with default settings.
146"
TESTING INDEPENDENCE,0.41309823677581864,"We take 20 high-dimensional simulation settings [38], consisting of various linear, monotone, and
147"
TESTING INDEPENDENCE,0.4156171284634761,"strongly nonlinear dependencies with p increasing, q = 1, and n = 100. To estimate the testing
148"
TESTING INDEPENDENCE,0.4181360201511335,"power in each setting, we generate dependent (xi, yi) for i = 1, . . . , n, compute the test statistic
149"
TESTING INDEPENDENCE,0.42065491183879095,"for each method, repeat for r = 10000 times. Via the empirical alternative and null distribution of
150"
TESTING INDEPENDENCE,0.42317380352644834,"the test statistic, we estimate the testing power of each method at type 1 error level of α = 0.05.
151"
TESTING INDEPENDENCE,0.4256926952141058,"The power result is shown in Figure 1 shows that KMERF achieves superior performance for most
152"
TESTING INDEPENDENCE,0.4282115869017632,"simulation modalities, except a few like circle and ellipse.
153"
TWO SAMPLE TESTING,0.43073047858942065,"5.2
Two Sample Testing
154"
TWO SAMPLE TESTING,0.4332493702770781,"Here, we compare the performance in the two-sample testing regime. It has been shown that
155"
TWO SAMPLE TESTING,0.4357682619647355,"all independence measures can be used for two-sample testing [21, 30], allowing all previous
156"
TWO SAMPLE TESTING,0.43828715365239296,"independence testing methods to be compared here as well. Once again, we investigate statistical
157"
TWO SAMPLE TESTING,0.44080604534005036,"3
1000
0 1"
TWO SAMPLE TESTING,0.4433249370277078,Linear
TWO SAMPLE TESTING,0.44584382871536526,"3
1000"
TWO SAMPLE TESTING,0.44836272040302266,Exponential
TWO SAMPLE TESTING,0.4508816120906801,"3
1000 Cubic 3
10"
TWO SAMPLE TESTING,0.4534005037783375,"Joint Normal 3
20 Step"
TWO SAMPLE TESTING,0.45591939546599497,"3
20
0 1"
TWO SAMPLE TESTING,0.45843828715365237,"Quadratic 3
20"
TWO SAMPLE TESTING,0.4609571788413098,"W-Shaped 3
20"
TWO SAMPLE TESTING,0.4634760705289673,"Spiral 3
100"
TWO SAMPLE TESTING,0.4659949622166247,"Bernoulli 3
100"
TWO SAMPLE TESTING,0.46851385390428213,Logarithmic
TWO SAMPLE TESTING,0.47103274559193953,"3
20
0 1"
TWO SAMPLE TESTING,0.473551637279597,"Fourth Root 3
10"
TWO SAMPLE TESTING,0.4760705289672544,"Sine 4 3
10"
TWO SAMPLE TESTING,0.47858942065491183,"Sine 16 3
40"
TWO SAMPLE TESTING,0.4811083123425693,"Square 3
20"
TWO SAMPLE TESTING,0.4836272040302267,Two Parabolas
TWO SAMPLE TESTING,0.48614609571788414,"3
20
0 1"
TWO SAMPLE TESTING,0.48866498740554154,"Circle 3
20"
TWO SAMPLE TESTING,0.491183879093199,"Ellipse 3
40"
TWO SAMPLE TESTING,0.49370277078085645,"Diamond 3
10"
TWO SAMPLE TESTING,0.49622166246851385,"Multiplicative 3
100"
TWO SAMPLE TESTING,0.4987405541561713,Independence
TWO SAMPLE TESTING,0.5012594458438288,"KMERF
MGC
Dcorr
Hsic
HHG
CCA
RV"
TWO SAMPLE TESTING,0.5037783375314862,Multivariate Independence Testing (Increasing Dimension)
TWO SAMPLE TESTING,0.5062972292191436,Dimension
TWO SAMPLE TESTING,0.5088161209068011,Statistical Power
TWO SAMPLE TESTING,0.5113350125944585,"Figure 1: Multivariate independence testing power for 20 different settings with increasing p, ﬁxed
q = 1, and n = 100. For the majority of the simulations and simulation dimensions, KMERF
performs as well as, or better than, existing multivariate independence tests in high-dimensional
dependence testing."
TWO SAMPLE TESTING,0.5138539042821159,"power differences with 20 simulation settings consisting of various linear and nonlinear, monotonic
158"
TWO SAMPLE TESTING,0.5163727959697733,"and nonmonotonic functions with dimension increasing from p = 3, . . . , 10, q = 1, and n = 100.
159"
TWO SAMPLE TESTING,0.5188916876574308,"We then apply a random rotation to this generated simulation and generate the second independent
160"
TWO SAMPLE TESTING,0.5214105793450882,"sample (via a rigid transformation).
161"
TWO SAMPLE TESTING,0.5239294710327456,"Figure 2 shows that, once again, for the majority of simulations settings, KMERF performs at or
162"
TWO SAMPLE TESTING,0.5264483627204031,"better than other tests in nearly all simulations and simulation dimensions. For certain simulation
163"
TWO SAMPLE TESTING,0.5289672544080605,"settings, especially the exponential, cubic, and fourth root, KMERF vastly outperforms other metrics
164"
TWO SAMPLE TESTING,0.5314861460957179,"as dimensions increases.
165"
INTERPRETABILITY,0.5340050377833753,"5.3
Interpretability
166"
INTERPRETABILITY,0.5365239294710328,"Not only does KMERF typically offer empirically better statistical power compared to alternatives, it
167"
INTERPRETABILITY,0.5390428211586902,"also offers insights into which features are the most important within the data set. Figure 3 shows
168"
INTERPRETABILITY,0.5415617128463476,"normalized 95% conﬁdence intervals of relative feature importances for each simulation, where the
169"
INTERPRETABILITY,0.5440806045340051,"black line shows the mean and the light grey line shows the 95% conﬁdence interval. Mean and
170"
INTERPRETABILITY,0.5465994962216625,"individual tree feature importances were normalized using min-max feature scaling. The simulations
171"
INTERPRETABILITY,0.5491183879093199,"were modiﬁed such that the weighting of each feature decreased as feature importance increased,
172"
INTERPRETABILITY,0.5516372795969773,"with the expectation that the algorithm would detect a decrease in feature importance as dimension
173"
INTERPRETABILITY,0.5541561712846348,"increased. With these simulations, we are able to determine that exact feature importance trend,
174"
INTERPRETABILITY,0.5566750629722922,"3
10
0 1"
INTERPRETABILITY,0.5591939546599496,"Linear 3
10"
INTERPRETABILITY,0.5617128463476071,"Exponential 3
10 Cubic 3
10"
INTERPRETABILITY,0.5642317380352645,"Joint Normal 3
10 Step"
INTERPRETABILITY,0.5667506297229219,"3
10
0 1"
INTERPRETABILITY,0.5692695214105793,"Quadratic 3
10"
INTERPRETABILITY,0.5717884130982368,"W-Shaped 3
10"
INTERPRETABILITY,0.5743073047858942,"Spiral 3
10"
INTERPRETABILITY,0.5768261964735516,"Bernoulli 3
10"
INTERPRETABILITY,0.5793450881612091,Logarithmic
INTERPRETABILITY,0.5818639798488665,"3
10
0 1"
INTERPRETABILITY,0.5843828715365239,"Fourth Root 3
10"
INTERPRETABILITY,0.5869017632241813,"Sine 4 3
10"
INTERPRETABILITY,0.5894206549118388,"Sine 16 3
10"
INTERPRETABILITY,0.5919395465994962,"Square 3
10"
INTERPRETABILITY,0.5944584382871536,Two Parabolas
INTERPRETABILITY,0.5969773299748111,"3
10
0 1"
INTERPRETABILITY,0.5994962216624685,"Circle 3
10"
INTERPRETABILITY,0.6020151133501259,"Ellipse 3
10"
INTERPRETABILITY,0.6045340050377834,"Diamond 3
10"
INTERPRETABILITY,0.6070528967254408,"Multiplicative 3
10"
INTERPRETABILITY,0.6095717884130982,Independence
INTERPRETABILITY,0.6120906801007556,"KMERF
MGC
Energy
MMD
HHG
CCA
RV"
INTERPRETABILITY,0.6146095717884131,Multivariate Two-Sample Testing (Increasing Dimension)
INTERPRETABILITY,0.6171284634760705,Dimension
INTERPRETABILITY,0.6196473551637279,Statistical Power
INTERPRETABILITY,0.6221662468513854,"Figure 2: Multivariate two-sample testing power for 20 different settings with increasing p, ﬁxed
q = 1, and n = 100. For nearly all simulations and simulation dimensions, KMERF performs as
well as, or better than, existing multivariate two-sample tests in high-dimensional dependence testing."
INTERPRETABILITY,0.6246851385390428,"except for a few of the more complex simulations. The process we used to generate this ﬁgure can be
175"
INTERPRETABILITY,0.6272040302267002,"trivially extended to a two-sample or k-sample case.
176"
REAL DATA,0.6297229219143576,"6
Real Data
177"
REAL DATA,0.6322418136020151,"We then applied KMERF to a date set consisting of proteolytic peptides derived from the blood
178"
REAL DATA,0.6347607052896725,"samples of 95 individuals harboring pancreatic (n=10), ovarian (n=24), colorectal cancer (n=28), and
179"
REAL DATA,0.6372795969773299,"healthy controls (n=33) [38]. The processed data included 318 peptides derived from 121 proteins
180"
REAL DATA,0.6397984886649875,"(see Appendix D for full details). Figure 4 shows the p-values for KMERF between pancreatatic
181"
REAL DATA,0.6423173803526449,"and healthy subjects compared to the p-values for KMERF between pancreatic cancer and all other
182"
REAL DATA,0.6448362720403022,"subjects. The test identiﬁes neurogranin as a potentially valuable marker for pancreatic cancer, which
183"
REAL DATA,0.6473551637279596,"the literature also corroborates [41, 40]. Meanwhile, while some of the other tests identiﬁed this
184"
REAL DATA,0.6498740554156172,"biomarker, they identiﬁed others that are upregulated in other types of cancers as well (false positives).
185"
REAL DATA,0.6523929471032746,"We also show in the ﬁgure that the biomarker chosen be KMERF provides better true positive
186"
REAL DATA,0.654911838790932,"detection when compared to the other tests (there is no ground truth in this case, so a leave-one-out
187"
REAL DATA,0.6574307304785895,"k-nearest-neighbor classiﬁcation approach was used instead).
188 0"
"LINEAR
EXPONENTIAL
CUBIC
JOINT NORMAL
STEP",0.6599496221662469,"1
Linear
Exponential
Cubic
Joint Normal
Step 0"
"QUADRATIC
W-SHAPED
SPIRAL
BERNOULLI
LOGARITHMIC",0.6624685138539043,"1
Quadratic
W-Shaped
Spiral
Bernoulli
Logarithmic 0"
FOURTH ROOT,0.6649874055415617,"1 Fourth Root
Sine 4
Sine 16
Square
Two Parabolas 1
5
0"
CIRCLE,0.6675062972292192,"1
Circle 1
5"
CIRCLE,0.6700251889168766,"Ellipse 1
5"
CIRCLE,0.672544080604534,"Diamond 1
5"
CIRCLE,0.6750629722921915,"Multiplicative 1
5"
CIRCLE,0.6775818639798489,Independence
CIRCLE,0.6801007556675063,Feature Importances
CIRCLE,0.6826196473551638,Dimension (Sorted by Feature Importance)
CIRCLE,0.6851385390428212,Estimated Feature Importance
CIRCLE,0.6876574307304786,"Figure 3: Normalized mean (black) and 95% conﬁdence intervals (light grey) using min-max
normalization for relative feature importances derived from random forest over ﬁve dimensions for
each simulation tested for 100 samples. The features were sorted from most to least informative for
all simulations except for the Independence simulation). As expected, estimated feature importance
decreases as dimension increases. A feature of KMERF is insights into interpretability, and we show
here which dimensions of our simulations inﬂuence the outcome of independence test the most."
CIRCLE,0.690176322418136,"10
5
10
3
10
1"
CIRCLE,0.6926952141057935,"p-values for Panc vs Norm 10
4 10
3 10
2 10
1"
CIRCLE,0.6952141057934509,p-values for Panc vs All
CIRCLE,0.6977329974811083,neurogranin
CIRCLE,0.7002518891687658,Cancer Biomarker Discovery
CIRCLE,0.7027707808564232,"False Positives True Positives
0 2 4 6 8"
CIRCLE,0.7052896725440806,# True / False Positives
CIRCLE,0.707808564231738,Biomarker kNN Classification
CIRCLE,0.7103274559193955,"KMERF
Hsic
HHG"
CIRCLE,0.7128463476070529,"Figure 4: (A) For each peptide, the p-values for testing dependence between pancreatic and healthy
subjects by KMERF is compared to the p-value for testing dependence between pancreatic and all
other subjects. At the critical level 0.05, KMERF identiﬁes a unique protein. (B) The true and false
positive counts using a k-nearest neighbor (choosing the best k ∈[1, 10]) leave-one-out classiﬁcation
using only the signiﬁcant peptides identiﬁed by each method. The peptide identiﬁed by KMERF
achieves the best true and false positive rates."
DISCUSSION,0.7153652392947103,"7
Discussion
189"
DISCUSSION,0.7178841309823678,"KMERF is, to the best of our knowledge, one of the ﬁrst learned kernel that is proven to be
190"
DISCUSSION,0.7204030226700252,"characteristic. The empirical experiments presented here illustrate the potential advantages of
191"
DISCUSSION,0.7229219143576826,"learning kernels, speciﬁcally for independence and k-sample testing.
192"
DISCUSSION,0.72544080604534,"In fact, multiscale graph correlation [38, 31] can be thought of, in a sense, as kernel learning: given n
193"
DISCUSSION,0.7279596977329975,"samples, and a pair of kernel or distance functions, it chooses one of the approximately n2 sparsiﬁed
194"
DISCUSSION,0.7304785894206549,"kernels, by excluding all but the nearest neighbors for each data point [38, 31]. Because random
195"
DISCUSSION,0.7329974811083123,"forest can be thought of as a nearest neighbor algorithm [17], in a sense, the forest induced kernel is
196"
DISCUSSION,0.7355163727959698,"a natural extension of Vogelstein et al. [38], which leads to far more data-adaptive estimates of the
197"
DISCUSSION,0.7380352644836272,"nearest neighbors using supervised information. Moreover, proving that the random-forest induced
198"
DISCUSSION,0.7405541561712846,"kernel is characteristic is a ﬁrst step towards building lifelong learning kernel machines with strong
199"
DISCUSSION,0.743073047858942,"theoretical guarantees [24, 39].
200"
DISCUSSION,0.7455919395465995,"As the choice of kernel is crucial for empirical performance, this manuscript offers a new kernel
201"
DISCUSSION,0.7481108312342569,"construction that is not only universally consistent for testing independence, but also exhibits strong
202"
DISCUSSION,0.7506297229219143,"empirical advantages, especially for high-dimensional testing. What is unique to this choice of kernel
203"
DISCUSSION,0.7531486146095718,"is the robustness and interpretability. It will be worthwhile to further understand the underlying
204"
DISCUSSION,0.7556675062972292,"theoretical mechanism of the induced characteristic kernel, as well as evaluating the performance
205"
DISCUSSION,0.7581863979848866,"of these forest induced kernels on other learning problems, including classiﬁcation, regression,
206"
DISCUSSION,0.760705289672544,"clustering, and embedding [26].
207"
REFERENCES,0.7632241813602015,"References
208"
REFERENCES,0.7657430730478589,"[1] S. Athey, J. Tibshirani, and S. Wager. Generalized random forests. Annals of Statistics, 47(2):
209"
REFERENCES,0.7682619647355163,"1148–1178, 2018.
210"
REFERENCES,0.7707808564231738,"[2] R. Blaser and P. Fryzlewicz. Random rotation ensembles. Journal of Machine Learning
211"
REFERENCES,0.7732997481108312,"Research, 17(4):1–26, 2016.
212"
REFERENCES,0.7758186397984886,"[3] L. Breiman. Random forests. Machine Learning, 4(1):5–32, October 2001.
213"
REFERENCES,0.7783375314861462,"[4] L. Breiman. Some inﬁnity theory for predictor ensembles. Journal of Combinatorial Theory,
214"
REFERENCES,0.7808564231738035,"Series A, 98:175–191, 2002.
215"
REFERENCES,0.783375314861461,"[5] R. Caruana and A. Niculescu-Mizil. An empirical comparison of supervised learning algorithms.
216"
REFERENCES,0.7858942065491183,"In Proceedings of the 23rd international conference on Machine learning, pages 161–168. ACM,
217"
REFERENCES,0.7884130982367759,"2006.
218"
REFERENCES,0.7909319899244333,"[6] R. Caruana, N. Karampatziakis, and A. Yessenalina. An empirical evaluation of supervised
219"
REFERENCES,0.7934508816120907,"learning in high dimensions. Proceedings of the 25th International Conference on Machine
220"
REFERENCES,0.7959697732997482,"Learning, 2008.
221"
REFERENCES,0.7984886649874056,"[7] A. Davies and Z. Ghahramani. The random forest kernel and creating other kernels for big data
222"
REFERENCES,0.801007556675063,"from random partitions. arXiv:1402.4293v1, 2014.
223"
REFERENCES,0.8035264483627204,"[8] L. Devroye, L. Györﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer,
224"
REFERENCES,0.8060453400503779,"1996.
225"
REFERENCES,0.8085642317380353,"[9] P. Good. Permutation, Parametric, and Bootstrap Tests of Hypotheses. Springer, 2005.
226"
REFERENCES,0.8110831234256927,"[10] A. Gretton, R. Herbrich, A. Smola, O. Bousquet, and B. Scholkopf. Kernel methods for
227"
REFERENCES,0.8136020151133502,"measuring independence. Journal of Machine Learning Research, 6:2075–2129, 2005.
228"
REFERENCES,0.8161209068010076,"[11] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, and A. Smola. A Kernel Two-Sample
229"
REFERENCES,0.818639798488665,"Test. Journal of Machine Learning Research, 13(25):723–773, 2012. ISSN 1533-7928.
230"
REFERENCES,0.8211586901763224,"[12] A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. M. Pontil, K. Fukumizu, and
231"
REFERENCES,0.8236775818639799,"B. K. Sriperumbudur. Optimal kernel choice for large-scale two-sample tests. In Advances in
232"
REFERENCES,0.8261964735516373,"neural information processing systems 25, pages 1205–1213, 2012.
233"
REFERENCES,0.8287153652392947,"[13] R. Heller, Y. Heller, and M. Gorﬁne. A consistent multivariate test of association based on ranks
234"
REFERENCES,0.8312342569269522,"of distances. Biometrika, 100(2):503–510, 2013.
235"
REFERENCES,0.8337531486146096,"[14] C. Huang and X. Huo. A statistically and numerically efﬁcient independence test based on
236"
REFERENCES,0.836272040302267,"random projections and distance covariance. arXiv, 2017.
237"
REFERENCES,0.8387909319899244,"[15] W. Jitkrittum, Z. Szabó, K. P. Chwialkowski, and A. Gretton. Interpretable distribution features
238"
REFERENCES,0.8413098236775819,"with maximum testing power. In Advances in Neural Information Processing Systems, pages
239"
REFERENCES,0.8438287153652393,"181–189, 2016.
240"
REFERENCES,0.8463476070528967,"[16] M. G. Kendall. Rank Correlation Methods. London: Grifﬁn, 1970.
241"
REFERENCES,0.8488664987405542,"[17] Y. Lin and Y. Jeon. Random forests and adaptive nearest neighbors. J. Am. Stat. Assoc., 101
242"
REFERENCES,0.8513853904282116,"(474):578–590, 2006.
243"
REFERENCES,0.853904282115869,"[18] R. Lyons. Distance covariance in metric spaces. Annals of Probability, 41(5):3284–3305, 2013.
244"
REFERENCES,0.8564231738035264,"[19] R. Lyons. Errata to “distance covariance in metric spaces”. Annals of Probability, 46(4):
245"
REFERENCES,0.8589420654911839,"2400–2405, 2018.
246"
REFERENCES,0.8614609571788413,"[20] S. Panda, S. Palaniappan, J. Xiong, E. W. Bridgeford, R. Mehta, C. Shen, and J. T. Vogelstein.
247"
REFERENCES,0.8639798488664987,"hyppo: A comprehensive multivariate hypothesis testing python package, 2020.
248"
REFERENCES,0.8664987405541562,"[21] S. Panda, C. Shen, R. Perry, J. Zorn, A. Lutz, C. E. Priebe, and J. T. Vogelstein. Nonpar manova
249"
REFERENCES,0.8690176322418136,"via independence testing, 2021.
250"
REFERENCES,0.871536523929471,"[22] K. Pearson. Notes on regression and inheritance in the case of two parents. Proceedings of the
251"
REFERENCES,0.8740554156171285,"Royal Society of London, 58:240–242, 1895.
252"
REFERENCES,0.8765743073047859,"[23] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
253"
REFERENCES,0.8790931989924433,"P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
254"
REFERENCES,0.8816120906801007,"M. Perrot, and Édouard Duchesnay. Scikit-learn: Machine learning in python. Journal of
255"
REFERENCES,0.8841309823677582,"Machine Learning Research, 12(85):2825–2830, 2011. URL http://jmlr.org/papers/
256"
REFERENCES,0.8866498740554156,"v12/pedregosa11a.html.
257"
REFERENCES,0.889168765743073,"[24] A. Pentina and S. Ben-David. Multi-task and lifelong learning of kernels. In Algorithmic
258"
REFERENCES,0.8916876574307305,"Learning Theory, pages 194–208. Springer International Publishing, 2015.
259"
REFERENCES,0.8942065491183879,"[25] A. Ramdas, S. J. Reddi, B. Póczos, A. Singh, and L. Wasserman. On the decreasing power of
260"
REFERENCES,0.8967254408060453,"kernel and distance based nonparametric hypothesis tests in high dimensions. In 29th AAAI
261"
REFERENCES,0.8992443324937027,"Conference on Artiﬁcial Intelligence, 2015.
262"
REFERENCES,0.9017632241813602,"[26] B. Schölkopf and A. J. Smola. Learning with kernels: support vector machines, regularization,
263"
REFERENCES,0.9042821158690176,"optimization, and beyond. MIT press, 2002.
264"
REFERENCES,0.906801007556675,"[27] D. Sejdinovic, A. Gretton, B. Sriperumbudur, and K. Fukumizu. Hypothesis testing using
265"
REFERENCES,0.9093198992443325,"pairwise distances and associated kernels (with appendix). arXiv preprint arXiv:1205.0411,
266"
REFERENCES,0.9118387909319899,"2012.
267"
REFERENCES,0.9143576826196473,"[28] D. Sejdinovic, B. Sriperumbudur, A. Gretton, and K. Fukumizu. Equivalence of distance-based
268"
REFERENCES,0.9168765743073047,"and rkhs-based statistics in hypothesis testing. Annals of Statistics, 41(5):2263–2291, 2013.
269"
REFERENCES,0.9193954659949622,"[29] C. Shen. Scaling up independence testing: Maximum marginal distance correlation and the
270"
REFERENCES,0.9219143576826196,"chi-square test. https://arxiv.org/abs/2001.01095, 2023.
271"
REFERENCES,0.924433249370277,"[30] C. Shen and J. T. Vogelstein. The exact equivalence of distance and kernel methods in hypothesis
272"
REFERENCES,0.9269521410579346,"testing. AStA Advances in Statistical Analysis, 105(3):385–403, 2021.
273"
REFERENCES,0.929471032745592,"[31] C. Shen, C. E. Priebe, and J. T. Vogelstein. From distance correlation to multiscale graph
274"
REFERENCES,0.9319899244332494,"correlation. Journal of the American Statistical Association, 115(529):280–291, 2020.
275"
REFERENCES,0.9345088161209067,"[32] C. Shen, S. Panda, and J. T. Vogelstein. The chi-square test of distance correlation. Journal of
276"
REFERENCES,0.9370277078085643,"Computational and Graphical Statistics, 31(1):254–262, 2022.
277"
REFERENCES,0.9395465994962217,"[33] C. Spearman. The Proof and Measurement of Association between Two Things. The American
278"
REFERENCES,0.9420654911838791,"Journal of Psychology, 15(1):72, 1904.
ISSN 00029556.
doi: 10.2307/1412159.
URL
279"
REFERENCES,0.9445843828715366,"http://www.jstor.org/stable/1412159?origin=crossref.
280"
REFERENCES,0.947103274559194,"[34] G. Szekely and M. Rizzo. Partial distance correlation with methods for dissimilarities. Annals
281"
REFERENCES,0.9496221662468514,"of Statistics, 42(6):2382–2412, 2014.
282"
REFERENCES,0.9521410579345088,"[35] G. Székely and M. L. Rizzo. The distance correlation t-test of independence in high dimension.
283"
REFERENCES,0.9546599496221663,"Journal of Multivariate Analysis, 117:193–213, 2013.
284"
REFERENCES,0.9571788413098237,"[36] G. Székely, M. L. Rizzo, and N. K. Bakirov. Measuring and testing independence by correlation
285"
REFERENCES,0.9596977329974811,"of distances. Annals of Statistics, 35(6):2769–2794, 2007.
286"
REFERENCES,0.9622166246851386,"[37] T. Tomita, J. Browne, C. Shen, J. Chung, J. Patsolic, B. Falk, J. Yim, C. E. Priebe, R. Burns,
287"
REFERENCES,0.964735516372796,"M. Maggioni, and J. T. Vogelstein. Sparse projection oblique randomer forests. Journal of
288"
REFERENCES,0.9672544080604534,"Machine Learning Research, 21(104):1–39, 2020.
289"
REFERENCES,0.9697732997481109,"[38] J. T. Vogelstein, Q. Wang, E. W. Bridgeford, C. E. Priebe, M. Maggioni, and C. Shen. Discover-
290"
REFERENCES,0.9722921914357683,"ing and deciphering relationships across disparate data modalities. eLife, 8:e41690, 2019.
291"
REFERENCES,0.9748110831234257,"[39] J. T. Vogelstein, J. Dey, H. S. Helm, W. LeVine, R. D. Mehta, T. M. Tomita, H. Xu, A. Geisa,
292"
REFERENCES,0.9773299748110831,"Q. Wang, G. M. van de Ven, C. Gao, W. Yang, B. Tower, J. Larson, C. M. White, and C. E.
293"
REFERENCES,0.9798488664987406,"Priebe. Representation ensembling for synergistic lifelong learning with quasilinear complexity,
294"
REFERENCES,0.982367758186398,"2023.
295"
REFERENCES,0.9848866498740554,"[40] E. A. Willemse, A. De Vos, E. M. Herries, U. Andreasson, S. Engelborghs, W. M. Van Der Flier,
296"
REFERENCES,0.9874055415617129,"P. Scheltens, D. Crimmins, J. H. Ladenson, E. Vanmechelen, et al. Neurogranin as cerebrospinal
297"
REFERENCES,0.9899244332493703,"ﬂuid biomarker for alzheimer disease: an assay comparison study. Clinical chemistry, 64(6):
298"
REFERENCES,0.9924433249370277,"927–937, 2018.
299"
REFERENCES,0.9949622166246851,"[41] J. Yang, F. K. Korley, M. Dai, and A. D. Everett. Serum neurogranin measurement as a
300"
REFERENCES,0.9974811083123426,"biomarker of acute traumatic brain injury. Clinical biochemistry, 48(13-14):843–848, 2015.
301"
