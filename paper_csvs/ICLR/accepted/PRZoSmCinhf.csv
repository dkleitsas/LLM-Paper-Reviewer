Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0021008403361344537,"Improving sample-efﬁciency and safety are crucial challenges when deploying
reinforcement learning in high-stakes real world applications.
We propose
LAMBDA, a novel model-based approach for policy optimization in safety
critical tasks modeled via constrained Markov decision processes. Our approach
utilizes Bayesian world models, and harnesses the resulting uncertainty to
maximize optimistic upper bounds on the task objective, as well as pessimistic
upper bounds on the safety constraints. We demonstrate LAMBDA’s state of the
art performance on the Safety-Gym benchmark suite in terms of sample efﬁciency
and constraint violation."
INTRODUCTION,0.004201680672268907,"1
INTRODUCTION"
INTRODUCTION,0.0063025210084033615,LAMBDA 0.0 0.5 1.0
INTRODUCTION,0.008403361344537815,Normalized metrics
INTRODUCTION,0.01050420168067227,CEM-MPC
INTRODUCTION,0.012605042016806723,PPO-Lagrangian
INTRODUCTION,0.014705882352941176,TRPO-Lagrangian CPO
INTRODUCTION,0.01680672268907563,"Task objective
Safety at the end of training
Safety during training"
INTRODUCTION,0.018907563025210083,"Figure 1:
Normalized perfor-
mance and safety metrics, aver-
aged across tasks of the Safety-
Gym (Ray et al., 2019) SG6
benchmark. LAMBDA achieves
constraint satisfaction in all tasks
of the SG6 benchmark while sig-
niﬁcantly improving performance
and sample efﬁciency.
See Ap-
pendix H for further details on
normalization."
INTRODUCTION,0.02100840336134454,"A central challenge in deploying reinforcement learning (RL)
agents in real-world systems is to avoid unsafe and harmful
situations (Amodei et al., 2016). We thus seek agents that can
learn efﬁciently and safely by acting cautiously and ensuring
the safety of themselves and their surroundings."
INTRODUCTION,0.023109243697478993,"A common paradigm in RL for modeling safety critical envi-
ronments are constrained Markov decision processes (CMDP)
(Altman, 1999). CMDPs augment the common notion of the
reward by an additional cost function, e.g., indicating unsafe
state-action pairs. By bounding the cost, one obtains bounds on
the probability of harmful events. For the tabular case, CMDPs
can be solved via Linear Programs (LP) (Altman, 1999). Most
prior work for solving non-tabular CMDPs, utilize model-free
algorithms (Chow et al., 2015; Achiam et al., 2017; Ray et al.,
2019; Chow et al., 2019).
Most notably, these methods are
typically shown to asymptotically converge to a constraint-
satisfying policy. However, similar to model-free approaches
in unconstrained MDPs, these approaches typically have a very
high sample complexity, i.e., require a large number of – poten-
tially harmful – interactions with the environment."
INTRODUCTION,0.025210084033613446,"A promising alternative to improve sample efﬁciency is to
use model-based reinforcement learning (MBRL) approaches
(Deisenroth & Rasmussen, 2011; Chua et al., 2018; Hafner et al., 2019a; Janner et al., 2019). In
particular, Bayesian approaches to MBRL quantify uncertainty in the estimated model (Depeweg
et al., 2017), which can be used to guide exploration, e.g., via the celebrated optimism in the face
of uncertainty principle (Brafman & Tennenholtz, 2003; Auer & Ortner, 2007; Curi et al., 2020a).
While extensively explored in the unconstrained setting, Bayesian model-based deep RL approaches
for solving general CMDPs remain largely unexplored. In this paper, we close this gap by proposing
LAMBDA, a Bayesian approach to model-based policy optimization in CMDPs. LAMBDA learns a
safe policy by experiencing unsafe as well as rewarding events through model-generated trajectories
instead of real ones. Our main contributions are as follows:"
INTRODUCTION,0.0273109243697479,"∗Correspondence to: yardas@ethz.ch
†Equal contribution."
INTRODUCTION,0.029411764705882353,Published as a conference paper at ICLR 2022
INTRODUCTION,0.031512605042016806,"(a) PointGoal1
(b) PointGoal2
(c) CarGoal1
(d) PointPush1
(e) PointButton1 (f) DoggoGoal1"
INTRODUCTION,0.03361344537815126,"Figure 2: Safety-Gym SG6 benchmark tasks. In our experiments, we use ﬁrst-person-view images of
size 64×64 pixels as observations. Green objects represent goals that should be reached by the robot.
Apart from the yellow box, that should be pushed to the goal area in the PointPush1 task, hitting all
other types of objects is considered an unsafe behavior. In Safety-Gym, stochasticity is achieved by
performing a random number of simulation steps before exposing the agent with a new observation."
INTRODUCTION,0.03571428571428571,"• We show that it is possible to perform constrained optimization by back-propagating gra-
dients of both task and safety objectives through the world-model. We use the Augmented
Lagrangian (Nocedal & Wright, 2006; Li et al., 2021) approach for constrained optimiza-
tion."
INTRODUCTION,0.037815126050420166,"• We harness the probabilistic world model to trade-off between optimism for exploration
and pessimism for safety."
INTRODUCTION,0.03991596638655462,"• We empirically show that LAMBDA successfully solves the SG6 benchmark tasks in
Safety-Gym (Ray et al., 2019) benchmark suite with ﬁrst-person-view observations as illus-
trated in Figure 2. Furthermore, we show that LAMBDA outperforms other model-based
and model-free methods in this benchmark."
RELATED WORK,0.04201680672268908,"2
RELATED WORK"
RELATED WORK,0.04411764705882353,"Interpretations of safety in RL research
We ﬁrst acknowledge that there exist different deﬁ-
nitions for safety (Garc´ıa et al., 2015). One deﬁnition uses reversible Markov decision processes
(Moldovan & Abbeel, 2012) that, informally, deﬁne safety as reachability between states and use
backup policies (Eysenbach et al., 2017) to ensure safety. Another deﬁnition adopts robust Markov
decision processes (Nilim & Ghaoui, 2005; Tamar et al., 2013; Tessler et al., 2019), which try to
maximize performance under the worst-case transition model. Finally, in this work we use non-
tabular CMDPs (Altman, 1999) that deﬁne the safety requirements as a cost function that should be
bounded by a predeﬁned threshold. Similarly, Achiam et al. (2017); Dalal et al. (2018); Ray et al.
(2019); Chow et al. (2019); Stooke et al. (2020); Bharadhwaj et al. (2021); Turchetta et al. (2021) use
non-tabular CMDPs as well, but utilize model-free techniques together with function approximators
to solve the constrained policy optimization problem. As we further demonstrate in our experiments,
it is possible to achieve better sample efﬁciency with model-based methods."
RELATED WORK,0.046218487394957986,"Safe model-based RL
A successful approach in applying Bayesian modeling to low-dimensional
continuous-control problems is to use Gaussian Processes (GP) for model learning.
Notably,
Berkenkamp et al. (2017) use GPs to construct conﬁdence intervals around Lyapunov functions
which are used to optimize a policy such that it is always within a Lyapunov-stable region of attrac-
tion. Furthermore, Koller et al. (2018); Wabersich & Zeilinger (2021) use GP models to certify the
safety of actions within a model predictive control (MPC) scheme. Likewise, Liu et al. (2021) apply
MPC for high-dimensional continuous-control problems together with ensembles of neural networks
(NN), the Cross Entropy Method (CEM) (Kroese et al., 2006) and rejection sampling to maximize
expected returns of safe action sequences. However, by planning online only for short horizons and
not using critics, this method can lead to myopic behaviors, as we later show in our experiments."
RELATED WORK,0.04831932773109244,Published as a conference paper at ICLR 2022
RELATED WORK,0.05042016806722689,"Lastly, similarly to this work, Zanger et al. (2021) use NNs and constrained model-based policy op-
timization. However, they do not use model uncertainty within an optimistic-pessimistic framework
but rather to limit the inﬂuence of erroneous model predictions on their policy optimization. Even
though using accurate model predictions can accelerate policy learning, this approach does not take
advantage of the epistemic uncertainty (e.g., through optimism and pessimism) when such accurate
predictions are rare."
RELATED WORK,0.052521008403361345,"Exploration, optimism and pessimism
Curi et al. (2021) and Derman et al. (2019) also take
a Bayesian optimistic-pessimistic perspective to ﬁnd robust policies. However, these approaches
do not use CMDPs and generally do not explicitly address safety. Similarly, Bharadhwaj et al.
(2021) apply pessimism for conservative safety critics, and use them for constrained model-free
policy optimization. Finally, Efroni et al. (2020) lay a theoretical foundation for the exploration-
exploitation dilemma and optimism in the setting of tabular CMPDs."
PRELIMINARIES,0.0546218487394958,"3
PRELIMINARIES"
SAFE MODEL-BASED REINFORCEMENT LEARNING,0.05672268907563025,"3.1
SAFE MODEL-BASED REINFORCEMENT LEARNING"
SAFE MODEL-BASED REINFORCEMENT LEARNING,0.058823529411764705,"Markov decision processes
We consider an episodic Markov decision process with discrete time
steps t ∈{0, . . . , T}. We deﬁne the environment’s state as st ∈Rn and an action taken by the
agent, as at ∈Rm. Each episode starts by sampling from the initial-state distribution s0 ∼ρ(s0).
After observing s0 and at each step thereafter, the agent takes an action by sampling from a policy
distribution at ∼π(·|st). The next state is then sampled from an unknown transition distribution
st+1 ∼p(·|st, at). Given a state, the agent observes a reward, generated by rt ∼p(·|st, at). We
deﬁne the performance of a pair of policy π and dynamics p as"
SAFE MODEL-BASED REINFORCEMENT LEARNING,0.06092436974789916,"J(π, p) = Eat∼π,st+1∼p,s0∼ρ "" T
X"
SAFE MODEL-BASED REINFORCEMENT LEARNING,0.06302521008403361,"t=0
rt
s0 # .
(1)"
SAFE MODEL-BASED REINFORCEMENT LEARNING,0.06512605042016807,"Constrained Markov decision processes
To make the agent adhere to human-deﬁned safety con-
straints, we adopt the constrained Markov decision process formalism of Altman (1999). In CMPDs,
alongside with the reward, the agent observes cost signals ci
t generated by ci
t ∼p(·|st, at) where
i = 1, . . . , C denote the distinct unsafe behaviors we want the agent to avoid. Given ci
t, we deﬁne
the constraints as"
SAFE MODEL-BASED REINFORCEMENT LEARNING,0.06722689075630252,"Ji(π, p) = Eat∼π,st+1∼p,s0∼ρ "" T
X"
SAFE MODEL-BASED REINFORCEMENT LEARNING,0.06932773109243698,"t=0
ci
t
s0 #"
SAFE MODEL-BASED REINFORCEMENT LEARNING,0.07142857142857142,"≤di, ∀i ∈{1, . . . , C},
(2)"
SAFE MODEL-BASED REINFORCEMENT LEARNING,0.07352941176470588,"where di are human-deﬁned thresholds. For example, a common cost function is ci(st) = 1st∈Hi,
where Hi is the set of harmful states (e.g., all the states in which a robot hits an obstacle). In this
case, the constraint (2) can be interpreted as a bound on the probability of visiting the harmful states.
Given the constraints, we aim to ﬁnd a policy that solves, for the true unknown dynamics p⋆,"
SAFE MODEL-BASED REINFORCEMENT LEARNING,0.07563025210084033,"max
π∈Π J(π, p⋆)
s.t. Ji(π, p⋆) ≤di, ∀i ∈{1, . . . , C}.
(3)"
SAFE MODEL-BASED REINFORCEMENT LEARNING,0.07773109243697479,"Model-based reinforcement learning
Model-based reinforcement learning revolves around the
repetition of an iterative process with three fundamental steps. First, the agent gathers observed
transitions (either by following an initial policy, or an ofﬂine dataset) of {st+1, st, at} into a dataset
D. Following that, the dataset is used to ﬁt a statistical model p(st+1|st, at, θ) that approximates
the true transition distribution p⋆. Finally, the agent uses the statistical model for planning, either
within an online MPC scheme or via ofﬂine policy optimization. In this work, we consider the cost
and reward functions as unknown and similarly to the transition distribution, we ﬁt statistical models
for them as well. Modeling the transition density allows us to cheaply generate synthetic sequences
of experience through the factorization p(sτ:τ+H|sτ−1, aτ−1:τ+H−1, θ) = Qτ+H
t=τ p(st+1|st, at, θ)
whereby H is a predeﬁned sequence horizon (see also Appendix I). Therefore, as already shown em-
pirically (Deisenroth & Rasmussen, 2011; Chua et al., 2018; Hafner et al., 2019b), MBRL achieves
superior sample efﬁciency compared to its model-free counterparts. We emphasize that in safety-
critical tasks, where human supervision during training might be required, sample efﬁciency is par-
ticularly important since it can reduce the need for human supervision."
SAFE MODEL-BASED REINFORCEMENT LEARNING,0.07983193277310924,Published as a conference paper at ICLR 2022
SAFE MODEL-BASED REINFORCEMENT LEARNING,0.0819327731092437,"Learning a world model
Aiming to tighten the gap between RL and real-world problems, we
relax the typical full observability assumption and consider problems where the agent receives an
observation ot ∼p(·|st) instead of st at each time step. To infer the transition density from ob-
servations, we base our world model on the Recurrent State Space Model (RSSM) introduced in
Hafner et al. (2019a). To solve this inference task, the RSSM approximates the posterior distri-
bution sτ:τ+H ∼qφ(·|oτ:τ+H, aτ−1:τ+H−1) via an inference network parameterized by φ. We
utilize the inference network to ﬁlter the latent state as new observations arrive, and use the in-
ferred latent state as the policy’s input. Furthermore, the RSSM models the predictive distribution
p(sτ:τ+H|sτ−1, aτ−1:τ+H−1, θ) as a differentiable function. This property allows us to perform
the constrained policy optimization by backpropagating gradients through the model. We highlight
that the only requirement for the world model is that it models p(sτ:τ+H|sτ−1, aτ−1:τ+H−1, θ) as
a differentiable function. Hence, it is possible to use other architectures for the world model, as long
as p(sτ:τ+H|sτ−1, aτ−1:τ+H−1, θ) is differentiable (e.g., see GP-models in Curi et al. (2020b))."
A BAYESIAN VIEW ON MODEL-BASED REINFORCEMENT LEARNING,0.08403361344537816,"3.2
A BAYESIAN VIEW ON MODEL-BASED REINFORCEMENT LEARNING"
A BAYESIAN VIEW ON MODEL-BASED REINFORCEMENT LEARNING,0.0861344537815126,"The Bayesian predictive distribution
A common challenge in MBRL is that policy optimization
can “overﬁt” by exploiting inaccuracies of the estimated model. A natural remedy is to quantify
uncertainty in the model, to identify in which situations the model is more likely to be wrong.
This is especially true for safety-critical tasks in which model inaccuracies can mislead the agent
into taking unsafe actions. A natural approach to uncertainty quantiﬁcation is to take a Bayesian
perspective, where we adopt a prior on the model parameters, and perform (approximate) Bayesian
inference to obtain the posterior. Given such a posterior distribution over the model parameters
p(θ|D), we marginalize over θ to get a Bayesian predictive distribution. Therefore, by maintaining
such posterior, p(sτ:τ+H|sτ−1, aτ−1:τ+H−1) is determined by the law of total probability
p(sτ:τ+H|sτ−1, aτ−1:τ+H−1) = Eθ∼p(θ|D) [p(sτ:τ+H|sτ−1, aτ−1:τ+H−1, θ)] .
(4)
Such Bayesian reasoning forms the basis of celebrated MBRL algorithms such as PETS (Chua et al.,
2018) and PILCO (Deisenroth & Rasmussen, 2011), among others."
A BAYESIAN VIEW ON MODEL-BASED REINFORCEMENT LEARNING,0.08823529411764706,"Maintaining a posterior over model parameters
Since exact Bayesian inference is typically
infeasible, we rely on approximations. In contrast to the popular approach of bootstrapped ensem-
bles (Lakshminarayanan et al., 2017) widely used in RL research, we use the Stochastic Weight
Averaging-Gaussian (SWAG) approximation (Maddox et al., 2019) of p(θ|D). Brieﬂy, SWAG con-
structs a posterior distribution over model parameters by performing moment-matching over iterates
of stochastic gradient decent (SGD) (Robbins, 2007). The main motivation behind this choice is the
lower computational and memory footprint of SWAG compared to bootstrapped ensembles. This
consideration is crucial when working with parameter-rich world models (such as the RSSM). Note
that our approach is ﬂexible and admits other variants of approximate Bayesian inference, such as
variational techniques (Graves, 2011; Kingma & Welling, 2014)."
A BAYESIAN VIEW ON MODEL-BASED REINFORCEMENT LEARNING,0.09033613445378151,"4
LAGRANGIAN MODEL-BASED AGENT (LAMBDA)"
A BAYESIAN VIEW ON MODEL-BASED REINFORCEMENT LEARNING,0.09243697478991597,"We ﬁrst present our proposed approach for planning, i.e., given a world model representing a CMDP,
how to use it to solve the CMPD. Following that, we present our proposed method for ﬁnding the
optimistic and pessimistic models with which the algorithm solves the CMDP before collecting
more data."
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.09453781512605042,"4.1
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.09663865546218488,"The Augmented Lagrangian
We ﬁrst consider the optimization problem in Equation (3). To ﬁnd
a policy that solves Equation (3), we take advantage of the Augmented Lagrangian with proximal
relaxation method as described by Nocedal & Wright (2006). First, we observe that"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.09873949579831932,"max
π∈Π min
λ≥0 """
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.10084033613445378,"J(π) − C
X"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.10294117647058823,"i=1
λi  
Ji(π) −di
#"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.10504201680672269,"= max
π∈Π"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.10714285714285714,"J(π)
if π is feasible
−∞
otherwise
(5)"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.1092436974789916,"is an equivalent form of Equation (3). Hereby λi are the Lagrange multipliers, each corresponding
to a safety constraint measured by Ji(π). In particular, if π is feasible, we have Ji(π) ≤di ∀i ∈"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.11134453781512606,Published as a conference paper at ICLR 2022
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.1134453781512605,"{1, . . . , C} and so the maximum over λi is satisﬁed if λi = 0 ∀i ∈{1, . . . , C}. Conversely, if π is
infeasible, at least one λi can be arbitrarily large to solve the problem in Equation (5). Particularly,
Equation (5) is non-smooth when π transitions between the feasibility and infeasibility sets. Thus,
in practice, we use the following relaxation:"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.11554621848739496,"max
π∈Π min
λ≥0 """
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.11764705882352941,"J(π) − C
X"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.11974789915966387,"i=1
λi  
Ji(π) −di
+ 1 µk C
X i=1"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.12184873949579832," 
λi −λi
k
2
# ,
(∗)"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.12394957983193278,"where µk is a non-decreasing penalty term corresponding to gradient step k. Note how the last term
in (∗) encourages λi to stay proximal to the previous estimate λi
k and as a consequence making (∗)
a smooth approximation of the left hand side term in Equation (5). Differentiating (∗) with respect
to λi and substituting back leads to the following update rule for the Lagrange multipliers:"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.12605042016806722,"∀i ∈{1, . . . , C} : λi
k+1 =
λi
k + µk(Ji(π) −di)
if λi
k + µk(Ji(π) −di) ≥0
0
otherwise
.
(6)"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.12815126050420167,We take gradient steps of the following unconstrained objective:
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.13025210084033614,"˜J(π; λk, µk) = J(π) − C
X"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.1323529411764706,"i=1
Ψ i(π; λi
k, µk),
(7) where"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.13445378151260504,"Ψ i(π; λi
k, µk) ="
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.13655462184873948,"(
λi
k(Ji(π) −di) + µk"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.13865546218487396,"2
 
Ji(π) −di2
if λi
k + µk(Ji(π) −di) ≥0"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.1407563025210084,"−(λi
k)2"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.14285714285714285,"2µk
otherwise.
(8)"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.14495798319327732,"Task and safety critics
For the task and safety critics, we use the reward value function v(st) and
the cost value function of each constraint vi(st) ∀i ∈{1, . . . , C}. We model v(st) and vi(st) as
neural networks with parameters ψ and ψi respectively. Note that we omit here the dependency of
the critics in π to reduce the notational clutter. As in Hafner et al. (2019b), we use TD(λ) (Sutton
& Barto, 2018) to trade-off the bias and variance of the critics with bootstrapping and Monte-Carlo
value estimation. To learn the task and safety critics, we minimize the following loss function"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.14705882352941177,"Lvπ
ψ(ψ) = Eπ,p(sτ:τ+H|sτ−1,aτ−1:τ+H−1,θ)"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.14915966386554622,"""
1
2H τ+H
X t=τ"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.15126050420168066," 
vπ
ψ(st) −Vλ(st)
2
# (9)"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.15336134453781514,"which uses the model to generate samples from p(sτ:τ+H|sτ−1, aτ−1:τ+H−1, θ). We denote vπ
ψ
as the neural network that approximates its corresponding value function and Vλ as the TD(λ)
value as presented in Hafner et al. (2019b). Similarly, Vi
λ is the TD(λ) value of the ith constraint
∀i ∈{1, . . . , C}. Note that, while we show here only the loss for the task critic, the loss function
for the safety critics is equivalent to Equation (9)."
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.15546218487394958,"Policy optimization
We model the policy as a Gaussian distribution via a neural network with
parameters ξ such that πξ(at|st) = N(at; NNµ
ξ (st), NNσ
ξ (st)). We sample a sequence sτ:τ+H ∼
p(sτ:τ+H|sτ−1, aτ−1:τ+H−1, θ), utilizing πξ to generate actions on the ﬂy (see Appendix I). To
estimate ˜J(π; λk, µk), we compute Vλ and Vi
λ for each state in the sampled sequence. This approx-
imation leads to the following loss function for policy learning"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.15756302521008403,"Lπξ(ξ) = Eπξ,p(sτ:τ+H|sτ−1,aτ−1:τ+H−1,θ) ""
1
H τ+H
X"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.15966386554621848,"t=τ
−Vλ(st) # + C
X"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.16176470588235295,"i=1
Ψ i(πξ; λi
k, µk).
(10)"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.1638655462184874,"We approximate Ji(πξ) in Ψ i(πξ; λi
k, µk) (recall Equation (8)) as"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.16596638655462184,"Ji(πξ) ≈Eπξ,p(sτ:τ+H|sτ−1,aτ−1:τ+H−1,θ) ""
1
H τ+H
X"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.16806722689075632,"t=τ
Vi
λ(st) #"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.17016806722689076,".
(11)"
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.1722689075630252,"Moreover, the ﬁrst expression in the policy’s loss (10) approximates J(πξ). We backpropagate
through p(sτ:τ+H|sτ−1, aτ−1:τ+H−1, θ) using path-wise gradient estimators (Mohamed et al.,
2020)."
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.17436974789915966,Published as a conference paper at ICLR 2022 θ1 θ2 θ3 θ4
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.17647058823529413,"θ5
Sample models,
generate trajectories."
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.17857142857142858,"Estimate the objective/constraints
for each model.
θ5
Choose largest estimate
as an upper bound."
SOLVING CMPDS WITH MODEL-BASED CONSTRAINED POLICY OPTIMIZATION,0.18067226890756302,"Figure 3: Posterior sampling: We sample j = 1, . . . , N models θj ∼p(θ|D) (e.g., in this illus-
tration, N = 5). For each model, we simulate trajectories that are conditioned on the same policy
and initial state. Objective and constraints: For a given posterior sample θj, we use the simulated
trajectories to estimate J(π, pθj) and Ji(π, pθj) ∀i ∈{1, . . . , C} with their corresponding critics.
Upper bounds: Choose largest estimate for each of J(π, pθj) and Ji(π, pθj) ∀i ∈{1, . . . , C}
among their N realizations."
ADOPTING OPTIMISM AND PESSIMISM TO EXPLORE IN CMDPS,0.18277310924369747,"4.2
ADOPTING OPTIMISM AND PESSIMISM TO EXPLORE IN CMDPS"
ADOPTING OPTIMISM AND PESSIMISM TO EXPLORE IN CMDPS,0.18487394957983194,"Optimistic and pessimistic models
As already noted by Curi et al. (2020a), greedily maximizing
the expected returns by averaging over posterior samples in Equation (4) is not necessarily the best
strategy for exploration. In particular, this greedy maximization does not deliberately leverage the
epistemic uncertainty to guide exploration. Therefore, driven by the concepts of optimism in the
face of uncertainty and upper conﬁdence reinforcement learning (UCRL) (Auer et al., 2009; Curi
et al., 2020a), we deﬁne a set of statistically plausible transition distributions denoted by P and let
pθ ∈P be a particular transition density in this set. Crucially, we assume that the true model p⋆is
within the support of P, and that by sampling θ ∼p(θ|D) the conditional predictive density satisﬁes
p(st+1|st, at, θ) = pθ ∈P. These assumptions lead us to the following constrained problem"
ADOPTING OPTIMISM AND PESSIMISM TO EXPLORE IN CMDPS,0.1869747899159664,"max
π∈Π max
pθ∈P J(π, pθ)
s.t.
max
pθi∈P Ji(π, pθi) ≤di, ∀i ∈{1, . . . , C}.
(12)"
ADOPTING OPTIMISM AND PESSIMISM TO EXPLORE IN CMDPS,0.18907563025210083,"The main intuition here is that jointly maximizing J(π, pθ) with respect to π and pθ can lead the
agent to try behaviors with potentially high reward due to optimism. On the other hand, the pes-
simism that arises through the inner maximization term is crucial to enforce the safety constraints.
Being only optimistic can easily lead to dangerous behaviors, while being pessimistic can lead the
agent to not explore enough. Consequently, we conjecture that optimizing for an optimistic task ob-
jective J(π, pθ) combined with pessimistic safety constraints Ji(π, pθi) allows the agent to explore
better task-solving behaviors while being robust to model uncertainties."
ADOPTING OPTIMISM AND PESSIMISM TO EXPLORE IN CMDPS,0.19117647058823528,"Estimating the upper bounds
We propose an algorithm that estimates the objective and con-
straints in Equation (12) through sampling, as illustrated in Figure 3. We demonstrate our method in
Algorithm 1. Importantly, we present Algorithm 1 only with the notation of the task critic. However,
we use it to approximate the model’s upper bound for each critic independently, i.e., for the costs as
well as the reward value function critics."
ADOPTING OPTIMISM AND PESSIMISM TO EXPLORE IN CMDPS,0.19327731092436976,Algorithm 1 Upper conﬁdence bounds estimation via posterior sampling
ADOPTING OPTIMISM AND PESSIMISM TO EXPLORE IN CMDPS,0.1953781512605042,"Require: N, p(θ|D), p(sτ:τ+H|sτ−1, aτ−1:τ+H−1, θ), sτ−1, π(at, |st)."
ADOPTING OPTIMISM AND PESSIMISM TO EXPLORE IN CMDPS,0.19747899159663865,"1: Initialize V = {}
# Set of objective estimates, under different posterior samples.
2: for j = 1 to N do
3:
θ ∼p(θ|D).
# Posterior sampling (e.g., via SWAG).
4:
sτ:τ+H ∼p(sτ:τ+H|sτ−1, aτ−1:τ+H−1, θ) .
# Sequence sampling, see Appendix I.
5:
Append V ←V ∪Pτ+H
t=τ Vλ(st).
6: end for
7: return max V."
ADOPTING OPTIMISM AND PESSIMISM TO EXPLORE IN CMDPS,0.19957983193277312,"The LAMBDA algorithm
Algorithm 2 describes how all the previously shown components in-
teract with each other to form a model-based policy optimization algorithm. For each update step,
we sample a batch of B sequences with length L from a replay buffer to train the model. Then, we
sample N models from the posterior and use them to generate novel sequences with horizon H from
every state in the replay buffer sampled sequences."
ADOPTING OPTIMISM AND PESSIMISM TO EXPLORE IN CMDPS,0.20168067226890757,Published as a conference paper at ICLR 2022
ADOPTING OPTIMISM AND PESSIMISM TO EXPLORE IN CMDPS,0.20378151260504201,Algorithm 2 LAMBDA
ADOPTING OPTIMISM AND PESSIMISM TO EXPLORE IN CMDPS,0.20588235294117646,"1: Initialize D by following a random policy or from an ofﬂine dataset.
2: while not converged do
3:
for u = 1 to U update steps do
4:
Sample B sequences {(aτ ′−1:τ ′+L−1, oτ ′:τ ′+L, rτ ′:τ ′+L, ci
τ ′:τ ′+L)} ∼D uniformly.
5:
Update model parameters θ and φ.
# E.g., see Hafner et al. (2019a) for the RSSM.
6:
Infer sτ ′:τ ′+L ∼qφ(·|oτ:τ+L, aτ ′−1:τ ′+L−1)."
ADOPTING OPTIMISM AND PESSIMISM TO EXPLORE IN CMDPS,0.20798319327731093,"7:
Compute Pτ+H
t=τ Vλ(st), Pτ+H
t=τ Vi
λ(st) via Algorithm 1. Use each state in sτ ′:τ ′+L as an
initial state for sequence generation.
8:
Update ψ and ψi via Equation (9) with Pτ+H
t=τ Vλ(st) and Pτ+H
t=τ Vi
λ(st)."
ADOPTING OPTIMISM AND PESSIMISM TO EXPLORE IN CMDPS,0.21008403361344538,"9:
Update ξ according to Equation (10) with Pτ+H
t=τ Vλ(st) and Pτ+H
t=τ Vi
λ(st).
10:
Update λi via Equations (6) and (11).
11:
end for
12:
for t = 1 to T do
13:
Infer st ∼qφ(·|ot, at−1, st−1).
14:
Sample at ∼πξ(·|st).
15:
Take action at, observe rt, ci
t, ot+1 received from the environment.
16:
end for
17:
Update dataset D ←D ∪

o1:T , a1:T , r1:T , ci
1:T
	
.
18: end while"
EXPERIMENTS,0.21218487394957983,"5
EXPERIMENTS"
EXPERIMENTS,0.21428571428571427,"We conduct our experiments with the SG6 benchmark as described by Ray et al. (2019), aiming to
answer the following questions:"
EXPERIMENTS,0.21638655462184875,"• How does our model-based approach compare to model-free variants in terms of perfor-
mance, sample efﬁciency and constraint violation?"
EXPERIMENTS,0.2184873949579832,"• What is the effect of replacing our proposed policy optimization method with an online
planning method? More speciﬁcally, how does LAMBDA’s policy compare to CEM-MPC
of Liu et al. (2021)?"
EXPERIMENTS,0.22058823529411764,"• How does our proposed optimism-pessimism formulation compare to greedy exploitation
in terms of performance, sample efﬁciency and constraint violation?"
EXPERIMENTS,0.22268907563025211,"We provide an open-source code for our experiments, including videos of the trained agents at
https://github.com/yardenas/la-mbda."
EXPERIMENTS,0.22478991596638656,"5.1
SG6 BENCHMARK"
EXPERIMENTS,0.226890756302521,"Experimental setup
In all of our experiments with LAMBDA, the agent observes 64×64 pixels
RGB images, taken from the robot’s point-of-view, as shown in Figure 2. Also, since there is only
one safety constraint, we let Jc(π) ≡J1(π). We measure performance with the following metrics,
as proposed in Ray et al. (2019):"
EXPERIMENTS,0.22899159663865545,• Average undiscounted episodic return for E episodes: ˆJ(π) = 1
EXPERIMENTS,0.23109243697478993,"E
PE
i=1
PTep
t=0 rt"
EXPERIMENTS,0.23319327731092437,• Average undiscounted episodic cost return for E episodes: ˆJc(π) = 1
EXPERIMENTS,0.23529411764705882,"E
PE
i=1
PTep
t=0 ct
• Normalized sum of costs during training, namely the cost regret: for a given number of
total interaction steps T, we deﬁne ρc(π) ="
EXPERIMENTS,0.23739495798319327,"PT
t=0 ct"
EXPERIMENTS,0.23949579831932774,"T
as the cost regret."
EXPERIMENTS,0.2415966386554622,"We compute ˆJ(π) and ˆJc(π) by averaging the sum of costs and rewards across E = 10 evaluation
episodes of length Tep = 1000, without updating the agent’s networks and discarding the interac-
tions made during evaluation. In contrast to the other metrics, to compute ρc(π), we sum the costs
accumulated during training and not evaluation episodes. The results for all methods are recorded
once the agent reached 1M environment steps for PointGoal1, PointGoal2, CarGoal1 and 2M steps"
EXPERIMENTS,0.24369747899159663,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.24579831932773108,PointGoal1
EXPERIMENTS,0.24789915966386555,PointGoal2
EXPERIMENTS,0.25,CarGoal1
EXPERIMENTS,0.25210084033613445,PointButton1
EXPERIMENTS,0.2542016806722689,PointPush1
EXPERIMENTS,0.25630252100840334,DoggoGoal1
EXPERIMENTS,0.25840336134453784,"0
2
4
¯J(π)"
EXPERIMENTS,0.2605042016806723,SG6 (average)
EXPERIMENTS,0.26260504201680673,"0.0
0.2
0.4
0.6
¯Jc(π)"
EXPERIMENTS,0.2647058823529412,"0.0
0.5
1.0
1.5
¯ρc(π)"
EXPERIMENTS,0.2668067226890756,"LAMBDA
CEM-MPC
PPO-Lagrangian
TRPO-Lagrangian
CPO"
EXPERIMENTS,0.2689075630252101,"Figure 4: Experimental results for the SG6 benchmark. As done in Ray et al. (2019), we normalize
the metrics and denote ¯J(π), ¯Jc(π), ¯ρc(π) as the normalized metrics (see also Appendix H). We
note that LAMBDA achieves better performance while satisfying the constraints during test time.
Furthermore, LAMBDA attains similar result to the baseline in terms of the cost regret metric. We
also note that CPO performs similarly to LAMBDA in solving some of the tasks but fails to satisfy
the constraints in all tasks."
EXPERIMENTS,0.2710084033613445,"for PointButton1, PointPush2, DoggoGoal1 environments respectively. To reproduce the scores of
TRPO-Lagrangian, PPO-Lagrangian and CPO, we follow the experimental protocol of Ray et al.
(2019). We give further details on our experiment with CEM-MPC at the ablation study section."
EXPERIMENTS,0.27310924369747897,"Practical aspects on runtime
The simulator’s integration step times are 0.002 and 0.004 seconds
for the Point and Car robots, making each learning task run roughly 30 and 60 minutes in real life re-
spectively. The simulator’s integration step time of the Doggo robot is 0.01 seconds thereby training
this task in real life would take about 167 minutes. These results show that in principle, the data ac-
quisition time in real life can be very short. However in practice, the main bottleneck is the gradient
step computation which takes roughly 0.5 seconds on a single unit of Nvidia GeForceRTX2080Ti
GPU. In total, it takes about 18 hours to train an agent for 1M interaction steps, assuming we take
100 gradient steps per episode. In addition, for the hyperparameters in Appendix C, we get a total
of 12M simulated interactions used for policy and value function learning per episode."
EXPERIMENTS,0.27521008403361347,"Results
The results of our experiments are summarized in Figure 4.
As shown in Figure 4,
LAMBDA is the only agent that satisﬁes the safety constraints in all of the SG6 tasks. Further-
more, thanks to its model-based policy optimization method, LAMBDA requires only 2M steps to
successfully solve the DoggoGoal1 task and signiﬁcantly outperform the other approaches. In Ap-
pendix B we examine further LAMBDA’s sample efﬁciency compared to the model-free baseline
algorithms. Additionally, in the PointGoal2 task, which is denser and harder to solve in terms of
safety, LAMBDA signiﬁcantly improves over the baseline algorithms in all metrics. Moreover, in
Figure 1 we compare LAMBDA’s ability to trade-off between the average performance and safety
metrics across all of the SG6 tasks. One main shortcoming of LAMBDA is visible in the Point-
Push1 environment where the algorithm fails to learn to push to box to the goal area. We attribute
this failure to the more strict partial observability of this task and further analyse it in Appendix D."
ABLATION STUDY,0.2773109243697479,"5.2
ABLATION STUDY"
ABLATION STUDY,0.27941176470588236,"Unsafe LAMBDA
As our ﬁrst ablation, we remove the second term in Equation (10) such that
the policy’s loss only comprises the task objective (i.e., with only optimistic exploration). We make
the following observations: (1) Both LAMBDA and unsafe LAMBDA solve the majority of the
SG6 tasks, depending on their level of partial observability; (2) in the PointButton1, PointGoal1
and CarGoal1, LAMBDA achieves the same performance of unsafe LAMBDA, while satisfying the
safety constraints; (3) LAMBDA is able to reach similar performance to unsafe LAMBDA in the
PointGoal2 task which is strictly harder than the other tasks in terms of safety, as shown in Figure 5.
We note that partial observability is present in all of the SG6 tasks due to the restricted ﬁeld of view"
ABLATION STUDY,0.2815126050420168,Published as a conference paper at ICLR 2022
ABLATION STUDY,0.28361344537815125,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps ×106 0 5 10 15 20"
ABLATION STUDY,0.2857142857142857,Average reward return
ABLATION STUDY,0.28781512605042014,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps ×106 0 100 200"
ABLATION STUDY,0.28991596638655465,Average cost return
ABLATION STUDY,0.2920168067226891,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps ×106 0.05 0.10 0.15 0.20 0.25"
ABLATION STUDY,0.29411764705882354,Cost regret
ABLATION STUDY,0.296218487394958,"CPO (10M steps, proprio)
PPO Lagrangian (10M steps, proprio)
TRPO Lagrangian (10M steps, proprio)
CPO (10M steps, proprio)
PPO Lagrangian (10M steps, proprio)
TRPO Lagrangian (10M steps, proprio)
LAMBDA
Unsafe LAMBDA"
ABLATION STUDY,0.29831932773109243,"Figure 5: Learning curves of LAMBDA and its unsafe version on the PointGoal2 environment. As
shown, LAMBDA exhibits similar performance in solving the task while maintaining the safety
constraint and signiﬁcantly improving over the baseline algorithms."
ABLATION STUDY,0.3004201680672269,"of the robot; i.e., the goal and obstacles are not always visible to the agent. In the DoggoGoal1 task,
where the agent does not observe any information about the joint angles, LAMBDA is still capable
of learning complex walking locomotion. However, in the PointPush1 task, LAMBDA struggles to
ﬁnd a task-solving policy, due to the harder partial observability of this task (see Appendix D). We
show the learning curves for this experiment in Appendix E."
ABLATION STUDY,0.3025210084033613,"Ablating policy optimization
Next, we replace our actor-critic procedure and instead of perform-
ing policy optimization, we implement the proposed MPC method of Liu et al. (2021). Speciﬁcally,
for their policy, Liu et al. (2021) suggest to use the CEM to maximize rewards over action sequences
while rejecting the unsafe ones. By utilizing the same world model, we are able to directly compare
the planning performance for both methods. As shown in Figure 11, LAMBDA performs consid-
erably better than CEM-MPC. We attribute the signiﬁcant performance difference to the fact that
the CEM-MPC approach does not use any critics for its policy, making its policy short-sighted and
limits its ability to address the credit assignment problem."
ABLATION STUDY,0.30462184873949577,"Optimism and pessimism compared to greedy exploitation
Finally, we compare our upper
bounds estimation procedure with the more common approach of greedily maximizing the expected
performance. More speciﬁcally, instead of employing Algorithm 1, we use Monte-Carlo sampling to
estimate Equation (4) by generating trajectories with N = 5 sampled models and taking the average
trajectory over the samples. By doing so, we get an estimate of the mean trajectory over sampled
models together with the intrinsic stochasticity of the environment. We report our experiment results
in Appendix G and note that LAMBDA is able to ﬁnd safer and more performant policies than its
greedy version."
CONCLUSIONS,0.3067226890756303,"6
CONCLUSIONS"
CONCLUSIONS,0.3088235294117647,"We introduce LAMBDA, a Bayesian model-based policy optimization algorithm that conforms
to human-speciﬁed safety constraints.
LAMBDA uses its Bayesian world model to generate
trajectories and estimate an optimistic bound for the task objective and pessimistic bounds for
the constraints. For policy search, LAMBDA uses the Augmented Lagrangian method to solve
the constrained optimization problem, based on the optimistic and pessimistic bounds.
In our
experiments we show that LAMBDA outperforms the baseline algorithms in the Safety-Gym
benchmark suite in terms of sample efﬁciency as well as safety and task-solving metrics. LAMBDA
learns its policy directly from observations in an end-to-end fashion and without prior knowledge.
However, we believe that introducing prior knowledge with respect to the safety speciﬁcations (e.g.,
the mapping between a state and its cost) can improve LAMBDA’s performance. By integrating
this prior knowledge, LAMBDA can potentially learn a policy that satisﬁes constraints only from its
model-generated experience and without ever violating the constraints in the real world. This leads
to a notable open question on how to integrate this prior knowledge within the world model such that
the safety constraints are satisﬁed during learning and not only at the end of the training process."
CONCLUSIONS,0.31092436974789917,Published as a conference paper at ICLR 2022
ACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING,0.3130252100840336,"7
ACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING"
ACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING,0.31512605042016806,"This project has received funding from the European Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation programme grant agreement No 815943, the Swiss
National Science Foundation under NCCR Automation, grant agreement 51NF40 180545 and the
Swiss National Science Foundation, under grant SNSF 200021 172781."
ACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING,0.3172268907563025,Published as a conference paper at ICLR 2022
REFERENCES,0.31932773109243695,REFERENCES
REFERENCES,0.32142857142857145,"Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization, 2017."
REFERENCES,0.3235294117647059,"E. Altman. Constrained Markov Decision Processes. Chapman and Hall, 1999."
REFERENCES,0.32563025210084034,"Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e. Con-
crete problems in ai safety, 2016."
REFERENCES,0.3277310924369748,"Peter Auer and Ronald Ortner. Logarithmic online regret bounds for undiscounted reinforcement
learning. In B. Sch¨olkopf, J. Platt, and T. Hoffman (eds.), Advances in Neural Information Pro-
cessing Systems, volume 19. MIT Press, 2007. URL https://proceedings.neurips.
cc/paper/2006/file/c1b70d965ca504aa751ddb62ad69c63f-Paper.pdf."
REFERENCES,0.32983193277310924,"Peter Auer,
Thomas Jaksch,
and Ronald Ortner.
Near-optimal regret bounds for re-
inforcement
learning.
In
D.
Koller,
D.
Schuurmans,
Y.
Bengio,
and
L.
Bottou
(eds.), Advances in Neural Information Processing Systems, volume 21. Curran Asso-
ciates, Inc., 2009. URL https://proceedings.neurips.cc/paper/2008/file/
e4a6222cdb5b34375400904f03d8e6a5-Paper.pdf."
REFERENCES,0.3319327731092437,"Felix Berkenkamp, Matteo Turchetta, Angela P. Schoellig, and Andreas Krause. Safe model-based
reinforcement learning with stability guarantees, 2017."
REFERENCES,0.33403361344537813,"Homanga Bharadhwaj, Aviral Kumar, Nicholas Rhinehart, Sergey Levine, Florian Shkurti, and Ani-
mesh Garg. Conservative safety critics for exploration, 2021."
REFERENCES,0.33613445378151263,"Ronen I. Brafman and Moshe Tennenholtz. R-max - a general polynomial time algorithm for near-
optimal reinforcement learning.
J. Mach. Learn. Res., 3(null):213–231, March 2003.
ISSN
1532-4435.
doi: 10.1162/153244303765208377.
URL https://doi.org/10.1162/
153244303765208377."
REFERENCES,0.3382352941176471,"Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained re-
inforcement learning with percentile risk criteria. CoRR, abs/1512.01629, 2015. URL http:
//arxiv.org/abs/1512.01629."
REFERENCES,0.3403361344537815,"Yinlam Chow, Oﬁr Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad
Ghavamzadeh. Lyapunov-based safe policy optimization for continuous control, 2019."
REFERENCES,0.34243697478991597,"Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine.
Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. CoRR, abs/1805.12114, 2018.
URL http://arxiv.org/abs/1805.12114."
REFERENCES,0.3445378151260504,"Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus), 2016."
REFERENCES,0.34663865546218486,"Sebastian Curi, Felix Berkenkamp, and Andreas Krause. Efﬁcient model-based reinforcement learn-
ing through optimistic policy search and planning, 2020a."
REFERENCES,0.3487394957983193,"Sebastian Curi, Silvan Melchior, Felix Berkenkamp, and Andreas Krause. Structured variational
inference in partially observable unstable gaussian process state space models. In Learning for
Dynamics and Control, pp. 147–157. PMLR, 2020b."
REFERENCES,0.35084033613445376,"Sebastian Curi, Ilija Bogunovic, and Andreas Krause. Combining pessimism with optimism for
robust and efﬁcient model-based deep reinforcement learning, 2021."
REFERENCES,0.35294117647058826,"Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval
Tassa. Safe exploration in continuous action spaces, 2018."
REFERENCES,0.3550420168067227,"Marc Peter Deisenroth and Carl Edward Rasmussen. Pilco: A model-based and data-efﬁcient ap-
proach to policy search. In Proceedings of the 28th International Conference on International
Conference on Machine Learning, ICML’11, pp. 465–472, Madison, WI, USA, 2011. Omnipress.
ISBN 9781450306195."
REFERENCES,0.35714285714285715,"Stefan Depeweg, Jos´e Miguel Hern´andez-Lobato, Finale Doshi-Velez, and Steffen Udluft. Decom-
position of uncertainty in bayesian deep learning for efﬁcient and risk-sensitive learning, 2017."
REFERENCES,0.3592436974789916,Published as a conference paper at ICLR 2022
REFERENCES,0.36134453781512604,"Esther Derman, Daniel Mankowitz, Timothy Mann, and Shie Mannor. A bayesian approach to
robust reinforcement learning, 2019."
REFERENCES,0.3634453781512605,"Yonathan Efroni, Shie Mannor, and Matteo Pirotta. Exploration-exploitation in constrained mdps,
2020."
REFERENCES,0.36554621848739494,"Benjamin Eysenbach, Shixiang Gu, Julian Ibarz, and Sergey Levine. Leave no trace: Learning to
reset for safe and autonomous reinforcement learning, 2017."
REFERENCES,0.36764705882352944,"Javier Garc´ıa, Fern, and o Fern´andez. A comprehensive survey on safe reinforcement learning.
Journal of Machine Learning Research, 16(42):1437–1480, 2015. URL http://jmlr.org/
papers/v16/garcia15a.html."
REFERENCES,0.3697478991596639,"Alex
Graves.
Practical
variational
inference
for
neural
networks.
In
J.
Shawe-
Taylor,
R. Zemel,
P. Bartlett,
F. Pereira,
and K. Q. Weinberger (eds.),
Advances
in
Neural
Information
Processing
Systems,
volume
24.
Curran
Associates,
Inc.,
2011.
URL
https://proceedings.neurips.cc/paper/2011/file/
7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf."
REFERENCES,0.37184873949579833,"Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algo-
rithms and applications, 2019."
REFERENCES,0.3739495798319328,"Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning, pp. 2555–2565, 2019a."
REFERENCES,0.3760504201680672,"Danijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and Mohammad Norouzi.
Dream to con-
trol: Learning behaviors by latent imagination. CoRR, abs/1912.01603, 2019b. URL http:
//arxiv.org/abs/1912.01603."
REFERENCES,0.37815126050420167,"Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson.
Averaging weights leads to wider optima and better generalization, 2019."
REFERENCES,0.3802521008403361,"Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization, 2019."
REFERENCES,0.38235294117647056,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014."
REFERENCES,0.38445378151260506,"Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2014."
REFERENCES,0.3865546218487395,"Torsten Koller, Felix Berkenkamp, Matteo Turchetta, and Andreas Krause. Learning-based model
predictive control for safe exploration. In 2018 IEEE Conference on Decision and Control (CDC),
pp. 6059–6066. IEEE, 2018."
REFERENCES,0.38865546218487396,"Dirk P. Kroese, Sergey Porotsky, and Reuven Y. Rubinstein. The cross-entropy method for con-
tinuous multi-extremal optimization.
Methodology and Computing in Applied Probability, 8
(3):383–407, Sep 2006. ISSN 1573-7713. doi: 10.1007/s11009-006-9753-0. URL https:
//doi.org/10.1007/s11009-006-9753-0."
REFERENCES,0.3907563025210084,"Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles, 2017."
REFERENCES,0.39285714285714285,"Jingqi
Li,
David
Fridovich-Keil,
Somayeh
Sojoudi,
and
Claire
J.
Tomlin.
Aug-
mented lagrangian method for instantaneously constrained reinforcement learning prob-
lems. 2021. URL https://people.eecs.berkeley.edu/˜sojoudi/Augmented_
Lagrangian_Constrained_RL.pdf."
REFERENCES,0.3949579831932773,"Zuxin Liu, Hongyi Zhou, Baiming Chen, Sicheng Zhong, Martial Hebert, and Ding Zhao. Con-
strained model-based reinforcement learning with robust cross-entropy method, 2021."
REFERENCES,0.39705882352941174,"Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, and Andrew Gordon Wilson. A
simple baseline for bayesian uncertainty in deep learning, 2019."
REFERENCES,0.39915966386554624,Published as a conference paper at ICLR 2022
REFERENCES,0.4012605042016807,"Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient esti-
mation in machine learning, 2020."
REFERENCES,0.40336134453781514,"Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in markov decision processes. CoRR,
abs/1205.4810, 2012. URL http://arxiv.org/abs/1205.4810."
REFERENCES,0.4054621848739496,"Vinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference on International Conference on Machine
Learning, ICML’10, pp. 807–814, Madison, WI, USA, 2010. Omnipress. ISBN 9781605589077."
REFERENCES,0.40756302521008403,"Arnab Nilim and Laurent El Ghaoui.
Robust Control of Markov Decision Processes
with Uncertain Transition Matrices.
Operations Research, 53(5):780–798, October 2005.
doi:
10.1287/opre.1050.0216.
URL https://ideas.repec.org/a/inm/oropre/
v53y2005i5p780-798.html."
REFERENCES,0.4096638655462185,"Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, New York, NY, USA,
second edition, 2006."
REFERENCES,0.4117647058823529,"Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking Safe Exploration in Deep Reinforce-
ment Learning. 2019."
REFERENCES,0.41386554621848737,"Herbert E. Robbins. A stochastic approximation method. Annals of Mathematical Statistics, 22:
400–407, 2007."
REFERENCES,0.41596638655462187,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms, 2017."
REFERENCES,0.4180672268907563,"Adam Stooke, Joshua Achiam, and Pieter Abbeel. Responsive safety in reinforcement learning by
pid lagrangian methods, 2020."
REFERENCES,0.42016806722689076,"Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. A Bradford
Book, Cambridge, MA, USA, 2018. ISBN 0262039249."
REFERENCES,0.4222689075630252,"Aviv Tamar, Huan Xu, and Shie Mannor. Scaling up robust mdps by reinforcement learning, 2013."
REFERENCES,0.42436974789915966,"Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and appli-
cations in continuous control, 2019."
REFERENCES,0.4264705882352941,"Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, and Alekh Agarwal. Safe rein-
forcement learning via curriculum induction, 2021."
REFERENCES,0.42857142857142855,"Kim P. Wabersich and Melanie N. Zeilinger. A predictive safety ﬁlter for learning-based control of
constrained nonlinear dynamical systems, 2021."
REFERENCES,0.43067226890756305,"Moritz A. Zanger, Karam Daaboul, and J. Marius Z¨ollner. Safe continuous control with constrained
model-based policy optimization, 2021."
REFERENCES,0.4327731092436975,Published as a conference paper at ICLR 2022
REFERENCES,0.43487394957983194,"A
LEARNING CURVES FOR THE SG6 BENCHMARK"
REFERENCES,0.4369747899159664,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0 10 20"
REFERENCES,0.43907563025210083,Average reward return
REFERENCES,0.4411764705882353,CarGoal1
REFERENCES,0.4432773109243697,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0 50 100"
REFERENCES,0.44537815126050423,Average cost return
REFERENCES,0.4474789915966387,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0.04 0.06 0.08"
REFERENCES,0.4495798319327731,Cost regret
REFERENCES,0.45168067226890757,"0.0
0.5
1.0
1.5
2.0
×106 0 2 4"
REFERENCES,0.453781512605042,Average reward return
REFERENCES,0.45588235294117646,PointPush1
REFERENCES,0.4579831932773109,"0.0
0.5
1.0
1.5
2.0
×106 0 25 50 75 100"
REFERENCES,0.46008403361344535,Average cost return
REFERENCES,0.46218487394957986,"0.0
0.5
1.0
1.5
2.0
×106 0.02 0.04 0.06 0.08"
REFERENCES,0.4642857142857143,Cost regret
REFERENCES,0.46638655462184875,"0.0
0.5
1.0
1.5
2.0
×106 0 10 20"
REFERENCES,0.4684873949579832,Average reward return
REFERENCES,0.47058823529411764,DoggoGoal1
REFERENCES,0.4726890756302521,"0.0
0.5
1.0
1.5
2.0
×106 0 50 100 150"
REFERENCES,0.47478991596638653,Average cost return
REFERENCES,0.47689075630252103,"0.0
0.5
1.0
1.5
2.0
×106 0.02 0.04 0.06 0.08"
REFERENCES,0.4789915966386555,Cost regret
REFERENCES,0.4810924369747899,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0 5 10 15"
REFERENCES,0.4831932773109244,Average reward return
REFERENCES,0.4852941176470588,PointGoal2
REFERENCES,0.48739495798319327,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0 50 100 150 200"
REFERENCES,0.4894957983193277,Average cost return
REFERENCES,0.49159663865546216,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0.05 0.10 0.15"
REFERENCES,0.49369747899159666,Cost regret
REFERENCES,0.4957983193277311,"0.0
0.5
1.0
1.5
2.0
×106 0 5 10 15"
REFERENCES,0.49789915966386555,Average reward return
REFERENCES,0.5,PointButton1
REFERENCES,0.5021008403361344,"0.0
0.5
1.0
1.5
2.0
×106 0 100 200 300"
REFERENCES,0.5042016806722689,Average cost return
REFERENCES,0.5063025210084033,"0.0
0.5
1.0
1.5
2.0
×106 0.05 0.10 0.15 0.20"
REFERENCES,0.5084033613445378,Cost regret
REFERENCES,0.5105042016806722,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps
×106 0 10 20"
REFERENCES,0.5126050420168067,Average reward return
REFERENCES,0.5147058823529411,PointGoal1
REFERENCES,0.5168067226890757,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps
×106 0 50 100 150 200"
REFERENCES,0.5189075630252101,Average cost return
REFERENCES,0.5210084033613446,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps
×106 0.04 0.06 0.08"
REFERENCES,0.523109243697479,Cost regret
REFERENCES,0.5252100840336135,"LAMBDA
CPO (proprio)
PPO Lagrangian (proprio)
TRPO Lagrangian (proprio)"
REFERENCES,0.5273109243697479,"Figure 6: Benchmark results of LAMBDA. Solid red lines indicate the threshold value d = 25.
Dashed lines correspond to the benchmark results for the baseline algorithms after 10M training
steps for all tasks except the DoggoGoal1, which is trained for 100M environment steps, as in Ray
et al. (2019). Shaded areas represent the 5% and 95% conﬁdence intervals across 5 different random
seeds."
REFERENCES,0.5294117647058824,Published as a conference paper at ICLR 2022
REFERENCES,0.5315126050420168,"B
SAMPLE EFFICIENCY"
REFERENCES,0.5336134453781513,"We record LAMBDA’s performance ˆJ(π) at the end of training and ﬁnd the average number of steps
(across seeds) required by the baseline model-free methods to match this performance. We demon-
strate LAMBDA’s sample efﬁciency with the ratio of the amount of steps required by the baseline
methods and amount of steps at the end of LAMBDA’s training. As shown in Figure 7, in the major-
ity of tasks, LAMBDA is substantially more sample efﬁcient. In the PointPush1 task LAMBDA is
outperformed by the baseline algorithms, however we assign this to the partial observability of this
task, as further analyzed in Appendix D. It is important to note that by taking LAMBDA’s perfor-
mance at the end of training, we take a conservative approach, as convergence can occur with many
less steps, as shown in Figure 6."
REFERENCES,0.5357142857142857,PointGoal1 1× 10×
REFERENCES,0.5378151260504201,Steps ratio
REFERENCES,0.5399159663865546,PointGoal2
REFERENCES,0.542016806722689,CarGoal1
REFERENCES,0.5441176470588235,PointButton1
REFERENCES,0.5462184873949579,PointPush1
REFERENCES,0.5483193277310925,DoggoGoal1
REFERENCES,0.5504201680672269,Required amount of steps to reach LAMBDA's performance
REFERENCES,0.5525210084033614,"PPO-Lagrangian
TRPO-Lagrangian
CPO
✓ ✘ ✘"
REFERENCES,0.5546218487394958,"✓
✘
✓
✘ ✘ ✓
✘ ✓ ✘
✘ ✘
✘ ✘
✘
✘"
REFERENCES,0.5567226890756303,"Figure 7: LAMBDA solves most of the tasks with signiﬁcantly less interactions with the envi-
ronment, compared to the baseline model-free methods. Green check marks and red ‘x’ indicate
constraint satisfaction after reaching the required number of steps."
REFERENCES,0.5588235294117647,Published as a conference paper at ICLR 2022
REFERENCES,0.5609243697478992,"C
HYPERPARAMETERS"
REFERENCES,0.5630252100840336,"In this section we specify the most prominent hyperparameters for LAMBDA, however we encour-
age the reader to visit https://github.com/yardenas/la-mbda as it holds many speciﬁc
(but important) implementation details. Table 1 summarizes the hyperparameters used in our algo-
rithm."
REFERENCES,0.5651260504201681,"World model
As mentioned before, for our world model we use the RSSM proposed in Hafner
et al. (2019a). Therefore, most of the architectural design choices (e.g., speciﬁc details of the con-
volutional layers) are based on the work there."
REFERENCES,0.5672268907563025,"Cost function
We exploit our prior knowledge of the cost function’s structure in the Safety-Gym
tasks. Since we know a-priori that the cost function is implemented as an indicator function, our
neural network approximation for it is modeled as a binary classiﬁer. As training progresses, the
dataset becomes less balanced as the agent observes less unsafe states. To deal with this issue, we
give higher weight to the unsafe interactions in the binray classiﬁcation loss."
REFERENCES,0.569327731092437,"Posterior over model parameters
in supervised learning, SWAG is typically used under the as-
sumption of a ﬁxed dataset D, such that weight averaging takes place only during the last few
training epochs. To adapt SWAG to the dataset’s distributional shifts that occur during training in
RL, we use an exponentially decaying running average. This allows us to maintain a posterior even
within early stages of learning, as opposed to the original design of SWAG that normally uses the
last few iterates of SGD to construct the posterior. Furthermore, we use a cyclic learning rate (Iz-
mailov et al., 2019; Maddox et al., 2019) to help SWAG span over different regions of the weight
space."
REFERENCES,0.5714285714285714,"Policy
The policy outputs actions such that each element in at is within the range [−1, 1]. How-
ever, since we model πξ(at|st) as N(at; NNµ
ξ (st), NNσ
ξ (st)), we perform squashing of the Gaus-
sian distribution into the range [−1, 1], as proposed in Haarnoja et al. (2019) and Hafner et al.
(2019b) by transforming it through a tangent hyperbolic bijector. We scale the actions in this way so
that the model, which takes the actions as an input is more easily optimized. Furthermore, to improve
numerical stability, the standard deviation term NNσ
ξ (st) is passed through a softplus function."
REFERENCES,0.5735294117647058,"Value functions
To ensure learning stability for the critics, we maintain a shadow instance for each
value function which is used in the bootstrapping in Equation (9). We clone the shadow instance
such that it lags U update steps behind its corresponding value function. Furthermore, we stop
gradient on Vλ when computing the loss function in Equation (9)."
REFERENCES,0.5756302521008403,"General
Parameter update for all of the neural networks is done with ADAM optimizer (Kingma
& Ba, 2014). We use ELU (Clevert et al., 2016) as activation function for all of the networks except
for the convolutional layers in the world model in which we use ReLU (Nair & Hinton, 2010)."
REFERENCES,0.5777310924369747,Published as a conference paper at ICLR 2022
REFERENCES,0.5798319327731093,"Table 1: Hyperparameters for LAMBDA. For other safety tasks, we recommend ﬁrst tuning the
initial Lagrangian, penalty and penalty power factor at different scales and then ﬁne-tune the safety
discount factor to improve constraint satisfaction. We emphasize that it is possible to improve the
performance of each task separately by ﬁne-tuning the hyperparameters on a per-task basis."
REFERENCES,0.5819327731092437,"Name
Symbol
Value
Additional"
REFERENCES,0.5840336134453782,World model
REFERENCES,0.5861344537815126,"Batch size
B
32
Sequence length
L
50
Learning rate
1e-4 SWAG"
REFERENCES,0.5882352941176471,"Burn-in steps
500
Steps before weight averaging starts.
Period steps
200
Use weights every ‘Period steps’ to update
weights running average.
Models
20
Averaging buffer length.
Decay
0.8
Exponential averaging decay factor.
Cyclic LR factor
5.0
End cyclic lr period with the base LR times
this factor.
Posterior samples
N
5"
REFERENCES,0.5903361344537815,Safety
REFERENCES,0.592436974789916,"Safety critic learning rate
2e-4
Initial penalty
µ0
5e-9
Initial Lagrangian
λ1
0
1e-6
Penalty power factor
1e-5
Multiply µk by this factor at each gradient
step.
Safety discount factor
0.995"
REFERENCES,0.5945378151260504,General
REFERENCES,0.5966386554621849,"Update steps
U
100
Critic learning rate
8e-5
Policy learning rate
8e-5
Action repeat
2
Repeat same action for this amount of steps.
Discount factor
0.99
TD(λ) factor
λ
0.95
Sequence generation horizon
H
15"
REFERENCES,0.5987394957983193,Published as a conference paper at ICLR 2022
REFERENCES,0.6008403361344538,"D
POINTPUSH1 ENVIRONMENT WITH A TRANSPARENT BOX"
REFERENCES,0.6029411764705882,"Figure 8: PointPush1 with partially transparent box. When the color of the box is solid LAMBDA
struggles in solving the task due to occlusion of the goal by the box. By changing the transparency
of the box, we make the PointPush1 task less partially observable and thus easier to solve."
REFERENCES,0.6050420168067226,"In all of our experiments with the PointPush1 task, we provide the agent image observations in
which the box is observed as a solid non-transparant object. As previously shown, we note that
in this setting, LAMBDA and unsafe LAMBDA fail to learn the task. We maintain that this failure
arises from the fact that this task is signiﬁcantly harder in terms of partial observability, as the goal is
occluded by the box while the robot pushes the box.1 Furthermore, in Ray et al. (2019), the authors
eliminate issues of partial observability by using what they term as “pseudo-LiDAR” which is not
susceptible to occlusions as it can see through objects. To verify our hypothesis, we change the
transparency of the box such that the goal is visible through it, as shown in Figure 8 and compare
LAMBDA’s performance with the previously tested PointPush1 task. We present the learning curves
of the experiments in Figure 9. As shown, LAMBDA is able to safely solve the PointPush1 task if"
REFERENCES,0.6071428571428571,"0.0
0.5
1.0
1.5
2.0
Training steps
×106 0 10"
REFERENCES,0.6092436974789915,Average reward return
REFERENCES,0.6113445378151261,"0.0
0.5
1.0
1.5
2.0
Training steps
×106 0 50 100 150"
REFERENCES,0.6134453781512605,Average cost return
REFERENCES,0.615546218487395,"0.0
0.5
1.0
1.5
2.0
Training steps
×106 0.02 0.04 0.06 0.08 0.10"
REFERENCES,0.6176470588235294,Cost regret
REFERENCES,0.6197478991596639,"Transparent box
Opaque box
CPO (10M steps, proprio)
PPO Lagrangian (10M steps, proprio)
TRPO Lagrangian (10M steps, proprio)"
REFERENCES,0.6218487394957983,"Figure 9: Learning curves for the PointPush1 task and ObservablePointPush1 task which uses par-
tially transparent box. We also show the baseline algorithms performance with a “pseudo-LiDAR”
observation."
REFERENCES,0.6239495798319328,"the goal is visible through the box. We conclude that the PointPush1 task makes an interesting test
case for future research on partially observable environments."
REFERENCES,0.6260504201680672,1Please see https://github.com/yardenas/la-mbda for a video illustration.
REFERENCES,0.6281512605042017,Published as a conference paper at ICLR 2022
REFERENCES,0.6302521008403361,"E
UNSAFE LAMBDA"
REFERENCES,0.6323529411764706,"0.0
0.5
1.0
1.5
2.0
×106 0 5 10"
REFERENCES,0.634453781512605,Average reward return
REFERENCES,0.6365546218487395,"PointButton1
PointButton1"
REFERENCES,0.6386554621848739,"0.0
0.5
1.0
1.5
2.0
×106 0 100 200 300"
REFERENCES,0.6407563025210085,Average cost return
REFERENCES,0.6428571428571429,"0.0
0.5
1.0
1.5
2.0
×106 0.1 0.2"
REFERENCES,0.6449579831932774,Cost regret
REFERENCES,0.6470588235294118,"0.0
0.5
1.0
1.5
2.0
×106 0 20 40 60"
REFERENCES,0.6491596638655462,Average reward return
REFERENCES,0.6512605042016807,"DoggoGoal1
DoggoGoal1"
REFERENCES,0.6533613445378151,"0.0
0.5
1.0
1.5
2.0
×106 0 50 100 150"
REFERENCES,0.6554621848739496,Average cost return
REFERENCES,0.657563025210084,"0.0
0.5
1.0
1.5
2.0
×106 0.04 0.06 0.08"
REFERENCES,0.6596638655462185,Cost regret
REFERENCES,0.6617647058823529,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0 5 10 15 20"
REFERENCES,0.6638655462184874,Average reward return
REFERENCES,0.6659663865546218,"PointGoal1
PointGoal1"
REFERENCES,0.6680672268907563,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0 50 100"
REFERENCES,0.6701680672268907,Average cost return
REFERENCES,0.6722689075630253,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0.02 0.04 0.06 0.08"
REFERENCES,0.6743697478991597,Cost regret
REFERENCES,0.6764705882352942,"0.0
0.5
1.0
1.5
2.0
×106 −5.0 −2.5 0.0 2.5"
REFERENCES,0.6785714285714286,Average reward return
REFERENCES,0.680672268907563,"PointPush1
PointPush1"
REFERENCES,0.6827731092436975,"0.0
0.5
1.0
1.5
2.0
×106 0 50 100"
REFERENCES,0.6848739495798319,Average cost return
REFERENCES,0.6869747899159664,"0.0
0.5
1.0
1.5
2.0
×106 0.02 0.04 0.06 0.08"
REFERENCES,0.6890756302521008,Cost regret
REFERENCES,0.6911764705882353,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0 5 10 15 20"
REFERENCES,0.6932773109243697,Average reward return
REFERENCES,0.6953781512605042,"CarGoal1
CarGoal1"
REFERENCES,0.6974789915966386,"0.0
0.2
0.4
0.6
0.8
1.0
×106 50 100"
REFERENCES,0.6995798319327731,Average cost return
REFERENCES,0.7016806722689075,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0.025 0.050 0.075 0.100 0.125"
REFERENCES,0.7037815126050421,Cost regret
REFERENCES,0.7058823529411765,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps
×106 0 5 10 15 20"
REFERENCES,0.707983193277311,Average reward return
REFERENCES,0.7100840336134454,"PointGoal2
PointGoal2"
REFERENCES,0.7121848739495799,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps
×106 0 100 200"
REFERENCES,0.7142857142857143,Average cost return
REFERENCES,0.7163865546218487,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps
×106 0.1 0.2 0.3"
REFERENCES,0.7184873949579832,Cost regret
REFERENCES,0.7205882352941176,"LAMBDA
Unsafe LAMBDA"
REFERENCES,0.7226890756302521,"Figure 10: Benchmark results of LAMBDA and its unsafe implementation. In the majority of the
tasks, LAMBDA is able to ﬁnd policies that perform similarly to the unsafe version while satisfying
the constraints. Interestingly, apart from the DoggoGoal1 and PointGoal2 tasks LAMBDA’s policies
are able to achieve similar returns while learning to satisfy the constraints."
REFERENCES,0.7247899159663865,Published as a conference paper at ICLR 2022
REFERENCES,0.726890756302521,"F
COMPARISON WITH CEM-MPC"
REFERENCES,0.7289915966386554,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0 5 10 15 20"
REFERENCES,0.7310924369747899,Average reward return
REFERENCES,0.7331932773109243,"CarGoal1
CarGoal1"
REFERENCES,0.7352941176470589,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0 50 100"
REFERENCES,0.7373949579831933,Average cost return
REFERENCES,0.7394957983193278,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0.04 0.06 0.08"
REFERENCES,0.7415966386554622,Cost regret
REFERENCES,0.7436974789915967,"0.0
0.5
1.0
1.5
2.0
×106 −1 0 1 2"
REFERENCES,0.7457983193277311,Average reward return
REFERENCES,0.7478991596638656,"PointPush1
PointPush1"
REFERENCES,0.75,"0.0
0.5
1.0
1.5
2.0
×106 0 25 50 75 100"
REFERENCES,0.7521008403361344,Average cost return
REFERENCES,0.7542016806722689,"0.0
0.5
1.0
1.5
2.0
×106 0.02 0.04 0.06 0.08"
REFERENCES,0.7563025210084033,Cost regret
REFERENCES,0.7584033613445378,"0.0
0.5
1.0
1.5
2.0
×106 0 5 10 15"
REFERENCES,0.7605042016806722,Average reward return
REFERENCES,0.7626050420168067,"DoggoGoal1
DoggoGoal1"
REFERENCES,0.7647058823529411,"0.0
0.5
1.0
1.5
2.0
×106 0 50 100 150"
REFERENCES,0.7668067226890757,Average cost return
REFERENCES,0.7689075630252101,"0.0
0.5
1.0
1.5
2.0
×106 0.02 0.04 0.06 0.08 0.10"
REFERENCES,0.7710084033613446,Cost regret
REFERENCES,0.773109243697479,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0 5 10 15"
REFERENCES,0.7752100840336135,Average reward return
REFERENCES,0.7773109243697479,"PointGoal2
PointGoal2"
REFERENCES,0.7794117647058824,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0 50 100 150 200"
REFERENCES,0.7815126050420168,Average cost return
REFERENCES,0.7836134453781513,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0.05 0.10 0.15"
REFERENCES,0.7857142857142857,Cost regret
REFERENCES,0.7878151260504201,"0.0
0.5
1.0
1.5
2.0
×106 0.0 2.5 5.0 7.5 10.0"
REFERENCES,0.7899159663865546,Average reward return
REFERENCES,0.792016806722689,"PointButton1
PointButton1"
REFERENCES,0.7941176470588235,"0.0
0.5
1.0
1.5
2.0
×106 0 100 200 300"
REFERENCES,0.7962184873949579,Average cost return
REFERENCES,0.7983193277310925,"0.0
0.5
1.0
1.5
2.0
×106 0.05 0.10 0.15 0.20"
REFERENCES,0.8004201680672269,Cost regret
REFERENCES,0.8025210084033614,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps
×106 0 5 10 15 20"
REFERENCES,0.8046218487394958,Average reward return
REFERENCES,0.8067226890756303,"PointGoal1
PointGoal1"
REFERENCES,0.8088235294117647,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps
×106 0 50 100 150 200"
REFERENCES,0.8109243697478992,Average cost return
REFERENCES,0.8130252100840336,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps
×106 0.02 0.04 0.06 0.08"
REFERENCES,0.8151260504201681,Cost regret
REFERENCES,0.8172268907563025,"LAMBDA
CEM-MPC"
REFERENCES,0.819327731092437,"Figure 11: Comparison of LAMBDA when ablating the policy optimization and using CEM-MPC
with rejection sampling as introduced in Liu et al. (2021). As shown, LAMBDA performs substan-
tially better than CEM-MPC. We believe that when the goal is not visible to the agent, CEM-MPC’s
policy fails to locate it and drive the robot to it. On the contrary, in our experiments, we observed
that LAMBDA typically rotates until the goal becomes visible, thus allowing the robot to gather
signiﬁcantly more goals."
REFERENCES,0.8214285714285714,Published as a conference paper at ICLR 2022
REFERENCES,0.8235294117647058,"G
OPTIMISM AND PESSIMISM COMPARED TO GREEDY EXPLOITATION"
REFERENCES,0.8256302521008403,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0 5 10 15 20"
REFERENCES,0.8277310924369747,Average reward return
REFERENCES,0.8298319327731093,"CarGoal1
CarGoal1"
REFERENCES,0.8319327731092437,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0 50 100"
REFERENCES,0.8340336134453782,Average cost return
REFERENCES,0.8361344537815126,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0.04 0.06 0.08 0.10"
REFERENCES,0.8382352941176471,Cost regret
REFERENCES,0.8403361344537815,"0.0
0.5
1.0
1.5
2.0
×106 −4 −2 0 2"
REFERENCES,0.842436974789916,Average reward return
REFERENCES,0.8445378151260504,"PointPush1
PointPush1"
REFERENCES,0.8466386554621849,"0.0
0.5
1.0
1.5
2.0
×106 0 25 50 75 100"
REFERENCES,0.8487394957983193,Average cost return
REFERENCES,0.8508403361344538,"0.0
0.5
1.0
1.5
2.0
×106 0.02 0.04 0.06 0.08"
REFERENCES,0.8529411764705882,Cost regret
REFERENCES,0.8550420168067226,"0.0
0.5
1.0
1.5
2.0
×106 0 5 10 15 20"
REFERENCES,0.8571428571428571,Average reward return
REFERENCES,0.8592436974789915,"DoggoGoal1
DoggoGoal1"
REFERENCES,0.8613445378151261,"0.0
0.5
1.0
1.5
2.0
×106 0 50 100 150"
REFERENCES,0.8634453781512605,Average cost return
REFERENCES,0.865546218487395,"0.0
0.5
1.0
1.5
2.0
×106 0.02 0.04 0.06"
REFERENCES,0.8676470588235294,Cost regret
REFERENCES,0.8697478991596639,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0 5 10 15"
REFERENCES,0.8718487394957983,Average reward return
REFERENCES,0.8739495798319328,"PointGoal2
PointGoal2"
REFERENCES,0.8760504201680672,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0 50 100 150 200"
REFERENCES,0.8781512605042017,Average cost return
REFERENCES,0.8802521008403361,"0.0
0.2
0.4
0.6
0.8
1.0
×106 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.8823529411764706,Cost regret
REFERENCES,0.884453781512605,"0.0
0.5
1.0
1.5
2.0
×106 0.0 2.5 5.0 7.5 10.0"
REFERENCES,0.8865546218487395,Average reward return
REFERENCES,0.8886554621848739,"PointButton1
PointButton1"
REFERENCES,0.8907563025210085,"0.0
0.5
1.0
1.5
2.0
×106 0 100 200 300"
REFERENCES,0.8928571428571429,Average cost return
REFERENCES,0.8949579831932774,"0.0
0.5
1.0
1.5
2.0
×106 0.05 0.10 0.15 0.20"
REFERENCES,0.8970588235294118,Cost regret
REFERENCES,0.8991596638655462,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps
×106 0 5 10 15 20"
REFERENCES,0.9012605042016807,Average reward return
REFERENCES,0.9033613445378151,"PointGoal1
PointGoal1"
REFERENCES,0.9054621848739496,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps
×106 0 50 100 150 200"
REFERENCES,0.907563025210084,Average cost return
REFERENCES,0.9096638655462185,"0.0
0.2
0.4
0.6
0.8
1.0
Training steps
×106 0.04 0.06 0.08"
REFERENCES,0.9117647058823529,Cost regret
REFERENCES,0.9138655462184874,"LAMBDA
Greedy LAMBDA"
REFERENCES,0.9159663865546218,"Figure 12: Comparison of LAMBDA and greedy exploitation. LAMBDA generally solves that
tasks with better performance and with improved sample efﬁciency. Notably, the greedy exploitation
variant fails to solve the the PointButton1 task."
REFERENCES,0.9180672268907563,Published as a conference paper at ICLR 2022
REFERENCES,0.9201680672268907,"H
COMPARING ALGORITHMS IN THE SG6 BENCHMARK"
REFERENCES,0.9222689075630253,"To get a summary of how different algorithms behave across all of the SG6 tasks, we follow the
proposed protocol in Ray et al. (2019). That is, we obtain “characteristic metrics” for each of the
environments by taking the metrics recorded at the end of training of an unconstrained PPO agent
(Schulman et al., 2017). We denote these metrics as ˆJPPO, ˆJPPO
c
and ˆρPPO
c
. We then normalize the
recorded metrics for each environment as follows:"
REFERENCES,0.9243697478991597,"¯J(π) =
ˆJ(π)
ˆJPPO"
REFERENCES,0.9264705882352942,"¯Jc(π) =
max(0, ˆJc(π) −d)"
REFERENCES,0.9285714285714286,"max(10−6, ˆJPPO
c
−d)"
REFERENCES,0.930672268907563,¯ρc(π) = ρc(π)
REFERENCES,0.9327731092436975,"ρPPO
c
. (13)"
REFERENCES,0.9348739495798319,"By performing this normalization with respect to the performance of PPO, we can take an average
of each metric across all of the SG6 environments."
REFERENCES,0.9369747899159664,"To produce Figure 1, we scale the normalized metrics to [0, 1], such that for each metric, the best
performing algorithm attains a score of 1.0 and the worst performing algorithm attains a score of 0."
REFERENCES,0.9390756302521008,Published as a conference paper at ICLR 2022
REFERENCES,0.9411764705882353,"I
BACKPROPAGATING GRADIENTS THROUGH A SEQUENCE"
REFERENCES,0.9432773109243697,"Algorithm 3 Sampling from the predictive density pθ(sτ:τ+H|sτ−1, aτ−1:τ+H−1, θ)"
REFERENCES,0.9453781512605042,"Require: πξ(at|st), pθ(st+1|st, at), sτ−1"
REFERENCES,0.9474789915966386,"1: for t = τ −1 to τ + H do
2:
at ∼πξ(·|stop gradient(st))
# Stop gradient from st when conditioning the policy on it.
3:
st+1 ∼p(·|st, at, θ)
4: end for
5: return sτ:τ+H"
REFERENCES,0.9495798319327731,"We use the reparametrization trick (Kingma & Welling, 2014) to compute gradients through sam-
pling procedures as both πξ and pθ are modeled as normal distributions. Backpropagating gradients
through the model can be easily implemented with modern automatic differentiation tools."
REFERENCES,0.9516806722689075,"Importantly, we stop the gradient computation in Algorithm 3 when conditioning the policy on
st−1. We do so to avoid any recurrent connections between an action at−1 and the preceding states
to st such that eventually, backpropagation to actions occurs only from their dependant succeeding
values. We further illustrate this in Figure 13."
REFERENCES,0.9537815126050421,"st−1
st
st+1"
REFERENCES,0.9558823529411765,"vt−1
vt
vt+1"
REFERENCES,0.957983193277311,"at
at−1"
REFERENCES,0.9600840336134454,"...
..."
REFERENCES,0.9621848739495799,"(a) Forward pass of trajectory sampling. We stop gra-
dient ﬂow on the magenta dashed arrows."
REFERENCES,0.9642857142857143,"st−1
st
st+1"
REFERENCES,0.9663865546218487,"vt−1
vt
vt+1"
REFERENCES,0.9684873949579832,"at
at−1"
REFERENCES,0.9705882352941176,"...
..."
REFERENCES,0.9726890756302521,"(b) Backward pass from values to their inducing ac-
tions."
REFERENCES,0.9747899159663865,Figure 13: Computational graphs for the backward and forward passes of Algorithm 3.
REFERENCES,0.976890756302521,Published as a conference paper at ICLR 2022
REFERENCES,0.9789915966386554,"J
SCORES FOR SG6"
REFERENCES,0.9810924369747899,"The .json format ﬁles, summarizing the scores for the experiments of this work are available at
https://github.com/yardenas/la-mbda."
REFERENCES,0.9831932773109243,Table 2: LAMBDA’s unnormalized scores for the SG6 tasks.
REFERENCES,0.9852941176470589,"ˆJ(π)
ˆJc(π)
ρc(π)"
REFERENCES,0.9873949579831933,"PointGoal1
18.822
11.200
0.034
PointGoal2
13.300
9.100
0.043
CarGoal1
16.745
23.100
0.036
PointPush1
0.314
21.400
0.017
PointButton1
5.372
21.700
0.038
DoggoGoal1
5.867
11.400
0.046"
REFERENCES,0.9894957983193278,"Average
10.07
16.317
0.0360"
REFERENCES,0.9915966386554622,"Table 3: Experiment results for the SG6 benchmark.
We present the results with the tuple
( ¯J(π), ¯Jc(π), ¯ρc(π)) of the normalized metrics."
REFERENCES,0.9936974789915967,"TRPO-Lagrangian
PPO-Lagrangian
CPO
LAMBDA"
REFERENCES,0.9957983193277311,"PointGoal1
0.51, 0.004, 0.405
0.24, 0.0, 0.419
0.898, 0.302, 0.599
1.077, 0.0, 0.483
PointGoal2
0.119, 0.059, 0.304
0.09, 0.197, 0.349
0.306, 0.132, 0.377
0.902, 0.0, 0.229
CarGoal1
0.501, 0.0, 0.522
0.255, 0.0, 0.474
1.579, 0.604, 0.924
1.284, 0.0, 0.704
PointPush1
0.714, 0.0, 0.315
0.185, 0.0, 0.249
1.606, 0.311, 0.687
0.203, 0.0, 0.309
PointButton1
0.077, 0.0, 0.223
0.058, 0.0, 0.242
0.516, 0.343, 0.495
0.287, 0.0, 0.302
DoggoGoal1
-1.257, 0.227, 0.624 -0.891, 0.293, 0.707 -0.723, 0.643, 0.769 5.400, 0.0, 0.770"
REFERENCES,0.9978991596638656,"SG6 (average) 0.111, 0.048, 0.399
-0.011, 0.082, 0.407 0.697, 0.389, 0.642
1.526, 0.0, 0.466"
