Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0022271714922048997,"Black-box variational inference is a widely-used framework for Bayesian posterior
1"
ABSTRACT,0.004454342984409799,"inference, but in some cases suffers from high variance in gradient estimates, harm-
2"
ABSTRACT,0.0066815144766146995,"ing accuracy and efﬁciency. This variance comes from two sources of randomness:
3"
ABSTRACT,0.008908685968819599,"Data subsampling and Monte Carlo sampling. Whereas existing control variates
4"
ABSTRACT,0.011135857461024499,"only address Monte Carlo noise and incremental gradient methods typically only
5"
ABSTRACT,0.013363028953229399,"address data subsampling, we propose a new ""dual"" control variate capable of
6"
ABSTRACT,0.015590200445434299,"jointly reducing variance from both sources of noise. We conﬁrm that this leads to
7"
ABSTRACT,0.017817371937639197,"reduced variance and improved optimization in several real-world applications.
8"
INTRODUCTION,0.0200445434298441,"1
Introduction
9"
INTRODUCTION,0.022271714922048998,"Black-box variational inference (BBVI) [12, 22, 16, 2] has become a popular alternative to Markov
10"
INTRODUCTION,0.024498886414253896,"Chain Monte Carlo (MCMC) methods. The idea is to posit a variational family and optimize it to be
11"
INTRODUCTION,0.026726057906458798,"close to the posterior, using only ""black-box"" access to the target model (evaluations of the density
12"
INTRODUCTION,0.028953229398663696,"or gradient). This is done by estimating a stochastic gradient of the KL-divergence and deploying
13"
INTRODUCTION,0.031180400890868598,"it in stochastic optimization. A key advantage of this procedure is that it allows the use of data
14"
INTRODUCTION,0.0334075723830735,"subsampling in each iteration, which can greatly speed-up optimization with large datasets.
15"
INTRODUCTION,0.035634743875278395,"The optimization of BBVI is often described as a doubly-stochastic optimization problem [30, 27] in
16"
INTRODUCTION,0.0378619153674833,"that BBVI’s gradient estimation involves two sources of randomness: Monte Carlo sampling from
17"
INTRODUCTION,0.0400890868596882,"the variational posterior and data subsampling from the full dataset. Because of the doubly-stochastic
18"
INTRODUCTION,0.042316258351893093,"nature, one common challenge for BBVI is the variance of the gradient estimates: If this is very high,
19"
INTRODUCTION,0.044543429844097995,"it forces very small stepsizes, leading to slow optimization convergence [20, 3].
20"
INTRODUCTION,0.0467706013363029,"Numerous works have been devoted to reducing the ""Monte Carlo"" noise that results from drawing
21"
INTRODUCTION,0.04899777282850779,"samples from the current variational distribution [19, 25, 8, 9, 4]. These methods can typically be
22"
INTRODUCTION,0.051224944320712694,"seen as creating an approximation of the objective for which the Monte Carlo noise can be integrated
23"
INTRODUCTION,0.053452115812917596,"exactly, and using this to deﬁne a zero-mean random variable, i.e. a control variate, that is negatively
24"
INTRODUCTION,0.0556792873051225,"correlated with the original gradient estimator. These methods can be used with subsampling by
25"
INTRODUCTION,0.05790645879732739,"creating approximations for each datum. However, they are only able to reduce Monte Carlo noise
26"
INTRODUCTION,0.060133630289532294,"for each datum—they do not reduce subsampling noise. This is critical, as subsampling noise is often
27"
INTRODUCTION,0.062360801781737196,"the dominant source of gradient variance (Sec. 3).
28"
INTRODUCTION,0.0645879732739421,"At the same time, for (non-BBVI) optimization problems with only subsampling noise, the opti-
29"
INTRODUCTION,0.066815144766147,"mization community has developed incremental gradient methods that ""recycle"" previous gradient
30"
INTRODUCTION,0.06904231625835189,"evaluations [26, 28, 13, 6, 7], leading to faster convergence. These methods do not address Monte
31"
INTRODUCTION,0.07126948775055679,"Carlo noise. In fact, due to the way these methods rely on efﬁciently maintaining running averages,
32"
INTRODUCTION,0.07349665924276169,"they cannot typically be applied to doubly-stochastic problems at all.
33"
INTRODUCTION,0.0757238307349666,"In this paper, we present a method that jointly controls Monte Carlo and subsampling noise in BBVI.
34"
INTRODUCTION,0.0779510022271715,"The idea is to create approximations of the target for each datum where the Monte Carlo noise
35"
INTRODUCTION,0.0801781737193764,"can be integrated exactly. Then, we maintain running averages of the approximate gradients, with
36"
INTRODUCTION,0.08240534521158129,"noise integrated, overcoming the issue of applying incremental gradient ideas to doubly-stochastic
37"
INTRODUCTION,0.08463251670378619,"problems. The resulting method not only addresses both forms of noise but interactions between
38"
INTRODUCTION,0.08685968819599109,"them as well. We demonstrate through a series of experiments with diagonal Gaussian variational
39"
INTRODUCTION,0.08908685968819599,"inference on a range of probabilistic models that the method leads to lower variance and signiﬁcantly
40"
INTRODUCTION,0.09131403118040089,"faster convergence than existing methods.
41"
INTRODUCTION,0.0935412026726058,"2
Background: Black-box variational inference
42"
INTRODUCTION,0.0957683741648107,"Given a probabilistic model p(x, z) = QN"
INTRODUCTION,0.09799554565701558,"n=1 p(xn | z)p(z) and observed data {x1, . . . , xN},
43"
INTRODUCTION,0.10022271714922049,"variational inference’s goal is to ﬁnd a tractable distribution qw(z) with parameters w to approximate
44"
INTRODUCTION,0.10244988864142539,"the (often intractable) posterior p(z | x) over the latent variable z 2 Rd. BBVI achieves this by
45"
INTRODUCTION,0.10467706013363029,"ﬁnding the set of parameters w that minimize the KL-divergence from qw(z) to p(z | x), which is
46"
INTRODUCTION,0.10690423162583519,"equivalent to minimizing the negative Evidence Lower Bound (ELBO)
47"
INTRODUCTION,0.1091314031180401,f (w) = −E
INTRODUCTION,0.111358574610245,"n
E
qw(z) "
INTRODUCTION,0.11358574610244988,N log p(xn | z) + log p(z) #
INTRODUCTION,0.11581291759465479,"−H(w),
(1)"
INTRODUCTION,0.11804008908685969,"where H(w) denotes the entropy of qw.
48"
INTRODUCTION,0.12026726057906459,"Since the inner expectation with respect to z is typically intractable, BBVI methods rely on stochastic
49"
INTRODUCTION,0.12249443207126949,"optimization with unbiased gradient estimates. These gradient estimates are typically obtained using
50"
INTRODUCTION,0.12472160356347439,"the score function method [33] or the reparameterization trick [15, 23, 30]. The latter is often the
51"
INTRODUCTION,0.12694877505567928,"method of choice, as it usually seems to yield estimators with lower variance. The idea is to deﬁne a
52"
INTRODUCTION,0.1291759465478842,"ﬁxed base distribution s(✏) and a deterministic transformation Tw(✏) such that for ✏⇠s, Tw(✏) is
53"
INTRODUCTION,0.13140311804008908,"equal in distribution to qw. Then, the objective from Equation (1) can be re-written as
54"
INTRODUCTION,0.133630289532294,f(w) = E n E
INTRODUCTION,0.1358574610244989,"✏f (w; n, ✏),
where
f(w; n, ✏) = −N log p(xn | Tw(✏)) −log p(Tw(✏)) −H(w),"
INTRODUCTION,0.13808463251670378,"(2)
and its gradient can be estimated ""naively"" by drawing a random n and ✏, and evaluating
55"
INTRODUCTION,0.1403118040089087,"gnaive(w; n, ✏) = rf(w; n, ✏).
(3)"
INTRODUCTION,0.14253897550111358,"BBVI has two advantages. First, since it only evaluates log p (and its gradient) at various points,
56"
INTRODUCTION,0.1447661469933185,"it can be applied to a diverse range of models, including those with complex and non-conjugate
57"
INTRODUCTION,0.14699331848552338,"likelihoods. Second, by subsampling data it can be applied to large datasets that might be impractical
58"
INTRODUCTION,0.1492204899777283,"for traditional methods like MCMC [12, 16].
59"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.1514476614699332,"3
Sources of gradient variance in BBVI
60"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.15367483296213807,"Let Vn,✏[rf(w; n, ✏)] denote the variance1 of the naive estimator from Eq. 3. The two sources for
61"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.155902004454343,"this variance correspond to data subsampling (n) and Monte Carlo noise (✏). It is natural to ask how
62"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.15812917594654788,"much variance each of these sources contributes. We study this by (numerically) integrating out each
63"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.1603563474387528,"of these random variables individually and comparing the variances of the resulting estimators.
64"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.16258351893095768,"Let f(w; n) = E✏f(w; n, ✏) be the objective for a single datum n with Monte Carlo noise integrated
65"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.16481069042316257,"out. This can be thought of as an estimator for datum n with a ""perfect"" control variate. Similarly,
66"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.16703786191536749,"let f(w; ✏) = En f(w; n, ✏) be the objective for a ﬁxed ✏evaluated on the full dataset. In Fig. 1 we
67"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.16926503340757237,"generate a single optimization trace using our gradient estimator (described below). Then, for each
68"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.1714922048997773,"iteration, we estimate the variance of rf(w; n, ✏), rf(w; ✏), and rf(w; n) 2 using a large number
69"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.17371937639198218,"of samples. In Table 1 we show the variance at the ﬁnal iterate on a variety of datasets. (For large
70"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.1759465478841871,"datasets, it is too expensive to compute the variance this way at each iteration.)
71"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.17817371937639198,"Our empirical ﬁndings suggest that, despite the exact mix of the two sources being task dependent,
72"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.18040089086859687,"subsampling noise is usually larger than MC noise. They also show the limits of reducing a single
73"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.18262806236080179,"source of noise: No control variate applied to each datum could do better than rf(w; n), while no
74"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.18485523385300667,"incremental-gradient-type method could do better than rf(w; ✏).
75"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.1870824053452116,"1For a vector-valued random variable z, we let V[z] = tr C[z]
2Aligned with the experiments in Sec. 7, our evaluation of subsampling variance uses mini-batches, i.e.
VB [En2B rf(w; n)], where B are mini batches sampled without replacement from {1, . . . , N}."
SOURCES OF GRADIENT VARIANCE IN BBVI,0.18930957683741648,"Figure 1: Gradient Variance Decomposition in Bayesian Logistic Regression using Mean-ﬁeld
BBVI. The orange line denotes variance from data subsampling (n), and the green line denotes Monte
Carlo (MC) noise variance (✏). For Sonar, both noise sources exhibit similar scales with a batch size
of 5. However, for Australian, subsampling noise dominates. Regardless, our proposed gradient
estimator gdual (red line, Eq. (4)) mitigates subsampling noise and controls MC noise, aligning
closely with or below the green line (i.e. the variance without data subsampling) in both datasets."
SOURCES OF GRADIENT VARIANCE IN BBVI,0.1915367483296214,"Task
Vn,✏[rf(w; n, ✏)]
Vn[rf(w; n)]
V✏[rf(w; ✏)]"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.19376391982182628,"Sonar
4.04 ⇥104
2.02 ⇥104
1.16 ⇥104"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.19599109131403117,"Australian
9.16 ⇥104
8.61 ⇥104
2.07 ⇥103"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.19821826280623608,"MNIST
4.21 ⇥108
3.21 ⇥108
1.75 ⇥104"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.20044543429844097,"PPCA
1.69 ⇥1010
1.68 ⇥1010
3.73 ⇥107"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.2026726057906459,"Tennis
9.96 ⇥107
9.59 ⇥107
8.56 ⇥104"
SOURCES OF GRADIENT VARIANCE IN BBVI,0.20489977728285078,"Table 1: BBVI gradient variance decomposition across various tasks, computed at the optimization
endpoint. Using a batch size of 100, step size of 1e−2 for MNIST, PPCA, and Tennis, and a batch
size of 5, step size of 5e−4 for Sonar and Australian. We generally observe subsampling noise
Vn[rf(w; n)] surpassing MC noise V✏[rf(w; ✏)]."
DUAL CONTROL VARIATE,0.2071269487750557,"4
Dual Control Variate
76"
DUAL CONTROL VARIATE,0.20935412026726058,"We now introduce the dual control variate, a new approach for controlling the variance of gradient
77"
DUAL CONTROL VARIATE,0.21158129175946547,"estimators for BBVI. Control variates [24] can reduce the variance of a gradient estimator by adding
78"
DUAL CONTROL VARIATE,0.21380846325167038,"a zero-mean random variable that is negatively correlated with the gradient estimator. Considering
79"
DUAL CONTROL VARIATE,0.21603563474387527,"that the objective of BBVI is a function of both n and ✏, an ideal control variate should also be a
80"
DUAL CONTROL VARIATE,0.2182628062360802,"function of these variables. We take two steps to construct such a control variate.
81"
DUAL CONTROL VARIATE,0.22048997772828507,"1. Inspired by existing control variates for BBVI [19, 9], we create an approximation ˜f(w; n, ✏)
82"
DUAL CONTROL VARIATE,0.22271714922049,"of the true objective f(w; n, ✏), designed so that the expectation E✏r ˜f(w; n, ✏) can easily be
83"
DUAL CONTROL VARIATE,0.22494432071269488,"computed for any datum n. A common strategy for this is a Taylor-appproximation—to replace
84"
DUAL CONTROL VARIATE,0.22717149220489977,"f with a low-order polynomial. Then, if the base distribution s(✏) is simple, the expectation
85"
DUAL CONTROL VARIATE,0.22939866369710468,"E✏[r ˜f(w; n, ✏)] is often available in closed-form.
86"
DUAL CONTROL VARIATE,0.23162583518930957,"2. Inspired by SAGA [6], we maintain a table W = {w1, . . . , wN} that stores the variational
87"
DUAL CONTROL VARIATE,0.23385300668151449,"parameters at the last iteration each of the data points x1, · · · , xN were accessed. We also
88"
DUAL CONTROL VARIATE,0.23608017817371937,"maintain a running average of gradient estimates evaluated at the stored parameters, denoted by
89"
DUAL CONTROL VARIATE,0.2383073496659243,"M. Unlike SAGA, however, this running average is for the gradients of the approximation ˜f, with
90"
DUAL CONTROL VARIATE,0.24053452115812918,"the Monte Carlo noise ✏integrated out, i.e. M = En E✏r ˜f(wn; n, ✏).
91"
DUAL CONTROL VARIATE,0.24276169265033407,"Intuitively, as optimization nears the solution, the weights w tend to change slowly. This means
92"
DUAL CONTROL VARIATE,0.24498886414253898,"that the entries wn in W will tend to become close to the current iterate w. Thus, if ˜f is a good
93"
DUAL CONTROL VARIATE,0.24721603563474387,"approximation of the true objective, we can expect rf(w; n, ✏) to be close to r ˜f(wn; n, ✏), meaning
94"
DUAL CONTROL VARIATE,0.24944320712694878,"the two will be strongly correlated. However, thanks to the running average M, the full expectation
95"
DUAL CONTROL VARIATE,0.2516703786191537,"of r ˜f(wn; n, ✏) is available in closed-form. This leads to our proposed gradient estimator
96"
DUAL CONTROL VARIATE,0.25389755011135856,"gdual(w; n, ✏) = rf(w; n, ✏) + E m E"
DUAL CONTROL VARIATE,0.2561247216035635,"⌘r ˜f(wm; m, ⌘) −r ˜f(wn; n, ✏)
|
{z
}
zero mean control variate cdual(w;n,✏) .
(4)"
DUAL CONTROL VARIATE,0.2583518930957684,Algorithm 1 Black-box variational inference with the dual control variate.
DUAL CONTROL VARIATE,0.26057906458797325,"Require: Learning rate λ, variational family qw(z), target p(z, x)
Require: Estimator f(w; n, ✏) whose expectation over n and ✏is the negative ELBO from qw and p (Eq. 2)
Require: Approximate estimator ˜f(w; n, ✏) that has an expectation over ✏in closed form"
DUAL CONTROL VARIATE,0.26280623608017817,"Initialize the parameter w0, the parameter table W = {w1, . . . , wN}
Initialize running mean M = E m E"
DUAL CONTROL VARIATE,0.2650334075723831,"⌘r ˜f(w0; m, ⌘)
. Closed-form expectation over ⌘, explicit sum over m"
DUAL CONTROL VARIATE,0.267260579064588,"for k = 1, 2, · · · do"
DUAL CONTROL VARIATE,0.26948775055679286,"Sample n and ✏
Extract the value of wn from the table W
Compute the base gradient g  rf(wk; n, ✏)
Compute the control variate c  E m E"
DUAL CONTROL VARIATE,0.2717149220489978,"⌘r ˜f(wm; m, ⌘) −r ˜f(wn; n, ✏) using E m E"
DUAL CONTROL VARIATE,0.2739420935412027,"⌘r ˜f(wm; m, ⌘) = M"
DUAL CONTROL VARIATE,0.27616926503340755,Update the running mean M  M + 1 N ! E
DUAL CONTROL VARIATE,0.27839643652561247,"⌘r ˜f(wk; n, ⌘) −E"
DUAL CONTROL VARIATE,0.2806236080178174,"⌘r ˜f(wn; n, ⌘) """
DUAL CONTROL VARIATE,0.2828507795100223,". Closed-form over ⌘
Update the table wn  wk
Update the parameter wk+1  wk −λ(g + c).
. Or use g + c in any stochastic optimization algorithm
end for"
DUAL CONTROL VARIATE,0.28507795100222716,"The running average M = En E✏r ˜f(wn; n, ✏) can be cheaply maintained through optimization,
97"
DUAL CONTROL VARIATE,0.2873051224944321,"since a single value wn changes per iteration and E✏r ˜f(w; n, ✏) is known in closed form. The
98"
DUAL CONTROL VARIATE,0.289532293986637,"variance of the proposed gradient estimator is given by
99"
DUAL CONTROL VARIATE,0.29175946547884185,V[gdual] = V
DUAL CONTROL VARIATE,0.29398663697104677,"✏,n[rf(w; n, ✏) −r ˜f(wn; n, ✏)].
(5)"
DUAL CONTROL VARIATE,0.2962138084632517,"Critically, this expression illustrates that the variance of gdual can be arbitrarily small, only limited by
100"
DUAL CONTROL VARIATE,0.2984409799554566,"how close ˜f is to f and how close the stored values wn are to the current parameters w.
101"
DUAL CONTROL VARIATE,0.30066815144766146,"We illustrate how this gradient estimator can be used for black-box variational inference in Alg. 1.
102"
DUAL CONTROL VARIATE,0.3028953229398664,"The same idea could be applied more generally to doubly-stochastic objectives in other domains,
103"
DUAL CONTROL VARIATE,0.3051224944320713,"using the more generic version of the algorithm given in Appendix. D.
104"
VARIANCE REDUCTION FOR STOCHASTIC OPTIMIZATION,0.30734966592427615,"5
Variance reduction for stochastic optimization
105"
VARIANCE REDUCTION FOR STOCHASTIC OPTIMIZATION,0.30957683741648107,"This section considers existing variance reduction techniques and how they compare to the proposed
106"
VARIANCE REDUCTION FOR STOCHASTIC OPTIMIZATION,0.311804008908686,"dual estimator.
107"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.31403118040089084,"5.1
Monte Carlo sampling and approximation-based control variates
108"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.31625835189309576,"Consider the variational objective from Eq. 2 where we sum over the full dataset in each iteration to
109"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.3184855233853007,"deﬁne the objective f(w) = E✏f(w; ✏). The gradient estimator obtained by sampling ✏has been
110"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.3207126948775056,"observed to sometimes have problematic variance. Previous work [21, 31, 11, 4] proposed to reduce
111"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.32293986636971045,"this variance by constructing a (zero-mean) control variate c(w; ✏) and deﬁning the new estimator
112"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.32516703786191536,"g(w; ✏) = rf(w; ✏) + c(w; ✏).
(6)"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.3273942093541203,"The hope is that c(w; ✏) ⇡rf(w) −rf(w; ✏) approximates the noise of the original estimator,
113"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.32962138084632514,"which can lead to large reductions in variance and thus more efﬁcient and reliable inference.
114"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.33184855233853006,"A general way to construct control variates involves using an approximation function ˜f ⇡f for
115"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.33407572383073497,"which the expectation E✏˜f(w, ✏) is available in closed-form [19, 9]. Then, the control variate is
116"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.3363028953229399,"deﬁned as c(w; ✏) = E⌘r ˜f(w; ⌘) −r ˜f(w; ✏), and the estimator from Eq. (6) becomes
117"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.33853006681514475,g(w; ✏) = rf(w; ✏) + E
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.34075723830734966,"⌘r ˜f(w; ⌘) −r ˜f(w; ✏).
(7)"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.3429844097995546,"Intuitively, the better ˜f approximates f, the lower the variance of this estimator tends to be (for a
118"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.34521158129175944,"perfect approximation, the variance is fully removed). A popular choice for ˜f involves a quadratic
119"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.34743875278396436,"function, either learned [9] or obtained through a second order Taylor expansion [19], since their
120"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.34966592427616927,"expectation under general Gaussian variational distributions is tractable.
121"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.3518930957683742,"In doubly-stochastic problems with objectives of the form f(w; n, ✏), data n is subsampled as well as
122"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.35412026726057905,"✏. While the above control variate has most commonly been used without subsampling, it can also be
123"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.35634743875278396,"used with subsampling, by developing an approximation ˜f(w; n, ✏) to f(w; n, ✏) for each datum n.
124"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.3585746102449889,"This leads to the control variate E⌘r ˜f(w; n, ⌘) −r ˜f(w; n, ✏) and gradient estimator
125"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.36080178173719374,"gcv(w; n, ✏) = rf(w; n, ✏) + E"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.36302895322939865,"⌘r ˜f(w; n, ⌘) −r ˜f(w; n, ✏)
|
{z
}
zero mean control variate ccv(w;n,✏) .
(8)"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.36525612472160357,"It is important to note that this control variate is unable to reduce variance coming from data
126"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.3674832962138085,"subsampling. Even if ˜f(w; n, ✏) were a perfect approximation there would still be gradient variance
127"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.36971046770601335,"due to n being sampled randomly. This can be shown by noting that the variance of this estimator is
128"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.37193763919821826,"given by (see Appendix. B.1 for a full derivation using the law of total variance)
129"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.3741648106904232,V[gcv] = E n V
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.37639198218262804,"✏[rf(w; n, ✏) −r ˜f(w; n, ✏)] + V"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.37861915367483295,n [rf(w; n)] ≥V
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.38084632516703787,"n [rf(w; n)].
(9)"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.3830734966592428,"While the ﬁrst term of the expression above can be made arbitrarily small in the ideal case of a perfect
130"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.38530066815144765,"approximation ˜f ⇡f, the second term is irreducible, regardless of the quality of the approximation
131"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.38752783964365256,"used. Therefore, this approach cannot reduce subsampling variance. As shown in Fig. 2 and Table 1,
132"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.3897550111358575,"subsampling variance is typically substantial, and often several orders of magnitude larger than
133"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.39198218262806234,"Monte-Carlo variance. When this is true, this control variate, which is only able to reduce variance
134"
MONTE CARLO SAMPLING AND APPROXIMATION-BASED CONTROL VARIATES,0.39420935412026725,"coming from Monte Carlo sampling, will have minimal effect on the overall gradient variance.
135"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.39643652561247217,"5.2
Data subsampling and incremental gradient methods
136"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.3986636971046771,"We now consider a stochastic optimization problem with objective f(w) = En f(w; n), where n is
137"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.40089086859688194,"uniformly distributed on {1, . . . , N}, representing data indices, but no other stochasticity (i.e. no
138"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.40311804008908686,"Monte Carlo sampling). While one could compute f or its gradient exactly, this is expensive when N
139"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.4053452115812918,"is large. A popular alternative involves drawing a random n and using the estimator rf(w; n) with a
140"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.40757238307349664,"stochastic optimization method, such as stochastic gradient descent. Alternatively, for such problems,
141"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.40979955456570155,"incremental gradient methods [26, 28, 13, 7, 10] often lead to faster convergence.
142"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.41202672605790647,"While details vary by algorithm, the basic idea of incremental gradient methods is to ""recycle""
143"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.4142538975501114,"previous gradient evaluations to reduce randomness. For example, SAGA [6] stores the parameters
144"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.41648106904231624,"wn of the most recent iteration where f(w; n) was evaluated and takes a step as
145"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.41870824053452116,w  w −λ ⇣
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.4209354120267261,rf(w; n) + E
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.42316258351893093,m rf(wm; m) −rf(wn; n) ⌘
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.42538975501113585,",
(10)"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.42761692650334077,"where λ is a step size and the expectation over m is tracked efﬁciently using a running average,
146"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.4298440979955457,"meaning the cost per iteration is independent of N. The update rule above can be interpreted as using
147"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.43207126948775054,"a control variate to reduce the variance of the naive estimator rf(w; n) as
148"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.43429844097995546,g(w; n) = rf(w; n) + E
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.4365256124721604,"m rf(wm; m) −rf(wn; n)
|
{z
}
zero mean control variate"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.43875278396436523,".
(11)"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.44097995545657015,"When wm ⇡w, the ﬁrst and last terms in Eq. (11) will approximately cancel, leading to a gradient
149"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.44320712694877507,"estimator with signiﬁcantly lower variance.
150"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.44543429844098,"We now consider a doubly-stochastic objective f(w; n, ✏). In principle, one might consider computing
151"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.44766146993318484,"the estimator from Eq. (11) for each value of ✏, i.e. using the gradient estimator
152"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.44988864142538976,"ginc(w; n, ✏) = rfn(w; n, ✏) + E"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.4521158129175947,"m rf(wm; m, ✏) −rf(wn; n, ✏)
|
{z
}
zero mean control variate cinc(w;n,✏)"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.45434298440979953,".
(12)"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.45657015590200445,"This has two issues. First, the resulting method does not address Monte Carlo noise due to sampling
153"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.45879732739420936,"✏. This can be shown by noting that the variance of this estimator is given by (see Appendix B.2)
154"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.4610244988864143,V[ginc] = E ✏V
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.46325167037861914,"n [rf(w; n, ✏) −rf(wn; n, ✏)] + V"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.46547884187082406,✏[rf(w; ✏)] ≥V
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.46770601336302897,"✏[rf(w; ✏)].
(13)"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.46993318485523383,"Since the second term in the variance expression above is irreducible, the variance cannot be expected
155"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.47216035634743875,"to go to zero, no matter how close all the stored vectors wn are to the current parameters. Intuitively,
156"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.47438752783964366,"this approach cannot do better than simply evaluating the objective on the full dataset for a random ✏.
157"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.4766146993318486,"The second issue is more critical: ginc cannot be implemented efﬁciently. The value of rf(wn; n, ✏)
158"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.47884187082405344,"is dependent on ✏, which is resampled at each iteration. Therefore, it is not possible to efﬁciently
159"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.48106904231625836,"maintain Em rf(wm; m, ✏) needed by Eq. (12) as a running average. In general, this can only
160"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.48329621380846327,"be computed by looping over the full dataset in each iteration. While possible, this destroys the
161"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.48552338530066813,"computational advantage of subsampling. For some models with special structure [32, 34] it is
162"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.48775055679287305,"possible to efﬁciently maintain the needed running gradient. However, this can only be done in
163"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.48997772828507796,"special cases with model-speciﬁc derivations, breaking the universality of BBVI.
164"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.4922048997772829,"It may seem odd that ginc has these computational issues, while gdual—an estimator intended to
165"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.49443207126948774,"reduce variance even further—does not. The fundamental reason is that the dual estimator only stores
166"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.49665924276169265,"(approximate) gradients after integrating over the Monte Carlo variable ✏, so the needed running
167"
DATA SUBSAMPLING AND INCREMENTAL GRADIENT METHODS,0.49888641425389757,"average is independent of ✏.
168"
ENSEMBLES OF CONTROL VARIATE,0.5011135857461024,"5.3
Ensembles of control variate
169"
ENSEMBLES OF CONTROL VARIATE,0.5033407572383074,"It is possible to combine multiple control variates. For example, [8] combined control variates that
170"
ENSEMBLES OF CONTROL VARIATE,0.5055679287305123,"reduced Monte Carlo noise [19] with one that reduced subsampling noise [32] (for a special case
171"
ENSEMBLES OF CONTROL VARIATE,0.5077951002227171,"where ginc is tractable). While this approach can be better than either control variate alone, it still does
172"
ENSEMBLES OF CONTROL VARIATE,0.5100222717149221,"not reduce joint variance. To see this, consider a gradient estimator that uses a convex combination of
173"
ENSEMBLES OF CONTROL VARIATE,0.512249443207127,"the two above control variates. For any β 2 (0, 1) write
174"
ENSEMBLES OF CONTROL VARIATE,0.5144766146993318,"gcombo(w; n, ✏) = rf(w; n, ✏) + βccv(w; n, ✏) + (1 −β)cinc(w; n, ✏)
|
{z
}
ccombo(w;n,✏)"
ENSEMBLES OF CONTROL VARIATE,0.5167037861915368,".
(14)"
ENSEMBLES OF CONTROL VARIATE,0.5189309576837416,"It can be shown (Appendix B.3) that if both ccv and cinc are ""perfect"", that is, if ˜f(w; n, ✏) =
175"
ENSEMBLES OF CONTROL VARIATE,0.5211581291759465,"f(w; n, ✏) and wn = w for all n, then
176"
ENSEMBLES OF CONTROL VARIATE,0.5233853006681515,V[gcombo] = β2 V
ENSEMBLES OF CONTROL VARIATE,0.5256124721603563,n [rf(w; n)] + (1 −β)2 V
ENSEMBLES OF CONTROL VARIATE,0.5278396436525612,"✏[rf(w; ✏)].
(15)"
ENSEMBLES OF CONTROL VARIATE,0.5300668151447662,"Even in this idealized scenario, such an estimator cannot reduce variance to zero, because each of
177"
ENSEMBLES OF CONTROL VARIATE,0.532293986636971,"the individual control variates leaves one source of noise uncontrolled. The dual control variate
178"
ENSEMBLES OF CONTROL VARIATE,0.534521158129176,"overcomes this because it models interactions between ✏and n.
179"
RELATED WORK,0.5367483296213809,"6
Related work
180"
RELATED WORK,0.5389755011135857,"Recent work proposed to approximate the optimal batch-dependent control variate for BBVI using
181"
RELATED WORK,0.5412026726057907,"a recognition network [4]. Similar to our work, they take into account the usage of subsampling
182"
RELATED WORK,0.5434298440979956,"when designing their variance reduction techniques for BBVI. However, like gcv, their control variate
183"
RELATED WORK,0.5456570155902004,"reduces the conditional variance of MC noise (conditioned on n) but is unable to reduce subsampling
184"
RELATED WORK,0.5478841870824054,"noise (like gcv).
185"
RELATED WORK,0.5501113585746102,"It is also worth discussing a special incremental gradient method called SMISO [1], designed
186"
RELATED WORK,0.5523385300668151,"for doubly-stochastic problems. Intuitively, SMISO uses exponential averaging to approximately
187"
RELATED WORK,0.5545657015590201,"marginalize out ✏, and then runs MISO/Finito [7, 18] (an incremental gradient method similar to
188"
RELATED WORK,0.5567928730512249,"SAGA) to control the subsampling noise. While the method is similar to running SGD with an
189"
RELATED WORK,0.5590200445434298,"incremental control variate, it is not obvious how to separate the control variate from the algorithm,
190"
RELATED WORK,0.5612472160356348,"meaning we cannot use the SMISO idea as a control variate to get a gradient estimator that can be
191"
RELATED WORK,0.5634743875278396,"used with other optimizers like Adam, we include a detailed discussion on this issue in Appendix. A.
192"
RELATED WORK,0.5657015590200446,"Nevertheless, we still include SMISO as one of our baselines.
193"
EXPERIMENTS,0.5679287305122495,"7
Experiments
194"
EXPERIMENTS,0.5701559020044543,"This section empirically demonstrates the effectiveness of the dual control variate for BBVI. We
195"
EXPERIMENTS,0.5723830734966593,"focus on mean-ﬁeld Gaussian BBVI, where the variational posterior follows a multivariate Gaussian
196"
EXPERIMENTS,0.5746102449888641,"with diagonal covariance qw(z) = N(µ, diag(σ2)), with parameters w = (µ, log(σ)).
197"
EXPERIMENTS,0.576837416481069,"The gradient estimators gcv(w; n, ✏) and gdual(w; n, ✏) require an approximation function with
198"
EXPERIMENTS,0.579064587973274,"expectation over ✏available in closed form. Inspired by previous work [19], we get an approximation
199"
EXPERIMENTS,0.5812917594654788,"for f (w; n, ✏) using a second order Taylor expansion for the negative total likelihood kn(z) =
200"
EXPERIMENTS,0.5835189309576837,"Figure 2: Dual control variate helps reduce gradient variance. The naive gradient estimator
(Eq. (3)) is the baseline, while the cv estimator (Eq. (8)) controls the Monte Carlo noise, the inc
estimator (Eq. (12)) controls for subsampling noise, and the proposed dual estimator (Eq. (4)) controls
for both. The variance of cv and inc, as is shown in Eq. (9) and Eq. (13) are lower-bounded by the
dotted lines, while dual is capable of reducing the variance to signiﬁcantly lower values, leading to
better and faster convergence (Fig. 3)."
EXPERIMENTS,0.5857461024498887,"N log p(xn | z) + log p(z) around z0 = Tw(0)3, which yields
201"
EXPERIMENTS,0.5879732739420935,"˜f (w; n, ✏) = kn(z0) + (Tw(✏) −z0)>rkn(z0) + 1"
EXPERIMENTS,0.5902004454342984,"2(Tw(✏) −z0)>r2kn(z0)(Tw(✏) −z0)> + H(w), (16)"
EXPERIMENTS,0.5924276169265034,"where we assume the entropy can be computed in closed-form. For a mean-ﬁeld Gaussian variational
202"
EXPERIMENTS,0.5946547884187082,"distribution, the expected gradient of the approximation Eq. (16) can only be computed efﬁciently
203"
EXPERIMENTS,0.5968819599109132,"(via Hessian-vector products) with respect to the mean parameter µ but not for the scale parameter
204"
EXPERIMENTS,0.5991091314031181,"σ, which means gcv(w; n, ✏) and gdual(w; n, ✏) can only be used as the gradient estimator for µ.
205"
EXPERIMENTS,0.6013363028953229,"Fortunately, controlling only the gradient variance on µ often means controlling most of the variance,
206"
EXPERIMENTS,0.6035634743875279,"as, with mean-ﬁeld Gaussians, the total gradient variance is often dominated by variance from µ [9].
207"
EXPERIMENT SETUP,0.6057906458797327,"7.1
Experiment setup
208"
EXPERIMENT SETUP,0.6080178173719376,"We evaluate our methods by performing BBVI on a range of tasks: binary Bayesian logistic regression
209"
EXPERIMENT SETUP,0.6102449888641426,"on two datasets, Sonar (number of samples N = 208, dimensionality D = 60) and Australian (N =
210"
EXPERIMENT SETUP,0.6124721603563474,"690, D = 14); multi-class Bayesian logistic regression on MNIST [17] (N = 60000, D = 7840);
211"
EXPERIMENT SETUP,0.6146993318485523,"probabilistic principal component analysis [29] (PPCA, N = 60000, D = 12544); and Bradley-
212"
EXPERIMENT SETUP,0.6169265033407573,"Terry model [5] for tennis player ranking (Tennis, N = 169405, D = 5525). We give full model
213"
EXPERIMENT SETUP,0.6191536748329621,"descriptions in Sec. 7.3.
214"
EXPERIMENT SETUP,0.621380846325167,"Baselines. We compare gdual (Eq. (4)) with gnaive (Eq. (3)) and gcv (Eq. (8)). For Sonar and
215"
EXPERIMENT SETUP,0.623608017817372,"Australian (small datasets) we include ginc (Eq. (12)) as well, which requires a full pass through the
216"
EXPERIMENT SETUP,0.6258351893095768,"full dataset at each iteration. For larger-scale problems, ginc becomes intractable, so we use SMISO
217"
EXPERIMENT SETUP,0.6280623608017817,"instead.
218"
EXPERIMENT SETUP,0.6302895322939867,"Optimization details. We optimize using Adam [14] for the larger-scale MNIST, PPCA, and
219"
EXPERIMENT SETUP,0.6325167037861915,"Tennis datasets and SGD without momentum for the small-scale Sonar and Australian dataset for
220"
EXPERIMENT SETUP,0.6347438752783965,"transparency. The optimizer for SMISO is pre-determined by its algorithmic structure and cannot
221"
EXPERIMENT SETUP,0.6369710467706013,"be changed. For all estimators, we perform a step-size search (see Appendix C) to ensure a fair
222"
EXPERIMENT SETUP,0.6391982182628062,"comparison and use a single shared ✏for all samples in the batch.
223"
EXPERIMENT SETUP,0.6414253897550112,"Mini-batching. In practice, for efﬁcient implementation on GPUs, we draw a mini-batch B of data at
224"
EXPERIMENT SETUP,0.643652561247216,"each iteration (reshufﬂing for each epoch). For inc, dual, and SMISO, we update multiple entities
225"
EXPERIMENT SETUP,0.6458797327394209,"in the parameter table per iteration and adjust the running mean accordingly. For the Sonar and
226"
EXPERIMENT SETUP,0.6481069042316259,"Australian datasets, due to their small sizes, we use |B| = 5. For other datasets we use |B| = 100.
227"
EXPERIMENT SETUP,0.6503340757238307,"Evaluation metrics. We track the ELBO on the full dataset, explicitly computing En (summing
228"
EXPERIMENT SETUP,0.6525612472160356,"over the full dataset) and approximating E✏with 5000 Monte Carlo samples. We present ELBO
229"
EXPERIMENT SETUP,0.6547884187082406,"vs. iterations plots for a single example learning rate as well as ELBO values for the best learning
230"
EXPERIMENT SETUP,0.6570155902004454,"rate chosen retrospectively for each iteration. In addition, we present the ﬁnal ELBO after training
231"
EXPERIMENT SETUP,0.6592427616926503,"vs. step size at different iterations. For the Sonar and Australian datasets, given the small size, we
232"
EXPERIMENT SETUP,0.6614699331848553,"include a detailed trace of gradient variance on µ across different estimators. This enables empirical
233"
EXPERIMENT SETUP,0.6636971046770601,"validation of the lower bounds derived in Eq. (9) and Eq. (13).
234"
EXPERIMENT SETUP,0.6659242761692651,3We use z0 = stop_gradient (Tw(0)) so that the gradient does not backpropagate from z0 to w.
EXPERIMENT SETUP,0.6681514476614699,"
	

!! $ $"
EXPERIMENT SETUP,0.6703786191536748, 
EXPERIMENT SETUP,0.6726057906458798,"
	

!!  !"
EXPERIMENT SETUP,0.6748329621380846,"

!"
EXPERIMENT SETUP,0.6770601336302895,!!
EXPERIMENT SETUP,0.6792873051224945,"

!"
EXPERIMENT SETUP,0.6815144766146993,!!
EXPERIMENT SETUP,0.6837416481069042,"#
#

"""
EXPERIMENT SETUP,0.6859688195991092,"
	

 !	 !	 "
EXPERIMENT SETUP,0.688195991091314, 
EXPERIMENT SETUP,0.6904231625835189,
EXPERIMENT SETUP,0.6926503340757239,"
	

"
EXPERIMENT SETUP,0.6948775055679287,
EXPERIMENT SETUP,0.6971046770601337,"

"
EXPERIMENT SETUP,0.6993318485523385,
EXPERIMENT SETUP,0.7015590200445434,"

"
EXPERIMENT SETUP,0.7037861915367484,
EXPERIMENT SETUP,0.7060133630289532,"Figure 3: With reduced variance (Fig. 2), the dual estimator provides better convergence at a
larger step size. On Sonar, Monte Carlo noise and subsampling noise are of similar scale, therefore
jointly controlling them shows better performance than methods that only control one source of noise.
On Australian, where the subsampling noise dominates, dual shows similar performance compared
with inc, which controls subsampling noise but cannot be efﬁciently computed (requires pass over
the full dataset at each iteration)."
EXPERIMENT SETUP,0.7082405345211581,"Initialization. The variational parameters are randomly initialized using a standard Gaussian and
235"
EXPERIMENT SETUP,0.7104677060133631,"all results reported are averages over multiple independent trials: We run 10 trials for Sonar and
236"
EXPERIMENT SETUP,0.7126948775055679,"Australian, and 5 for the larger scale problems due to resource constraint.
237"
RESULTS,0.7149220489977728,"7.2
Results
238"
RESULTS,0.7171492204899778,"The experiment results for Sonar and Australian are presented in Fig. 2 and Fig. 3. Both the inc and
239"
RESULTS,0.7193763919821826,"cv estimators have lower variance than the naive estimator, but the improvement varies by the dataset.
240"
RESULTS,0.7216035634743875,"The excellent performance of the (impractical) inc estimator on Australian shows the importance
241"
RESULTS,0.7238307349665924,"of reducing subsampling noise. Overall, the dual estimator has the lowest variance, which enables
242"
RESULTS,0.7260579064587973,"larger learning rates and thus faster optimization.
243"
RESULTS,0.7282850779510023,"Similar results can be observed on MNIST, PPCA, and Tennis in Fig. 4 (for these datasets inc
244"
RESULTS,0.7305122494432071,"is intractable, so we include SMISO as a baseline instead). Again, dual yields faster and better
245"
RESULTS,0.732739420935412,"convergence than naive and cv. Whereas SMISO, which does not adopt momentum nor adaptive step
246"
RESULTS,0.734966592427617,"size, suffers from slow convergence speed in that it has to utilize a small step size to prevent diverging
247"
RESULTS,0.7371937639198218,"during optimization. We provide comparisons of different estimators using SGD in Appendix. E.
248"
MODEL DESCRIPTIONS,0.7394209354120267,"7.3
Model descriptions
249"
MODEL DESCRIPTIONS,0.7416481069042317,"Binary/Multi-class Bayesian logistic regression. A standard logistic regression model with standard
250"
MODEL DESCRIPTIONS,0.7438752783964365,"Gaussian prior.
251"
MODEL DESCRIPTIONS,0.7461024498886414,"Probabilistic principal component analysis (PPCA). Given a centered dataset x1, . . . , xN 2 RD,
252"
MODEL DESCRIPTIONS,0.7483296213808464,"PPCA [29] seeks to extract its principal axes W 2 RD⇥K by assuming xn ⇠N(0, W W > +
253"
MODEL DESCRIPTIONS,0.7505567928730512,"diag(λ2)). In our experiments, we employ a standard Gaussian prior on W and use BBVI to
254"
MODEL DESCRIPTIONS,0.7527839643652561,"approximate the posterior over W . We then test PPCA on the standardized training set of MNIST
255"
MODEL DESCRIPTIONS,0.755011135857461,"with K = 16 and λ = 1.
256"
MODEL DESCRIPTIONS,0.7572383073496659,"Bradley Terry model (Tennis). This is a model used to rank players from pair-wise matches.
257"
MODEL DESCRIPTIONS,0.7594654788418709,"Each player is represented by a score ✓i, and each score is assigned a standard Gaussian prior. The
258"
MODEL DESCRIPTIONS,0.7616926503340757,"result of a match between two players is modeled by the inverse logit of their score difference
259"
MODEL DESCRIPTIONS,0.7639198218262806,"yn ⇠Bernoulli(logit−1(✓i −✓j)) where yn = 1 denotes a win by player n. We subsample over
260"
MODEL DESCRIPTIONS,0.7661469933184856,"matches and perform inference over the score of each player. We evaluate the model on men’s tennis
261"
MODEL DESCRIPTIONS,0.7683741648106904,"matches log starting from 1960, which contains the results of 169405 matches among 5525 players.
262"
MODEL DESCRIPTIONS,0.7706013363028953,"Figure 4: On larger scale problems, the dual estimator leads to improved convergence. In
large-scale problems, cv shows little or no improvement upon naive while dual converges faster.
We suspect that most of the improvement in the dual estimator comes from reducing subsampling
variance. SMISO shows slow convergence. We suspect that is because it is an “SGD-type” algorithm
while all others use Adam. Note that the step size for SMISO is rescaled for visualization. The loss
shows periodic structure in Tennis, this happens because gradients have correlated noise that cancels
out at the end of each epoch."
MODEL DESCRIPTIONS,0.7728285077951003,"Estimator
Variance lower bound
rf evals per iteration
Wall-clock time per iteration"
MODEL DESCRIPTIONS,0.7750556792873051,"MNIST
PPCA
Tennis"
MODEL DESCRIPTIONS,0.77728285077951,"naive
Vn,✏[rf(w; n, ✏)]
1
10.4ms
12.8ms
10.2ms
cv
Vn[rf(w; n)]
2
12.8ms
18.5ms
14.6ms
inc
V✏[rf(w; ✏)]
N+2
328ms
897ms
588ms
dual
0
3
17.6ms
31.2ms
29.6ms
Fullbatch-naive
V✏[rf(w; ✏)]
N
201ms
740ms
203ms
Fullbatch-ccv
0
2N
360ms
1606ms
246ms
Table 2: Variance, oracle complexity, and wall-clock time for different estimators. Notice that inc
is more expensive than Fullbatch-naive. We hypothesize this is because inc uses separate wn for
different data points, which is less efﬁcient for parallelism."
MODEL DESCRIPTIONS,0.779510022271715,"7.4
Efﬁciency analysis
263"
MODEL DESCRIPTIONS,0.7817371937639198,"We now study the computational cost of different estimators. In terms of the number of ""oracle""
264"
MODEL DESCRIPTIONS,0.7839643652561247,"evaluations (i.e. evaluations of f(w; n, ✏) or its gradient), the naive estimator is the most efﬁcient,
265"
MODEL DESCRIPTIONS,0.7861915367483296,"requiring a single oracle evaluation per iteration. The cv estimator requires one gradient and also
266"
MODEL DESCRIPTIONS,0.7884187082405345,"one Hessian-vector product, while the dual estimator needs one gradient and two Hessian-vector
267"
MODEL DESCRIPTIONS,0.7906458797327395,"products, one for the control variate and one for updating the running mean M.
268"
MODEL DESCRIPTIONS,0.7928730512249443,"Additionally, Table 2 shows measured runtimes based on a JAX implementation on an Nvidia 2080ti
269"
MODEL DESCRIPTIONS,0.7951002227171492,"GPU. All numbers are for a single optimization step, averaged over 200 steps. Overall, each iteration
270"
MODEL DESCRIPTIONS,0.7973273942093542,"with the dual estimator is between 1.5 to 2.5 times slower than naive, and around 1.2 times slower
271"
MODEL DESCRIPTIONS,0.799554565701559,"than cv. Lastly, given that dual achieves a given performance in an order of magnitude fewer
272"
MODEL DESCRIPTIONS,0.8017817371937639,"iterations (Figs. 3 and 4), it is the fastest in terms of wall-clock time. The exact wall-clock time v.s.
273"
MODEL DESCRIPTIONS,0.8040089086859689,"ELBO results are presented in Appendix. F.
274"
REFERENCES,0.8062360801781737,"References
275"
REFERENCES,0.8084632516703786,"[1] Alberto Bietti and Julien Mairal. Stochastic optimization with variance reduction for inﬁnite
276"
REFERENCES,0.8106904231625836,"datasets with ﬁnite sum structure. Advances in Neural Information Processing Systems, 30:1623–
277"
REFERENCES,0.8129175946547884,"1633, 2017.
278"
REFERENCES,0.8151447661469933,"[2] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for
279"
REFERENCES,0.8173719376391982,"statisticians. Journal of the American statistical Association, 112(518):859–877, 2017.
280"
REFERENCES,0.8195991091314031,"[3] Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
281"
REFERENCES,0.821826280623608,"learning. Siam Review, 60(2):223–311, 2018.
282"
REFERENCES,0.8240534521158129,"[4] Ayman Boustati, Sattar Vakili, James Hensman, and ST John. Amortized variance reduction
283"
REFERENCES,0.8262806236080178,"for doubly stochastic objective. In Conference on Uncertainty in Artiﬁcial Intelligence, pages
284"
REFERENCES,0.8285077951002228,"61–70. PMLR, 2020.
285"
REFERENCES,0.8307349665924276,"[5] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the
286"
REFERENCES,0.8329621380846325,"method of paired comparisons. Biometrika, 39(3/4):324–345, 1952.
287"
REFERENCES,0.8351893095768375,"[6] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient
288"
REFERENCES,0.8374164810690423,"method with support for non-strongly convex composite objectives. In Advances in neural
289"
REFERENCES,0.8396436525612472,"information processing systems, pages 1646–1654, 2014.
290"
REFERENCES,0.8418708240534521,"[7] Aaron Defazio, Justin Domke, et al. Finito: A faster, permutable incremental gradient method
291"
REFERENCES,0.844097995545657,"for big data problems. In International Conference on Machine Learning, pages 1125–1133.
292"
REFERENCES,0.8463251670378619,"PMLR, 2014.
293"
REFERENCES,0.8485523385300668,"[8] Tomas Geffner and Justin Domke. Using large ensembles of control variates for variational
294"
REFERENCES,0.8507795100222717,"inference. In Advances in Neural Information Processing Systems, pages 9982–9992, 2018.
295"
REFERENCES,0.8530066815144766,"[9] Tomas Geffner and Justin Domke. Approximation based variance reduction for reparameteriza-
296"
REFERENCES,0.8552338530066815,"tion gradients. Advances in Neural Information Processing Systems, 33, 2020.
297"
REFERENCES,0.8574610244988864,"[10] Robert M Gower, Mark Schmidt, Francis Bach, and Peter Richtárik. Variance-reduced methods
298"
REFERENCES,0.8596881959910914,"for machine learning. Proceedings of the IEEE, 108(11):1968–1983, 2020.
299"
REFERENCES,0.8619153674832962,"[11] Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation
300"
REFERENCES,0.8641425389755011,"through the void: Optimizing control variates for black-box gradient estimation. In International
301"
REFERENCES,0.8663697104677061,"Conference on Learning Representations, 2018.
302"
REFERENCES,0.8685968819599109,"[12] Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational
303"
REFERENCES,0.8708240534521158,"inference. Journal of Machine Learning Research, 2013.
304"
REFERENCES,0.8730512249443207,"[13] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
305"
REFERENCES,0.8752783964365256,"reduction. Advances in neural information processing systems, 26:315–323, 2013.
306"
REFERENCES,0.8775055679287305,"[14] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
307"
REFERENCES,0.8797327394209354,"arXiv:1412.6980, 2014.
308"
REFERENCES,0.8819599109131403,"[15] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In International
309"
REFERENCES,0.8841870824053452,"Conference on Learning Representations, 2014.
310"
REFERENCES,0.8864142538975501,"[16] Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M Blei. Automatic
311"
REFERENCES,0.888641425389755,"differentiation variational inference. The Journal of Machine Learning Research, 18(1):430–474,
312"
REFERENCES,0.89086859688196,"2017.
313"
REFERENCES,0.8930957683741648,"[17] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
314"
REFERENCES,0.8953229398663697,"applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
315"
REFERENCES,0.8975501113585747,"[18] Julien Mairal. Incremental majorization-minimization optimization with application to large-
316"
REFERENCES,0.8997772828507795,"scale machine learning. SIAM Journal on Optimization, 25(2):829–855, 2015.
317"
REFERENCES,0.9020044543429844,"[19] Andrew C Miller, Nicholas J Foti, Alexander D’Amour, and Ryan P Adams.
Reducing
318"
REFERENCES,0.9042316258351893,"reparameterization gradient variance. Advances in Neural Information Processing Systems,
319"
REFERENCES,0.9064587973273942,"2017:3709–3719, 2017.
320"
REFERENCES,0.9086859688195991,"[20] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
321"
REFERENCES,0.910913140311804,"approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574–
322"
REFERENCES,0.9131403118040089,"1609, 2009.
323"
REFERENCES,0.9153674832962138,"[21] John Paisley, David M Blei, and Michael I Jordan. Variational bayesian inference with stochastic
324"
REFERENCES,0.9175946547884187,"search. In Proceedings of the 29th International Coference on International Conference on
325"
REFERENCES,0.9198218262806236,"Machine Learning, pages 1363–1370, 2012.
326"
REFERENCES,0.9220489977728286,"[22] Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artiﬁcial
327"
REFERENCES,0.9242761692650334,"intelligence and statistics, pages 814–822. PMLR, 2014.
328"
REFERENCES,0.9265033407572383,"[23] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation
329"
REFERENCES,0.9287305122494433,"and approximate inference in deep generative models. In International conference on machine
330"
REFERENCES,0.9309576837416481,"learning, pages 1278–1286. PMLR, 2014.
331"
REFERENCES,0.933184855233853,"[24] Christian P Robert, George Casella, and George Casella. Monte Carlo statistical methods,
332"
REFERENCES,0.9354120267260579,"volume 2. Springer, 1999.
333"
REFERENCES,0.9376391982182628,"[25] Geoffrey Roeder, Yuhuai Wu, and David K Duvenaud. Sticking the landing: Simple, lower-
334"
REFERENCES,0.9398663697104677,"variance gradient estimators for variational inference. In I. Guyon, U. Von Luxburg, S. Bengio,
335"
REFERENCES,0.9420935412026726,"H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Informa-
336"
REFERENCES,0.9443207126948775,"tion Processing Systems, volume 30. Curran Associates, Inc., 2017.
337"
REFERENCES,0.9465478841870824,"[26] Nicolas Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an expo-
338"
REFERENCES,0.9487750556792873,"nential convergence _rate for ﬁnite training sets. Advances in neural information processing
339"
REFERENCES,0.9510022271714922,"systems, 25, 2012.
340"
REFERENCES,0.9532293986636972,"[27] Hugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep gaussian
341"
REFERENCES,0.955456570155902,"processes. Advances in neural information processing systems, 30, 2017.
342"
REFERENCES,0.9576837416481069,"[28] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized
343"
REFERENCES,0.9599109131403119,"loss minimization. Journal of Machine Learning Research, 14(2), 2013.
344"
REFERENCES,0.9621380846325167,"[29] Michael E Tipping and Christopher M Bishop. Probabilistic principal component analysis.
345"
REFERENCES,0.9643652561247216,"Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611–622,
346"
REFERENCES,0.9665924276169265,"1999.
347"
REFERENCES,0.9688195991091314,"[30] Michalis Titsias and Miguel Lázaro-Gredilla. Doubly stochastic variational bayes for non-
348"
REFERENCES,0.9710467706013363,"conjugate inference. In International conference on machine learning, pages 1971–1979. PMLR,
349"
REFERENCES,0.9732739420935412,"2014.
350"
REFERENCES,0.9755011135857461,"[31] George Tucker, Andriy Mnih, Chris J Maddison, Dieterich Lawson, and Jascha Sohl-Dickstein.
351"
REFERENCES,0.977728285077951,"Rebar: low-variance, unbiased gradient estimates for discrete latent variable models.
In
352"
REFERENCES,0.9799554565701559,"Proceedings of the 31st International Conference on Neural Information Processing Systems,
353"
REFERENCES,0.9821826280623608,"pages 2624–2633, 2017.
354"
REFERENCES,0.9844097995545658,"[32] Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic
355"
REFERENCES,0.9866369710467706,"gradient optimization. Advances in neural information processing systems, 26, 2013.
356"
REFERENCES,0.9888641425389755,"[33] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-
357"
REFERENCES,0.9910913140311804,"ment learning. Reinforcement learning, pages 5–32, 1992.
358"
REFERENCES,0.9933184855233853,"[34] Shuai Zheng and James Tin-Yau Kwok. Lightweight stochastic optimization for minimizing
359"
REFERENCES,0.9955456570155902,"ﬁnite sums with inﬁnite data. In International Conference on Machine Learning, pages 5932–
360"
REFERENCES,0.9977728285077951,"5940. PMLR, 2018.
361"
